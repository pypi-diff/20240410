# Comparing `tmp/datarobotx-0.1.8-py3-none-any.whl.zip` & `tmp/datarobotx-0.1.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,58 +1,63 @@
-Zip file size: 143113 bytes, number of entries: 56
--rw-rw-r--  2.0 unx     1566 b- defN 23-May-25 20:03 datarobotx/__init__.py
--rw-rw-r--  2.0 unx      271 b- defN 23-May-25 20:03 datarobotx/_version.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-25 20:09 datarobotx/py.typed
--rw-rw-r--  2.0 unx        0 b- defN 23-May-25 20:09 datarobotx/client/__init__.py
--rw-rw-r--  2.0 unx     9402 b- defN 23-May-25 20:03 datarobotx/client/datasets.py
--rw-rw-r--  2.0 unx    16124 b- defN 23-May-25 20:03 datarobotx/client/deployments.py
--rw-rw-r--  2.0 unx     1002 b- defN 23-May-25 20:03 datarobotx/client/prediction_servers.py
--rw-rw-r--  2.0 unx    25695 b- defN 23-May-25 20:03 datarobotx/client/projects.py
--rw-rw-r--  2.0 unx     2425 b- defN 23-May-25 20:03 datarobotx/client/status.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-25 20:09 datarobotx/common/__init__.py
--rw-rw-r--  2.0 unx     7760 b- defN 23-May-25 20:03 datarobotx/common/client.py
--rw-rw-r--  2.0 unx     9358 b- defN 23-May-25 20:03 datarobotx/common/config.py
--rw-rw-r--  2.0 unx     4588 b- defN 23-May-25 20:03 datarobotx/common/configurator.py
--rw-rw-r--  2.0 unx   131191 b- defN 23-May-25 20:03 datarobotx/common/dr_config.py
--rw-rw-r--  2.0 unx    10494 b- defN 23-May-25 20:03 datarobotx/common/logging.py
--rw-rw-r--  2.0 unx     3980 b- defN 23-May-25 20:03 datarobotx/common/transformations.py
--rw-rw-r--  2.0 unx    15032 b- defN 23-May-25 20:03 datarobotx/common/ts_helpers.py
--rw-rw-r--  2.0 unx      605 b- defN 23-May-25 20:03 datarobotx/common/types.py
--rw-rw-r--  2.0 unx    27474 b- defN 23-May-25 20:03 datarobotx/common/utils.py
--rw-rw-r--  2.0 unx      143 b- defN 23-May-25 20:03 datarobotx/llm/__init__.py
--rw-rw-r--  2.0 unx     3769 b- defN 23-May-25 20:03 datarobotx/llm/utils.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-25 20:09 datarobotx/llm/chains/__init__.py
--rw-rw-r--  2.0 unx    12665 b- defN 23-May-25 20:03 datarobotx/llm/chains/data_dict.py
--rw-rw-r--  2.0 unx    12342 b- defN 23-May-25 20:03 datarobotx/llm/chains/enrich.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-25 20:09 datarobotx/models/__init__.py
--rw-rw-r--  2.0 unx     2754 b- defN 23-May-25 20:03 datarobotx/models/autoanomaly.py
--rw-rw-r--  2.0 unx     3332 b- defN 23-May-25 20:03 datarobotx/models/autocluster.py
--rw-rw-r--  2.0 unx     2465 b- defN 23-May-25 20:03 datarobotx/models/automl.py
--rw-rw-r--  2.0 unx    11349 b- defN 23-May-25 20:03 datarobotx/models/autopilot.py
--rw-rw-r--  2.0 unx    11242 b- defN 23-May-25 20:03 datarobotx/models/autots.py
--rw-rw-r--  2.0 unx    14976 b- defN 23-May-25 20:03 datarobotx/models/colreduce.py
--rw-rw-r--  2.0 unx    24551 b- defN 23-May-25 20:03 datarobotx/models/deploy.py
--rw-rw-r--  2.0 unx    33947 b- defN 23-May-25 20:03 datarobotx/models/deployment.py
--rw-rw-r--  2.0 unx     7914 b- defN 23-May-25 20:03 datarobotx/models/evaluation.py
--rw-rw-r--  2.0 unx    16937 b- defN 23-May-25 20:03 datarobotx/models/featurediscovery.py
--rw-rw-r--  2.0 unx     5593 b- defN 23-May-25 20:03 datarobotx/models/intraproject.py
--rw-rw-r--  2.0 unx    28420 b- defN 23-May-25 20:03 datarobotx/models/model.py
--rw-rw-r--  2.0 unx     9736 b- defN 23-May-25 20:03 datarobotx/models/selfdiscovery.py
--rw-rw-r--  2.0 unx     4422 b- defN 23-May-25 20:03 datarobotx/models/share.py
--rw-rw-r--  2.0 unx    20800 b- defN 23-May-25 20:03 datarobotx/models/sparkingest.py
--rw-rw-r--  2.0 unx        0 b- defN 23-May-25 20:09 datarobotx/viz/__init__.py
--rw-rw-r--  2.0 unx     4906 b- defN 23-May-25 20:03 datarobotx/viz/charts.py
--rw-rw-r--  2.0 unx     6326 b- defN 23-May-25 20:03 datarobotx/viz/leaderboard.py
--rw-rw-r--  2.0 unx     9407 b- defN 23-May-25 20:03 datarobotx/viz/modelcard.py
--rw-rw-r--  2.0 unx     9508 b- defN 23-May-25 20:03 datarobotx/viz/viz.py
--rw-rw-r--  2.0 unx      926 b- defN 23-May-25 20:03 datarobotx/viz/templates/drx_button.html
--rw-rw-r--  2.0 unx     1490 b- defN 23-May-25 20:03 datarobotx/viz/templates/leaderboard.html
--rw-rw-r--  2.0 unx      414 b- defN 23-May-25 20:03 datarobotx/viz/templates/leaderboard.md
--rw-rw-r--  2.0 unx     5433 b- defN 23-May-25 20:03 datarobotx/viz/templates/model_card.html
--rw-rw-r--  2.0 unx       48 b- defN 23-May-25 20:03 datarobotx/viz/templates/model_card.md
--rw-rw-r--  2.0 unx     3140 b- defN 23-May-25 20:03 datarobotx/viz/templates/robot.svg
--rw-rw-r--  2.0 unx     3135 b- defN 23-May-25 20:03 datarobotx/viz/templates/robot_large.svg
--rw-rw-r--  2.0 unx     5731 b- defN 23-May-25 20:09 datarobotx-0.1.8.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-25 20:09 datarobotx-0.1.8.dist-info/WHEEL
--rw-rw-r--  2.0 unx       11 b- defN 23-May-25 20:09 datarobotx-0.1.8.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     4837 b- defN 23-May-25 20:09 datarobotx-0.1.8.dist-info/RECORD
-56 files, 549728 bytes uncompressed, 135413 bytes compressed:  75.4%
+Zip file size: 154770 bytes, number of entries: 61
+-rw-rw-r--  2.0 unx     1573 b- defN 23-Jun-14 22:32 datarobotx/__init__.py
+-rw-rw-r--  2.0 unx      271 b- defN 23-Jun-14 22:32 datarobotx/_version.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-14 22:32 datarobotx/py.typed
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-14 22:32 datarobotx/client/__init__.py
+-rw-rw-r--  2.0 unx     9435 b- defN 23-Jun-14 22:32 datarobotx/client/datasets.py
+-rw-rw-r--  2.0 unx    16785 b- defN 23-Jun-14 22:32 datarobotx/client/deployments.py
+-rw-rw-r--  2.0 unx     1002 b- defN 23-Jun-14 22:32 datarobotx/client/prediction_servers.py
+-rw-rw-r--  2.0 unx    25796 b- defN 23-Jun-14 22:32 datarobotx/client/projects.py
+-rw-rw-r--  2.0 unx     2393 b- defN 23-Jun-14 22:32 datarobotx/client/status.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-14 22:32 datarobotx/common/__init__.py
+-rw-rw-r--  2.0 unx     7772 b- defN 23-Jun-14 22:32 datarobotx/common/client.py
+-rw-rw-r--  2.0 unx     9292 b- defN 23-Jun-14 22:32 datarobotx/common/config.py
+-rw-rw-r--  2.0 unx     4600 b- defN 23-Jun-14 22:32 datarobotx/common/configurator.py
+-rw-rw-r--  2.0 unx   131191 b- defN 23-Jun-14 22:32 datarobotx/common/dr_config.py
+-rw-rw-r--  2.0 unx    10296 b- defN 23-Jun-14 22:32 datarobotx/common/logging.py
+-rw-rw-r--  2.0 unx     3980 b- defN 23-Jun-14 22:32 datarobotx/common/transformations.py
+-rw-rw-r--  2.0 unx    15032 b- defN 23-Jun-14 22:32 datarobotx/common/ts_helpers.py
+-rw-rw-r--  2.0 unx      934 b- defN 23-Jun-14 22:32 datarobotx/common/types.py
+-rw-rw-r--  2.0 unx    27452 b- defN 23-Jun-14 22:32 datarobotx/common/utils.py
+-rw-rw-r--  2.0 unx      279 b- defN 23-Jun-14 22:32 datarobotx/llm/__init__.py
+-rw-rw-r--  2.0 unx     3857 b- defN 23-Jun-14 22:32 datarobotx/llm/utils.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-14 22:32 datarobotx/llm/chains/__init__.py
+-rw-rw-r--  2.0 unx    12693 b- defN 23-Jun-14 22:32 datarobotx/llm/chains/data_dict.py
+-rw-rw-r--  2.0 unx    12398 b- defN 23-Jun-14 22:32 datarobotx/llm/chains/enrich.py
+-rw-rw-r--  2.0 unx      284 b- defN 23-Jun-14 22:32 datarobotx/mlflow/__init__.py
+-rw-rw-r--  2.0 unx    26469 b- defN 23-Jun-14 22:32 datarobotx/mlflow/drx_mlflow.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-14 22:32 datarobotx/models/__init__.py
+-rw-rw-r--  2.0 unx     2754 b- defN 23-Jun-14 22:32 datarobotx/models/autoanomaly.py
+-rw-rw-r--  2.0 unx     3332 b- defN 23-Jun-14 22:32 datarobotx/models/autocluster.py
+-rw-rw-r--  2.0 unx     2465 b- defN 23-Jun-14 22:32 datarobotx/models/automl.py
+-rw-rw-r--  2.0 unx    11502 b- defN 23-Jun-14 22:32 datarobotx/models/autopilot.py
+-rw-rw-r--  2.0 unx    11210 b- defN 23-Jun-14 22:32 datarobotx/models/autots.py
+-rw-rw-r--  2.0 unx    15037 b- defN 23-Jun-14 22:32 datarobotx/models/colreduce.py
+-rw-rw-r--  2.0 unx    24902 b- defN 23-Jun-14 22:32 datarobotx/models/deploy.py
+-rw-rw-r--  2.0 unx    33947 b- defN 23-Jun-14 22:32 datarobotx/models/deployment.py
+-rw-rw-r--  2.0 unx     7914 b- defN 23-Jun-14 22:32 datarobotx/models/evaluation.py
+-rw-rw-r--  2.0 unx    16937 b- defN 23-Jun-14 22:32 datarobotx/models/featurediscovery.py
+-rw-rw-r--  2.0 unx     5593 b- defN 23-Jun-14 22:32 datarobotx/models/intraproject.py
+-rw-rw-r--  2.0 unx    28701 b- defN 23-Jun-14 22:32 datarobotx/models/model.py
+-rw-rw-r--  2.0 unx     9722 b- defN 23-Jun-14 22:32 datarobotx/models/selfdiscovery.py
+-rw-rw-r--  2.0 unx     4422 b- defN 23-Jun-14 22:32 datarobotx/models/share.py
+-rw-rw-r--  2.0 unx    21316 b- defN 23-Jun-14 22:32 datarobotx/models/sparkingest.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-14 22:32 datarobotx/openblueprints/__init__.py
+-rw-rw-r--  2.0 unx     6473 b- defN 23-Jun-14 22:32 datarobotx/openblueprints/blueprint_string_converter.py
+-rw-rw-r--  2.0 unx     2126 b- defN 23-Jun-14 22:32 datarobotx/openblueprints/mapping_object.py
+-rw-rw-r--  2.0 unx        0 b- defN 23-Jun-14 22:32 datarobotx/viz/__init__.py
+-rw-rw-r--  2.0 unx     4906 b- defN 23-Jun-14 22:32 datarobotx/viz/charts.py
+-rw-rw-r--  2.0 unx     6633 b- defN 23-Jun-14 22:32 datarobotx/viz/leaderboard.py
+-rw-rw-r--  2.0 unx     9461 b- defN 23-Jun-14 22:32 datarobotx/viz/modelcard.py
+-rw-rw-r--  2.0 unx     9447 b- defN 23-Jun-14 22:32 datarobotx/viz/viz.py
+-rw-rw-r--  2.0 unx      926 b- defN 23-Jun-14 22:32 datarobotx/viz/templates/drx_button.html
+-rw-rw-r--  2.0 unx     1490 b- defN 23-Jun-14 22:32 datarobotx/viz/templates/leaderboard.html
+-rw-rw-r--  2.0 unx      414 b- defN 23-Jun-14 22:32 datarobotx/viz/templates/leaderboard.md
+-rw-rw-r--  2.0 unx     5433 b- defN 23-Jun-14 22:32 datarobotx/viz/templates/model_card.html
+-rw-rw-r--  2.0 unx       48 b- defN 23-Jun-14 22:32 datarobotx/viz/templates/model_card.md
+-rw-rw-r--  2.0 unx     3140 b- defN 23-Jun-14 22:32 datarobotx/viz/templates/robot.svg
+-rw-rw-r--  2.0 unx     3135 b- defN 23-Jun-14 22:32 datarobotx/viz/templates/robot_large.svg
+-rw-rw-r--  2.0 unx     5743 b- defN 23-Jun-14 22:33 datarobotx-0.1.9.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Jun-14 22:33 datarobotx-0.1.9.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       11 b- defN 23-Jun-14 22:33 datarobotx-0.1.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     5314 b- defN 23-Jun-14 22:33 datarobotx-0.1.9.dist-info/RECORD
+61 files, 588330 bytes uncompressed, 146300 bytes compressed:  75.1%
```

## zipnote {}

```diff
@@ -66,14 +66,20 @@
 
 Filename: datarobotx/llm/chains/data_dict.py
 Comment: 
 
 Filename: datarobotx/llm/chains/enrich.py
 Comment: 
 
+Filename: datarobotx/mlflow/__init__.py
+Comment: 
+
+Filename: datarobotx/mlflow/drx_mlflow.py
+Comment: 
+
 Filename: datarobotx/models/__init__.py
 Comment: 
 
 Filename: datarobotx/models/autoanomaly.py
 Comment: 
 
 Filename: datarobotx/models/autocluster.py
@@ -114,14 +120,23 @@
 
 Filename: datarobotx/models/share.py
 Comment: 
 
 Filename: datarobotx/models/sparkingest.py
 Comment: 
 
+Filename: datarobotx/openblueprints/__init__.py
+Comment: 
+
+Filename: datarobotx/openblueprints/blueprint_string_converter.py
+Comment: 
+
+Filename: datarobotx/openblueprints/mapping_object.py
+Comment: 
+
 Filename: datarobotx/viz/__init__.py
 Comment: 
 
 Filename: datarobotx/viz/charts.py
 Comment: 
 
 Filename: datarobotx/viz/leaderboard.py
@@ -150,20 +165,20 @@
 
 Filename: datarobotx/viz/templates/robot.svg
 Comment: 
 
 Filename: datarobotx/viz/templates/robot_large.svg
 Comment: 
 
-Filename: datarobotx-0.1.8.dist-info/METADATA
+Filename: datarobotx-0.1.9.dist-info/METADATA
 Comment: 
 
-Filename: datarobotx-0.1.8.dist-info/WHEEL
+Filename: datarobotx-0.1.9.dist-info/WHEEL
 Comment: 
 
-Filename: datarobotx-0.1.8.dist-info/top_level.txt
+Filename: datarobotx-0.1.9.dist-info/top_level.txt
 Comment: 
 
-Filename: datarobotx-0.1.8.dist-info/RECORD
+Filename: datarobotx-0.1.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## datarobotx/__init__.py

```diff
@@ -1,9 +1,10 @@
 # flake8: noqa
 
+from ._version import __version__
 from .common.config import Context
 from .common.configurator import Configurator
 from .common.dr_config import (
     DataConfig,
     DRConfig,
     FeaturesAutoTSConfig,
     FeaturesConfig,
@@ -35,19 +36,11 @@
 from .models.deploy import deploy
 from .models.deployment import Deployment
 from .models.evaluation import evaluate, import_parametric_model
 from .models.featurediscovery import FeatureDiscoveryModel, Relationship
 from .models.model import Model
 from .models.selfdiscovery import SelfDiscoveryModel
 from .models.share import share
+from .models.sparkingest import downsample_spark, spark_to_ai_catalog, SparkIngestModel
 
-try:
-    from datarobotx import llm
-except ImportError:
-    pass
-
-try:
-    from .models.sparkingest import downsample_spark, spark_to_ai_catalog, SparkIngestModel
-except ImportError:
-    pass
-
-from ._version import __version__ as VERSION
+# __version__ is expected by downstream applications that need consume DRX as dependency like MLFlow
+VERSION = __version__
```

## datarobotx/_version.py

```diff
@@ -6,8 +6,8 @@
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
 #
-__version__ = "0.1.8"
+__version__ = "0.1.9"
```

## datarobotx/client/datasets.py

```diff
@@ -55,15 +55,15 @@
     url = "/datasets/fromFile/"
     async with session.post(url, data=form_data, timeout=None) as resp:
         if resp.status != 202:
             await raise_value_error(resp)
         if catalog_name is not None:
             resp_json = await resp.json()
             await patch_dataset(resp_json["catalogId"], name=catalog_name)
-        return resp.headers["Location"]
+        return cast(str, resp.headers["Location"])
 
 
 async def await_dataset(status_url: str, name: Optional[str] = None) -> Dict[str, Any]:
     """Poll dataset creation status"""
     coro_args = [status_url]
     if name is not None:
         logger.info("Awaiting dataset '%s' registration...", name)
@@ -116,15 +116,15 @@
 async def patch_datasets_shared_roles(did: str, json: Dict[str, Any]) -> int:
     """Share dataset with other roles"""
     url = f"/datasets/{did}/sharedRoles/"
 
     async with session.patch(url, json=json) as resp:
         if resp.status != 204:
             await raise_value_error(resp)
-    return resp.status
+    return cast(int, resp.status)
 
 
 async def resolve_dataset_id(string: str) -> str:
     """Retrieve dataset id corresponding to user-provided string
 
     String can be a dataset id itself or the name of the AI catalog
     dataset. If name is ambiguous, ValueError will be raised
@@ -206,15 +206,15 @@
     }
     async with session.post(url, json=body, timeout=None) as resp:
         if resp.status != 202:
             await raise_value_error(resp)
         if catalog_name is not None:
             resp_json = await resp.json()
             await patch_dataset(resp_json["catalogId"], name=catalog_name)
-        return resp.headers["Location"]
+        return cast(str, resp.headers["Location"])
 
 
 async def post_dataset_from_spark_df(
     spark_df: "pyspark.DataFrame",  # type: ignore[name-defined] # noqa: F821
     name: str,
     max_rows: int,
 ) -> Dict[str, Any]:
```

## datarobotx/client/deployments.py

```diff
@@ -5,14 +5,16 @@
 #
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
+from __future__ import annotations
+
 import io
 import json
 import logging
 import re
 from typing import Any, cast, Dict, List, Optional, Tuple
 
 import aiohttp
@@ -367,15 +369,15 @@
 
 async def patch_shared_roles(did: str, json: Dict[str, Any]) -> int:
     """Share deployment with other users"""
     url = f"/deployments/{did}/sharedRoles/"
     async with session.patch(url, json=json) as resp:
         if resp.status != 204:
             await raise_value_error(resp)
-    return resp.status
+    return cast(int, resp.status)
 
 
 async def await_batch_job_completion(status_url: str) -> pd.DataFrame:
     """Poll whether batch job is done then get the data."""
     coro_args = [status_url]
     coro_kwargs: Dict[str, Any] = {}
     pid = await poll(await_status, coro_args=coro_args, coro_kwargs=coro_kwargs)
@@ -426,7 +428,22 @@
             training_data_status_url = resp.headers["Location"]
 
     logger.info("Awaiting training dataset creation...")
     dataset_id = await poll(await_status, coro_args=[training_data_status_url])
     await poll(return_if_finished, coro_args=[dataset_id])
     dataset = await get_dataset_file(dataset_id)
     return dataset
+
+
+async def search_deployments_for_match(pid: str) -> (Dict[str, Any] | None):
+    """Search for the first deployment affiliated with a project"""
+    url = "/deployments?limit=100"
+    async with session.get(url) as resp:
+        while True:
+            deployments = await resp.json()
+            for deployment in deployments["data"]:
+                if deployment["model"]["projectId"] == pid:
+                    return cast(Dict[str, Any], deployment)
+            if deployments["next"] is not None:
+                resp = await session.get(deployments["next"])
+            else:
+                return None
```

## datarobotx/client/projects.py

```diff
@@ -5,14 +5,16 @@
 #
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
+from __future__ import annotations
+
 from collections.abc import Callable
 import io
 import json
 import logging
 from typing import Any, cast, Dict, List, Optional, Tuple
 from urllib.parse import quote, urlencode
 
@@ -42,21 +44,21 @@
         sender = FilesSender(payload)
         form_data = aiohttp.FormData()
         form_data.add_field("file", sender.reader(file_name), filename=file_name)
         for key, value in json.items():  # type: ignore[union-attr]
             form_data.add_field(key, value)
         kwargs["data"] = form_data
     else:
-        kwargs["json"] = json  # type: ignore[assignment]
+        kwargs["json"] = json
 
     url = "/projects/"
     async with session.post(url, timeout=None, **kwargs) as resp:
         if resp.status != 202:
             await raise_value_error(resp)
-        return resp.headers["Location"]
+        return cast(str, resp.headers["Location"])
 
 
 async def await_proj(status_url: str) -> str:
     """Poll project creation status and update progress.
     Returns project id upon successful creation.
     """
     coro_args = [status_url]
@@ -191,15 +193,15 @@
 async def post_models(pid: str, bp_id: str) -> str:
     """Train a new model from a blueprint"""
     url = f"/projects/{pid}/models/"
     json = {"blueprintId": bp_id}
     async with session.post(url, json=json, timeout=None) as resp:
         if resp.status != 202:
             await raise_value_error(resp)
-        return resp.headers["Location"]
+        return cast(str, resp.headers["Location"])
 
 
 async def await_model(status_url: str) -> str:
     """Wait for model to finish training"""
     coro_args = [status_url]
 
     model_id = await poll(await_status, coro_args=coro_args)
@@ -396,15 +398,15 @@
     async def _post_feature_impact() -> Optional[str]:  # type: ignore[return]
         async with session.post(url) as resp:
             if resp.status in (422, 200):
                 return None  # Feature impact already ran
             elif resp.status == 202:
                 if log_calc_with is not None:
                     log_calc_with()
-                status_url = resp.headers["Location"]
+                status_url: str = resp.headers["Location"]
                 return status_url
             else:
                 await raise_value_error(resp)
 
     async def _get_feature_impact() -> Dict[str, Any]:
         async with session.get(url) as resp:
             json = await resp.json()
@@ -570,33 +572,33 @@
 
 async def patch_access_control(pid: str, json: Dict[str, Any]) -> int:
     """Share project with other users"""
     url = f"/projects/{pid}/accessControl/"
     async with session.patch(url, json=json, allow_redirects=False) as resp:
         if resp.status != 204:
             await raise_value_error(resp)
-    return resp.status
+    return cast(int, resp.status)
 
 
 async def get_feature_discovery_recipe_sqls(pid: str) -> str:
     """Get SQL to generate features from feature discovery"""
     url = f"/projects/{pid}/featureDiscoveryRecipeSQLs/download/"
     async with session.get(url) as resp:
         if resp.status != 200:
             await raise_value_error(resp)
-        return await resp.text()
+        return cast(str, await resp.text())
 
 
 async def post_feature_discovery_recipe_sql_exports(pid: str) -> str:
     """Generate sql export file for feature discovery"""
     url = f"/projects/{pid}/featureDiscoveryRecipeSqlExports/"
     async with session.post(url, allow_redirects=False) as resp:
         if resp.status != 202:
             await raise_value_error(resp)
-        return resp.headers["Location"]
+        return cast(str, resp.headers["Location"])
 
 
 async def await_sql_recipes(pid: str) -> str:
     """Request and await sql recipes export."""
     status_url = await post_feature_discovery_recipe_sql_exports(pid)
     coro_args = [status_url]
     await poll(await_status, coro_args=coro_args)
@@ -621,15 +623,15 @@
 
 async def get_datetime_partitioning(pid: str) -> Dict[str, Any]:
     """Get time unit and time step of each series"""
     url = f"/projects/{pid}/datetimePartitioning"
     async with session.get(url) as resp:
         if resp.status != 200:
             await raise_value_error(resp)
-        return await resp.json()
+        return cast(Dict[str, Any], await resp.json())
 
 
 async def get_blueprints(pid: str) -> Dict[str, Any]:
     """Get a list of blueprints from a modeling project"""
     url = f"/projects/{pid}/blueprints/"
     async with session.get(url) as resp:
         if resp.status != 200:
@@ -650,15 +652,15 @@
 
 
 async def post_advanced_tuning_parameters(pid: str, mid: str, payload: Dict[str, Any]) -> str:
     """Post advanced tuning options"""
     url = f"/projects/{pid}/models/{mid}/advancedTuning/"
     async with session.post(url, json=payload) as resp:
         if resp.status == 202:
-            return resp.headers["Location"]
+            return cast(str, resp.headers["Location"])
         else:
             message = await resp.text()
             if (
                 "This job duplicates a job or jobs that are in the queue or have completed"
                 in message
             ):
                 return cast(str, json.loads(message)["previousJob"]["url"])
```

## datarobotx/client/status.py

```diff
@@ -5,15 +5,14 @@
 #
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
-# pylint: disable=cyclic-import
 import asyncio
 import time
 from typing import Any, cast, Iterable, Mapping, Optional
 
 from datarobotx.common.client import (
     raise_status_error,
     raise_timeout_error,
```

## datarobotx/common/client.py

```diff
@@ -187,17 +187,15 @@
     )
     raise ValueError(message)
 
 
 session = Session()
 
 
-async def read_resp_data(
-    resp: aiohttp.ClientResponse, f: io.BytesIO, pbar: tqdm.tqdm  # type: ignore[type-arg]
-) -> None:
+async def read_resp_data(resp: aiohttp.ClientResponse, f: io.BytesIO, pbar: tqdm.tqdm) -> None:
     """Read response data with progress updating
 
     Designed to be easily mocked due to:
     https://github.com/kevin1024/vcrpy/issues/502
     """
     async for data in resp.content.iter_chunked(1024):
         f.write(data)
@@ -219,16 +217,16 @@
         reraise=True,
         stop=tenacity.stop_after_attempt(10),
         wait=tenacity.wait_exponential(multiplier=1, min=1, max=10),
         retry=tenacity.retry_if_exception_type(DrxTooManyAttempts),
     )
     async def retry_wrapper() -> Awaitable[Any]:
         try:
-            return await coro(*args, **kwargs)
+            return cast(Awaitable[Any], await coro(*args, **kwargs))
         except ValueError as e:
             try:
                 assert getattr(e, "status", None) == 429
                 raise DrxTooManyAttempts("Received HTTP 429")
             except AssertionError:
                 raise e
 
-    return await retry_wrapper()
+    return cast(Awaitable[Any], await retry_wrapper())
```

## datarobotx/common/config.py

```diff
@@ -36,16 +36,16 @@
 
 
 def is_notebook_env() -> bool:
     """Detect if this is an interactive, ipython-based notebook"""
     try:
         from IPython import get_ipython, InteractiveShell  # pylint: disable=import-outside-toplevel
 
-        ip: Optional[InteractiveShell] = get_ipython()  # type: ignore[no-untyped-call]
-        if ip is not None and ip.has_trait("kernel"):  # type: ignore[no-untyped-call]
+        ip: Optional[InteractiveShell] = get_ipython()
+        if ip is not None and ip.has_trait("kernel"):
             return True
     except ImportError:
         pass
     return False
 
 
 _force_task_blocking = contextvars.ContextVar("force_task_blocking", default=not is_notebook_env())
```

## datarobotx/common/configurator.py

```diff
@@ -14,15 +14,15 @@
 import datetime
 from typing import Optional, Tuple, Union
 
 import pandas as pd
 
 from datarobotx.common import utils
 from datarobotx.common.dr_config import DRConfig
-from datarobotx.models.autopilot import AutopilotModel
+from datarobotx.common.types import AutopilotModelType
 
 
 class Configurator:
     """Synthesize and apply common DR configuration patterns
 
     As with drx.DRConfig, parameters that are omitted or None will be assigned
     default values automatically by DataRobot as required.
@@ -32,23 +32,23 @@
 
     Parameters
     ----------
     base : AutopilotModel or DRConfig
         Base model or configuration object to apply configuration to.
     """
 
-    def __init__(self, base: Union[AutopilotModel, DRConfig]):
+    def __init__(self, base: Union[AutopilotModelType, DRConfig]):
         self.base = base
 
     def _apply(self, **kwargs) -> "Configurator":  # type: ignore[no-untyped-def]
         """Apply parameters"""
         for key, value in kwargs.items():
             if value is None:
                 kwargs[key] = utils.DrxNull()
-        if isinstance(self.base, AutopilotModel):
+        if isinstance(self.base, AutopilotModelType):
             return Configurator(self.base.set_params(**kwargs))
         else:
             self.base._update(DRConfig._from_dict(kwargs))
             return Configurator(self.base)
 
     def otv(
         self,
@@ -109,15 +109,15 @@
                     holdout_start_date=holdout_start.isoformat(),
                     holdout_duration=dr_duration_str(holdout_duration),
                     disable_holdout=False,
                 )
         return self._apply(**dr_params)
 
     def __repr__(self) -> str:
-        if isinstance(self.base, AutopilotModel):
+        if isinstance(self.base, AutopilotModelType):
             return self.base.get_params().__repr__()
         else:  # DRConfig
             return self.base.__repr__()
 
 
 def dr_duration_str(td: pd.Timedelta) -> str:
     """Translate timedelta to DataRobot duration string"""
```

## datarobotx/common/logging.py

```diff
@@ -17,15 +17,15 @@
 import os
 import re
 import textwrap
 from typing import cast, Optional, Tuple, Union
 
 from termcolor import colored
 import tqdm as tqdm_typing
-from tqdm.notebook import tqdm_notebook, TqdmHBox  # type: ignore[attr-defined]
+from tqdm.notebook import tqdm_notebook, TqdmHBox
 from tqdm.std import tqdm as std_tqdm
 
 from datarobotx.common.config import is_notebook_env
 
 logger = logging.getLogger("drx")
 logger.setLevel(logging.INFO)
 # Context variable for the jupyter widget logging messages  + progress bars should be directed toward
@@ -55,40 +55,40 @@
     except LookupError:
         if is_widgets_nb_env():
             from IPython import display  # pylint: disable=import-outside-toplevel
             import ipywidgets as widgets  # pylint: disable=import-outside-toplevel
 
             box = widgets.VBox()
             # lock widget to the calling cell
-            display.display(box)  # type: ignore[no-untyped-call]
+            display.display(box)
             return box
         else:
             return None
 
 
-class drx_tqdm(std_tqdm):  # type: ignore[type-arg]
+class drx_tqdm(std_tqdm):
     """Indent terminal progress bars before outputting"""
 
     def __init__(self, *args, **kwargs) -> None:  # type: ignore[no-untyped-def]
         if "bar_format" in kwargs:
             kwargs["bar_format"] = "    " + kwargs["bar_format"]
         else:
             kwargs["bar_format"] = "    {l_bar}{bar}{r_bar}"
         if "ncols" in kwargs:
             kwargs["ncols"] = min(kwargs["ncols"], 80)
         else:
             kwargs["ncols"] = 80
         super().__init__(*args, **kwargs)
 
 
-class drx_tqdm_notebook(tqdm_notebook):  # type: ignore[type-arg]
+class drx_tqdm_notebook(tqdm_notebook):
     """Subclass that renders tqdm_notebook in the context of a drx ipywidget"""
 
     def __init__(self, *args, display: bool = False, **kwargs) -> None:  # type: ignore[no-untyped-def]
-        super().__init__(*args, display=display, **kwargs)  # type: ignore[call-overload]
+        super().__init__(*args, display=display, **kwargs)
         self.displayed = True
         self._last_total = self.total
 
     @staticmethod
     def status_printer(*args, **kwargs) -> TqdmHBox:  # type: ignore[no-untyped-def]
         """Render tqdm bar within existing widget container in target cell"""
         box_widget = get_widget_for_output()
@@ -98,15 +98,15 @@
             widgets.HTML("<pre>&nbsp;&nbsp;&nbsp;&nbsp;</pre>", layout=widgets.Layout(margin="0")),
             container,
         )
         if box_widget is not None:
             box_widget.children = box_widget.children + (outer_container,)
         return container
 
-    def refresh(  # type: ignore[override]
+    def refresh(
         self,
         nolock: bool = False,
         lock_args: Optional[
             Union[Tuple[Optional[bool], Optional[float]], Tuple[Optional[bool]]]
         ] = None,
     ) -> Optional[bool]:
         """Ensure changing bar totals render properly"""
@@ -254,15 +254,15 @@
                 widgets.HTML(self.format(record), layout=widgets.Layout(margin="0")),
             )
 
 except ImportError:
     pass
 
 
-async def refresh_bar(tqdm_bar: tqdm_typing.tqdm, delay: float = 0.5) -> None:  # type: ignore[type-arg]
+async def refresh_bar(tqdm_bar: tqdm_typing.tqdm, delay: float = 0.5) -> None:
     """Refreshes a bar until it is closed."""
     while not tqdm_bar.disable:
         tqdm_bar.refresh()
         await asyncio.sleep(delay)
 
 
 def setup_default_log_handler(force_terminal_output: bool = False) -> logging.Handler:
```

## datarobotx/common/types.py

```diff
@@ -5,20 +5,32 @@
 #
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
-from typing import Optional
+from __future__ import annotations
+
+from typing import Any, Dict, Optional, Union
 
 from typing_extensions import TypedDict
 
 
 class TimeSeriesPredictParams(TypedDict):
     """Typed dict for time series predict parameters"""
 
     forecastPoint: Optional[str]
     predictionsStartDate: Optional[str]
     predictionsEndDate: Optional[str]
     type: Optional[str]
     relaxKnownInAdvanceFeaturesCheck: Optional[bool]
+
+
+class AutopilotModelType:
+    """Type for type checking if an AutopilotModel"""
+
+    def set_params(self, **kwargs: Any) -> AutopilotModelType:
+        raise NotImplementedError()
+
+    def get_params(self) -> Union[Dict[Any, Any], Any]:
+        raise NotImplementedError()
```

## datarobotx/common/utils.py

```diff
@@ -673,15 +673,15 @@
     """
     name = names_generator.generate_name(style=style)
     if max_len is not None and max_len > 0:
         name = name[:max_len]
     return cast(str, name)
 
 
-class FutureDataFrame(pd.DataFrame):  # type: ignore[misc]
+class FutureDataFrame(pd.DataFrame):
     """Lazily evaluatable subclass of pandas DataFrame
 
     Blocks caller attempting to access any pandas attributes until future
     is completed and DataFrame can be materialized
 
     Parameters
     ----------
```

## datarobotx/llm/__init__.py

```diff
@@ -1,7 +1,9 @@
 # flake8: noqa
 
 try:
     from .chains.data_dict import DataDictChain
     from .chains.enrich import embed, enrich
-except ImportError:
-    pass
+except ImportError as e:
+    raise ImportError(
+        "datarobotx.llm requires additional dependencies; consider using `pip install datarobotx[llm]`"
+    ) from e
```

## datarobotx/llm/utils.py

```diff
@@ -7,15 +7,15 @@
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
 from functools import wraps
 from math import floor, log10
-from typing import Any, Callable, Dict, List, Optional
+from typing import Any, Callable, cast, Dict, List, Optional, TypeVar
 
 import langchain.cache
 
 
 def round_sig_figs(value: Any, coerce: bool = False, n: int = 2) -> str:
     """LLM prompt helper function for rounding pd.Series to n sig figs
     Parameters
@@ -44,38 +44,41 @@
         if rounded >= 10:
             rounded = int(rounded)
         as_str = repr(rounded)
         if as_str.endswith(".0"):
             as_str = as_str[:-2]
         return as_str
     except TypeError:
-        return value
+        return value  # type: ignore[no-any-return]
+
+
+F = TypeVar("F", bound=Callable[..., Any])
 
 
 def with_llm_cache(
     cache: Optional[langchain.cache.BaseCache],
-) -> Callable[[Callable[..., Any]], Any]:
+) -> Callable[[Callable[..., Any]], F]:
     """Decorator for applying a custom llm cache for the duration of the function"""
 
-    def outer_wrapper(f: Callable[..., Any]) -> Callable[..., Any]:
+    def outer_wrapper(f: F) -> F:
         @wraps(f)
         def inner_wrapper(*args, **kwargs) -> Any:  # type: ignore[no-untyped-def]
             llm_cache = langchain.llm_cache
             langchain.llm_cache = cache
             try:
                 return f(*args, **kwargs)
             finally:
                 langchain.llm_cache = llm_cache
 
         if cache is not None:
-            return inner_wrapper
+            return cast(F, inner_wrapper)
         else:
             return f
 
-    return outer_wrapper
+    return cast(Callable[..., F], outer_wrapper)
 
 
 # Stop gap until this guy is merged: https://github.com/hwchase17/langchain/pull/1930/
 class InMemoryEmbeddingsCache:
     """Cache that stores things in memory."""
 
     def __init__(self) -> None:
```

## datarobotx/llm/chains/data_dict.py

```diff
@@ -9,15 +9,15 @@
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
 from __future__ import annotations
 
 import asyncio
 import json
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, cast, Dict, List, Optional, Tuple, Union
 
 from langchain.chains.base import Chain
 from langchain.chains.llm import LLMChain
 from langchain.llms.base import BaseLLM
 from langchain.memory import ConversationTokenBufferMemory
 from langchain.prompts.base import BasePromptTemplate
 from langchain.prompts.prompt import PromptTemplate
@@ -90,15 +90,15 @@
         return values
 
     @property
     def input_keys(self) -> List[str]:
         return ["context", "feature"]
 
     def prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
-        inputs = super().prep_inputs(inputs)
+        inputs = cast(Dict[str, Any], super().prep_inputs(inputs))
         if "stop" not in inputs:
             inputs["stop"] = self.default_stop
         return inputs
 
 
 class DataDictChain(Chain):
     """Generate a data dictionary using an LLM
```

## datarobotx/llm/chains/enrich.py

```diff
@@ -7,15 +7,15 @@
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
 from __future__ import annotations
 
-from typing import Any, Callable, Dict, List, Optional, Union
+from typing import Any, Callable, cast, Dict, List, Optional, Union
 import weakref
 
 from langchain import embeddings
 from langchain.cache import InMemoryCache
 from langchain.chains.base import Chain
 from langchain.chains.llm import LLMChain
 from langchain.embeddings.base import Embeddings
@@ -97,15 +97,15 @@
     default_stop: List[str] = ["\n"]
 
     @property
     def input_keys(self) -> List[str]:
         return ["question"]
 
     def prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
-        inputs = super().prep_inputs(inputs)
+        inputs = cast(Dict[str, Any], super().prep_inputs(inputs))
         if "stop" not in inputs:
             inputs["stop"] = self.default_stop
         return inputs
 
     def _validate_outputs(self, outputs: Dict[str, str]) -> None:
         super()._validate_outputs(outputs)
         if outputs[self.output_key] not in ["numeric", "categorical", "date", "free-text"]:
@@ -160,15 +160,15 @@
         return values
 
     @property
     def input_keys(self) -> List[str]:
         return ["question", "output_type"]
 
     def prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, Any]:
-        inputs = super().prep_inputs(inputs)
+        inputs = cast(Dict[str, Any], super().prep_inputs(inputs))
         if "stop" not in inputs:
             inputs["stop"] = self.default_stop
         if inputs["output_type"] == "categorical":
             # emphasize scalar nature for llm to avoid completing all cat levels
             inputs["output_type"] = "single, categorical"
         return inputs
 
@@ -315,15 +315,15 @@
         unit_divisor=1000,
     )
     if using is None:
         using = embeddings.SentenceTransformerEmbeddings(model="all-MiniLM-L6-v2")
     else:
         assert isinstance(using, Embeddings)
 
-    def embedder(s: pd.Series) -> str:
+    def embedder(s: pd.Series) -> pd.Series:
         prompt_template = PromptTemplate.from_template(text)
         formatted_q = prompt_template.format(
             **{
                 key: value
                 for key, value in s.to_dict().items()
                 if key in prompt_template.input_variables
             }
```

## datarobotx/models/autopilot.py

```diff
@@ -22,21 +22,22 @@
 import pandas as pd
 
 import datarobotx.client.datasets as dataset_client
 import datarobotx.client.projects as proj_client
 from datarobotx.common import utils
 from datarobotx.common.config import context
 from datarobotx.common.dr_config import DRConfig
+from datarobotx.common.types import AutopilotModelType
 from datarobotx.models.model import ModelOperator
 
 logger = logging.getLogger("drx")
 
 
 @utils.hidden_instance_classmethods
-class AutopilotModel(ModelOperator):
+class AutopilotModel(ModelOperator, AutopilotModelType):
     """
     Abstract base class for autopilot orchestration
 
     Trains challenger models asynchronously and exposes the present champion
     for predictions or deployment. Training is performed within an
     automatically created DataRobot project.
 
@@ -236,15 +237,19 @@
         await self._start_autopilot()
         logger.info("Fitting models...")
         await proj_client.await_autopilot(
             pid=self._project_id,  # type: ignore[arg-type]
             champion_handler=partial(self._refresh_leaderboard, callback=champion_handler),
         )
         assert self._best_model is not None
-        await self._log_champion(best_model=self._best_model, ascii_art=not bool(champion_handler))
+        await self._log_champion(
+            best_model=self._best_model,
+            ascii_art=not bool(champion_handler),
+            leaderboard=self._leaderboard,
+        )
         logger.info("Autopilot complete", extra={"is_header": True})
 
     async def _create_project(self, X: Union[pd.DataFrame, str]) -> None:
         """Create a DR project from dataframe or AI catalog dataset"""
         json_ = self._dr_config._to_json("post_projects")
         if isinstance(X, pd.DataFrame):
             payload = utils.prepare_df_upload(X)
```

## datarobotx/models/autots.py

```diff
@@ -98,15 +98,15 @@
             params["forecast_window_start"] = start
             params["forecast_window_end"] = end
 
         params["use_time_series"] = True
         params["cv_method"] = "datetime"
         return dict(params)
 
-    def fit(  # type: ignore[no-untyped-def]
+    def fit(
         self,
         X: Union[pd.DataFrame, str],
         datetime_partition_column: str,
         target: Optional[str] = None,
         kia_columns: Optional[List[str]] = None,
         multiseries_id_columns: Optional[str] = None,
         segmentation_id_column: Optional[str] = None,
```

## datarobotx/models/colreduce.py

```diff
@@ -241,15 +241,17 @@
                 champion_handler=champion_handler,
             )
             last_featurelist = next_featurelist
 
             if last_best_model == self._best_model._model_id:
                 lives -= 1
                 ratio *= ratio
-            await self._log_champion(best_model=self._best_model, ascii_art=(lives == 0))
+            await self._log_champion(
+                best_model=self._best_model, ascii_art=(lives == 0), leaderboard=self._leaderboard
+            )
             if lives > 0:
                 impacts = await self._get_feature_impacts()
 
         logger.info("Column reduction complete", extra={"is_header": True})
 
     async def _refresh_leaderboard(self, callback: Optional[Callable] = None) -> None:  # type: ignore[type-arg]
         """
```

## datarobotx/models/deploy.py

```diff
@@ -16,15 +16,15 @@
 import os
 import pathlib
 import pickle
 import sys
 import textwrap
 from typing import Any, Callable, cast, Dict, List, Optional, Tuple, Union
 
-import pkg_resources  # type: ignore[import]
+import pkg_resources
 
 import datarobotx.client.deployments as deploy_client
 from datarobotx.common import utils
 from datarobotx.common.config import context
 from datarobotx.common.utils import PayloadType
 from datarobotx.models.deployment import Deployment
 from datarobotx.models.model import Model
@@ -123,14 +123,21 @@
     ...                       hooks={'read_input_data': force_schema})
 
     """
 
     if isinstance(model, Model):
         return model.deploy()
     else:
+        try:
+            import cloudpickle  # noqa: 401  # pylint: disable=unused-import
+            import sklearn  # noqa: 401  # pylint: disable=unused-import
+        except ImportError as e:
+            raise ImportError(
+                "datarobotx.deploy() requires additional dependencies; consider using `pip install datarobotx[deploy]`"
+            ) from e
         logger.info("Deploying custom model", extra={"is_header": True})
         deployment = Deployment()
         deployer = CustomDeployer(
             deployment,
             model,
             *args,
             target=target,
@@ -268,15 +275,15 @@
         """Return running python interpreter version as a string e.g. '3.7'."""
         version = sys.version_info
         major = version.major
         minor = version.minor
         micro = version.micro
         if major != 3 or minor > 10 or minor < 4:
             # limited by datarobot DRUM
-            raise TypeError("drx.deploy() and datarobot-drum supports python >=3.4, <3.11")
+            raise TypeError("datarobotx.deploy() and datarobot-drum supports python >=3.4, <3.11")
         return f"{major}.{minor}.{micro}"
 
     async def prepare_payloads(self) -> None:
         """
         Build a zip archive for a custom environment to be uploaded
         to DataRobot and also export the model itself.
```

## datarobotx/models/model.py

```diff
@@ -5,15 +5,14 @@
 #
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
-# pylint: disable=cyclic-import
 from __future__ import annotations
 
 import asyncio
 from collections.abc import Awaitable, Callable
 from functools import wraps
 import logging
 import re
@@ -183,15 +182,15 @@
         predict
         """
         future = utils.create_task_new_thread(self._predict(X, class_probabilities=True, **kwargs))
         return utils.FutureDataFrame(future=future)
 
     async def _predict(
         self, X: Union[pd.DataFrame, str], class_probabilities: bool = False, **kwargs: Any
-    ) -> None:
+    ) -> pd.DataFrame:
         """Orchestrate training model batch predictions."""
         await self._validate_predict_args(class_probabilities)
         project_id = str(self._project_id)
         project_data = await proj_client.get_project(project_id)
         uses_ts_helpers = (
             project_data["partition"]["useTimeSeries"]
             and project_data["unsupervisedType"] != "clustering"
@@ -245,15 +244,15 @@
     @staticmethod
     async def _create_pred_dataset(
         pid: str, X: Union[pd.DataFrame, str], relax_kia_check: bool = False
     ) -> Dict[str, Any]:
         """Create a prediction dataset server-side for DR to make predictions"""
         if isinstance(X, pd.DataFrame):
             prediction_ds = await proj_client.post_prediction_datasets_file_uploads(
-                pid=pid,  # type: ignore[arg-type]
+                pid=pid,
                 payload=utils.prepare_df_upload(X),
                 relax_kia_check=relax_kia_check,
             )
         elif isinstance(X, str):
             dataset_id = await dataset_client.resolve_dataset_id(X)
             prediction_ds = await proj_client.post_prediction_datasets_dataset_uploads(
                 pid=pid, dataset_id=dataset_id, relax_kia_check=relax_kia_check
@@ -670,27 +669,33 @@
     def _wait(self) -> ModelOperator:
         """Wait for fitting to finish"""
         while self._fitting_underway:
             time.sleep(context._concurrency_poll_interval)
         return self
 
     @staticmethod
-    async def _log_champion(best_model: Optional[Model] = None, ascii_art: bool = True) -> None:
+    async def _log_champion(
+        best_model: Optional[Model], leaderboard: Optional[List[str]], ascii_art: bool = True
+    ) -> None:
         """Log the champion model"""
         if best_model is None:
             return
         model_json = await proj_client.get_model(best_model._project_id, best_model._model_id)  # type: ignore[arg-type]
         url = (
             context._webui_base_url
             + f"/projects/{best_model._project_id}/models/{model_json['id']}/blueprint"
         )
         champion = f"Champion model: [{model_json['modelType'].rstrip()}]({url})"
-        extra = {}
+        # note: multiple handlers are watching for this message such as MLFlow
+        extra: Dict[str, Union[str, bool, Optional[str], Optional[List[str]]]] = {}
         if ascii_art:
             extra["is_champion_msg"] = True
+        extra["leaderboard"] = leaderboard
+        extra["project_id"] = best_model._project_id
+        extra["project_ready_for_logging"] = True
         logger.info(champion, extra=extra)
 
     def _ipython_display_(self) -> None:
         """
         Render a leaderboard for widget-enabled ipython environments
 
         Called by ipython when last expression in a cell is not an assignment operation
```

## datarobotx/models/selfdiscovery.py

```diff
@@ -5,28 +5,27 @@
 #
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
-# pylint: disable=cyclic-import
 from __future__ import annotations
 
 from functools import partial
 import logging
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import pandas as pd
 
-from datarobotx import Deployment
 import datarobotx.client.datasets as datasets_client
 from datarobotx.common import utils
 from datarobotx.common.utils import FutureDataFrame
 from datarobotx.models.autopilot import AutopilotModel
+from datarobotx.models.deployment import Deployment
 from datarobotx.models.featurediscovery import FeatureDiscoveryModel, Relationship
 from datarobotx.models.intraproject import IntraProjectModel
 from datarobotx.models.model import ModelOperator
 
 logger = logging.getLogger("drx")
```

## datarobotx/models/sparkingest.py

```diff
@@ -5,34 +5,47 @@
 #
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
+from __future__ import annotations
+
 from functools import partial
 from itertools import chain
 import logging
 from math import ceil, floor, sqrt
-from typing import Any, cast, Dict, Optional, Tuple, Union
+from typing import Any, cast, Dict, Optional, Tuple, TYPE_CHECKING, Union
 
 import pandas as pd
-import pyspark.sql
-from pyspark.sql.functions import create_map, lit, when
 
 from datarobotx.client.datasets import post_dataset_from_spark_df
 from datarobotx.common import utils
 from datarobotx.common.config import context
 from datarobotx.models.autopilot import AutopilotModel
 from datarobotx.models.intraproject import IntraProjectModel
 from datarobotx.models.model import ModelOperator
 
+if TYPE_CHECKING:
+    import pyspark.sql
+
 logger = logging.getLogger("drx")
 
 
+def check_spark() -> None:
+    try:
+        import pyspark.sql  # noqa: F401  # pylint: disable=unused-import
+    except ImportError as e:
+        raise ImportError(
+            "datarobotx.SparkIngestModel() requires additional dependencies; "
+            + "consider using `pip install datarobotx[spark]`"
+        ) from e
+
+
 class SparkIngestModel(IntraProjectModel):
     """
     Train on a Spark dataframe
 
     Ingests a Spark dataframe into DataRobot for model training, downsampling if needed.
 
     An AI catalog entry will automatically be created for the ingested data and Autopilot
@@ -68,14 +81,15 @@
 
     def __init__(
         self,
         base_model: Union[AutopilotModel, IntraProjectModel],
         dataset_name: Optional[str] = None,
         sampling_strategy: str = "uniform",
     ):
+        check_spark()
         super().__init__(base_model, dataset_name=dataset_name, sampling_strategy=sampling_strategy)
         if dataset_name is None:
             dataset_name = self._get_params_root()["project_name"]
             self.set_params(dataset_name=dataset_name)
 
     def fit(self, X: pyspark.sql.DataFrame, *args: Any, **kwargs: Any) -> None:
         """
@@ -95,14 +109,16 @@
     @ModelOperator._with_fitting_underway
     async def _fit(
         self,
         X: Union[pyspark.sql.DataFrame, pd.DataFrame, str],
         *args: Any,
         **kwargs: Any,
     ) -> None:
+        import pyspark.sql
+
         if not isinstance(X, pyspark.sql.DataFrame):
             raise TypeError(
                 "SparkIngestModel requires training data to be of type pyspark.sql.DataFrame"
             )
         logger.info("Ingesting Spark data and running autopilot", extra={"is_header": True})
         sampled_df, max_rows = downsample_spark(
             X,
@@ -219,14 +235,16 @@
     time.
 
     In most cases 'most_recent' is the only form of sampling for which this limit
     needs to be enforced at upload time, however 'uniform' may occasionally
     require row limit enforcement because Spark sample() does not provide row count
     guarantees.
     """
+    check_spark()
+
     logger.info("Preparing for DataRobot ingest", extra={"is_header": True})
     logger.info("Estimating row limit for ingest...")
     total_rows = spark_df.count()
     max_rows = estimate_max_sample_rows(
         spark_df, context._max_dr_ingest, with_weights="smart" in sampling_strategy
     )
 
@@ -364,14 +382,15 @@
 
     This means that there's a roughly 99.7% chance that the size of the sample that is taken
     is less than max_file_size.
 
     Reference:
     https://openstax.org/books/introductory-statistics/pages/7-2-the-central-limit-theorem-for-sums
     """
+    from pyspark.sql.functions import lit
 
     # Add an extra two bytes for the new line + line feed (excel csv dialect)
     if with_weights:
         row_sizes = (
             df.fillna("")
             .withColumn("dr_sample_weights", lit(1.1234567890123456))
             .selectExpr("octet_length(concat_ws(',', *)) + 2 as bytes")
@@ -418,14 +437,15 @@
     :param sample_rows: The number of rows in the sample when using the total_rows extent_type
     :param sample_pct: The percentage of the original dataset size when using the percentage extent_type
     :param weights_col_name: The name of an optional weights column in the dataset to use when calculating the final
         weights in the sampled dataset
     :param problem_type: The type of data science problem this sample is being prepared for (classification or
         zero_inflated)
     """
+    from pyspark.sql.functions import create_map, lit, when
 
     if problem_type == "zero_inflated":
         df = df.withColumn(
             "_tmp_class_name",
             when(df[column_name] == 0, 0).when(df[column_name] > 0, 1).otherwise(-1),
         )
         sample_column_name = "_tmp_class_name"
```

## datarobotx/viz/charts.py

```diff
@@ -119,15 +119,15 @@
         .encode(x=alt.X("Predicted:Q"), y=alt.Y("Actual:Q"), tooltip=["Residual"])
     )
     line_data = pd.DataFrame(
         [[0, 0, 0, 0], residuals_data.max().values.tolist()],
         columns=["Actual", "Predicted", "Residual", "Row Number"],
     )
     line_chart = (
-        alt.Chart(line_data, width=400)
+        alt.Chart(line_data, width=500)
         .mark_line(color="grey", strokeDash=[6, 4], strokeOpacity=0.6)
         .encode(x=alt.X("Predicted:Q"), y=alt.Y("Actual:Q"))
     )
     return chart + line_chart
 
 
 def residuals_chart(project_id: str, model_id: str, ds_id: str) -> alt.Chart:
```

## datarobotx/viz/leaderboard.py

```diff
@@ -5,19 +5,22 @@
 #
 # DataRobot, Inc.
 #
 # This is proprietary source code of DataRobot, Inc. and its
 # affiliates.
 #
 # Released under the terms of DataRobot Tool and Utility Agreement.
+
+# Disable cyclic imports because we have a circular import during type checking
+# pylint: disable=cyclic-import
 from __future__ import annotations
 
 import asyncio
 import logging
-from typing import Any, Dict, List, Optional, TYPE_CHECKING, Union
+from typing import Any, cast, Dict, List, Optional, TYPE_CHECKING, Union
 
 import datarobotx.client.projects as proj_client
 from datarobotx.common.config import context
 from datarobotx.viz.viz import jinja_env
 
 if TYPE_CHECKING:
     from datarobotx.models.model import ModelOperator
@@ -86,20 +89,23 @@
     @staticmethod
     def format_terminal(record: logging.LogRecord) -> str:
         """Format logging record for terminal output"""
         if hasattr(record, "proj_json") and hasattr(record, "models_list"):
             jinja_env.filters["metric_parser"] = LeaderboardFormatter._metric_parser
             template = jinja_env.get_template("leaderboard.md")
             if len(record.models_list) > 0:
-                return template.render(
-                    models=record.models_list,
-                    metrics=record.models_list[0]["metrics"].keys(),
-                    key_metric=record.proj_json["metric"],
-                    projectId=record.proj_json["id"],
-                    project_name=record.proj_json["projectName"],
+                return cast(
+                    str,
+                    template.render(
+                        models=record.models_list,
+                        metrics=record.models_list[0]["metrics"].keys(),
+                        key_metric=record.proj_json["metric"],
+                        projectId=record.proj_json["id"],
+                        project_name=record.proj_json["projectName"],
+                    ),
                 )
             else:
                 return "Project does not have a leaderboard yet"
         else:
             return "Project does not have a leaderboard yet"
 
     @staticmethod
@@ -109,23 +115,26 @@
             hasattr(record, "models_list")
             and hasattr(record, "proj_json")
             and len(getattr(record, "models_list", [])) > 0
         ):
             jinja_env.filters["metric_parser"] = LeaderboardFormatter._metric_parser
             template = jinja_env.get_template("leaderboard.html")
             metrics = record.models_list[0]["metrics"].keys()
-            return template.render(
-                models=record.models_list,
-                recommended_model_id=getattr(record, "recommended_model_id", None),
-                metrics=metrics,
-                key_metric=record.proj_json["metric"],
-                projectId=record.proj_json["id"],
-                project_name=record.proj_json["projectName"],
-                webui_base_url=context._webui_base_url,
-                fine_print=getattr(record, "fine_print", None),
+            return cast(
+                str,
+                template.render(
+                    models=record.models_list,
+                    recommended_model_id=getattr(record, "recommended_model_id", None),
+                    metrics=metrics,
+                    key_metric=record.proj_json["metric"],
+                    projectId=record.proj_json["id"],
+                    project_name=record.proj_json["projectName"],
+                    webui_base_url=context._webui_base_url,
+                    fine_print=getattr(record, "fine_print", None),
+                ),
             )
         elif hasattr(record, "fitting_underway") and not record.fitting_underway:
             return "<h3>Project does not have a leaderboard yet</h3>"
         else:
             return ""
 
     @staticmethod
```

## datarobotx/viz/modelcard.py

```diff
@@ -187,16 +187,19 @@
         """Format logging record as html to render the model"""
         if not hasattr(record, "proj_json") or not hasattr(record, "model_json"):
             return ""
         params = self.get_model_card_params(
             getattr(record, "proj_json"), getattr(record, "model_json")
         )
         template = jinja_env.get_template("model_card.md")
-        return template.render(
-            params=params,
+        return cast(
+            str,
+            template.render(
+                params=params,
+            ),
         )
 
     @staticmethod
     def render_charts(chart_jsons: Dict[str, Any]) -> Dict[str, str]:
         """Render charts as chart-name -> altair json pairs"""
         with drx_viz_theme():
             chart_renderers = {
```

## datarobotx/viz/viz.py

```diff
@@ -14,32 +14,28 @@
 
 e.g. implementing __repr__()/__str__()/_ipython_display_()
 """
 import asyncio
 from contextlib import contextmanager
 import logging
 import typing as t
-from typing import TYPE_CHECKING
 
 import altair as alt
 from altair.utils.plugin_registry import PluginEnabler
 from ipywidgets import widgets
 from jinja2 import Environment, PackageLoader, select_autoescape
 
 from datarobotx.common import utils
 from datarobotx.common.config import context
 from datarobotx.common.logging import (
     get_widget_for_output,
     is_widgets_nb_env,
     logging_output_widget,
 )
 
-if TYPE_CHECKING:
-    pass
-
 logger = logging.getLogger("drx")
 
 SEQUENTIAL_BLUE = [
     "#2d8fe2",
     "#559ee6",
     "#73adea",
     "#8ebcee",
```

## Comparing `datarobotx-0.1.8.dist-info/METADATA` & `datarobotx-0.1.9.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: datarobotx
-Version: 0.1.8
+Version: 0.1.9
 Summary: DataRobotX is a collection of DataRobot extensions
 Home-page: https://datarobot.github.io/drx
 Author: DataRobot
 Author-email: datarobotx@datarobot.com
 License: DataRobot Tool and Utility Agreement
 Classifier: Development Status :: 3 - Alpha
 Classifier: Intended Audience :: Developers
@@ -17,35 +17,34 @@
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 Requires-Dist: aiohttp
-Requires-Dist: altair (<5.0.0)
+Requires-Dist: altair
 Requires-Dist: datarobot
 Requires-Dist: IPython
 Requires-Dist: ipywidgets
 Requires-Dist: names-generator
 Requires-Dist: pandas
 Requires-Dist: PyYaml
 Requires-Dist: setuptools
 Requires-Dist: tenacity
 Requires-Dist: termcolor
 Requires-Dist: tqdm
 Requires-Dist: urllib3 (<2.0.0)
 Provides-Extra: deploy
 Requires-Dist: cloudpickle ; extra == 'deploy'
 Requires-Dist: scikit-learn ; extra == 'deploy'
-Requires-Dist: transformers ; extra == 'deploy'
-Requires-Dist: torch ; extra == 'deploy'
 Provides-Extra: dev
 Requires-Dist: flake8 (==5.0.4) ; extra == 'dev'
 Requires-Dist: pylint (==2.15.0) ; extra == 'dev'
 Requires-Dist: black (==22.8.0) ; extra == 'dev'
+Requires-Dist: black[jupyter] ; extra == 'dev'
 Requires-Dist: isort (==5.10.1) ; extra == 'dev'
 Requires-Dist: pytest ; extra == 'dev'
 Requires-Dist: pytest-sphinx ; extra == 'dev'
 Requires-Dist: pytest-asyncio ; extra == 'dev'
 Requires-Dist: vcrpy ; extra == 'dev'
 Requires-Dist: pytest-cov ; extra == 'dev'
 Requires-Dist: twine (>=1.11.0) ; extra == 'dev'
@@ -57,45 +56,47 @@
 Requires-Dist: sphinx-copybutton (==0.5.0) ; extra == 'dev'
 Requires-Dist: sphinx-autobuild (==2021.3.14) ; extra == 'dev'
 Requires-Dist: sphinx-autodoc-typehints ; extra == 'dev'
 Requires-Dist: sphinx-design ; extra == 'dev'
 Requires-Dist: packaging ; extra == 'dev'
 Requires-Dist: aiohttp ; extra == 'dev'
 Requires-Dist: datarobot ; extra == 'dev'
+Requires-Dist: urllib3 (<2.0.0) ; extra == 'dev'
 Requires-Dist: ipython (<8.13.0) ; extra == 'dev'
 Requires-Dist: ipywidgets (==7.7.2) ; extra == 'dev'
 Requires-Dist: names-generator ; extra == 'dev'
 Requires-Dist: pandas ; extra == 'dev'
 Requires-Dist: PyYaml ; extra == 'dev'
 Requires-Dist: tenacity ; extra == 'dev'
 Requires-Dist: termcolor ; extra == 'dev'
 Requires-Dist: tqdm ; extra == 'dev'
 Requires-Dist: altair (<5.0.0) ; extra == 'dev'
 Requires-Dist: cloudpickle ; extra == 'dev'
 Requires-Dist: transformers ; extra == 'dev'
 Requires-Dist: scikit-learn ; extra == 'dev'
 Requires-Dist: torch ; extra == 'dev'
 Requires-Dist: pyspark ; extra == 'dev'
+Requires-Dist: mlflow ; extra == 'dev'
 Requires-Dist: typing-extensions (<4.6.0) ; extra == 'dev'
 Requires-Dist: sentence-transformers ; extra == 'dev'
 Requires-Dist: torch (>=1.6.0) ; extra == 'dev'
 Requires-Dist: openai ; extra == 'dev'
 Requires-Dist: pyarrow (==11.0.0) ; extra == 'dev'
 Requires-Dist: mypy (==1.0.0) ; extra == 'dev'
-Requires-Dist: urllib3 (<2.0.0) ; extra == 'dev'
 Requires-Dist: langchain ; (python_version >= "3.8") and extra == 'dev'
 Requires-Dist: pydantic ; (python_version >= "3.8") and extra == 'dev'
 Requires-Dist: tiktoken (==0.3.3) ; (python_version >= "3.8") and extra == 'dev'
 Provides-Extra: llm
 Requires-Dist: langchain ; extra == 'llm'
-Requires-Dist: pydantic ; extra == 'llm'
 Requires-Dist: tiktoken ; extra == 'llm'
 Requires-Dist: openai ; extra == 'llm'
 Requires-Dist: torch ; extra == 'llm'
 Requires-Dist: sentence-transformers ; extra == 'llm'
+Provides-Extra: mlflow
+Requires-Dist: mlflow ; extra == 'mlflow'
 Provides-Extra: spark
 Requires-Dist: pyspark ; extra == 'spark'
 
 <img align="center" src="https://s3.amazonaws.com/datarobot_public/drx/drx_gifs/logo.png" alt="drx">
 
 ![PyPI](https://img.shields.io/pypi/v/datarobotx)
 ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/datarobotx)
```

## Comparing `datarobotx-0.1.8.dist-info/RECORD` & `datarobotx-0.1.9.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,56 +1,61 @@
-datarobotx/__init__.py,sha256=znA3kfQXkSRBeNke-g_y-KrY4bBPDplDMl2DH_uBUFg,1566
-datarobotx/_version.py,sha256=YbmjoFAJFffTB02BogXW2qVn8XkYm31j1NmWsUFpwWs,271
+datarobotx/__init__.py,sha256=-fD8B4672hSGaAf3E4qDs94IVCRztzJkkOMvCv4sItc,1573
+datarobotx/_version.py,sha256=mFP6D5I7Pn7J8E-prATuokIprNCVyiUvLBFLhl7OUq0,271
 datarobotx/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/client/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-datarobotx/client/datasets.py,sha256=l1SoA0yl2Eqbb3pQNcoa9G_sj9MK_xObxt9pt7wvON8,9402
-datarobotx/client/deployments.py,sha256=KThuu3P5eeeFix8pRItg3NOD_cQlhxGpluBBwsRW2r4,16124
+datarobotx/client/datasets.py,sha256=-ZdgJwxWLnD7MvS-x-FmzyR46hNz9LEPjAkjLfXvzF0,9435
+datarobotx/client/deployments.py,sha256=LNss3i0zztddQp0KCtffnhsfhyAFR7k767-MnW_1Ayc,16785
 datarobotx/client/prediction_servers.py,sha256=BS6Z1fT5ev6ldiIOxKZcuELGcT2k5scZ5j8VH7q3ads,1002
-datarobotx/client/projects.py,sha256=3bVdOMN8tTNrIYcyB7_KdlTghjy02fcSEkxPKmNj5W8,25695
-datarobotx/client/status.py,sha256=FeL6fiyhLtBxqIuLK_BWDgAR0wcBT218XFwbEAYalqk,2425
+datarobotx/client/projects.py,sha256=s1fgptVn50kM0Oug2fjrVe3l6aAiJd-L3l21PFSPMhw,25796
+datarobotx/client/status.py,sha256=8NaDj8qCSj1BKwEG-u73zWez-Xwhrly7cU5PIqyic_Q,2393
 datarobotx/common/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-datarobotx/common/client.py,sha256=TrjwGAIoxU2efp_YXa3PiZlQDIyRM5-sxopqJpVETPo,7760
-datarobotx/common/config.py,sha256=kVkaVX9JCjxuM_rzWfjgbATOoBvDuzDf9WuMn10zadY,9358
-datarobotx/common/configurator.py,sha256=vdTL4Esk_QYyWNg31EGap7PsBvhL7QizGDVr0FEKxSo,4588
+datarobotx/common/client.py,sha256=46ss1jHo7b_N0TAndGNkF1PWAgnXdy7vRZSXMWFiqRI,7772
+datarobotx/common/config.py,sha256=YaWPKO8rv59x-JBH2JoghQZHwZ5H7s77-HnHVUwEF4Q,9292
+datarobotx/common/configurator.py,sha256=-_zKi8uGHjtmQPAytun64yDxvDUyR1YbLFPKeuKjwHo,4600
 datarobotx/common/dr_config.py,sha256=RCFE7VEMFxmSFF2Ry2NvONC4g9xh98_QChuubgaq1OI,131191
-datarobotx/common/logging.py,sha256=r9vB5UAT4yGhX9jp98rtHGr3TykFS2yDYiAJvygaF6Q,10494
+datarobotx/common/logging.py,sha256=T_w9R94Tc_1FMEgXznf2VvqFS2OqykD44ES2XAhRV_4,10296
 datarobotx/common/transformations.py,sha256=ZZpNBmgu1jZvfrNoVtwfLsvSvwGDE5WCTrKkfYosngA,3980
 datarobotx/common/ts_helpers.py,sha256=SF1l_WxaYgt2Z6cKH5ab5WX6ExH3c4BW0aEgrm1qMz8,15032
-datarobotx/common/types.py,sha256=HJqSsXwz5kr7TeVzNc3J5vPsBwCO3pgMCkEartnoG9o,605
-datarobotx/common/utils.py,sha256=U0rZkyksp9xgadBi6stZGFBPa2F93hZFWMV9MkwPVCc,27474
-datarobotx/llm/__init__.py,sha256=MHMQ_XOreud0vpjRRPAP43r6rb1TjrqvZJol84HMFO0,143
-datarobotx/llm/utils.py,sha256=nd41kCCdlp3PJjDoqZfkpgecvNabkvWwZ2llBaUuIpU,3769
+datarobotx/common/types.py,sha256=amp5OkDc_5EN7t1aPzecMCsXlPD_spRKILhYpB6OgUE,934
+datarobotx/common/utils.py,sha256=0g7J8q4uO5SjkA_waEhVU2rkHN74FbL7XhdIJCWXeTQ,27452
+datarobotx/llm/__init__.py,sha256=OTpdKj39Erh1gfXz5ptoUP6r2ptaBsDNdREpwJU4yO0,279
+datarobotx/llm/utils.py,sha256=vnIKiEOWb2s55mIYl9RaFK9jPCZcrpmYZWb-681Xpug,3857
 datarobotx/llm/chains/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-datarobotx/llm/chains/data_dict.py,sha256=Ee4761z8v5AvcbX7nykMhEHhEW-TnXr56K6BDe8q5G0,12665
-datarobotx/llm/chains/enrich.py,sha256=QXRyxttsyfTMpC5Jo1eEq9ccEoh7MPzcB7ktqMS4ej0,12342
+datarobotx/llm/chains/data_dict.py,sha256=ruhZKwJ1dKHi048bu2W1NePsTpApmIpdVPABo_6hNLo,12693
+datarobotx/llm/chains/enrich.py,sha256=EYDOuKF1DvCQ1xAGkK-AHE8IxEO2t_ZMxyK8tQBdDtk,12398
+datarobotx/mlflow/__init__.py,sha256=v0fP8ZI3ogxKP2PSqzFRY69-Ou92AOPuGcebJXggFl0,284
+datarobotx/mlflow/drx_mlflow.py,sha256=lVH1thzdvTahhktkUTRcJR5hKvmhYTIBp1Nxp8d3pTQ,26469
 datarobotx/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 datarobotx/models/autoanomaly.py,sha256=IkhJzilTX1NWiiiEQbVgnd9rWRHnFdeUtdv03kREDXc,2754
 datarobotx/models/autocluster.py,sha256=UBwcx1_oFrlXnae4oNx3poNybmq1bBh_MPnsPbF_46w,3332
 datarobotx/models/automl.py,sha256=K3tTFn84WVdLobzDBDvEnOtyXazrIcZ86y0hN7elSho,2465
-datarobotx/models/autopilot.py,sha256=v2WlNOM55nQuaRtmCaBsnXJqlFtsfg71EAUXKkIo7BI,11349
-datarobotx/models/autots.py,sha256=zf5UmPJ_M1fF_lkiB_oixQ4hWb196F1KofHHm3ym6Fw,11242
-datarobotx/models/colreduce.py,sha256=Y0gXfziUEKOtOTMgGwprl4AZAAxtAvYT3zATuWHBJWg,14976
-datarobotx/models/deploy.py,sha256=D0UyHy7YtT3BPJvrdWqEbSNn2eAOXhJGbvsPVMSaoEY,24551
+datarobotx/models/autopilot.py,sha256=gh1M4QCg6LQXzXc2G60vAAiGtZDROKDWlM23E0nmtHM,11502
+datarobotx/models/autots.py,sha256=TwPdPJbLvTXfd-Rjm7toj71X5jRoTLKBDwUuI-FpGkU,11210
+datarobotx/models/colreduce.py,sha256=jtQ1MR1eRCLnJ7L3qhsIebC4JJ021qYl7ndYr5q7_IM,15037
+datarobotx/models/deploy.py,sha256=UP5tYcPVxLzISCDhoSJMNAfJlAPhKc3AqvdoSpowqxk,24902
 datarobotx/models/deployment.py,sha256=7Dd1Fi8G08lh2ztuMXmZZDRGb9vhnkwotQt-JX5ECCs,33947
 datarobotx/models/evaluation.py,sha256=zoIgR5myd42gK5w3D0pFICBBoskgeVF1moP1FfZ3_pA,7914
 datarobotx/models/featurediscovery.py,sha256=MvqXKQOKZcGNVF8lv5L1oh1JEF9bZNUmCG4vGZ1-_gk,16937
 datarobotx/models/intraproject.py,sha256=3B4d26qjwgq73HCkMLLh2m05Rtxe7mDhX1epmmFsWnM,5593
-datarobotx/models/model.py,sha256=owuGy1RTiRlBG2KA62LRLv6QKDj9t_0LEkaGtVhFc5Q,28420
-datarobotx/models/selfdiscovery.py,sha256=4cJ-ADXcQ1ten0ZNHVp2KvKLZ8Jb8xKIT7Dj50laPgA,9736
+datarobotx/models/model.py,sha256=1wLKyhcJt4XUd6L1AYGZtOFpaz605ssnb-STK8uHp_8,28701
+datarobotx/models/selfdiscovery.py,sha256=fmHPDGvzpxJkuVihX8S-5EMtsLa4p1o_vx_dWcQky68,9722
 datarobotx/models/share.py,sha256=DHUu-V08sDS0uLEYI35_KXkX6PHbvyJJUdOss38Bsno,4422
-datarobotx/models/sparkingest.py,sha256=E6bjHRPYzAxMwaPCRQdUFbjWodfvrTha05dpCvW0sIo,20800
+datarobotx/models/sparkingest.py,sha256=MVEKjr23uXE-BuTdY76a4LdZ-R5f5aKQ0LGHOlL24Ow,21316
+datarobotx/openblueprints/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+datarobotx/openblueprints/blueprint_string_converter.py,sha256=u4qIN2ESOKWTmZ9BMLghbXs7b7kp4iN8UyTAbxSshYI,6473
+datarobotx/openblueprints/mapping_object.py,sha256=YXgSShwgzs2GH3-HOrHprIYznotY0RI1wcAdVZJEE8A,2126
 datarobotx/viz/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-datarobotx/viz/charts.py,sha256=Kw9ZSnNbNmCXk-kYioNtaAPQoNrfIvJp0Xn-Qh_NNts,4906
-datarobotx/viz/leaderboard.py,sha256=6pZyYdcSYAttUldsAEHY48lSZHzSbj90Z7hjysKmmes,6326
-datarobotx/viz/modelcard.py,sha256=NqTrKeCMNfo9sIi05e_SANF-2ExVO1tX5FSKe1Ucf7s,9407
-datarobotx/viz/viz.py,sha256=deS2zmkfNWl5JfLycipyF1e0aeshsXm7FDB2ZPNxbhY,9508
+datarobotx/viz/charts.py,sha256=l_2IrjSNPD4ZeuotOyTG3p_8STKCkreEcwJTorSs1ZQ,4906
+datarobotx/viz/leaderboard.py,sha256=PzLbHlOGjIYCZffWmHLDcO8uACXoCgXu9ukxmoUOWO8,6633
+datarobotx/viz/modelcard.py,sha256=RsdmNnGlHvBs6GUXynLY9iZVczGK6lXR_eXuji7C5lk,9461
+datarobotx/viz/viz.py,sha256=2MArSk0Wcd8nzCdckUXCSoRR9xCJuN1RwDLbnEJVfOA,9447
 datarobotx/viz/templates/drx_button.html,sha256=z2wsHdX64v0kbsleMCyo943QnEMGxi0RfHI34hyHMCg,926
 datarobotx/viz/templates/leaderboard.html,sha256=SYSaWRuNBXEzNcm0KE9TUbCo4KDHVB8wABbeQiiz3f8,1490
 datarobotx/viz/templates/leaderboard.md,sha256=QeFvLJVpLSJ1SwHF1cWdj46toXo_1ZD7OznMSG1Bx-k,414
 datarobotx/viz/templates/model_card.html,sha256=H2YKd4gNBGnaEMLCGeywx1DR6O3zvFM_ci9cxfvQ-p0,5433
 datarobotx/viz/templates/model_card.md,sha256=5Bdd7jKLEYY-hpeZ2I99C6sJwMqaeMS5jRbwLcmrCuc,48
 datarobotx/viz/templates/robot.svg,sha256=ZVruQowP1A7ri6frXGiCZnpOnY_KM8Osi66nMJAdyiM,3140
 datarobotx/viz/templates/robot_large.svg,sha256=lAbBH7yv151ngs7Y1jlMw8anqQ-PG77tzTJawLCrc3E,3135
-datarobotx-0.1.8.dist-info/METADATA,sha256=taSD6WnVI14S1WdXjl0FT8F25xHNcZmh3hEEGo_phB8,5731
-datarobotx-0.1.8.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-datarobotx-0.1.8.dist-info/top_level.txt,sha256=_lN2CPexvLnaRX18n5OTMk9KDKnkKNABD7EzuwyfUpI,11
-datarobotx-0.1.8.dist-info/RECORD,,
+datarobotx-0.1.9.dist-info/METADATA,sha256=pKEdIqSsVm4fMT1qDz1ffev5G0cm19RcdnFXcOIIR9A,5743
+datarobotx-0.1.9.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+datarobotx-0.1.9.dist-info/top_level.txt,sha256=_lN2CPexvLnaRX18n5OTMk9KDKnkKNABD7EzuwyfUpI,11
+datarobotx-0.1.9.dist-info/RECORD,,
```

