# Comparing `tmp/keras-cv-attention-models-1.3.9.tar.gz` & `tmp/keras-cv-attention-models-1.4.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "/home/runner/work/keras_cv_attention_models/keras_cv_attention_models/dist/.tmp-ae36lcek/keras-cv-attention-models-1.3.9.tar", last modified: Wed Jan 18 09:42:18 2023, max compression
+gzip compressed data, was "keras-cv-attention-models-1.4.1.tar", last modified: Wed Apr 10 05:21:03 2024, max compression
```

## Comparing `keras-cv-attention-models-1.3.9.tar` & `keras-cv-attention-models-1.4.1.tar`

### file list

```diff
@@ -1,175 +1,266 @@
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/
--rw-r--r--   0 runner    (1001) docker     (122)     1067 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/LICENSE
--rw-r--r--   0 runner    (1001) docker     (122)   139354 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)   138228 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/README.md
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/kecam/
--rw-r--r--   0 runner    (1001) docker     (122)       40 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/kecam/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/
--rw-r--r--   0 runner    (1001) docker     (122)     2925 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/aotnet/
--rw-r--r--   0 runner    (1001) docker     (122)     7296 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/aotnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    21682 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/aotnet/aotnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/attention_layers/
--rw-r--r--   0 runner    (1001) docker     (122)    10847 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/attention_layers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/beit/
--rw-r--r--   0 runner    (1001) docker     (122)    10421 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/beit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    22104 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/beit/beit.py
--rw-r--r--   0 runner    (1001) docker     (122)     1145 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/beit/eva.py
--rw-r--r--   0 runner    (1001) docker     (122)     2820 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/beit/flexivit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/botnet/
--rw-r--r--   0 runner    (1001) docker     (122)     6692 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/botnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13056 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/botnet/botnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/caformer/
--rw-r--r--   0 runner    (1001) docker     (122)     5769 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/caformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9331 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/caformer/caformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/cmt/
--rw-r--r--   0 runner    (1001) docker     (122)     6493 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/cmt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15773 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/cmt/cmt.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coat/
--rw-r--r--   0 runner    (1001) docker     (122)     5744 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coat/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18162 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coat/coat.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coatnet/
--rw-r--r--   0 runner    (1001) docker     (122)     7484 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coatnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14326 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coatnet/coatnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/
--rw-r--r--   0 runner    (1001) docker     (122)     1760 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    32784 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/anchors_func.py
--rw-r--r--   0 runner    (1001) docker     (122)    33048 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/data.py
--rw-r--r--   0 runner    (1001) docker     (122)    25907 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/eval_func.py
--rw-r--r--   0 runner    (1001) docker     (122)    25451 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/losses.py
--rw-r--r--   0 runner    (1001) docker     (122)    29829 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/common_layers.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/convnext/
--rw-r--r--   0 runner    (1001) docker     (122)     6700 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/convnext/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8033 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/convnext/convnext.py
--rw-r--r--   0 runner    (1001) docker     (122)     2782 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/convnext/convnext_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/cotnet/
--rw-r--r--   0 runner    (1001) docker     (122)     4804 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/cotnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8706 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/cotnet/cotnet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/davit/
--rw-r--r--   0 runner    (1001) docker     (122)     5364 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/davit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13613 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/davit/davit.py
--rw-r--r--   0 runner    (1001) docker     (122)    16520 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/download_and_load.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/edgenext/
--rw-r--r--   0 runner    (1001) docker     (122)     5279 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/edgenext/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11602 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/edgenext/edgenext.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientdet/
--rw-r--r--   0 runner    (1001) docker     (122)     6354 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientdet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    22757 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientdet/efficientdet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientformer/
--rw-r--r--   0 runner    (1001) docker     (122)     4673 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6478 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientformer/efficientformer.py
--rw-r--r--   0 runner    (1001) docker     (122)    13277 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientformer/efficientformer_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientnet/
--rw-r--r--   0 runner    (1001) docker     (122)    12794 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    33979 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientnet/convert_effnetv2_model.py
--rw-r--r--   0 runner    (1001) docker     (122)     8523 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientnet/efficientnet_v1.py
--rw-r--r--   0 runner    (1001) docker     (122)    17385 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientnet/efficientnet_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/gcvit/
--rw-r--r--   0 runner    (1001) docker     (122)     2618 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/gcvit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     7917 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/gcvit/gcvit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/ghostnet/
--rw-r--r--   0 runner    (1001) docker     (122)     3488 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/ghostnet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1695 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/ghostnet/ghostnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     7505 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/ghostnet/ghostnet_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/gpvit/
--rw-r--r--   0 runner    (1001) docker     (122)     3028 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/gpvit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    12563 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/gpvit/gpvit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/halonet/
--rw-r--r--   0 runner    (1001) docker     (122)     5808 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/halonet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    17404 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/halonet/halonet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/hornet/
--rw-r--r--   0 runner    (1001) docker     (122)     6773 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/hornet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10681 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/hornet/hornet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/iformer/
--rw-r--r--   0 runner    (1001) docker     (122)     2955 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/iformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8667 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/iformer/iformer.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/
--rw-r--r--   0 runner    (1001) docker     (122)     5258 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    47798 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/augment.py
--rw-r--r--   0 runner    (1001) docker     (122)    11599 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/callbacks.py
--rw-r--r--   0 runner    (1001) docker     (122)    29409 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/data.py
--rw-r--r--   0 runner    (1001) docker     (122)    15708 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/eval_func.py
--rw-r--r--   0 runner    (1001) docker     (122)     3447 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/losses.py
--rw-r--r--   0 runner    (1001) docker     (122)     6002 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/token_label.py
--rw-r--r--   0 runner    (1001) docker     (122)    12894 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/train_func.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/levit/
--rw-r--r--   0 runner    (1001) docker     (122)     8162 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/levit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    16767 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/levit/levit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/maxvit/
--rw-r--r--   0 runner    (1001) docker     (122)     4768 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/maxvit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10403 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/maxvit/maxvit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/
--rw-r--r--   0 runner    (1001) docker     (122)    20245 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4911 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/gated_mlp.py
--rw-r--r--   0 runner    (1001) docker     (122)     8591 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/mlp_mixer.py
--rw-r--r--   0 runner    (1001) docker     (122)     7093 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/res_mlp.py
--rw-r--r--   0 runner    (1001) docker     (122)     9567 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/wave_mlp.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/
--rw-r--r--   0 runner    (1001) docker     (122)     9186 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1865 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/fbnetv3.py
--rw-r--r--   0 runner    (1001) docker     (122)     1912 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/lcnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     9535 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/mobilenetv3.py
--rw-r--r--   0 runner    (1001) docker     (122)     2450 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/tinynet.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilevit/
--rw-r--r--   0 runner    (1001) docker     (122)     7033 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilevit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15445 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilevit/mobilevit.py
--rw-r--r--   0 runner    (1001) docker     (122)     3027 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilevit/mobilevit_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/model_surgery/
--rw-r--r--   0 runner    (1001) docker     (122)      733 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/model_surgery/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    26023 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/model_surgery/model_surgery.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/nat/
--rw-r--r--   0 runner    (1001) docker     (122)     6192 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/nat/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    11373 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/nat/nat.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/nfnets/
--rw-r--r--   0 runner    (1001) docker     (122)     7230 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/nfnets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15999 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/nfnets/nfnets.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/pvt/
--rw-r--r--   0 runner    (1001) docker     (122)     3165 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/pvt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9294 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/pvt/pvt.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnest/
--rw-r--r--   0 runner    (1001) docker     (122)     3820 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4534 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnest/resnest.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/
--rw-r--r--   0 runner    (1001) docker     (122)    11201 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9648 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/regnet.py
--rw-r--r--   0 runner    (1001) docker     (122)     1889 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/resnet_deep.py
--rw-r--r--   0 runner    (1001) docker     (122)     6272 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/resnet_quad.py
--rw-r--r--   0 runner    (1001) docker     (122)     2879 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/resnext.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/swin_transformer_v2/
--rw-r--r--   0 runner    (1001) docker     (122)    13127 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/swin_transformer_v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    25278 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2.py
--rw-r--r--   0 runner    (1001) docker     (122)    17312 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2_timm.py
--rw-r--r--   0 runner    (1001) docker     (122)   409173 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/test_images.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/tinyvit/
--rw-r--r--   0 runner    (1001) docker     (122)     3497 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/tinyvit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     6710 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/tinyvit/tinyvit.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/uniformer/
--rw-r--r--   0 runner    (1001) docker     (122)     5701 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/uniformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    13847 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/uniformer/uniformer.py
--rw-r--r--   0 runner    (1001) docker     (122)       22 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/version.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/
--rw-r--r--   0 runner    (1001) docker     (122)     7837 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8127 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/attention_score_maps.py
--rw-r--r--   0 runner    (1001) docker     (122)     9883 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/gradcam_heatmap.py
--rw-r--r--   0 runner    (1001) docker     (122)     4505 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/plot_func.py
--rw-r--r--   0 runner    (1001) docker     (122)     5178 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/visualize_filters.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/volo/
--rw-r--r--   0 runner    (1001) docker     (122)    12831 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/volo/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    22809 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/volo/volo.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolor/
--rw-r--r--   0 runner    (1001) docker     (122)     5406 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    18959 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolor/yolor.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolov7/
--rw-r--r--   0 runner    (1001) docker     (122)     4924 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolov7/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    23056 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolov7/yolov7.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolox/
--rw-r--r--   0 runner    (1001) docker     (122)     4557 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolox/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    15507 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolox/yolox.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models.egg-info/
--rw-r--r--   0 runner    (1001) docker     (122)   139354 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)     5904 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (122)      198 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (122)       32 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/keras_cv_attention_models.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (122)       38 2023-01-18 09:42:18.000000 keras-cv-attention-models-1.3.9/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (122)     2101 2023-01-18 09:41:17.000000 keras-cv-attention-models-1.3.9/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/
+-rw-r--r--   0 runner    (1001) docker     (127)     1067 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)   188273 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)   181168 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.525475 keras-cv-attention-models-1.4.1/kecam/
+-rw-r--r--   0 runner    (1001) docker     (127)      251 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/kecam/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.529475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/
+-rw-r--r--   0 runner    (1001) docker     (127)     4417 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.529475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/aotnet/
+-rw-r--r--   0 runner    (1001) docker     (127)     7296 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/aotnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22272 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/aotnet/aotnet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.529475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/attention_layers/
+-rw-r--r--   0 runner    (1001) docker     (127)    11307 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/attention_layers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7468 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/backend.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.533475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/
+-rw-r--r--   0 runner    (1001) docker     (127)    19158 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40039 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/beit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1758 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/dinov2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1117 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/eva.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1875 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/eva02.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1512 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/flexivit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1470 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/meta_transformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2343 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/vit.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.533475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/botnet/
+-rw-r--r--   0 runner    (1001) docker     (127)     5531 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/botnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13201 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/botnet/botnet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.533475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/caformer/
+-rw-r--r--   0 runner    (1001) docker     (127)     5769 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/caformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10822 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/caformer/caformer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.533475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/clip/
+-rw-r--r--   0 runner    (1001) docker     (127)      829 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/clip/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8390 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/clip/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2115 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/clip/tf_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15814 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/clip/tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3518 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/clip/torch_data.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.533475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/cmt/
+-rw-r--r--   0 runner    (1001) docker     (127)     6493 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/cmt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14985 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/cmt/cmt.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.533475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coat/
+-rw-r--r--   0 runner    (1001) docker     (127)     4601 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coat/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17242 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coat/coat.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.533475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coatnet/
+-rw-r--r--   0 runner    (1001) docker     (127)     7484 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coatnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14561 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coatnet/coatnet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.533475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/
+-rw-r--r--   0 runner    (1001) docker     (127)     2489 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37979 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/anchors_func.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30908 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/eval_func.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1173 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/info.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30960 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/tf_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31802 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/tf_losses.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15930 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/torch_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19753 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/torch_losses.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37169 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/common_layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.537475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/convnext/
+-rw-r--r--   0 runner    (1001) docker     (127)     6961 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/convnext/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11026 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/convnext/convnext.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3151 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/convnext/convnext_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.537475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/cotnet/
+-rw-r--r--   0 runner    (1001) docker     (127)     4804 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/cotnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10269 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/cotnet/cotnet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.537475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/cspnext/
+-rw-r--r--   0 runner    (1001) docker     (127)     2045 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/cspnext/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7972 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/cspnext/cspnext.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.537475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/davit/
+-rw-r--r--   0 runner    (1001) docker     (127)     5364 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/davit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17278 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/davit/davit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32095 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/download_and_load.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.537475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/edgenext/
+-rw-r--r--   0 runner    (1001) docker     (127)     5348 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/edgenext/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14175 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/edgenext/edgenext.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.537475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientdet/
+-rw-r--r--   0 runner    (1001) docker     (127)     6631 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientdet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24283 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientdet/efficientdet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.537475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientformer/
+-rw-r--r--   0 runner    (1001) docker     (127)     4674 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8408 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientformer/efficientformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15435 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientformer/efficientformer_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.537475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/
+-rw-r--r--   0 runner    (1001) docker     (127)    14861 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    33979 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/convert_effnetv2_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2890 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/efficientnet_edgetpu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8846 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/efficientnet_v1.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18505 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/efficientnet_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.537475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientvit/
+-rw-r--r--   0 runner    (1001) docker     (127)     8885 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientvit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13529 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientvit/efficientvit_b.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13574 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientvit/efficientvit_m.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/fasternet/
+-rw-r--r--   0 runner    (1001) docker     (127)     2974 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/fasternet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6531 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/fasternet/fasternet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/fastervit/
+-rw-r--r--   0 runner    (1001) docker     (127)     3408 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/fastervit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16412 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/fastervit/fastervit.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/fastvit/
+-rw-r--r--   0 runner    (1001) docker     (127)     3863 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/fastvit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13383 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/fastvit/fastvit.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/gcvit/
+-rw-r--r--   0 runner    (1001) docker     (127)     3192 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/gcvit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10796 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/gcvit/gcvit.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/ghostnet/
+-rw-r--r--   0 runner    (1001) docker     (127)     3617 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/ghostnet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1803 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/ghostnet/ghostnet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8712 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/ghostnet/ghostnet_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/gpt2/
+-rw-r--r--   0 runner    (1001) docker     (127)     2098 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/gpt2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14275 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/gpt2/gpt2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/gpvit/
+-rw-r--r--   0 runner    (1001) docker     (127)     3028 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/gpvit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14091 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/gpvit/gpvit.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/halonet/
+-rw-r--r--   0 runner    (1001) docker     (127)     5278 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/halonet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18874 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/halonet/halonet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/hiera/
+-rw-r--r--   0 runner    (1001) docker     (127)     2794 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/hiera/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11153 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/hiera/hiera.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/hornet/
+-rw-r--r--   0 runner    (1001) docker     (127)     6773 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/hornet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11412 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/hornet/hornet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.541475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/iformer/
+-rw-r--r--   0 runner    (1001) docker     (127)     2904 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/iformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10480 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/iformer/iformer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.545475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/
+-rw-r--r--   0 runner    (1001) docker     (127)     5837 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    47911 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/augment.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24060 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/callbacks.py
+-rw-r--r--   0 runner    (1001) docker     (127)    32596 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/data.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13733 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/eval_func.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3447 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/losses.py
+-rw-r--r--   0 runner    (1001) docker     (127)      577 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11367 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/tensorrt_engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6002 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/token_label.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15814 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/train_func.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.545475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/inceptionnext/
+-rw-r--r--   0 runner    (1001) docker     (127)     2560 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/inceptionnext/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6133 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/inceptionnext/inceptionnext.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3288 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/keras_core_functional.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.545475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/levit/
+-rw-r--r--   0 runner    (1001) docker     (127)     8185 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/levit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18042 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/levit/levit.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.545475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/llama2/
+-rw-r--r--   0 runner    (1001) docker     (127)     2492 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/llama2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11111 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/llama2/llama2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.545475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/maxvit/
+-rw-r--r--   0 runner    (1001) docker     (127)     4768 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/maxvit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11483 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/maxvit/maxvit.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.545475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/
+-rw-r--r--   0 runner    (1001) docker     (127)    20334 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6268 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/gated_mlp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9796 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/mlp_mixer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9026 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/res_mlp.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10113 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/wave_mlp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.545475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/
+-rw-r--r--   0 runner    (1001) docker     (127)     9186 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1973 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/fbnetv3.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2068 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/lcnet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9958 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/mobilenetv3.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2558 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/tinynet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.549475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilevit/
+-rw-r--r--   0 runner    (1001) docker     (127)     7033 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilevit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19732 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilevit/mobilevit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3090 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilevit/mobilevit_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.549475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/model_surgery/
+-rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/model_surgery/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    53336 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/model_surgery/model_surgery.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3668 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/models.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.549475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/moganet/
+-rw-r--r--   0 runner    (1001) docker     (127)     2743 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/moganet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8821 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/moganet/moganet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.549475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/nat/
+-rw-r--r--   0 runner    (1001) docker     (127)     7844 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/nat/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3063 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/nat/dinat.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16784 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/nat/nat.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.549475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/nfnets/
+-rw-r--r--   0 runner    (1001) docker     (127)     7230 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/nfnets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16298 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/nfnets/nfnets.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20348 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/plot_func.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.549475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pvt/
+-rw-r--r--   0 runner    (1001) docker     (127)     3165 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pvt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11099 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pvt/pvt.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.549475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/
+-rw-r--r--   0 runner    (1001) docker     (127)     1984 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/callbacks.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19706 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/functional.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4462 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/initializers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    69035 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3827 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/losses.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2447 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29428 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/models.py
+-rw-r--r--   0 runner    (1001) docker     (127)      272 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/optimizers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3568 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.549475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/repvit/
+-rw-r--r--   0 runner    (1001) docker     (127)     3178 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/repvit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9099 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/repvit/repvit.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.549475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnest/
+-rw-r--r--   0 runner    (1001) docker     (127)     3820 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5057 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnest/resnest.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.553475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/
+-rw-r--r--   0 runner    (1001) docker     (127)    11201 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9916 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/regnet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2013 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/resnet_deep.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6618 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/resnet_quad.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3035 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/resnext.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.553475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/segment_anything/
+-rw-r--r--   0 runner    (1001) docker     (127)     3371 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/segment_anything/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4973 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/segment_anything/image_encoders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8312 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/segment_anything/mask_decoders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9738 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/segment_anything/prompt_encoders.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9929 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/segment_anything/sam.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.553475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/stable_diffusion/
+-rw-r--r--   0 runner    (1001) docker     (127)     3406 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/stable_diffusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7207 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/stable_diffusion/data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7803 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/stable_diffusion/encoder_decoder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7009 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/stable_diffusion/eval_func.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16593 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/stable_diffusion/stable_diffusion.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13559 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/stable_diffusion/unet.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.553475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/swin_transformer_v2/
+-rw-r--r--   0 runner    (1001) docker     (127)    14181 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/swin_transformer_v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30571 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19027 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2_timm.py
+-rw-r--r--   0 runner    (1001) docker     (127)   419557 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/test_images.py
+-rw-r--r--   0 runner    (1001) docker     (127)      598 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/tf_functional.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.553475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/tinyvit/
+-rw-r--r--   0 runner    (1001) docker     (127)     3497 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/tinyvit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7952 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/tinyvit/tinyvit.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.553475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/uniformer/
+-rw-r--r--   0 runner    (1001) docker     (127)     5701 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/uniformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14592 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/uniformer/uniformer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.553475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/vanillanet/
+-rw-r--r--   0 runner    (1001) docker     (127)     5292 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/vanillanet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9295 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/vanillanet/vanillanet.py
+-rw-r--r--   0 runner    (1001) docker     (127)       22 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/visualizing/
+-rw-r--r--   0 runner    (1001) docker     (127)     7782 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/visualizing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8926 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/visualizing/attention_score_maps.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9883 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/visualizing/gradcam_heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5166 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/visualizing/visualize_filters.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/volo/
+-rw-r--r--   0 runner    (1001) docker     (127)    12831 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/volo/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24680 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/volo/volo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolor/
+-rw-r--r--   0 runner    (1001) docker     (127)     5683 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21496 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolor/yolor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov7/
+-rw-r--r--   0 runner    (1001) docker     (127)     5201 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov7/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25917 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov7/yolov7.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov8/
+-rw-r--r--   0 runner    (1001) docker     (127)     7630 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov8/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7847 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov8/train.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10449 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov8/yolo_nas.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22588 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov8/yolov8.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolox/
+-rw-r--r--   0 runner    (1001) docker     (127)     4834 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolox/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17893 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolox/yolox.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/keras_cv_attention_models.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)   188273 2024-04-10 05:21:03.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     9353 2024-04-10 05:21:03.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-10 05:21:03.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      154 2024-04-10 05:21:03.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       32 2024-04-10 05:21:03.000000 keras-cv-attention-models-1.4.1/keras_cv_attention_models.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     2597 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-10 05:21:03.557475 keras-cv-attention-models-1.4.1/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)    15857 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/tests/test_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27435 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/tests/test_models.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1735 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/tests/test_models_tf.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4015 2024-04-10 05:19:39.000000 keras-cv-attention-models-1.4.1/tests/test_switch_to_deploy_tf.py
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive (GNU)
+POSIX tar archive
```

### Comparing `keras-cv-attention-models-1.3.9/LICENSE` & `keras-cv-attention-models-1.4.1/LICENSE`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,58 +1,89 @@
-from .version import __version__
+from keras_cv_attention_models import backend
+
+from keras_cv_attention_models.version import __version__
+from keras_cv_attention_models import plot_func
 from keras_cv_attention_models import attention_layers
-from keras_cv_attention_models import model_surgery
 from keras_cv_attention_models import beit
 from keras_cv_attention_models.beit import flexivit
 from keras_cv_attention_models.beit import eva
+from keras_cv_attention_models.beit import eva02
+from keras_cv_attention_models.beit import dinov2
+from keras_cv_attention_models.beit import meta_transformer
+from keras_cv_attention_models.beit import vit
 from keras_cv_attention_models import botnet
 from keras_cv_attention_models import caformer
 from keras_cv_attention_models import coat
 from keras_cv_attention_models import coatnet
 from keras_cv_attention_models import convnext
 from keras_cv_attention_models import cotnet
 from keras_cv_attention_models import cmt
+from keras_cv_attention_models import cspnext
 from keras_cv_attention_models import davit
 from keras_cv_attention_models import efficientnet
 from keras_cv_attention_models import edgenext
 from keras_cv_attention_models import efficientformer
-from keras_cv_attention_models import efficientdet
+from keras_cv_attention_models import fasternet
 from keras_cv_attention_models import gcvit
 from keras_cv_attention_models import ghostnet
-from keras_cv_attention_models import ghostnet as ghostnetv2  # Will be removed
+from keras_cv_attention_models import ghostnet as ghostnetv2  # alias name
+from keras_cv_attention_models import gpt2
+from keras_cv_attention_models import llama2
 from keras_cv_attention_models import halonet
+from keras_cv_attention_models import hiera
 from keras_cv_attention_models import hornet
 from keras_cv_attention_models import iformer
 from keras_cv_attention_models import levit
 from keras_cv_attention_models import mlp_family
 from keras_cv_attention_models.mlp_family import mlp_mixer
 from keras_cv_attention_models.mlp_family import res_mlp
 from keras_cv_attention_models.mlp_family import gated_mlp
 from keras_cv_attention_models.mlp_family import wave_mlp
 from keras_cv_attention_models.mobilenetv3_family import fbnetv3
 from keras_cv_attention_models.mobilenetv3_family import lcnet
 from keras_cv_attention_models.mobilenetv3_family import mobilenetv3
 from keras_cv_attention_models.mobilenetv3_family import tinynet
+from keras_cv_attention_models import efficientvit
+from keras_cv_attention_models.efficientvit import efficientvit_b
+from keras_cv_attention_models.efficientvit import efficientvit_m
+from keras_cv_attention_models import inceptionnext
 from keras_cv_attention_models import maxvit
 from keras_cv_attention_models import mobilevit
+from keras_cv_attention_models import moganet
 from keras_cv_attention_models import nat
-from keras_cv_attention_models import nfnets
+from keras_cv_attention_models.nat import dinat
 from keras_cv_attention_models import pvt
+from keras_cv_attention_models import repvit
 from keras_cv_attention_models import tinyvit
 from keras_cv_attention_models import resnest
 from keras_cv_attention_models import resnet_family
 from keras_cv_attention_models.resnet_family import resnext
 from keras_cv_attention_models.resnet_family import resnet_quad
 from keras_cv_attention_models.resnet_family import resnet_deep
 from keras_cv_attention_models.resnet_family import regnet
 from keras_cv_attention_models import gpvit
 from keras_cv_attention_models import swin_transformer_v2
 from keras_cv_attention_models import uniformer
-from keras_cv_attention_models import volo
+from keras_cv_attention_models import fastervit
+from keras_cv_attention_models import fastvit
+from keras_cv_attention_models import vanillanet
+from keras_cv_attention_models import download_and_load
+from keras_cv_attention_models import imagenet
+from keras_cv_attention_models import test_images
+from keras_cv_attention_models import model_surgery
+from keras_cv_attention_models import efficientdet
 from keras_cv_attention_models import yolox
 from keras_cv_attention_models import yolor
 from keras_cv_attention_models import yolov7
-from keras_cv_attention_models import download_and_load
-from keras_cv_attention_models import visualizing
-from keras_cv_attention_models import imagenet
+from keras_cv_attention_models import yolov8
+from keras_cv_attention_models.yolov8 import yolo_nas
 from keras_cv_attention_models import coco
-from keras_cv_attention_models import test_images
+from keras_cv_attention_models import clip
+from keras_cv_attention_models.clip import tokenizer
+from keras_cv_attention_models import stable_diffusion
+from keras_cv_attention_models import segment_anything
+from keras_cv_attention_models import segment_anything as sam  # Alias name
+
+if backend.is_tensorflow_backend:
+    from keras_cv_attention_models import nfnets
+    from keras_cv_attention_models import volo
+    from keras_cv_attention_models import visualizing
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/aotnet/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/aotnet/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/aotnet/aotnet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/aotnet/aotnet.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,10 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
-import os
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     anti_alias_downsample,
     batchnorm_with_activation,
     conv2d_no_bias,
     drop_block,
     drop_connect_rates_split,
@@ -70,24 +69,24 @@
         nn = attention_layers.split_attention_conv2d(nn, **attn_params, filters=filters, strides=strides, activation=attn_act, name=name + "sa_")
     elif attn_type == "cot":  # cot_attention from cotnet
         nn = attention_layers.cot_attention(nn, **attn_params, strides=strides, activation=attn_act, name=name + "cot_")
     elif attn_type == "outlook":  # outlook_attention from volo
         nn = attention_layers.outlook_attention(nn, filters, **attn_params, name=name + "outlook_")
         need_downsaple = True
     # elif attn_type == "groups_conv":  # ResNeXt like
-    #     nn = conv2d_no_bias(nn, filters, **attn_params, strides=strides, padding="SAME", name=name + "GC_")
+    #     nn = conv2d_no_bias(nn, filters, **attn_params, strides=strides, padding="same", name=name + "GC_")
     else:  # ResNet and `groups > 1` for ResNeXt like
         groups = groups if group_size == 0 else filters // group_size
         conv_name = (name + "GC_") if groups > 1 else name
-        nn = conv2d_no_bias(nn, filters, 3, strides=strides, padding="SAME", groups=groups, name=conv_name)
+        nn = conv2d_no_bias(nn, filters, 3, strides=strides, padding="same", groups=groups, name=conv_name)
 
     # if strides != 1 and (nn.shape[1] == inputs.shape[1] or nn.shape[2] == inputs.shape[2]):  # Downsample
     if strides != 1 and need_downsaple:  # Downsample
-        # nn = keras.layers.ZeroPadding2D(padding=1, name=name + "pad")(nn)
-        nn = keras.layers.AveragePooling2D(pool_size=2, strides=strides, name=name + "pool")(nn)
+        # nn = layers.ZeroPadding2D(padding=1, name=name + "pad")(nn)
+        nn = layers.AvgPool2D(pool_size=2, strides=strides, name=name + "pool")(nn)
 
     if bn_after_attn:
         bn_params = {"use_evo_norm": use_evo_norm, "evo_norm_group_size": evo_norm_group_size, "epsilon": epsilon}
         nn = batchnorm_with_activation(nn, activation, zero_gamma=False, **bn_params, name=name)
 
     if attn_type is None and se_ratio:
         nn = se_module(nn, se_ratio=se_ratio, divisor=se_divisor, activation=attn_act, name=name + "se_")
@@ -98,15 +97,15 @@
 
 
 def conv_shortcut_branch(inputs, filters, preact=False, strides=1, shortcut_type="conv", bn_act_params={}, name=""):
     if shortcut_type is None:
         return None
 
     if strides > 1 and shortcut_type == "avg":
-        shortcut = keras.layers.AvgPool2D(strides, strides=strides, padding="SAME", name=name + "shortcut_down")(inputs)
+        shortcut = layers.AvgPool2D(strides, strides=strides, padding="same", name=name + "shortcut_down")(inputs)
         strides = 1
     elif strides > 1 and shortcut_type == "anti_alias":
         shortcut = anti_alias_downsample(inputs, kernel_size=3, strides=2, name=name + "shortcut_down")
         strides = 1
     else:
         shortcut = inputs
     shortcut = conv2d_no_bias(shortcut, filters, 1, strides=strides, name=name + "shortcut_")
@@ -116,25 +115,25 @@
 
 
 def deep_branch(
     inputs, filters, strides=1, hidden_channel_ratio=0.25, use_3x3_kernel=False, bn_after_attn=True, bn_act_params={}, attn_block_params={}, name=""
 ):
     hidden_filter = int(filters * hidden_channel_ratio)
     if use_3x3_kernel:
-        nn = conv2d_no_bias(inputs, hidden_filter, 3, strides=1, padding="SAME", name=name + "deep_1_")  # Using strides=1 for not changing input shape
-        # nn = conv2d_no_bias(inputs, hidden_filter, 3, strides=strides, padding="SAME", name=name + "1_")
+        nn = conv2d_no_bias(inputs, hidden_filter, 3, strides=1, padding="same", name=name + "deep_1_")  # Using strides=1 for not changing input shape
+        # nn = conv2d_no_bias(inputs, hidden_filter, 3, strides=strides, padding="same", name=name + "1_")
         # strides = 1
     else:
-        nn = conv2d_no_bias(inputs, hidden_filter, 1, strides=1, padding="VALID", name=name + "deep_1_")
+        nn = conv2d_no_bias(inputs, hidden_filter, 1, strides=1, padding="valid", name=name + "deep_1_")
     nn = batchnorm_with_activation(nn, zero_gamma=False, **bn_act_params, name=name + "deep_1_")
     # bn_after_attn = False if use_3x3_kernel else bn_after_attn
     nn = attn_block(nn, hidden_filter, strides, **attn_block_params, **bn_act_params, bn_after_attn=bn_after_attn, name=name + "deep_2_")
 
     if not use_3x3_kernel:
-        nn = conv2d_no_bias(nn, filters, 1, strides=1, padding="VALID", name=name + "deep_3_")
+        nn = conv2d_no_bias(nn, filters, 1, strides=1, padding="valid", name=name + "deep_3_")
     return nn
 
 
 def aot_block(
     inputs,
     filters,
     strides=1,
@@ -151,49 +150,50 @@
     evo_norm_group_size=-1,
     activation="relu",
     attn_block_params={},
     name="",
 ):
     if attn_block_params.get("attn_type", None) == "halo":  # HaloAttention
         halo_block_size = attn_block_params.get("attn_params", {}).get("block_size", DEFAULT_PARAMS["halo"]["block_size"])
-        if inputs.shape[1] % halo_block_size != 0 or inputs.shape[2] % halo_block_size != 0:
-            gap_h = halo_block_size - inputs.shape[1] % halo_block_size
-            gap_w = halo_block_size - inputs.shape[2] % halo_block_size
+        height, width = inputs.shape[1:-1] if image_data_format() == "channels_last" else inputs.shape[2:]
+        if height % halo_block_size != 0 or width % halo_block_size != 0:
+            gap_h = halo_block_size - height % halo_block_size
+            gap_w = halo_block_size - width % halo_block_size
             pad_head_h, pad_tail_h = gap_h // 2, gap_h - gap_h // 2
             pad_head_w, pad_tail_w = gap_w // 2, gap_w - gap_w // 2
             # print(f">>>> Halo pad: {inputs.shape = }, {pad_head_h = }, {pad_tail_h = }, {pad_head_w = }, {pad_tail_w = }")
-            inputs = keras.layers.ZeroPadding2D(padding=((pad_head_h, pad_tail_h), (pad_head_w, pad_tail_w)), name=name + "gap_pad")(inputs)
+            inputs = layers.ZeroPadding2D(padding=((pad_head_h, pad_tail_h), (pad_head_w, pad_tail_w)), name=name + "gap_pad")(inputs)
 
     bn_params = {"use_evo_norm": use_evo_norm, "evo_norm_group_size": evo_norm_group_size, "epsilon": epsilon}
     bn_act_params = {"activation": activation, "use_evo_norm": use_evo_norm, "evo_norm_group_size": evo_norm_group_size, "epsilon": epsilon}
     if preact:  # ResNetV2
         pre_inputs = batchnorm_with_activation(inputs, zero_gamma=False, **bn_act_params, name=name + "preact_")
     else:
         pre_inputs = inputs
 
     if conv_shortcut:  # Set a new shortcut using conv
         # short_act = activation if attn_block_params["attn_type"] == "bot" else None
         shortcut = conv_shortcut_branch(pre_inputs, filters, preact, strides, shortcut_type, bn_params, name=name)  # activation=None
     else:
-        shortcut = keras.layers.MaxPooling2D(strides, strides=strides, padding="SAME")(inputs) if strides > 1 else inputs
+        shortcut = layers.MaxPool2D(strides, strides=strides, padding="same")(inputs) if strides > 1 else inputs
 
     deep = deep_branch(pre_inputs, filters, strides, hidden_channel_ratio, use_3x3_kernel, bn_after_attn, bn_act_params, attn_block_params, name=name)
 
     # print(f">>>> {inputs.shape = }, {shortcut if shortcut is None else shortcut.shape = }, {deep.shape = }, {filters = }, {strides = }")
     if preact:  # ResNetV2
         deep = drop_block(deep, drop_rate)
-        return keras.layers.Add(name=name + "output")([shortcut, deep]) if shortcut is not None else deep  # if no shortcut
+        return layers.Add(name=name + "output")([shortcut, deep]) if shortcut is not None else deep  # if no shortcut
     else:
         if not (use_3x3_kernel and bn_after_attn):
             deep = batchnorm_with_activation(deep, activation=None, zero_gamma=True, **bn_params, name=name + "3_")
         deep = drop_block(deep, drop_rate)
-        out = keras.layers.Add(name=name + "add")([shortcut, deep]) if shortcut is not None else deep  # if no shortcut
+        out = layers.Add(name=name + "add")([shortcut, deep]) if shortcut is not None else deep  # if no shortcut
         if use_block_output_activation:
             out = activation_by_name(out, activation, name=name + "out_")
-        return keras.layers.Activation("linear", name=name + "output")(out)  # Identity, Just need a name here
+        return layers.Activation("linear", name=name + "output")(out)  # Identity, Just need a name here
 
 
 def aot_stack(
     inputs,
     blocks,
     filters,
     strides=2,
@@ -208,16 +208,17 @@
     groups=1,
     group_size=0,
     name="",
 ):
     nn = inputs
     # print(">>>> attn_types:", attn_types)
     strides_block_id = 0 if strides_first else blocks - 1
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
     for id in range(blocks):
-        conv_shortcut = True if id == 0 and (strides != 1 or inputs.shape[-1] != filters) else False
+        conv_shortcut = True if id == 0 and (strides != 1 or inputs.shape[channel_axis] != filters) else False
         cur_strides = strides if id == strides_block_id else 1
         block_name = name + "block{}_".format(id + 1)
         block_drop_rate = stack_drop[id] if isinstance(stack_drop, (list, tuple)) else stack_drop
         cur_ratio = hidden_channel_ratio[id] if isinstance(hidden_channel_ratio, (list, tuple)) else hidden_channel_ratio
         attn_block_params = {  # Just save the line width..
             "attn_type": attn_types[id] if isinstance(attn_types, (list, tuple)) else attn_types,
             "attn_params": attn_params[id] if isinstance(attn_params, (list, tuple)) else attn_params,
@@ -314,23 +315,26 @@
     output_num_features=0,
     dropout=0,
     model_name="aotnet",
     pretrained=None,
     kwargs=None,
 ):
     """Stem"""
-    inputs = keras.layers.Input(shape=input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(shape=input_shape)
     bn_params = {"use_evo_norm": use_evo_norm, "evo_norm_group_size": evo_norm_group_size, "epsilon": bn_epsilon}
     nn = aot_stem(inputs, stem_width, stem_type, activation, quad_stem_act, last_strides=stem_last_strides, bn_params=bn_params, name="stem_")
 
     if not preact:
         nn = batchnorm_with_activation(nn, activation=activation, **bn_params, name="stem_")
     if stem_downsample:
-        nn = keras.layers.ZeroPadding2D(padding=1, name="stem_pool_pad")(nn)
-        nn = keras.layers.MaxPooling2D(pool_size=3, strides=2, name="stem_pool")(nn)
+        nn = layers.ZeroPadding2D(padding=1, name="stem_pool_pad")(nn)
+        nn = layers.MaxPool2D(pool_size=3, strides=2, name="stem_pool")(nn)
 
     """ Stacks """
     block_params = {  # params same for all blocks
         "preact": preact,
         "use_3x3_kernel": use_3x3_kernel,
         "use_block_output_activation": use_block_output_activation,
         "bn_after_attn": bn_after_attn,
@@ -361,69 +365,80 @@
     """ Output """
     # nn = output_block(nn, output_num_features, activation, num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
     if output_num_features > 0:
         nn = conv2d_no_bias(nn, output_num_features, 1, strides=1, name="features_")
         nn = batchnorm_with_activation(nn, activation=activation, **bn_params, name="features_")
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
         if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     # reload_model_weights(model, pretrained_dict={}, sub_release="aotnet", input_shape=input_shape, pretrained=pretrained)
     return model
 
 
+@register_model
 def AotNet50(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", strides=2, **kwargs):
     num_blocks = [3, 4, 6, 3]
     strides = strides if isinstance(strides, (list, tuple)) else [1, 2, 2, strides]
     return AotNet(**locals(), model_name=kwargs.pop("model_name", "aotnet50"), **kwargs)
 
 
+@register_model
 def AotNet101(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", strides=2, **kwargs):
     num_blocks = [3, 4, 23, 3]
     strides = strides if isinstance(strides, (list, tuple)) else [1, 2, 2, strides]
     return AotNet(**locals(), model_name=kwargs.pop("model_name", "aotnet101"), **kwargs)
 
 
+@register_model
 def AotNet152(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", strides=2, **kwargs):
     num_blocks = [3, 8, 36, 3]
     strides = strides if isinstance(strides, (list, tuple)) else [1, 2, 2, strides]
     return AotNet(**locals(), model_name=kwargs.pop("model_name", "aotnet152"), **kwargs)
 
 
+@register_model
 def AotNet200(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", strides=2, **kwargs):
     num_blocks = [3, 24, 36, 3]
     strides = strides if isinstance(strides, (list, tuple)) else [1, 2, 2, strides]
     return AotNet(**locals(), model_name=kwargs.pop("model_name", "aotnet200"), **kwargs)
 
 
-def AotNetV2(num_blocks, preact=True, strides=1, strides_first=False, **kwargs):
+""" AotNetV2 """
+
+
+def AotNetV2(num_blocks=[3, 4, 6, 3], preact=True, strides=1, strides_first=False, **kwargs):
     strides = strides if isinstance(strides, (list, tuple)) else [2, 2, 2, strides]
     return AotNet(num_blocks, preact=preact, strides=strides, strides_first=strides_first, **kwargs)
 
 
+@register_model
 def AotNet50V2(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", strides=2, **kwargs):
     num_blocks = [3, 4, 6, 3]
     strides = strides if isinstance(strides, (list, tuple)) else [2, 2, 2, strides]
     return AotNetV2(**locals(), model_name=kwargs.pop("model_name", "aotnet50v2"), **kwargs)
 
 
+@register_model
 def AotNet101V2(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", strides=2, **kwargs):
     num_blocks = [3, 4, 23, 3]
     strides = strides if isinstance(strides, (list, tuple)) else [2, 2, 2, strides]
     return AotNetV2(**locals(), model_name=kwargs.pop("model_name", "aotnet101v2"), **kwargs)
 
 
+@register_model
 def AotNet152V2(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", strides=2, **kwargs):
     num_blocks = [3, 8, 36, 3]
     strides = strides if isinstance(strides, (list, tuple)) else [2, 2, 2, strides]
     return AotNetV2(**locals(), model_name=kwargs.pop("model_name", "aotnet152v2"), **kwargs)
 
 
+@register_model
 def AotNet200V2(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", strides=2, **kwargs):
     num_blocks = [3, 24, 36, 3]
     strides = strides if isinstance(strides, (list, tuple)) else [2, 2, 2, strides]
     return AotNetV2(**locals(), model_name=kwargs.pop("model_name", "aotnet200v2"), **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/attention_layers/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/attention_layers/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 
 from keras_cv_attention_models.common_layers import (
     activation_by_name,
     anti_alias_downsample,
     batchnorm_with_activation,
     conv2d_no_bias,
     CompatibleExtractPatches,
+    dense_no_bias,
     depthwise_conv2d_no_bias,
     drop_block,
     drop_connect_rates_split,
     EvoNormalization,
     eca_module,
     fold_by_conv2d_transpose,
     global_context_module,
@@ -16,48 +17,59 @@
     hard_swish,
     layer_norm,
     make_divisible,
     output_block,
     se_module,
     addaptive_pooling_2d,
     PreprocessInput,
-    imagenet_decode_predictions,
     add_pre_post_process,
 )
 from keras_cv_attention_models.aotnet.aotnet import aot_stack, aot_block, deep_stem, quad_stem, tiered_stem
-from keras_cv_attention_models.botnet.botnet import RelativePositionalEmbedding, mhsa_with_relative_position_embedding
-from keras_cv_attention_models.cotnet.cotnet import cot_attention
-from keras_cv_attention_models.coat.coat import ConvPositionalEncoding, ConvRelativePositionalEncoding
 from keras_cv_attention_models.efficientnet.efficientnet_v2 import inverted_residual_block
-from keras_cv_attention_models.halonet.halonet import halo_attention
 from keras_cv_attention_models.resnest.resnest import rsoftmax, split_attention_conv2d
-from keras_cv_attention_models.volo.volo import outlook_attention, outlook_attention_simple, BiasLayer, PositionalEmbedding, ClassToken, MixupToken
 from keras_cv_attention_models.mlp_family.mlp_mixer import mlp_block, mlp_mixer_block
 from keras_cv_attention_models.mlp_family.res_mlp import ChannelAffine
 from keras_cv_attention_models.mlp_family.gated_mlp import spatial_gating_block
 from keras_cv_attention_models.mlp_family.wave_mlp import phase_aware_token_mixing
-from keras_cv_attention_models.levit.levit import (
-    MultiHeadPositionalEmbedding,
-    mhsa_with_multi_head_position,
-    mhsa_with_multi_head_position_and_strides,
+from keras_cv_attention_models.volo.volo import outlook_attention, outlook_attention_simple, BiasLayer, PositionalEmbedding, ClassToken, MixupToken
+from keras_cv_attention_models.gpt2.gpt2 import PositionalIndex, CausalMask
+from keras_cv_attention_models.llama2.llama2 import PositionalEncodingFourierRot1D, RMSNorm
+from keras_cv_attention_models.beit.beit import (
+    PositionalEncodingFourierRot,
+    MultiHeadRelativePositionalEmbedding,
+    HeadInitializer,
+    PatchConv2DWithResampleWeights,
+    qkv_to_multi_head_channels_last_format,
     scaled_dot_product_attention,
 )
+from keras_cv_attention_models.botnet.botnet import RelativePositionalEmbedding, mhsa_with_relative_position_embedding
+from keras_cv_attention_models.cotnet.cotnet import cot_attention
+from keras_cv_attention_models.coat.coat import ConvPositionalEncoding, ConvRelativePositionalEncoding
+from keras_cv_attention_models.halonet.halonet import halo_attention
+from keras_cv_attention_models.convnext.convnext import global_response_normalize, add_with_layer_scale_and_drop_block
+from keras_cv_attention_models.levit.levit import MultiHeadPositionalEmbedding, mhsa_with_multi_head_position, mhsa_with_multi_head_position_and_strides
 from keras_cv_attention_models.nat.nat import MultiHeadRelativePositionalKernelBias, neighborhood_attention
 from keras_cv_attention_models.nfnets.nfnets import ScaledStandardizedConv2D, ZeroInitGain
-from keras_cv_attention_models.beit.beit import MultiHeadRelativePositionalEmbedding, HeadInitializer, PatchConv2DWithResampleWeights
 from keras_cv_attention_models.coatnet.coatnet import mhsa_with_multi_head_relative_position_embedding
-from keras_cv_attention_models.convnext.convnext import global_response_normalize, add_with_layer_scale_and_drop_block
 from keras_cv_attention_models.cmt.cmt import light_mhsa_with_multi_head_relative_position_embedding, BiasPositionalEmbedding
 from keras_cv_attention_models.uniformer.uniformer import multi_head_self_attention
-from keras_cv_attention_models.davit.davit import multi_head_self_attention_channel, window_attention
+from keras_cv_attention_models.davit.davit import (
+    multi_head_self_attention_channel,
+    window_attention,
+    window_partition,
+    window_reverse,
+    pad_to_divisible_by_window_size,
+    reverse_padded_for_window_size,
+)
 from keras_cv_attention_models.edgenext.edgenext import PositionalEncodingFourier, cross_covariance_attention
+from keras_cv_attention_models.efficientvit.efficientvit_m import cascaded_mhsa_with_multi_head_position
 from keras_cv_attention_models.mobilevit.mobilevit import linear_self_attention
 from keras_cv_attention_models.swin_transformer_v2.swin_transformer_v2 import (
     ExpLogitScale,
-    PairWiseRelativePositionalEmbedding,
+    MlpPairwisePositionalEmbedding,
     shifted_window_attention,
     WindowAttentionMask,
     window_mhsa_with_pair_wise_positional_embedding,
 )
 from keras_cv_attention_models.hornet.hornet import ComplexDense, global_local_filter, gnconv
 from keras_cv_attention_models.gpvit.gpvit import PureWeigths
 from keras_cv_attention_models.pvt.pvt import mlp_block_with_depthwise_conv
@@ -70,15 +82,15 @@
 output (compressed=True): `[batch, height // strides,  width // strides, height_kernel * width_kernel * channel]`.
 output (compressed=False): `[batch, height // strides,  width // strides, height_kernel, width_kernel, channel]`.
 
 Args:
   sizes: could be `tf.image.extract_patches` sizes format `[1, 3, 3, 1]`, or `Conv2D` kernel_size format `3`.
   strides: could be `tf.image.extract_patches` strides format `[1, 2, 2, 1]`, or `Conv2D` strides format `2`.
   rates: could be `tf.image.extract_patches` rates format `[1, 1, 1, 1]`, or `Conv2D` dilation_rate format `1`.
-  padding: "VALID" or "SAME", will perform padding in PyTorch way if "SAME".
+  padding: "valid" or "same", will perform padding in PyTorch way if "same".
   compressed: boolean value if compress extracted `height_kernel`, `width_kernel`, `channel` into 1 dimension.
   force_conv: force using `Conv2D` instead of `tf.image.extract_patches`.
 
 Examples:
 
 >>> from keras_cv_attention_models import attention_layers
 >>> aa = tf.ones([1, 64, 27, 192])
@@ -88,15 +100,15 @@
 # (1, 32, 14, 3, 3, 192)
 
 # `Conv2D` version Performs slower than `extract_patches`.
 >>> cc = attention_layers.CompatibleExtractPatches(sizes=3, strides=2, force_conv=True)
 >>> cc(aa).shape  # init run
 >>> %timeit cc(aa)
 # 772 s  6.71 s per loop (mean  std. dev. of 7 runs, 1000 loops each)
->>> %timeit tf.image.extract_patches(aa, [1, 3, 3, 1], [1, 2, 2, 1], [1, 1, 1, 1], padding='SAME')
+>>> %timeit tf.image.extract_patches(aa, [1, 3, 3, 1], [1, 2, 2, 1], [1, 1, 1, 1], padding='same')
 # 108 s  153 ns per loop (mean  std. dev. of 7 runs, 10000 loops each)
 """
 
 fold_by_conv2d_transpose.__doc__ = """
 Fold function like `torch.nn.Fold` using `Conv2DTranspose`.
 
 input (compressed=True): `[batch, height // strides,  width // strides, height_kernel * width_kernel * channel]`.
@@ -106,15 +118,15 @@
 Args:
   patches: input tensor.
   output_shape: specific output shape in format `(height, width)`.
       Default `None` will just cut `padded` by `out[:, paded:-paded, paded:-paded, :]`.
   kernel_size: same as `Conv2DTranspose` kernel_size. Default `3`.
   strides: same as `Conv2DTranspose` strides. Default `2`.
   dilation_rate: same as `Conv2DTranspose` dilation_rate. Default `1`.
-  padding: "VALID" or "SAME", indicates if `patches` is generated from a padded input. Default "SAME".
+  padding: "valid" or "same", indicates if `patches` is generated from a padded input. Default "same".
   compressed: boolean value if `patches` last dimension is a compressed of `height_kernel`, `width_kernel`, `channel`.
       Default "auto" means auto judge from `patches` shape length.
 
 Examples:
 
 >>> inputs = np.random.uniform(size=[1, 64, 27, 192]).astype("float32")
 >>> kernel_size, strides = 3, 2
@@ -133,15 +145,15 @@
 >>> tf_fold = attention_layers.fold_by_conv2d_transpose(tf_patches, output_shape=inputs.shape[1:], kernel_size=kernel_size, strides=strides)
 >>> print(f"{np.allclose(tf_fold, torch_fold.permute([0, 2, 3, 1])) = }")
 # np.allclose(tf_fold, torch_fold.permute([0, 2, 3, 1])) = True
 
 >>> # ==== TF extract_patches ====
 >>> pad = kernel_size // 2
 >>> pad_inputs = tf.pad(inputs, [[0, 0], [pad, pad], [pad, pad], [0, 0]])
->>> patches = tf.image.extract_patches(pad_inputs, [1, kernel_size, kernel_size, 1], [1, strides, strides, 1], [1, 1, 1, 1], padding='VALID')
+>>> patches = tf.image.extract_patches(pad_inputs, [1, kernel_size, kernel_size, 1], [1, strides, strides, 1], [1, 1, 1, 1], padding='valid')
 >>> print(f"{np.allclose(tf_patches, patches) = }")
 # np.allclose(tf_patches, patches) = True
 """
 
 EvoNormalization.__doc__ = """
 Keras implementation of [evonorm](https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py).
 Paper [PDF 2004.02967 Evolving Normalization-Activation Layers](https://arxiv.org/pdf/2004.02967.pdf).
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/beit/eva.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/beit/eva.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 from keras_cv_attention_models.beit.beit import Beit, keras_model_load_weights_from_pytorch_model
+from keras_cv_attention_models.models import register_model
 
 
-def EvaLargePatch14(input_shape=(196, 196, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
+def EVA(layer_scale=0, use_abs_pos_emb=True, model_name="eva", **kwargs):
+    kwargs.pop("kwargs", None)
     patch_size = kwargs.pop("patch_size", 14)
+    force_reload_mismatch = patch_size != 14  # If patch_size not 14, force reload pos_emb and stem_conv weights
+    return Beit(**locals(), **kwargs)
+
+
+@register_model
+def EvaLargePatch14(input_shape=(196, 196, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
     embed_dim = 1024
     depth = 24
     num_heads = 16
-    gamma_init_value = 0
-    use_abs_pos_emb = True
     attn_qkv_bias = True
-    force_reload_mismatch = patch_size != 14  # If patch_size not 14, force reload pos_emb and stem_conv weights
-    return Beit(**locals(), model_name="eva_large_patch14", **kwargs)
+    return EVA(**locals(), model_name="eva_large_patch14", **kwargs)
 
 
+@register_model
 def EvaGiantPatch14(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
-    patch_size = kwargs.pop("patch_size", 14)
     mlp_ratio = 6144 / 1408
     embed_dim = 1408
     depth = 40
     num_heads = 16
-    gamma_init_value = 0
-    use_abs_pos_emb = True
-    force_reload_mismatch = patch_size != 14  # If patch_size not 14, force reload pos_emb and stem_conv weights
-    return Beit(**locals(), model_name="eva_giant_patch14", **kwargs)
+    return EVA(**locals(), model_name="eva_giant_patch14", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/botnet/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/botnet/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -97,41 +97,21 @@
 
 Args:
   position_height: positional embedding height. Default `0` for using `input_shape[2]`.
       Should be larger than `input_shape[2]`.
   position_width: positional embedding width. Default `0` for using `input_shape[3]` or `position_height` if set.
       Should be larger than `input_shape[3]`.
   use_absolute_pos: Set `True` to use absolute positional embeddings.
-  dynamic_shape: Set `True` for dynamically change output shape depending on inputs shape.
-      - Works only if coming inputs shape is smaller than orignal initialized `position_height` and `position_width`.
-      - For larger inputs, please reload layer weights by `self.load_resized_weights`.
 
 Examples:
 
 >>> from keras_cv_attention_models import attention_layers
 >>> aa = attention_layers.RelativePositionalEmbedding()
 >>> print(f"{aa(tf.ones([1, 4, 14, 16, 32])).shape = }")
 aa(tf.ones([1, 4, 14, 16, 32])).shape = TensorShape([1, 4, 14, 16, 14, 16])
 >>> print(f"{aa(tf.ones([1, 4, 4, 6, 32])).shape = }")  # last 2 dimension in output is `[position_height, position_width]`
 aa(tf.ones([1, 4, 4, 6, 32])).shape = TensorShape([1, 4, 4, 6, 14, 16])
 
 >>> print({ii.name:ii.shape for ii in aa.weights})
 {'relative_positional_embedding_6/r_height:0': TensorShape([32, 27]),
  'relative_positional_embedding_6/r_width:0': TensorShape([32, 31])}
-
-For `dynamic_shape=True`:
->>> aa = attention_layers.RelativePositionalEmbedding(dynamic_shape=True)
->>> print(f"{aa(tf.ones([1, 4, 14, 16, 32])).shape = }")
-aa(tf.ones([1, 4, 14, 16, 32])).shape = TensorShape([1, 4, 14, 16, 14, 16])
->>> print(f"{aa(tf.ones([1, 4, 4, 6, 32])).shape = }")  # last 2 dimension in output is `[height, width]`
-aa(tf.ones([1, 4, 4, 6, 32])).shape = TensorShape([1, 4, 4, 6, 4, 6])
-
-Reload layer weights by `self.load_resized_weights`:
->>> bb = attention_layers.RelativePositionalEmbedding(dynamic_shape=True)
->>> bb.build([None, 4, 24, 26, 32])
->>> print({ii.name:ii.shape for ii in bb.weights})
-{'r_height:0': TensorShape([32, 47]),
- 'r_width:0': TensorShape([32, 51])}
->>> bb.load_resized_weights(aa)
->>> print(f"{bb(tf.ones([1, 4, 21, 22, 32])).shape = }")
-bb(tf.ones([1, 4, 21, 22, 32])).shape = TensorShape([1, 4, 21, 22, 21, 22])
 """
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/botnet/botnet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/botnet/botnet.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,246 +1,243 @@
-"""
-A Keras version of `botnet`.
-Original TensorFlow version: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2
-"""
-
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, initializers, image_data_format
 from keras_cv_attention_models.aotnet import AotNet
-from keras_cv_attention_models.attention_layers import conv2d_no_bias
+from keras_cv_attention_models.models import register_model
+from keras_cv_attention_models.attention_layers import conv2d_no_bias, scaled_dot_product_attention, qkv_to_multi_head_channels_last_format
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 BATCH_NORM_DECAY = 0.9
 BATCH_NORM_EPSILON = 1e-5
 
 PRETRAINED_DICT = {
     # "botnet50": {"imagenet": "b221b45ca316166fc858fda1cf4fd946"},
     "botnet26t": {"imagenet": {256: "6d7a9548f866b4971ca2c9d17dd815fc"}},
     "botnext_eca26t": {"imagenet": {256: "170b9b4d7fba88dbcb41716047c047b9"}},
     "botnet_se33t": {"imagenet": {256: "f612743ec59d430f197bc38b3a7f8837"}},
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="botnet")
-class RelativePositionalEmbedding(keras.layers.Layer):
+@backend.register_keras_serializable(package="botnet")
+class RelativePositionalEmbedding(layers.Layer):
     def __init__(self, position_height=0, position_width=0, use_absolute_pos=False, dynamic_shape=False, **kwargs):
-        super(RelativePositionalEmbedding, self).__init__(**kwargs)
+        super().__init__(**kwargs)
         self.position_height = position_height
         self.position_width = position_width if position_width > 0 else position_height
         self.use_absolute_pos = use_absolute_pos
-        self.dynamic_shape = dynamic_shape
+        self.dynamic_shape = dynamic_shape  # Deprecated
 
     def build(self, input_shape):
         _, num_heads, height, width, key_dim = input_shape
         self.position_height = self.position_height if self.position_height > height else height
         self.position_width = self.position_width if self.position_width > width else width
         self.key_dim = key_dim
         stddev = key_dim**-0.5
+        self.num_heads, self.input_height, self.input_width = num_heads, height, width
 
         if self.use_absolute_pos:
             hh_shape = (key_dim, self.position_height)
             ww_shape = (key_dim, self.position_width)
         else:
             hh_shape = (key_dim, 2 * self.position_height - 1)
             ww_shape = (key_dim, 2 * self.position_width - 1)
 
-        initializer = tf.random_normal_initializer(stddev=stddev)
+        initializer = initializers.random_normal(stddev=stddev)
         self.pos_emb_h = self.add_weight(name="r_height", shape=hh_shape, initializer=initializer, trainable=True)
         self.pos_emb_w = self.add_weight(name="r_width", shape=ww_shape, initializer=initializer, trainable=True)
-        self.input_height, self.input_width = height, width
+
+        super().build(input_shape)
 
     def get_config(self):
         base_config = super(RelativePositionalEmbedding, self).get_config()
         base_config.update(
             {
                 "position_height": self.position_height,
                 "position_width": self.position_width,
                 "use_absolute_pos": self.use_absolute_pos,
                 "dynamic_shape": self.dynamic_shape,
             }
         )
         return base_config
 
-    def rel_to_abs(self, rel_pos):
+    def rel_to_abs(self, rel_pos, is_height):
         """
         Converts relative indexing to absolute.
         Input: [bs+heads, height, width, 2 * pos_dim - 1]
         Output: [bs+heads, height, width, pos_dim]
         """
-        bs_heads, hh, ww, dim = rel_pos.shape  # [bs+heads, height, width, 2 * width - 1]
-        pos_dim = (dim + 1) // 2
+        pos_dim = self.position_height if is_height else self.position_width  # Use static values
+        num_blocks = self.input_height if is_height else self.input_width  # Use static values
+
+        # pos_dim = (dim + 1) // 2
         if pos_dim == 1:
             return rel_pos
-        if ww == 1:
+        if num_blocks == 1:
             return rel_pos[:, :, :, -pos_dim:]
-        full_rank_gap = pos_dim - ww
+        _, hh, ww, dim = rel_pos.shape  # [bs+heads, height, width, 2 * width - 1]
+        full_rank_gap = pos_dim - num_blocks
         # [bs+heads, height, width * (2 * pos_dim - 1)] --> [bs+heads, height, width * (2 * pos_dim - 1) - width]
-        flat_x = tf.reshape(rel_pos, [-1, hh, ww * dim])[:, :, ww - 1 : -1]
+        flat_x = functional.reshape(rel_pos, [-1, hh, ww * dim])[:, :, ww - 1 : -1]
         # [bs+heads, height, width, 2 * (pos_dim - 1)] --> [bs+heads, height, width, pos_dim]
         # print(f">>>> {full_rank_gap = }, {flat_x.shape = }")
-        return tf.reshape(flat_x, [-1, hh, ww, 2 * (pos_dim - 1)])[:, :, :, full_rank_gap : pos_dim + full_rank_gap]
+        return functional.reshape(flat_x, [-1, hh, ww, 2 * (pos_dim - 1)])[:, :, :, full_rank_gap : pos_dim + full_rank_gap]
 
     def relative_logits(self, inputs):
-        bs, heads, hh, ww, cc = inputs.shape  # e.g.: [1, 4, 14, 16, 128]
-        inputs = tf.reshape(inputs, [-1, hh, ww, cc])  # Merge bs and heads, for supporting TFLite conversion
-        rel_logits_w = tf.matmul(inputs, self.pos_emb_w)  # [4, 14, 16, 31], 2 * 16 - 1 == 31
-        rel_logits_w = self.rel_to_abs(rel_logits_w)  # [4, 14, 16, 16]
-
-        query_h = tf.transpose(inputs, [0, 2, 1, 3])  # [4, 16, 14, 128], [bs+heads, ww, hh, dims], Exchange `ww` and `hh`
-        rel_logits_h = tf.matmul(query_h, self.pos_emb_h)  # [4, 16, 14, 27], 2 * 14 - 1 == 27
-        rel_logits_h = self.rel_to_abs(rel_logits_h)  # [4, 16, 14, 14]
-        rel_logits_h = tf.transpose(rel_logits_h, [0, 2, 1, 3])  # [4, 14, 16, 14], transpose back
+        # bs, heads, hh, ww, cc = inputs.shape  # e.g.: [1, 4, 14, 16, 128]
+        inputs = functional.reshape(inputs, [-1, self.input_height, self.input_width, self.key_dim])  # Merge bs and heads, for supporting TFLite conversion
+        rel_logits_w = functional.matmul(inputs, self.pos_emb_w)  # [4, 14, 16, 31], 2 * 16 - 1 == 31
+        rel_logits_w = self.rel_to_abs(rel_logits_w, is_height=False)  # [4, 14, 16, 16]
+
+        query_h = functional.transpose(inputs, [0, 2, 1, 3])  # [4, 16, 14, 128], [bs+heads, ww, hh, dims], Exchange `ww` and `hh`
+        rel_logits_h = functional.matmul(query_h, self.pos_emb_h)  # [4, 16, 14, 27], 2 * 14 - 1 == 27
+        rel_logits_h = self.rel_to_abs(rel_logits_h, is_height=True)  # [4, 16, 14, 14]
+        rel_logits_h = functional.transpose(rel_logits_h, [0, 2, 1, 3])  # [4, 14, 16, 14], transpose back
 
-        logits = tf.expand_dims(rel_logits_w, axis=-2) + tf.expand_dims(rel_logits_h, axis=-1)  # [4, 14, 16, 14, 16]
-        return tf.reshape(logits, [-1, heads, hh, ww, self.position_height, self.position_width])  # [1, 4, 14, 16, 14, 16]
+        logits = functional.expand_dims(rel_logits_w, axis=-2) + functional.expand_dims(rel_logits_h, axis=-1)  # [4, 14, 16, 14, 16]
+        return functional.reshape(logits, [-1, self.num_heads, self.input_height, self.input_width, self.position_height, self.position_width])
 
     def absolute_logits(self, inputs):
         # pos_emb = tf.expand_dims(self.pos_emb_w, -2) + tf.expand_dims(self.pos_emb_h, -1)
         # return tf.einsum("bxyhd,dpq->bhxypq", inputs, pos_emb)
-        rel_logits_w = tf.matmul(inputs, self.pos_emb_w)
-        rel_logits_h = tf.matmul(inputs, self.pos_emb_h)
-        return tf.expand_dims(rel_logits_w, axis=-2) + tf.expand_dims(rel_logits_h, axis=-1)
+        rel_logits_w = functional.matmul(inputs, self.pos_emb_w)
+        rel_logits_h = functional.matmul(inputs, self.pos_emb_h)
+        return functional.expand_dims(rel_logits_w, axis=-2) + functional.expand_dims(rel_logits_h, axis=-1)
 
     def call(self, inputs):
         pos_emb = self.absolute_logits(inputs) if self.use_absolute_pos else self.relative_logits(inputs)
         if self.dynamic_shape:
             _, _, hh, ww, _ = inputs.shape
             if hh < self.position_height or ww < self.position_width:
                 pos_emb = pos_emb[:, :, :, :, :hh, :ww]
         return pos_emb
 
-    def load_resized_weights(self, source_layer, method="nearest"):
+    def load_resized_weights(self, source_layer, method="bilinear"):
         # For input 224 --> [128, 27], convert to 480 --> [128, 30]
         if isinstance(source_layer, dict):
-            source_pos_emb_h = source_layer["r_height:0"]  # weights
-            source_pos_emb_w = source_layer["r_width:0"]  # weights
+            source_pos_emb_h, source_pos_emb_w = list(source_layer.values())
         else:
-            source_pos_emb_h = source_layer.pos_emb_h  # layer
-            source_pos_emb_w = source_layer.pos_emb_w  # layer
-        image_like_w = tf.expand_dims(tf.transpose(source_pos_emb_w, [1, 0]), 0)
-        resize_w = tf.image.resize(image_like_w, (1, self.pos_emb_w.shape[1]), method=method)[0]
-        self.pos_emb_w.assign(tf.transpose(resize_w, [1, 0]))
-
-        image_like_h = tf.expand_dims(tf.transpose(source_pos_emb_h, [1, 0]), 0)
-        resize_h = tf.image.resize(image_like_h, (1, self.pos_emb_h.shape[1]), method=method)[0]
-        self.pos_emb_h.assign(tf.transpose(resize_h, [1, 0]))
+            source_pos_emb_h, source_pos_emb_w = source_layer.pos_emb_h, source_layer.pos_emb_w  # layer
+        source_pos_emb_h = np.array(source_pos_emb_h.detach() if hasattr(source_pos_emb_h, "detach") else source_pos_emb_h).astype("float32")
+        source_pos_emb_w = np.array(source_pos_emb_w.detach() if hasattr(source_pos_emb_w, "detach") else source_pos_emb_w).astype("float32")
+
+        image_like_h = np.expand_dims(np.transpose(source_pos_emb_h, [1, 0]), 0)
+        resize_h = backend.numpy_image_resize(image_like_h, target_shape=(1, self.pos_emb_h.shape[1]), method=method)[0]
+        resize_h = np.transpose(resize_h, [1, 0])
+
+        image_like_w = np.expand_dims(np.transpose(source_pos_emb_w, [1, 0]), 0)
+        resize_w = backend.numpy_image_resize(image_like_w, target_shape=(1, self.pos_emb_w.shape[1]), method=method)[0]
+        resize_w = np.transpose(resize_w, [1, 0])
+
+        self.set_weights([resize_h, resize_w])
 
     def show_pos_emb(self, base_size=4):
         import matplotlib.pyplot as plt
 
+        pos_emb_h = self.pos_emb_h.detach() if hasattr(self.pos_emb_h, "detach") else self.pos_emb_h
+        pos_emb_h = self.pos_emb_h.numpy() if hasattr(self.pos_emb_h, "numpy") else np.array(self.pos_emb_h)
+        pos_emb_w = self.pos_emb_w.detach() if hasattr(self.pos_emb_w, "detach") else self.pos_emb_w
+        pos_emb_w = self.pos_emb_w.numpy() if hasattr(self.pos_emb_w, "numpy") else np.array(self.pos_emb_w)
+
         fig, axes = plt.subplots(1, 3, figsize=(base_size * 3, base_size * 1))
-        axes[0].imshow(self.pos_emb_h)
-        axes[1].imshow(self.pos_emb_w)
-        hh_sum = tf.ones([1, self.pos_emb_h.shape[0]]) @ self.pos_emb_h
-        ww_sum = tf.ones([1, self.pos_emb_w.shape[0]]) @ self.pos_emb_w
-        axes[2].imshow(tf.transpose(hh_sum) + ww_sum)
+        axes[0].imshow(pos_emb_h)
+        axes[1].imshow(pos_emb_w)
+        hh_sum = np.ones([1, pos_emb_h.shape[0]]) @ pos_emb_h
+        ww_sum = np.ones([1, pos_emb_w.shape[0]]) @ pos_emb_w
+        axes[2].imshow(np.transpose(hh_sum) + ww_sum)
         titles = ["pos_emb_h", "pos_emb_w", "sum"]
         for ax, title in zip(axes.flatten(), titles):
             ax.set_title(title)
             ax.set_axis_off()
         fig.tight_layout()
         return fig
 
 
 def mhsa_with_relative_position_embedding(
-    inputs, num_heads=4, key_dim=0, relative=True, out_shape=None, out_weight=True, out_bias=False, attn_dropout=0, name=None
+    inputs, num_heads=4, key_dim=0, relative=True, out_shape=None, out_weight=True, out_bias=False, attn_dropout=0, data_format=None, name=None
 ):
-    _, hh, ww, cc = inputs.shape
-    key_dim = key_dim if key_dim > 0 else cc // num_heads
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
-    out_shape = cc if out_shape is None or not out_weight else out_shape
+    data_format = image_data_format() if data_format is None else data_format
+    channel_axis = -1 if data_format == "channels_last" else 1
+    input_channels = inputs.shape[channel_axis]
+    hh, ww = inputs.shape[1:-1] if data_format == "channels_last" else inputs.shape[2:]
+
+    key_dim = key_dim if key_dim > 0 else input_channels // num_heads
+    out_shape = input_channels if out_shape is None or not out_weight else out_shape
     qk_out = num_heads * key_dim
     vv_dim = out_shape // num_heads
 
-    # qkv = keras.layers.Dense(emb_dim * 3, use_bias=False, name=name and name + "qkv")(inputs)
+    # qkv = layers.Dense(qk_out * 2 + out_shape, use_bias=False, name=name and name + "qkv")(inputs)
     qkv = conv2d_no_bias(inputs, qk_out * 2 + out_shape, kernel_size=1, name=name and name + "qkv_")
-    qkv = tf.reshape(qkv, [-1, inputs.shape[1] * inputs.shape[2], qkv.shape[-1]])
-    query, key, value = tf.split(qkv, [qk_out, qk_out, out_shape], axis=-1)
-    # query = [batch, num_heads, hh * ww, key_dim]
-    query = tf.transpose(tf.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 1, 3])
-    # key = [batch, num_heads, key_dim, hh * ww]
-    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])
-    # value = [batch, num_heads, hh * ww, vv_dim]
-    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, vv_dim]), [0, 2, 1, 3])
-
-    # query *= qk_scale
-    # [batch, num_heads, hh * ww, hh * ww]
-    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale
+    # qkv = functional.reshape(qkv, [-1, inputs.shape[1] * inputs.shape[2], qkv.shape[-1]])
+    query, key, value = functional.split(qkv, [qk_out, qk_out, out_shape], axis=channel_axis)
+    query, key, value = qkv_to_multi_head_channels_last_format(query, key, value, num_heads, data_format=data_format)
+
     # pos_query = [batch, num_heads, hh, ww, key_dim]
-    pos_query = tf.reshape(query, [-1, num_heads, inputs.shape[1], inputs.shape[2], key_dim])
+    pos_query = functional.reshape(query, [-1, num_heads, hh, ww, key_dim])
     pos_emb = RelativePositionalEmbedding(use_absolute_pos=not relative, name=name and name + "pos_emb")(pos_query)
-    pos_emb = tf.reshape(pos_emb, [-1, *attention_scores.shape[1:]])
-    attention_scores = keras.layers.Add()([attention_scores, pos_emb])
-    # attention_scores = tf.nn.softmax(attention_scores, axis=-1)
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
-
-    if attn_dropout > 0:
-        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
-    # value = [batch, num_heads, hh * ww, vv_dim]
-    # attention_output = tf.matmul(attention_scores, value)  # [batch, num_heads, hh * ww, vv_dim]
-    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
-    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
-    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * vv_dim])
-    # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
-
-    if out_weight:
-        # [batch, hh, ww, num_heads * vv_dim] * [num_heads * vv_dim, out] --> [batch, hh, ww, out]
-        attention_output = keras.layers.Dense(out_shape, use_bias=out_bias, name=name and name + "output")(attention_output)
-    return attention_output
+
+    output_shape = [hh, ww, out_shape]
+    pos_emb_func = lambda attention_scores: attention_scores + functional.reshape(pos_emb, [-1, *attention_scores.shape[1:]])
+    out = scaled_dot_product_attention(query, key, value, output_shape, pos_emb_func, out_weight, out_bias, dropout=attn_dropout, name=name)
+    return out if data_format == "channels_last" else layers.Permute([3, 1, 2])(out)
 
 
 def BotNet(input_shape=(224, 224, 3), strides=1, pretrained="imagenet", **kwargs):
     attn_types = [None, None, None, "bot"]
     attn_params = {"num_heads": 4, "out_weight": False}
     strides = strides if isinstance(strides, (list, tuple)) else [1, 2, 2, strides]
 
     model = AotNet(input_shape=input_shape, attn_types=attn_types, attn_params=attn_params, strides=strides, **kwargs)
     reload_model_weights(model, PRETRAINED_DICT, "botnet", pretrained, RelativePositionalEmbedding)
     return model
 
 
+@register_model
 def BotNet50(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", strides=1, **kwargs):
     num_blocks = [3, 4, 6, 3]
     return BotNet(**locals(), model_name="botnet50", **kwargs)
 
 
+@register_model
 def BotNet101(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained=None, strides=1, **kwargs):
     num_blocks = [3, 4, 23, 3]
     return BotNet(**locals(), model_name="botnet101", **kwargs)
 
 
+@register_model
 def BotNet152(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained=None, strides=1, **kwargs):
     num_blocks = [3, 8, 36, 3]
     return BotNet(**locals(), model_name="botnet152", **kwargs)
 
 
+@register_model
 def BotNet26T(input_shape=(256, 256, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 2, 2]
     attn_types = [None, None, [None, "bot"], "bot"]
     attn_params = {"num_heads": 4, "out_weight": False}
     stem_type = "tiered"
 
     model = AotNet(model_name="botnet26t", **locals(), **kwargs)
     reload_model_weights(model, PRETRAINED_DICT, "botnet", pretrained, RelativePositionalEmbedding)
     return model
 
 
+@register_model
 def BotNextECA26T(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 2, 2]
     attn_types = [None, None, [None, "bot"], "bot"]
     attn_params = {"num_heads": 4, "key_dim": 16, "out_weight": False}
     use_eca = True
     group_size = 16
     stem_type = "tiered"
     model = AotNet(model_name="botnext_eca26t", **locals(), **kwargs)
     reload_model_weights(model, PRETRAINED_DICT, "botnet", pretrained, RelativePositionalEmbedding)
     return model
 
 
+@register_model
 def BotNetSE33T(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     stem_type = "tiered"
     stem_last_strides = 2
     stem_downsample = False
     out_channels = [256, 512, 1024, 1536]
     hidden_channel_ratio = [1 / 4, 1 / 4, 1 / 4, 1 / 3]
     num_blocks = [2, 3, 3, 2]
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/caformer/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/caformer/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/caformer/caformer.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/caformer/caformer.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,10 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     add_with_layer_scale_and_drop_block,
     conv2d_no_bias,
     layer_norm,
     mlp_block,
     mlp_block_with_depthwise_conv,
@@ -48,29 +49,34 @@
         "imagenet": {224: "c96d0f4720c36ae19ab8eee02c6e6034", 384: "109236ff75aabe1885ef625c3bfe756c"},
         "imagenet21k-ft1k": {224: "64774d61660d2e95df8d274785f7708a", 384: "b7e350d127a37ddfa7cd611ce21b4e4b"},
     },
 }
 
 
 def meta_former_block(inputs, use_attn=False, head_dim=32, mlp_ratio=4, layer_scale=0, residual_scale=0, drop_rate=0, activation="star_relu", name=""):
+    # channnel_axis = -1 if image_data_format() == "channels_last" else 1
     input_channel = inputs.shape[-1]
 
     """ attention """
-    nn = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, center=False, name=name + "attn_")
+    nn = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, center=False, axis=-1, name=name + "attn_")
     # nn = conv_pool_attention_mixer(nn, num_heads, num_attn_low_heads=num_attn_low_heads, pool_size=pool_size, activation=activation, name=name + "attn_")
     if use_attn:
         nn = multi_head_self_attention(nn, num_heads=input_channel // head_dim, name=name + "mhsa_")
     else:
         nn = mlp_block_with_depthwise_conv(nn, input_channel * 2, kernel_size=7, use_bias=False, activation=(activation, None), name=name + "mlp_sep_")
-    attn_out = add_with_layer_scale_and_drop_block(inputs, nn, layer_scale=layer_scale, residual_scale=residual_scale, drop_rate=drop_rate, name=name + "attn_")
+    attn_out = add_with_layer_scale_and_drop_block(
+        inputs, nn, layer_scale=layer_scale, residual_scale=residual_scale, drop_rate=drop_rate, axis=-1, name=name + "attn_"
+    )
 
     """ MLP """
-    nn = layer_norm(attn_out, epsilon=LAYER_NORM_EPSILON, center=False, name=name + "mlp_")
+    nn = layer_norm(attn_out, epsilon=LAYER_NORM_EPSILON, center=False, axis=-1, name=name + "mlp_")
     nn = mlp_block(nn, input_channel * mlp_ratio, use_bias=False, activation=activation, name=name + "mlp_")
-    nn = add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, residual_scale=residual_scale, drop_rate=drop_rate, name=name + "mlp_")
+    nn = add_with_layer_scale_and_drop_block(
+        attn_out, nn, layer_scale=layer_scale, residual_scale=residual_scale, drop_rate=drop_rate, axis=-1, name=name + "mlp_"
+    )
     return nn
 
 
 def CAFormer(
     num_blocks=[3, 3, 9, 3],
     out_channels=[64, 128, 320, 512],
     block_types=["conv", "conv", "transform", "transform"],
@@ -86,95 +92,117 @@
     layer_scales=0,
     residual_scales=[0, 0, 1, 1],
     classifier_activation="softmax",
     pretrained=None,
     model_name="caformer",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
 
     """ Stem """
-    nn = keras.layers.ZeroPadding2D(padding=2, name="stem_")(inputs)  # padding=2
+    nn = layers.ZeroPadding2D(padding=2, name="stem_")(inputs)  # padding=2
     nn = conv2d_no_bias(nn, out_channels[0], kernel_size=7, strides=4, padding="valid", use_bias=True, name="stem_")
-    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, center=False, name="stem_")
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1], name="stem_pre_permute")(nn)
+    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, center=False, axis=-1, name="stem_")
 
     """ stacks """
     total_blocks = sum(num_blocks)
     global_block_id = 0
     for stack_id, (num_block, out_channel, block_type) in enumerate(zip(num_blocks, out_channels, block_types)):
         stack_name = "stack{}_".format(stack_id + 1)
         if stack_id > 0:
-            nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, center=False, name=stack_name + "downsample_")
+            nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, center=False, axis=-1, name=stack_name + "downsample_")
+            nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2], name=stack_name + "permute_pre")(nn)
             nn = conv2d_no_bias(nn, out_channel, 3, strides=2, padding="same", use_bias=True, name=stack_name + "downsample_")
+            nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1], name=stack_name + "permute_post")(nn)
 
         use_attn = True if block_type[0].lower() == "t" else False
+        if use_attn:
+            block_height, block_width = nn.shape[1:-1]
+            nn = functional.reshape(nn, [-1, block_height * block_width, nn.shape[-1]])  # Using 3D for attention inputs
+
         mlp_ratio = mlp_ratios[stack_id] if isinstance(mlp_ratios, (list, tuple)) else mlp_ratios
         layer_scale = layer_scales[stack_id] if isinstance(layer_scales, (list, tuple)) else layer_scales
         residual_scale = residual_scales[stack_id] if isinstance(residual_scales, (list, tuple)) else residual_scales
         for block_id in range(num_block):
             name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             nn = meta_former_block(nn, use_attn, head_dim, mlp_ratio, layer_scale, residual_scale, block_drop_rate, activation=activation, name=name)
             global_block_id += 1
 
+        if use_attn:
+            nn = functional.reshape(nn, [-1, block_height, block_width, nn.shape[-1]])  # Revert 3D to 4D
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2], name="pre_output_permute")(nn)
+
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
         nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="pre_output_")
         if head_filter > 0:
-            nn = keras.layers.Dense(head_filter, use_bias=True, name="feature_dense")(nn)
+            nn = layers.Dense(head_filter, use_bias=True, name="feature_dense")(nn)
             head_filter_activation = head_filter_activation if head_filter_activation is not None else activation
             nn = activation_by_name(nn, activation=head_filter_activation, name="feature_")
             nn = layer_norm(nn, name="feature_")  # epsilon=1e-5
         if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
-    model = tf.keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "caformer", pretrained)
     return model
 
 
+@register_model
 def CAFormerS18(input_shape=(224, 224, 3), num_classes=1000, activation="star_relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     kwargs.pop("kwargs", None)
     return CAFormer(**locals(), model_name=kwargs.pop("model_name", "caformer_s18"), **kwargs)
 
 
+@register_model
 def CAFormerS36(input_shape=(224, 224, 3), num_classes=1000, activation="star_relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 12, 18, 3]
     kwargs.pop("kwargs", None)
     return CAFormer(**locals(), model_name=kwargs.pop("model_name", "caformer_s36"), **kwargs)
 
 
+@register_model
 def CAFormerM36(input_shape=(224, 224, 3), num_classes=1000, activation="star_relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 12, 18, 3]
     out_channels = [96, 192, 384, 576]
     head_filter = out_channels[-1] * 4
     kwargs.pop("kwargs", None)
     return CAFormer(**locals(), model_name=kwargs.pop("model_name", "caformer_m36"), **kwargs)
 
 
+@register_model
 def CAFormerB36(input_shape=(224, 224, 3), num_classes=1000, activation="star_relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 12, 18, 3]
     out_channels = [128, 256, 512, 768]
     head_filter = out_channels[-1] * 4
     kwargs.pop("kwargs", None)
     return CAFormer(**locals(), model_name=kwargs.pop("model_name", "caformer_b36"), **kwargs)
 
 
 """ ConvFormer """
 
 
+@register_model
 def ConvFormerS18(input_shape=(224, 224, 3), num_classes=1000, activation="star_relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return CAFormerS18(**locals(), block_types=["conv", "conv", "conv", "conv"], model_name="convformer_s18", **kwargs)
 
 
+@register_model
 def ConvFormerS36(input_shape=(224, 224, 3), num_classes=1000, activation="star_relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return CAFormerS36(**locals(), block_types=["conv", "conv", "conv", "conv"], model_name="convformer_s36", **kwargs)
 
 
+@register_model
 def ConvFormerM36(input_shape=(224, 224, 3), num_classes=1000, activation="star_relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return CAFormerM36(**locals(), block_types=["conv", "conv", "conv", "conv"], model_name="convformer_m36", **kwargs)
 
 
+@register_model
 def ConvFormerB36(input_shape=(224, 224, 3), num_classes=1000, activation="star_relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return CAFormerB36(**locals(), block_types=["conv", "conv", "conv", "conv"], model_name="convformer_b36", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/cmt/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/cmt/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/cmt/cmt.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/cmt/cmt.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,32 +1,35 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     batchnorm_with_activation,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
     drop_block,
     layer_norm,
     MultiHeadRelativePositionalEmbedding,
+    qkv_to_multi_head_channels_last_format,
+    scaled_dot_product_attention,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
-    "cmt_tiny": {"imagenet": {160: "cb269248643e9d50d8bea051563e20a6", 224: "d0f2f0cf649a7aea48a1a4e3a476606c"}},
-    "cmt_tiny_torch": {"imagenet": {160: "d800c892ef5581d73cbdef5ba61bc443"}},
-    "cmt_xs_torch": {"imagenet": {192: "cb8250ce61d3bcd0d24ace6c4a803f8b"}},
-    "cmt_small_torch": {"imagenet": {224: "2efa6fafb040dc617f6eb8f3cbfd051a"}},
-    "cmt_base_torch": {"imagenet": {256: "7cb17018b0bc73d33892e1fb3c57f82b"}},
+    "cmt_tiny": {"imagenet": {160: "e2f84138c3b994a5722c4b742ad5c62e", 224: "8c1778fe8f9db8e12c58f744a585d747"}},
+    "cmt_tiny_torch": {"imagenet": {160: "7105c2dcbdc08f2074b1ecabfdeaa166"}},
+    "cmt_xs_torch": {"imagenet": {192: "62bab26382b36c4a811bcfc8ba1bc699"}},
+    "cmt_small_torch": {"imagenet": {224: "2afbdf0f3b18d589ffe87cf8f1817c0c"}},
+    "cmt_base_torch": {"imagenet": {256: "2663258907b68c20c7ad0a51f8aed7c4"}},
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="kecam/cmt")
-class BiasPositionalEmbedding(keras.layers.Layer):
+@backend.register_keras_serializable(package="kecam/cmt")
+class BiasPositionalEmbedding(layers.Layer):
     def __init__(self, axis=[1, 2, 3], attn_height=-1, initializer="zeros", **kwargs):
         super().__init__(**kwargs)
         self.axis, self.initializer, self.attn_height = axis, initializer, attn_height
 
     def build(self, input_shape):
         if self.axis == -1 or self.axis == len(input_shape) - 1:
             bb_shape = (input_shape[-1],)
@@ -34,147 +37,128 @@
             bb_shape = [1] * len(input_shape)
             axis = self.axis if isinstance(self.axis, (list, tuple)) else [self.axis]
             for ii in axis:
                 bb_shape[ii] = input_shape[ii]
             bb_shape = bb_shape[1:]  # exclude batch dimension
         self.bb = self.add_weight(name="positional_embedding", shape=bb_shape, initializer=self.initializer, trainable=True)
 
-        self.query_hh = int(tf.math.sqrt(float(input_shape[2]))) if self.attn_height == -1 else self.attn_height
+        self.query_hh = int(float(input_shape[2]) ** 0.5) if self.attn_height == -1 else self.attn_height
         self.query_ww = int(float(input_shape[2]) / self.query_hh)
-        sr_ratio = int(tf.math.sqrt(float(input_shape[2]) / float(input_shape[3])))
+        sr_ratio = int((float(input_shape[2]) / float(input_shape[3])) ** 0.5)
         self.kv_hh = self.query_hh // sr_ratio
         self.kv_ww = int(float(input_shape[3]) / self.kv_hh)
 
         super().build(input_shape)
 
     def call(self, inputs, **kwargs):
         return inputs + self.bb
 
     def get_config(self):
         config = super().get_config()
         config.update({"axis": self.axis, "attn_height": self.attn_height})  # Not saving initializer in config
         return config
 
-    def load_resized_weights(self, source_layer, method="nearest"):
+    def load_resized_weights(self, source_layer, method="bilinear"):
         if isinstance(source_layer, dict):
-            source_tt = source_layer["positional_embedding:0"]  # weights
+            source_tt = list(source_layer.values())[0]  # weights
             # source_tt = source_layer["pos_emb:0"]  # weights
         else:
             source_tt = source_layer.bb  # layer
+        source_tt = np.array(source_tt.detach() if hasattr(source_tt, "detach") else source_tt).astype("float32")
+
         num_heads = source_tt.shape[0]
-        source_query_hh = source_query_ww = int(tf.math.sqrt(float(source_tt.shape[1])))  # assume source weights are all square shape
-        source_kv_hh = source_kv_ww = int(tf.math.sqrt(float(source_tt.shape[2])))  # assume source weights are all square shape
+        source_query_hh = source_query_ww = int(float(source_tt.shape[1]) ** 0.5)  # assume source weights are all square shape
+        source_kv_hh = source_kv_ww = int(float(source_tt.shape[2]) ** 0.5)  # assume source weights are all square shape
 
-        tt = tf.reshape(source_tt, [num_heads, source_query_hh, source_query_ww, source_kv_hh * source_kv_ww])  # resize on query dimension first
-        tt = tf.image.resize(tt, [self.query_hh, self.query_ww], method=method)  # [num_heads, self.query_hh, self.query_ww, source_kv_hh * source_kv_ww]
-        tt = tf.reshape(tt, [num_heads, self.query_hh * self.query_ww, source_kv_hh, source_kv_ww])  # resize on key_value dimension
-        tt = tf.transpose(tt, [0, 2, 3, 1])  # [num_heads, source_kv_hh, source_kv_ww, self.query_hh * self.query_ww]
-        tt = tf.image.resize(tt, [self.kv_hh, self.kv_ww], method=method)  # [num_heads, self.kv_hh, self.kv_ww, self.query_hh * self.query_ww]
-        tt = tf.reshape(tt, [num_heads, self.kv_hh * self.kv_ww, self.query_hh * self.query_ww])
-        tt = tf.transpose(tt, [0, 2, 1])  # [num_heads, self.query_hh * self.query_ww, self.kv_hh * self.kv_ww]
-        self.bb.assign(tt)
+        tt = np.reshape(source_tt, [num_heads, source_query_hh, source_query_ww, source_kv_hh * source_kv_ww])  # resize on query dimension first
+        tt = backend.numpy_image_resize(tt, [self.query_hh, self.query_ww], method=method)  # [num_heads, query_hh, query_ww, source_kv_hh * source_kv_ww]
+        tt = np.reshape(tt, [num_heads, self.query_hh * self.query_ww, source_kv_hh, source_kv_ww])  # resize on key_value dimension
+        tt = np.transpose(tt, [0, 2, 3, 1])  # [num_heads, source_kv_hh, source_kv_ww, query_hh * query_ww]
+
+        tt = backend.numpy_image_resize(tt, [self.kv_hh, self.kv_ww], method=method)  # [num_heads, self.kv_hh, self.kv_ww, self.query_hh * self.query_ww]
+        tt = np.reshape(tt, [num_heads, self.kv_hh * self.kv_ww, self.query_hh * self.query_ww])
+        tt = np.transpose(tt, [0, 2, 1])  # [num_heads, self.query_hh * self.query_ww, self.kv_hh * self.kv_ww]
+        self.set_weights([tt])
 
 
 def light_mhsa_with_multi_head_relative_position_embedding(
     inputs, num_heads=4, key_dim=0, sr_ratio=1, qkv_bias=False, pos_emb=None, use_bn=False, out_shape=None, out_weight=True, out_bias=False, dropout=0, name=""
 ):
-    _, hh, ww, cc = inputs.shape
-    key_dim = key_dim if key_dim > 0 else cc // num_heads
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
-    out_shape = cc if out_shape is None or not out_weight else out_shape
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    input_channel = inputs.shape[channel_axis]
+    height, width = inputs.shape[1:-1] if image_data_format() == "channels_last" else inputs.shape[2:]
+    key_dim = key_dim if key_dim > 0 else input_channel // num_heads
+    out_shape = input_channel if out_shape is None or not out_weight else out_shape
     emb_dim = num_heads * key_dim
 
-    query = keras.layers.Dense(emb_dim, use_bias=qkv_bias, name=name and name + "query")(inputs) * qk_scale
+    # query = layers.Dense(emb_dim, use_bias=qkv_bias, name=name and name + "query")(inputs)
+    query = conv2d_no_bias(inputs, emb_dim, use_bias=qkv_bias, name=name and name + "query_")
     # print(f">>>> {inputs.shape = }, {query.shape = }, {sr_ratio = }")
     # query = [batch, num_heads, hh * ww, key_dim]
-    query = tf.transpose(tf.reshape(query, [-1, inputs.shape[1] * inputs.shape[2], num_heads, key_dim]), [0, 2, 1, 3])
 
     if sr_ratio > 1:
         key_value = depthwise_conv2d_no_bias(inputs, kernel_size=sr_ratio, strides=sr_ratio, use_bias=qkv_bias, name=name + "kv_sr_")
         key_value = batchnorm_with_activation(key_value, activation=None, name=name + "kv_sr_") if use_bn else layer_norm(key_value, name=name + "kv_sr_")
-        # key_value = keras.layers.AvgPool2D(sr_ratio, strides=sr_ratio, name=name + "kv_sr_")(inputs)
+        # key_value = layers.AvgPool2D(sr_ratio, strides=sr_ratio, name=name + "kv_sr_")(inputs)
     else:
         key_value = inputs
     _, kv_hh, kv_ww, _ = key_value.shape
     # key_value = [batch, num_heads, hh, ww, kv_kernel * kv_kernel, key_dim * 2]
-    key_value = keras.layers.Dense(emb_dim * 2, use_bias=qkv_bias, name=name and name + "key_value")(key_value)
-    # key = keras.layers.Dense(emb_dim, use_bias=qkv_bias, name=name and name + "key")(key_value)
-    # value = keras.layers.Dense(emb_dim, use_bias=qkv_bias, name=name and name + "value")(key_value)
-    # key_value = conv2d_no_bias(inputs, emb_dim * 2, kernel_size=sr_ratio, strides=sr_ratio, use_bias=False, name=name + "key_value")
+    # key = layers.Dense(emb_dim, use_bias=qkv_bias, name=name and name + "key")(key_value)
+    # value = layers.Dense(emb_dim, use_bias=qkv_bias, name=name and name + "value")(key_value)
+    key_value = conv2d_no_bias(key_value, emb_dim * 2, use_bias=qkv_bias, name=name and name + "key_value_")
     # print(f">>>> {key_value.shape = }")
 
-    # dim, head, kv
-    key_value = tf.reshape(key_value, [-1, kv_hh * kv_ww, key_dim, num_heads, 2])
-    key = tf.transpose(key_value[:, :, :, :, 0], [0, 3, 2, 1])  # [batch, num_heads, key_dim, hh * ww]
-    value = tf.transpose(key_value[:, :, :, :, 1], [0, 3, 1, 2])  # [batch, num_heads, hh * ww, key_dim]
-    # kv, head, dim
-    # key, value = tf.split(key_value, 2, axis=-1)
-    # key = tf.transpose(tf.reshape(key, [-1, kv_hh * kv_ww, num_heads, key_dim]), [0, 2, 3, 1]) # [batch, num_heads, key_dim, hh * ww]
-    # value = tf.transpose(tf.reshape(value, [-1, kv_hh * kv_ww, num_heads, key_dim]), [0, 2, 1, 3]) # [batch, num_heads, hh * ww, key_dim]
-
-    # print(f">>>> {attn_query.shape = }, {key.shape = }, {value.shape = }, {kv_inp.shape = }, {pos_query.shape = }")
-    # attention_scores = [batch, num_heads, hh * ww, kv_hh * kv_ww]
-    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key])
-    if pos_emb is None:
-        attention_scores = MultiHeadRelativePositionalEmbedding(with_cls_token=False, attn_height=hh, name=name and name + "pos_emb")(attention_scores)
-    else:
-        attention_scores = pos_emb(attention_scores)
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
+    key, value = functional.split(key_value, 2, axis=channel_axis)
+    query, key, value = qkv_to_multi_head_channels_last_format(query, key, value, num_heads=num_heads)
 
-    if dropout > 0:
-        attention_scores = keras.layers.Dropout(dropout, name=name and name + "attn_drop")(attention_scores)
-    # value = [batch, num_heads, key_hh * key_ww, key_dim]
-    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
-    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
-    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * key_dim])
-    # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
-
-    if out_weight:
-        # [batch, hh, ww, num_heads * key_dim] * [num_heads * key_dim, out] --> [batch, hh, ww, out]
-        attention_output = keras.layers.Dense(out_shape, use_bias=out_bias, name=name and name + "output")(attention_output)
-    return attention_output
+    if pos_emb is None:
+        pos_emb = MultiHeadRelativePositionalEmbedding(with_cls_token=False, attn_height=height, name=name and name + "pos_emb")
+    output_shape = (height, width, out_shape)
+    out = scaled_dot_product_attention(query, key, value, output_shape, pos_emb=pos_emb, out_weight=out_weight, out_bias=out_bias, dropout=dropout, name=name)
+    return out if image_data_format() == "channels_last" else layers.Permute([3, 1, 2], name=name and name + "output_perm")(out)
 
 
 def inverted_residual_feed_forward(inputs, expansion=4, activation="gelu", name=""):
     """IRFFN(X) = Conv(F(Conv(X))), F(X) = DWConv(X) + X"""
-    in_channel = inputs.shape[-1]
+    in_channel = inputs.shape[-1 if image_data_format() == "channels_last" else 1]
     expanded = conv2d_no_bias(inputs, int(in_channel * expansion), kernel_size=1, use_bias=True, name=name + "1_")
     expanded = batchnorm_with_activation(expanded, activation=activation, act_first=True, name=name + "1_")
 
-    dw = depthwise_conv2d_no_bias(expanded, kernel_size=3, padding="SAME", use_bias=True, name=name)
-    dw = keras.layers.Add(name=name + "dw_add")([expanded, dw])
+    dw = depthwise_conv2d_no_bias(expanded, kernel_size=3, padding="same", use_bias=True, name=name)
+    dw = layers.Add(name=name + "dw_add")([expanded, dw])
     dw = batchnorm_with_activation(dw, activation=activation, act_first=True, name=name + "2_")
 
     pw = conv2d_no_bias(dw, in_channel, kernel_size=1, use_bias=True, name=name + "3_")
     pw = batchnorm_with_activation(pw, activation=None, name=name + "3_")
     return pw
 
 
 def cmt_block(
     inputs, num_heads=4, sr_ratio=1, expansion=4, qkv_bias=False, pos_emb=None, attn_use_bn=False, attn_out_bias=False, activation="gelu", drop_rate=0, name=""
 ):
     """X0 = LPU(Xi), X1 = LMHSA(LN(X0)) + X0, X2 = IRFFN(LN(X1)) + X1"""
     """ Local Perception Unit, LPU(X) = DWConv(X) + X """
-    lpu = depthwise_conv2d_no_bias(inputs, kernel_size=3, padding="SAME", use_bias=True, name=name)
+    lpu = depthwise_conv2d_no_bias(inputs, kernel_size=3, padding="same", use_bias=True, name=name)
     # lpu = batchnorm_with_activation(lpu, activation=activation, name=name + "lpu_", act_first=True)
-    lpu_out = keras.layers.Add(name=name + "lpu_out")([inputs, lpu])
+    lpu_out = layers.Add(name=name + "lpu_out")([inputs, lpu])
 
     """ light multi head self attention """
     attn = layer_norm(lpu_out, name=name + "attn_")
     attn = light_mhsa_with_multi_head_relative_position_embedding(
         attn, num_heads=num_heads, sr_ratio=sr_ratio, qkv_bias=qkv_bias, pos_emb=pos_emb, use_bn=attn_use_bn, out_bias=attn_out_bias, name=name + "light_mhsa_"
     )
     attn = drop_block(attn, drop_rate=drop_rate)
-    attn_out = keras.layers.Add(name=name + "attn_out")([lpu_out, attn])
+    attn_out = layers.Add(name=name + "attn_out")([lpu_out, attn])
 
     """ inverted residual feed forward """
     ffn = layer_norm(attn_out, name=name + "ffn_")
     ffn = inverted_residual_feed_forward(ffn, expansion=expansion, activation=activation, name=name + "ffn_")
     ffn = drop_block(ffn, drop_rate=drop_rate)
-    ffn_out = keras.layers.Add(name=name + "ffn_output")([attn_out, ffn])
+    ffn_out = layers.Add(name=name + "ffn_output")([attn_out, ffn])
 
     return ffn_out
 
 
 def cmt_stem(inputs, stem_width, activation="gelu", name="", **kwargs):
     nn = conv2d_no_bias(inputs, stem_width, kernel_size=3, strides=2, padding="same", use_bias=True, name=name + "1_")
     nn = batchnorm_with_activation(nn, activation=activation, act_first=True, name=name + "1_")
@@ -205,28 +189,32 @@
     classifier_activation="softmax",
     output_num_features=1280,
     dropout=0,
     pretrained=None,
     model_name="cmt",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
     nn = cmt_stem(inputs, stem_width=stem_width, activation=activation, name="stem_")
 
     """ stage [1, 2, 3, 4] """
     total_blocks = sum(num_blocks)
     global_block_id = 0
     for stack_id, (num_block, out_channel, num_head, sr_ratio) in enumerate(zip(num_blocks, out_channels, num_heads, sr_ratios)):
         stage_name = "stack{}_".format(stack_id + 1)
         nn = conv2d_no_bias(nn, out_channel, kernel_size=2, strides=2, use_bias=True, name=stage_name + "down_sample")
         nn = layer_norm(nn, name=stage_name)
 
         if use_block_pos_emb:
-            block_pos_emb = BiasPositionalEmbedding(axis=[1, 2, 3], attn_height=nn.shape[1], name=stage_name + "pos_emb")
-            block_pos_emb.build([None, num_head, nn.shape[1] * nn.shape[2], (nn.shape[1] // sr_ratio) * (nn.shape[2] // sr_ratio)])
+            height, width = nn.shape[1:-1] if image_data_format() == "channels_last" else nn.shape[2:]
+            block_pos_emb = BiasPositionalEmbedding(axis=[1, 2, 3], attn_height=height, name=stage_name + "pos_emb")
+            block_pos_emb.build([None, num_head, height * width, (height // sr_ratio) * (width // sr_ratio)])
         else:
             block_pos_emb = None
 
         for block_id in range(num_block):
             name = stage_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             global_block_id += 1
@@ -236,62 +224,67 @@
 
     if output_num_features > 0:
         nn = conv2d_no_bias(nn, output_num_features, 1, strides=1, use_bias=True, name="features_")
         feature_activation = activation if feature_activation is None else feature_activation
         nn = batchnorm_with_activation(nn, activation=feature_activation, act_first=feature_act_first, name="features_")
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
         if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     mismatch_class = BiasPositionalEmbedding if use_block_pos_emb else MultiHeadRelativePositionalEmbedding
     reload_model_weights(model, PRETRAINED_DICT, "cmt", pretrained, mismatch_class)
     return model
 
 
 def CMT_torch(qkv_bias=True, attn_out_bias=True, attn_use_bn=True, use_block_pos_emb=True, feature_activation="swish", feature_act_first=False, **kwargs):
     kwargs.pop("kwargs", None)
     return CMT(**locals(), **kwargs)
 
 
+@register_model
 def CMTTiny(input_shape=(160, 160, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 10, 2]
     out_channels = [46, 92, 184, 368]
     stem_width = 16
     ffn_expansion = 3.6
     return CMT(**locals(), model_name="cmt_tiny", **kwargs)
 
 
+@register_model
 def CMTTiny_torch(input_shape=(160, 160, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 10, 2]
     out_channels = [46, 92, 184, 368]
     stem_width = 16
     ffn_expansion = 3.6
     return CMT_torch(**locals(), model_name="cmt_tiny_torch", **kwargs)
 
 
+@register_model
 def CMTXS_torch(input_shape=(192, 192, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 3, 12, 3]
     out_channels = [52, 104, 208, 416]
     stem_width = 16
     ffn_expansion = 3.77
     return CMT_torch(**locals(), model_name="cmt_xs_torch", **kwargs)
 
 
+@register_model
 def CMTSmall_torch(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 3, 16, 3]
     out_channels = [64, 128, 256, 512]
     stem_width = 32
     ffn_expansion = 4
     return CMT_torch(**locals(), model_name="cmt_small_torch", **kwargs)
 
 
+@register_model
 def CMTBase_torch(input_shape=(256, 256, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [4, 4, 20, 4]
     out_channels = [76, 152, 304, 608]
     stem_width = 38
     ffn_expansion = 4
     return CMT_torch(**locals(), model_name="cmt_base_torch", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/coat/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/coat/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -41,66 +41,53 @@
   | CoaTLiteTiny  | 5.7M   | 1.60G | 224   | 77.5     |
   | CoaTLiteMini  | 11M    | 2.00G | 224   | 79.1     |
   | CoaTLiteSmall | 20M    | 3.97G | 224   | 81.9     |
   | CoaTTiny      | 5.5M   | 4.33G | 224   | 78.3     |
   | CoaTMini      | 10M    | 6.78G | 224   | 81.0     |
 """
 
-__default_doc__ = __head_doc__ + """
-[{model_name} architecture] serial_depths: {serial_depths}, embed_dims: {embed_dims}, mlp_ratios: {mlp_ratios},
-                            parallel_depth: {parallel_depth}, patch_size: {patch_size}, num_heads: {num_heads}.
+CoaTLiteTiny.__doc__ = __head_doc__ + """
 Args:
 """ + __tail_doc__
-
-CoaTLiteTiny.__doc__ = __default_doc__.format(model_name="CoaTLiteTiny", **coat.BLOCK_CONFIGS["lite_tiny"])
-CoaTLiteMini.__doc__ = __default_doc__.format(model_name="CoaTLiteMini", **coat.BLOCK_CONFIGS["lite_mini"])
-CoaTLiteSmall.__doc__ = __default_doc__.format(model_name="CoaTLiteSmall", **coat.BLOCK_CONFIGS["lite_small"])
-CoaTTiny.__doc__ = __default_doc__.format(model_name="CoaTTiny", **coat.BLOCK_CONFIGS["tiny"])
-CoaTMini.__doc__ = __default_doc__.format(model_name="CoaTMini", **coat.BLOCK_CONFIGS["mini"])
+CoaTLiteMini.__doc__ = CoaTLiteTiny.__doc__
+CoaTLiteSmall.__doc__ = CoaTLiteTiny.__doc__
+CoaTTiny.__doc__ = CoaTLiteTiny.__doc__
+CoaTMini.__doc__ = CoaTLiteTiny.__doc__
 
 ConvPositionalEncoding.__doc__ = __head_doc__ + """
 Convolutional Position Encoding. Note: This module is similar to the conditional position encoding in CPVT.
 Applies a `DepthwiseConv2D` layer with input, then adds with input.
+Not a layer, just wappered a class for reusable.
 
 input: `[batch, class_token + height * width, channel]`.
 output: `[batch, class_token + height * width, channel]`.
 
 Args:
   kernel_size: `DepthwiseConv2D` kernel size.
 
 Examples:
 >>> from keras_cv_attention_models import attention_layers
 >>> aa = attention_layers.ConvPositionalEncoding()
 >>> print(f"{aa(tf.ones([1, 1 + 14 * 14, 256])).shape = }")
-aa(tf.ones([1, 1 + 14 * 14, 256])).shape = TensorShape([1, 197, 256])
-
->>> print({ii.name:ii.numpy().shape for ii in aa.weights})
-{'conv_positional_encoding/depthwise_kernel:0': (3, 3, 256, 1), 'conv_positional_encoding/bias:0': (256,)}
+# aa(tf.ones([1, 1 + 14 * 14, 256])).shape = TensorShape([1, 197, 256])
 """
 
 ConvRelativePositionalEncoding.__doc__ = __head_doc__ + """
 Convolutional with Relative Position Encoding.
 Applies multi `DepthwiseConv2D` layers with split input, then adds with input.
+Not a layer, just wappered a class for reusable.
 
 input:
     query: `[batch, num_heads, class_token + height * width, channel // num_heads]`.
     value: `[batch, num_heads, class_token + height * width, channel // num_heads]`.
 output: `[batch, num_heads, 1 * zero + height * width, channel // num_heads]`.
 
 Args:
   head_splits: split head list. Should be sum eqauls `num_heads`. Default `[2, 3, 3]` indicates split `8` head into 3 groups.
   head_kernel_size: kernel_size for each split head. Defualt `[3, 5, 7]`.
 
 Examples:
 >>> from keras_cv_attention_models import attention_layers
 >>> aa = attention_layers.ConvRelativePositionalEncoding()
 >>> print(f"{aa(tf.ones([1, 8, 1 + 14 * 14, 6]), tf.ones([1, 8, 1 + 14 * 14, 6])).shape = }")
-aa(tf.ones([1, 8, 1 + 14 * 14, 6]), tf.ones([1, 8, 1 + 14 * 14, 6])).shape = TensorShape([1, 8, 197, 6])
-
->>> print({ii.name:ii.numpy().shape for ii in aa.weights})
-{'conv_relative_positional_encoding/depth_conv_1/depthwise_kernel:0': (3, 3, 12, 1),
- 'conv_relative_positional_encoding/depth_conv_1/bias:0': (12,),
- 'conv_relative_positional_encoding/depth_conv_2/depthwise_kernel:0': (5, 5, 18, 1),
- 'conv_relative_positional_encoding/depth_conv_2/bias:0': (18,),
- 'conv_relative_positional_encoding/depth_conv_3/depthwise_kernel:0': (7, 7, 18, 1),
- 'conv_relative_positional_encoding/depth_conv_3/bias:0': (18,)}
+# aa(tf.ones([1, 8, 1 + 14 * 14, 6]), tf.ones([1, 8, 1 + 14 * 14, 6])).shape = TensorShape([1, 8, 197, 6])
 """
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/coat/coat.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/coat/coat.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,253 +1,228 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.python.keras import backend as K
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
-from keras_cv_attention_models.attention_layers import layer_norm, conv2d_no_bias, activation_by_name, add_pre_post_process
+from keras_cv_attention_models.attention_layers import ClassToken, layer_norm, conv2d_no_bias, activation_by_name, add_pre_post_process
 
 
 PRETRAINED_DICT = {
-    "coat_lite_tiny": {"imagenet": "7738d1efb345d17f7569929330a2cf7d"},
-    "coat_lite_mini": {"imagenet": "9ad2fa037addee382e70c6fac1941a68"},
-    "coat_lite_small": {"imagenet": "0c8012cfba5b1d1b97305770587730ff"},
-    "coat_tiny": {"imagenet": "0b20a82b7f82a3d73cca9fb5b66db8fb"},
-    "coat_mini": {"imagenet": "883a0c3083b82f19f1245572ef068311"},
+    "coat_lite_tiny": {"imagenet": "e45487e7bfb44faac97b1af51f8bbd01"},
+    "coat_lite_mini": {"imagenet": "e5e3f5e4b86765ee75f8bf03973d70a0"},
+    "coat_lite_small": {"imagenet": "eddffc46a64eb0a21b7ecc057f231756"},
+    "coat_tiny": {"imagenet": "6418d9580ad9ea0a6755c77d8d7bad49"},
+    "coat_mini": {"imagenet": "dc284967f6bd32df8e1e03074b2d773d"},
 }
 
 
-def mlp_block(inputs, hidden_dim, activation="gelu", name=None):
-    nn = keras.layers.Dense(hidden_dim, name=name + "dense_0")(inputs)
-    nn = activation_by_name(nn, activation, name=name and name + activation)
-    nn = keras.layers.Dense(inputs.shape[-1], name=name + "dense_1")(nn)
-    return nn
-
-
-@keras.utils.register_keras_serializable(package="coat")
-class ConvPositionalEncoding(keras.layers.Layer):
-    def __init__(self, kernel_size=3, input_height=-1, **kwargs):
-        super(ConvPositionalEncoding, self).__init__(**kwargs)
-        self.kernel_size, self.input_height = kernel_size, input_height
-        self.pad = [[0, 0], [kernel_size // 2, kernel_size // 2], [kernel_size // 2, kernel_size // 2], [0, 0]]
-        self.supports_masking = False
+# Not a layer, just for reusable
+class ConvPositionalEncoding:
+    def __init__(self, kernel_size=3, input_height=-1, name=None):
+        self.kernel_size, self.input_height, self.name = kernel_size, input_height, name
+        if image_data_format() == "channels_last":
+            self.pad = [[0, 0], [kernel_size // 2, kernel_size // 2], [kernel_size // 2, kernel_size // 2], [0, 0]]
+        else:
+            self.pad = [[0, 0], [0, 0], [kernel_size // 2, kernel_size // 2], [kernel_size // 2, kernel_size // 2]]
+        self.built = False
 
     def build(self, input_shape):
-        self.height = self.input_height if self.input_height > 0 else int(tf.math.sqrt(float(input_shape[1] - 1)))
+        self.height = self.input_height if self.input_height > 0 else int(float(input_shape[1] - 1) ** 0.5)
         self.width = (input_shape[1] - 1) // self.height
 
         self.channel = input_shape[-1]
-        # Conv2D with goups=self.channel
-        self.dconv = keras.layers.DepthwiseConv2D(
-            self.kernel_size,
-            strides=1,
-            padding="VALID",
-            name=self.name and self.name + "depth_conv",
-        )
-        self.dconv.build([None, self.height, self.width, self.channel])
-        super(ConvPositionalEncoding, self).build(input_shape)
+        self.dconv = layers.DepthwiseConv2D(self.kernel_size, strides=1, padding="valid", name=self.name and self.name + "depth_conv")
+
+    def __call__(self, inputs, **kwargs):
+        if not self.built:
+            self.build(inputs.shape)
+            self.built = True
 
-    def call(self, inputs, **kwargs):
         cls_token, img_token = inputs[:, :1], inputs[:, 1:]
-        img_token = tf.reshape(img_token, [-1, self.height, self.width, self.channel])
-        nn = self.dconv(tf.pad(img_token, self.pad)) + img_token
-        nn = tf.reshape(nn, [-1, self.height * self.width, self.channel])
-        return tf.concat([cls_token, nn], axis=1)
-
-    def compute_output_shape(self, input_shape):
-        return input_shape
-
-    def get_config(self):
-        base_config = super(ConvPositionalEncoding, self).get_config()
-        base_config.update({"kernel_size": self.kernel_size, "input_height": self.input_height})
-        return base_config
-
-
-@keras.utils.register_keras_serializable(package="coat")
-class ConvRelativePositionalEncoding(keras.layers.Layer):
-    def __init__(self, head_splits=[2, 3, 3], head_kernel_size=[3, 5, 7], input_height=-1, **kwargs):
-        super(ConvRelativePositionalEncoding, self).__init__(**kwargs)
-        self.head_splits, self.head_kernel_size, self.input_height = head_splits, head_kernel_size, input_height
-        self.supports_masking = False
+        img_token = functional.reshape(img_token, [-1, self.height, self.width, self.channel])
+        nn = img_token if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(img_token)
+        # print(f"{nn.shape = }")
+        nn = self.dconv(functional.pad(nn, self.pad))
+        nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)
+        nn = layers.Add()([nn, img_token])
+        nn = functional.reshape(nn, [-1, self.height * self.width, self.channel])
+        return functional.concat([cls_token, nn], axis=1)
+
+
+# Not a layer, just for reusable
+class ConvRelativePositionalEncoding:
+    def __init__(self, head_splits=[2, 3, 3], head_kernel_size=[3, 5, 7], input_height=-1, name=None):
+        self.head_splits, self.head_kernel_size, self.input_height, self.name = head_splits, head_kernel_size, input_height, name
+        self.built = False
 
     def build(self, query_shape):
         # print(query_shape)
-        self.height = self.input_height if self.input_height > 0 else int(tf.math.sqrt(float(query_shape[2] - 1)))
+        self.height = self.input_height if self.input_height > 0 else int(float(query_shape[2] - 1) ** 0.5)
         self.width = (query_shape[2] - 1) // self.height
         self.num_heads, self.query_dim = query_shape[1], query_shape[-1]
         self.channel_splits = [ii * self.query_dim for ii in self.head_splits]
 
         self.dconvs = []
         self.pads = []
         for id, (head_split, kernel_size) in enumerate(zip(self.head_splits, self.head_kernel_size)):
-            name_scope = "depth_conv_" + str(id + 1)
-            with tf.name_scope(name_scope) as scope:
-                dconv = keras.layers.DepthwiseConv2D(
-                    kernel_size,
-                    strides=1,
-                    padding="VALID",
-                    name=name_scope if self.name is None else self.name + name_scope,
-                )
-                # print(query_shape, [None, self.height, self.width, int(head_split * self.query_dim)])
-                dconv.build([None, self.height, self.width, int(head_split * self.query_dim)])
-            pad = [[0, 0], [kernel_size // 2, kernel_size // 2], [kernel_size // 2, kernel_size // 2], [0, 0]]
+            name = self.name and self.name + "depth_conv_" + str(id + 1)
+            dconv = layers.DepthwiseConv2D(kernel_size, strides=1, padding="valid", name=name)
+            if image_data_format() == "channels_last":
+                pad = [[0, 0], [kernel_size // 2, kernel_size // 2], [kernel_size // 2, kernel_size // 2], [0, 0]]
+            else:
+                pad = [[0, 0], [0, 0], [kernel_size // 2, kernel_size // 2], [kernel_size // 2, kernel_size // 2]]
             self.dconvs.append(dconv)
             self.pads.append(pad)
 
-    def call(self, query, value, **kwargs):
+    def __call__(self, query, value, **kwargs):
+        if not self.built:
+            self.build(query.shape)
+            self.built = True
         img_token_q, img_token_v = query[:, :, 1:, :], value[:, :, 1:, :]
 
-        img_token_v = tf.transpose(img_token_v, [0, 2, 1, 3])  # [batch, blocks, num_heads, query_dim]
-        img_token_v = tf.reshape(img_token_v, [-1, self.height, self.width, self.num_heads * self.query_dim])
-        split_values = tf.split(img_token_v, self.channel_splits, axis=-1)
-        nn = [dconv(tf.pad(split_value, pad)) for split_value, dconv, pad in zip(split_values, self.dconvs, self.pads)]
-        nn = tf.concat(nn, axis=-1)
-        conv_v_img = tf.reshape(nn, [-1, self.height * self.width, self.num_heads, self.query_dim])
-        conv_v_img = tf.transpose(conv_v_img, [0, 2, 1, 3])
+        if image_data_format() == "channels_last":
+            img_token_v = functional.transpose(img_token_v, [0, 2, 1, 3])  # [batch, blocks, num_heads, query_dim]
+            img_token_v = functional.reshape(img_token_v, [-1, self.height, self.width, self.num_heads * self.query_dim])
+            split_values = functional.split(img_token_v, self.channel_splits, axis=-1)
+        else:
+            img_token_v = functional.transpose(img_token_v, [0, 1, 3, 2])  # [batch, num_heads, query_dim, blocks]
+            img_token_v = functional.reshape(img_token_v, [-1, self.num_heads * self.query_dim, self.height, self.width])
+            split_values = functional.split(img_token_v, self.channel_splits, axis=1)
+        nn = [dconv(functional.pad(split_value, pad)) for split_value, dconv, pad in zip(split_values, self.dconvs, self.pads)]
+
+        if image_data_format() == "channels_last":
+            nn = functional.concat(nn, axis=-1)
+            conv_v_img = functional.reshape(nn, [-1, self.height * self.width, self.num_heads, self.query_dim])
+            conv_v_img = functional.transpose(conv_v_img, [0, 2, 1, 3])
+        else:
+            nn = functional.concat(nn, axis=1)
+            conv_v_img = functional.reshape(nn, [-1, self.num_heads, self.query_dim, self.height * self.width])
+            conv_v_img = functional.transpose(conv_v_img, [0, 1, 3, 2])
 
         EV_hat_img = img_token_q * conv_v_img
-        return tf.pad(EV_hat_img, [[0, 0], [0, 0], [1, 0], [0, 0]])
-
-    def get_config(self):
-        base_config = super(ConvRelativePositionalEncoding, self).get_config()
-        base_config.update({"head_splits": self.head_splits, "head_kernel_size": self.head_kernel_size, "input_height": self.input_height})
-        return base_config
-
-
-@keras.utils.register_keras_serializable(package="coat")
-class ClassToken(keras.layers.Layer):
-    def __init__(self, **kwargs):
-        super(ClassToken, self).__init__(**kwargs)
-        self.token_init = tf.initializers.TruncatedNormal(stddev=0.2)
-        self.supports_masking = False
-
-    def build(self, input_shape):
-        self.class_tokens = self.add_weight(name="tokens", shape=(1, 1, input_shape[-1]), initializer=self.token_init, trainable=True)
-        super(ClassToken, self).build(input_shape)
-
-    def call(self, inputs, **kwargs):
-        class_tokens = tf.tile(self.class_tokens, [tf.shape(inputs)[0], 1, 1])
-        return tf.concat([class_tokens, inputs], axis=1)
-
-    def compute_output_shape(self, input_shape):
-        return (input_shape[0], input_shape[1] + 1, input_shape[2])
+        return functional.pad(EV_hat_img, [[0, 0], [0, 0], [1, 0], [0, 0]])
 
 
 def factor_attention_conv_relative_positional_encoding(inputs, shared_crpe=None, num_heads=8, qkv_bias=True, name=""):
     blocks, dim = inputs.shape[1], inputs.shape[-1]
     key_dim = dim // num_heads
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
+    qk_scale = 1.0 / (float(key_dim) ** 0.5)
 
-    qkv = keras.layers.Dense(dim * 3, use_bias=qkv_bias, name=name + "qkv")(inputs)
-    qkv = keras.layers.Reshape([blocks, 3, num_heads, key_dim])(qkv)
-    qq, kk, vv = tf.transpose(qkv, [2, 0, 3, 1, 4])  # [qkv, batch, num_heads, blocks, key_dim]
+    qkv = layers.Dense(dim * 3, use_bias=qkv_bias, name=name + "qkv")(inputs)
+    qq, kk, vv = functional.split(qkv, 3, axis=-1)
+    qq = functional.transpose(functional.reshape(qq, [-1, blocks, num_heads, key_dim]), [0, 2, 1, 3])
+    kk = functional.transpose(functional.reshape(kk, [-1, blocks, num_heads, key_dim]), [0, 2, 3, 1])
+    vv = functional.transpose(functional.reshape(vv, [-1, blocks, num_heads, key_dim]), [0, 2, 1, 3])
     # print(f">>>> {qkv.shape = }, {qq.shape = }, {kk.shape = }, {vv.shape = }")
 
     # Factorized attention.
-    # kk = tf.nn.softmax(kk, axis=2)  # On `blocks` dimension
-    kk = keras.layers.Softmax(axis=2, name=name and name + "attention_scores")(kk)  # On `blocks` dimension
-    # attn = tf.matmul(kk, vv, transpose_a=True)  # 'b h n k, b h n v -> b h k v', [batch, num_heads, key_dim, key_dim]
-    # factor_att = tf.matmul(qq, attn)    # 'b h n k, b h k v -> b h n v', [batch, num_heads, blocks, key_dim]
-    attn = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1], transpose_a=True))([kk, vv])
-    factor_att = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([qq, attn])
+    kk = layers.Softmax(axis=-1, name=name and name + "attention_scores")(kk)  # On `blocks` dimension
+    factor_att = qq @ (kk @ vv)
 
     # Convolutional relative position encoding.
-    crpe_out = shared_crpe(qq, vv) if shared_crpe is not None else ConvRelativePositionalEncoding(name=name + "crpe")(qq, vv)
+    crpe_out = shared_crpe(qq, vv) if shared_crpe is not None else ConvRelativePositionalEncoding(name=name + "crpe_")(qq, vv)
 
     # Merge and reshape.
-    nn = keras.layers.Add()([factor_att * qk_scale, crpe_out])
-    nn = keras.layers.Permute([2, 1, 3])(nn)
-    nn = keras.layers.Reshape([blocks, dim])(nn)
-
-    # Output projection.
-    nn = keras.layers.Dense(dim, name=name + "out")(nn)
-    # Drop
+    nn = layers.Add()([factor_att * qk_scale, crpe_out])
+    nn = layers.Permute([2, 1, 3])(nn)
+    nn = layers.Reshape([blocks, dim])(nn)
+    nn = layers.Dense(dim, name=name + "out")(nn)
     return nn
 
 
-def __cpe_norm_crpe__(inputs, shared_cpe=None, shared_crpe=None, num_heads=8, name=""):
-    cpe_out = shared_cpe(inputs) if shared_cpe is not None else ConvPositionalEncoding(name=name + "cpe")(inputs)  # shared
-    nn = layer_norm(cpe_out, name=name + "norm1")
+def cpe_norm_crpe(inputs, shared_cpe=None, shared_crpe=None, num_heads=8, name=""):
+    cpe_out = shared_cpe(inputs) if shared_cpe is not None else ConvPositionalEncoding(name=name + "cpe_")(inputs)  # shared
+    nn = layer_norm(cpe_out, axis=-1, name=name + "norm1")
     crpe_out = factor_attention_conv_relative_positional_encoding(nn, shared_crpe=shared_crpe, num_heads=num_heads, name=name + "factoratt_crpe_")
     return cpe_out, crpe_out
 
 
-def __res_mlp_block__(cpe_out, crpe_out, mlp_ratio=4, drop_rate=0, activation="gelu", name=""):
+def res_mlp_block(cpe_out, crpe_out, mlp_ratio=4, drop_rate=0, activation="gelu", name=""):
     if drop_rate > 0:
-        crpe_out = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop_1")(crpe_out)
-    cpe_crpe = keras.layers.Add()([cpe_out, crpe_out])
+        crpe_out = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop_1")(crpe_out)
+    cpe_crpe = layers.Add()([cpe_out, crpe_out])
 
     # MLP
-    nn = layer_norm(cpe_crpe, name=name + "norm2")
-    nn = mlp_block(nn, nn.shape[-1] * mlp_ratio, activation=activation, name=name + "mlp_")
+    pre_mlp = layer_norm(cpe_crpe, axis=-1, name=name + "norm2")
+    nn = layers.Dense(pre_mlp.shape[-1] * mlp_ratio, name=name + "mlp_dense_0")(pre_mlp)
+    nn = activation_by_name(nn, activation, name=name + "mlp_")
+    nn = layers.Dense(pre_mlp.shape[-1], name=name + "mlp_dense_1")(nn)
+
     if drop_rate > 0:
-        nn = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop_2")(nn)
-    return keras.layers.Add(name=name + "output")([cpe_crpe, nn])
+        nn = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop_2")(nn)
+    return layers.Add(name=name + "output")([cpe_crpe, nn])
 
 
 def serial_block(inputs, embed_dim, shared_cpe=None, shared_crpe=None, num_heads=8, mlp_ratio=4, drop_rate=0, activation="gelu", name=""):
-    cpe_out, crpe_out = __cpe_norm_crpe__(inputs, shared_cpe, shared_crpe, num_heads, name=name)
-    out = __res_mlp_block__(cpe_out, crpe_out, mlp_ratio, drop_rate, activation=activation, name=name)
+    cpe_out, crpe_out = cpe_norm_crpe(inputs, shared_cpe, shared_crpe, num_heads, name=name)
+    out = res_mlp_block(cpe_out, crpe_out, mlp_ratio, drop_rate, activation=activation, name=name)
     return out
 
 
 def resample(image, target_shape, class_token=None):
-    out_image = tf.cast(tf.image.resize(image, target_shape, method="bilinear"), image.dtype)
+    # print(f"{image.shape = }, {target_shape = }")
+    image = image if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(image)
+    # out_image = functional.cast(functional.resize(image, target_shape, method="bilinear"), image.dtype)
+    out_image = functional.resize(image, target_shape, method="bilinear")
+    out_image = out_image if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(out_image)
 
     if class_token is not None:
-        out_image = tf.reshape(out_image, [-1, out_image.shape[1] * out_image.shape[2], out_image.shape[-1]])
-        return tf.concat([class_token, out_image], axis=1)
+        out_image = functional.reshape(out_image, [-1, out_image.shape[1] * out_image.shape[2], out_image.shape[-1]])
+        return functional.concat([class_token, out_image], axis=1)
     else:
         return out_image
 
 
 def parallel_block(inputs, shared_cpes=None, shared_crpes=None, block_heights=[], num_heads=8, mlp_ratios=[], drop_rate=0, activation="gelu", name=""):
     # Conv-Attention.
     # print(f">>>> {block_heights = }")
     cpe_outs, crpe_outs, crpe_images, resample_shapes = [], [], [], []
     block_heights = block_heights[1:]
     for id, (xx, shared_cpe, shared_crpe) in enumerate(zip(inputs[1:], shared_cpes[1:], shared_crpes[1:])):
         cur_name = name + "{}_".format(id + 2)
-        cpe_out, crpe_out = __cpe_norm_crpe__(xx, shared_cpe, shared_crpe, num_heads, name=cur_name)
+        cpe_out, crpe_out = cpe_norm_crpe(xx, shared_cpe, shared_crpe, num_heads, name=cur_name)
         cpe_outs.append(cpe_out)
         crpe_outs.append(crpe_out)
-        height = block_heights[id] if len(block_heights) > id else int(tf.math.sqrt(float(crpe_out.shape[1] - 1)))
+        height = block_heights[id] if len(block_heights) > id else int(float(crpe_out.shape[1] - 1) ** 0.5)
         width = (crpe_out.shape[1] - 1) // height
-        crpe_images.append(tf.reshape(crpe_out[:, 1:, :], [-1, height, width, crpe_out.shape[-1]]))
+        crpe_images.append(functional.reshape(crpe_out[:, 1:, :], [-1, height, width, crpe_out.shape[-1]]))
         resample_shapes.append([height, width])
         # print(f">>>> {crpe_out.shape = }, {crpe_images[-1].shape = }")
     crpe_stack = [  # [[None, 28, 28, 152], [None, 14, 14, 152], [None, 7, 7, 152]]
         crpe_outs[0] + resample(crpe_images[1], resample_shapes[0], crpe_outs[1][:, :1]) + resample(crpe_images[2], resample_shapes[0], crpe_outs[2][:, :1]),
         crpe_outs[1] + resample(crpe_images[2], resample_shapes[1], crpe_outs[2][:, :1]) + resample(crpe_images[0], resample_shapes[1], crpe_outs[0][:, :1]),
         crpe_outs[2] + resample(crpe_images[1], resample_shapes[2], crpe_outs[1][:, :1]) + resample(crpe_images[0], resample_shapes[2], crpe_outs[0][:, :1]),
     ]
 
     # MLP
     outs = []
     for id, (cpe_out, crpe_out, mlp_ratio) in enumerate(zip(cpe_outs, crpe_stack, mlp_ratios[1:])):
         cur_name = name + "{}_".format(id + 2)
-        out = __res_mlp_block__(cpe_out, crpe_out, mlp_ratio, drop_rate, activation=activation, name=cur_name)
+        out = res_mlp_block(cpe_out, crpe_out, mlp_ratio, drop_rate, activation=activation, name=cur_name)
         outs.append(out)
     return inputs[:1] + outs  # inputs[0] directly out
 
 
 def patch_embed(inputs, embed_dim, patch_size=2, input_height=-1, name=""):
     if len(inputs.shape) == 3:
-        input_height = input_height if input_height > 0 else int(tf.math.sqrt(float(inputs.shape[1])))
+        input_height = input_height if input_height > 0 else int(float(inputs.shape[1]) ** 0.5)
         input_width = inputs.shape[1] // input_height
-        inputs = keras.layers.Reshape([input_height, input_width, inputs.shape[-1]])(inputs)
+        inputs = layers.Reshape([input_height, input_width, inputs.shape[-1]])(inputs)
+        inputs = inputs if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(inputs)
     nn = conv2d_no_bias(inputs, embed_dim, kernel_size=patch_size, strides=patch_size, use_bias=True, name=name)
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)
     block_height = nn.shape[1]
-    nn = keras.layers.Reshape([nn.shape[1] * nn.shape[2], nn.shape[-1]])(nn)  # flatten(2)
-    nn = layer_norm(nn, name=name)
+    nn = layers.Reshape([nn.shape[1] * nn.shape[2], nn.shape[3]])(nn)  # flatten(2)
+    nn = layer_norm(nn, axis=-1, name=name)
     return nn, block_height
 
 
 def CoaT(
-    serial_depths,
-    embed_dims,
-    mlp_ratios,
+    serial_depths=[2, 2, 2, 2],
+    embed_dims=[64, 128, 256, 320],
+    mlp_ratios=[8, 8, 4, 4],
     parallel_depth=0,
     patch_size=4,
     num_heads=8,
     head_splits=[2, 3, 3],
     head_kernel_size=[3, 5, 7],
     use_shared_cpe=True,  # For checking model architecture only, keep input_shape height == width if set False
     use_shared_crpe=True,  # For checking model architecture only, keep input_shape height == width if set False
@@ -257,15 +232,18 @@
     activation="gelu",
     drop_connect_rate=0,
     classifier_activation="softmax",
     pretrained="imagenet",
     model_name="coat",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
 
     # serial blocks
     nn = inputs
     classfier_outs = []
     shared_cpes = []
     shared_crpes = []
     block_heights = []
@@ -274,16 +252,16 @@
         patch_size = patch_size if sid == 0 else 2
         patch_input_height = -1 if sid == 0 else block_heights[-1]
         # print(f">>>> {nn.shape = }")
         nn, block_height = patch_embed(nn, embed_dim, patch_size=patch_size, input_height=patch_input_height, name=name + "patch_")
         block_heights.append(block_height)
         # print(f">>>> {nn.shape = }, {block_height = }")
         nn = ClassToken(name=name + "class_token")(nn)
-        shared_cpe = ConvPositionalEncoding(kernel_size=3, input_height=block_height, name="cpe_" + str(sid + 1)) if use_shared_cpe else None
-        shared_crpe = ConvRelativePositionalEncoding(head_splits, head_kernel_size, block_height, name="crpe_" + str(sid + 1)) if use_shared_crpe else None
+        shared_cpe = ConvPositionalEncoding(kernel_size=3, input_height=block_height, name="cpe{}_".format(sid + 1)) if use_shared_cpe else None
+        shared_crpe = ConvRelativePositionalEncoding(head_splits, head_kernel_size, block_height, name="crpe{}_".format(sid + 1)) if use_shared_crpe else None
         for bid in range(depth):
             block_name = name + "block{}_".format(bid + 1)
             nn = serial_block(nn, embed_dim, shared_cpe, shared_crpe, num_heads, mlp_ratio, activation=activation, name=block_name)
         classfier_outs.append(nn)
         shared_cpes.append(shared_cpe)
         shared_crpes.append(shared_crpe)
         nn = nn[:, 1:, :]  # remove class token
@@ -292,85 +270,55 @@
     for pid in range(parallel_depth):
         name = "parallel{}_".format(pid + 1)
         classfier_outs = parallel_block(classfier_outs, shared_cpes, shared_crpes, block_heights, num_heads, mlp_ratios, activation=activation, name=name)
 
     if out_features is not None:  # Return intermediate features (for down-stream tasks).
         nn = [classfier_outs[id][:, 1:, :] for id in out_features]
     elif parallel_depth == 0:  # Lite model, only serial blocks, Early return.
-        nn = layer_norm(classfier_outs[-1], name="out_")[:, 0]
+        nn = layer_norm(classfier_outs[-1], axis=-1, name="out_")[:, 0]
     else:
-        nn = [layer_norm(xx, name="out_{}_".format(id + 1))[:, :1, :] for id, xx in enumerate(classfier_outs[1:])]
-        nn = keras.layers.Concatenate(axis=1)(nn)
-        nn = keras.layers.Permute([2, 1])(nn)
-        nn = keras.layers.Conv1D(1, 1, name="aggregate")(nn)[:, :, 0]
-
+        nn = [layer_norm(xx, axis=-1, name="out_{}_".format(id + 1))[:, :1, :] for id, xx in enumerate(classfier_outs[1:])]
+        nn = layers.Concatenate(axis=1)(nn)
+        nn = layers.Permute([2, 1])(nn) if image_data_format() == "channels_last" else nn
+        nn = layers.Conv1D(1, 1, name="aggregate")(nn)
+        nn = nn[:, :, 0] if image_data_format() == "channels_last" else nn[:, 0]
     if out_features is None and num_classes > 0:
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="coat", pretrained=pretrained)
     return model
 
 
-BLOCK_CONFIGS = {
-    "lite_tiny": {
-        "serial_depths": [2, 2, 2, 2],
-        "embed_dims": [64, 128, 256, 320],
-        "mlp_ratios": [8, 8, 4, 4],
-        "parallel_depth": 0,
-        "patch_size": 4,
-        "num_heads": 8,
-    },
-    "lite_mini": {
-        "serial_depths": [2, 2, 2, 2],
-        "embed_dims": [64, 128, 320, 512],
-        "mlp_ratios": [8, 8, 4, 4],
-        "parallel_depth": 0,
-        "patch_size": 4,
-        "num_heads": 8,
-    },
-    "lite_small": {
-        "serial_depths": [3, 4, 6, 3],
-        "embed_dims": [64, 128, 320, 512],
-        "mlp_ratios": [8, 8, 4, 4],
-        "parallel_depth": 0,
-        "patch_size": 4,
-        "num_heads": 8,
-    },
-    "tiny": {
-        "serial_depths": [2, 2, 2, 2],
-        "embed_dims": [152, 152, 152, 152],
-        "mlp_ratios": [4, 4, 4, 4],
-        "parallel_depth": 6,
-        "patch_size": 4,
-        "num_heads": 8,
-    },
-    "mini": {
-        "serial_depths": [2, 2, 2, 2],
-        "embed_dims": [152, 216, 216, 216],
-        "mlp_ratios": [4, 4, 4, 4],
-        "parallel_depth": 6,
-        "patch_size": 4,
-        "num_heads": 8,
-    },
-}
-
-
+@register_model
 def CoaTLiteTiny(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    return CoaT(**BLOCK_CONFIGS["lite_tiny"], **locals(), model_name="coat_lite_tiny", **kwargs)
+    return CoaT(**locals(), model_name="coat_lite_tiny", **kwargs)
 
 
+@register_model
 def CoaTLiteMini(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    return CoaT(**BLOCK_CONFIGS["lite_mini"], **locals(), model_name="coat_lite_mini", **kwargs)
+    embed_dims = [64, 128, 320, 512]
+    return CoaT(**locals(), model_name="coat_lite_mini", **kwargs)
 
 
+@register_model
 def CoaTLiteSmall(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    return CoaT(**BLOCK_CONFIGS["lite_small"], **locals(), model_name="coat_lite_small", **kwargs)
+    serial_depths = [3, 4, 6, 3]
+    embed_dims = [64, 128, 320, 512]
+    return CoaT(**locals(), model_name="coat_lite_small", **kwargs)
 
 
+@register_model
 def CoaTTiny(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    return CoaT(**BLOCK_CONFIGS["tiny"], **locals(), model_name="coat_tiny", **kwargs)
+    embed_dims = [152, 152, 152, 152]
+    mlp_ratios = [4, 4, 4, 4]
+    parallel_depth = 6
+    return CoaT(**locals(), model_name="coat_tiny", **kwargs)
 
 
+@register_model
 def CoaTMini(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    return CoaT(**BLOCK_CONFIGS["mini"], **locals(), model_name="coat_mini", **kwargs)
+    embed_dims = [152, 216, 216, 216]
+    mlp_ratios = [4, 4, 4, 4]
+    parallel_depth = 6
+    return CoaT(**locals(), model_name="coat_mini", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/coatnet/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/coatnet/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/coatnet/coatnet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/coatnet/coatnet.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,74 +1,82 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     batchnorm_with_activation,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
     drop_block,
     layer_norm,
     se_module,
     output_block,
     MultiHeadRelativePositionalEmbedding,
+    qkv_to_multi_head_channels_last_format,
+    scaled_dot_product_attention,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {"coatnet0": {"imagenet": {160: "bc4375d2f03b99ac4252770331f0d22f", 224: "5d1e563f959f7efb6d395bde7373ed26"}}}
 
 
 def mhsa_with_multi_head_relative_position_embedding(
-    inputs, num_heads=4, key_dim=0, global_query=None, out_shape=None, out_weight=True, qkv_bias=False, out_bias=False, attn_dropout=0, name=None
+    inputs,
+    num_heads=4,
+    key_dim=0,
+    global_query=None,
+    out_shape=None,
+    out_weight=True,
+    qkv_bias=False,
+    out_bias=False,
+    attn_dropout=0,
+    data_format=None,
+    name=None,
 ):
-    _, hh, ww, cc = inputs.shape
-    key_dim = key_dim if key_dim > 0 else cc // num_heads
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
-    out_shape = cc if out_shape is None or not out_weight else out_shape
+    data_format = image_data_format() if data_format is None else data_format
+    input_channel = inputs.shape[-1 if data_format == "channels_last" else 1]
+    height, width = inputs.shape[1:-1] if data_format == "channels_last" else inputs.shape[2:]
+
+    key_dim = key_dim if key_dim > 0 else input_channel // num_heads
+    out_shape = input_channel if out_shape is None or not out_weight else out_shape
     qk_out = num_heads * key_dim
     # vv_dim = out_shape // num_heads
     vv_dim = key_dim
-    blocks = hh * ww
+    blocks = height * width
+
+    # Permute for conv if given data_format not matching actual image_data_format
+    if image_data_format() == "channels_last" and data_format == "channels_first":
+        inputs = layers.Permute([2, 3, 1])(inputs)
+    elif image_data_format() == "channels_first" and data_format == "channels_last":
+        inputs = layers.Permute([3, 1, 2])(inputs)
+    conv_channel_axis = -1 if image_data_format() == "channels_last" else 1
 
     if global_query is not None:
-        # kv = keras.layers.Dense(qk_out * 2, use_bias=qkv_bias, name=name and name + "kv")(inputs)  # For GCViT weights
+        # kv = layers.Dense(qk_out * 2, use_bias=qkv_bias, name=name and name + "kv")(inputs)  # For GCViT weights
         kv = conv2d_no_bias(inputs, qk_out * 2, kernel_size=1, use_bias=qkv_bias, name=name and name + "kv_")
-        kv = tf.reshape(kv, [-1, blocks, kv.shape[-1]])
-        key, value = tf.split(kv, [qk_out, out_shape], axis=-1)
+        kv = functional.reshape(kv, [-1, blocks, kv.shape[-1]] if image_data_format() == "channels_last" else [-1, kv.shape[1], blocks])
+        key, value = functional.split(kv, [qk_out, out_shape], axis=conv_channel_axis)
         query = global_query
+        _, key, value = qkv_to_multi_head_channels_last_format(None, key, value, num_heads=num_heads, data_format=None)
     else:
         # qkv = conv2d_no_bias(inputs, qk_out * 2 + out_shape, use_bias=qkv_bias, kernel_size=1, name=name and name + "qkv_")
-        # qkv = keras.layers.Dense(qk_out * 3, use_bias=qkv_bias, name=name and name + "qkv")(inputs)  # For GCViT weights
-        # query = keras.layers.Dense(qk_out, use_bias=qkv_bias, name=name and name + "query")(inputs)  # For MaxViT weights
-        # key = keras.layers.Dense(qk_out, use_bias=qkv_bias, name=name and name + "key")(inputs)  # For MaxViT weights
-        # value = keras.layers.Dense(qk_out, use_bias=qkv_bias, name=name and name + "value")(inputs)  # For MaxViT weights
+        # qkv = layers.Dense(qk_out * 3, use_bias=qkv_bias, name=name and name + "qkv")(inputs)  # For GCViT weights
+        # query = layers.Dense(qk_out, use_bias=qkv_bias, name=name and name + "query")(inputs)  # For MaxViT weights
+        # key = layers.Dense(qk_out, use_bias=qkv_bias, name=name and name + "key")(inputs)  # For MaxViT weights
+        # value = layers.Dense(qk_out, use_bias=qkv_bias, name=name and name + "value")(inputs)  # For MaxViT weights
         qkv = conv2d_no_bias(inputs, qk_out * 3, kernel_size=1, use_bias=qkv_bias, name=name and name + "qkv_")
-        query, key, value = tf.split(qkv, [qk_out, qk_out, qk_out], axis=-1)
-        # print(f"{query.shape = }, {key.shape = }, {value.shape = }, {num_heads = }, {key_dim = }, {vv_dim = }")
-        query = tf.transpose(tf.reshape(query, [-1, blocks, num_heads, key_dim]), [0, 2, 1, 3])  #  [batch, num_heads, hh * ww, key_dim]
-    key = tf.transpose(tf.reshape(key, [-1, blocks, num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-    value = tf.transpose(tf.reshape(value, [-1, blocks, num_heads, vv_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, vv_dim]
-
-    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale  # [batch, num_heads, hh * ww, hh * ww]
-    # print(f"{query.shape = }, {key.shape = }, {value.shape = }, {attention_scores.shape = }, {hh = }")
-    attention_scores = MultiHeadRelativePositionalEmbedding(with_cls_token=False, attn_height=hh, name=name and name + "pos_emb")(attention_scores)
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
-    attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores) if attn_dropout > 0 else attention_scores
-
-    # value = [batch, num_heads, hh * ww, vv_dim], attention_output = [batch, num_heads, hh * ww, vv_dim]
-    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
-    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
-    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * vv_dim])
-    # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
-
-    if out_weight:
-        # [batch, hh, ww, num_heads * vv_dim] * [num_heads * vv_dim, out] --> [batch, hh, ww, out]
-        attention_output = keras.layers.Dense(out_shape, use_bias=out_bias, name=name and name + "output")(attention_output)
-    # attention_output = keras.layers.Dropout(output_dropout, name=name and name + "out_drop")(attention_output) if output_dropout > 0 else attention_output
-    return attention_output
+        query, key, value = functional.split(qkv, [qk_out, qk_out, qk_out], axis=conv_channel_axis)
+        query, key, value = qkv_to_multi_head_channels_last_format(query, key, value, num_heads=num_heads, data_format=None)
+    # print(f"{query.shape = }, {key.shape = }, {value.shape = }, {num_heads = }, {key_dim = }, {vv_dim = }")
+
+    pos_emb = MultiHeadRelativePositionalEmbedding(with_cls_token=False, attn_height=height, name=name and name + "pos_emb")
+    output_shape = (height, width, out_shape)
+    out = scaled_dot_product_attention(query, key, value, output_shape, pos_emb, out_weight=out_weight, out_bias=out_bias, dropout=attn_dropout, name=name)
+    return out if data_format == "channels_last" else layers.Permute([3, 1, 2], name=name and name + "output_perm")(out)
 
 
 def res_MBConv(
     inputs,
     output_channel,
     conv_short_cut=True,
     strides=1,
@@ -80,70 +88,70 @@
     activation="gelu",
     name="",
 ):
     """x  Proj(Pool(x)) + Conv (DepthConv (Conv (Norm(x), stride = 2))))"""
     preact = batchnorm_with_activation(inputs, activation=None, zero_gamma=False, name=name + "preact_")
 
     if conv_short_cut:
-        shortcut = keras.layers.MaxPool2D(strides, strides=strides, padding="SAME", name=name + "shortcut_pool")(inputs) if strides > 1 else inputs
+        shortcut = layers.MaxPool2D(strides, strides=strides, padding="same", name=name + "shortcut_pool")(inputs) if strides > 1 else inputs
         shortcut = conv2d_no_bias(shortcut, output_channel, 1, strides=1, name=name + "shortcut_")
         # shortcut = batchnorm_with_activation(shortcut, activation=activation, zero_gamma=False, name=name + "shortcut_")
     else:
         shortcut = inputs
 
     # MBConv
-    input_channel = inputs.shape[-1]
+    input_channel = inputs.shape[-1 if image_data_format() == "channels_last" else 1]
     conv_strides, dw_strides = (1, strides) if use_dw_strides else (strides, 1)  # May swap stirdes with DW
     nn = conv2d_no_bias(preact, input_channel * expansion, 1, strides=conv_strides, use_bias=bn_act_first, padding="same", name=name + "expand_")
     nn = batchnorm_with_activation(nn, activation=activation, act_first=bn_act_first, name=name + "expand_")
     nn = depthwise_conv2d_no_bias(nn, 3, strides=dw_strides, use_bias=bn_act_first, padding="same", name=name + "MB_")
     nn = batchnorm_with_activation(nn, activation=activation, act_first=bn_act_first, zero_gamma=False, name=name + "MB_dw_")
     if se_ratio:
         nn = se_module(nn, se_ratio=se_ratio / expansion, activation=activation, name=name + "se_")
     nn = conv2d_no_bias(nn, output_channel, 1, strides=1, padding="same", name=name + "MB_pw_")
     # nn = batchnorm_with_activation(nn, activation=None, zero_gamma=True, name=name + "MB_pw_")
     nn = drop_block(nn, drop_rate=drop_rate, name=name)
-    return keras.layers.Add(name=name + "output")([shortcut, nn])
+    return layers.Add(name=name + "output")([shortcut, nn])
 
 
 def res_ffn(inputs, expansion=4, kernel_size=1, drop_rate=0, activation="gelu", name=""):
     """x  x + Module (Norm(x)), similar with typical MLP block"""
     # preact = batchnorm_with_activation(inputs, activation=None, zero_gamma=False, name=name + "preact_")
     preact = layer_norm(inputs, name=name + "preact_")
 
-    input_channel = inputs.shape[-1]
+    input_channel = inputs.shape[-1 if image_data_format() == "channels_last" else 1]
     nn = conv2d_no_bias(preact, input_channel * expansion, kernel_size, name=name + "1_")
     nn = activation_by_name(nn, activation=activation, name=name)
     nn = conv2d_no_bias(nn, input_channel, kernel_size, name=name + "2_")
     nn = drop_block(nn, drop_rate=drop_rate, name=name)
-    # return keras.layers.Add(name=name + "output")([preact, nn])
-    return keras.layers.Add(name=name + "output")([inputs, nn])
+    # return layers.Add(name=name + "output")([preact, nn])
+    return layers.Add(name=name + "output")([inputs, nn])
 
 
 def res_mhsa(inputs, output_channel, conv_short_cut=True, strides=1, head_dimension=32, drop_rate=0, activation="gelu", name=""):
     """x  Proj(Pool(x)) + Attention (Pool(Norm(x)))"""
     # preact = batchnorm_with_activation(inputs, activation=None, zero_gamma=False, name=name + "preact_")
     preact = layer_norm(inputs, name=name + "preact_")
 
     if conv_short_cut:
-        shortcut = keras.layers.MaxPool2D(strides, strides=strides, padding="SAME", name=name + "shortcut_pool")(inputs) if strides > 1 else inputs
+        shortcut = layers.MaxPool2D(strides, strides=strides, padding="same", name=name + "shortcut_pool")(inputs) if strides > 1 else inputs
         shortcut = conv2d_no_bias(shortcut, output_channel, 1, strides=1, name=name + "shortcut_")
         # shortcut = batchnorm_with_activation(shortcut, activation=activation, zero_gamma=False, name=name + "shortcut_")
     else:
         shortcut = inputs
 
     nn = preact
     if strides != 1:  # Downsample
-        # nn = keras.layers.ZeroPadding2D(padding=1, name=name + "pad")(nn)
-        nn = keras.layers.MaxPool2D(pool_size=2, strides=strides, padding="SAME", name=name + "pool")(nn)
-    num_heads = nn.shape[-1] // head_dimension
+        # nn = layers.ZeroPadding2D(padding=1, name=name + "pad")(nn)
+        nn = layers.MaxPool2D(pool_size=2, strides=strides, padding="same", name=name + "pool")(nn)
+    num_heads = nn.shape[-1 if image_data_format() == "channels_last" else 1] // head_dimension
     nn = mhsa_with_multi_head_relative_position_embedding(nn, num_heads=num_heads, key_dim=head_dimension, out_shape=output_channel, name=name + "mhsa_")
     nn = drop_block(nn, drop_rate=drop_rate, name=name)
     # print(f"{name = }, {inputs.shape = }, {shortcut.shape = }, {nn.shape = }")
-    return keras.layers.Add(name=name + "output")([shortcut, nn])
+    return layers.Add(name=name + "output")([shortcut, nn])
 
 
 def CoAtNet(
     num_blocks,
     out_channels,
     stem_width=64,
     block_types=["conv", "conv", "transform", "transform"],
@@ -159,15 +167,18 @@
     drop_connect_rate=0,
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="coatnet",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
 
     """ stage 0, Stem_stage """
     nn = conv2d_no_bias(inputs, stem_width, 3, strides=2, use_bias=bn_act_first, padding="same", name="stem_1_")
     nn = batchnorm_with_activation(nn, activation=activation, act_first=bn_act_first, name="stem_1_")
     nn = conv2d_no_bias(nn, stem_width, 3, strides=1, use_bias=bn_act_first, padding="same", name="stem_2_")
     # nn = batchnorm_with_activation(nn, activation=activation, name="stem_2_")
 
@@ -190,80 +201,89 @@
                     nn, out_channel, conv_short_cut, stride, expansion, block_se_ratio, block_drop_rate, use_dw_strides, bn_act_first, activation, name=name
                 )
             else:
                 nn = res_mhsa(nn, out_channel, conv_short_cut, stride, head_dimension, block_drop_rate, activation=activation, name=name)
                 nn = res_ffn(nn, expansion=expansion, drop_rate=block_drop_rate, activation=activation, name=name + "ffn_")
 
     nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation, act_first=bn_act_first)
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "coatnet", pretrained, MultiHeadRelativePositionalEmbedding)
     return model
 
 
+@register_model
 def CoAtNetT(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0, classifier_activation="softmax", **kwargs):
     num_blocks = [3, 4, 6, 3]
     out_channels = [64, 128, 256, 512]
     stem_width = 64
     return CoAtNet(**locals(), model_name="coatnett", **kwargs)
 
 
+@register_model
 def CoAtNet0(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 3, 5, 2]
     out_channels = [96, 192, 384, 768]
     stem_width = 64
     return CoAtNet(**locals(), model_name="coatnet0", **kwargs)
 
 
+@register_model
 def CoAtNet1(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.3, classifier_activation="softmax", **kwargs):
     num_blocks = [2, 6, 14, 2]
     out_channels = [96, 192, 384, 768]
     stem_width = 64
     return CoAtNet(**locals(), model_name="coatnet1", **kwargs)
 
 
+@register_model
 def CoAtNet2(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.5, classifier_activation="softmax", **kwargs):
     num_blocks = [2, 6, 14, 2]
     out_channels = [128, 256, 512, 1024]
     stem_width = 128
     return CoAtNet(**locals(), model_name="coatnet2", **kwargs)
 
 
+@register_model
 def CoAtNet3(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.7, classifier_activation="softmax", **kwargs):
     num_blocks = [2, 6, 14, 2]
     out_channels = [192, 384, 768, 1536]
     stem_width = 192
     return CoAtNet(**locals(), model_name="coatnet3", **kwargs)
 
 
+@register_model
 def CoAtNet4(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.2, classifier_activation="softmax", **kwargs):
     num_blocks = [2, 12, 28, 2]
     out_channels = [192, 384, 768, 1536]
     stem_width = 192
     return CoAtNet(**locals(), model_name="coatnet4", **kwargs)
 
 
+@register_model
 def CoAtNet5(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.2, classifier_activation="softmax", **kwargs):
     num_blocks = [2, 12, 28, 2]
     out_channels = [256, 512, 1280, 2048]
     stem_width = 192
     head_dimension = 64
     return CoAtNet(**locals(), model_name="coatnet5", **kwargs)
 
 
+@register_model
 def CoAtNet6(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.2, classifier_activation="softmax", **kwargs):
     num_blocks = [2, 4, 8, 42, 2]
     out_channels = [192, 384, 768, 1536, 2048]
     block_types = ["conv", "conv", "conv", "transfrom", "transform"]
     strides = [2, 2, 2, 1, 2]
     stem_width = 192
     head_dimension = 128
     return CoAtNet(**locals(), model_name="coatnet6", **kwargs)
 
 
+@register_model
 def CoAtNet7(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0.2, classifier_activation="softmax", **kwargs):
     num_blocks = [2, 4, 8, 42, 2]
     out_channels = [256, 512, 1024, 2048, 3072]
     block_types = ["conv", "conv", "conv", "transfrom", "transform"]
     strides = [2, 2, 2, 1, 2]
     stem_width = 192
     head_dimension = 128
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/anchors_func.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/anchors_func.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,104 +1,115 @@
-import tensorflow as tf
-from tensorflow.keras import backend as K
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional
+
+
+if backend.is_tensorflow_backend:
+    import tensorflow as tf  # Needed for all assigning achors functions
+
 
 EFFICIENTDET_MODE = "efficientdet"
 ANCHOR_FREE_MODE = "anchor_free"
 YOLOR_MODE = "yolor"
-NUM_ANCHORS = {ANCHOR_FREE_MODE: 1, YOLOR_MODE: 3, EFFICIENTDET_MODE: 9}
+YOLOV8_MODE = "yolov8"
+NUM_ANCHORS = {ANCHOR_FREE_MODE: 1, YOLOV8_MODE: 1, YOLOR_MODE: 3, EFFICIENTDET_MODE: 9}
 
 """ Init anchors """
 
 
 def get_anchors_mode_parameters(anchors_mode, use_object_scores="auto", num_anchors="auto", anchor_scale="auto"):
     if anchors_mode == ANCHOR_FREE_MODE:
         use_object_scores = True if use_object_scores == "auto" else use_object_scores
         num_anchors = NUM_ANCHORS[anchors_mode] if num_anchors == "auto" else num_anchors
     elif anchors_mode == YOLOR_MODE:
         use_object_scores = True if use_object_scores == "auto" else use_object_scores
         num_anchors = NUM_ANCHORS[anchors_mode] if num_anchors == "auto" else num_anchors
+    elif anchors_mode == YOLOV8_MODE:
+        use_object_scores = False if use_object_scores == "auto" else use_object_scores
+        num_anchors = NUM_ANCHORS[anchors_mode] if num_anchors == "auto" else num_anchors
     else:
         use_object_scores = False if use_object_scores == "auto" else use_object_scores
         num_anchors = NUM_ANCHORS.get(anchors_mode, NUM_ANCHORS[EFFICIENTDET_MODE]) if num_anchors == "auto" else num_anchors
         anchor_scale = 4 if anchor_scale == "auto" else anchor_scale
     return use_object_scores, num_anchors, anchor_scale
 
 
 def get_feature_sizes(input_shape, pyramid_levels=[3, 7]):
     # https://github.com/google/automl/tree/master/efficientdet/utils.py#L509
     feature_sizes = [input_shape[:2]]
     for _ in range(max(pyramid_levels)):
         pre_feat_size = feature_sizes[-1]
-        feature_sizes.append(((pre_feat_size[0] - 1) // 2 + 1, (pre_feat_size[1] - 1) // 2 + 1))  # ceil mode, like padding="SAME" downsampling
+        feature_sizes.append(((pre_feat_size[0] - 1) // 2 + 1, (pre_feat_size[1] - 1) // 2 + 1))  # ceil mode, like padding="same" downsampling
     return feature_sizes
 
 
 def get_anchors(input_shape=(512, 512, 3), pyramid_levels=[3, 7], aspect_ratios=[1, 2, 0.5], num_scales=3, anchor_scale=4, grid_zero_start=False):
     """
     >>> from keras_cv_attention_models.coco import anchors_func
     >>> input_shape = [512, 128]
     >>> anchors = anchors_func.get_anchors([512, 128], pyramid_levels=[7])
     >>> anchors.draw_bboxes(anchors * [512, 128, 512, 128])
 
     grid_zero_start: grid starts from 0, else from strides // 2. False for efficientdet anchors, True for yolo anchors.
     """
     # base anchors
     scales = [2 ** (ii / num_scales) * anchor_scale for ii in range(num_scales)]
-    aspect_ratios_tensor = tf.convert_to_tensor(aspect_ratios, dtype="float32")
+    aspect_ratios_tensor = np.array(aspect_ratios, dtype="float32")
     if len(aspect_ratios_tensor.shape) == 1:
         # aspect_ratios = [0.5, 1, 2]
-        sqrt_ratios = tf.sqrt(aspect_ratios_tensor)
+        sqrt_ratios = np.sqrt(aspect_ratios_tensor)
         ww_ratios, hh_ratios = sqrt_ratios, 1 / sqrt_ratios
     else:
         # aspect_ratios = [(1, 1), (1.4, 0.7), (0.7, 1.4)]
         ww_ratios, hh_ratios = aspect_ratios_tensor[:, 0], aspect_ratios_tensor[:, 1]
-    base_anchors_hh = tf.reshape(tf.expand_dims(scales, 1) * tf.expand_dims(hh_ratios, 0), [-1])
-    base_anchors_ww = tf.reshape(tf.expand_dims(scales, 1) * tf.expand_dims(ww_ratios, 0), [-1])
+    base_anchors_hh = np.reshape(np.expand_dims(scales, 1) * np.expand_dims(hh_ratios, 0), [-1])
+    base_anchors_ww = np.reshape(np.expand_dims(scales, 1) * np.expand_dims(ww_ratios, 0), [-1])
     base_anchors_hh_half, base_anchors_ww_half = base_anchors_hh / 2, base_anchors_ww / 2
-    base_anchors = tf.stack([base_anchors_hh_half * -1, base_anchors_ww_half * -1, base_anchors_hh_half, base_anchors_ww_half], axis=1)
+    base_anchors = np.stack([base_anchors_hh_half * -1, base_anchors_ww_half * -1, base_anchors_hh_half, base_anchors_ww_half], axis=1)
     # base_anchors = tf.gather(base_anchors, [3, 6, 0, 4, 7, 1, 5, 8, 2])  # re-order according to official generated anchors
+    # For anchor_free, base_anchors = np.array([[-0.5, -0.5, 0.5, 0.5]])
 
     # make grid
     pyramid_levels = list(range(min(pyramid_levels), max(pyramid_levels) + 1))
     feature_sizes = get_feature_sizes(input_shape, pyramid_levels)
 
     all_anchors = []
     for level in pyramid_levels:
         stride_hh, stride_ww = feature_sizes[0][0] / feature_sizes[level][0], feature_sizes[0][1] / feature_sizes[level][1]
         top, left = (0, 0) if grid_zero_start else (stride_hh / 2, stride_ww / 2)
-        hh_centers = tf.range(top, input_shape[0], stride_hh)
-        ww_centers = tf.range(left, input_shape[1], stride_ww)
-        ww_grid, hh_grid = tf.meshgrid(ww_centers, hh_centers)
-        grid = tf.reshape(tf.stack([hh_grid, ww_grid, hh_grid, ww_grid], 2), [-1, 1, 4])
-        anchors = tf.expand_dims(base_anchors * [stride_hh, stride_ww, stride_hh, stride_ww], 0) + tf.cast(grid, base_anchors.dtype)
-        anchors = tf.reshape(anchors, [-1, 4])
+        hh_centers = np.arange(top, input_shape[0], stride_hh)
+        ww_centers = np.arange(left, input_shape[1], stride_ww)
+        ww_grid, hh_grid = np.meshgrid(ww_centers, hh_centers)
+        grid = np.reshape(np.stack([hh_grid, ww_grid, hh_grid, ww_grid], 2), [-1, 1, 4])
+        anchors = np.expand_dims(base_anchors * [stride_hh, stride_ww, stride_hh, stride_ww], 0) + grid.astype(base_anchors.dtype)
+        anchors = np.reshape(anchors, [-1, 4])
         all_anchors.append(anchors)
-    all_anchors = tf.concat(all_anchors, axis=0) / [input_shape[0], input_shape[1], input_shape[0], input_shape[1]]
+    all_anchors = np.concatenate(all_anchors, axis=0) / [input_shape[0], input_shape[1], input_shape[0], input_shape[1]]
     # if width_first:
     #      all_anchors = tf.gather(all_anchors, [1, 0, 3, 2], axis=-1)
 
-    return all_anchors
+    return functional.convert_to_tensor(all_anchors.astype("float32"))
 
 
 def get_anchor_free_anchors(input_shape=(512, 512, 3), pyramid_levels=[3, 5], grid_zero_start=True):
     return get_anchors(input_shape, pyramid_levels, aspect_ratios=[1], num_scales=1, anchor_scale=1, grid_zero_start=grid_zero_start)
 
 
 def get_yolor_anchors(input_shape=(512, 512), pyramid_levels=[3, 5], offset=0.5, is_for_training=False):
     assert max(pyramid_levels) - min(pyramid_levels) < 5
     # Original yolor using width first, height first here
     if max(pyramid_levels) - min(pyramid_levels) < 3:  # [3, 5], YOLOR_CSP / YOLOR_CSPX
-        anchor_ratios = tf.convert_to_tensor([[[16.0, 12], [36, 19], [28, 40]], [[75, 36], [55, 76], [146, 72]], [[110, 142], [243, 192], [401, 459]]])
+        anchor_ratios = np.array([[[16.0, 12], [36, 19], [28, 40]], [[75, 36], [55, 76], [146, 72]], [[110, 142], [243, 192], [401, 459]]])
         # anchor_ratios = tf.convert_to_tensor([[[13.0, 10], [30, 16], [23, 33]], [[61, 30], [45, 62], [119, 59]], [[90, 116], [198, 156], [326, 373]]])
     elif max(pyramid_levels) - min(pyramid_levels) < 4:  # [3, 6], YOLOR_*6
-        anchor_ratios = tf.convert_to_tensor(
+        anchor_ratios = np.array(
             [[[27.0, 19], [40, 44], [94, 38]], [[68, 96], [152, 86], [137, 180]], [[301, 140], [264, 303], [542, 238]], [[615, 436], [380, 739], [792, 925]]]
         )
     else:  # [3, 7] from YOLOV4_P7, using first 3 for each level
-        anchor_ratios = tf.convert_to_tensor(
+        anchor_ratios = np.array(
             [
                 [[17.0, 13], [25, 22], [66, 27]],
                 [[88, 57], [69, 112], [177, 69]],
                 [[138, 136], [114, 287], [275, 134]],
                 [[248, 268], [504, 232], [416, 445]],
                 [[393, 812], [808, 477], [908, 1070]],
             ]
@@ -106,115 +117,150 @@
 
     pyramid_levels = list(range(min(pyramid_levels), max(pyramid_levels) + 1))
     feature_sizes = get_feature_sizes(input_shape, pyramid_levels)
     # print(f"{pyramid_levels = }, {feature_sizes = }, {anchor_ratios = }")
     if is_for_training:
         # YOLOLayer https://github.com/WongKinYiu/yolor/blob/main/models/models.py#L351
         anchor_ratios = anchor_ratios[: len(pyramid_levels)] / [[[2**ii]] for ii in pyramid_levels]
-        feature_sizes = tf.convert_to_tensor(feature_sizes[min(pyramid_levels) : max(pyramid_levels) + 1], tf.float32)
-        return anchor_ratios, feature_sizes
+        feature_sizes = np.array(feature_sizes[min(pyramid_levels) : max(pyramid_levels) + 1], "int32")
+        return functional.convert_to_tensor(anchor_ratios.astype("float32")), feature_sizes
 
     all_anchors = []
     for level, anchor_ratio in zip(pyramid_levels, anchor_ratios):
         stride_hh, stride_ww = feature_sizes[0][0] / feature_sizes[level][0], feature_sizes[0][1] / feature_sizes[level][1]
         # hh_grid, ww_grid = tf.meshgrid(tf.range(feature_sizes[level][0]), tf.range(feature_sizes[level][1]))
-        ww_grid, hh_grid = tf.meshgrid(tf.range(feature_sizes[level][1]), tf.range(feature_sizes[level][0]))
-        grid = tf.cast(tf.stack([hh_grid, ww_grid], 2), "float32") - offset
-        grid = tf.reshape(grid, [-1, 1, 2])  # [1, level_feature_sizes, 2]
-        cur_base_anchors = anchor_ratio[tf.newaxis, :, :]  # [num_anchors, 1, 2]
-
-        grid_nd = tf.repeat(grid, cur_base_anchors.shape[1], axis=1) * [stride_hh, stride_ww]
-        cur_base_anchors_nd = tf.repeat(cur_base_anchors, grid.shape[0], axis=0)
-        stride_nd = tf.zeros_like(grid_nd) + [stride_hh, stride_ww]
+        ww_grid, hh_grid = np.meshgrid(np.arange(feature_sizes[level][1]), np.arange(feature_sizes[level][0]))
+        grid = np.stack([hh_grid, ww_grid], 2).astype("float32") - offset
+        grid = np.reshape(grid, [-1, 1, 2])  # [1, level_feature_sizes, 2]
+        cur_base_anchors = anchor_ratio[np.newaxis, :, :]  # [num_anchors, 1, 2]
+
+        grid_nd = np.repeat(grid, cur_base_anchors.shape[1], axis=1) * [stride_hh, stride_ww]
+        cur_base_anchors_nd = np.repeat(cur_base_anchors, grid.shape[0], axis=0)
+        stride_nd = np.zeros_like(grid_nd) + [stride_hh, stride_ww]
         # yield grid_nd, cur_base_anchors_nd, stride_nd
-        anchors = tf.concat([grid_nd, cur_base_anchors_nd, stride_nd], axis=-1)
-        all_anchors.append(tf.reshape(anchors, [-1, 6]))
-    all_anchors = tf.concat(all_anchors, axis=0) / ([input_shape[0], input_shape[1]] * 3)
-    return all_anchors  # [center_h, center_w, anchor_h, anchor_w, stride_h, stride_w]
+        anchors = np.concatenate([grid_nd, cur_base_anchors_nd, stride_nd], axis=-1)
+        all_anchors.append(np.reshape(anchors, [-1, 6]))
+    all_anchors = np.concatenate(all_anchors, axis=0) / ([input_shape[0], input_shape[1]] * 3)
+    return functional.convert_to_tensor(all_anchors.astype("float32"))  # [center_h, center_w, anchor_h, anchor_w, stride_h, stride_w]
 
 
-def get_anchors_mode_by_anchors(input_shape, total_anchors, num_anchors="auto", pyramid_levels_min=3, num_anchors_at_each_level_cumsum=None):
+def get_anchors_mode_by_anchors(input_shape, total_anchors, num_anchors="auto", pyramid_levels_min=3, num_anchors_at_each_level_cumsum=None, regression_len=4):
     if num_anchors_at_each_level_cumsum is None:
         feature_sizes = get_feature_sizes(input_shape, [pyramid_levels_min, pyramid_levels_min + 10])[pyramid_levels_min:]
-        feature_sizes = tf.convert_to_tensor(feature_sizes, dtype="int32")
-        num_anchors_at_each_level_cumsum = tf.cumsum(tf.reduce_prod(feature_sizes, axis=-1))
+        feature_sizes = np.array(feature_sizes, dtype="int32")
+        num_anchors_at_each_level_cumsum = np.cumsum(np.prod(feature_sizes, axis=-1))
 
     if num_anchors == "auto":
         # Pick from [1, 3, 9], 1 for anchor_free, 3 for yolor, 9 for efficientdet
-        picks = tf.convert_to_tensor([1, 3, 9], dtype=tf.int32)
+        picks = np.array([1, 3, 9], dtype="int32")
         max_anchors = num_anchors_at_each_level_cumsum[-1] * picks
-        num_anchors = tf.math.ceil(total_anchors / max_anchors[0]) if total_anchors > max_anchors[-1] else picks[tf.argmax(total_anchors < max_anchors)]
+        num_anchors = np.ceil(total_anchors / max_anchors[0]) if total_anchors > max_anchors[-1] else picks[np.argmax(total_anchors < max_anchors)]
         num_anchors = int(num_anchors)
-    dd = {1: ANCHOR_FREE_MODE, 3: YOLOR_MODE, 9: EFFICIENTDET_MODE}
+    dd = {1: ANCHOR_FREE_MODE if regression_len == 4 else YOLOV8_MODE, 3: YOLOR_MODE, 9: EFFICIENTDET_MODE}
     return dd.get(num_anchors, EFFICIENTDET_MODE), num_anchors
 
 
 def get_pyramid_levels_by_anchors(input_shape, total_anchors, num_anchors="auto", pyramid_levels_min=3):
     feature_sizes = get_feature_sizes(input_shape, [pyramid_levels_min, pyramid_levels_min + 10])[pyramid_levels_min:]
-    feature_sizes = tf.convert_to_tensor(feature_sizes, dtype="int32")
-    num_anchors_at_each_level_cumsum = tf.cumsum(tf.reduce_prod(feature_sizes, axis=-1))
+    feature_sizes = np.array(feature_sizes, dtype="int32")
+    num_anchors_at_each_level_cumsum = np.cumsum(np.prod(feature_sizes, axis=-1))
     if num_anchors == "auto":
         _, num_anchors = get_anchors_mode_by_anchors(input_shape, total_anchors, num_anchors, pyramid_levels_min, num_anchors_at_each_level_cumsum)
 
     total_anchors = total_anchors // num_anchors
-    pyramid_levels_max = pyramid_levels_min + tf.argmax(num_anchors_at_each_level_cumsum > total_anchors) - 1
+    pyramid_levels_max = pyramid_levels_min + np.argmax(num_anchors_at_each_level_cumsum > total_anchors) - 1
     return [pyramid_levels_min, int(pyramid_levels_max)]
 
 
-""" Assign achors """
+""" Bbox functions """
 
 
 def iou_nd(bboxes, anchors):
     # bboxes: [[top, left, bottom, right]], anchors: [[top, left, bottom, right]]
-    anchors_nd, bboxes_nd = tf.expand_dims(anchors, 0), tf.expand_dims(bboxes, 1)
-    inter_top_left = tf.maximum(anchors_nd[:, :, :2], bboxes_nd[:, :, :2])
-    inter_bottom_right = tf.minimum(anchors_nd[:, :, 2:], bboxes_nd[:, :, 2:])
-    inter_hw = tf.maximum(inter_bottom_right - inter_top_left, 0)
+    anchors_nd, bboxes_nd = functional.expand_dims(anchors, 0), functional.expand_dims(bboxes, 1)
+    inter_top_left = functional.maximum(anchors_nd[:, :, :2], bboxes_nd[:, :, :2])
+    inter_bottom_right = functional.minimum(anchors_nd[:, :, 2:], bboxes_nd[:, :, 2:])
+    inter_hw = functional.maximum(inter_bottom_right - inter_top_left, 0)
     inter_area = inter_hw[:, :, 0] * inter_hw[:, :, 1]
 
     bboxes_area = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])
     anchors_area = (anchors[:, 2] - anchors[:, 0]) * (anchors[:, 3] - anchors[:, 1])
-    union_area = (tf.expand_dims(bboxes_area, 1) + tf.expand_dims(anchors_area, 0)) - inter_area
+    union_area = (functional.expand_dims(bboxes_area, 1) + functional.expand_dims(anchors_area, 0)) - inter_area
     return inter_area / union_area
 
 
 def corners_to_center_yxhw_nd(ss):
     """input: [top, left, bottom, right], output: [center_h, center_w], [height, width]"""
     return (ss[:, :2] + ss[:, 2:]) * 0.5, ss[:, 2:] - ss[:, :2]
 
 
 def center_yxhw_to_corners_nd(ss):
     """input: [center_h, center_w, height, width], output: [top, left, bottom, right]"""
     top_left = ss[:, :2] - ss[:, 2:] * 0.5
     bottom_right = top_left + ss[:, 2:]
-    return tf.concat([top_left, bottom_right], axis=-1)
+    return functional.concat([top_left, bottom_right], axis=-1)
 
 
-def decode_bboxes(preds, anchors, return_centers=False):
-    preds_center, preds_hw, preds_others = tf.split(preds, [2, 2, -1], axis=-1)
-    if anchors.shape[-1] == 6:  # Currently, it's yolor anchors
-        # anchors: [grid_y, grid_x, base_anchor_y, base_anchor_x, stride_y, stride_x]
-        bboxes_center = preds_center * 2 * anchors[:, 4:] + anchors[:, :2]
-        bboxes_hw = (preds_hw * 2) ** 2 * anchors[:, 2:4]
-    else:
-        anchors_hw = anchors[:, 2:] - anchors[:, :2]
-        anchors_center = (anchors[:, :2] + anchors[:, 2:]) * 0.5
+def _efficientdet_decode_bboxes(preds, anchors):
+    preds_center, preds_hw, preds_others = functional.split(preds, [2, 2, -1], axis=-1)
+
+    anchors_hw = anchors[:, 2:] - anchors[:, :2]
+    anchors_center = (anchors[:, :2] + anchors[:, 2:]) * 0.5
+
+    bboxes_center = preds_center * anchors_hw + anchors_center
+    bboxes_hw = functional.exp(preds_hw) * anchors_hw
+    return bboxes_center, bboxes_hw, preds_others
+
+
+def _yolor_decode_bboxes(preds, anchors):
+    preds_center, preds_hw, preds_others = functional.split(preds, [2, 2, -1], axis=-1)
+
+    # anchors: [grid_y, grid_x, base_anchor_y, base_anchor_x, stride_y, stride_x]
+    bboxes_center = preds_center * 2 * anchors[:, 4:] + anchors[:, :2]
+    bboxes_hw = (preds_hw * 2) ** 2 * anchors[:, 2:4]
+    return bboxes_center, bboxes_hw, preds_others
+
+
+def _yolov8_decode_bboxes(preds, anchors, regression_len=64):
+    preds_bbox, preds_others = functional.split(preds, [regression_len, -1], axis=-1)
+    preds_bbox = functional.reshape(preds_bbox, [*preds_bbox.shape[:-1], 4, regression_len // 4])
+    preds_bbox = functional.softmax(preds_bbox, axis=-1) * functional.range(preds_bbox.shape[-1], dtype="float32")
+    preds_bbox = functional.reduce_sum(preds_bbox, axis=-1)
+    preds_top_left, preds_bottom_right = functional.split(preds_bbox, [2, 2], axis=-1)
+
+    anchors_hw = anchors[:, 2:] - anchors[:, :2]
+    anchors_center = (anchors[:, :2] + anchors[:, 2:]) * 0.5
 
-        bboxes_center = preds_center * anchors_hw + anchors_center
-        bboxes_hw = tf.math.exp(preds_hw) * anchors_hw
+    bboxes_center = (preds_bottom_right - preds_top_left) / 2 * anchors_hw + anchors_center
+    bboxes_hw = (preds_bottom_right + preds_top_left) * anchors_hw
+    return bboxes_center, bboxes_hw, preds_others
+
+
+def decode_bboxes(preds, anchors, regression_len=4, return_centers=False):
+    if anchors.shape[-1] == 6:  # Currently, it's yolor / yolov7 anchors
+        bboxes_center, bboxes_hw, preds_others = _yolor_decode_bboxes(preds, anchors)
+    elif regression_len > 4:  # YOLOV8
+        bboxes_center, bboxes_hw, preds_others = _yolov8_decode_bboxes(preds, anchors, regression_len)
+    else:  # Currently, it's yolox / efficientdet anchors
+        bboxes_center, bboxes_hw, preds_others = _efficientdet_decode_bboxes(preds, anchors)
 
     if return_centers:
-        return tf.concat([bboxes_center, bboxes_hw, preds_others], axis=-1)
+        return functional.concat([bboxes_center, bboxes_hw, preds_others], axis=-1)
     else:
         preds_top_left = bboxes_center - 0.5 * bboxes_hw
         pred_bottom_right = preds_top_left + bboxes_hw
-        return tf.concat([preds_top_left, pred_bottom_right, preds_others], axis=-1)
+        return functional.concat([preds_top_left, pred_bottom_right, preds_others], axis=-1)
+
+
+""" Assign achors """
 
 
 def assign_anchor_classes_by_iou_with_bboxes(bbox_labels, anchors, ignore_threshold=0.4, overlap_threshold=0.5):
+    import tensorflow as tf
+
     num_anchors = anchors.shape[0]
     bbox_labels = tf.gather_nd(bbox_labels, tf.where(bbox_labels[:, -1] > 0))
     bboxes, labels = bbox_labels[:, :4], bbox_labels[:, 4]
 
     anchor_ious = iou_nd(bboxes, anchors)  # [num_bboxes, num_anchors]
     anchor_best_iou_ids = tf.argmax(anchor_ious, axis=0)  # [num_anchors]
     # anchor_best_ious = tf.gather_nd(anchor_ious, tf.stack([anchor_best_iou_ids, tf.range(num_anchors, dtype=anchor_best_iou_ids.dtype)], axis=-1))
@@ -266,14 +312,16 @@
     >>> anchors = anchors_func.get_yolor_anchors(mm.input_shape[1:-1], pyramid_levels=[3, 5], is_for_training=False)
     >>> assigned, anchors = assigned[assigned[:, -1] > 0], anchors[assigned[:, -1] > 0]
     >>> decoded_centers = (assigned[:, :2] + 0.5) * anchors[:, 4:] + anchors[:, :2]
     >>> decoded_hw = assigned[:, 2:4] * anchors[:, 4:]  # assigned[:, 2:4] is multiplied with feature_size ==> assigned[:, 2:4] * strides / input_shape
     >>> decoded_corner = anchors_func.center_yxhw_to_corners_nd(tf.concat([decoded_centers, decoded_hw], axis=-1))
     >>> data.show_image_with_bboxes(test_images.dog_cat(), decoded_corner, assigned[:, -1:])
     """
+    import tensorflow as tf
+
     bbox_labels = tf.gather_nd(bbox_labels, tf.where(bbox_labels[:, -1] > 0))
     bboxes, labels = bbox_labels[:, :4], bbox_labels[:, 4:]
     num_anchors, num_bboxes_true, num_output_channels = anchor_ratios.shape[1], tf.shape(bboxes)[0], bbox_labels.shape[-1]
 
     rrs = []
     # for anchor_ratio, feature_size in zip(anchor_ratios, feature_sizes):
     for id in range(feature_sizes.shape[0]):
@@ -346,15 +394,15 @@
     # Actual assigning test:
     >>> from keras_cv_attention_models import yolox, test_images
     >>> from keras_cv_attention_models.coco import anchors_func, data
     >>> mm = yolox.YOLOXS()
     >>> img = test_images.dog_cat()
     >>> pred = mm(mm.preprocess_input(img))
 
-    >>> aa = anchors_func.AnchorFreeAssignMatching([640, 640])
+    >>> aa = anchors_func.AnchorFreeAssignMatching(mm.input_shape[1:-1])
     >>> bbs, lls, ccs = mm.decode_predictions(pred)[0]
     >>> bbox_labels_true = tf.concat([bbs, tf.one_hot(lls, 80), tf.ones([bbs.shape[0], 1])], axis=-1)
     >>> bbox_labels_true_assined = aa(bbox_labels_true, pred[0])
     >>> bboxes_true, bboxes_true_encoded, labels_true, object_true_idx_nd = tf.split(bbox_labels_true_assined, [4, 4, -1, 1], axis=-1)
     >>> object_true_idx_nd = tf.cast(object_true_idx_nd, tf.int32)
     >>> object_true_idx = object_true_idx_nd[:, 0]
     >>> object_true = tf.tensor_scatter_nd_update(tf.zeros_like(pred[0, :, -1]), object_true_idx_nd, tf.ones_like(bboxes_true[:, -1]))
@@ -367,27 +415,41 @@
     >>> # Show gathered bbox ground truth
     >>> data.show_image_with_bboxes(img, bboxes_true, labels_true.numpy().argmax(-1), labels_true.numpy().max(-1))
     >>> # Show gathered encoded, bbox ground truth
     >>> bboxes_true_decode = anchors_func.decode_bboxes(bboxes_true_encoded, anchors)
     >>> data.show_image_with_bboxes(img, bboxes_true_decode, labels_true.numpy().argmax(-1), labels_true.numpy().max(-1))
     """
 
-    def __init__(self, input_shape, pyramid_levels=[3, 5], center_radius=2.5, topk_ious_max=10, grid_zero_start=True, epsilon=1e-8):
-        self.center_radius, self.topk_ious_max, self.epsilon = center_radius, topk_ious_max, epsilon
-        self.input_shape, self.grid_zero_start = input_shape, grid_zero_start
+    def __init__(
+        self,
+        input_shape,
+        pyramid_levels=[3, 5],
+        center_radius=2.5,
+        topk_ious_max=10,
+        grid_zero_start=True,  # False for YOLOV8, True for YOLOX
+        use_object_scores=True,  # False for YOLOV8, True for YOLOX
+        regression_len=4,  # 64 fro YOLOV8, 4 for YOLOX
+        with_encoded_bboxes=True,  # False for YOLOX not using l1 loss, else False
+        epsilon=1e-8,
+    ):
+        self.center_radius, self.topk_ious_max, self.with_encoded_bboxes, self.epsilon = center_radius, topk_ious_max, with_encoded_bboxes, epsilon
+        self.input_shape, self.grid_zero_start, self.use_object_scores, self.regression_len = input_shape, grid_zero_start, use_object_scores, regression_len
         # pyramid_levels = get_pyramid_levels_by_num_anchors(self.input_shape, self.num_anchors)
         self.anchors = get_anchors(self.input_shape, pyramid_levels, aspect_ratios=[1], num_scales=1, anchor_scale=1, grid_zero_start=self.grid_zero_start)
 
         # Anchors constant values
         self.anchors_centers = (self.anchors[:, :2] + self.anchors[:, 2:]) * 0.5
         self.anchors_hws = self.anchors[:, 2:] - self.anchors[:, :2]
         self.anchors_nd = tf.expand_dims(self.anchors, 0)  # [1, num_anchors, 4]
         self.anchors_centers_nd, self.anchors_hws_nd = tf.expand_dims(self.anchors_centers, 0), tf.expand_dims(self.anchors_hws, 0)  # [1, num_anchors, 2]
         self.centers_enlarge_nd = self.anchors_hws_nd * self.center_radius
 
+        self.__decode_bboxes__ = self.__yolov8_decode_bboxes__ if regression_len > 4 else self.__yolox_decode_bboxes__
+        self.__encode_bboxes__ = self.__yolov8_encode_bboxes__ if regression_len > 4 else self.__yolox_encode_bboxes__
+
     def __picking_anchors_by_center_within_bboxes__(self, bboxes_true_nd):
         # get_in_boxes_info https://github.com/Megvii-BaseDetection/YOLOX/tree/master/yolox/models/yolo_head.py#L522
         # bboxes: [[top, left, bottom, right]], anchors: [[top, left, bottom, right]]
         # anchors_centers_nd: [1, num_anchors, 2], bboxes_true_nd: [num_bboxes, 1, 4]
         # is_anchor_in_bbox: [num_bboxes, num_anchors, 2]
         is_anchor_in_bbox = tf.logical_and(bboxes_true_nd[:, :, :2] < self.anchors_centers_nd, bboxes_true_nd[:, :, 2:] > self.anchors_centers_nd)
         is_anchor_in_bbox = tf.reduce_all(is_anchor_in_bbox, axis=-1)  # All 4 points matching: [num_bboxes, num_anchors]
@@ -395,29 +457,50 @@
         bboxes_centers_nd = (bboxes_true_nd[:, :, :2] + bboxes_true_nd[:, :, 2:]) * 0.5
         is_anchor_in_center_top_left = self.anchors_centers_nd > (bboxes_centers_nd - self.centers_enlarge_nd)
         is_anchor_in_center_bottom_right = self.anchors_centers_nd < (bboxes_centers_nd + self.centers_enlarge_nd)
         is_anchor_in_center = tf.logical_and(is_anchor_in_center_top_left, is_anchor_in_center_bottom_right)
         is_anchor_in_center = tf.reduce_all(is_anchor_in_center, axis=-1)
         return is_anchor_in_bbox, is_anchor_in_center
 
-    def __decode_bboxes__(self, bboxes_pred, anchors_centers, anchors_hws):
+    def __yolox_decode_bboxes__(self, bboxes_pred, anchors_centers, anchors_hws):
         bboxes_pred_center = bboxes_pred[:, :2] * anchors_hws + anchors_centers
         bboxes_pred_hw = tf.math.exp(bboxes_pred[:, 2:]) * anchors_hws
         bboxes_pred_top_left = bboxes_pred_center - 0.5 * bboxes_pred_hw
         bboxes_pred_bottom_right = bboxes_pred_top_left + bboxes_pred_hw
         return bboxes_pred_top_left, bboxes_pred_bottom_right, bboxes_pred_center, bboxes_pred_hw
 
-    def __encode_bboxes__(self, bboxes_true, anchors_centers, anchors_hws):
+    def __yolox_encode_bboxes__(self, bboxes_true, anchors_centers, anchors_hws):
         # bboxes_true_center, bboxes_true_hw = corners_to_center_yxhw_nd(bboxes_true)
         bboxes_true_hw = bboxes_true[:, 2:] - bboxes_true[:, :2]
         bboxes_true_center = (bboxes_true[:, 2:] + bboxes_true[:, :2]) / 2.0
         bboxes_true_center_encoded = (bboxes_true_center - anchors_centers) / anchors_hws
         bboxes_true_hw_encoded = tf.math.log(bboxes_true_hw / anchors_hws + self.epsilon)
         return tf.concat([bboxes_true_center_encoded, bboxes_true_hw_encoded], axis=-1)
 
+    def __yolov8_decode_bboxes__(self, bboxes_pred, anchors_centers, anchors_hws):
+        regression_len = bboxes_pred.shape[-1] // 4
+        bboxes_pred = functional.reshape(bboxes_pred, [-1, *bboxes_pred.shape[1:-1], 4, regression_len])
+        bboxes_pred = functional.softmax(bboxes_pred, axis=-1) * functional.range(regression_len, dtype="float32")
+        bboxes_pred = functional.reduce_sum(bboxes_pred, axis=-1)
+        preds_top_left, preds_bottom_right = functional.split(bboxes_pred, [2, 2], axis=-1)
+
+        preds_top_left, preds_bottom_right = preds_top_left * anchors_hws, preds_bottom_right * anchors_hws
+        bboxes_pred_hw = preds_top_left + preds_bottom_right
+        bboxes_pred_top_left = anchors_centers - preds_top_left
+        bboxes_pred_bottom_right = anchors_centers + preds_bottom_right
+        bboxes_pred_center = (bboxes_pred_top_left + bboxes_pred_bottom_right) / 2
+        return bboxes_pred_top_left, bboxes_pred_bottom_right, bboxes_pred_center, bboxes_pred_hw
+
+    def __yolov8_encode_bboxes__(self, bboxes_true, anchors_centers, anchors_hws):
+        bboxes_true_top_left, bboxes_true_bottom_right = bboxes_true[:, :2], bboxes_true[:, 2:]
+        bboxes_true_top_left_encoded = (anchors_centers - bboxes_true_top_left) / anchors_hws
+        bboxes_true_bottom_right_encoded = (bboxes_true_bottom_right - anchors_centers) / anchors_hws
+        out = tf.concat([bboxes_true_top_left_encoded, bboxes_true_bottom_right_encoded], axis=-1)
+        return tf.clip_by_value(out, 0, self.regression_len / 4 - 1.01)
+
     def __center_iou_nd__(self, bboxes_true_nd, bboxes_pred_top_left, bboxes_pred_bottom_right, bboxes_pred_hw):
         # bboxes_true_nd: [num_bboxes, 1, *[top, left, bottom, right]]
         bboxes_pred_top_left_nd, bboxes_pred_bottom_right_nd = tf.expand_dims(bboxes_pred_top_left, 0), tf.expand_dims(bboxes_pred_bottom_right, 0)
         inter_top_left = tf.maximum(bboxes_pred_top_left_nd, bboxes_true_nd[:, :, :2])
         inter_bottom_right = tf.minimum(bboxes_pred_bottom_right_nd, bboxes_true_nd[:, :, 2:])
         inter_hw = tf.maximum(inter_bottom_right - inter_top_left, 0)
         inter_area = inter_hw[:, :, 0] * inter_hw[:, :, 1]
@@ -449,14 +532,17 @@
         return tf.cond(
             tf.reduce_any(check_cond),
             lambda: self.__filter_anchors_matching_multi_bboxes__(topk_anchors, cost, check_cond),
             lambda: topk_anchors,
         )
 
     def __call__(self, bbox_labels_true, bbox_labels_pred):
+        import tensorflow as tf
+        from tensorflow.keras import backend as K
+
         # get_assignments https://github.com/Megvii-BaseDetection/YOLOX/tree/master/yolox/models/yolo_head.py#425
         bbox_labels_true = tf.gather_nd(bbox_labels_true, tf.where(bbox_labels_true[:, -1] > 0))
 
         bboxes_true, labels_true = bbox_labels_true[:, :4], bbox_labels_true[:, 4:-1]
         bboxes_true_nd = tf.expand_dims(bboxes_true, 1)
 
         # is_anchor_in_bbox, is_anchor_in_center: [num_bboxes, num_anchors]
@@ -464,24 +550,28 @@
         # [num_anchors]
         is_anchor_match_any_bbox = tf.logical_or(tf.reduce_any(is_anchor_in_bbox, axis=0), tf.reduce_any(is_anchor_in_center, axis=0))
         pick_cond = tf.where(is_anchor_match_any_bbox)[:, 0]
         # [num_bboxes, num_picked_anchors]
         is_anchor_valid = tf.logical_and(tf.gather(is_anchor_in_bbox, pick_cond, axis=-1), tf.gather(is_anchor_in_center, pick_cond, axis=-1))
 
         bbox_labels_pred = bbox_labels_pred[is_anchor_match_any_bbox]
-        bboxes_pred, labels_pred, object_pred = bbox_labels_pred[:, :4], bbox_labels_pred[:, 4:-1], bbox_labels_pred[:, -1:]
+        if self.use_object_scores:
+            bboxes_pred, labels_pred, object_pred = tf.split(bbox_labels_pred, [self.regression_len, -1, 1], axis=-1)
+            obj_labels_pred = tf.sqrt(labels_pred * object_pred)
+        else:
+            bboxes_pred, labels_pred = tf.split(bbox_labels_pred, [self.regression_len, -1], axis=-1)
+            obj_labels_pred = tf.sqrt(labels_pred)
 
         # decode_bboxes
         anchors_centers, anchors_hws = self.anchors_centers[is_anchor_match_any_bbox], self.anchors_hws[is_anchor_match_any_bbox]
         bboxes_pred_top_left, bboxes_pred_bottom_right, bboxes_pred_center, bboxes_pred_hw = self.__decode_bboxes__(bboxes_pred, anchors_centers, anchors_hws)
 
         ious = self.__center_iou_nd__(bboxes_true_nd, bboxes_pred_top_left, bboxes_pred_bottom_right, bboxes_pred_hw)  # [num_bboxes, num_picked_anchors]
         ious_loss = -tf.math.log(ious + self.epsilon)
 
-        obj_labels_pred = tf.sqrt(labels_pred * object_pred)
         cls_loss = K.binary_crossentropy(tf.expand_dims(labels_true, 1), tf.expand_dims(obj_labels_pred, 0))  # [num_bboxes, num_picked_anchors, num_classes]
         cls_loss = tf.reduce_sum(cls_loss, -1)  # [num_bboxes, num_picked_anchors]
         cost = cls_loss + 3.0 * ious_loss + 1e5 * tf.cast(tf.logical_not(is_anchor_valid), cls_loss.dtype)  # [num_bboxes, num_picked_anchors]
 
         # dynamic_k_matching
         bbox_matched_k_anchors = self.__dynamic_k_matching__(ious, cost)  # [num_bboxes, num_picked_anchors], contains only 0, 1
         is_anchor_iou_match_any = tf.reduce_any(bbox_matched_k_anchors > 0, axis=0)  # [num_picked_anchors]
@@ -494,16 +584,23 @@
 
         # get_losses after get_assignments. Bboxes for iou loss, [top, left, bottom, right]
         out_bboxes_true = tf.gather(bboxes_true, anchor_best_matching_bbox)
         out_labels_true = anchor_labels * tf.expand_dims(pred_iou_loss, -1)
 
         # object loss, [num_anchors]
         out_object_true = tf.tensor_scatter_nd_update(is_anchor_match_any_bbox, tf.where(is_anchor_match_any_bbox), is_anchor_iou_match_any)
-        object_true_idx = tf.where(out_object_true)  # [num_picked_anchors, 1]
-
-        # l1_target loss, encoded [center_top, center_left, height, width]
-        anchors_centers_valid = tf.gather_nd(anchors_centers, is_anchor_iou_match_any_idx)
-        anchors_hws_valid = tf.gather_nd(anchors_hws, is_anchor_iou_match_any_idx)
-        out_bboxes_true_encoded = self.__encode_bboxes__(out_bboxes_true, anchors_centers_valid, anchors_hws_valid)
-
-        # tf.stop_gradient requires returning value been a single tensor with same dtype as inputs.
-        return tf.concat([out_bboxes_true, out_bboxes_true_encoded, out_labels_true, tf.cast(object_true_idx, out_bboxes_true.dtype)], axis=-1)
+        if tf.executing_eagerly():
+            object_true_idx = tf.where(out_object_true)  # [num_picked_anchors, 1]
+        else:
+            with tf.xla.experimental.jit_scope(compile_ops=False):
+                object_true_idx = tf.where(out_object_true)  # [num_picked_anchors, 1]
+
+        if self.with_encoded_bboxes:
+            # l1_target loss, encoded [center_top, center_left, height, width]
+            anchors_centers_valid = tf.gather_nd(anchors_centers, is_anchor_iou_match_any_idx)
+            anchors_hws_valid = tf.gather_nd(anchors_hws, is_anchor_iou_match_any_idx)
+            out_bboxes_true_encoded = self.__encode_bboxes__(out_bboxes_true, anchors_centers_valid, anchors_hws_valid)
+
+            # tf.stop_gradient requires returning value been a single tensor with same dtype as inputs.
+            return tf.concat([out_bboxes_true, out_bboxes_true_encoded, out_labels_true, tf.cast(object_true_idx, out_bboxes_true.dtype)], axis=-1)
+        else:
+            return tf.concat([out_bboxes_true, out_labels_true, tf.cast(object_true_idx, out_bboxes_true.dtype)], axis=-1)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/data.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/tf_data.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,14 @@
+import os
 import tensorflow as tf
 from tensorflow import keras
-from keras_cv_attention_models.imagenet.data import init_mean_std_by_rescale_mode, tf_imread, random_crop_and_resize_image
+from keras_cv_attention_models.common_layers import init_mean_std_by_rescale_mode
+from keras_cv_attention_models.imagenet.data import tf_imread, random_crop_and_resize_image
 from keras_cv_attention_models.coco import anchors_func
-
-COCO_LABELS = """person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign,
-    parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie,
-    suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket,
-    bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut,
-    cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven,
-    toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush"""
-COCO_80_LABEL_DICT = {id: ii.strip() for id, ii in enumerate(COCO_LABELS.split(","))}
-INVALID_ID_90 = [11, 25, 28, 29, 44, 65, 67, 68, 70, 82]
-COCO_90_LABEL_DICT = {id: ii for id, ii in zip(set(range(90)) - set(INVALID_ID_90), COCO_80_LABEL_DICT.values())}
-COCO_90_LABEL_DICT.update({ii: "Unknown" for ii in INVALID_ID_90})
-COCO_80_to_90_LABEL_DICT = {id_80: id_90 for id_80, id_90 in enumerate(set(range(90)) - set(INVALID_ID_90))}
+from keras_cv_attention_models.plot_func import draw_bboxes, show_image_with_bboxes
 
 
 """ Bboxes augment """
 
 
 def rerange_scale_offset_to_01(source_height, source_width, target_height, target_width, scale_hh, scale_ww, offset_hh=0, offset_ww=0):
     # Input: image size firstly rescale with (scale_hh, scale_ww), then crop as [offset_hh: offset_hh + target_height, offset_ww: offset_ww + target_width]
@@ -110,14 +101,15 @@
 
 def aspect_aware_resize_and_crop_image(image, target_shape, scale=-1, crop_y=0, crop_x=0, letterbox_pad=-1, method="bilinear", antialias=False):
     letterbox_target_shape = (target_shape[0] - letterbox_pad, target_shape[1] - letterbox_pad) if letterbox_pad > 0 else target_shape
     height, width = tf.cast(tf.shape(image)[0], "float32"), tf.cast(tf.shape(image)[1], "float32")
     if scale == -1:
         scale = tf.minimum(letterbox_target_shape[0] / height, letterbox_target_shape[1] / width)
     scaled_hh, scaled_ww = int(height * scale), int(width * scale)
+    # image = tf.cast(image, "float32")
     image = tf.image.resize(image, [scaled_hh, scaled_ww], method=method, antialias=antialias)
     image = image[crop_y : crop_y + letterbox_target_shape[0], crop_x : crop_x + letterbox_target_shape[1]]
     cropped_shape = tf.shape(image)
 
     pad_top, pad_left = ((target_shape[0] - cropped_shape[0]) // 2, (target_shape[1] - cropped_shape[1]) // 2) if letterbox_pad >= 0 else (0, 0)
     image = tf.image.pad_to_bounding_box(image, pad_top, pad_left, target_shape[0], target_shape[1])
     return image, scale, pad_top, pad_left
@@ -372,18 +364,30 @@
 
 
 def detection_dataset_from_custom_json(data_path, with_info=False):
     import json
 
     with open(data_path, "r") as ff:
         aa = json.load(ff)
-
     test_key = "validation" if "validation" in aa else "test"
-    train, test, info = aa["train"], aa[test_key], aa["info"]
-    total_images, num_classes = len(train), info["num_classes"]
+    train, test, info = aa["train"], aa[test_key], aa.get("info", {})
+    total_images, num_classes = len(train), info.get("num_classes", 0)
+    if num_classes <= 0:
+        num_classes = max([max([int(jj) for jj in ii["objects"]["label"]]) for ii in train]) + 1
+        print(">>>> Using max value from train as num_classes:", num_classes)
+
+    if "base_path" in info and len(info["base_path"]) > 0:
+        base_path = os.path.expanduser(info["base_path"])
+        for ii in train:
+            ii["image"] = os.path.join(base_path, ii["image"])
+            ii["objects"]["bbox"] = tf.reshape(ii["objects"]["bbox"], [-1, 4])
+        for ii in test:
+            ii["image"] = os.path.join(base_path, ii["image"])
+            ii["objects"]["bbox"] = tf.reshape(ii["objects"]["bbox"], [-1, 4])
+
     objects_signature = {"bbox": tf.TensorSpec(shape=(None, 4), dtype=tf.float32), "label": tf.TensorSpec(shape=(None,), dtype=tf.int64)}
     output_signature = {"image": tf.TensorSpec(shape=(), dtype=tf.string), "objects": objects_signature}
     train_ds = tf.data.Dataset.from_generator(lambda: (ii for ii in train), output_signature=output_signature)
     test_ds = tf.data.Dataset.from_generator(lambda: (ii for ii in test), output_signature=output_signature)
 
     options = tf.data.Options()
     options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
@@ -413,22 +417,23 @@
     color_augment_method="random_hsv",  # [augment] one of ["random_hsv", "autoaug", "randaug"], or totally custom one like `lambda image: image`
     positional_augment_methods="rts",  # [augment] Positional augment method besides scale, combine of r: rotate, t: transplate, s: shear, x: scale_x + scale_y
     magnitude=0,
     num_layers=2,
     seed=None,
     **augment_kwargs,  # Too many...
 ):
-    import tensorflow_datasets as tfds
-
     is_tpu = True if len(tf.config.list_logical_devices("TPU")) > 0 else False  # Set True for try_gcs and drop_remainder
+    try_gcs, drop_remainder = is_tpu, is_tpu
 
     if data_name.endswith(".json"):
         dataset, total_images, num_classes = detection_dataset_from_custom_json(data_name, with_info=True)
     else:
-        dataset, info = tfds.load(data_name, with_info=True, try_gcs=is_tpu)
+        import tensorflow_datasets as tfds
+
+        dataset, info = tfds.load(data_name, with_info=True, try_gcs=try_gcs)
         num_classes = info.features["objects"]["label"].num_classes
         total_images = info.splits["train"].num_examples
     steps_per_epoch = int(tf.math.ceil(total_images / float(batch_size)))
     if info_only:
         return total_images, num_classes, steps_per_epoch
 
     AUTOTUNE = tf.data.AUTOTUNE
@@ -440,15 +445,15 @@
         resize_method=resize_method,
         resize_antialias=resize_antialias,
         color_augment_method=color_augment_method,
         magnitude=magnitude,
         num_layers=num_layers,
         **augment_kwargs,
     )
-    train_dataset = dataset["train"].shuffle(buffer_size, seed=seed).map(train_process).batch(batch_size, drop_remainder=is_tpu)
+    train_dataset = dataset["train"].shuffle(buffer_size, seed=seed).map(train_process).batch(batch_size, drop_remainder=drop_remainder)
     # return train_dataset
 
     if mosaic_mix_prob > 0:
         mosaic_mix = lambda xx, yy: tf.cond(
             tf.random.uniform(()) > mosaic_mix_prob,
             lambda: (xx, yy),
             lambda: mosaic_mix_batch(xx, yy[0], yy[1]),
@@ -459,15 +464,15 @@
     if magnitude > 0 and positional_augment_methods is not None and len(positional_augment_methods) != 0:
         # Apply randaug rotate / shear / transform after mosaic mix
         max_labels_per_image = (max_labels_per_image * 4) if mosaic_mix_prob > 0 else max_labels_per_image
         pos_aug = PositionalRandAugmentWithBboxes(magnitude, num_layers, max_labels_per_image, positional_augment_methods, **augment_kwargs)
         print(">>>> positional augment methods:", pos_aug.pos_randaug.available_ops)
         train_dataset = train_dataset.map(pos_aug, num_parallel_calls=AUTOTUNE)
 
-    if anchors_mode == anchors_func.ANCHOR_FREE_MODE:  # == "anchor_free"
+    if anchors_mode == anchors_func.ANCHOR_FREE_MODE or anchors_mode == anchors_func.YOLOV8_MODE:  # == "anchor_free"
         # Don't need anchors here, anchor assigning is after getting model predictions.
         bbox_process = lambda bb: to_one_hot_with_class_mark(tf.concat([bb[0], tf.cast(tf.expand_dims(bb[1], -1), bb[0].dtype)], axis=-1), num_classes)
     elif anchors_mode == anchors_func.YOLOR_MODE:  # == "yolor":
         anchor_ratios, feature_sizes = anchors_func.get_yolor_anchors(input_shape[:2], anchor_pyramid_levels, is_for_training=True)
         total_anchors = tf.cast(anchor_ratios.shape[1] * tf.reduce_sum(feature_sizes[:, 0] * feature_sizes[:, 1]), tf.int32)
         empty_label = tf.zeros([total_anchors, 4 + num_classes + 1])  # All 0
         bbox_process = lambda bb: __yolor_bboxes_labels_batch_func__(bb[0], bb[1], anchor_ratios, feature_sizes, empty_label, num_classes)
@@ -485,81 +490,27 @@
     train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
     # return train_dataset
 
     """ Test dataset """
     test_dataset = dataset.get("validation", dataset.get("test", None))
     if test_dataset is not None:
         test_process = RandomProcessImageWithBboxes(target_shape=input_shape, resize_method=resize_method, resize_antialias=resize_antialias, magnitude=-1)
-        test_dataset = test_dataset.map(test_process).batch(batch_size, drop_remainder=is_tpu).map(lambda xx, yy: (rescaling(xx), bbox_process(yy)))
+        test_dataset = test_dataset.map(test_process).batch(batch_size, drop_remainder=drop_remainder).map(lambda xx, yy: (rescaling(xx), bbox_process(yy)))
 
     return train_dataset, test_dataset, total_images, num_classes, steps_per_epoch
 
 
 """ Show """
 
 
-def draw_bboxes(bboxes, ax=None):
-    import matplotlib.pyplot as plt
-    import numpy as np
-
-    if ax is None:
-        fig, ax = plt.subplots()
-    bboxes = np.array(bboxes).astype("int32")
-    for bb in bboxes:
-        ax.plot(bb[[1, 1, 3, 3, 1]], bb[[0, 2, 2, 0, 0]])
-    plt.show()
-    return ax
-
-
-def show_image_with_bboxes(
-    image, bboxes, labels=None, confidences=None, is_bbox_width_first=False, ax=None, label_font_size=8, num_classes=80, indices_2_labels=None
-):
-    import matplotlib.pyplot as plt
-    import numpy as np
-
-    need_plt_show = False
-    if ax is None:
-        fig, ax = plt.subplots()
-        need_plt_show = True
-    ax.imshow(image)
-    bboxes = np.array(bboxes)
-    if is_bbox_width_first:
-        bboxes = bboxes[:, [1, 0, 3, 2]]
-    for id, bb in enumerate(bboxes):
-        # bbox is [top, left, bottom, right]
-        bb = [bb[0] * image.shape[0], bb[1] * image.shape[1], bb[2] * image.shape[0], bb[3] * image.shape[1]]
-        bb = np.array(bb).astype("int32")
-        ax.plot(bb[[1, 1, 3, 3, 1]], bb[[0, 2, 2, 0, 0]])
-
-        if labels is not None:
-            label = int(labels[id])
-            if indices_2_labels is not None:
-                label = indices_2_labels.get(label, indices_2_labels.get(str(label), "None"))
-            elif num_classes == 90:
-                label = COCO_90_LABEL_DICT[label]
-            elif num_classes == 80:
-                label = COCO_80_LABEL_DICT[label]
-
-            if confidences is not None:
-                label += ": {:.4f}".format(float(confidences[id]))
-            color = ax.lines[-1].get_color()
-            # ax.text(bb[1], bb[0] - 5, "label: {}, {}".format(label, COCO_80_LABEL_DICT[label]), color=color, fontsize=8)
-            ax.text(bb[1], bb[0] - 5, label, color=color, fontsize=label_font_size)
-    ax.set_axis_off()
-    plt.tight_layout()
-    if need_plt_show:
-        plt.show()
-    return ax
-
-
 def show_batch_sample(
     dataset, rescale_mode="torch", rows=-1, label_font_size=8, base_size=3, anchors_mode="efficientdet", indices_2_labels=None, **anchor_kwargs
 ):
     import matplotlib.pyplot as plt
-    from keras_cv_attention_models.visualizing import get_plot_cols_rows
+    from keras_cv_attention_models.plot_func import get_plot_cols_rows, show_image_with_bboxes
 
     if isinstance(dataset, (list, tuple)):
         images, labels = dataset
     else:
         images, labels = dataset.as_numpy_iterator().next()
     mean, std = init_mean_std_by_rescale_mode(rescale_mode)
     images = (images * std + mean) / 255
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/eval_func.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/eval_func.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,18 @@
 import os
-import tensorflow as tf
-from keras_cv_attention_models.coco import anchors_func, data
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, callbacks
+from keras_cv_attention_models.coco import anchors_func, info
+from keras_cv_attention_models.models import no_grad_if_torch
 from tqdm import tqdm
 
 
-@tf.keras.utils.register_keras_serializable(package="kecam/coco")
-class DecodePredictions(tf.keras.layers.Layer):
+@backend.register_keras_serializable(package="kecam/coco")
+class DecodePredictions(layers.Layer):
     """
     The most simple version decoding prediction and NMS:
 
     >>> from keras_cv_attention_models import efficientdet, test_images
     >>> model = efficientdet.EfficientDetD0()
     >>> preds = model(model.preprocess_input(test_images.dog()))
 
@@ -30,69 +33,79 @@
         input_shape=512,
         pyramid_levels=[3, 7],
         anchors_mode=None,
         use_object_scores="auto",
         anchor_scale="auto",
         aspect_ratios=(1, 2, 0.5),
         num_scales=3,
+        regression_len=4,  # bbox output len, typical value is 4, for yolov8 reg_max=16 -> regression_len = 16 * 4 == 64
         score_threshold=0.3,  # decode parameter, can be set new value in `self.call`
         iou_or_sigma=0.5,  # decode parameter, can be set new value in `self.call`
         max_output_size=100,  # decode parameter, can be set new value in `self.call`
         method="hard",  # decode parameter, can be set new value in `self.call`
         mode="global",  # decode parameter, can be set new value in `self.call`
         topk=0,  # decode parameter, can be set new value in `self.call`
         use_static_output=False,  # Set to True if using this as an actual layer, especially for converting tflite
+        use_sigmoid_on_score=False,  # wether applying sigmoid on score outputs. Set True if model is built using `classifier_activation=None`
         **kwargs,
     ):
         super().__init__(**kwargs)
 
         self.pyramid_levels = list(range(min(pyramid_levels), max(pyramid_levels) + 1))
         use_object_scores, num_anchors, anchor_scale = anchors_func.get_anchors_mode_parameters(anchors_mode, use_object_scores, "auto", anchor_scale)
-        self.aspect_ratios, self.num_scales = aspect_ratios, num_scales
+        self.regression_len, self.aspect_ratios, self.num_scales = regression_len, aspect_ratios, num_scales
         self.anchors_mode, self.use_object_scores, self.anchor_scale = anchors_mode, use_object_scores, anchor_scale  # num_anchors not using
-        if input_shape is not None and (isinstance(input_shape, (list, tuple)) and input_shape[0] is not None):
+        if input_shape is not None and (isinstance(input_shape, (list, tuple)) and input_shape[1] is not None):
             self.__init_anchor__(input_shape)
         else:
             self.anchors = None
         self.__input_shape__ = input_shape
-        self.use_static_output = use_static_output
+        self.use_static_output, self.use_sigmoid_on_score = use_static_output, use_sigmoid_on_score
         self.nms_kwargs = {
             "score_threshold": score_threshold,
             "iou_or_sigma": iou_or_sigma,
             "max_output_size": max_output_size,
             "method": method,
             "mode": mode,
             "topk": topk,
         }
         super().build(input_shape)
 
     def __init_anchor__(self, input_shape):
-        input_shape = input_shape[:2] if isinstance(input_shape, (list, tuple)) else (input_shape, input_shape)
+        if isinstance(input_shape, (list, tuple)) and len(input_shape) > 2:
+            # input_shape = input_shape[:2] if backend.image_data_format() == "channels_last" else input_shape[-2:]
+            channel_axis, channel_dim = min(enumerate(input_shape), key=lambda xx: xx[1])  # Assume the smallest value is the channel dimension
+            input_shape = [dim for axis, dim in enumerate(input_shape) if axis != channel_axis]
+        elif isinstance(input_shape, int):
+            input_shape = (input_shape, input_shape)
+
         if self.anchors_mode == anchors_func.ANCHOR_FREE_MODE:
             self.anchors = anchors_func.get_anchor_free_anchors(input_shape, self.pyramid_levels)
         elif self.anchors_mode == anchors_func.YOLOR_MODE:
             self.anchors = anchors_func.get_yolor_anchors(input_shape, self.pyramid_levels)
+        elif self.anchors_mode == anchors_func.YOLOV8_MODE:
+            self.anchors = anchors_func.get_anchor_free_anchors(input_shape, self.pyramid_levels, grid_zero_start=False)
         else:
             grid_zero_start = False
             self.anchors = anchors_func.get_anchors(input_shape, self.pyramid_levels, self.aspect_ratios, self.num_scales, self.anchor_scale, grid_zero_start)
         self.__input_shape__ = input_shape
         return self.anchors
 
     def __topk_class_boxes_single__(self, pred, topk=5000):
         # https://github.com/google/automl/tree/master/efficientdet/tf2/postprocess.py#L82
-        bbox_outputs, class_outputs = pred[:, :4], pred[:, 4:]
+        bbox_outputs, class_outputs = pred[:, : self.regression_len], pred[:, self.regression_len :]
         num_classes = class_outputs.shape[-1]
-        class_outputs_flatten = tf.reshape(class_outputs, -1)
-        topk = class_outputs_flatten.shape[0] if topk == -1 else topk  # select all if -1
-        _, class_topk_indices = tf.nn.top_k(class_outputs_flatten, k=topk, sorted=False)
+        class_outputs_flatten = functional.reshape(class_outputs, -1)
+        topk = class_outputs_flatten.shape[0] if topk == -1 else min(topk, class_outputs_flatten.shape[0])  # select all if -1
+        _, class_topk_indices = functional.top_k(class_outputs_flatten, k=topk, sorted=False)
         # get original indices for class_outputs, original_indices_hh -> picking indices, original_indices_ww -> picked labels
         original_indices_hh, original_indices_ww = class_topk_indices // num_classes, class_topk_indices % num_classes
-        class_indices = tf.stack([original_indices_hh, original_indices_ww], axis=-1)
-        scores_topk = tf.gather_nd(class_outputs, class_indices)
-        bboxes_topk = tf.gather(bbox_outputs, original_indices_hh)
+        class_indices = functional.stack([original_indices_hh, original_indices_ww], axis=-1)
+        scores_topk = functional.gather_nd(class_outputs, class_indices)
+        bboxes_topk = functional.gather(bbox_outputs, original_indices_hh)
         return bboxes_topk, scores_topk, original_indices_ww, original_indices_hh
 
     # def __nms_per_class__(self, bbs, ccs, labels, score_threshold=0.3, iou_threshold=0.5, soft_nms_sigma=0.5, max_output_size=100):
     #     # https://github.com/google/automl/tree/master/efficientdet/tf2/postprocess.py#L409
     #     # Not using, same result with `torchvision.ops.batched_nms`
     #     rrs = []
     #     for ii in tf.unique(labels)[0]:
@@ -108,60 +121,65 @@
     #     bboxes, labels, scores = rrs[:, :4], rrs[:, 4], rrs[:, -1]
     #     return bboxes.numpy(), labels.numpy(), scores.numpy()
 
     def __nms_per_class__(self, bbs, ccs, labels, score_threshold=0.3, iou_threshold=0.5, soft_nms_sigma=0.5, max_output_size=100):
         # From torchvision.ops.batched_nms strategy: in order to perform NMS independently per class. we add an offset to all the boxes.
         # The offset is dependent only on the class idx, and is large enough so that boxes from different classes do not overlap
         # Same result with per_class method: https://github.com/google/automl/tree/master/efficientdet/tf2/postprocess.py#L409
-        cls_offset = tf.cast(labels, bbs.dtype) * (tf.reduce_max(bbs) + 1)
-        bbs_per_class = bbs + tf.expand_dims(cls_offset, -1)
-        rr, nms_scores = tf.image.non_max_suppression_with_scores(bbs_per_class, ccs, max_output_size, iou_threshold, score_threshold, soft_nms_sigma)
-        return tf.gather(bbs, rr), tf.gather(labels, rr), nms_scores
+        cls_offset = functional.cast(labels, bbs.dtype) * (functional.reduce_max(bbs) + 1)
+        bbs_per_class = bbs + functional.expand_dims(cls_offset, -1)
+        rr, nms_scores = functional.non_max_suppression_with_scores(bbs_per_class, ccs, max_output_size, iou_threshold, score_threshold, soft_nms_sigma)
+        return functional.gather(bbs, rr), functional.gather(labels, rr), nms_scores
 
     def __nms_global__(self, bbs, ccs, labels, score_threshold=0.3, iou_threshold=0.5, soft_nms_sigma=0.5, max_output_size=100):
-        rr, nms_scores = tf.image.non_max_suppression_with_scores(bbs, ccs, max_output_size, iou_threshold, score_threshold, soft_nms_sigma)
-        return tf.gather(bbs, rr), tf.gather(labels, rr), nms_scores
+        rr, nms_scores = functional.non_max_suppression_with_scores(bbs, ccs, max_output_size, iou_threshold, score_threshold, soft_nms_sigma)
+        return functional.gather(bbs, rr), functional.gather(labels, rr), nms_scores
 
     def __object_score_split__(self, pred):
         return pred[:, :-1], pred[:, -1]  # May overwrite
 
     def __to_static__(self, bboxs, lables, confidences, max_output_size=100):
-        indices = tf.expand_dims(tf.range(tf.shape(bboxs)[0]), -1)
-        lables = tf.cast(lables, bboxs.dtype)
-        concated = tf.concat([bboxs, tf.expand_dims(lables, -1), tf.expand_dims(confidences, -1)], axis=-1)
-        concated = tf.tensor_scatter_nd_update(tf.zeros([max_output_size, concated.shape[-1]], dtype=bboxs.dtype), indices, concated)
+        indices = functional.expand_dims(functional.range(functional.shape(bboxs)[0]), -1)
+        lables = functional.cast(lables, bboxs.dtype)
+        concated = functional.concat([bboxs, functional.expand_dims(lables, -1), functional.expand_dims(confidences, -1)], axis=-1)
+        concated = functional.tensor_scatter_nd_update(functional.zeros([max_output_size, concated.shape[-1]], dtype=bboxs.dtype), indices, concated)
         return concated
 
     def __decode_single__(self, pred, score_threshold=0.3, iou_or_sigma=0.5, max_output_size=100, method="hard", mode="global", topk=0, input_shape=None):
         # https://github.com/google/automl/tree/master/efficientdet/tf2/postprocess.py#L159
-        pred = tf.cast(pred, tf.float32)
+        pred = functional.cast(pred.detach() if hasattr(pred, "detach") else pred, "float32")
         if input_shape is not None:
             self.__init_anchor__(input_shape)
 
         if self.use_object_scores:  # YOLO outputs: [bboxes, classses_score, object_score]
             pred, object_scores = self.__object_score_split__(pred)
 
         if topk != 0:
             bbs, ccs, labels, picking_indices = self.__topk_class_boxes_single__(pred, topk)
-            anchors = tf.gather(self.anchors, picking_indices)
+            anchors = functional.gather(self.anchors, picking_indices)
             if self.use_object_scores:
-                ccs *= tf.gather(object_scores, picking_indices)
+                ccs = ccs * functional.gather(object_scores, picking_indices)
         else:
-            bbs, ccs, labels = pred[:, :4], tf.reduce_max(pred[:, 4:], axis=-1), tf.argmax(pred[:, 4:], axis=-1)
+            bbs, scores = pred[:, : self.regression_len], pred[:, self.regression_len :]
+            ccs, labels = functional.reduce_max(scores, axis=-1), functional.argmax(scores, axis=-1)
             anchors = self.anchors
             if self.use_object_scores:
-                ccs *= object_scores
+                ccs = ccs * object_scores
+        ccs = functional.sigmoid(ccs) if self.use_sigmoid_on_score else ccs
 
-        bbs_decoded = anchors_func.decode_bboxes(bbs, anchors)
+        # print(f"{bbs.shape = }, {anchors.shape = }")
+        bbs_decoded = anchors_func.decode_bboxes(bbs, anchors, regression_len=self.regression_len)
         iou_threshold, soft_nms_sigma = (1.0, iou_or_sigma / 2) if method.lower() == "gaussian" else (iou_or_sigma, 0.0)
 
         if mode == "per_class":
             bboxs, lables, confidences = self.__nms_per_class__(bbs_decoded, ccs, labels, score_threshold, iou_threshold, soft_nms_sigma, max_output_size)
-        else:
+        elif mode == "global":
             bboxs, lables, confidences = self.__nms_global__(bbs_decoded, ccs, labels, score_threshold, iou_threshold, soft_nms_sigma, max_output_size)
+        else:
+            bboxs, lables, confidences = bbs_decoded, labels, ccs  # Return raw decoded data for testing
 
         return self.__to_static__(bboxs, lables, confidences, max_output_size) if self.use_static_output else (bboxs, lables, confidences)
 
     def call(self, preds, input_shape=None, training=False, **nms_kwargs):
         """
         https://github.com/google/automl/tree/master/efficientdet/tf2/postprocess.py#L159
 
@@ -173,15 +191,15 @@
               If use_static_output=True, fixed output shape will be `[batch, max_output_size, 6]`.
           method: "gaussian" or "hard".  Default "hard".
           mode: "global" or "per_class". "per_class" is strategy from `torchvision.ops.batched_nms`. Default "global".
           topk: Using topk highest scores, each bbox may have multi labels. Set `0` to disable, `-1` using all. Default 0.
         """
         self.nms_kwargs.update(nms_kwargs)
         if self.use_static_output:
-            return tf.map_fn(lambda xx: self.__decode_single__(xx, **nms_kwargs), preds)
+            return functional.map_fn(lambda xx: self.__decode_single__(xx, **nms_kwargs), preds)
         elif len(preds.shape) == 3:
             return [self.__decode_single__(pred, **self.nms_kwargs, input_shape=input_shape) for pred in preds]
         else:
             return self.__decode_single__(preds, **self.nms_kwargs, input_shape=input_shape)
 
     def get_config(self):
         config = super().get_config()
@@ -197,35 +215,52 @@
                 "use_static_output": self.use_static_output,
             }
         )
         config.update(self.nms_kwargs)
         return config
 
 
+""" COCO Evaluation """
+
+
 def scale_bboxes_back_single(bboxes, image_shape, scale, pad_top, pad_left, target_shape):
     # height, width = target_shape[0] / scale, target_shape[1] / scale
     # bboxes *= [height, width, height, width]
     bboxes *= [target_shape[0], target_shape[1], target_shape[0], target_shape[1]]
     bboxes -= [pad_top, pad_left, pad_top, pad_left]
     bboxes /= scale
-    bboxes = tf.clip_by_value(bboxes, 0, clip_value_max=[image_shape[0], image_shape[1], image_shape[0], image_shape[1]])
+    clip_value_max = functional.convert_to_tensor([image_shape[0], image_shape[1], image_shape[0], image_shape[1]], dtype="float32")
+    bboxes = functional.clip_by_value(bboxes, 0, clip_value_max=clip_value_max)
     # [top, left, bottom, right] -> [left, top, width, height]
-    bboxes = tf.stack([bboxes[:, 1], bboxes[:, 0], bboxes[:, 3] - bboxes[:, 1], bboxes[:, 2] - bboxes[:, 0]], axis=-1)
+    bboxes = functional.stack([bboxes[:, 1], bboxes[:, 0], bboxes[:, 3] - bboxes[:, 1], bboxes[:, 2] - bboxes[:, 0]], axis=-1)
     return bboxes
 
 
 def image_process(image, target_shape, mean, std, resize_method="bilinear", resize_antialias=False, use_bgr_input=False, letterbox_pad=-1):
-    if len(image.shape) < 2:
-        image = data.tf_imread(image)  # it's image path
-    original_image_shape = tf.shape(image)[:2]
-    image = tf.cast(image, "float32")
-    image = (image - mean) / std  # automl behavior: rescale -> resize
-    image, scale, pad_top, pad_left = data.aspect_aware_resize_and_crop_image(
+    if backend.is_tensorflow_backend:
+        from keras_cv_attention_models.coco.tf_data import tf_imread as imread, aspect_aware_resize_and_crop_image
+    else:
+        import cv2
+        from keras_cv_attention_models.coco.torch_data import aspect_aware_resize_and_crop_image
+
+        imread = lambda image_path: cv2.imread(image_path)[:, :, ::-1]  # BGR -> RGB
+
+    if isinstance(image, str) or len(image.shape) < 2:
+        image = imread(image)  # it's image path
+
+    if backend.is_tensorflow_backend:
+        original_image_shape = functional.shape(image)[:2]
+        image = functional.cast(image, "float32")
+    else:
+        original_image_shape, image = image.shape[:2], image.astype("float32")
+
+    image, scale, pad_top, pad_left = aspect_aware_resize_and_crop_image(
         image, target_shape, letterbox_pad=letterbox_pad, method=resize_method, antialias=resize_antialias
     )
+    image = (image - mean) / std  # automl behavior: rescale -> resize
     if use_bgr_input:
         image = image[:, :, ::-1]
     return image, scale, pad_top, pad_left, original_image_shape
 
 
 def init_eval_dataset(
     data_name="coco/2017",
@@ -233,62 +268,94 @@
     batch_size=8,
     rescale_mode="torch",
     resize_method="bilinear",
     resize_antialias=False,
     letterbox_pad=-1,
     use_bgr_input=False,
 ):
-    import tensorflow_datasets as tfds
+    if backend.is_tensorflow_backend:
+        from keras_cv_attention_models.coco import tf_data
+
+        if data_name.endswith(".json"):
+            dataset, _, num_classes = tf_data.detection_dataset_from_custom_json(data_name, with_info=True)
+        else:
+            import tensorflow_datasets as tfds
 
-    dataset = data.detection_dataset_from_custom_json(data_name) if data_name.endswith(".json") else tfds.load(data_name)
-    ds = dataset.get("validation", dataset.get("test", None))
+            dataset, info = tfds.load(data_name, with_info=True)
+            num_classes = info.features["objects"]["label"].num_classes
 
-    mean, std = data.init_mean_std_by_rescale_mode(rescale_mode)
-    __image_process__ = lambda image: image_process(image, input_shape, mean, std, resize_method, resize_antialias, use_bgr_input, letterbox_pad)
-    # ds: [resized_image, scale, pad_top, pad_left, original_image_shape, image_id]
-    ds = ds.map(lambda datapoint: (*__image_process__(datapoint["image"]), datapoint.get("image/id", datapoint["image"])))
-    ds = ds.batch(batch_size)
-    return ds
-
-
-def model_detection_and_decode(model, eval_dataset, pred_decoder, nms_kwargs={}, is_coco=True, image_id_map=None):
-    target_shape = (eval_dataset.element_spec[0].shape[1], eval_dataset.element_spec[0].shape[2])
-    num_classes = model.output_shape[-1] - 4
+        ds = dataset.get("validation", dataset.get("test", None))
+
+        mean, std = tf_data.init_mean_std_by_rescale_mode(rescale_mode)
+        __image_process__ = lambda image: image_process(image, input_shape, mean, std, resize_method, resize_antialias, use_bgr_input, letterbox_pad)
+        # ds: [resized_image, scale, pad_top, pad_left, original_image_shape, image_id]
+        ds = ds.map(lambda datapoint: (*__image_process__(datapoint["image"]), datapoint.get("image/id", datapoint["image"])))
+        ds = ds.batch(batch_size)
+        return ds, num_classes
+    else:
+        import torch
+        from torch.utils.data import Dataset, DataLoader
+        from keras_cv_attention_models.coco import torch_data
+
+        _, test, total_images, num_classes = torch_data.load_from_custom_json(data_name)
+        mean, std = torch_data.init_mean_std_by_rescale_mode(rescale_mode, convert_to_image_data_format=False)
+
+        class EvalDataset(Dataset):
+            def __len__(self):
+                return len(test)
+
+            def __getitem__(self, index):
+                image_path = test[index]["image"]
+                image, scale, pad_top, pad_left, original_image_shape = image_process(
+                    image_path, input_shape, mean, std, resize_method, resize_antialias, use_bgr_input, letterbox_pad
+                )
+                image = torch.from_numpy(image).permute([2, 0, 1]).contiguous()
+                return image, scale, pad_top, pad_left, torch.as_tensor(original_image_shape), image_path
+
+        ds = DataLoader(EvalDataset(), batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, sampler=None, drop_last=False)
+        # ds.element_spec = next(iter(ds))
+        return ds, num_classes
+
+
+def model_detection_and_decode(model, eval_dataset, pred_decoder, nms_kwargs={}, is_coco=True, image_id_map=None, num_classes=80):
+    sample_image = next(iter(eval_dataset))[0]
+    target_shape = sample_image.shape[1:-1] if backend.image_data_format() == "channels_last" else sample_image.shape[2:]
+    # num_classes = model.output_shape[-1] - 4
     if is_coco:
-        to_91_labels = (lambda label: label + 1) if num_classes >= 90 else (lambda label: data.COCO_80_to_90_LABEL_DICT[label] + 1)
+        to_91_labels = (lambda label: label + 1) if num_classes >= 90 else (lambda label: info.COCO_80_to_90_LABEL_DICT[label] + 1)
     else:
         to_91_labels = lambda label: label
     # Format: [image_id, x, y, width, height, score, class]
     to_coco_eval_single = lambda image_id, bbox, label, score: [image_id, *bbox.tolist(), score, to_91_labels(label)]
 
     results = []
     for images, scales, pad_tops, pad_lefts, original_image_shapes, image_ids in tqdm(eval_dataset):
-        preds = model(images)
-        preds = tf.cast(preds, tf.float32)
+        preds = model.predict(images).cpu().float() if backend.is_torch_backend else functional.cast(model(images), "float32")
         # decoded_preds: [[bboxes, labels, scores], [bboxes, labels, scores], ...]
         decoded_preds = pred_decoder(preds, **nms_kwargs)
 
         # Loop on batch
         for rr, image_shape, scale, pad_top, pad_left, image_id in zip(decoded_preds, original_image_shapes, scales, pad_tops, pad_lefts, image_ids):
             bboxes, labels, scores = rr
-            image_id, bboxes, labels, scores = image_id.numpy(), bboxes.numpy(), labels.numpy(), scores.numpy()
+            image_id, bboxes, labels, scores = np.array(image_id).item(), bboxes.numpy(), labels.numpy(), scores.numpy()
             if image_id_map is not None:
                 image_id = image_id_map[image_id.decode() if isinstance(image_id, bytes) else image_id]
             bboxes = scale_bboxes_back_single(bboxes, image_shape, scale, pad_top, pad_left, target_shape).numpy()
             results.extend([to_coco_eval_single(image_id, bb, cc, ss) for bb, cc, ss in zip(bboxes, labels, scores)])  # Loop on prediction results
-    return tf.convert_to_tensor(results).numpy()
+    return np.array(results)
 
 
 class COCOEvaluation:
     def __init__(self, annotations=None):
         from pycocotools.coco import COCO
 
         if annotations is None:
-            url = "https://github.com/leondgarse/keras_cv_attention_models/releases/download/efficientdet/coco_annotations_instances_val2017.json"
-            annotations = tf.keras.utils.get_file(origin=url)
+            url = "https://github.com/leondgarse/keras_cv_attention_models/releases/download/assets/coco_annotations_instances_val2017.json"
+            file_hash = "b681580a54b900b3cb44022fd1102ad5"
+            annotations = backend.get_file(origin=url, file_hash=file_hash)
 
         if isinstance(annotations, dict):  # json already loaded as dict
             coco_gt = COCO()
             coco_gt.dataset = annotations
             coco_gt.createIndex()
         else:
             coco_gt = COCO(annotations)
@@ -323,43 +390,47 @@
     from PIL import Image
 
     with open(json_path, "r") as ff:
         aa = json.load(ff)
 
     # int conversion just in case key is str
     categories = {int(kk): vv for kk, vv in aa["indices_2_labels"].items()} if "indices_2_labels" in aa else {}
+    base_path = os.path.expanduser(aa["info"]["base_path"]) if "base_path" in aa.get("info", {}) and len(aa["info"]["base_path"]) > 0 else None
     annotations, images, image_id_map = [], [], {}
     for image_id, ii in enumerate(aa.get("validation", aa.get("test", []))):
-        width, height = Image.open(ii["image"]).size  # For decoding bboxes, not actually openning images
+        image_file = os.path.join(base_path, ii["image"]) if base_path else ii["image"]
+        width, height = Image.open(image_file).size  # For decoding bboxes, not actually openning images
         for bb, label in zip(ii["objects"]["bbox"], ii["objects"]["label"]):
             # bb [top, left, bottom, right] in [0, 1] -> [left, top, bbox_width, bbox_height] with actual coordinates
             top = bb[0] * height
             left = bb[1] * width
             bbox_height = bb[2] * height - top
             bbox_width = bb[3] * width - left
             bb = [left, top, bbox_width, bbox_height]
             area = bbox_width * bbox_height  # Actual area in COCO is the segmentation area, doesn't matter in detection mission
 
             label = int(label)
             annotations.append({"bbox": bb, "category_id": label, "image_id": image_id, "id": len(annotations), "iscrowd": 0, "area": area})
             if label not in categories:
                 categories[label] = str(len(categories))
-        images.append({"id": image_id, "file_name": ii["image"], "height": height, "width": width})
-        image_id_map[ii["image"]] = image_id
+        images.append({"id": image_id, "file_name": image_file, "height": height, "width": width})
+        image_id_map[image_file] = image_id
     categories = [{"id": kk, "name": vv} for kk, vv in categories.items()]
     return {"images": images, "annotations": annotations, "categories": categories}, image_id_map
 
 
-# Wrapper a callback for using in training
-class COCOEvalCallback(tf.keras.callbacks.Callback):
+""" Wrapper a callback for using in training """
+
+
+class COCOEvalCallback(callbacks.Callback):
     """
     Basic test:
     >>> from keras_cv_attention_models import efficientdet, coco
     >>> model = efficientdet.EfficientDetD0()
-    >>> ee = coco.eval_func.COCOEvalCallback(batch_size=4, model_basic_save_name='test', rescale_mode='raw', anchors_mode="anchor_free")
+    >>> ee = coco.eval_func.COCOEvalCallback(batch_size=4)
     >>> ee.model = model
     >>> ee.on_epoch_end()
     """
 
     def __init__(
         self,
         data_name="coco/2017",  # [init_eval_dataset parameters]
@@ -369,16 +440,16 @@
         rescale_mode="auto",
         letterbox_pad=-1,
         use_bgr_input=False,
         take_samples=-1,
         nms_score_threshold=0.001,  # [model_detection_and_decode parameters]
         nms_iou_or_sigma=0.5,
         nms_max_output_size=100,
-        nms_method="gaussian",
-        nms_mode="per_class",
+        nms_method="gaussian",  # gaussian or hard
+        nms_mode="per_class",  # per_class or global
         nms_topk=5000,
         anchors_mode="auto",  # [model anchors related parameters]
         anchor_scale=4,  # Init anchors for model prediction. "auto" means 1 if (anchors_mode=="anchor_free" or anchors_mode=="yolor"), else 4
         aspect_ratios=(1, 2, 0.5),  # For efficientdet anchors only
         num_scales=3,  # For efficientdet anchors only
         annotation_file=None,
         save_json=None,
@@ -421,73 +492,83 @@
             self.annotation_file, self.image_id_map = to_coco_annotation(self.data_name)
         else:
             self.image_id_map = None
 
         self.built = False
 
     def build(self, input_shape, output_shape):
-        input_shape = (int(input_shape[1]), int(input_shape[2]))
-        self.eval_dataset = init_eval_dataset(input_shape=input_shape, **self.dataset_kwargs)
+        import re
+
+        input_shape = (
+            (int(input_shape[1]), int(input_shape[2])) if backend.image_data_format() == "channels_last" else (int(input_shape[2]), int(input_shape[3]))
+        )
+        self.eval_dataset, self.num_classes = init_eval_dataset(input_shape=input_shape, **self.dataset_kwargs)
+        print("\n>>>> [COCOEvalCallback] self.dataset_kwargs:", self.dataset_kwargs)
+        regression_len = (output_shape[-1] - self.num_classes) // 4 * 4
+
         if self.anchors_mode is None or self.anchors_mode == "auto":
-            self.anchors_mode, num_anchors = anchors_func.get_anchors_mode_by_anchors(input_shape, total_anchors=output_shape[1])
+            self.anchors_mode, num_anchors = anchors_func.get_anchors_mode_by_anchors(input_shape, total_anchors=output_shape[1], regression_len=regression_len)
         elif self.anchors_mode == anchors_func.EFFICIENTDET_MODE:
             num_anchors = self.efficient_det_num_anchors
         else:
             num_anchors = anchors_func.NUM_ANCHORS.get(self.anchors_mode, 9)
         pyramid_levels = anchors_func.get_pyramid_levels_by_anchors(input_shape, total_anchors=output_shape[1], num_anchors=num_anchors)
-        print("\n>>>> [COCOEvalCallback] input_shape: {}, pyramid_levels: {}, anchors_mode: {}".format(input_shape, pyramid_levels, self.anchors_mode))
+        print(">>>> [COCOEvalCallback] input_shape: {}, pyramid_levels: {}, anchors_mode: {}".format(input_shape, pyramid_levels, self.anchors_mode))
         # print(">>>>", self.dataset_kwargs)
         # print(">>>>", self.nms_kwargs)
 
-        self.pred_decoder = DecodePredictions(input_shape, pyramid_levels, self.anchors_mode, **self.anchor_kwargs)
+        use_sigmoid_on_score = not any([ii.name.endswith("_sigmoid") for ii in self.model.layers[-50:]])
+        print(">>>> use_sigmoid_on_score:", use_sigmoid_on_score)
+        self.pred_decoder = DecodePredictions(
+            input_shape, pyramid_levels, self.anchors_mode, regression_len=regression_len, use_sigmoid_on_score=use_sigmoid_on_score, **self.anchor_kwargs
+        )
 
         # Training saving best
         if self.model_basic_save_name is not None:
-            self.monitor_save = os.path.join(self.save_path, self.model_basic_save_name + "_epoch_{}_" + self.item_key + "_{}.h5")
-            self.monitor_save_re = self.monitor_save.format("*", "*")
+            monitor_save_name = self.model_basic_save_name + "_epoch_{}_" + self.item_key + "_{}.h5"
+            self.monitor_save_re = re.compile(monitor_save_name.format(r"\d*", r"[\d\.]*"))
+            self.monitor_save = os.path.join(self.save_path, monitor_save_name)
             self.is_better = lambda cur, pre: cur >= pre
             self.pre_best = -1e5
 
         self.coco_evaluation = COCOEvaluation(self.annotation_file)
         self.built = True
 
+    @no_grad_if_torch
     def on_epoch_end(self, epoch=0, logs=None):
         if not self.built:
             if self.dataset_kwargs["rescale_mode"] == "auto":
                 self.dataset_kwargs["rescale_mode"] = getattr(self.model, "rescale_mode", "torch")
             self.build(self.model.input_shape, self.model.output_shape)
 
         if epoch + 1 < self.start_epoch or epoch % self.frequency != 0:
             return
 
         # pred_decoder = self.model.decode_predictions
         eval_dataset = self.eval_dataset.take(self.take_samples) if self.take_samples > 0 else self.eval_dataset
-        detection_results = model_detection_and_decode(self.model, eval_dataset, self.pred_decoder, self.nms_kwargs, self.is_coco, self.image_id_map)
-        try:
-            coco_eval = self.coco_evaluation(detection_results)
-        except:
-            print(">>>> Error in running coco_evaluation")
-            coco_eval = None
-            data_name = self.data_name.replace("/", "_")
-            self.save_json = "{}_{}_detection_results_error.json".format(self.model.name, data_name) if self.save_json is None else self.save_json
+        detection_results = model_detection_and_decode(
+            self.model, eval_dataset, self.pred_decoder, self.nms_kwargs, self.is_coco, self.image_id_map, self.num_classes
+        )
+        coco_eval = None if len(detection_results) == 0 else self.coco_evaluation(detection_results)
 
         if self.save_json is not None:
             to_coco_json(detection_results, self.save_json)
             print(">>>> Detection results saved to:", self.save_json)
 
         if hasattr(self.model, "history") and hasattr(self.model.history, "history"):
-            self.model.history.history.setdefault(self.item_key, []).append(coco_eval.stats.tolist())
+            self.model.history.history.setdefault(self.item_key, []).append(([0] * 12) if coco_eval is None else coco_eval.stats.tolist())
 
         # Training save best
         cur_ap = coco_eval.stats[0] if coco_eval is not None else 0
         if self.model_basic_save_name is not None and self.is_better(cur_ap, self.pre_best):
             self.pre_best = cur_ap
-            pre_monitor_saves = tf.io.gfile.glob(self.monitor_save_re)
+            # pre_monitor_saves = glob(self.monitor_save_re)
+            pre_monitor_saves = [ii for ii in os.listdir(self.save_path) if self.monitor_save_re.match(ii)]
             # tf.print(">>>> pre_monitor_saves:", pre_monitor_saves)
             if len(pre_monitor_saves) != 0:
-                os.remove(pre_monitor_saves[0])
+                os.remove(os.path.join(self.save_path, pre_monitor_saves[0]))
             monitor_save = self.monitor_save.format(epoch + 1, "{:.4f}".format(cur_ap))
-            tf.print("\n>>>> Save best to:", monitor_save)
+            print("\n>>>> Save best to:", monitor_save)
             if self.model is not None:
                 self.model.save(monitor_save)
 
         return coco_eval
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/coco/losses.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/coco/tf_losses.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import math
 import tensorflow as tf
 from tensorflow.keras import backend as K
 
 
-def __bbox_iou__(true_top_left, true_bottom_right, true_hw, pred_top_left, pred_bottom_right, pred_hw, use_ciou=False, epsilon=1e-8):
+def __bbox_iou__(true_top_left, true_bottom_right, true_hw, pred_top_left, pred_bottom_right, pred_hw, use_ciou=False, epsilon=1e-6):
     # Use all top_left, bottom_right, hw as parameters, as hw usaully already calculated before calling this function, like center_hw bboxes.
     inter_top_left = tf.maximum(true_top_left, pred_top_left)
     inter_bottom_right = tf.minimum(true_bottom_right, pred_bottom_right)
     inter_hw = tf.maximum(inter_bottom_right - inter_top_left, 0)
     inter_area = inter_hw[:, 0] * inter_hw[:, 1]
 
     bboxes_trues_area = true_hw[:, 0] * true_hw[:, 1]
@@ -22,15 +22,15 @@
         outer_area = outer_hw[:, 0] ** 2 + outer_hw[:, 1] ** 2 + epsilon
 
         rho_height = (true_top_left[:, 0] + true_bottom_right[:, 0] - pred_top_left[:, 0] - pred_bottom_right[:, 0]) ** 2
         rho_width = (true_top_left[:, 1] + true_bottom_right[:, 1] - pred_top_left[:, 1] - pred_bottom_right[:, 1]) ** 2
         rho = (rho_height + rho_width) / 4
         vv_scale = 4 / math.pi**2
         vv = vv_scale * (tf.atan(true_hw[:, 1] / (true_hw[:, 0] + epsilon)) - tf.atan(pred_hw[:, 1] / (pred_hw[:, 0] + epsilon))) ** 2
-        alpha = tf.stop_gradient(vv / ((1 + epsilon) - iou + vv))
+        alpha = tf.stop_gradient(vv / (1 + epsilon - iou + vv))
         return iou - (rho / outer_area + vv * alpha)
     else:
         return iou
 
 
 @tf.keras.utils.register_keras_serializable(package="kecamLoss")
 class FocalLossWithBbox(tf.keras.losses.Loss):
@@ -153,135 +153,254 @@
 
     def __init__(
         self,
         input_shape,  # Required for initing anchors...
         pyramid_levels=[3, 5],  # Required for initing anchors...
         use_l1_loss=False,
         bbox_loss_weight=5.0,
+        class_loss_weight=1.0,
         anchor_assign_center_radius=2.5,
         anchor_assign_topk_ious_max=10,
         anchor_grid_zero_start=True,
-        epsilon=1e-8,
+        use_object_scores=True,  # False for YOLOV8, True for YOLOX
+        regression_len=4,  # 64 fro YOLOV8, 4 for YOLOX
+        use_ciou=False,  # False for YOLOX, True for YOLOV8
+        class_weight=None,  # list value informat like `[0.2, 0.3, 0.5, ...]` indicates weights for different classes. Must be same length with `num_classes`.
+        epsilon=1e-6,
         label_smoothing=0.0,
         from_logits=False,
         **kwargs,
     ):
         from keras_cv_attention_models.coco import anchors_func
 
         super().__init__(**kwargs)
-        self.bbox_loss_weight, self.use_l1_loss, self.epsilon = bbox_loss_weight, use_l1_loss, epsilon
-        self.label_smoothing, self.from_logits = label_smoothing, from_logits
-        self.input_shape, self.pyramid_levels, self.anchor_grid_zero_start = input_shape, pyramid_levels, anchor_grid_zero_start
+        self.bbox_loss_weight, self.class_loss_weight, self.use_l1_loss, self.use_ciou = bbox_loss_weight, class_loss_weight, use_l1_loss, use_ciou
+        self.use_object_scores, self.regression_len, self.label_smoothing, self.from_logits = use_object_scores, regression_len, label_smoothing, from_logits
+        self.input_shape, self.pyramid_levels, self.anchor_grid_zero_start, self.epsilon = input_shape, pyramid_levels, anchor_grid_zero_start, epsilon
         self.anchor_assign_center_radius, self.anchor_assign_topk_ious_max = anchor_assign_center_radius, anchor_assign_topk_ious_max
+        self.use_dfl_loss = regression_len > 4
+
+        self.with_encoded_bboxes = regression_len > 4 or use_l1_loss
         self.anchor_assign = anchors_func.AnchorFreeAssignMatching(
-            input_shape, pyramid_levels, anchor_assign_center_radius, anchor_assign_topk_ious_max, anchor_grid_zero_start, epsilon=epsilon
+            input_shape=input_shape,
+            pyramid_levels=pyramid_levels,
+            center_radius=anchor_assign_center_radius,
+            topk_ious_max=anchor_assign_topk_ious_max,
+            grid_zero_start=anchor_grid_zero_start,  # False for YOLOV8, True for YOLOX
+            use_object_scores=use_object_scores,  # False for YOLOV8, True for YOLOX
+            regression_len=regression_len,  # 64 fro YOLOV8, 4 for YOLOX
+            with_encoded_bboxes=self.with_encoded_bboxes,  # True for YOLOX using l1 loss, else False
+            epsilon=epsilon,
         )
         self.class_acc = tf.Variable(0, dtype="float32", trainable=False)
         # self.class_acc = tf.Variable(0, dtype="float32", trainable=False, aggregation=tf.VariableAggregation.MEAN)
 
+        if class_weight is not None:
+            self.class_weight = tf.convert_to_tensor(class_weight, dtype="float32")
+            self.__class_loss_with_class_weight__ = tf.keras.losses.BinaryCrossentropy(reduction="none")
+        else:
+            self.class_weight = None
+
+    def __reduce_with_class_weight__(self, labels_true, class_loss, bbox_loss, l1_loss, dfl_loss):
+        # tf.print(class_loss.shape, bbox_loss.shape, l1_loss.shape, dfl_loss.shape)  # [18, 80] [18] [18, 4] [18]
+        if self.class_weight is not None:
+            cur_class_weight = tf.gather(self.class_weight, tf.argmax(labels_true, axis=-1))
+            class_loss = tf.reduce_sum(class_loss, axis=-1) * cur_class_weight
+            bbox_loss = bbox_loss * cur_class_weight
+            l1_loss = (tf.reduce_sum(l1_loss, axis=-1) * cur_class_weight) if self.use_l1_loss else 0.0
+            dfl_loss = (dfl_loss * cur_class_weight) if self.use_dfl_loss else 0.0
+        return tf.reduce_sum(class_loss), tf.reduce_sum(bbox_loss), tf.reduce_sum(l1_loss), tf.reduce_sum(dfl_loss)
+
     def __iou_loss__(self, bboxes_trues, pred_top_left, pred_bottom_right, pred_hw):
         # bboxes_trues: [[top, left, bottom, right]]
-        # inter_top_left = tf.maximum(bboxes_trues[:, :2], pred_top_left)
-        # inter_bottom_right = tf.minimum(bboxes_trues[:, 2:], pred_bottom_right)
-        # inter_hw = tf.maximum(inter_bottom_right - inter_top_left, 0)
-        # inter_area = inter_hw[:, 0] * inter_hw[:, 1]
-        #
-        # bboxes_trues_area = (bboxes_trues[:, 2] - bboxes_trues[:, 0]) * (bboxes_trues[:, 3] - bboxes_trues[:, 1])
-        # bboxes_preds_area = pred_hw[:, 0] * pred_hw[:, 1]
-        # union_area = bboxes_trues_area + bboxes_preds_area - inter_area
-        # iou = inter_area / (union_area + self.epsilon)
-        # true_top_left, true_bottom_right = bboxes_trues[:, :2], bboxes_trues[:, 2:4]
         true_top_left, true_bottom_right, _ = tf.split(bboxes_trues, [2, 2, -1], axis=-1)
         true_hw = true_bottom_right - true_top_left
-        iou = __bbox_iou__(true_top_left, true_bottom_right, true_hw, pred_top_left, pred_bottom_right, pred_hw, epsilon=self.epsilon)
+        iou = __bbox_iou__(true_top_left, true_bottom_right, true_hw, pred_top_left, pred_bottom_right, pred_hw, use_ciou=self.use_ciou, epsilon=self.epsilon)
         return 1 - iou**2
 
+    def __dfl_loss__(self, bboxes_true_encoded, bboxes_pred, labels_true):
+        target_low_bound = tf.cast(tf.floor(bboxes_true_encoded), bboxes_pred.dtype)
+        target_up_bound = target_low_bound + 1
+        weight_low = target_up_bound - bboxes_true_encoded
+
+        bboxes_pred_reg = tf.reshape(bboxes_pred, [-1, 4, self.regression_len // 4])
+        dfl_loss_low = K.sparse_categorical_crossentropy(target_low_bound, bboxes_pred_reg, from_logits=True) * weight_low
+        dfl_loss_up = K.sparse_categorical_crossentropy(target_up_bound, bboxes_pred_reg, from_logits=True) * (1 - weight_low)
+        dfl_loss = tf.reduce_mean(dfl_loss_low, axis=-1) + tf.reduce_mean(dfl_loss_up, axis=-1)
+        return dfl_loss
+        # return tf.reduce_sum(dfl_loss * tf.reduce_sum(labels_true, axis=-1)) / tf.maximum(tf.reduce_sum(labels_true), 1.0)
+
+    def __l1_loss__(self, bboxes_true_encoded, bboxes_pred):
+        if self.regression_len == 4:
+            return tf.abs(bboxes_true_encoded - bboxes_pred)  # mean absolute error
+        else:
+            dist = tf.abs(tf.expand_dims(bboxes_true_encoded, -1) - tf.reshape(bboxes_pred, [-1, 4, self.regression_len // 4]))
+            return tf.reduce_mean(dist, axis=-1)
+
     def __valid_call_single__(self, bbox_labels_true, bbox_labels_pred):
         bbox_labels_true_assined = tf.stop_gradient(self.anchor_assign(bbox_labels_true, bbox_labels_pred))
-        bboxes_true, bboxes_true_encoded, labels_true, object_true_idx_nd = tf.split(bbox_labels_true_assined, [4, 4, -1, 1], axis=-1)
+        if self.with_encoded_bboxes:
+            bboxes_true, bboxes_true_encoded, labels_true, object_true_idx_nd = tf.split(bbox_labels_true_assined, [4, 4, -1, 1], axis=-1)
+        else:
+            bboxes_true, labels_true, object_true_idx_nd = tf.split(bbox_labels_true_assined, [4, -1, 1], axis=-1)
         object_true_idx_nd = tf.cast(object_true_idx_nd, tf.int32)
         object_true = tf.tensor_scatter_nd_update(tf.zeros_like(bbox_labels_pred[:, -1]), object_true_idx_nd, tf.ones_like(bboxes_true[:, -1]))
 
-        # object_true_idx = object_true_idx_nd[:, 0]
-        # bbox_labels_pred_valid = tf.gather(bbox_labels_pred, object_true_idx)
         bbox_labels_pred_valid = tf.gather_nd(bbox_labels_pred, object_true_idx_nd)
-        bboxes_pred, labels_pred, object_pred = bbox_labels_pred_valid[:, :4], bbox_labels_pred_valid[:, 4:-1], bbox_labels_pred[:, -1]
+        if self.use_object_scores:
+            bboxes_pred, labels_pred, _ = tf.split(bbox_labels_pred_valid, [self.regression_len, -1, 1], axis=-1)
+            object_pred = bbox_labels_pred[:, -1]
+        else:
+            bboxes_pred, labels_pred = tf.split(bbox_labels_pred_valid, [self.regression_len, -1], axis=-1)
         # bboxes_true.set_shape(bboxes_pred.shape)
 
-        # anchors_centers = tf.gather(self.anchor_assign.anchors_centers, object_true_idx)
-        # anchors_hws = tf.gather(self.anchor_assign.anchors_hws, object_true_idx)
         anchors_centers = tf.gather_nd(self.anchor_assign.anchors_centers, object_true_idx_nd)
         anchors_hws = tf.gather_nd(self.anchor_assign.anchors_hws, object_true_idx_nd)
         bboxes_pred_top_left, bboxes_pred_bottom_right, bboxes_pred_center, bboxes_pred_hw = self.anchor_assign.__decode_bboxes__(
             bboxes_pred, anchors_centers, anchors_hws
         )
 
         if self.label_smoothing > 0:
             labels_true = labels_true * (1.0 - self.label_smoothing) + 0.5 * self.label_smoothing
-        class_loss = tf.reduce_sum(K.binary_crossentropy(labels_true, labels_pred))
-        object_loss = tf.reduce_sum(K.binary_crossentropy(tf.cast(object_true, object_pred.dtype), object_pred))
-        bbox_loss = tf.reduce_sum(self.__iou_loss__(bboxes_true, bboxes_pred_top_left, bboxes_pred_bottom_right, bboxes_pred_hw))
-        if self.use_l1_loss:
-            l1_loss = tf.reduce_sum(tf.abs(bboxes_true_encoded - bboxes_pred))  # mean absolute error
-        else:
-            l1_loss = 0.0
+        # class_loss = tf.reduce_sum(K.binary_crossentropy(labels_true, labels_pred))
+        class_loss = K.binary_crossentropy(labels_true, labels_pred)
+        bbox_loss = self.__iou_loss__(bboxes_true, bboxes_pred_top_left, bboxes_pred_bottom_right, bboxes_pred_hw)
+
+        l1_loss = self.__l1_loss__(bboxes_true_encoded, bboxes_pred) if self.use_l1_loss else 0.0  # mean absolute error
+        dfl_loss = self.__dfl_loss__(bboxes_true_encoded, bboxes_pred, labels_true) if self.use_dfl_loss else 0.0
+        # class_loss, bbox_loss, l1_loss, dfl_loss = tf.reduce_mean(class_loss), tf.reduce_mean(bbox_loss), tf.reduce_mean(l1_loss), tf.reduce_mean(dfl_loss)
+        class_loss, bbox_loss, l1_loss, dfl_loss = self.__reduce_with_class_weight__(labels_true, class_loss, bbox_loss, l1_loss, dfl_loss)
+
+        object_loss = tf.reduce_sum(K.binary_crossentropy(tf.cast(object_true, object_pred.dtype), object_pred)) if self.use_object_scores else 0.0
 
         num_valid_anchors = tf.cast(tf.shape(bboxes_pred)[0], bboxes_pred.dtype)
         class_acc = tf.reduce_mean(tf.cast(tf.argmax(labels_true, axis=-1) == tf.argmax(labels_pred, axis=-1), "float32"))
-        return class_loss, bbox_loss, object_loss, l1_loss, num_valid_anchors, class_acc
+        return class_loss, bbox_loss, object_loss, l1_loss, dfl_loss, num_valid_anchors, class_acc
 
     def __call_single__(self, inputs):
         bbox_labels_true, bbox_labels_pred = inputs[0], inputs[1]
         return tf.cond(
             tf.reduce_any(bbox_labels_true[:, -1] > 0),
             lambda: self.__valid_call_single__(bbox_labels_true, bbox_labels_pred),
-            lambda: (0.0, 0.0, tf.reduce_sum(K.binary_crossentropy(0.0, bbox_labels_pred[:, -1])), 0.0, 0.0, 0.0),  # Object loss only, target is all False.
+            lambda: (0.0, 0.0, tf.reduce_sum(K.binary_crossentropy(0.0, bbox_labels_pred[:, -1])), 0.0, 0.0, 0.0, 0.0),  # Object loss only, target is all False
         )
 
     def call(self, y_true, y_pred):
         if self.from_logits:
             bbox_pred, class_pred = y_pred[:, :, :4], y_pred[:, :, 4:]
             class_pred = tf.sigmoid(class_pred)
             y_pred = tf.concat([bbox_pred, class_pred], axis=-1)
 
-        out_dtype = (y_pred.dtype,) * 6
-        class_loss, bbox_loss, object_loss, l1_loss, num_valid, class_acc = tf.map_fn(self.__call_single__, (y_true, y_pred), fn_output_signature=out_dtype)
+        out_dtype = (y_pred.dtype,) * 7
+        class_loss, bbox_loss, object_loss, l1_loss, dfl_loss, num_valid, class_acc = tf.map_fn(
+            self.__call_single__, (y_true, y_pred), fn_output_signature=out_dtype
+        )
+        # tf.print(class_loss.shape, bbox_loss.shape, object_loss.shape, l1_loss.shape)
 
-        num_valid = tf.maximum(tf.reduce_sum(num_valid), 1.0)
-        class_loss, bbox_loss, l1_loss = tf.reduce_sum(class_loss) / num_valid, tf.reduce_sum(bbox_loss) / num_valid, tf.reduce_sum(l1_loss) / num_valid
-        object_loss = tf.reduce_sum(object_loss) / num_valid  # [ ??? ] why not divide actual object shape?
+        num_valid = tf.maximum(tf.reduce_sum(num_valid), 1.0) / tf.cast(tf.shape(y_true)[0], num_valid.dtype)  # Divide batch_size, for further call mean
+        class_loss, bbox_loss = class_loss / num_valid, bbox_loss / num_valid
 
         # Calulate accuracy here, will use it in metrics
         self.class_acc.assign(tf.reduce_mean(class_acc))
 
         if self.use_l1_loss:
-            tf.print(" - l1_loss:", l1_loss, end="")
-        tf.print(" - cls_loss:", class_loss, "- bbox_loss:", bbox_loss, "- obj_loss:", object_loss, end="\r")
-        return class_loss + object_loss + l1_loss + bbox_loss * self.bbox_loss_weight
+            l1_loss = l1_loss / num_valid
+            tf.print(" - l1_loss:", tf.reduce_mean(l1_loss), end="")
+        if self.use_dfl_loss:
+            dfl_loss = dfl_loss / num_valid
+            tf.print(" - dfl_loss:", tf.reduce_mean(dfl_loss), end="")
+        if self.use_object_scores:
+            object_loss = object_loss / num_valid  # [ ??? ] why not divide actual object shape?
+            tf.print(" - obj_loss:", tf.reduce_mean(object_loss), end="")
+        tf.print(" - cls_loss:", tf.reduce_mean(class_loss), "- bbox_loss:", tf.reduce_mean(bbox_loss), end="\r")
+        return class_loss * self.class_loss_weight + object_loss + dfl_loss + l1_loss + bbox_loss * self.bbox_loss_weight
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "input_shape": self.input_shape,
                 "pyramid_levels": self.pyramid_levels,
                 "use_l1_loss": self.use_l1_loss,
                 "bbox_loss_weight": self.bbox_loss_weight,
+                "class_loss_weight": self.class_loss_weight,
                 "anchor_assign_center_radius": self.anchor_assign_center_radius,
                 "anchor_assign_topk_ious_max": self.anchor_assign_topk_ious_max,
                 "anchor_grid_zero_start": self.anchor_grid_zero_start,
+                "use_object_scores": self.use_object_scores,
+                "regression_len": self.regression_len,
+                "use_ciou": self.use_ciou,
                 "epsilon": self.epsilon,
                 "label_smoothing": self.label_smoothing,
                 "from_logits": self.from_logits,
             }
         )
         return config
 
 
 @tf.keras.utils.register_keras_serializable(package="kecamLoss")
+class YOLOV8Loss(AnchorFreeLoss):
+    """
+    # Basic test:
+    >>> from keras_cv_attention_models.coco import losses, anchors_func
+    >>> aa = losses.YOLOV8Loss(input_shape=(640, 640))
+
+    >>> from keras_cv_attention_models import yolov8, test_images
+    >>> from keras_cv_attention_models.coco import anchors_func, data
+    >>> mm = yolov8.YOLOV8_S()
+    >>> img = test_images.dog_cat()
+    >>> pred = mm(mm.preprocess_input(img))
+
+    >>> bbs, lls, ccs = mm.decode_predictions(pred)[0]
+    >>> bbox_labels_true = tf.concat([bbs, tf.one_hot(lls, 80), tf.ones([bbs.shape[0], 1])], axis=-1)
+    >>> print("\n", aa(tf.expand_dims(bbox_labels_true, 0), pred))
+    >>> # - dfl_loss: 0.910405695 - cls_loss: 1.84504449 - bbox_loss: 0.120341115
+    >>> # tf.Tensor(2.126511, shape=(), dtype=float32)
+    """
+
+    def __init__(
+        self,
+        input_shape,  # Required for initing anchors...
+        pyramid_levels=[3, 5],  # Required for initing anchors...
+        use_l1_loss=False,
+        bbox_loss_weight=5.0,
+        class_loss_weight=0.333,  # box_weight=7.5, cls_weight=0.5, dfl_weight=1.5
+        anchor_assign_center_radius=2.5,
+        anchor_assign_topk_ious_max=10,
+        anchor_grid_zero_start=False,
+        use_object_scores=False,  # False for YOLOV8, True for YOLOX
+        regression_len=64,  # 64 fro YOLOV8, 4 for YOLOX
+        use_ciou=True,  # False for YOLOX, True for YOLOV8
+        class_weight=None,  # list value informat like `[0.2, 0.3, 0.5, ...]` indicates weights for different classes. Must be same length with `num_classes`.
+        epsilon=1e-6,
+        label_smoothing=0.0,
+        from_logits=False,
+        **kwargs,
+    ):
+        super().__init__(
+            input_shape=input_shape,
+            pyramid_levels=pyramid_levels,
+            use_l1_loss=use_l1_loss,
+            bbox_loss_weight=bbox_loss_weight,
+            class_loss_weight=class_loss_weight,
+            anchor_assign_center_radius=anchor_assign_center_radius,
+            anchor_assign_topk_ious_max=anchor_assign_topk_ious_max,
+            anchor_grid_zero_start=anchor_grid_zero_start,
+            use_object_scores=use_object_scores,
+            regression_len=regression_len,
+            use_ciou=use_ciou,
+            class_weight=class_weight,
+            epsilon=epsilon,
+            label_smoothing=label_smoothing,
+            from_logits=from_logits,
+            **kwargs,
+        )
+
+
+@tf.keras.utils.register_keras_serializable(package="kecamLoss")
 class YOLORLossWithBbox(tf.keras.losses.Loss):
     """
     # Test with dataset:
     >>> from keras_cv_attention_models import coco, yolor
     >>> input_shape = (640, 640, 3)
     >>> tf.random.set_seed(0)
     >>> train_dataset = coco.init_dataset(
@@ -417,15 +536,15 @@
 
     def update_state(self, y_true, y_pred, sample_weight=None):
         pick = tf.where(y_true[:, :, -1] > 0)
         cls_true_valid = tf.argmax(tf.gather_nd(y_true[:, :, 4:-1], pick), axis=-1)
         cls_pred_valid = tf.argmax(tf.gather_nd(y_pred[:, :, 4:], pick), axis=-1)
         cls_acc = tf.reduce_mean(tf.cast(cls_true_valid == cls_pred_valid, "float32"))
         # tf.assert_less(cls_acc, 1.1)
-        self.cls_acc.assign_add(self.loss_calss_with_acc.cls_acc)
+        self.cls_acc.assign_add(cls_acc)
         self.count.assign_add(1.0)
 
 
 @tf.keras.utils.register_keras_serializable(package="kecamLoss")
 class ClassAccuracyWithBboxWrapper(tf.keras.metrics.Metric):
     def __init__(self, loss_class_with_acc=None, name="cls_acc", **kwargs):
         super().__init__(name=name, **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/common_layers.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/common_layers.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,103 +1,136 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, initializers, image_data_format
 
 BATCH_NORM_DECAY = 0.9
 BATCH_NORM_EPSILON = 1e-5
 TF_BATCH_NORM_EPSILON = 0.001
 LAYER_NORM_EPSILON = 1e-5
-CONV_KERNEL_INITIALIZER = keras.initializers.VarianceScaling(scale=2.0, mode="fan_out", distribution="truncated_normal")
-# CONV_KERNEL_INITIALIZER = 'glorot_uniform'
 
 
 """ Wrapper for default parameters """
 
 
-@tf.keras.utils.register_keras_serializable(package="kecamCommon")
+@backend.register_keras_serializable(package="kecamCommon")
 def hard_swish(inputs):
     """`out = xx * relu6(xx + 3) / 6`, arxiv: https://arxiv.org/abs/1905.02244"""
-    return inputs * tf.nn.relu6(inputs + 3) / 6
+    return inputs * functional.relu6(inputs + 3) / 6
 
 
-@tf.keras.utils.register_keras_serializable(package="kecamCommon")
+@backend.register_keras_serializable(package="kecamCommon")
 def hard_sigmoid_torch(inputs):
     """https://pytorch.org/docs/stable/generated/torch.nn.Hardsigmoid.html
     toch.nn.Hardsigmoid: 0 if x <= 3 else (1 if x >= 3 else x / 6 + 1/2)
     keras.activations.hard_sigmoid: 0 if x <= 2.5 else (1 if x >= 2.5 else x / 5 + 1/2) -> tf.clip_by_value(inputs / 5 + 0.5, 0, 1)
     """
-    return tf.clip_by_value(inputs / 6 + 0.5, 0, 1)
+    return functional.clip_by_value(inputs / 6 + 0.5, 0, 1)
 
 
-@tf.keras.utils.register_keras_serializable(package="kecamCommon")
+@backend.register_keras_serializable(package="kecamCommon")
 def mish(inputs):
     """Mish: A Self Regularized Non-Monotonic Neural Activation Function.
     Paper: [Mish: A Self Regularized Non-Monotonic Neural Activation Function](https://arxiv.org/abs/1908.08681)
     Copied from https://github.com/tensorflow/addons/blob/master/tensorflow_addons/activations/mish.py
     """
-    return inputs * tf.math.tanh(tf.math.softplus(inputs))
+    return inputs * functional.tanh(functional.softplus(inputs))
 
 
-@tf.keras.utils.register_keras_serializable(package="kecamCommon")
+@backend.register_keras_serializable(package="kecamCommon")
 def phish(inputs):
     """Phish is defined as f(x) = xTanH(GELU(x)) with no discontinuities in the f(x) derivative.
     Paper: https://www.techrxiv.org/articles/preprint/Phish_A_Novel_Hyper-Optimizable_Activation_Function/17283824
     """
-    return inputs * tf.math.tanh(tf.nn.gelu(inputs))
+    return inputs * functional.tanh(functional.gelu(inputs))
+
+
+def gelu_quick(inputs):
+    """https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py#L90-L98
+    Applies GELU approximation that is fast but somewhat inaccurate. See: https://github.com/hendrycks/GELUs
+    """
+    return inputs * functional.sigmoid(1.702 * inputs)
+
+
+def gelu_linear(inputs):
+    """
+    >>> from keras_cv_attention_models.common_layers import gelu_linear
+    >>> xx = np.arange(-4, 4, 0.01)
+    >>> plt.plot(xx, tf.nn.gelu(xx), label='gelu')
+    >>> plt.plot(xx, tf.nn.gelu(xx, approximate=True), label='gelu, approximate')
+    >>> plt.plot(xx, gelu_linear(xx), label='gelu_linear')
+    >>> plt.legend()
+    >>> plt.grid(True)
+    """
+    inputs_abs = functional.abs(inputs)
+    inputs_sign = functional.sign(inputs)
+
+    erf = inputs_abs * -0.7071
+    erf = functional.relu(erf + 1.769)
+    erf = erf**2 * -0.1444 + 0.5
+    return inputs * (erf * inputs_sign + 0.5)
 
 
 def activation_by_name(inputs, activation="relu", name=None):
     """Typical Activation layer added hard_swish and prelu."""
     if activation is None:
         return inputs
 
     layer_name = name and activation and name + activation
     activation_lower = activation.lower()
     if activation_lower == "hard_swish":
-        return keras.layers.Activation(activation=hard_swish, name=layer_name)(inputs)
+        return layers.Activation(activation=hard_swish, name=layer_name)(inputs)
+    if activation_lower == "leaky_relu":
+        return layers.LeakyReLU(name=layer_name)(inputs)
     elif activation_lower == "mish":
-        return keras.layers.Activation(activation=mish, name=layer_name)(inputs)
+        return layers.Activation(activation=mish, name=layer_name)(inputs)
     elif activation_lower == "phish":
-        return keras.layers.Activation(activation=phish, name=layer_name)(inputs)
+        return layers.Activation(activation=phish, name=layer_name)(inputs)
     elif activation_lower == "prelu":
         shared_axes = list(range(1, len(inputs.shape)))
-        shared_axes.pop(-1 if K.image_data_format() == "channels_last" else 0)
+        shared_axes.pop(-1 if backend.image_data_format() == "channels_last" else 0)
         # print(f"{shared_axes = }")
-        return keras.layers.PReLU(shared_axes=shared_axes, alpha_initializer=tf.initializers.Constant(0.25), name=layer_name)(inputs)
+        return layers.PReLU(shared_axes=shared_axes, alpha_initializer=initializers.Constant(0.25), name=layer_name)(inputs)
     elif activation_lower.startswith("gelu/app"):
         # gelu/approximate
-        return tf.nn.gelu(inputs, approximate=True, name=layer_name)
+        return functional.gelu(inputs, approximate=True)
+    elif activation_lower.startswith("gelu/linear"):
+        return gelu_linear(inputs)
+    elif activation_lower.startswith("gelu/quick"):
+        return gelu_quick(inputs)
     elif activation_lower.startswith("leaky_relu/"):
         # leaky_relu with alpha parameter
         alpha = float(activation_lower.split("/")[-1])
-        return keras.layers.LeakyReLU(alpha=alpha, name=layer_name)(inputs)
+        return layers.LeakyReLU(alpha=alpha, name=layer_name)(inputs)
     elif activation_lower == ("hard_sigmoid_torch"):
-        return keras.layers.Activation(activation=hard_sigmoid_torch, name=layer_name)(inputs)
+        return layers.Activation(activation=hard_sigmoid_torch, name=layer_name)(inputs)
     elif activation_lower == ("squaredrelu") or activation_lower == ("squared_relu"):
-        return tf.nn.relu(inputs) ** 2  # Squared ReLU: https://arxiv.org/abs/2109.08668
+        return functional.pow(functional.relu(inputs), 2)  # Squared ReLU: https://arxiv.org/abs/2109.08668
     elif activation_lower == ("starrelu") or activation_lower == ("star_relu"):
         from keras_cv_attention_models.nfnets.nfnets import ZeroInitGain
 
-        return ZeroInitGain(use_bias=True, weight_init_value=1.0, name=layer_name)(tf.nn.relu(inputs) ** 2)  # StarReLU: s * relu(x) ** 2 + b
+        # StarReLU: s * relu(x) ** 2 + b
+        return ZeroInitGain(use_bias=True, weight_init_value=1.0, name=layer_name)(functional.pow(functional.relu(inputs), 2))
     else:
-        return keras.layers.Activation(activation=activation, name=layer_name)(inputs)
+        return layers.Activation(activation=activation, name=layer_name)(inputs)
 
 
-@tf.keras.utils.register_keras_serializable(package="kecamCommon")
-class EvoNormalization(tf.keras.layers.Layer):
+@backend.register_keras_serializable(package="kecamCommon")
+class EvoNormalization(layers.Layer):
     def __init__(self, nonlinearity=True, num_groups=-1, zero_gamma=False, momentum=0.99, epsilon=0.001, data_format="auto", **kwargs):
         # [evonorm](https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py)
         # EVONORM_B0: nonlinearity=True, num_groups=-1
         # EVONORM_S0: nonlinearity=True, num_groups > 0
         # EVONORM_B0 / EVONORM_S0 linearity: nonlinearity=False, num_groups=-1
         # EVONORM_S0A linearity: nonlinearity=False, num_groups > 0
         super().__init__(**kwargs)
         self.data_format, self.nonlinearity, self.zero_gamma, self.num_groups = data_format, nonlinearity, zero_gamma, num_groups
         self.momentum, self.epsilon = momentum, epsilon
-        self.is_channels_first = True if data_format == "channels_first" or (data_format == "auto" and K.image_data_format() == "channels_first") else False
+        self.is_channels_first = (
+            True if data_format == "channels_first" or (data_format == "auto" and backend.image_data_format() == "channels_first") else False
+        )
 
     def build(self, input_shape):
         all_axes = list(range(len(input_shape)))
         param_shape = [1] * len(input_shape)
         if self.is_channels_first:
             param_shape[1] = input_shape[1]
             self.reduction_axes = all_axes[:1] + all_axes[2:]
@@ -108,17 +141,17 @@
         self.gamma = self.add_weight(name="gamma", shape=param_shape, initializer="zeros" if self.zero_gamma else "ones", trainable=True)
         self.beta = self.add_weight(name="beta", shape=param_shape, initializer="zeros", trainable=True)
         if self.num_groups <= 0:  # EVONORM_B0
             self.moving_variance = self.add_weight(
                 name="moving_variance",
                 shape=param_shape,
                 initializer="ones",
-                synchronization=tf.VariableSynchronization.ON_READ,
+                # synchronization=tf.VariableSynchronization.ON_READ,
                 trainable=False,
-                aggregation=tf.VariableAggregation.MEAN,
+                # aggregation=tf.VariableAggregation.MEAN,
             )
         if self.nonlinearity:
             self.vv = self.add_weight(name="vv", shape=param_shape, initializer="ones", trainable=True)
 
         if self.num_groups > 0:  # EVONORM_S0
             channels_dim = input_shape[1] if self.is_channels_first else input_shape[-1]
             num_groups = int(self.num_groups)
@@ -138,53 +171,53 @@
                 self.group_shape = [-1, *input_shape[1:-1], self.__num_groups__, self.groups_dim]
                 self.group_reduction_axes = list(range(1, len(self.group_shape) - 2)) + [len(self.group_shape) - 1]  # [1, 2, 4]
                 self.group_axes = -1
                 self.var_shape = [-1, *param_shape[1:]]
 
     def __group_std__(self, inputs):
         # _group_std, https://github.com/tensorflow/tpu/blob/main/models/official/resnet/resnet_model.py#L171
-        grouped = tf.reshape(inputs, self.group_shape)
-        _, var = tf.nn.moments(grouped, self.group_reduction_axes, keepdims=True)
-        std = tf.sqrt(var + self.epsilon)
-        std = tf.repeat(std, self.groups_dim, axis=self.group_axes)
-        return tf.reshape(std, self.var_shape)
+        grouped = functional.reshape(inputs, self.group_shape)
+        _, var = functional.moments(grouped, self.group_reduction_axes, keepdims=True)
+        std = functional.sqrt(var + self.epsilon)
+        std = functional.repeat(std, self.groups_dim, axis=self.group_axes)
+        return functional.reshape(std, self.var_shape)
 
     def __batch_std__(self, inputs, training=None):
         # _batch_std, https://github.com/tensorflow/tpu/blob/main/models/official/resnet/resnet_model.py#L120
         def _call_train_():
-            _, var = tf.nn.moments(inputs, self.reduction_axes, keepdims=True)
+            _, var = functional.moments(inputs, self.reduction_axes, keepdims=True)
             # update_op = tf.assign_sub(moving_variance, (moving_variance - variance) * (1 - decay))
             delta = (self.moving_variance - var) * (1 - self.momentum)
             self.moving_variance.assign_sub(delta)
             return var
 
         def _call_test_():
             return self.moving_variance
 
-        var = K.in_train_phase(_call_train_, _call_test_, training=training)
-        return tf.sqrt(var + self.epsilon)
+        var = backend.in_train_phase(_call_train_, _call_test_, training=training)
+        return functional.sqrt(var + self.epsilon)
 
     def __instance_std__(self, inputs):
         # _instance_std, https://github.com/tensorflow/tpu/blob/main/models/official/resnet/resnet_model.py#L111
         # axes = [1, 2] if data_format == 'channels_last' else [2, 3]
-        _, var = tf.nn.moments(inputs, self.reduction_axes[1:], keepdims=True)
-        return tf.sqrt(var + self.epsilon)
+        _, var = functional.moments(inputs, self.reduction_axes[1:], keepdims=True)
+        return functional.sqrt(var + self.epsilon)
 
     def call(self, inputs, training=None, **kwargs):
         if self.nonlinearity and self.num_groups > 0:  # EVONORM_S0
             den = self.__group_std__(inputs)
-            inputs = inputs * tf.nn.sigmoid(self.vv * inputs) / den
+            inputs = inputs * functional.sigmoid(self.vv * inputs) / den
         elif self.num_groups > 0:  # EVONORM_S0a
             # EvoNorm2dS0a https://github.com/rwightman/pytorch-image-models/blob/main/timm/models/layers/evo_norm.py#L239
             den = self.__group_std__(inputs)
             inputs = inputs / den
         elif self.nonlinearity:  # EVONORM_B0
             left = self.__batch_std__(inputs, training)
             right = self.vv * inputs + self.__instance_std__(inputs)
-            inputs = inputs / tf.maximum(left, right)
+            inputs = inputs / functional.maximum(left, right)
         return inputs * self.gamma + self.beta
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "nonlinearity": self.nonlinearity,
@@ -195,233 +228,286 @@
                 "data_format": self.data_format,
             }
         )
         return config
 
 
 def batchnorm_with_activation(
-    inputs, activation=None, zero_gamma=False, epsilon=1e-5, momentum=0.9, act_first=False, use_evo_norm=False, evo_norm_group_size=-1, name=None
+    inputs, activation=None, zero_gamma=False, epsilon=1e-5, momentum=0.9, axis="auto", act_first=False, use_evo_norm=False, evo_norm_group_size=-1, name=None
 ):
     """Performs a batch normalization followed by an activation."""
     if use_evo_norm:
         nonlinearity = False if activation is None else True
         num_groups = inputs.shape[-1] // evo_norm_group_size  # Currently using gorup_size as parameter only
         return EvoNormalization(nonlinearity, num_groups=num_groups, zero_gamma=zero_gamma, epsilon=epsilon, momentum=momentum, name=name + "evo_norm")(inputs)
 
-    bn_axis = -1 if K.image_data_format() == "channels_last" else 1
-    gamma_initializer = tf.zeros_initializer() if zero_gamma else tf.ones_initializer()
+    bn_axis = (-1 if backend.image_data_format() == "channels_last" else 1) if axis == "auto" else axis
+    gamma_initializer = initializers.zeros() if zero_gamma else initializers.ones()
     if act_first and activation:
         inputs = activation_by_name(inputs, activation=activation, name=name)
-    nn = keras.layers.BatchNormalization(
+    nn = layers.BatchNormalization(
         axis=bn_axis,
         momentum=momentum,
         epsilon=epsilon,
         gamma_initializer=gamma_initializer,
         name=name and name + "bn",
     )(inputs)
     if not act_first and activation:
         nn = activation_by_name(nn, activation=activation, name=name)
     return nn
 
 
-def layer_norm(inputs, zero_gamma=False, epsilon=LAYER_NORM_EPSILON, center=True, name=None):
+def layer_norm(inputs, zero_gamma=False, epsilon=LAYER_NORM_EPSILON, center=True, axis="auto", name=None):
     """Typical LayerNormalization with epsilon=1e-5"""
-    norm_axis = -1 if K.image_data_format() == "channels_last" else 1
-    gamma_init = tf.zeros_initializer() if zero_gamma else tf.ones_initializer()
-    return keras.layers.LayerNormalization(axis=norm_axis, epsilon=epsilon, gamma_initializer=gamma_init, center=center, name=name and name + "ln")(inputs)
+    norm_axis = (-1 if backend.image_data_format() == "channels_last" else 1) if axis == "auto" else axis
+    gamma_init = initializers.zeros() if zero_gamma else initializers.ones()
+    return layers.LayerNormalization(axis=norm_axis, epsilon=epsilon, gamma_initializer=gamma_init, center=center, name=name and name + "ln")(inputs)
 
 
-def group_norm(inputs, groups=32, epsilon=BATCH_NORM_EPSILON, name=None):
+def group_norm(inputs, groups=32, epsilon=BATCH_NORM_EPSILON, axis="auto", name=None):
     """Typical GroupNormalization with epsilon=1e-5"""
-    if hasattr(keras.layers, "GroupNormalization"):
-        GroupNormalization = keras.layers.GroupNormalization  # GroupNormalization is added after TF 2.11.0
+    if hasattr(layers, "GroupNormalization"):
+        GroupNormalization = layers.GroupNormalization  # GroupNormalization is added after TF 2.11.0
     else:
         from tensorflow_addons.layers import GroupNormalization
 
-    norm_axis = -1 if K.image_data_format() == "channels_last" else 1
+    norm_axis = (-1 if backend.image_data_format() == "channels_last" else 1) if axis == "auto" else axis
     return GroupNormalization(groups=groups, axis=norm_axis, epsilon=epsilon, name=name and name + "group_norm")(inputs)
 
 
-def conv2d_no_bias(inputs, filters, kernel_size=1, strides=1, padding="VALID", use_bias=False, groups=1, use_torch_padding=True, name=None, **kwargs):
-    """Typical Conv2D with `use_bias` default as `False` and fixed padding"""
-    pad = (kernel_size[0] // 2, kernel_size[1] // 2) if isinstance(kernel_size, (list, tuple)) else (kernel_size // 2, kernel_size // 2)
-    if use_torch_padding and padding.upper() == "SAME" and max(pad) != 0:
-        inputs = keras.layers.ZeroPadding2D(padding=pad, name=name and name + "pad")(inputs)
-        padding = "VALID"
+def conv2d_no_bias(inputs, filters, kernel_size=1, strides=1, padding="valid", use_bias=False, groups=1, use_torch_padding=True, name=None, **kwargs):
+    """Typical Conv2D with `use_bias` default as `False` and fixed padding,
+    and torch initializer `uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)`
+    """
+    kernel_size = kernel_size if isinstance(kernel_size, (list, tuple)) else (kernel_size, kernel_size)
+    if isinstance(padding, str):
+        padding = padding.lower()
+        pad = (kernel_size[0] // 2, kernel_size[1] // 2) if use_torch_padding and padding == "same" else (0, 0)
+    else:  # int or list or tuple with specific value
+        pad = padding if isinstance(padding, (list, tuple)) else (padding, padding)
+        padding = "same" if max(pad) > 0 else "valid"
+
+    if use_torch_padding and not backend.is_torch_backend and padding == "same":
+        inputs = layers.ZeroPadding2D(padding=pad, name=name and name + "pad")(inputs) if max(pad) > 0 else inputs
+        padding = "valid"
+
+    kernel_initializer = kwargs.get("kernel_initializer", None)
+    if kernel_initializer is None and not backend.is_torch_backend:
+        fan_in = 1 / (float(inputs.shape[-1] * kernel_size[0] * kernel_size[1]) ** 0.5)
+        kernel_initializer = initializers.RandomUniform(-fan_in, fan_in)
 
     groups = max(1, groups)
-    return keras.layers.Conv2D(
+    return layers.Conv2D(
         filters,
         kernel_size,
         strides=strides,
-        padding=padding,
+        padding="valid" if padding == "valid" else (pad if use_torch_padding else "same"),
         use_bias=use_bias,
         groups=groups,
-        kernel_initializer=CONV_KERNEL_INITIALIZER,
+        kernel_initializer=kernel_initializer,
         name=name and name + "conv",
         **kwargs,
     )(inputs)
 
 
-def depthwise_conv2d_no_bias(inputs, kernel_size, strides=1, padding="VALID", use_bias=False, use_torch_padding=True, name=None, **kwargs):
-    """Typical DepthwiseConv2D with `use_bias` default as `False` and fixed padding"""
-    pad = (kernel_size[0] // 2, kernel_size[1] // 2) if isinstance(kernel_size, (list, tuple)) else (kernel_size // 2, kernel_size // 2)
-    if use_torch_padding and padding.upper() == "SAME" and max(pad) != 0:
-        inputs = keras.layers.ZeroPadding2D(padding=pad, name=name and name + "dw_pad")(inputs)
-        padding = "VALID"
-    return keras.layers.DepthwiseConv2D(
+def depthwise_conv2d_no_bias(inputs, kernel_size, strides=1, padding="valid", use_bias=False, use_torch_padding=True, name=None, **kwargs):
+    """Typical DepthwiseConv2D with `use_bias` default as `False` and fixed padding
+    and torch initializer `uniform(-1/sqrt(k), 1/sqrt(k)), where k = weight.size(1) * prod(*kernel_size)`
+    """
+    kernel_size = kernel_size if isinstance(kernel_size, (list, tuple)) else (kernel_size, kernel_size)
+    if isinstance(padding, str):
+        padding = padding.lower()
+        pad = (kernel_size[0] // 2, kernel_size[1] // 2) if use_torch_padding and padding == "same" else (0, 0)
+    else:  # int or list or tuple with specific value
+        pad = padding if isinstance(padding, (list, tuple)) else (padding, padding)
+        padding = "same" if max(pad) > 0 else "valid"
+
+    if use_torch_padding and not backend.is_torch_backend and padding == "same":
+        inputs = layers.ZeroPadding2D(padding=pad, name=name and name + "pad")(inputs) if max(pad) > 0 else inputs
+        padding = "valid"
+
+    depthwise_initializer = kwargs.get("depthwise_initializer", None)
+    if depthwise_initializer is None and not backend.is_torch_backend:
+        fan_in = 1 / (float(inputs.shape[-1] * kernel_size[0] * kernel_size[1]) ** 0.5)
+        depthwise_initializer = initializers.RandomUniform(-fan_in, fan_in)
+
+    return layers.DepthwiseConv2D(
         kernel_size,
         strides=strides,
-        padding=padding,
+        padding="valid" if padding == "valid" else (pad if use_torch_padding else "same"),
         use_bias=use_bias,
-        kernel_initializer=CONV_KERNEL_INITIALIZER,
+        depthwise_initializer=depthwise_initializer,
         name=name and name + "dw_conv",
         **kwargs,
     )(inputs)
 
 
+def dense_no_bias(inputs, units, use_bias=False, name=None, **kwargs):
+    """Typical Dense with `use_bias` default as `False`, and Torch Linear initializer `uniform(-1/sqrt(in_features), 1/sqrt(in_features))`"""
+    kernel_initializer = kwargs.get("kernel_initializer", None)
+    if kernel_initializer is None and not backend.is_torch_backend:
+        fan_in = 1 / (float(inputs.shape[-1]) ** 0.5)
+        kernel_initializer = initializers.RandomUniform(-fan_in, fan_in)
+    return layers.Dense(units, kernel_initializer=kernel_initializer, use_bias=use_bias, name=name, **kwargs)(inputs)
+
+
 """ Blocks """
 
 
 def output_block(inputs, filters=0, activation="relu", num_classes=1000, drop_rate=0, classifier_activation="softmax", is_torch_mode=True, act_first=False):
     nn = inputs
     if filters > 0:  # efficientnet like
         bn_eps = BATCH_NORM_EPSILON if is_torch_mode else TF_BATCH_NORM_EPSILON
         nn = conv2d_no_bias(nn, filters, 1, strides=1, use_bias=act_first, use_torch_padding=is_torch_mode, name="features_")  # Also use_bias for act_first
         nn = batchnorm_with_activation(nn, activation=activation, act_first=act_first, epsilon=bn_eps, name="features_")
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn) if len(nn.shape) == 4 else nn
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn) if len(nn.shape) == 4 else nn
         if drop_rate > 0:
-            nn = keras.layers.Dropout(drop_rate, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(drop_rate, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
     return nn
 
 
 def global_context_module(inputs, use_attn=True, ratio=0.25, divisor=1, activation="relu", use_bias=True, name=None):
     """Global Context Attention Block, arxiv: https://arxiv.org/pdf/1904.11492.pdf"""
-    height, width, filters = inputs.shape[1], inputs.shape[2], inputs.shape[-1]
+    is_channels_last = image_data_format() == "channels_last"
+    filters = inputs.shape[-1 if is_channels_last else 1]
+    height_axis, width_axis = (1, 2) if is_channels_last else (2, 3)
+    height, width = inputs.shape[height_axis], inputs.shape[width_axis]
 
     # activation could be ("relu", "hard_sigmoid")
     hidden_activation, output_activation = activation if isinstance(activation, (list, tuple)) else (activation, "sigmoid")
     reduction = make_divisible(filters * ratio, divisor, limit_round_down=0.0)
 
     if use_attn:
-        attn = keras.layers.Conv2D(1, kernel_size=1, use_bias=use_bias, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name and name + "attn_conv")(inputs)
-        attn = tf.reshape(attn, [-1, 1, 1, height * width])  # [batch, height, width, 1] -> [batch, 1, 1, height * width]
-        attn = tf.nn.softmax(attn, axis=-1)
-        context = tf.reshape(inputs, [-1, 1, height * width, filters])
-        context = attn @ context  # [batch, 1, 1, filters]
+        attn = layers.Conv2D(1, kernel_size=1, use_bias=use_bias, name=name and name + "attn_conv")(inputs)
+        attn = functional.reshape(attn, [-1, 1, height * width])  # [batch, height, width, 1] or [batch, 1, height, width] -> [batch, 1, height * width]
+        attn = functional.softmax(attn, axis=-1)
+        context = inputs if is_channels_last else functional.transpose(inputs, [0, 2, 3, 1])
+        context = functional.reshape(context, [-1, height * width, filters])
+        context = attn @ context  # [batch, 1, filters]
+        context = functional.reshape(context, [-1, 1, 1, filters]) if is_channels_last else functional.reshape(context, [-1, filters, 1, 1])
     else:
-        context = tf.reduce_mean(inputs, [1, 2], keepdims=True)
+        context = functional.reduce_mean(inputs, [height_axis, width_axis], keepdims=True)
 
-    mlp = keras.layers.Conv2D(reduction, kernel_size=1, use_bias=use_bias, name=name and name + "mlp_1_conv")(context)
-    mlp = keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, name=name and name + "ln")(mlp)
+    mlp = layers.Conv2D(reduction, kernel_size=1, use_bias=use_bias, name=name and name + "mlp_1_conv")(context)
+    mlp = layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, name=name and name + "ln")(mlp)
     mlp = activation_by_name(mlp, activation=hidden_activation, name=name)
-    mlp = keras.layers.Conv2D(filters, kernel_size=1, use_bias=use_bias, name=name and name + "mlp_2_conv")(mlp)
+    mlp = layers.Conv2D(filters, kernel_size=1, use_bias=use_bias, name=name and name + "mlp_2_conv")(mlp)
     mlp = activation_by_name(mlp, activation=output_activation, name=name)
-    return keras.layers.Multiply(name=name and name + "out")([inputs, mlp])
+    return layers.Multiply(name=name and name + "out")([inputs, mlp])
 
 
 def se_module(inputs, se_ratio=0.25, divisor=8, limit_round_down=0.9, activation="relu", use_bias=True, use_conv=True, name=None):
     """Squeeze-and-Excitation block, arxiv: https://arxiv.org/pdf/1709.01507.pdf"""
-    channel_axis = 1 if K.image_data_format() == "channels_first" else -1
-    h_axis, w_axis = [2, 3] if K.image_data_format() == "channels_first" else [1, 2]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    h_axis, w_axis = [1, 2] if image_data_format() == "channels_last" else [2, 3]
 
     # activation could be ("relu", "hard_sigmoid") for mobilenetv3
     hidden_activation, output_activation = activation if isinstance(activation, (list, tuple)) else (activation, "sigmoid")
     filters = inputs.shape[channel_axis]
     reduction = make_divisible(filters * se_ratio, divisor, limit_round_down=limit_round_down)
     # print(f"{filters = }, {se_ratio = }, {divisor = }, {reduction = }")
-    se = tf.reduce_mean(inputs, [h_axis, w_axis], keepdims=True)
+    se = functional.reduce_mean(inputs, [h_axis, w_axis], keepdims=True if use_conv else False)
     if use_conv:
-        se = keras.layers.Conv2D(reduction, kernel_size=1, use_bias=use_bias, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name and name + "1_conv")(se)
+        se = layers.Conv2D(reduction, kernel_size=1, use_bias=use_bias, name=name and name + "1_conv")(se)
     else:
-        se = keras.layers.Dense(reduction, use_bias=use_bias, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name and name + "1_dense")(se)
+        se = layers.Dense(reduction, use_bias=use_bias, name=name and name + "1_dense")(se)
     se = activation_by_name(se, activation=hidden_activation, name=name)
     if use_conv:
-        se = keras.layers.Conv2D(filters, kernel_size=1, use_bias=use_bias, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name and name + "2_conv")(se)
+        se = layers.Conv2D(filters, kernel_size=1, use_bias=use_bias, name=name and name + "2_conv")(se)
     else:
-        se = keras.layers.Dense(filters, use_bias=use_bias, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name and name + "2_dense")(se)
+        se = layers.Dense(filters, use_bias=use_bias, name=name and name + "2_dense")(se)
     se = activation_by_name(se, activation=output_activation, name=name)
-    return keras.layers.Multiply(name=name and name + "out")([inputs, se])
+    se = se if use_conv else functional.reshape(se, [-1, 1, 1, filters] if image_data_format() == "channels_last" else [-1, filters, 1, 1])
+    return layers.Multiply(name=name and name + "out")([inputs, se])
 
 
 def eca_module(inputs, gamma=2.0, beta=1.0, name=None, **kwargs):
     """Efficient Channel Attention block, arxiv: https://arxiv.org/pdf/1910.03151.pdf"""
-    channel_axis = 1 if K.image_data_format() == "channels_first" else -1
-    h_axis, w_axis = [2, 3] if K.image_data_format() == "channels_first" else [1, 2]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    h_axis, w_axis = [1, 2] if image_data_format() == "channels_last" else [2, 3]
 
     filters = inputs.shape[channel_axis]
     beta, gamma = float(beta), float(gamma)
-    tt = int((tf.math.log(float(filters)) / tf.math.log(2.0) + beta) / gamma)
+    tt = int((np.log(float(filters)) / np.log(2.0) + beta) / gamma)
     kernel_size = max(tt if tt % 2 else tt + 1, 3)
     pad = kernel_size // 2
 
-    nn = tf.reduce_mean(inputs, [h_axis, w_axis], keepdims=False)
-    nn = tf.pad(nn, [[0, 0], [pad, pad]])
-    nn = tf.expand_dims(nn, channel_axis)
+    nn = functional.reduce_mean(inputs, [h_axis, w_axis], keepdims=False)
+    nn = functional.pad(nn, [[0, 0], [pad, pad]])
+    nn = functional.expand_dims(nn, channel_axis)
 
-    nn = keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding="VALID", use_bias=False, name=name and name + "conv1d")(nn)
-    nn = tf.squeeze(nn, axis=channel_axis)
+    nn = layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding="valid", use_bias=False, name=name and name + "conv1d")(nn)
+    nn = functional.squeeze(nn, axis=channel_axis)
     nn = activation_by_name(nn, activation="sigmoid", name=name)
-    return keras.layers.Multiply(name=name and name + "out")([inputs, nn])
+    nn = nn[:, None, None] if image_data_format() == "channels_last" else nn[:, :, None, None]
+    # print(f"{inputs.shape = }, {nn.shape = }")
+    return layers.Multiply(name=name and name + "out")([inputs, nn])
 
 
 def drop_connect_rates_split(num_blocks, start=0.0, end=0.0):
     """split drop connect rate in range `(start, end)` according to `num_blocks`"""
-    drop_connect_rates = tf.split(tf.linspace(start, end, sum(num_blocks)), num_blocks)
-    return [ii.numpy().tolist() for ii in drop_connect_rates]
+    # drop_connect_rates = functional.split(functional.linspace(start, end, sum(num_blocks)), num_blocks)
+    cum_split = [sum(num_blocks[: id + 1]) for id, _ in enumerate(num_blocks[:-1])]
+    drop_connect_rates = np.split(np.linspace(start, end, sum(num_blocks)), cum_split)
+    return [ii.tolist() for ii in drop_connect_rates]
 
 
 def drop_block(inputs, drop_rate=0, name=None):
     """Stochastic Depth block by Dropout, arxiv: https://arxiv.org/abs/1603.09382"""
     if drop_rate > 0:
         noise_shape = [None] + [1] * (len(inputs.shape) - 1)  # [None, 1, 1, 1]
-        return keras.layers.Dropout(drop_rate, noise_shape=noise_shape, name=name and name + "drop")(inputs)
+        return layers.Dropout(drop_rate, noise_shape=noise_shape, name=name and name + "drop")(inputs)
     else:
         return inputs
 
 
-def addaptive_pooling_2d(inputs, output_size, reduce="mean", name=None):
+def addaptive_pooling_2d(inputs, output_size, reduce="mean", data_format="auto", name=None):
     """Auto set `pool_size` and `strides` for `MaxPool2D` or `AvgPool2D` fitting `output_size`.
     (in_height - (pool_size - strides)) / strides == out_height
     condition: pool_size >= strides, pool_size != 0, strides != 0
     strides being as large as possible: strides == in_height // out_height
     ==> pool_size = in_height - (out_height - 1) * strides, not in_height % strides, in case in_height == strides  will be 0
     """
+    data_format = image_data_format() if data_format == "auto" else data_format
+    height, width = inputs.shape[1:-1] if image_data_format() == "channels_last" else inputs.shape[2:]
     h_bins, w_bins = output_size[:2] if isinstance(output_size, (list, tuple)) else (output_size, output_size)
-    reduce_function = keras.layers.MaxPool2D if reduce.lower() == "max" else keras.layers.AvgPool2D
+    reduce_function = layers.MaxPool2D if reduce.lower() == "max" else layers.AvgPool2D
 
-    h_strides, w_strides = inputs.shape[1] // h_bins, inputs.shape[2] // w_bins
-    h_pool_size, w_pool_size = inputs.shape[1] - (h_bins - 1) * h_strides, inputs.shape[2] - (w_bins - 1) * w_strides
-    # print(f"{h_pool_size = }, {w_pool_size = }, {h_strides = }, {w_strides = }")
+    h_strides, w_strides = height // h_bins, width // w_bins
+    h_pool_size, w_pool_size = height - (h_bins - 1) * h_strides, width - (w_bins - 1) * w_strides
+    # print(f"{inputs.shape = }, {h_pool_size = }, {w_pool_size = }, {h_strides = }, {w_strides = }")
     return reduce_function(pool_size=(h_pool_size, w_pool_size), strides=(h_strides, w_strides), name=name and name + "pool")(inputs)
 
 
 """ Other layers / functions """
 
 
-@tf.keras.utils.register_keras_serializable(package="kecamCommon")
+@backend.register_keras_serializable(package="kecamCommon")
 def __anti_alias_downsample_initializer__(weight_shape, dtype="float32"):
     import numpy as np
 
-    kernel_size, channel = weight_shape[0], weight_shape[2]
-    ww = tf.cast(np.poly1d((0.5, 0.5)) ** (kernel_size - 1), dtype)
-    ww = tf.expand_dims(ww, 0) * tf.expand_dims(ww, 1)
-    ww = tf.repeat(ww[:, :, tf.newaxis, tf.newaxis], channel, axis=-2)
-    return ww
+    kernel_size, channel = (weight_shape[0], weight_shape[2]) if backend.image_data_format() == "channels_last" else (weight_shape[2], weight_shape[0])
+    ww = np.array(np.poly1d((0.5, 0.5)) ** (kernel_size - 1)).astype("float32")
+    ww = np.expand_dims(ww, 0) * np.expand_dims(ww, 1)
+    if backend.image_data_format() == "channels_last":
+        ww = np.repeat(ww[:, :, None, None], channel, axis=-2)
+    else:
+        ww = np.repeat(ww[None, None, :, :], channel, axis=0)
+    return functional.convert_to_tensor(ww, dtype=dtype)
 
 
-def anti_alias_downsample(inputs, kernel_size=3, strides=2, padding="SAME", trainable=False, name=None):
+def anti_alias_downsample(inputs, kernel_size=3, strides=2, padding="same", trainable=False, name=None):
     """DepthwiseConv2D performing anti-aliasing downsample, arxiv: https://arxiv.org/pdf/1904.11486.pdf"""
-    return keras.layers.DepthwiseConv2D(
+    return layers.DepthwiseConv2D(
         kernel_size=kernel_size,
         strides=strides,
-        padding="SAME",
+        padding="same",
         use_bias=False,
         trainable=trainable,
         depthwise_initializer=__anti_alias_downsample_initializer__,
         name=name and name + "anti_alias_down",
     )(inputs)
 
 
@@ -432,126 +518,138 @@
     new_v = max(min_value, int(vv + divisor / 2) // divisor * divisor)
     # Make sure that round down does not go down by more than 10%.
     if new_v < limit_round_down * vv:
         new_v += divisor
     return new_v
 
 
-@tf.keras.utils.register_keras_serializable(package="kecamCommon")
+@backend.register_keras_serializable(package="kecamCommon")
 def __unfold_filters_initializer__(weight_shape, dtype="float32"):
     kernel_size = weight_shape[0]
     kernel_out = kernel_size * kernel_size
-    ww = tf.reshape(tf.eye(kernel_out), [kernel_size, kernel_size, 1, kernel_out])
+    ww = np.reshape(np.eye(kernel_out, dtype="float32"), [kernel_size, kernel_size, 1, kernel_out])
     if len(weight_shape) == 5:  # Conv3D or Conv3DTranspose
-        ww = tf.expand_dims(ww, 2)
-    return ww
+        ww = np.expand_dims(ww, 2)
+    return functional.convert_to_tensor(ww)
 
 
-def fold_by_conv2d_transpose(patches, output_shape=None, kernel_size=3, strides=2, dilation_rate=1, padding="SAME", compressed="auto", name=None):
+def fold_by_conv2d_transpose(patches, output_shape=None, kernel_size=3, strides=2, dilation_rate=1, padding="same", compressed="auto", name=None):
     paded = kernel_size // 2 if padding else 0
     if compressed == "auto":
         compressed = True if len(patches.shape) == 4 else False
 
     if compressed:
         _, hh, ww, cc = patches.shape
         channel = cc // kernel_size // kernel_size
-        conv_rr = tf.reshape(patches, [-1, hh * ww, kernel_size * kernel_size, channel])
+        conv_rr = functional.reshape(patches, [-1, hh * ww, kernel_size * kernel_size, channel])
     else:
         _, hh, ww, _, _, channel = patches.shape
         # conv_rr = patches
-        conv_rr = tf.reshape(patches, [-1, hh * ww, kernel_size * kernel_size, channel])
-    conv_rr = tf.transpose(conv_rr, [0, 3, 1, 2])  # [batch, channnel, hh * ww, kernel * kernel]
-    conv_rr = tf.reshape(conv_rr, [-1, hh, ww, kernel_size * kernel_size])
+        conv_rr = functional.reshape(patches, [-1, hh * ww, kernel_size * kernel_size, channel])
+    conv_rr = functional.transpose(conv_rr, [0, 3, 1, 2])  # [batch, channnel, hh * ww, kernel * kernel]
+    conv_rr = functional.reshape(conv_rr, [-1, hh, ww, kernel_size * kernel_size])
 
-    convtrans_rr = keras.layers.Conv2DTranspose(
+    convtrans_rr = layers.Conv2DTranspose(
         filters=1,
         kernel_size=kernel_size,
         strides=strides,
         dilation_rate=dilation_rate,
-        padding="VALID",
+        padding="valid",
         output_padding=paded,
         use_bias=False,
         trainable=False,
         kernel_initializer=__unfold_filters_initializer__,
         name=name and name + "fold_convtrans",
     )(conv_rr)
 
-    out = tf.reshape(convtrans_rr[..., 0], [-1, channel, convtrans_rr.shape[1], convtrans_rr.shape[2]])
-    out = tf.transpose(out, [0, 2, 3, 1])
+    out = functional.reshape(convtrans_rr[..., 0], [-1, channel, convtrans_rr.shape[1], convtrans_rr.shape[2]])
+    out = functional.transpose(out, [0, 2, 3, 1])
     if output_shape is None:
         output_shape = [-paded, -paded]
     else:
         output_shape = [output_shape[0] + paded, output_shape[1] + paded]
     out = out[:, paded : output_shape[0], paded : output_shape[1]]
     return out
 
 
-@tf.keras.utils.register_keras_serializable(package="kecamCommon")
-class CompatibleExtractPatches(keras.layers.Layer):
-    def __init__(self, sizes=3, strides=2, rates=1, padding="SAME", compressed=True, force_conv=False, **kwargs):
+@backend.register_keras_serializable(package="kecamCommon")
+class CompatibleExtractPatches(layers.Layer):
+    def __init__(self, sizes=3, strides=2, rates=1, padding="same", compressed=True, force_conv=False, **kwargs):
         super().__init__(**kwargs)
         self.sizes, self.strides, self.rates, self.padding = sizes, strides, rates, padding
         self.compressed, self.force_conv = compressed, force_conv
 
         self.kernel_size = sizes[1] if isinstance(sizes, (list, tuple)) else sizes
         self.strides = strides[1] if isinstance(strides, (list, tuple)) else strides
-        self.dilation_rate = rates[1] if isinstance(rates, (list, tuple)) else rates
+        # dilation_rate can be 2 different values, used in DiNAT
+        self.dilation_rate = (rates if len(rates) == 2 else rates[1:3]) if isinstance(rates, (list, tuple)) else (rates, rates)
         self.filters = self.kernel_size * self.kernel_size
 
-        if len(tf.config.experimental.list_logical_devices("TPU")) != 0 or self.force_conv:
-            self.use_conv = True
+        if backend.backend() == "tensorflow":
+            import tensorflow as tf
+
+            if len(tf.config.experimental.list_logical_devices("TPU")) != 0 or self.force_conv:
+                self.use_conv = True
+            else:
+                self.use_conv = False
         else:
-            self.use_conv = False
+            self.use_conv = force_conv
 
     def build(self, input_shape):
         _, self.height, self.width, self.channel = input_shape
-        if self.padding.upper() == "SAME":
-            pad = self.kernel_size // 2
-            self.pad_value = [[0, 0], [pad, pad], [pad, pad], [0, 0]]
-            self.height, self.width = self.height + pad * 2, self.width + pad * 2
+        if self.padding.lower() == "same":
+            pad_value = self.kernel_size // 2
+            self.pad_value_list = [[0, 0], [pad_value, pad_value], [pad_value, pad_value], [0, 0]]
+            self.height, self.width = self.height + pad_value * 2, self.width + pad_value * 2
+            self.pad_value = pad_value
+        else:
+            self.pad_value = 0
 
         if self.use_conv:
-            self.conv = keras.layers.Conv2D(
+            self.conv = layers.Conv2D(
                 filters=self.filters,
                 kernel_size=self.kernel_size,
                 strides=self.strides,
                 dilation_rate=self.dilation_rate,
-                padding="VALID",
+                padding="valid",
                 use_bias=False,
                 trainable=False,
                 kernel_initializer=__unfold_filters_initializer__,
                 name=self.name and self.name + "unfold_conv",
             )
             self.conv.build([None, *input_shape[1:-1], 1])
         else:
             self._sizes_ = [1, self.kernel_size, self.kernel_size, 1]
             self._strides_ = [1, self.strides, self.strides, 1]
-            self._rates_ = [1, self.dilation_rate, self.dilation_rate, 1]
+            self._rates_ = [1, *self.dilation_rate, 1]
+        # output_size = backend.compute_conv_output_size([self.height, self.width], self.kernel_size, self.strides, self.padding, self.dilation_rate)
+        # self.output_height, self.output_width = output_size
+        super().build(input_shape)
 
     def call(self, inputs):
-        if self.padding.upper() == "SAME":
-            inputs = tf.pad(inputs, self.pad_value)
+        if self.pad_value > 0:
+            inputs = functional.pad(inputs, self.pad_value_list)
 
         if self.use_conv:
-            merge_channel = tf.transpose(inputs, [0, 3, 1, 2])
-            merge_channel = tf.reshape(merge_channel, [-1, self.height, self.width, 1])
+            merge_channel = functional.transpose(inputs, [0, 3, 1, 2])
+            merge_channel = functional.reshape(merge_channel, [-1, self.height, self.width, 1])
             conv_rr = self.conv(merge_channel)
 
             # TFLite not supporting `tf.transpose` with len(perm) > 4...
-            out = tf.reshape(conv_rr, [-1, self.channel, conv_rr.shape[1] * conv_rr.shape[2], self.filters])
-            out = tf.transpose(out, [0, 2, 3, 1])  # [batch, hh * ww, kernel * kernel, channnel]
+            out = functional.reshape(conv_rr, [-1, self.channel, conv_rr.shape[1] * conv_rr.shape[2], self.filters])
+            out = functional.transpose(out, [0, 2, 3, 1])  # [batch, hh * ww, kernel * kernel, channnel]
             if self.compressed:
-                out = tf.reshape(out, [-1, conv_rr.shape[1], conv_rr.shape[2], self.filters * self.channel])
+                out = functional.reshape(out, [-1, conv_rr.shape[1], conv_rr.shape[2], self.filters * self.channel])
             else:
-                out = tf.reshape(out, [-1, conv_rr.shape[1], conv_rr.shape[2], self.kernel_size, self.kernel_size, self.channel])
+                out = functional.reshape(out, [-1, conv_rr.shape[1], conv_rr.shape[2], self.kernel_size, self.kernel_size, self.channel])
         else:
-            out = tf.image.extract_patches(inputs, self._sizes_, self._strides_, self._rates_, "VALID")
+            out = functional.extract_patches(inputs, self._sizes_, self._strides_, self._rates_, "VALID")  # must be upper word VALID/SAME
             if not self.compressed:
                 # [batch, hh, ww, kernel, kernel, channnel]
-                out = tf.reshape(out, [-1, out.shape[1], out.shape[2], self.kernel_size, self.kernel_size, self.channel])
+                out = functional.reshape(out, [-1, out.shape[1], out.shape[2], self.kernel_size, self.kernel_size, self.channel])
         return out
 
     def get_config(self):
         base_config = super().get_config()
         base_config.update(
             {
                 "sizes": self.sizes,
@@ -561,43 +659,79 @@
                 "compressed": self.compressed,
                 "force_conv": self.force_conv,
             }
         )
         return base_config
 
 
+""" Preprocess input and decode predictions """
+
+
+def init_mean_std_by_rescale_mode(rescale_mode, convert_to_image_data_format=True):
+    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std
+        mean, std = rescale_mode
+    elif rescale_mode == "torch":
+        mean = np.array([0.485, 0.456, 0.406]).astype("float32") * 255.0
+        std = np.array([0.229, 0.224, 0.225]).astype("float32") * 255.0
+        if backend.image_data_format() != "channels_last" and convert_to_image_data_format:
+            mean, std = mean[:, None, None], std[:, None, None]
+    elif rescale_mode == "tf":  # [0, 255] -> [-1, 1]
+        mean, std = 127.5, 127.5
+        # mean, std = 127.5, 128.0
+    elif rescale_mode == "tf128":  # [0, 255] -> [-1, 1]
+        mean, std = 128.0, 128.0
+    elif rescale_mode == "raw01":
+        mean, std = 0, 255.0  # [0, 255] -> [0, 1]
+    elif rescale_mode == "clip":  # value from openai/CLIP
+        mean = np.array([0.48145466, 0.4578275, 0.40821073]).astype("float32") * 255.0
+        std = np.array([0.26862954, 0.26130258, 0.27577711]).astype("float32") * 255.0
+        if backend.image_data_format() != "channels_last" and convert_to_image_data_format:
+            mean, std = mean[:, None, None], std[:, None, None]
+    else:
+        mean, std = 0, 1  # raw inputs [0, 255]
+    return mean, std
+
+
 class PreprocessInput:
     """`rescale_mode` `torch` means `(image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]`, `tf` means `(image - 0.5) / 0.5`"""
 
     def __init__(self, input_shape=(224, 224, 3), rescale_mode="torch"):
-        self.rescale_mode = rescale_mode
-        self.input_shape = input_shape[1:-1] if len(input_shape) == 4 else input_shape[:2]
+        self.set_input_shape(input_shape)
+        self.set_rescale_mode(rescale_mode)
 
-    def __call__(self, image, resize_method="bilinear", resize_antialias=False, input_shape=None):
-        input_shape = self.input_shape if input_shape is None else input_shape[:2]
-        image = tf.convert_to_tensor(image)
-        if tf.reduce_max(image) < 2:
-            image *= 255
-        image = tf.image.resize(image, input_shape, method=resize_method, antialias=resize_antialias)
-        if len(image.shape) == 3:
-            image = tf.expand_dims(image, 0)
-
-        if self.rescale_mode == "raw":
-            return image
-        elif self.rescale_mode == "raw01":
-            return image / 255.0
+    def set_input_shape(self, input_shape):
+        input_shape = input_shape[1:] if len(input_shape) == 4 else input_shape
+        if None in input_shape:
+            self.input_shape = (None, None)  # Dynamic input_shape
+        elif len(input_shape) == 2:
+            self.input_shape = input_shape
         else:
-            return tf.keras.applications.imagenet_utils.preprocess_input(image, mode=self.rescale_mode)
+            channel_axis, channel_dim = min(enumerate(input_shape), key=lambda xx: xx[1])  # Assume the smallest value is the channel dimension
+            self.input_shape = [dim for axis, dim in enumerate(input_shape) if axis != channel_axis]
 
+    def set_rescale_mode(self, rescale_mode):
+        self.mean, self.std = init_mean_std_by_rescale_mode(rescale_mode)
+        self.rescale_mode = rescale_mode
 
-def imagenet_decode_predictions(preds, top=5):
-    from keras_cv_attention_models.imagenet.eval_func import decode_predictions
+    def __call__(self, image, resize_method="bilinear", resize_antialias=False, input_shape=None):
+        if input_shape is not None:
+            self.set_input_shape(input_shape)
+        images = np.array([image] if len(np.shape(image)) == 3 else image).astype("float32")
+        images = (images * 255) if images.max() < 2 else images
+
+        images = images if backend.image_data_format() == "channels_last" else images.transpose([0, 3, 1, 2])
+        images = functional.convert_to_tensor(images)
+        images = functional.resize(images, self.input_shape, method=resize_method, antialias=resize_antialias)
+        images = (images - self.mean) / self.std
+        return images
 
-    preds = preds.numpy() if isinstance(preds, tf.Tensor) else preds
-    return decode_predictions(preds, top=top)
 
+def add_pre_post_process(model, rescale_mode="tf", input_shape=None, post_process=None, features=None):
+    from keras_cv_attention_models.imagenet.eval_func import decode_predictions
 
-def add_pre_post_process(model, rescale_mode="tf", input_shape=None, post_process=None):
-    input_shape = model.input_shape[1:-1] if input_shape is None else input_shape
+    input_shape = model.input_shape[1:] if input_shape is None else input_shape
     model.preprocess_input = PreprocessInput(input_shape, rescale_mode=rescale_mode)
-    model.decode_predictions = imagenet_decode_predictions if post_process is None else post_process
+    model.decode_predictions = decode_predictions if post_process is None else post_process
     model.rescale_mode = rescale_mode
+
+    if features is not None:
+        model.extract_features = lambda: features
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/convnext/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/convnext/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -2,14 +2,15 @@
     global_response_normalize,
     ConvNeXt,
     ConvNeXtTiny,
     ConvNeXtSmall,
     ConvNeXtBase,
     ConvNeXtLarge,
     ConvNeXtXlarge,
+    ConvNeXtXXlarge,
 )
 from keras_cv_attention_models.convnext.convnext_v2 import (
     ConvNeXtV2,
     ConvNeXtV2Atto,
     ConvNeXtV2Femto,
     ConvNeXtV2Pico,
     ConvNeXtV2Nano,
@@ -44,54 +45,57 @@
       or a tuple value like `(0, 0.2)` indicates the drop probability linearly changes from `0 --> 0.2` for `top --> bottom` layers.
       A higher value means a higher probability will drop the deep branch. or `0` to disable.
       Default 0.1.
   classifier_activation: A `str` or callable. The activation function to use on the "top" layer if `num_classes > 0`.
       Set `classifier_activation=None` to return the logits of the "top" layer.
       Default is `None`.
   dropout: dropout rate if top layers is included.
-  pretrained: one of `None` (random initialization) or 'imagenet' (pre-training on ImageNet).
-      or 'imagenet21k-ft1k' (pre-trined on ImageNet21k, fine-tuning on ImageNet).
+  pretrained: one of `None` (random initialization) or 'imagenet' (pre-training on ImageNet),
+      or 'imagenet21k-ft1k' (pre-trined on ImageNet21k, fine-tuning on ImageNet),
+      or 'clip-ft1k' (IN-1k fine-tune and IN-12k intermediate fine-tunes for CLIP models).
       Will try to download and load pre-trained model weights if not None.
 
 Returns:
     A `keras.Model` instance.
 """
 
 __v1_tail_doc__ = """
 Model architectures:
-  | Model               | Params | FLOPs   | Input | Top1 Acc |
-  | ------------------- | ------ | ------- | ----- | -------- |
-  | ConvNeXtTiny        | 28M    | 4.49G   | 224   | 82.1     |
-  | - ImageNet21k-ft1k  | 28M    | 4.49G   | 224   | 82.9     |
-  | - ImageNet21k-ft1k  | 28M    | 13.19G  | 384   | 84.1     |
-  | ConvNeXtSmall       | 50M    | 8.73G   | 224   | 83.1     |
-  | - ImageNet21k-ft1k  | 50M    | 8.73G   | 224   | 84.6     |
-  | - ImageNet21k-ft1k  | 50M    | 25.67G  | 384   | 85.8     |
-  | ConvNeXtBase        | 89M    | 15.42G  | 224   | 83.8     |
-  | ConvNeXtBase        | 89M    | 45.32G  | 384   | 85.1     |
-  | - ImageNet21k-ft1k  | 89M    | 15.42G  | 224   | 85.8     |
-  | - ImageNet21k-ft1k  | 89M    | 45.32G  | 384   | 86.8     |
-  | ConvNeXtLarge       | 198M   | 34.46G  | 224   | 84.3     |
-  | ConvNeXtLarge       | 198M   | 101.28G | 384   | 85.5     |
-  | - ImageNet21k-ft1k  | 198M   | 34.46G  | 224   | 86.6     |
-  | - ImageNet21k-ft1k  | 198M   | 101.28G | 384   | 87.5     |
-  | ConvNeXtXLarge, 21k | 350M   | 61.06G  | 224   | 87.0     |
-  | ConvNeXtXLarge, 21k | 350M   | 179.43G | 384   | 87.8     |
+  | Model                 | Params | FLOPs   | Input | Top1 Acc |
+  | --------------------- | ------ | ------- | ----- | -------- |
+  | ConvNeXtTiny          | 28M    | 4.49G   | 224   | 82.1     |
+  | - ImageNet21k-ft1k    | 28M    | 4.49G   | 224   | 82.9     |
+  | - ImageNet21k-ft1k    | 28M    | 13.19G  | 384   | 84.1     |
+  | ConvNeXtSmall         | 50M    | 8.73G   | 224   | 83.1     |
+  | - ImageNet21k-ft1k    | 50M    | 8.73G   | 224   | 84.6     |
+  | - ImageNet21k-ft1k    | 50M    | 25.67G  | 384   | 85.8     |
+  | ConvNeXtBase          | 89M    | 15.42G  | 224   | 83.8     |
+  | ConvNeXtBase          | 89M    | 45.32G  | 384   | 85.1     |
+  | - ImageNet21k-ft1k    | 89M    | 15.42G  | 224   | 85.8     |
+  | - ImageNet21k-ft1k    | 89M    | 45.32G  | 384   | 86.8     |
+  | ConvNeXtLarge         | 198M   | 34.46G  | 224   | 84.3     |
+  | ConvNeXtLarge         | 198M   | 101.28G | 384   | 85.5     |
+  | - ImageNet21k-ft1k    | 198M   | 34.46G  | 224   | 86.6     |
+  | - ImageNet21k-ft1k    | 198M   | 101.28G | 384   | 87.5     |
+  | ConvNeXtXLarge, 21k   | 350M   | 61.06G  | 224   | 87.0     |
+  | ConvNeXtXLarge, 21k   | 350M   | 179.43G | 384   | 87.8     |
+  | ConvNeXtXXLarge, clip | 846M   | 198.09G | 256   | 88.6     |
 """
 
 ConvNeXt.__doc__ = __v1_head_doc__ + __common_head_doc__ + __common_tail_doc__ + __v1_tail_doc__
 
 ConvNeXtTiny.__doc__ = __v1_head_doc__ + """
 Args:
 """ + __common_tail_doc__ + __v1_tail_doc__
 
 ConvNeXtSmall.__doc__ = ConvNeXtTiny.__doc__
 ConvNeXtBase.__doc__ = ConvNeXtTiny.__doc__
 ConvNeXtLarge.__doc__ = ConvNeXtTiny.__doc__
 ConvNeXtLarge.__doc__ = ConvNeXtTiny.__doc__
+ConvNeXtXXlarge.__doc__ = ConvNeXtTiny.__doc__
 
 __v2_head_doc__ = """
 Keras implementation of [ConvNeXt](https://github.com/facebookresearch/ConvNeXt-V2).
 Paper [PDF 2301.00808 ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/pdf/2301.00808.pdf).
 """
 
 __v2_tail_doc__ = """
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/convnext/convnext.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/cspnext/cspnext.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,169 +1,165 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
-    ChannelAffine,
+    batchnorm_with_activation,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
-    drop_block,
-    layer_norm,
-    HeadInitializer,
+    output_block,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
-LAYER_NORM_EPSILON = 1e-6
 PRETRAINED_DICT = {
-    "convnext_tiny": {
-        "imagenet": "1deac703865e190528899d5c489afa37",
-        "imagenet21k-ft1k": {224: "b70650cc030ec528802762f58940095d", 384: "d6653ede30e25e0c6240f546675393ad"},
-    },
-    "convnext_small": {
-        "imagenet": "7e75873348d445eb2aab4200a5d49f80",
-        "imagenet21k-ft1k": {224: "da7c257650b112c1537f2753166fae49", 384: "37ff23f51f2ec9d9b6de2ea7d732ac5f"},
-    },
-    "convnext_base": {
-        "imagenet": {224: "dddac5dcd13bffc1e05688f529726f8c", 384: "ae8dc9bbca6472dc12de30db95ea1018"},
-        "imagenet21k-ft1k": {224: "40f78cec6cd327392a9d24f968f9e76b", 384: "4829ff932a930117525920317083d317"},
-    },
-    "convnext_large": {
-        "imagenet": {224: "32d401c254b623d36c22f232884000ba", 384: "01b4e72ca589c2f0ac15551e06d29818"},
-        "imagenet21k-ft1k": {224: "dc211e955875f8ab6de7518253e41a46", 384: "68ef87754d6ca634e32d2326c34ddd0b"},
-    },
-    "convnext_xlarge": {"imagenet21k-ft1k": {224: "7c7ab46f41ac34655f3e035b873a2163", 384: "636db850c0a73ba10e8ab32e91c38df6"}},
-    "convnext_v2_atto": {"imagenet": "e604fa1edfefe6207957feec4f5612db"},
-    "convnext_v2_base": {
-        "imagenet": "879caa3189ed74ed969f9348b82afe47",
-        "imagenet21k-ft1k": {224: "8d15a1e29f28e3fd8f0e6691e872ebee", 384: "b267df29706944ec4bc60b57c9778be0"},
-    },
-    "convnext_v2_femto": {"imagenet": "46d4e39a2efb4dc0aa543442b9000d89"},
-    "convnext_v2_huge": {
-        "imagenet": "347d28c6354964c30a04c5f6cadf0ebc",
-        "imagenet21k-ft1k": {384: "dfad27a621300ae254ff812827a03354", 512: "0b40599908e70e42e32c2a206f94abf3"},
-    },
-    "convnext_v2_large": {
-        "imagenet": "18327817424ada5a1c4ea257079e0694",
-        "imagenet21k-ft1k": {224: "4bce3ade2680d7181c782b65df8ed929", 384: "6d01f83513538e1f02640314e044d00e"},
-    },
-    "convnext_v2_nano": {
-        "imagenet": "32911de07188225277a47219dbdb4134",
-        "imagenet21k-ft1k": {224: "e1761b343263167eb9f4d6c33c6c892d", 384: "2980c5a37ad16cfbc6c90b8a8bb1c83f"},
-    },
-    "convnext_v2_pico": {"imagenet": "27ed3ae499e0ca6f6b5e3cf8e041ab92"},
-    "convnext_v2_tiny": {
-        "imagenet": "4b0a70c87400993385b668853c4e3654",
-        "imagenet21k-ft1k": {224: "de1db9ab2d8c565767cf81401ceed6ae", 384: "cc9028f2baa22ac1799ca1219e7b2991"},
-    },
+    "cspnext_large": {"imagenet": "e5972549741e0154356c44e6b7a5fa36"},
+    "cspnext_medium": {"imagenet": "b9f2a20571fadc26eac2b725402f4496"},
+    "cspnext_small": {"imagenet": "ac6865ca29c87bafede707fae324263a"},
+    "cspnext_tiny": {"imagenet": "edef1d5d1f42f3ee2757dcb2173fef74"},
+    "cspnext_xlarge": {"imagenet": "55f4fac58a219c40efe9bb229f94d8c2"},
 }
 
 
-def global_response_normalize(inputs, name=None):
-    nn = tf.norm(inputs, axis=(1, 2), keepdims=True)
-    nn = nn / (tf.reduce_mean(nn, axis=-1, keepdims=True) + 1e-6)
-    nn = ChannelAffine(use_bias=True, weight_init_value=0, name=name and name + "gamma")(inputs * nn)
-    return nn + inputs
-
-
-def add_with_layer_scale_and_drop_block(short, deep, layer_scale=0, residual_scale=0, drop_rate=0, name=""):
-    """Just simplify calling, perform `out = short + drop_block(layer_scale(deep))`"""
-    short = ChannelAffine(use_bias=False, weight_init_value=residual_scale, name=name + "res_gamma")(short) if residual_scale > 0 else short
-    deep = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "gamma")(deep) if layer_scale > 0 else deep
-    deep = drop_block(deep, drop_rate=drop_rate, name=name)
-    # print(f">>>> {short.shape = }, {deep.shape = }")
-    return keras.layers.Add(name=name + "output")([short, deep])
-
-
-def block(inputs, output_channel, layer_scale_init_value=1e-6, use_grn=False, drop_rate=0, activation="gelu", name=""):
-    nn = depthwise_conv2d_no_bias(inputs, kernel_size=7, padding="SAME", use_bias=True, name=name)
-    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name=name)
-    nn = keras.layers.Dense(4 * output_channel, name=name + "up_dense")(nn)
-    nn = activation_by_name(nn, activation, name=name)
-    if use_grn:
-        nn = global_response_normalize(nn, name=name + "grn_")
-    nn = keras.layers.Dense(output_channel, name=name + "down_dense")(nn)
-    return add_with_layer_scale_and_drop_block(inputs, nn, layer_scale=layer_scale_init_value, drop_rate=drop_rate, name=name)
-
-
-def ConvNeXt(
-    num_blocks=[3, 3, 9, 3],
-    out_channels=[96, 192, 384, 768],
-    stem_width=-1,
-    layer_scale_init_value=1e-6,  # 1e-6 for v1, 0 for v2
-    use_grn=False,  # False for v1, True for v2
-    head_init_scale=1.0,
+BATCH_NORM_EPSILON = 1e-3
+BATCH_NORM_MOMENTUM = 0.97
+
+
+def channel_attention(inputs, activation="hard_sigmoid_torch", name=None):
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    h_axis, w_axis = [1, 2] if image_data_format() == "channels_last" else [2, 3]
+
+    filters = inputs.shape[channel_axis]
+    nn = functional.reduce_mean(inputs, [h_axis, w_axis], keepdims=True)
+    nn = layers.Conv2D(filters, kernel_size=1, strides=1, padding="valid", use_bias=True, name=name and name + "conv")(nn)
+    nn = activation_by_name(nn, activation=activation, name=name)
+    return layers.Multiply(name=name and name + "out")([inputs, nn])
+
+
+def conv_dw_pw_block(inputs, filters, kernel_size=1, strides=1, use_depthwise_conv=False, activation="swish", name=""):
+    nn = inputs
+    if use_depthwise_conv:
+        nn = depthwise_conv2d_no_bias(nn, kernel_size, strides, padding="same", name=name)
+        nn = batchnorm_with_activation(nn, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name + "dw_")
+        kernel_size, strides = 1, 1
+    nn = conv2d_no_bias(nn, filters, kernel_size, strides, padding="same", name=name)
+    nn = batchnorm_with_activation(nn, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name)
+    return nn
+
+
+def csp_block(inputs, expansion=0.5, use_shortcut=True, use_depthwise_conv=True, activation="swish", name=""):
+    input_channels = inputs.shape[-1 if image_data_format() == "channels_last" else 1]
+    nn = conv_dw_pw_block(inputs, int(input_channels * expansion), kernel_size=3, activation=activation, name=name + "1_")
+    nn = conv_dw_pw_block(nn, input_channels, kernel_size=5, strides=1, use_depthwise_conv=use_depthwise_conv, activation=activation, name=name + "2_")
+    if use_shortcut:
+        nn = layers.Add()([inputs, nn])
+    return nn
+
+
+def csp_stack(inputs, depth, out_channels=-1, expansion=0.5, use_shortcut=True, use_depthwise_conv=True, activation="swish", name=""):
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    out_channels = inputs.shape[channel_axis] if out_channels == -1 else out_channels
+    hidden_channels = int(out_channels * expansion)
+    short = conv_dw_pw_block(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "short_")
+
+    deep = conv_dw_pw_block(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "deep_")
+    for id in range(depth):
+        block_name = name + "block{}_".format(id + 1)
+        deep = csp_block(deep, 1, use_shortcut=use_shortcut, use_depthwise_conv=use_depthwise_conv, activation=activation, name=block_name)
+
+    out = functional.concat([deep, short], axis=channel_axis)
+    out = channel_attention(out, name=name + "channel_attention_")
+    out = conv_dw_pw_block(out, out_channels, kernel_size=1, activation=activation, name=name + "output_")
+    return out
+
+
+def spatial_pyramid_pooling(inputs, pool_sizes=(5, 9, 13), activation="swish", name=""):
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    input_channels = inputs.shape[channel_axis]
+    nn = conv_dw_pw_block(inputs, input_channels // 2, kernel_size=1, activation=activation, name=name + "1_")
+    pp = [layers.MaxPool2D(pool_size=ii, strides=1, padding="same")(nn) for ii in pool_sizes]
+    nn = functional.concat([nn, *pp], axis=channel_axis)
+    nn = conv_dw_pw_block(nn, input_channels, kernel_size=1, activation=activation, name=name + "2_")
+    return nn
+
+
+def CSPNeXt(
+    num_blocks=[3, 6, 6, 3],
+    out_channels=[128, 256, 512, 1024],
+    stem_width=64,
     input_shape=(224, 224, 3),
     num_classes=1000,
-    activation="gelu",
-    drop_connect_rate=0.1,
-    classifier_activation="softmax",
+    activation="swish",
     dropout=0,
+    classifier_activation="softmax",
     pretrained=None,
-    model_name="convnext",
+    model_name="cspnext",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
 
     """ Stem """
     stem_width = stem_width if stem_width > 0 else out_channels[0]
-    nn = conv2d_no_bias(inputs, stem_width, kernel_size=4, strides=4, padding="VALID", use_bias=True, name="stem_")
-    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="stem_")
+    nn = conv_dw_pw_block(inputs, stem_width // 2, kernel_size=3, strides=2, activation=activation, name="stem_1_")
+    nn = conv_dw_pw_block(nn, stem_width // 2, kernel_size=3, strides=1, activation=activation, name="stem_2_")
+    nn = conv_dw_pw_block(nn, stem_width, kernel_size=3, strides=1, activation=activation, name="stem_3_")
 
     """ Blocks """
-    total_blocks = sum(num_blocks)
-    global_block_id = 0
-    for stack_id, (num_block, out_channel) in enumerate(zip(num_blocks, out_channels)):
-        stack_name = "stack{}_".format(stack_id + 1)
-        if stack_id > 0:
-            nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name=stack_name + "downsample_")
-            nn = conv2d_no_bias(nn, out_channel, kernel_size=2, strides=2, use_bias=True, name=stack_name + "downsample_")
-        for block_id in range(num_block):
-            block_name = stack_name + "block{}_".format(block_id + 1)
-            block_drop_rate = drop_connect_rate * global_block_id / total_blocks
-            nn = block(nn, out_channel, layer_scale_init_value, use_grn, block_drop_rate, activation, name=block_name)
-            global_block_id += 1
-
-    """  Output head """
-    if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
-        if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="head_")
-        head_init = HeadInitializer(scale=head_init_scale)
-        nn = keras.layers.Dense(
-            num_classes, dtype="float32", activation=classifier_activation, kernel_initializer=head_init, bias_initializer=head_init, name="predictions"
-        )(nn)
+    use_spps = [False, False, False, True]
+    use_shortcuts = [True, True, True, False]
 
-    model = keras.models.Model(inputs, nn, name=model_name)
+    for stack_id, (num_block, out_channel, use_spp, use_shortcut) in enumerate(zip(num_blocks, out_channels, use_spps, use_shortcuts)):
+        stack_name = "stack{}_".format(stack_id + 1)
+        nn = conv_dw_pw_block(nn, out_channel, kernel_size=3, strides=2, activation=activation, name=stack_name + "downsample_")
+        if use_spp:
+            nn = spatial_pyramid_pooling(nn, activation=activation, name=stack_name + "spp_")
+        nn = csp_stack(nn, num_block, use_shortcut=use_shortcut, activation=activation, name=stack_name)
+
+    """ Output head """
+    nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
-    reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="convnext", pretrained=pretrained)
+    reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="cspnext", pretrained=pretrained)
     return model
 
 
-def ConvNeXtTiny(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 3, 9, 3]
+@register_model
+def CSPNeXtTiny(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [1, 1, 1, 1]
+    out_channels = [48, 96, 192, 384]
+    stem_width = 24
+    return CSPNeXt(**locals(), model_name="cspnext_tiny", **kwargs)
+
+
+@register_model
+def CSPNeXtSmall(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [1, 2, 2, 1]
+    out_channels = [64, 128, 256, 512]
+    stem_width = 32
+    return CSPNeXt(**locals(), model_name="cspnext_small", **kwargs)
+
+
+@register_model
+def CSPNeXtMedium(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [2, 4, 4, 2]
     out_channels = [96, 192, 384, 768]
-    return ConvNeXt(**locals(), model_name="convnext_tiny", **kwargs)
+    stem_width = 48
+    return CSPNeXt(**locals(), model_name="cspnext_medium", **kwargs)
 
 
-def ConvNeXtSmall(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 3, 27, 3]
-    out_channels = [96, 192, 384, 768]
-    return ConvNeXt(**locals(), model_name="convnext_small", **kwargs)
-
-
-def ConvNeXtBase(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 3, 27, 3]
+@register_model
+def CSPNeXtLarge(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [3, 6, 6, 3]
     out_channels = [128, 256, 512, 1024]
-    return ConvNeXt(**locals(), model_name="convnext_base", **kwargs)
-
-
-def ConvNeXtLarge(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 3, 27, 3]
-    out_channels = [192, 384, 768, 1536]
-    return ConvNeXt(**locals(), model_name="convnext_large", **kwargs)
+    stem_width = 64
+    return CSPNeXt(**locals(), model_name="cspnext_large", **kwargs)
 
 
-def ConvNeXtXlarge(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
-    num_blocks = [3, 3, 27, 3]
-    out_channels = [256, 512, 1024, 2048]
-    return ConvNeXt(**locals(), model_name="convnext_xlarge", **kwargs)
+@register_model
+def CSPNeXtXLarge(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [4, 8, 8, 4]
+    out_channels = [160, 320, 640, 1280]
+    stem_width = 80
+    return CSPNeXt(**locals(), model_name="cspnext_xlarge", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/convnext/convnext_v2.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/convnext/convnext_v2.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,69 +1,80 @@
 from keras_cv_attention_models.convnext.convnext import ConvNeXt
+from keras_cv_attention_models.models import register_model
 
 
 def ConvNeXtV2(
     num_blocks=[3, 3, 9, 3],
     out_channels=[96, 192, 384, 768],
     stem_width=-1,
     layer_scale_init_value=0,  # 1e-6 for v1, 0 for v2
     use_grn=True,  # False for v1, True for v2
     head_init_scale=1.0,
+    layer_norm_epsilon=1e-6,  # 1e-5 for ConvNeXtXXlarge, 1e-6 for others
+    output_num_filters=-1,  # If apply additional dense + activation before output dense, <0 for not using
     input_shape=(224, 224, 3),
     num_classes=1000,
     activation="gelu",
     drop_connect_rate=0.1,
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="convnext_v2",
     kwargs=None,
 ):
     return ConvNeXt(**locals())
 
 
+@register_model
 def ConvNeXtV2Atto(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 6, 2]
     out_channels = [40, 80, 160, 320]
     return ConvNeXtV2(**locals(), model_name="convnext_v2_atto", **kwargs)
 
 
+@register_model
 def ConvNeXtV2Femto(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 6, 2]
     out_channels = [48, 96, 192, 384]
     return ConvNeXtV2(**locals(), model_name="convnext_v2_femto", **kwargs)
 
 
+@register_model
 def ConvNeXtV2Pico(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 6, 2]
     out_channels = [64, 128, 256, 512]
     return ConvNeXtV2(**locals(), model_name="convnext_v2_pico", **kwargs)
 
 
+@register_model
 def ConvNeXtV2Nano(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 8, 2]
     out_channels = [80, 160, 320, 640]
     return ConvNeXtV2(**locals(), model_name="convnext_v2_nano", **kwargs)
 
 
+@register_model
 def ConvNeXtV2Tiny(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 3, 9, 3]
     out_channels = [96, 192, 384, 768]
     return ConvNeXtV2(**locals(), model_name="convnext_v2_tiny", **kwargs)
 
 
+@register_model
 def ConvNeXtV2Base(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 3, 27, 3]
     out_channels = [128, 256, 512, 1024]
     return ConvNeXtV2(**locals(), model_name="convnext_v2_base", **kwargs)
 
 
+@register_model
 def ConvNeXtV2Large(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 3, 27, 3]
     out_channels = [192, 384, 768, 1536]
     return ConvNeXtV2(**locals(), model_name="convnext_v2_large", **kwargs)
 
 
+@register_model
 def ConvNeXtV2Huge(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 3, 27, 3]
     out_channels = [352, 704, 1408, 2816]
     return ConvNeXtV2(**locals(), model_name="convnext_v2_huge", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/cotnet/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/cotnet/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/cotnet/cotnet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/cotnet/cotnet.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
 from keras_cv_attention_models.aotnet import AotNet
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
 from keras_cv_attention_models.attention_layers import batchnorm_with_activation, conv2d_no_bias, CompatibleExtractPatches, group_norm
 
 BATCH_NORM_EPSILON = 1e-5
 PRETRAINED_DICT = {
     "cotnet101": {"imagenet": {224: "589d2c817699d96085136f2af2dd2036"}},
     "cotnet50": {"imagenet": {224: "e2b1fd313834deebb1c1f83451525254"}},
@@ -13,104 +14,122 @@
     "cotnet_se152d": {"imagenet": {224: "6a2744af16b8cc4177fef52aba7ff083", 320: "9dad11a2ec3d2c8ecac9832fcf1e9ad3"}},
     "cotnet_se50d": {"imagenet": {224: "d1e40b172d26925794f0c9dea090dba7"}},
 }
 
 
 def cot_attention(inputs, kernel_size=3, strides=1, downsample_first=True, activation="relu", name=None):
     if downsample_first and strides > 1:
-        inputs = keras.layers.ZeroPadding2D(padding=1, name=name and name + "pool_pad")(inputs)
-        inputs = keras.layers.AveragePooling2D(3, strides=2, name=name and name + "pool")(inputs)
+        inputs = layers.ZeroPadding2D(padding=1, name=name and name + "pool_pad")(inputs)
+        inputs = layers.AvgPool2D(3, strides=2, name=name and name + "pool")(inputs)
 
     # inputs, kernel_size, strides, activation, name = tf.ones([1, 7, 7, 512]), 3, 1, "relu", ""
-    filters = inputs.shape[-1]
+    height_axis, width_axis, channel_axis = (1, 2, 3) if image_data_format() == "channels_last" else (2, 3, 1)
+    filters = inputs.shape[channel_axis]
     randix = 2
 
     # key_embed
     if kernel_size // 2 != 0:
-        key_input = keras.layers.ZeroPadding2D(padding=kernel_size // 2, name=name and name + "conv_pad")(inputs)
+        key_input = layers.ZeroPadding2D(padding=kernel_size // 2, name=name and name + "conv_pad")(inputs)
     else:
         key_input = inputs
     key = conv2d_no_bias(key_input, filters, kernel_size, groups=4, name=name and name + "key_")
     key = batchnorm_with_activation(key, activation=activation, zero_gamma=False, name=name and name + "key_")
 
     # query key
-    qk = keras.layers.Concatenate(axis=-1)([inputs, key])
-    _, height, width, _ = qk.shape
+    qk = layers.Concatenate(axis=channel_axis)([inputs, key])
+    height, width = qk.shape[height_axis], qk.shape[width_axis]
 
     # embed weights from query and key, ignore `num_heads`, as it's set as `1`
     reduction = 8
     embed_ww = conv2d_no_bias(qk, filters // randix, 1, name=name and name + "embed_ww_1_")
     embed_ww = batchnorm_with_activation(embed_ww, activation=activation, zero_gamma=False, name=name and name + "embed_ww_1_")
     embed_filters = kernel_size * kernel_size * filters // reduction
     embed_ww = conv2d_no_bias(embed_ww, embed_filters, 1, use_bias=True, name=name and name + "embed_ww_2_")
     embed_ww = group_norm(embed_ww, groups=filters // reduction, epsilon=BATCH_NORM_EPSILON, name=name and name + "embed_ww_")
-    embed_ww = tf.reshape(embed_ww, (-1, height, width, filters // reduction, kernel_size * kernel_size))
-    embed_ww = tf.expand_dims(tf.transpose(embed_ww, [0, 1, 2, 4, 3]), axis=-2)  # expand dim on `reduction` axis
 
     # matmul, local_conv
     embed = conv2d_no_bias(inputs, filters, 1, name=name and name + "embed_1_")
     embed = batchnorm_with_activation(embed, activation=None, zero_gamma=False, name=name and name + "embed_1_")
 
     # unfold_j = torch.nn.Unfold(kernel_size=kernel_size, dilation=1, padding=1, stride=1)
     # x2 = unfold_j(bb).view(-1, reduction, filters // reduction, kernel_size * kernel_size, height, width)
     # y2 = (ww.unsqueeze(2) * x2.unsqueeze(1)).sum(-3).view(-1, filters, height, width)
     # sizes, patch_strides = [1, kernel_size, kernel_size, 1], [1, 1, 1, 1]
-    # embed = keras.layers.ZeroPadding2D(padding=kernel_size // 2, name=name and name + "embed_pad")(embed)
-    # embed = tf.image.extract_patches(embed, sizes=sizes, strides=patch_strides, rates=(1, 1, 1, 1), padding="VALID")
-    embed = CompatibleExtractPatches(sizes=kernel_size, strides=1, name=name and name + "patchs_")(embed)
-    embed = tf.reshape(embed, [-1, height, width, kernel_size * kernel_size, reduction, filters // reduction])
-
-    embed_out = keras.layers.Multiply(name=name and name + "local_conv_mul")([embed, embed_ww])
-    embed_out = tf.reduce_sum(embed_out, axis=-3)  # reduce on `kernel_size * kernel_size` axis
-    embed_out = tf.reshape(embed_out, [-1, height, width, filters])
+    # embed = layers.ZeroPadding2D(padding=kernel_size // 2, name=name and name + "embed_pad")(embed)
+    # embed = functional.extract_patches(embed, sizes=sizes, strides=patch_strides, rates=(1, 1, 1, 1), padding="valid")
+    if backend.is_torch_backend:
+        embed = functional.extract_patches(embed, sizes=kernel_size, strides=1, padding="same", data_format=image_data_format())
+    else:
+        embed = CompatibleExtractPatches(sizes=kernel_size, strides=1, padding="same", name=name and name + "patchs_")(embed)
+
+    if image_data_format() == "channels_last":
+        embed_ww = functional.reshape(embed_ww, (-1, height, width, filters // reduction, kernel_size * kernel_size))
+        reduction_axis, kernel_axis = -2, -3
+        embed_ww = functional.expand_dims(functional.transpose(embed_ww, [0, 1, 2, 4, 3]), axis=reduction_axis)  # expand dim on `reduction` axis
+        embed = functional.reshape(embed, [-1, height, width, kernel_size * kernel_size, reduction, filters // reduction])
+    else:
+        embed_ww = functional.reshape(embed_ww, (-1, filters // reduction, kernel_size * kernel_size, height, width))
+        reduction_axis, kernel_axis = 2, 1
+        embed_ww = functional.expand_dims(functional.transpose(embed_ww, [0, 2, 1, 3, 4]), axis=reduction_axis)  # expand dim on `reduction` axis
+        embed = functional.reshape(embed, [-1, filters, kernel_size * kernel_size, height * width])
+        embed = functional.transpose(embed, [0, 2, 1, 3])
+        embed = functional.reshape(embed, [-1, kernel_size * kernel_size, reduction, filters // reduction, height, width])
+
+    embed_out = layers.Multiply(name=name and name + "local_conv_mul")([embed, embed_ww])
+    embed_out = functional.reduce_sum(embed_out, axis=kernel_axis)  # reduce on `kernel_size * kernel_size` axis
+    embed_out = functional.reshape(embed_out, [-1, height, width, filters] if image_data_format() == "channels_last" else [-1, filters, height, width])
     embed_out = batchnorm_with_activation(embed_out, activation="swish", zero_gamma=False, name=name and name + "embed_2_")
 
     # attention
-    attn = keras.layers.Add()([embed_out, key])
-    attn = tf.reduce_mean(attn, axis=[1, 2], keepdims=True)
+    attn = layers.Add()([embed_out, key])
+    attn = functional.reduce_mean(attn, axis=[height_axis, width_axis], keepdims=True)
     # attn se module
     attn_se_filters = max(filters * randix // 4, 32)
-    # attn = keras.layers.Dense(attn_se_filters, use_bias=True, name=name and name + "attn_se_dense_1")(attn)
+    # attn = layers.Dense(attn_se_filters, use_bias=True, name=name and name + "attn_se_dense_1")(attn)
     attn = conv2d_no_bias(attn, attn_se_filters, 1, use_bias=True, name=name and name + "attn_se_1_")
     attn = batchnorm_with_activation(attn, activation=activation, zero_gamma=False, name=name and name + "attn_se_")
-    # attn = keras.layers.Dense(filters * randix, use_bias=True, name=name and name + "attn_se_dense_2")(attn)
+    # attn = layers.Dense(filters * randix, use_bias=True, name=name and name + "attn_se_dense_2")(attn)
     attn = conv2d_no_bias(attn, filters * randix, 1, use_bias=True, name=name and name + "attn_se_2_")
-    attn = tf.reshape(attn, [-1, 1, 1, filters, randix])
-    # attn = tf.nn.softmax(attn, axis=-1)
-    attn = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
+    attn = functional.reshape(attn, [-1, 1, 1, filters, randix] if image_data_format() == "channels_last" else [-1, filters, randix, 1, 1])
+    # attn = functional.nn.softmax(attn, axis=-1)
+    randix_axis = -1 if image_data_format() == "channels_last" else 2
+    attn = layers.Softmax(axis=randix_axis, name=name and name + "attention_scores")(attn)
 
     # value and output
-    value = keras.layers.Concatenate(axis=-1)([tf.expand_dims(embed_out, -1), tf.expand_dims(key, -1)])
-    output = keras.layers.Multiply()([value, attn])
-    output = tf.reduce_sum(output, axis=-1, name=name and name + "out")
+    value = layers.Concatenate(axis=randix_axis)([functional.expand_dims(embed_out, randix_axis), functional.expand_dims(key, randix_axis)])
+    output = layers.Multiply()([value, attn])
+    output = functional.reduce_sum(output, axis=randix_axis, name=name and name + "out")
 
     if not downsample_first and strides > 1:
-        output = keras.layers.ZeroPadding2D(padding=1, name=name and name + "pool_pad")(output)
-        output = keras.layers.AveragePooling2D(3, strides=2, name=name and name + "pool")(output)
+        output = layers.ZeroPadding2D(padding=1, name=name and name + "pool_pad")(output)
+        output = layers.AvgPool2D(3, strides=2, name=name and name + "pool")(output)
     return output
 
 
 def CotNet(input_shape=(224, 224, 3), bn_after_attn=False, shortcut_type="avg", attn_types="cot", pretrained="imagenet", **kwargs):
     model = AotNet(input_shape=input_shape, attn_types=attn_types, bn_after_attn=bn_after_attn, shortcut_type=shortcut_type, **kwargs)
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="cotnet", pretrained=pretrained)
     return model
 
 
+@register_model
 def CotNet50(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 6, 3]
     strides = [1, 2, 2, 2]
     return CotNet(**locals(), **kwargs, model_name="cotnet50")
 
 
+@register_model
 def CotNet101(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 23, 3]
     strides = [1, 2, 2, 2]
     return CotNet(**locals(), **kwargs, model_name="cotnet101")
 
 
+@register_model
 def CotNetSE50D(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 6, 3]
     strides = [2, 2, 2, 2]
     attn_types = ["sa", "sa", ["cot", "sa"] * 3, "cot"]
     attn_params = [
         {"downsample_first": True, "groups": 1, "activation": "swish"},  # sa, stack1
         {"downsample_first": True, "groups": 1, "activation": "swish"},  # sa, stack2
@@ -119,14 +138,15 @@
     ]
     stem_type = "deep"
     stem_width = 64
     stem_downsample = False
     return CotNet(**locals(), **kwargs, model_name="cotnet_se50d")
 
 
+@register_model
 def CotNetSE101D(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 23, 3]
     strides = [2, 2, 2, 2]
     attn_types = ["sa", "sa", ["cot", "sa"] * 12, "cot"]
     attn_params = [
         {"downsample_first": True, "groups": 1, "activation": "swish"},  # sa, stack1
         {"downsample_first": True, "groups": 1, "activation": "swish"},  # sa, stack2
@@ -135,14 +155,15 @@
     ]
     stem_type = "deep"
     stem_width = 128
     stem_downsample = False
     return CotNet(**locals(), **kwargs, model_name="cotnet_se101d")
 
 
+@register_model
 def CotNetSE152D(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 8, 36, 3]
     strides = [2, 2, 2, 2]
     attn_types = ["sa", "sa", ["cot", "sa"] * 18, "cot"]
     attn_params = [
         {"downsample_first": True, "groups": 1, "activation": "swish"},  # sa, stack1
         {"downsample_first": True, "groups": 1, "activation": "swish"},  # sa, stack2
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/davit/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/davit/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/davit/davit.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/stable_diffusion/unet.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,251 +1,239 @@
-import tensorflow as tf
-from tensorflow import keras
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, models, initializers, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
-    ChannelAffine,
+    activation_by_name,
     conv2d_no_bias,
-    depthwise_conv2d_no_bias,
-    drop_block,
-    layer_norm,
-    mlp_block,
-    multi_head_self_attention,
-    output_block,
+    group_norm,
     add_pre_post_process,
+    qkv_to_multi_head_channels_last_format,
+    scaled_dot_product_attention,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
+from keras_cv_attention_models.stable_diffusion.eval_func import RunPrediction
 
-PRETRAINED_DICT = {
-    "davit_t": {"imagenet": "040a215cbcf1c0ce06db665cdcf6f9ac"},
-    "davit_s": {"imagenet": "b95558071c639815f4ab2e9d09a4141f"},
-    "davit_b": {"imagenet": "89e50de7a70ea7b2404f8f57369d8015"},
-}
+LAYER_NORM_EPSILON = 1e-6
 
+PRETRAINED_DICT = {"unet": {"v1_5": "30b5e8755d2a04211980e608df14f133"}}
 
-def multi_head_self_attention_channel(
-    inputs, num_heads=4, key_dim=0, out_shape=None, out_weight=True, qkv_bias=False, out_bias=False, attn_dropout=0, output_dropout=0, name=None
-):
-    _, hh, ww, cc = inputs.shape
-    key_dim = key_dim if key_dim > 0 else cc // num_heads
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
-    out_shape = cc if out_shape is None or not out_weight else out_shape
-    qkv_out = num_heads * key_dim
-
-    qkv = keras.layers.Dense(qkv_out * 3, use_bias=qkv_bias, name=name and name + "qkv")(inputs)
-    qkv = tf.reshape(qkv, [-1, qkv.shape[1] * qkv.shape[2], qkv.shape[-1]])
-    value, query, key = tf.split(qkv, 3, axis=-1)  # Matching weights from PyTorch
-    query = tf.transpose(tf.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
-    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  #  [batch, num_heads, key_dim, hh * ww]
-
-    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale  # [batch, num_heads, key_dim, key_dim]
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
-    attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores) if attn_dropout > 0 else attention_scores
-
-    # value = [batch, num_heads, key_dim, hh * ww], attention_output = [batch, num_heads, key_dim, hh * ww]
-    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
-    attention_output = tf.transpose(attention_output, perm=[0, 3, 1, 2])  # [batch, hh * ww, num_heads, key_dim]
-    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * key_dim])
-    # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
-
-    if out_weight:
-        # [batch, hh, ww, num_heads * key_dim] * [num_heads * key_dim, out] --> [batch, hh, ww, out]
-        attention_output = keras.layers.Dense(out_shape, use_bias=out_bias, name=name and name + "output")(attention_output)
-    attention_output = keras.layers.Dropout(output_dropout, name=name and name + "out_drop")(attention_output) if output_dropout > 0 else attention_output
-    return attention_output
-
-
-def __window_partition__(inputs, patch_height, patch_width, window_height, window_width):
-    input_channel = inputs.shape[-1]
-    # print(f">>>> window_attention {inputs.shape = }, {patch_height = }, {patch_width = }, {window_height = }, {window_width = }")
-    # [batch * patch_height, window_height, patch_width, window_width * input_channel], limit transpose perm <= 4
-    nn = tf.reshape(inputs, [-1, window_height, patch_width, window_width * input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, patch_width, window_height, window_width * input_channel]
-    nn = tf.reshape(nn, [-1, window_height, window_width, input_channel])  # [batch * patch_height * patch_width, window_height, window_width, input_channel]
-    return nn
-
-
-def __window_reverse__(inputs, patch_height, patch_width, window_height, window_width):
-    input_channel = inputs.shape[-1]
-    # [batch * patch_height, patch_width, window_height, window_width * input_channel], limit transpose perm <= 4
-    nn = tf.reshape(inputs, [-1, patch_width, window_height, window_width * input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, window_height, patch_width, window_width * input_channel]
-    nn = tf.reshape(nn, [-1, patch_height * window_height, patch_width * window_width, input_channel])
-    return nn
 
+@backend.register_keras_serializable(package="kecam")
+class SinusoidalTimeStepEmbedding(layers.Layer):
+    def __init__(self, hidden_channels=320, max_period=10000, **kwargs):
+        super().__init__(**kwargs)
+        self.hidden_channels, self.max_period = hidden_channels, max_period
+
+    def build(self, input_shape):
+        # input_shape: [batch]
+        half = self.hidden_channels // 2  # half the channels are sin and the other half is cos
+        frequencies = np.exp(-np.log(self.max_period) * np.arange(0, half, dtype="float32") / half)
+        positions = np.arange(self.max_period).astype("float32")
+        embeddings = positions[:, None] * frequencies[None]
+        embeddings = np.concatenate([np.cos(embeddings), np.sin(embeddings)], axis=-1).astype("float32")
+
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("embeddings", functional.convert_to_tensor(embeddings, dtype=self.compute_dtype), persistent=False)
+        else:
+            self.embeddings = functional.convert_to_tensor(embeddings, dtype=self.compute_dtype)
+        super().build(input_shape)
+
+    def call(self, inputs, **kwargs):
+        return functional.embedding_lookup(self.embeddings, inputs)
+
+    def compute_output_shape(self, input_shape):
+        return [None, self.hidden_channels]
+
+    def get_config(self):
+        base_config = super().get_config()
+        base_config.update({"hidden_channels": self.hidden_channels, "max_period": self.max_period})
+        return base_config
+
+
+def cross_attention(inputs, condition=None, num_heads=4, head_dim=0, name=""):
+    _, bb, cc = inputs.shape
+    head_dim = head_dim if head_dim > 0 else cc // num_heads
+    emded_dim = int(num_heads * head_dim)
+    condition = inputs if condition is None else condition
+
+    query = layers.Dense(emded_dim, use_bias=False, name=name and name + "query")(inputs)
+    key = layers.Dense(emded_dim, use_bias=False, name=name and name + "key")(condition)
+    value = layers.Dense(emded_dim, use_bias=False, name=name and name + "value")(condition)
+
+    query, key, value = qkv_to_multi_head_channels_last_format(query, key, value, num_heads, data_format="channels_last")
+    output_shape = [-1, -1 if bb is None else bb, cc]
+    return scaled_dot_product_attention(query, key, value, output_shape, out_weight=True, out_bias=True, name=name)
+
+
+def attention_mlp_block(inputs, condition=None, mlp_ratio=4, num_heads=4, head_dim=0, name=""):
+    nn = layers.LayerNormalization(axis=-1, epsilon=LAYER_NORM_EPSILON, name=name + "attn_ln")(inputs)  # "channels_first" also using axis=-1
+    nn = cross_attention(nn, condition=None, num_heads=num_heads, head_dim=head_dim, name=name + "attn_")
+    attn_out = layers.Add(name=name + "attn_out")([inputs, nn])
+
+    """ Attention with condition """
+    if condition is not None:
+        nn = layers.LayerNormalization(axis=-1, epsilon=LAYER_NORM_EPSILON, name=name + "cond_attn_ln")(attn_out)  # "channels_first" also using axis=-1
+        nn = cross_attention(nn, condition=condition, num_heads=num_heads, head_dim=head_dim, name=name + "cond_attn_")
+        attn_out = layers.Add(name=name + "cond_attn_out")([attn_out, nn])
+
+    """ Feed forward """
+    input_channels = inputs.shape[-1]
+    nn = layers.LayerNormalization(axis=-1, epsilon=LAYER_NORM_EPSILON, name=name + "ffn_ln")(attn_out)  # "channels_first" also using axis=-1
+    nn = layers.Dense(input_channels * mlp_ratio * 2, use_bias=True, name=name + "ffn_gate_dense")(nn)
+    fft, fft_gate = functional.split(nn, 2, axis=-1)
+    nn = fft * activation_by_name(fft_gate, activation="gelu", name=name + "gate_")
 
-def __grid_window_partition__(inputs, patch_height, patch_width, window_height, window_width):
-    input_channel = inputs.shape[-1]
-    nn = tf.reshape(inputs, [-1, window_height, patch_height, window_width * patch_width * input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch, patch_height, window_height, window_width * patch_width * input_channel]
-    nn = tf.reshape(nn, [-1, window_height * window_width, patch_width, input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, patch_width, window_height * window_width, input_channel]
-    nn = tf.reshape(nn, [-1, window_height, window_width, input_channel])
+    nn = layers.Dense(input_channels, use_bias=True, name=name + "mlp.down_proj")(nn)
+    nn = layers.Add(name=name + "mlp_output")([attn_out, nn])
     return nn
 
 
-def __grid_window_reverse__(inputs, patch_height, patch_width, window_height, window_width):
-    input_channel = inputs.shape[-1]
-    nn = tf.reshape(inputs, [-1, patch_width, window_height * window_width, input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, window_height * window_width, patch_width, input_channel]
-    nn = tf.reshape(nn, [-1, patch_height, window_height, window_width * patch_width * input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch, window_height, patch_height, window_width * patch_width * input_channel]
-    nn = tf.reshape(nn, [-1, window_height * patch_height, window_width * patch_width, input_channel])
-    return nn
-
-
-def window_attention(inputs, window_size, num_heads=4, is_grid=False, attention_block=None, name=None, **kwargs):
-    input_channel = inputs.shape[-1]
-    window_size = window_size if isinstance(window_size, (list, tuple)) else [window_size, window_size]
-    window_height = window_size[0] if window_size[0] < inputs.shape[1] else inputs.shape[1]
-    window_width = window_size[1] if window_size[1] < inputs.shape[2] else inputs.shape[2]
-
-    # window_partition, partition windows, ceil mode
-    patch_height, patch_width = int(tf.math.ceil(inputs.shape[1] / window_height)), int(tf.math.ceil(inputs.shape[2] / window_width))
-    should_pad_hh, should_pad_ww = patch_height * window_height - inputs.shape[1], patch_width * window_width - inputs.shape[2]
-    # print(f">>>> window_attention {inputs.shape = }, {should_pad_hh = }, {should_pad_ww = }")
-    if should_pad_hh or should_pad_ww:
-        inputs = tf.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
-
-    if is_grid:
-        nn = __grid_window_partition__(inputs, patch_height, patch_width, window_height, window_width)
+def spatial_transformer_block(inputs, condition=None, num_attention_block=1, mlp_ratio=4, num_heads=4, head_dim=0, name=""):
+    input_channels = inputs.shape[-1 if backend.image_data_format() == "channels_last" else 1]
+    nn = group_norm(inputs, epsilon=LAYER_NORM_EPSILON, name=name + "in_layers_")
+    nn = conv2d_no_bias(nn, input_channels, kernel_size=1, use_bias=True, name=name + "in_layers_")
+
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)
+    pre_shape = functional.shape(nn) if None in nn.shape[1:] or -1 in nn.shape[1:] else [-1, *nn.shape[1:]]  # Could be dynamic shape, reshape back later
+    nn = layers.Reshape([-1, nn.shape[-1]])(nn)
+
+    for attention_block_id in range(num_attention_block):
+        block_name = name + "{}_".format(attention_block_id + 1)
+        nn = attention_mlp_block(nn, condition, mlp_ratio=mlp_ratio, num_heads=num_heads, head_dim=head_dim, name=block_name)
+    nn = functional.reshape(nn, pre_shape)
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)
+    nn = conv2d_no_bias(nn, input_channels, kernel_size=1, use_bias=True, name=name + "out_layers_")
+    return layers.Add(name=name + "output")([inputs, nn])
+
+
+def res_block(inputs, time_embedding=None, labels_embedding=None, out_channels=-1, epsilon=1e-5, activation="swish", dropout=0, name=""):
+    input_channels = inputs.shape[-1 if backend.image_data_format() == "channels_last" else 1]
+    out_channels = out_channels if out_channels > 0 else input_channels
+    if input_channels == out_channels:
+        short = inputs
     else:
-        nn = __window_partition__(inputs, patch_height, patch_width, window_height, window_width)
-
-    if attention_block:
-        nn = attention_block(nn, num_heads=num_heads, name=name, **kwargs)
-    else:
-        nn = multi_head_self_attention(nn, num_heads=num_heads, qkv_bias=True, out_bias=True, name=name)
-
-    # window_reverse, merge windows
-    if is_grid:
-        nn = __grid_window_reverse__(nn, patch_height, patch_width, window_height, window_width)
-    else:
-        nn = __window_reverse__(nn, patch_height, patch_width, window_height, window_width)
-
-    if should_pad_hh or should_pad_ww:
-        nn = nn[:, : nn.shape[1] - should_pad_hh, : nn.shape[2] - should_pad_ww, :]  # In case should_pad_hh or should_pad_ww is 0
-
-    return nn
-
-
-def conv_positional_encoding(inputs, kernel_size=3, use_norm=False, activation="gelu", name=""):
-    nn = depthwise_conv2d_no_bias(inputs, kernel_size, padding="SAME", use_bias=True, name=name)
-    if use_norm:
-        nn = layer_norm(nn, name=name)
-    if activation is not None:
-        nn = activation_by_name(nn, activation, name=name)
-    return keras.layers.Add(name=name + "output")([inputs, nn])
-
+        short = conv2d_no_bias(inputs, out_channels, kernel_size=1, use_bias=True, name=name + "short_")
 
-def davit_block(
-    inputs, window_size, num_heads=4, use_channel_attn=False, mlp_ratio=4, mlp_drop_rate=0, attn_drop_rate=0, drop_rate=0, layer_scale=-1, name=None
-):
-    input_channel = inputs.shape[-1]
-
-    pre_attn = conv_positional_encoding(inputs, 3, use_norm=False, activation=None, name=name + "pre_attn_cpe_")
-    attn = layer_norm(pre_attn, name=name + "attn_")
-    if use_channel_attn:
-        attn = multi_head_self_attention_channel(attn, num_heads, qkv_bias=True, out_bias=True, name=name + "channel_attn_")
-    else:
-        attn = window_attention(attn, window_size, num_heads, name=name + "attn_")
-    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "1_gamma")(attn) if layer_scale >= 0 else attn
-    attn = drop_block(attn, drop_rate=drop_rate, name=name + "attn_")
-    # print(f"{pre_attn.shape = }, {attn.shape = }, {inputs.shape = }")
-    attn_out = keras.layers.Add(name=name + "attn_out")([pre_attn, attn])
-
-    pre_ffn = conv_positional_encoding(attn_out, 3, use_norm=False, activation=None, name=name + "pre_ffn_cpe_")
-    mlp = layer_norm(pre_ffn, name=name + "mlp_")
-    mlp = mlp_block(mlp, int(input_channel * mlp_ratio), drop_rate=mlp_drop_rate, use_conv=False, activation="gelu", name=name + "mlp_")
-    mlp = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "2_gamma")(mlp) if layer_scale >= 0 else mlp
-    mlp = drop_block(mlp, drop_rate=drop_rate, name=name + "mlp_")
-    return keras.layers.Add(name=name + "output")([pre_ffn, mlp])
-
-
-def DaViT(
-    num_blocks=[2, 2, 6, 2],
-    out_channels=[96, 192, 384, 768],
-    num_heads=[3, 6, 12, 24],
-    stem_width=-1,
-    stem_patch_size=4,
-    # window_size=7,
-    window_ratio=32,
+    nn = group_norm(inputs, epsilon=epsilon, name=name + "in_layers_")
+    nn = activation_by_name(nn, activation=activation, name=name + "in_layers_")
+    nn = conv2d_no_bias(nn, out_channels, kernel_size=3, use_bias=True, padding="SAME", name=name + "in_layers_")
+
+    if time_embedding is not None:
+        emb = activation_by_name(time_embedding, activation=activation, name=name + "emb_layers_")
+        emb = layers.Dense(out_channels, name=name + "emb_layers_dense")(emb)
+        emb = emb[:, None, None, :] if backend.image_data_format() == "channels_last" else emb[:, :, None, None]
+        nn = nn + emb
+
+    if labels_embedding is not None:
+        emb = activation_by_name(labels_embedding, activation=activation, name=name + "labels_emb_layers_")
+        emb = layers.Dense(out_channels, name=name + "labels_emb_layers_dense")(emb)
+        emb = emb[:, None, None, :] if backend.image_data_format() == "channels_last" else emb[:, :, None, None]
+        nn = nn + emb
+
+    nn = group_norm(nn, epsilon=epsilon, name=name + "out_layers_")
+    nn = activation_by_name(nn, activation=activation, name=name + "out_layers_")
+    nn = conv2d_no_bias(nn, out_channels, kernel_size=3, use_bias=True, padding="SAME", name=name + "out_layers_")
+    nn = layers.Dropout(dropout=dropout)(nn) if dropout > 0 else nn
+    return layers.Add(name=name + "out")([nn, short])
+
+
+@register_model
+def UNet(
+    input_shape=(64, 64, 4),
+    num_blocks=[2, 2, 2, 2],
+    hidden_channels=320,
+    hidden_expands=[1, 2, 4, 4],
+    num_attention_blocks=[1, 1, 1, 0],  # attention_blocks after each res_block in each stack
+    num_heads=8,
     mlp_ratio=4,
-    layer_scale=-1,
-    input_shape=(224, 224, 3),
-    num_classes=1000,
-    drop_connect_rate=0,
-    classifier_activation="softmax",
+    conditional_embedding=768,  # > 0 value for using text conditional as generating instruction.
+    num_classes=0,  # > 0 value for also using labels as generating instruction.
+    activation="swish",
     dropout=0,
-    pretrained=None,
-    model_name="davit",
-    kwargs=None,
+    pretrained="v1_5",
+    model_name="unet",
+    kwargs=None,  # Not using, recieving parameter
 ):
-    """Patch stem"""
-    inputs = keras.layers.Input(input_shape)
-    stem_width = stem_width if stem_width > 0 else out_channels[0]
-    nn = conv2d_no_bias(inputs, stem_width, kernel_size=7, strides=stem_patch_size, use_bias=True, padding="SAME", name="stem_")
-    nn = layer_norm(nn, name="stem_")
-    # window_size = [input_shape[0] // window_ratio, input_shape[1] // window_ratio]
-    window_size = [int(tf.math.ceil(input_shape[0] / window_ratio)), int(tf.math.ceil(input_shape[1] / window_ratio))]
-    # window_size = window_size[:2] if isinstance(window_size, (list, tuple)) else [window_size, window_size]
-
-    """ stages """
-    total_blocks = sum(num_blocks)
-    global_block_id = 0
-    for stack_id, (num_block, out_channel, num_head) in enumerate(zip(num_blocks, out_channels, num_heads)):
-        stack_name = "stack{}_".format(stack_id + 1)
-        if stack_id > 0:
-            ds_name = stack_name + "downsample_"
-            nn = layer_norm(nn, name=ds_name)
-            # Set use_torch_padding=False, as kernel_size == 2, otherwise shape will be enlarged by 1
-            nn = conv2d_no_bias(nn, out_channel, kernel_size=2, strides=2, use_bias=True, padding="SAME", use_torch_padding=False, name=ds_name)
-        for block_id in range(num_block):
-            block_name = stack_name + "block{}_".format(block_id + 1)
-            block_drop_rate = drop_connect_rate * global_block_id / total_blocks
-            use_channel_attn = False if block_id % 2 == 0 else True
-            nn = davit_block(nn, window_size, num_head, use_channel_attn, mlp_ratio, drop_rate=block_drop_rate, layer_scale=layer_scale, name=block_name)
-            global_block_id += 1
-    nn = layer_norm(nn, name="pre_output_")
-
-    nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
-    model = keras.models.Model(inputs, nn, name=model_name)
-    add_pre_post_process(model, rescale_mode="torch")
-    reload_model_weights(model, PRETRAINED_DICT, "davit", pretrained)
-    return model
-
-
-def DaViT_T(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [2, 2, 6, 2]
-    return DaViT(**locals(), model_name="davit_t", **kwargs)
-
-
-def DaViT_S(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [2, 2, 18, 2]
-    return DaViT(**locals(), model_name="davit_s", **kwargs)
-
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    inputs = layers.Input(backend.align_input_shape_by_image_data_format(input_shape), name="inputs")
+    channel_axis = -1 if backend.image_data_format() == "channels_last" else 1
+    time_steps = layers.Input([], dtype="int64", name="time_steps")
+    condition = layers.Input([None, conditional_embedding], name="condition") if conditional_embedding > 0 else None
+
+    time_embedding = SinusoidalTimeStepEmbedding(hidden_channels=hidden_channels, name="time_embedding")(time_steps)
+    time_embedding = layers.Dense(hidden_channels * 4, name="time_embed_1_dense")(time_embedding)
+    time_embedding = activation_by_name(time_embedding, activation=activation, name="time_embed_")
+    time_embedding = layers.Dense(hidden_channels * 4, name="time_embed_2_dense")(time_embedding)
+
+    if num_classes > 0:
+        labels_inputs = layers.Input([], dtype="int64", name="labels_inputs")
+        labels_embedding = layers.Embedding(num_classes + 1, hidden_channels, mask_zero=True, name="labels_embedding")(labels_inputs)
+        labels_embedding = layers.Dense(hidden_channels * 4, name="labels_embed_1_dense")(labels_embedding)
+        labels_embedding = activation_by_name(labels_embedding, activation=activation, name="labels_embed_")
+        labels_embedding = layers.Dense(hidden_channels * 4, name="labels_embed_2_dense")(labels_embedding)
+    else:
+        labels_embedding = None
 
-def DaViT_B(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [2, 2, 18, 2]
-    out_channels = [128, 256, 512, 1024]
-    num_heads = [4, 8, 16, 32]
-    return DaViT(**locals(), model_name="davit_b", **kwargs)
+    nn = conv2d_no_bias(inputs, hidden_channels, kernel_size=3, use_bias=True, padding="SAME", name="latents_")
 
+    """ Down blocks """
+    skip_connections = [nn]
+    for stack_id, (num_block, hidden_expand, num_attention_block) in enumerate(zip(num_blocks, hidden_expands, num_attention_blocks)):
+        stack_name = "stack{}_".format(stack_id + 1)
+        out_channels = hidden_expand * hidden_channels
+        if stack_id > 0:
+            nn = conv2d_no_bias(nn, nn.shape[channel_axis], kernel_size=3, strides=2, use_bias=True, padding="SAME", name=stack_name + "downsample_")
+            skip_connections.append(nn)
 
-def DaViT_L(input_shape=(384, 384, 3), num_classes=1000, classifier_activation="softmax", pretrained=None, **kwargs):
-    num_blocks = [2, 2, 18, 2]
-    out_channels = [192, 384, 768, 1536]
-    num_heads = [6, 12, 24, 48]
-    return DaViT(**locals(), model_name="davit_l", **kwargs)
+        for block_id in range(num_block):
+            block_name = stack_name + "down_block{}_".format(block_id + 1)
+            nn = res_block(nn, time_embedding, labels_embedding, out_channels=out_channels, activation=activation, dropout=dropout, name=block_name)
+            if num_attention_block > 0:
+                nn = spatial_transformer_block(nn, condition, num_attention_block, mlp_ratio=mlp_ratio, num_heads=num_heads, name=block_name + "attn_")
+            skip_connections.append(nn)
+    # print(f">>>> {[ii.shape for ii in skip_connections] = }")
+
+    """ Middle blocks """
+    nn = res_block(nn, time_embedding, labels_embedding=labels_embedding, activation=activation, dropout=dropout, name="middle_block_1_")
+    nn = spatial_transformer_block(nn, condition, num_attention_block=1, name="middle_block_attn_")
+    nn = res_block(nn, time_embedding, labels_embedding=labels_embedding, activation=activation, dropout=dropout, name="middle_block_2_")
+
+    """ Up blocks """
+    for stack_id, (num_block, hidden_expand, num_attention_block) in enumerate(zip(num_blocks[::-1], hidden_expands[::-1], num_attention_blocks[::-1])):
+        stack_name = "stack{}_".format(len(num_blocks) + stack_id + 1)
+        out_channels = hidden_expand * hidden_channels
+        if stack_id > 0:
+            nn = layers.UpSampling2D(size=2, name=stack_name + "upsample_")(nn)
+            nn = conv2d_no_bias(nn, nn.shape[channel_axis], kernel_size=3, strides=1, use_bias=True, padding="SAME", name=stack_name + "upsample_")
 
+        for block_id in range(num_block + 1):
+            block_name = stack_name + "up_block{}_".format(block_id + 1)
+            skip_connection = skip_connections.pop(-1)
+            nn = functional.concat([nn, skip_connection], axis=channel_axis)
+
+            nn = res_block(nn, time_embedding, labels_embedding, out_channels=out_channels, activation=activation, dropout=dropout, name=block_name)
+            if num_attention_block > 0:
+                nn = spatial_transformer_block(nn, condition, num_attention_block, mlp_ratio=mlp_ratio, num_heads=num_heads, name=block_name + "attn_")
+
+    """ Output blocks """
+    nn = group_norm(nn, name="output_")
+    nn = activation_by_name(nn, activation=activation, name="output_")
+    nn = conv2d_no_bias(nn, inputs.shape[channel_axis], kernel_size=3, use_bias=True, padding="SAME", name="output_")
+    outputs = layers.Activation("linear", dtype="float32", name="output")(nn)
+
+    model_inputs = [inputs, labels_inputs, time_steps] if num_classes > 0 else [inputs, time_steps]
+    model_inputs += [condition] if conditional_embedding > 0 else []
+    model = models.Model(model_inputs, outputs, name=model_name)
+    reload_model_weights(model, PRETRAINED_DICT, "stable_diffusion", pretrained)
 
-def DaViT_H(input_shape=(512, 512, 3), num_classes=1000, classifier_activation="softmax", pretrained=None, **kwargs):
-    num_blocks = [2, 2, 18, 2]
-    out_channels = [256, 512, 1024, 2048]
-    num_heads = [8, 16, 32, 64]
-    return DaViT(**locals(), model_name="davit_h", **kwargs)
+    model.run_prediction = RunPrediction(model=model)
+    return model
 
 
-def DaViT_G(input_shape=(512, 512, 3), num_classes=1000, classifier_activation="softmax", pretrained=None, **kwargs):
-    num_blocks = [2, 2, 24, 6]
-    out_channels = [384, 768, 1536, 3072]
-    num_heads = [12, 24, 48, 96]
-    return DaViT(**locals(), model_name="davit_g", **kwargs)
+@register_model
+def UNetTest(input_shape=(32, 32, 3), conditional_embedding=0, num_classes=0, activation="swish", pretrained=None, **kwargs):
+    hidden_channels = 128
+    hidden_expands = [1, 2, 2, 4]
+    num_attention_blocks = [0, 0, 1, 1]
+    return UNet(**locals(), model_name="unet_test", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/edgenext/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/edgenext/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from keras_cv_attention_models.edgenext.edgenext import (
     EdgeNeXt,
     EdgeNeXt_XX_Small,
     EdgeNeXt_X_Small,
     EdgeNeXt_Small,
+    EdgeNeXt_Base,
     PositionalEncodingFourier,
     cross_covariance_attention,
 )
 
 __head_doc__ = """
 Keras implementation of [Github mmaaz60/EdgeNeXt](https://github.com/mmaaz60/EdgeNeXt).
 Paper [PDF 2206.10589 EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications](https://arxiv.org/pdf/2206.10589.pdf).
@@ -55,14 +56,15 @@
 
 EdgeNeXt_XX_Small.__doc__ = __head_doc__ + """
 Args:
 """ + __tail_doc__
 
 EdgeNeXt_X_Small.__doc__ = EdgeNeXt_XX_Small.__doc__
 EdgeNeXt_Small.__doc__ = EdgeNeXt_XX_Small.__doc__
+EdgeNeXt_Base.__doc__ = EdgeNeXt_XX_Small.__doc__
 
 PositionalEncodingFourier.__doc__ = __head_doc__ + """
 Positional Encoding Fourier layer.
 Layer weight shape depends on parameter `filters` and input channel dimension only.
 
 Args:
   filters: int number, encoded positional dimension for each point.
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/edgenext/edgenext.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/llama2/llama2.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,222 +1,236 @@
-import tensorflow as tf
-from tensorflow import keras
-from keras_cv_attention_models.attention_layers import (
-    ChannelAffine,
-    activation_by_name,
-    conv2d_no_bias,
-    depthwise_conv2d_no_bias,
-    drop_block,
-    layer_norm,
-    mlp_block,
-    multi_head_self_attention,
-    output_block,
-    add_pre_post_process,
-)
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
+from keras_cv_attention_models.attention_layers import activation_by_name, CausalMask
+from keras_cv_attention_models.gpt2.gpt2 import RunPrediction
+
 
 PRETRAINED_DICT = {
-    "edgenext_small": {
-        "imagenet": {256: "0234641a703283de1cb0d935bb0325e4"},
-        "usi": {256: "c237761b5bd5c32041d6b758186a0716"},
-    },
-    "edgenext_x_small": {"imagenet": {256: "472df7659422c7feffbec8012a0f6fa4"}},
-    "edgenext_xx_small": {"imagenet": {256: "4190ba28c7caa2fe73215448f8abebd6"}},
+    "llama2_110m": {"tiny_stories": "197f1ab5ceb0f2b80f33a297f99e0bfa"},
+    "llama2_15m": {"tiny_stories": "654d95c432044f03d9737da31ab2f0d7"},
+    "llama2_42m": {"tiny_stories": "b9641f8620a07e19484b55170d65fb93"},
+    "llama2_1b": {"tiny_llama_1.1B_chat_v0.4": "b785ef5ddd383067279ed72b903974ca"},  # From https://github.com/jzhang38/TinyLlama
 }
-LAYER_NORM_EPSILON = 1e-6
 
 
-@tf.keras.utils.register_keras_serializable(package="kecam/edgenext")
-class PositionalEncodingFourier(keras.layers.Layer):
-    def __init__(self, filters=32, temperature=1e4, **kwargs):
+@backend.register_keras_serializable(package="kecam/llama2")
+class PositionalEncodingFourierRot1D(layers.Layer):
+    def __init__(self, max_block_size, temperature=1e4, **kwargs):
         super().__init__(**kwargs)
-        self.filters, self.temperature = filters, float(temperature)
-        self.epsilon = 1e-6
-        self.scale = 2 * tf.acos(-1.0)  # 2 * pi
+        self.temperature, self.max_block_size = float(temperature), max_block_size
 
     def build(self, input_shape):
-        _, height, width, channels = input_shape  # ex: height, width, filters = 12, 27, 32
-        hh, ww = tf.range(height, dtype="float32"), tf.range(width, dtype="float32")
-        hh = (hh + 1) / (tf.cast(height, "float32") + self.epsilon) * self.scale
-        ww = (ww + 1) / (tf.cast(width, "float32") + self.epsilon) * self.scale
-
-        dim_t = self.temperature ** (2 * (tf.range(self.filters, dtype="float32") // 2) / self.filters)  # (filters,)
-        pos_hh, pos_ww = tf.expand_dims(hh, -1) / dim_t, tf.expand_dims(ww, -1) / dim_t  # pos_hh [12, 32], pos_ww [27, 32]
-        pos_hh = tf.stack([tf.sin(pos_hh[:, 0::2]), tf.cos(pos_hh[:, 1::2])], axis=-1)  # pos_hh [12, 16, 2]
-        pos_ww = tf.stack([tf.sin(pos_ww[:, 0::2]), tf.cos(pos_ww[:, 1::2])], axis=-1)  # pos_ww [27, 16, 2]
-        pos_hh = tf.repeat(tf.reshape(pos_hh, [height, 1, -1]), width, axis=1)  # [12, 27, 32]
-        pos_ww = tf.repeat(tf.reshape(pos_ww, [1, width, -1]), height, axis=0)  # [12, 27, 32]
-        self.positional_embedding = tf.concat([pos_hh, pos_ww], axis=-1)  # [12, 27, 64]
-
-        self.token_projection_ww = self.add_weight(name="ww", shape=(self.filters * 2, channels), trainable=True, dtype=self.dtype)
-        self.token_projection_bb = self.add_weight(name="bb", shape=(channels,), trainable=True, dtype=self.dtype)
+        # input: `[batch, ..., attn_height * attn_width, num_heads, channels // num_heads // 2, 2]`.
+        # print(input_shape)
+        self.channels = input_shape[-2] * input_shape[-1]
+        pos_filters = self.channels // 2
+        dim_t = self.temperature ** (np.arange(pos_filters, dtype="float32") / pos_filters)  # (filters,)
+        grid = np.expand_dims(np.arange(self.max_block_size, dtype="float32"), -1) / dim_t
+        pos_sin, pos_cos = np.expand_dims(np.sin(grid), -2), np.expand_dims(np.cos(grid), -2)
+        # print(f"{pos_sin.shape = }, {pos_cos.shape = }, {height = }, {width = }, {self.channels = }")
+
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("pos_sin", functional.convert_to_tensor(pos_sin, dtype=self.compute_dtype), persistent=False)
+            self.register_buffer("pos_cos", functional.convert_to_tensor(pos_cos, dtype=self.compute_dtype), persistent=False)
+        else:
+            self.pos_sin = functional.convert_to_tensor(pos_sin, dtype=self.compute_dtype)
+            self.pos_cos = functional.convert_to_tensor(pos_cos, dtype=self.compute_dtype)
+        super().build(input_shape)
 
     def call(self, inputs, **kwargs):
-        pos_emb = self.positional_embedding @ self.token_projection_ww + self.token_projection_bb
-        # tf.print(pos_emb.shape, attention_scores.shape)
-        return inputs + pos_emb
+        left, right = functional.unstack(inputs, axis=-2)
+        seq_len = functional.shape(left)[-3] if backend.is_tensorflow_backend else left.shape[-3]
+        pos_cos, pos_sin = self.pos_cos[:seq_len], self.pos_sin[:seq_len]
+        out = functional.stack([left * pos_cos - right * pos_sin, right * pos_cos + left * pos_sin], axis=-2)
+        return out
 
     def get_config(self):
         base_config = super().get_config()
-        base_config.update({"filters": self.filters, "temperature": self.temperature})
+        base_config.update({"temperature": self.temperature, "max_block_size": self.max_block_size})
         return base_config
 
 
-def norm_inverted_bottleneck(inputs, mlp_ratio=4, layer_scale=1e-6, drop_rate=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
-    nn = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, name=name)
-    nn = mlp_block(nn, input_channel * mlp_ratio, activation=activation, name=name)
-    nn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "gamma")(nn) if layer_scale >= 0 else nn
-    nn = drop_block(nn, drop_rate=drop_rate, name=name)
-    return nn
-
-
-def cross_covariance_attention(inputs, num_heads=4, key_dim=0, qkv_bias=True, out_bias=True, attn_dropout=0, out_dropout=0, name=None):
-    input_channel = inputs.shape[-1]
-    key_dim = key_dim if key_dim > 0 else input_channel // num_heads
-    qk_out = key_dim * num_heads
-
-    qkv = keras.layers.Dense(qk_out * 3, use_bias=True, name=name and name + "qkv")(inputs)
-    qkv = tf.reshape(qkv, [-1, qkv.shape[1] * qkv.shape[2], qkv.shape[-1]])
-    query, key, value = tf.split(qkv, 3, axis=-1)
-    query = tf.transpose(tf.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  #  [batch, num_heads, key_dim, hh * ww]
-    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
-    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-
-    norm_query, norm_key = tf.nn.l2_normalize(query, axis=-1, epsilon=1e-6), tf.nn.l2_normalize(key, axis=-2, epsilon=1e-6)
-    attn = tf.matmul(norm_query, norm_key)  # [batch, num_heads, key_dim, key_dim]
-    attn = ChannelAffine(axis=1, use_bias=False, name=name and name + "temperature/no_weight_decay")(attn)  # axis=1 means on head dimension
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
-
-    if attn_dropout > 0:
-        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
-    # [batch, num_heads, key_dim, key_dim] * [batch, num_heads, key_dim, hh * ww] -> [batch, num_heads, key_dim, hh * ww]
-    attention_output = tf.matmul(attention_scores, value)
-    attention_output = tf.transpose(attention_output, perm=[0, 3, 1, 2])  # [batch, hh * ww, num_heads, key_dim]
-    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * key_dim])  # [batch, hh, ww, num_heads * key_dim]
-    # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
-
-    # [batch, hh, ww, num_heads * key_dim] * [num_heads * key_dim, out] --> [batch, hh, ww, out]
-    attention_output = keras.layers.Dense(qk_out, use_bias=out_bias, name=name and name + "output")(attention_output)
-    attention_output = keras.layers.Dropout(out_dropout, name=name and name + "out_drop")(attention_output) if out_dropout > 0 else attention_output
-    return attention_output
+@backend.register_keras_serializable(package="kecam/llama2")
+class RMSNorm(layers.Layer):
+    def __init__(self, epsilon=1e-5, **kwargs):
+        super().__init__(**kwargs)
+        self.epsilon = epsilon
+
+    def build(self, input_shape):
+        self.gamma = self.add_weight(name="gamma", shape=(input_shape[-1],), initializer="ones", trainable=True)
+        super().build(input_shape)
 
+    def call(self, inputs):
+        norm = inputs * functional.rsqrt(functional.reduce_mean(inputs**2, keepdims=True, axis=-1) + self.epsilon)
+        return norm * self.gamma
 
-def split_depthwise_transpose_attention(
-    inputs, split=1, num_heads=4, mlp_ratio=4, use_pos_emb=False, layer_scale=1e-6, drop_rate=0, activation="gelu", name=""
-):
-    input_channel = inputs.shape[-1]
-    sub_channels = int(tf.math.ceil(input_channel / split))
+    def get_config(self):
+        base_config = super().get_config()
+        base_config.update({"epsilon": self.epsilon})
+        return base_config
 
-    spx, remainder = inputs[:, :, :, : (split - 1) * sub_channels], inputs[:, :, :, (split - 1) * sub_channels :]
-    spx = tf.split(spx, split - 1, axis=-1)
-    gathered_result = []
-    for id, ii in enumerate(spx):
-        sp = ii if id == 0 else (sp + ii)
-        sp = depthwise_conv2d_no_bias(sp, kernel_size=3, padding="SAME", use_bias=True, name=name + "spx_{}_".format(id + 1))
-        gathered_result.append(sp)
-    gathered_result.append(remainder)
-    attn = tf.concat(gathered_result, axis=-1)
-    # print(f"{inputs.shape = }, {attn.shape = }")
-
-    # XCA
-    attn = PositionalEncodingFourier(name=name + "pos")(attn) if use_pos_emb else attn
-    nn = layer_norm(attn, epsilon=LAYER_NORM_EPSILON, name=name + "xca_")
-    nn = cross_covariance_attention(nn, num_heads, name=name + "xca_")
-    nn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "xca_gamma")(nn) if layer_scale >= 0 else nn
-    nn = drop_block(nn, drop_rate=drop_rate, name=name + "xca_")
-    nn = keras.layers.Add(name=name + "xca")([attn, nn])
-
-    # Inverted Bottleneck
-    nn = norm_inverted_bottleneck(nn, mlp_ratio, layer_scale, drop_rate, activation=activation, name=name + "ir_")
-    return keras.layers.Add(name=name + "output")([inputs, nn])
-
-
-def conv_encoder(inputs, mlp_ratio=4, kernel_size=7, layer_scale=1e-6, drop_rate=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
-    nn = depthwise_conv2d_no_bias(inputs, kernel_size, use_bias=True, padding="SAME", name=name)
-    nn = norm_inverted_bottleneck(nn, mlp_ratio, layer_scale, drop_rate, activation=activation, name=name)
-    # print(f"{nn.shape = }, {inputs.shape = }")
-    return keras.layers.Add(name=name + "output")([inputs, nn])
-
-
-def EdgeNeXt(
-    num_blocks=[2, 2, 6, 2],
-    out_channels=[24, 48, 88, 168],
-    num_heads=4,
-    num_stda_layers=[0, 1, 1, 1],
-    stda_split=[2, 2, 3, 4],
-    stda_use_pos_emb=[False, True, False, False],
-    conv_kernel_size=[3, 5, 7, 9],
-    stem_width=-1,
-    mlp_ratio=4,
-    stem_patch_size=4,
-    layer_scale=1e-6,
-    input_shape=(224, 224, 3),
-    num_classes=1000,
-    activation="gelu",
-    drop_connect_rate=0,
-    classifier_activation="softmax",
-    dropout=0,
+
+def apply_positional_encoding_rotary(inputs, pos_emb_layer, name=""):
+    """Reshape is separated out from PositionalEncodingFourierRot1D for setting as dynamic"""
+    num_heads = inputs.shape[-2]
+    # transformers using `x1, x2 = inputs[:, :, :half], inputs[:, :, half:]`, different from timm EVA one `torch.stack([-x[..., 1::2], x[..., ::2]], -1)`
+    nn = layers.Reshape([-1, num_heads, 2, inputs.shape[-1] // 2], name=name + "pre_rope_reshape")(inputs)
+    nn = pos_emb_layer(nn)
+    out = layers.Reshape([-1, num_heads, inputs.shape[-1]], name=name + "post_rope_reshape")(nn)
+    return out
+
+
+def causal_self_attention(inputs, block_size, num_heads, num_kv_heads=-1, use_bias=False, dropout=0, name=""):
+    input_channels = inputs.shape[-1]
+    key_dim = input_channels // num_heads
+    qq_scale = 1.0 / (float(key_dim) ** 0.5)
+    num_kv_heads = num_kv_heads if num_kv_heads > 0 else num_heads
+
+    query = layers.Dense(num_heads * key_dim, use_bias=use_bias, name=name + "q_proj")(inputs)
+    key = layers.Dense(num_kv_heads * key_dim, use_bias=use_bias, name=name + "k_proj")(inputs)
+    value = layers.Dense(num_kv_heads * key_dim, use_bias=use_bias, name=name + "v_proj")(inputs)
+
+    # Create a new one every time, as there's no weights for this layer
+    rope = PositionalEncodingFourierRot1D(max_block_size=block_size, name=name + "rope")
+    query = layers.Reshape([-1, num_heads, key_dim], name=name + "query_reshape")(query)
+    query = apply_positional_encoding_rotary(query, rope, name=name + "query_")
+    query = functional.transpose(query, [0, 2, 1, 3])
+
+    key = layers.Reshape([-1, num_kv_heads, key_dim], name=name + "key_reshape")(key)
+    key = apply_positional_encoding_rotary(key, rope, name=name + "key_")
+    key = functional.transpose(key, [0, 2, 3, 1])
+
+    value = functional.transpose(layers.Reshape([-1, num_kv_heads, key_dim], name=name + "value_reshape")(value), [0, 2, 1, 3])
+
+    if num_kv_heads != num_heads:
+        assert (num_heads // num_kv_heads) * num_kv_heads == num_heads, "num_heads={} should be divisible by num_kv_heads={}".format(num_heads, num_kv_heads)
+        key = functional.repeat(key, repeats=num_heads // num_kv_heads, axis=1)
+        value = functional.repeat(value, repeats=num_heads // num_kv_heads, axis=1)
+
+    attn = (query @ key) * qq_scale
+    attn = CausalMask(block_size=block_size)(attn)
+    attn = layers.Softmax(axis=-1, name=name + "attention_scores")(attn)
+    attn_out = attn @ value
+
+    output = functional.transpose(attn_out, [0, 2, 1, 3])
+    output = layers.Reshape([-1, input_channels])(output)
+    output = layers.Dense(input_channels, use_bias=use_bias, name=name + "o_proj")(output)
+    output = layers.Dropout(dropout)(output)
+    return output
+
+
+def attention_fft_block(inputs, block_size, num_heads, num_kv_heads=-1, hidden_divisible=32, use_bias=False, dropout=0, activation="swish", name=""):
+    input_channels = inputs.shape[-1]
+    attn = RMSNorm(name=name + "input_layernorm")(inputs)
+    attn = causal_self_attention(attn, block_size, num_heads, num_kv_heads, use_bias, dropout, name=name + "self_attn.")
+    attn_out = inputs + attn
+
+    hidden_dim = 2 * 4 * input_channels // 3
+    hidden_dim = hidden_divisible * ((hidden_dim + hidden_divisible - 1) // hidden_divisible)
+    # print(f"{input_channels = }, {hidden_divisible = }, {hidden_dim = }")
+
+    fft = RMSNorm(name=name + "post_attention_layernorm")(attn_out)
+    fft_1 = layers.Dense(hidden_dim, use_bias=use_bias, name=name + "mlp.gate_proj")(fft)
+    fft_1 = activation_by_name(fft_1, activation=activation, name=name + "mlp.gate_proj.")
+    fft_3 = layers.Dense(hidden_dim, use_bias=use_bias, name=name + "mlp.up_proj")(fft)
+    fft = fft_1 * fft_3
+    fft = layers.Dense(input_channels, use_bias=use_bias, name=name + "mlp.down_proj")(fft)
+    fft = layers.Dropout(dropout)(fft)
+
+    return layers.Add(name=name + "output")([attn_out, fft])
+
+
+def LLaMA2(
+    num_blocks=12,
+    embedding_size=768,
+    hidden_divisible=32,
+    num_heads=12,
+    num_kv_heads=-1,  # Specific key value heads, num_heads should be divisible by num_kv_heads. Default -1 for equal with num_heads
+    block_use_bias=False,
+    vocab_size=50304,
+    max_block_size=2048,
+    include_top=True,
+    dropout=0.0,
+    activation="swish",
     pretrained=None,
-    model_name="edgenext",
+    model_name="llama2",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
-    stem_width = stem_width if stem_width > 0 else out_channels[0]
-    nn = conv2d_no_bias(inputs, stem_width, kernel_size=stem_patch_size, strides=stem_patch_size, use_bias=True, padding="VALID", name="stem_")
-    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="stem_")
-
-    """ stages """
-    total_blocks = sum(num_blocks)
-    global_block_id = 0
-    for stack_id, (num_block, out_channel, num_stda_layer) in enumerate(zip(num_blocks, out_channels, num_stda_layers)):
-        stack_name = "stack{}_".format(stack_id + 1)
-        if stack_id > 0:
-            ds_name = stack_name + "downsample_"
-            nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name=ds_name)
-            # Set use_torch_padding=False, as kernel_size == 2, otherwise shape will be enlarged by 1
-            nn = conv2d_no_bias(nn, out_channel, kernel_size=2, strides=2, use_bias=True, padding="VALID", name=ds_name)
-        for block_id in range(num_block):
-            block_name = stack_name + "block{}_".format(block_id + 1)
-            block_drop_rate = drop_connect_rate * global_block_id / total_blocks
-            if block_id > num_block - num_stda_layer - 1:
-                split = stda_split[stack_id]
-                use_pos_emb = stda_use_pos_emb[stack_id]
-                num_head = num_heads[stack_id] if isinstance(num_heads, (list, tuple)) else num_heads
-                nn = split_depthwise_transpose_attention(
-                    nn, split, num_head, mlp_ratio, use_pos_emb, layer_scale, block_drop_rate, activation, name=block_name + "stda_"
-                )
-            else:
-                kernel_size = conv_kernel_size[stack_id]
-                nn = conv_encoder(nn, mlp_ratio, kernel_size, layer_scale, block_drop_rate, activation=activation, name=block_name + "conv_")
-            global_block_id += 1
-
-    """ output """
-    if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
-        nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="pre_output_")
-        if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
-
-    model = keras.models.Model(inputs, nn, name=model_name)
-    add_pre_post_process(model, rescale_mode="torch")
-    reload_model_weights(model, PRETRAINED_DICT, "edgenext", pretrained)
+    inputs = layers.Input([None], dtype="int64")
+    tok_emb = layers.Embedding(vocab_size, embedding_size, name="embed_tokens")(inputs)
+    nn = layers.Dropout(dropout)(tok_emb)
+
+    for block_id in range(num_blocks):
+        block_name = "blocks.{}.".format(block_id)
+        nn = attention_fft_block(nn, max_block_size, num_heads, num_kv_heads, hidden_divisible, block_use_bias, dropout, activation, name=block_name)
+    nn = RMSNorm(name="norm")(nn)
+
+    if include_top:
+        nn = layers.Dense(vocab_size, use_bias=False, dtype="float32", name="lm_head")(nn)
+
+    model = models.Model(inputs, nn, name=model_name)
+    model.max_block_size = max_block_size  # or model.get_layer('pos_idx').block_size
+    model.run_prediction = RunPrediction(model, tokenizer="SentencePieceTokenizer")
+    reload_model_weights(model, PRETRAINED_DICT, "llama2", pretrained)
     return model
 
 
-def EdgeNeXt_XX_Small(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    return EdgeNeXt(**locals(), model_name="edgenext_xx_small", **kwargs)
+@register_model
+def LLaMA2_15M(max_block_size=256, vocab_size=32000, include_top=True, activation="swish", pretrained="tiny_stories", **kwargs):
+    num_blocks = 6
+    embedding_size = 288
+    num_heads = 6
+    return LLaMA2(**locals(), **kwargs, model_name="llama2_15m")
 
 
-def EdgeNeXt_X_Small(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 3, 9, 3]
-    out_channels = [32, 64, 100, 192]
-    return EdgeNeXt(**locals(), model_name="edgenext_x_small", **kwargs)
+@register_model
+def LLaMA2_42M(max_block_size=1024, vocab_size=32000, include_top=True, activation="swish", pretrained="tiny_stories", **kwargs):
+    num_blocks = 8
+    embedding_size = 512
+    num_heads = 8
+    return LLaMA2(**locals(), **kwargs, model_name="llama2_42m")
 
 
-def EdgeNeXt_Small(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 3, 9, 3]
-    out_channels = [48, 96, 160, 304]
-    num_heads = 8
-    return EdgeNeXt(**locals(), model_name="edgenext_small", **kwargs)
+@register_model
+def LLaMA2_110M(max_block_size=1024, vocab_size=32000, include_top=True, activation="swish", pretrained="tiny_stories", **kwargs):
+    num_blocks = 12
+    embedding_size = 768
+    num_heads = 12
+    return LLaMA2(**locals(), **kwargs, model_name="llama2_110m")
+
+
+@register_model
+def LLaMA2_1B(max_block_size=2048, vocab_size=32003, include_top=True, activation="swish", pretrained="tiny_llama_1.1B_chat_v0.4", **kwargs):
+    # From https://github.com/jzhang38/TinyLlama
+    num_blocks = 22
+    embedding_size = 2048
+    hidden_divisible = 256
+    num_heads = 32
+    num_kv_heads = 4
+    return LLaMA2(**locals(), **kwargs, model_name="llama2_1b")
+
+
+@register_model
+def LLaMA2_7B(max_block_size=2048, vocab_size=32000, include_top=True, activation="swish", pretrained=None, **kwargs):
+    num_blocks = 32
+    embedding_size = 4096
+    hidden_divisible = 256
+    num_heads = 32
+    return LLaMA2(**locals(), **kwargs, model_name="llama2_7b")
+
+
+""" Convert pytorch weights to h5 """
+
+
+def convert_huggingface_weights_to_h5(source_pt_path, save_path="AUTO", name_convert_map={}, to_fp16=False):
+    from keras_cv_attention_models.download_and_load import convert_torch_weights_to_h5
+
+    skip_weights = [".num_batches_tracked", ".inv_freq"]
+    name_convert_funcs = [
+        lambda name: name.replace("layers.", "blocks."),
+        lambda name: name[len("model.") :] if name.startswith("model.") else name,
+    ]
+    weight_convert_funcs = [lambda target_name, weight: weight.T if len(weight.shape) == 2 and "embed_tokens" not in target_name else weight]  # Dense weight
+    return convert_torch_weights_to_h5(source_pt_path, save_path, skip_weights, name_convert_funcs, name_convert_map, weight_convert_funcs, to_fp16)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientdet/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientdet/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -21,23 +21,26 @@
 Keras implementation of [google/automl/efficientdet](https://github.com/google/automl/tree/master/efficientdet).
 Paper [Paper 1911.09070 EfficientDet: Scalable and Efficient Object Detection](https://arxiv.org/pdf/1911.09070.pdf).
 
 Args:
   backbone: backbone model, could be any model with pyramid stage structure. {}
 """
 
-__tail_doc__ = """  anchors_mode: one of ["efficientdet", "anchor_free", "yolor"], controls which anchor to use.
+__tail_doc__ = """  regression_len: bbox output len, typical value is 4, for yolov8 reg_max=16 -> regression_len = 16 * 4 == 64.
+  anchors_mode: one of ["efficientdet", "anchor_free", "yolor"], controls which anchor to use.
       - efficientdet anchors default settings: use_object_scores=False, num_anchors=9, anchor_scale=4,
           aspect_ratios=[1, 2, 0.5], num_scales=3, grid_zero_start=False.
       - anchor_free default settings: use_object_scores=True, num_anchors=1, anchor_scale=1,
           aspect_ratios=[1], num_scales=1, grid_zero_start=True.
       - yolor default settings: use_object_scores=True, num_anchors=3.
+      - yolov8 default settings: use_object_scores=False, num_anchors=1, anchor_scale=1,
+          aspect_ratios=[1], num_scales=1, grid_zero_start=False.
       Default "efficientdet".
   num_anchors: number of anchors for a single grid point, should be same with dataset used value.
-      Default "auto" means: anchors_mode=="anchor_free" -> 1, anchors_mode=="yolor" -> 3, else 9.
+      Default "auto" means: anchors_mode=="anchor_free" / "yolov8" -> 1, anchors_mode=="yolor" -> 3, else 9.
   use_object_scores: bollean value if model header output includes `object_scores`.
       Default "auto" means: True if anchors_mode=="anchor_free" or anchors_mode=="yolor", else False.
   num_classes: total output classes. `90` for EfficientDet pretrained, `80` for `tfds.coco`.
       Set `0` to disable `classifier` output.
   use_sep_conv: set `False` for using `Conv2D` instead of `SeparableConv2D`.
   activation: activation used in whole model, default `swish`. Default "swish".
   classifier_activation: The activation function to use for classifier output if `num_classes > 0`.
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientdet/efficientdet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientdet/efficientdet.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,11 @@
-import tensorflow as tf
-from tensorflow import keras
+import math
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, models, initializers, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models import model_surgery
 from keras_cv_attention_models import efficientnet
 from keras_cv_attention_models.attention_layers import activation_by_name, add_pre_post_process
 from keras_cv_attention_models.download_and_load import reload_model_weights
 from keras_cv_attention_models.coco import eval_func, anchors_func
 
 BATCH_NORM_EPSILON = 1e-3
@@ -22,129 +24,144 @@
     "efficientdet_lite2": {"coco": {448: "b48fa6246a50d3d59785cc119c52fc94"}},
     "efficientdet_lite3": {"coco": {512: "945e66f31622d2806aeafde4dff3b4b7"}},
     "efficientdet_lite3x": {"coco": {640: "bcd55e31b99616439ad4f988e2337b86"}},
     "efficientdet_lite4": {"coco": {640: "9f5eadec38498faffb8d6777cb09e3a7"}},
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="efficientdet")
-class ReluWeightedSum(keras.layers.Layer):
+@backend.register_keras_serializable(package="efficientdet")
+class ReluWeightedSum(layers.Layer):
     def __init__(self, initializer="ones", epsilon=1e-4, **kwargs):
         super().__init__(**kwargs)
         self.initializer, self.epsilon = initializer, epsilon
 
     def build(self, input_shape):
         self.total = len(input_shape)
-        self.gain = self.add_weight(name="gain", shape=(self.total,), initializer=self.initializer, dtype=self.dtype, trainable=True)
-        self.__epsilon__ = tf.cast(self.epsilon, self._compute_dtype)
+        self.gain = self.add_weight(name="gain", shape=(self.total,), initializer=self.initializer, trainable=True)
+        self.__epsilon__ = float(self.epsilon)
+        super().build(input_shape)
 
     def call(self, inputs):
-        gain = tf.nn.relu(self.gain)
-        gain = gain / (tf.reduce_sum(gain) + self.__epsilon__)
-        return tf.reduce_sum([inputs[id] * gain[id] for id in range(self.total)], axis=0)
+        if backend.is_torch_backend:
+            # [???] Or will Type Error: Type 'tensor(float)' of input parameter (onnx::ReduceSum_1256) of operator (ReduceSum)
+            gain = self.gain.relu()
+            gain = gain / (gain.sum() + self.__epsilon__)
+        else:
+            gain = functional.relu(self.gain)
+            gain = gain / functional.reduce_sum(gain)
+        # return functional.reduce_sum([inputs[id] * gain[id] for id in range(self.total)], axis=0)
+        out = inputs[0] * gain[0]
+        for id in range(1, self.total):
+            out += inputs[id] * gain[id]
+        return out
 
     def compute_output_shape(self, input_shape):
         return input_shape[0]
 
     def get_config(self):
         base_config = super().get_config()
         base_config.update({"initializer": self.initializer, "epsilon": self.epsilon})
         return base_config
 
 
 def align_feature_channel(inputs, output_channel, name=""):
     # print(f">>>> align_feature_channel: {name = }, {inputs.shape = }, {output_channel = }")
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
     nn = inputs
-    if inputs.shape[-1] != output_channel:
-        nn = keras.layers.Conv2D(output_channel, kernel_size=1, name=name + "channel_conv")(nn)
-        nn = keras.layers.BatchNormalization(epsilon=BATCH_NORM_EPSILON, name=name + "channel_bn")(nn)
+    if inputs.shape[channel_axis] != output_channel:
+        nn = layers.Conv2D(output_channel, kernel_size=1, name=name + "channel_conv")(nn)
+        nn = layers.BatchNormalization(epsilon=BATCH_NORM_EPSILON, name=name + "channel_bn")(nn)
     return nn
 
 
 def resample_fuse(inputs, output_channel, use_weighted_sum=True, interpolation="nearest", use_sep_conv=True, activation="swish", name=""):
     inputs[0] = align_feature_channel(inputs[0], output_channel, name=name)
 
     if use_weighted_sum:
         nn = ReluWeightedSum(name=name + "wsm")(inputs)
     else:
-        nn = keras.layers.Add(name=name + "sum")(inputs)
+        nn = layers.Add(name=name + "sum")(inputs)
     nn = activation_by_name(nn, activation, name=name)
     if use_sep_conv:
-        nn = keras.layers.SeparableConv2D(output_channel, kernel_size=3, padding="SAME", use_bias=True, name=name + "sepconv")(nn)
+        nn = layers.SeparableConv2D(output_channel, kernel_size=3, padding="same", use_bias=True, name=name + "sepconv")(nn)
     else:
-        nn = keras.layers.Conv2D(output_channel, kernel_size=3, padding="SAME", use_bias=True, name=name + "conv")(nn)
-    nn = keras.layers.BatchNormalization(epsilon=BATCH_NORM_EPSILON, name=name + "bn")(nn)
+        nn = layers.Conv2D(output_channel, kernel_size=3, padding="same", use_bias=True, name=name + "conv")(nn)
+    nn = layers.BatchNormalization(epsilon=BATCH_NORM_EPSILON, name=name + "bn")(nn)
     return nn
 
 
 def bi_fpn(features, output_channel, use_weighted_sum=True, use_sep_conv=True, interpolation="nearest", activation="swish", name=""):
     # print(f">>>> bi_fpn: {[ii.shape for ii in features] = }")
     # features: [p3, p4, p5, p6, p7]
     up_features = [features[-1]]
     for id, feature in enumerate(features[:-1][::-1]):
         cur_name = name + "p{}_up_".format(len(features) - id + 1)
-        # up_feature = keras.layers.UpSampling2D(size=(2, 2), interpolation=interpolation, name=cur_name + "up")(up_features[-1])
-        up_feature = tf.image.resize(up_features[-1], tf.shape(feature)[1:-1], method=interpolation)
+        # up_feature = layers.UpSampling2D(size=(2, 2), interpolation=interpolation, name=cur_name + "up")(up_features[-1])
+        size = functional.shape(feature)[1:-1] if image_data_format() == "channels_last" else functional.shape(feature)[2:]
+        up_feature = functional.resize(up_features[-1], size, method=interpolation)
         up_feature = resample_fuse([feature, up_feature], output_channel, use_weighted_sum, use_sep_conv=use_sep_conv, activation=activation, name=cur_name)
         up_features.append(up_feature)
+    # print(f">>>> bi_fpn: {[ii.shape for ii in up_features] = }")
 
     # up_features: [p7, p6_up, p5_up, p4_up, p3_up]
     out_features = [up_features[-1]]  # [p3_up]
     up_features = up_features[1:-1][::-1]  # [p4_up, p5_up, p6_up]
     for id, feature in enumerate(features[1:]):
         cur_name = name + "p{}_out_".format(len(features) - 1 + id)
-        down_feature = keras.layers.MaxPooling2D(pool_size=3, strides=2, padding="SAME", name=cur_name + "max_down")(out_features[-1])
+        down_feature = layers.MaxPool2D(pool_size=3, strides=2, padding="same", name=cur_name + "max_down")(out_features[-1])
         fusion_feature = [feature, down_feature] if id == len(up_features) else [feature, up_features[id], down_feature]
         out_feature = resample_fuse(fusion_feature, output_channel, use_weighted_sum, use_sep_conv=use_sep_conv, activation=activation, name=cur_name)
         out_features.append(out_feature)
-
     # out_features: [p3_up, p4_out, p5_out, p6_out, p7_out]
     return out_features
 
 
 def det_header_pre(features, filters, depth, use_sep_conv=True, activation="swish", name=""):
+    # print(f">>>> det_header_pre: {[ii.shape for ii in features] = }")
     if use_sep_conv:
         names = [name + "{}_sepconv".format(id + 1) for id in range(depth)]
-        convs = [keras.layers.SeparableConv2D(filters, kernel_size=3, padding="SAME", use_bias=True, name=names[id]) for id in range(depth)]
+        convs = [layers.SeparableConv2D(filters, kernel_size=3, padding="same", use_bias=True, name=names[id]) for id in range(depth)]
     else:
         names = [name + "{}_conv".format(id + 1) for id in range(depth)]
-        convs = [keras.layers.Conv2D(filters, kernel_size=3, padding="SAME", use_bias=True, name=names[id]) for id in range(depth)]
+        convs = [layers.Conv2D(filters, kernel_size=3, padding="same", use_bias=True, name=names[id]) for id in range(depth)]
 
     outputs = []
     for feature_id, feature in enumerate(features):
         nn = feature
         for id in range(depth):
             nn = convs[id](nn)
             cur_name = name + "{}_{}_bn".format(id + 1, feature_id + 1)
-            nn = keras.layers.BatchNormalization(epsilon=BATCH_NORM_EPSILON, name=cur_name)(nn)
+            nn = layers.BatchNormalization(epsilon=BATCH_NORM_EPSILON, name=cur_name)(nn)
             nn = activation_by_name(nn, activation, name=cur_name + "{}_".format(id + 1))
         outputs.append(nn)
     return outputs
 
 
 def det_header_post(inputs, classes=80, anchors=9, bias_init="zeros", use_sep_conv=True, head_activation="sigmoid", name=""):
     if use_sep_conv:
-        header_conv = keras.layers.SeparableConv2D(classes * anchors, kernel_size=3, padding="SAME", bias_initializer=bias_init, name=name + "head")
+        header_conv = layers.SeparableConv2D(classes * anchors, kernel_size=3, padding="same", bias_initializer=bias_init, name=name + "head")
     else:
-        header_conv = keras.layers.Conv2D(classes * anchors, kernel_size=3, padding="SAME", bias_initializer=bias_init, name=name + "conv_head")
+        header_conv = layers.Conv2D(classes * anchors, kernel_size=3, padding="same", bias_initializer=bias_init, name=name + "conv_head")
     outputs = [header_conv(ii) for ii in inputs]
-    outputs = [keras.layers.Reshape([-1, classes])(ii) for ii in outputs]
-    outputs = tf.concat(outputs, axis=1)
+    outputs = outputs if image_data_format() == "channels_last" else [layers.Permute([2, 3, 1])(ii) for ii in outputs]
+    outputs = [layers.Reshape([-1, classes])(ii) for ii in outputs]
+    outputs = functional.concat(outputs, axis=1)
     outputs = activation_by_name(outputs, head_activation, name=name + "output_")
     return outputs
 
 
 def EfficientDet(
     backbone,
     features_pick=[-3, -2, -1],  # The last 3 ones, or specific layer_names / feature_indexes
     additional_features=2,  # Add p5->p6, p6->p7
     fpn_depth=3,
     head_depth=3,
     num_channels=64,
     use_weighted_sum=True,
+    regression_len=4,  # bbox output len, typical value is 4, for yolov8 reg_max=16 -> regression_len = 16 * 4 == 64
     anchors_mode="efficientdet",
     use_object_scores="auto",  # "auto" means: True if anchors_mode=="anchor_free" or anchors_mode=="yolor", else False
     num_anchors="auto",  # "auto" means: anchors_mode=="anchor_free" -> 1, anchors_mode=="yolor" -> 3, else 9
     num_classes=90,
     use_sep_conv=True,
     activation="swish",
     classifier_activation="sigmoid",
@@ -153,206 +170,229 @@
     model_name=None,
     pyramid_levels_min=3,  # Init anchors for model prediction, not for model structure
     anchor_scale="auto",  # Init anchors for model prediction. "auto" means 1 if (anchors_mode=="anchor_free" or anchors_mode=="yolor"), else 4
     rescale_mode="torch",  # Model precessing input, not for model structure
     input_shape=None,  # Not using, recieving parameter
     kwargs=None,  # Not using, recieving parameter
 ):
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
     backbone.trainable = False if freeze_backbone else True
     use_object_scores, num_anchors, anchor_scale = anchors_func.get_anchors_mode_parameters(anchors_mode, use_object_scores, num_anchors, anchor_scale)
 
     if isinstance(features_pick[0], str):
         fpn_features = [backbone.get_layer(layer_name) for layer_name in features_pick]
     else:
         features = model_surgery.get_pyramide_feature_layers(backbone)
         fpn_features = [features[id] for id in features_pick]
-    print(">>>> features:", {ii.name: ii.output_shape for ii in fpn_features})
+    feature_names, fpn_features = model_surgery.align_pyramide_feature_output_by_image_data_format(fpn_features)
+    print(">>>> features:", {ii: jj.shape for ii, jj in zip(feature_names, fpn_features)})
     print(">>>> num_anchors:", num_anchors)
-    fpn_features = [ii.output for ii in fpn_features]
 
     # Build additional input features that are not from backbone.
     for id in range(additional_features):
         cur_name = "p{}_p{}_".format(id + 5, id + 6)
         additional_feature = align_feature_channel(fpn_features[-1], num_channels, name=cur_name)
-        additional_feature = keras.layers.MaxPooling2D(pool_size=3, strides=2, padding="SAME", name=cur_name + "max_down")(additional_feature)
+        additional_feature = layers.MaxPool2D(pool_size=3, strides=2, padding="same", name=cur_name + "max_down")(additional_feature)
         fpn_features.append(additional_feature)
 
     # Bi-FPN
     for id in range(fpn_depth):
         fpn_features = bi_fpn(fpn_features, num_channels, use_weighted_sum, use_sep_conv, activation=activation, name="biFPN_{}_".format(id + 1))
 
     # Outputs
     bboxes_features = det_header_pre(fpn_features, num_channels, head_depth, use_sep_conv, activation=activation, name="regressor_")
-    bboxes_out = det_header_post(bboxes_features, 4, num_anchors, bias_init="zeros", use_sep_conv=use_sep_conv, head_activation=None, name="regressor_")
+    bboxes_out = det_header_post(
+        bboxes_features, regression_len, num_anchors, bias_init="zeros", use_sep_conv=use_sep_conv, head_activation=None, name="regressor_"
+    )
     if use_object_scores:
-        bias_init = tf.constant_initializer(-tf.math.log((1 - 0.01) / 0.01).numpy())
+        bias_init = initializers.constant(-math.log((1 - 0.01) / 0.01))
         object_out = det_header_post(bboxes_features, 1, num_anchors, bias_init, use_sep_conv, head_activation=classifier_activation, name="object_")
 
     if num_classes > 0:
-        bias_init = tf.constant_initializer(-tf.math.log((1 - 0.01) / 0.01).numpy())
+        bias_init = initializers.constant(-math.log((1 - 0.01) / 0.01))
         class_features = det_header_pre(fpn_features, num_channels, head_depth, use_sep_conv, activation=activation, name="classifier_")
         class_out = det_header_post(class_features, num_classes, num_anchors, bias_init, use_sep_conv, classifier_activation, name="classifier_")
-        outputs = tf.concat([bboxes_out, class_out, object_out], axis=-1) if use_object_scores else tf.concat([bboxes_out, class_out], axis=-1)
+        if use_object_scores:
+            outputs = functional.concat([bboxes_out, class_out, object_out], axis=-1)
+        else:
+            outputs = functional.concat([bboxes_out, class_out], axis=-1)
     else:
-        outputs = tf.concat([bboxes_out, object_out], axis=-1) if use_object_scores else bboxes_out
-    outputs = keras.layers.Activation("linear", dtype="float32", name="outputs_fp32")(outputs)
+        outputs = functional.concat([bboxes_out, object_out], axis=-1) if use_object_scores else bboxes_out
+    outputs = layers.Activation("linear", dtype="float32", name="outputs_fp32")(outputs)
 
     model_name = model_name or backbone.name + "_det"
-    model = keras.models.Model(inputs=backbone.inputs[0], outputs=outputs, name=model_name)
+    model = models.Model(inputs=backbone.inputs[0], outputs=outputs, name=model_name)
     reload_model_weights(model, PRETRAINED_DICT, "efficientdet", pretrained)
 
     # For prediction
     # AA = {"aspect_ratios": anchor_aspect_ratios, "num_scales": anchor_num_scales, "anchor_scale": anchor_scale, "grid_zero_start": anchor_grid_zero_start}
     pyramid_levels = [pyramid_levels_min, pyramid_levels_min + len(features_pick) + additional_features - 1]  # -> [3, 7]
-    post_process = eval_func.DecodePredictions(backbone.input_shape[1:], pyramid_levels, anchors_mode, use_object_scores, anchor_scale)
+    post_process = eval_func.DecodePredictions(
+        backbone.input_shape[1:], pyramid_levels, anchors_mode, use_object_scores, anchor_scale, regression_len=regression_len
+    )
     add_pre_post_process(model, rescale_mode=rescale_mode, post_process=post_process)
     # model.backbone = backbone
     return model
 
 
+@register_model
 def EfficientDetD0(input_shape=(512, 512, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="swish", pretrained="coco", **kwargs):
     if backbone is None:
         backbone_kwargs = {} if pretrained is None else {"pretrained": None}  # Load EfficientNet weights if EfficientDet pretrained not specified
         backbone = efficientnet.EfficientNetV1B0(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation, **backbone_kwargs)
         del backbone_kwargs
         features_pick = ["stack_2_block1_output", "stack_4_block2_output", "stack_6_block0_output"]
     model_name = kwargs.pop("model_name", "efficientdet_d0")
     return EfficientDet(**locals(), fpn_depth=3, head_depth=3, num_channels=64, **kwargs)
 
 
+@register_model
 def EfficientDetD1(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="swish", pretrained="coco", **kwargs):
     if backbone is None:
         backbone_kwargs = {} if pretrained is None else {"pretrained": None}  # Load EfficientNet weights if EfficientDet pretrained not specified
         backbone = efficientnet.EfficientNetV1B1(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation, **backbone_kwargs)
         del backbone_kwargs
         features_pick = ["stack_2_block2_output", "stack_4_block3_output", "stack_6_block1_output"]
     model_name = kwargs.pop("model_name", "efficientdet_d1")
     return EfficientDet(**locals(), fpn_depth=4, head_depth=3, num_channels=88, **kwargs)
 
 
+@register_model
 def EfficientDetD2(input_shape=(768, 768, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="swish", pretrained="coco", **kwargs):
     if backbone is None:
         backbone_kwargs = {} if pretrained is None else {"pretrained": None}  # Load EfficientNet weights if EfficientDet pretrained not specified
         backbone = efficientnet.EfficientNetV1B2(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation, **backbone_kwargs)
         del backbone_kwargs
         features_pick = ["stack_2_block2_output", "stack_4_block3_output", "stack_6_block1_output"]
     model_name = kwargs.pop("model_name", "efficientdet_d2")
     return EfficientDet(**locals(), fpn_depth=5, head_depth=3, num_channels=112, **kwargs)
 
 
+@register_model
 def EfficientDetD3(input_shape=(896, 896, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="swish", pretrained="coco", **kwargs):
     if backbone is None:
         backbone_kwargs = {} if pretrained is None else {"pretrained": None}  # Load EfficientNet weights if EfficientDet pretrained not specified
         backbone = efficientnet.EfficientNetV1B3(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation, **backbone_kwargs)
         del backbone_kwargs
         features_pick = ["stack_2_block2_output", "stack_4_block4_output", "stack_6_block1_output"]
     model_name = kwargs.pop("model_name", "efficientdet_d3")
     return EfficientDet(**locals(), fpn_depth=6, head_depth=4, num_channels=160, **kwargs)
 
 
+@register_model
 def EfficientDetD4(input_shape=(1024, 1024, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="swish", pretrained="coco", **kwargs):
     if backbone is None:
         backbone_kwargs = {} if pretrained is None else {"pretrained": None}  # Load EfficientNet weights if EfficientDet pretrained not specified
         backbone = efficientnet.EfficientNetV1B4(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation, **backbone_kwargs)
         del backbone_kwargs
         features_pick = ["stack_2_block3_output", "stack_4_block5_output", "stack_6_block1_output"]
     model_name = kwargs.pop("model_name", "efficientdet_d4")
     return EfficientDet(**locals(), fpn_depth=7, head_depth=4, num_channels=224, **kwargs)
 
 
+@register_model
 def EfficientDetD5(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="swish", pretrained="coco", **kwargs):
     if backbone is None:
         backbone_kwargs = {} if pretrained is None else {"pretrained": None}  # Load EfficientNet weights if EfficientDet pretrained not specified
         backbone = efficientnet.EfficientNetV1B5(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation, **backbone_kwargs)
         del backbone_kwargs
         features_pick = ["stack_2_block4_output", "stack_4_block6_output", "stack_6_block2_output"]
     model_name = kwargs.pop("model_name", "efficientdet_d5")
     return EfficientDet(**locals(), fpn_depth=7, head_depth=4, num_channels=288, **kwargs)
 
 
+@register_model
 def EfficientDetD6(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="swish", pretrained="coco", **kwargs):
     if backbone is None:
         backbone_kwargs = {} if pretrained is None else {"pretrained": None}  # Load EfficientNet weights if EfficientDet pretrained not specified
         backbone = efficientnet.EfficientNetV1B6(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation, **backbone_kwargs)
         del backbone_kwargs
         features_pick = ["stack_2_block5_output", "stack_4_block7_output", "stack_6_block2_output"]
     model_name = kwargs.pop("model_name", "efficientdet_d6")
     return EfficientDet(**locals(), fpn_depth=8, head_depth=5, num_channels=384, use_weighted_sum=False, **kwargs)
 
 
+@register_model
 def EfficientDetD7(input_shape=(1536, 1536, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="swish", pretrained="coco", **kwargs):
     if backbone is None:
         backbone_kwargs = {} if pretrained is None else {"pretrained": None}  # Load EfficientNet weights if EfficientDet pretrained not specified
         backbone = efficientnet.EfficientNetV1B6(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation, **backbone_kwargs)
         del backbone_kwargs
         features_pick = ["stack_2_block5_output", "stack_4_block7_output", "stack_6_block2_output"]
     anchor_scale = kwargs.pop("anchor_scale", 5)
     model_name = kwargs.pop("model_name", "efficientdet_d7")
     return EfficientDet(**locals(), fpn_depth=8, head_depth=5, num_channels=384, use_weighted_sum=False, **kwargs)
 
 
+@register_model
 def EfficientDetD7X(input_shape=(1536, 1536, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="swish", pretrained="coco", **kwargs):
     if backbone is None:
         backbone_kwargs = {} if pretrained is None else {"pretrained": None}  # Load EfficientNet weights if EfficientDet pretrained not specified
         backbone = efficientnet.EfficientNetV1B7(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation, **backbone_kwargs)
         del backbone_kwargs
         features_pick = ["stack_2_block6_output", "stack_4_block9_output", "stack_6_block3_output"]
     additional_features = 3
     model_name = kwargs.pop("model_name", "efficientdet_d7x")
     return EfficientDet(**locals(), fpn_depth=8, head_depth=5, num_channels=384, use_weighted_sum=False, **kwargs)
 
 
 """ EfficientDetLite models """
 
 
+@register_model
 def EfficientDetLite0(input_shape=(320, 320, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="relu6", pretrained="coco", **kwargs):
     if backbone is None:
         backbone = efficientnet.EfficientNetV1Lite0(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation)
         features_pick = ["stack_2_block1_output", "stack_4_block2_output", "stack_6_block0_output"]
     model_name = kwargs.pop("model_name", "efficientdet_lite0")
     anchor_scale = kwargs.pop("anchor_scale", 3)
     return EfficientDet(**locals(), fpn_depth=3, head_depth=3, num_channels=64, use_weighted_sum=False, rescale_mode="tf", **kwargs)
 
 
+@register_model
 def EfficientDetLite1(input_shape=(384, 384, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="relu6", pretrained="coco", **kwargs):
     if backbone is None:
         backbone = efficientnet.EfficientNetV1Lite1(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation)
         features_pick = ["stack_2_block2_output", "stack_4_block3_output", "stack_6_block0_output"]
     model_name = kwargs.pop("model_name", "efficientdet_lite1")
     anchor_scale = kwargs.pop("anchor_scale", 3)
     return EfficientDet(**locals(), fpn_depth=4, head_depth=3, num_channels=88, use_weighted_sum=False, rescale_mode="tf", **kwargs)
 
 
+@register_model
 def EfficientDetLite2(input_shape=(448, 448, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="relu6", pretrained="coco", **kwargs):
     if backbone is None:
         backbone = efficientnet.EfficientNetV1Lite2(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation)
         features_pick = ["stack_2_block2_output", "stack_4_block3_output", "stack_6_block0_output"]
     model_name = kwargs.pop("model_name", "efficientdet_lite2")
     anchor_scale = kwargs.pop("anchor_scale", 3)
     return EfficientDet(**locals(), fpn_depth=5, head_depth=3, num_channels=112, use_weighted_sum=False, rescale_mode="tf", **kwargs)
 
 
+@register_model
 def EfficientDetLite3(input_shape=(512, 512, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="relu6", pretrained="coco", **kwargs):
     if backbone is None:
         backbone = efficientnet.EfficientNetV1Lite3(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation)
         features_pick = ["stack_2_block2_output", "stack_4_block4_output", "stack_6_block0_output"]
     model_name = kwargs.pop("model_name", "efficientdet_lite3")
     anchor_scale = kwargs.pop("anchor_scale", 4)
     return EfficientDet(**locals(), fpn_depth=6, head_depth=4, num_channels=160, use_weighted_sum=False, rescale_mode="tf", **kwargs)
 
 
+@register_model
 def EfficientDetLite3X(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="relu6", pretrained="coco", **kwargs):
     if backbone is None:
         backbone = efficientnet.EfficientNetV1Lite3(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation)
         features_pick = ["stack_2_block2_output", "stack_4_block4_output", "stack_6_block0_output"]
     model_name = kwargs.pop("model_name", "efficientdet_lite3x")
     anchor_scale = kwargs.pop("anchor_scale", 3)
     return EfficientDet(**locals(), fpn_depth=6, head_depth=4, num_channels=200, use_weighted_sum=False, rescale_mode="tf", **kwargs)
 
 
+@register_model
 def EfficientDetLite4(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=90, backbone=None, activation="relu6", pretrained="coco", **kwargs):
     if backbone is None:
         backbone = efficientnet.EfficientNetV1Lite4(input_shape=input_shape, num_classes=0, output_conv_filter=0, activation=activation)
         features_pick = ["stack_2_block3_output", "stack_4_block5_output", "stack_6_block0_output"]
     model_name = kwargs.pop("model_name", "efficientdet_lite4")
     anchor_scale = kwargs.pop("anchor_scale", 4)
     return EfficientDet(**locals(), fpn_depth=7, head_depth=4, num_channels=224, use_weighted_sum=False, rescale_mode="tf", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientformer/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientformer/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -31,15 +31,15 @@
       or a tuple value like `(0, 0.2)` indicates the drop probability linearly changes from `0 --> 0.2` for `top --> bottom` layers.
       A higher value means a higher probability will drop the deep branch.
       or `0` to disable (default).
   dropout: top dropout rate if top layers is included. Default 0.
   classifier_activation: A `str` or callable. The activation function to use on the "top" layer if `num_classes > 0`.
       Set `classifier_activation=None` to return the logits of the "top" layer.
       Default is `None`.
-  use_distillation: Boolean value if output `distill_head`. Default `True`.
+  use_distillation: Boolean value if output `distill_head`. Default `False`.
   pretrained: one of `None` (random initialization) or 'imagenet' (pre-training on ImageNet).
       Will try to download and load pre-trained model weights if not None.
   **kwargs: other parameters if available.
 
 Returns:
     A `keras.Model` instance.
 """
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientformer/efficientformer.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/fasternet/fasternet.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,131 +1,139 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, initializers, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
-    ChannelAffine,
-    MultiHeadPositionalEmbedding,
+    activation_by_name,
+    add_with_layer_scale_and_drop_block,
     batchnorm_with_activation,
     conv2d_no_bias,
-    drop_block,
-    layer_norm,
-    mlp_block,
-    mhsa_with_multi_head_position,
+    # HeadInitializer,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
+LAYER_NORM_EPSILON = 1e-6
 PRETRAINED_DICT = {
-    "efficientformer_l1": {"imagenet": {224: "7698d40d502ccc548a7e2890fb33db34"}},
-    "efficientformer_l3": {"imagenet": {224: "ee3d11742d233bc2ec36648440cb5a0b"}},
-    "efficientformer_l7": {"imagenet": {224: "66c26fc1e0bd39bbf6886d570956d178"}},
+    "fasternet_l": {"imagenet": "42de42d7d4405716e575e25aa58e7445"},
+    "fasternet_m": {"imagenet": "2de1a10a5aa1092f82cd6f0104b29c6d"},
+    "fasternet_s": {"imagenet": "58f97e2986da1b6793ee4466b4421a16"},
+    "fasternet_t0": {"imagenet": "c330a6fa902f17993eba5d734c822551"},
+    "fasternet_t1": {"imagenet": "1eb9cb6c77542f5f485efc65c75fa780"},
+    "fasternet_t2": {"imagenet": "b4edf4df9e261766fb2e17f0bb50651b"},
 }
 
 
-def meta_block(inputs, is_attn_block=False, num_heads=8, key_dim=32, attn_ratio=4, mlp_ratio=4, layer_scale=0, drop_rate=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
-
-    if is_attn_block:
-        nn = layer_norm(inputs, name=name + "attn_")
-        nn = mhsa_with_multi_head_position(nn, num_heads, key_dim=key_dim, attn_ratio=attn_ratio, use_bn=False, qkv_bias=True, out_bias=True, name=name)
-    else:
-        nn = keras.layers.AvgPool2D(pool_size=3, strides=1, padding="SAME")(inputs)  # count_include_pad=False [ ??? ]
-        nn = nn - inputs
-    nn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "attn_gamma")(nn) if layer_scale >= 0 else nn
-    nn = drop_block(nn, drop_rate=drop_rate, name=name + "attn_")
-    attn_out = keras.layers.Add(name=name + "attn_out")([inputs, nn])
-
-    if is_attn_block:
-        nn = layer_norm(attn_out, name=name + "mlp_")
-        nn = mlp_block(nn, input_channel * mlp_ratio, activation=activation, name=name)
-    else:
-        nn = conv2d_no_bias(attn_out, input_channel * mlp_ratio, 1, strides=1, use_bias=True, name=name + "mlp_1_")
-        nn = batchnorm_with_activation(nn, activation=activation, name=name + "mlp_1_")
-        nn = conv2d_no_bias(nn, input_channel, 1, strides=1, use_bias=True, name=name + "mlp_2_")
-        nn = batchnorm_with_activation(nn, activation=None, name=name + "mlp_2_")
-    nn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "mlp_gamma")(nn) if layer_scale >= 0 else nn
-    nn = drop_block(nn, drop_rate=drop_rate, name=name + "mlp_")
-    return keras.layers.Add(name=name + "output")([attn_out, nn])
-
-
-def EfficientFormer(
-    num_blocks=[3, 2, 6, 4],
-    out_channels=[48, 96, 224, 448],
-    mlp_ratios=4,
-    num_attn_blocks_each_stack=[0, 0, 0, 1],
-    stem_width=-1,
-    stem_activation="relu",
-    layer_scale=1e-5,
+def block(inputs, mlp_ratio=2, partial_conv_ratio=0.25, layer_scale=1e-6, drop_rate=0, activation="gelu", name=""):
+    channel_axis = -1 if backend.image_data_format() == "channels_last" else 1
+    input_channel = inputs.shape[channel_axis]
+    conv_branch_channels = int(input_channel * partial_conv_ratio)
+
+    conv_branch, non_conv_branch = functional.split(inputs, [conv_branch_channels, -1], axis=channel_axis)
+    conv_branch = conv2d_no_bias(conv_branch, conv_branch_channels, kernel_size=3, strides=1, padding="same", name=name + "partial")
+    nn = functional.concat([conv_branch, non_conv_branch], axis=channel_axis)
+
+    mlp = conv2d_no_bias(nn, int(input_channel * mlp_ratio), kernel_size=1, name=name + "mlp_1_")
+    mlp = batchnorm_with_activation(mlp, activation=activation, name=name + "mlp_")
+    mlp = conv2d_no_bias(mlp, input_channel, kernel_size=1, name=name + "mlp_2_")
+    return add_with_layer_scale_and_drop_block(inputs, mlp, layer_scale=layer_scale, drop_rate=drop_rate, name=name)
+
+
+def FasterNet(
+    num_blocks=[1, 2, 8, 2],
+    embed_dim=40,
+    patch_size=4,
+    mlp_ratio=2,
+    partial_conv_ratio=0.25,
+    output_conv_filter=1280,
+    layer_scale=0,  # > 0 for applying layer_scale, 0 for not using
+    # head_init_scale=0.02,
     input_shape=(224, 224, 3),
     num_classes=1000,
     activation="gelu",
     drop_connect_rate=0,
-    classifier_activation=None,
-    use_distillation=True,
+    classifier_activation="softmax",
     dropout=0,
     pretrained=None,
-    model_name="efficientformer",
+    model_name="fasternet",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
-    stem_width = stem_width if stem_width > 0 else out_channels[0]
-    stem_activation = stem_activation if stem_activation is not None else activation
-    nn = conv2d_no_bias(inputs, stem_width // 2, 3, strides=2, use_bias=True, padding="same", name="stem_1_")
-    nn = batchnorm_with_activation(nn, activation=stem_activation, name="stem_1_")
-    nn = conv2d_no_bias(nn, stem_width, 3, strides=2, use_bias=True, padding="same", name="stem_2_")
-    nn = batchnorm_with_activation(nn, activation=stem_activation, name="stem_2_")
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+
+    """ Stem """
+    nn = conv2d_no_bias(inputs, embed_dim, kernel_size=patch_size, strides=patch_size, padding="valid", name="stem_")
+    nn = batchnorm_with_activation(nn, activation=None, name="stem_")
 
-    """ stages """
+    """ Blocks """
     total_blocks = sum(num_blocks)
     global_block_id = 0
-    for stack_id, (num_block, out_channel) in enumerate(zip(num_blocks, out_channels)):
+    for stack_id, num_block in enumerate(num_blocks):
         stack_name = "stack{}_".format(stack_id + 1)
         if stack_id > 0:
-            ds_name = stack_name + "downsample_"
-            nn = conv2d_no_bias(nn, out_channel, kernel_size=3, strides=2, use_bias=True, padding="SAME", name=ds_name)
-            nn = batchnorm_with_activation(nn, activation=None, name=ds_name)
-
-        cur_num_attn_blocks = num_attn_blocks_each_stack[stack_id] if isinstance(num_attn_blocks_each_stack, (list, tuple)) else num_attn_blocks_each_stack
-        cur_mlp_ratios = mlp_ratios[stack_id] if isinstance(mlp_ratios, (list, tuple)) else mlp_ratios
+            input_channel = nn.shape[-1] if backend.image_data_format() == "channels_last" else nn.shape[1]
+            nn = conv2d_no_bias(nn, input_channel * 2, kernel_size=2, strides=2, name=stack_name + "downsample_")
+            nn = batchnorm_with_activation(nn, activation=None, name=stack_name + "downsample_")
         for block_id in range(num_block):
             block_name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
-            mlp_ratio = cur_mlp_ratios[block_id] if isinstance(cur_mlp_ratios, (list, tuple)) else cur_mlp_ratios
-            is_attn_block = True if block_id > num_block - cur_num_attn_blocks - 1 else False
-            nn = meta_block(nn, is_attn_block, mlp_ratio=mlp_ratio, layer_scale=layer_scale, drop_rate=block_drop_rate, activation=activation, name=block_name)
+            nn = block(nn, mlp_ratio, partial_conv_ratio, layer_scale, block_drop_rate, activation, name=block_name)
             global_block_id += 1
 
-    """ output """
+    """  Output head """
     if num_classes > 0:
-        nn = layer_norm(nn, name="pre_output_")
-        nn = keras.layers.GlobalAveragePooling2D()(nn)  # tf.reduce_mean(nn, axis=1)
-        if dropout > 0 and dropout < 1:
-            nn = keras.layers.Dropout(dropout)(nn)
-        out = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(nn)
-
-        if use_distillation:
-            distill = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="distill_head")(nn)
-            out = [out, distill]
-    else:
-        out = nn
+        nn = layers.GlobalAveragePooling2D(name="avg_pool", keepdims=True)(nn)
+        nn = conv2d_no_bias(nn, output_conv_filter, 1, strides=1, padding="valid", name="post_")
+        nn = activation_by_name(nn, activation=activation, name="post_")
+        nn = layers.Flatten()(nn)
+
+        if dropout > 0:
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        head_init = initializers.TruncatedNormal(stddev=0.02)  # HeadInitializer(scale=head_init_scale)
+        bias_init = initializers.TruncatedNormal(stddev=0.02)  # HeadInitializer(scale=head_init_scale)
+        nn = layers.Dense(
+            num_classes, dtype="float32", activation=classifier_activation, kernel_initializer=head_init, bias_initializer=bias_init, name="predictions"
+        )(nn)
 
-    model = keras.models.Model(inputs, out, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
-    reload_model_weights(model, PRETRAINED_DICT, "efficientformer", pretrained, MultiHeadPositionalEmbedding)
+    reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="fasternet", pretrained=pretrained)
     return model
 
 
-def EfficientFormerL1(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=True, pretrained="imagenet", **kwargs):
-    return EfficientFormer(**locals(), model_name="efficientformer_l1", **kwargs)
+@register_model
+def FasterNetT0(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    return FasterNet(**locals(), model_name="fasternet_t0", **kwargs)
+
+
+@register_model
+def FasterNetT1(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    embed_dim = 64
+    return FasterNet(**locals(), model_name="fasternet_t1", **kwargs)
+
+
+@register_model
+def FasterNetT2(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    embed_dim = 96
+    return FasterNet(**locals(), model_name="fasternet_t2", **kwargs)
+
+
+@register_model
+def FasterNetS(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    embed_dim = 128
+    num_blocks = [1, 2, 13, 2]
+    return FasterNet(**locals(), model_name="fasternet_s", **kwargs)
+
+
+@register_model
+def FasterNetM(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    embed_dim = 144
+    num_blocks = [3, 4, 18, 3]
+    return FasterNet(**locals(), model_name="fasternet_m", **kwargs)
 
 
-def EfficientFormerL3(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=True, pretrained="imagenet", **kwargs):
-    num_blocks = [4, 4, 12, 6]
-    out_channels = [64, 128, 320, 512]
-    num_attn_blocks_each_stack = [0, 0, 0, 4]
-    return EfficientFormer(**locals(), model_name="efficientformer_l3", **kwargs)
-
-
-def EfficientFormerL7(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=True, pretrained="imagenet", **kwargs):
-    num_blocks = [6, 6, 18, 8]
-    out_channels = [96, 192, 384, 768]
-    num_attn_blocks_each_stack = [0, 0, 0, 8]
-    return EfficientFormer(**locals(), model_name="efficientformer_l7", **kwargs)
+@register_model
+def FasterNetL(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    embed_dim = 192
+    num_blocks = [3, 4, 18, 3]
+    return FasterNet(**locals(), model_name="fasternet_l", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientformer/efficientformer_v2.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientformer/efficientformer_v2.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,18 +1,20 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, models, is_channels_last
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     add_with_layer_scale_and_drop_block,
     batchnorm_with_activation,
     conv2d_no_bias,
     ChannelAffine,
     depthwise_conv2d_no_bias,
     drop_block,
     MultiHeadPositionalEmbedding,
+    qkv_to_multi_head_channels_last_format,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
     "efficientformer_v2_l": {"imagenet": {224: "3792a3ea9eb9d1e818d4a36c00e422a5"}},
     "efficientformer_v2_s0": {"imagenet": {224: "88de5ba4a8effd887d53df3020ba8433"}},
@@ -30,105 +32,121 @@
     attn_ratio=4,
     use_local_global_query=False,
     use_talking_head=True,
     qkv_bias=True,
     activation="gelu",
     name=None,
 ):
-    input_channel = inputs.shape[-1]
+    height_axis, width_axis, channel_aixs = (1, 2, 3) if is_channels_last() else (2, 3, 1)
+    height, width, input_channel = inputs.shape[height_axis], inputs.shape[width_axis], inputs.shape[channel_aixs]
     key_dim = key_dim if key_dim > 0 else input_channel // num_heads
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
+    # qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
+    qk_scale = 1.0 / (float(key_dim) ** 0.5)
     out_shape = input_channel if out_shape is None else out_shape
     qk_out = num_heads * key_dim
     value_out = attn_ratio * qk_out
     value_dim = attn_ratio * key_dim
 
     if strides > 1:
-        should_cut_height, should_cut_width = inputs.shape[1] % 2, inputs.shape[2] % 2  # keep shape same with inputs after later UpSampling2D
+        should_cut_height, should_cut_width = height % 2, width % 2  # keep shape same with inputs after later UpSampling2D
         inputs = depthwise_conv2d_no_bias(inputs, use_bias=True, kernel_size=3, strides=strides, padding="same", name=name and name + "down_sample_")
         inputs = batchnorm_with_activation(inputs, activation=None, name=name and name + "down_sample_")
 
-    kv_blocks = inputs.shape[1] * inputs.shape[2]
+    kv_blocks = height * width
 
     if use_local_global_query:
-        # pool_query = keras.layers.AvgPool2D(pool_size=1, strides=2)(inputs)
-        pool_query = inputs[:, ::2, ::2]  # nn.AvgPool2d(kernel_size=1, stride=2, padding=0)
+        # pool_query = layers.AvgPool2D(pool_size=1, strides=2)(inputs)
+        # pool_query = inputs[:, ::2, ::2] if is_channels_last() else inputs[:, :, ::2, ::2] # nn.AvgPool2d(kernel_size=1, stride=2, padding=0)
+        pool_query = layers.AvgPool2D(pool_size=1, strides=2)(inputs)
         local_query = depthwise_conv2d_no_bias(inputs, use_bias=qkv_bias, kernel_size=3, strides=2, padding="same", name=name and name + "local_query_")
         pre_query = pool_query + local_query
         vv_local_strides = 2
     else:
         pre_query = inputs
         vv_local_strides = 1
-    _, query_height, query_width, _ = pre_query.shape
+    query_height, query_width = pre_query.shape[1:-1] if is_channels_last() else pre_query.shape[2:]
 
     query = conv2d_no_bias(pre_query, qk_out, use_bias=qkv_bias, kernel_size=1, name=name and name + "query_")
     query = batchnorm_with_activation(query, activation=None, name=name and name + "query_")
-    query = tf.transpose(tf.reshape(query, [-1, query_height * query_width, num_heads, key_dim]), [0, 2, 1, 3])  #  [batch, num_heads, hh * ww, key_dim]
 
     key = conv2d_no_bias(inputs, qk_out, use_bias=qkv_bias, kernel_size=1, name=name and name + "key_")
     key = batchnorm_with_activation(key, activation=None, name=name and name + "key_")
-    key = tf.transpose(tf.reshape(key, [-1, kv_blocks, num_heads, key_dim]), [0, 2, 3, 1])  #  [batch, num_heads, key_dim, hh * ww]
 
     value = conv2d_no_bias(inputs, value_out, use_bias=qkv_bias, kernel_size=1, name=name and name + "value_")
     value = batchnorm_with_activation(value, activation=None, name=name and name + "value_")
     vv_local = depthwise_conv2d_no_bias(value, use_bias=qkv_bias, kernel_size=3, strides=vv_local_strides, padding="same", name=name and name + "value_local_")
     vv_local = batchnorm_with_activation(vv_local, activation=None, name=name and name + "value_local_")
-    value = tf.transpose(tf.reshape(value, [-1, kv_blocks, num_heads, value_dim]), [0, 2, 1, 3])  #  [batch, num_heads, hh * ww, value_dim]
 
-    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale  # [batch, num_heads, hh * ww, hh * ww]
+    # query = functional.transpose(
+    #     functional.reshape(query, [-1, query_height * query_width, num_heads, key_dim]), [0, 2, 1, 3]
+    # )  #  [batch, num_heads, hh * ww, key_dim]
+    # key = functional.transpose(functional.reshape(key, [-1, kv_blocks, num_heads, key_dim]), [0, 2, 3, 1])  #  [batch, num_heads, key_dim, hh * ww]
+    # value = functional.transpose(functional.reshape(value, [-1, kv_blocks, num_heads, value_dim]), [0, 2, 1, 3])  #  [batch, num_heads, hh * ww, value_dim]
+    query, key, value = qkv_to_multi_head_channels_last_format(query, key, value, num_heads=num_heads)
+
+    # attention_scores = layers.Lambda(lambda xx: functional.matmul(xx[0], xx[1]))([query, key]) * qk_scale  # [batch, num_heads, hh * ww, hh * ww]
     # print(f"{query.shape = }, {key.shape = }, {value.shape = }, {attention_scores.shape = }, {query_height = }")
+    attention_scores = (query @ key) * qk_scale
     attention_scores = MultiHeadPositionalEmbedding(query_height=query_height, name=name and name + "pos_emb")(attention_scores)
 
     if use_talking_head:
-        attention_scores = tf.transpose(attention_scores, [0, 2, 3, 1])  # [batch, hh * ww, hh * ww, num_heads]
+        # if channels_last, attention_scores [batch, num_heads, hh * ww, hh * ww] -> [batch, hh * ww, hh * ww, num_heads]
+        attention_scores = functional.transpose(attention_scores, [0, 2, 3, 1]) if is_channels_last() else attention_scores
         attention_scores = conv2d_no_bias(attention_scores, num_heads, use_bias=True, name=name and name + "talking_head_1_")
-        attention_scores = keras.layers.Softmax(axis=2, name=name and name + "attention_scores")(attention_scores)  # On previous last dimension
+        # On previous last dimension
+        attention_scores = layers.Softmax(axis=2 if is_channels_last() else -1, name=name and name + "attention_scores")(attention_scores)
         attention_scores = conv2d_no_bias(attention_scores, num_heads, use_bias=True, name=name and name + "talking_head_2_")
-        # attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores) if attn_dropout > 0 else attention_scores
-        attention_scores = tf.transpose(attention_scores, [0, 3, 1, 2])  # [batch, num_heads, hh * ww, hh * ww]
+        # if channels_last, attention_scores [batch, hh * ww, hh * ww, num_heads] -> [batch, num_heads, hh * ww, hh * ww]
+        attention_scores = functional.transpose(attention_scores, [0, 3, 1, 2]) if is_channels_last() else attention_scores
     else:
-        attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
-        # attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores) if attn_dropout > 0 else attention_scores
+        attention_scores = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
+        # attention_scores = layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores) if attn_dropout > 0 else attention_scores
 
     # value = [batch, num_heads, hh * ww, value_dim], attention_output = [batch, num_heads, hh * ww, value_dim]
-    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
-    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
-    attention_output = tf.reshape(attention_output, [-1, query_height, query_width, num_heads * value_dim])
+    # attention_output = layers.Lambda(lambda xx: functional.matmul(xx[0], xx[1]))([attention_scores, value])
+    attention_output = attention_scores @ value
+    if is_channels_last():
+        attention_output = functional.transpose(attention_output, [0, 2, 1, 3])
+        attention_output = functional.reshape(attention_output, [-1, query_height, query_width, num_heads * value_dim])
+    else:
+        attention_output = functional.transpose(attention_output, [0, 1, 3, 2])
+        attention_output = functional.reshape(attention_output, [-1, num_heads * value_dim, query_height, query_width])
     attention_output += vv_local
     # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
 
     if strides > 1:
-        attention_output = keras.layers.UpSampling2D(size=(strides, strides), interpolation="bilinear")(attention_output)
+        attention_output = layers.UpSampling2D(size=(strides, strides), interpolation="bilinear")(attention_output)
         if should_cut_height > 0 or should_cut_width > 0:
-            attention_output = attention_output[:, : attention_output.shape[1] - should_cut_height, : attention_output.shape[2] - should_cut_width]
+            keep_height, keep_width = attention_output.shape[height_axis] - should_cut_height, attention_output.shape[width_axis] - should_cut_width
+            attention_output = attention_output[:, :keep_height, :keep_width] if is_channels_last() else attention_output[:, :, :keep_height, :keep_width]
 
     # [batch, hh, ww, num_heads * value_dim] * [num_heads * value_dim, out] --> [batch, hh, ww, out]
     attention_output = activation_by_name(attention_output, activation=activation, name=name)
     attention_output = conv2d_no_bias(attention_output, out_shape, use_bias=True, kernel_size=1, name=name and name + "output_")
     attention_output = batchnorm_with_activation(attention_output, activation=None, name=name and name + "output_")
 
     return attention_output
 
 
 def mlp_block_with_additional_depthwise_conv(inputs, hidden_dim, output_channel=-1, drop_rate=0, activation="gelu", name=""):
-    output_channel = output_channel if output_channel > 0 else inputs.shape[-1]
+    output_channel = output_channel if output_channel > 0 else inputs.shape[-1 if is_channels_last() else 1]
     nn = conv2d_no_bias(inputs, hidden_dim, kernel_size=1, use_bias=True, name=name + "1_")
     nn = batchnorm_with_activation(nn, activation=activation, name=name + "1_")
 
     nn = depthwise_conv2d_no_bias(nn, use_bias=True, kernel_size=3, strides=1, padding="same", name=name + "mid_")
     nn = batchnorm_with_activation(nn, activation=activation, name=name + "mid_")
 
     nn = conv2d_no_bias(nn, output_channel, kernel_size=1, use_bias=True, name=name + "2_")
     nn = batchnorm_with_activation(nn, activation=None, name=name + "2_")
-    nn = keras.layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
+    nn = layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
     return nn
 
 
 def down_sample_block(inputs, out_channel, use_attn=False, activation="gelu", name=""):
-    conv_branch = conv2d_no_bias(inputs, out_channel, kernel_size=3, strides=2, use_bias=True, padding="SAME", name=name)
+    conv_branch = conv2d_no_bias(inputs, out_channel, kernel_size=3, strides=2, use_bias=True, padding="same", name=name)
     conv_branch = batchnorm_with_activation(conv_branch, activation=None, name=name)
     if use_attn:
         fixed_kwargs = {"num_heads": 8, "key_dim": 16, "strides": 1, "attn_ratio": 4, "use_local_global_query": True, "use_talking_head": False}
         attn_branch = conv_mhsa_with_multi_head_position(inputs, **fixed_kwargs, out_shape=out_channel, activation=activation, name=name + "attn_")
         nn = attn_branch + conv_branch
     else:
         nn = conv_branch
@@ -144,21 +162,24 @@
     stem_activation=None,
     layer_scale=1e-5,
     input_shape=(224, 224, 3),
     num_classes=1000,
     activation="gelu",
     drop_connect_rate=0,
     classifier_activation=None,
-    use_distillation=True,
+    use_distillation=False,
     dropout=0,
     pretrained=None,
     model_name="efficientformer_v2",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
     stem_width = stem_width if stem_width > 0 else out_channels[0]
     stem_activation = stem_activation if stem_activation is not None else activation
     nn = conv2d_no_bias(inputs, stem_width // 2, 3, strides=2, use_bias=True, padding="same", name="stem_1_")
     nn = batchnorm_with_activation(nn, activation=stem_activation, name="stem_1_")
     nn = conv2d_no_bias(nn, stem_width, 3, strides=2, use_bias=True, padding="same", name="stem_2_")
     nn = batchnorm_with_activation(nn, activation=stem_activation, name="stem_2_")
 
@@ -177,56 +198,70 @@
             block_name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             cur_mlp_ratio = cur_mlp_ratios[block_id] if isinstance(cur_mlp_ratios, (list, tuple)) else cur_mlp_ratios
             if block_id > num_block - cur_num_attn_blocks - 1:
                 strides = 2 if stack_id == 2 else 1
                 attn_out = conv_mhsa_with_multi_head_position(nn, num_heads=8, key_dim=32, strides=strides, activation=activation, name=block_name + "attn_")
                 nn = add_with_layer_scale_and_drop_block(nn, attn_out, layer_scale=layer_scale, drop_rate=block_drop_rate, name=block_name + "attn_")
-            mlp_out = mlp_block_with_additional_depthwise_conv(nn, nn.shape[-1] * cur_mlp_ratio, activation=activation, name=block_name + "mlp_")
+            mlp_out = mlp_block_with_additional_depthwise_conv(nn, out_channel * cur_mlp_ratio, activation=activation, name=block_name + "mlp_")
             nn = add_with_layer_scale_and_drop_block(nn, mlp_out, layer_scale=layer_scale, drop_rate=block_drop_rate, name=block_name + "mlp_")
             global_block_id += 1
 
     """ output """
     if num_classes > 0:
         nn = batchnorm_with_activation(nn, activation=None, name="pre_output_")
-        nn = keras.layers.GlobalAveragePooling2D()(nn)  # tf.reduce_mean(nn, axis=1)
+        nn = layers.GlobalAveragePooling2D()(nn)  # tf.reduce_mean(nn, axis=1)
         if dropout > 0 and dropout < 1:
-            nn = keras.layers.Dropout(dropout)(nn)
-        out = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(nn)
+            nn = layers.Dropout(dropout)(nn)
+        out = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(nn)
 
         if use_distillation:
-            distill = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="distill_head")(nn)
+            distill = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="distill_head")(nn)
             out = [out, distill]
     else:
         out = nn
 
-    model = keras.models.Model(inputs, out, name=model_name)
-    add_pre_post_process(model, rescale_mode="torch")
+    model = models.Model(inputs, out, name=model_name)
     reload_model_weights(model, PRETRAINED_DICT, "efficientformer", pretrained, MultiHeadPositionalEmbedding)
+
+    add_pre_post_process(model, rescale_mode="torch")
+    model.switch_to_deploy = lambda: switch_to_deploy(model)
     return model
 
 
-def EfficientFormerV2S0(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=True, pretrained="imagenet", **kwargs):
+def switch_to_deploy(model):
+    from keras_cv_attention_models.model_surgery.model_surgery import fuse_distill_head
+
+    new_model = fuse_distill_head(model, head_bn="pre_output_bn", distill_head_bn="pre_output_bn") if "head" in model.output_names else model
+    add_pre_post_process(new_model, rescale_mode=model.preprocess_input.rescale_mode, post_process=model.decode_predictions)
+    return new_model
+
+
+@register_model
+def EfficientFormerV2S0(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=False, pretrained="imagenet", **kwargs):
     return EfficientFormerV2(**locals(), model_name="efficientformer_v2_s0", **kwargs)
 
 
-def EfficientFormerV2S1(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=True, pretrained="imagenet", **kwargs):
+@register_model
+def EfficientFormerV2S1(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=False, pretrained="imagenet", **kwargs):
     num_blocks = [3, 3, 9, 6]
     out_channels = [32, 48, 120, 224]
     mlp_ratios = [4, 4, [4, 4, 3, 3, 3, 3, 4, 4, 4], [4, 4, 3, 3, 4, 4]]
     return EfficientFormerV2(**locals(), model_name="efficientformer_v2_s1", **kwargs)
 
 
-def EfficientFormerV2S2(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=True, pretrained="imagenet", **kwargs):
+@register_model
+def EfficientFormerV2S2(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=False, pretrained="imagenet", **kwargs):
     num_blocks = [4, 4, 12, 8]
     out_channels = [32, 64, 144, 288]
     mlp_ratios = [4, 4, [4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4], [4, 4, 3, 3, 3, 3, 4, 4]]
     num_attn_blocks_each_stack = [0, 0, 4, 4]
     return EfficientFormerV2(**locals(), model_name="efficientformer_v2_s2", **kwargs)
 
 
-def EfficientFormerV2L(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=True, pretrained="imagenet", **kwargs):
+@register_model
+def EfficientFormerV2L(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=False, pretrained="imagenet", **kwargs):
     num_blocks = [5, 5, 15, 10]
     out_channels = [40, 80, 192, 384]
     mlp_ratios = [4, 4, [4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4], [4, 4, 4, 3, 3, 3, 3, 4, 4, 4]]
     num_attn_blocks_each_stack = [0, 0, 6, 6]
     return EfficientFormerV2(**locals(), model_name="efficientformer_v2_l", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientnet/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -24,14 +24,20 @@
     EfficientNetV1L2,
     EfficientNetV1Lite0,
     EfficientNetV1Lite1,
     EfficientNetV1Lite2,
     EfficientNetV1Lite3,
     EfficientNetV1Lite4,
 )
+from keras_cv_attention_models.efficientnet.efficientnet_edgetpu import (
+    EfficientNetEdgeTPU,
+    EfficientNetEdgeTPUSmall,
+    EfficientNetEdgeTPUMedium,
+    EfficientNetEdgeTPULarge,
+)
 
 __v2_head_doc__ = """
 Keras implementation of [Official efficientnetv2](https://github.com/google/automl/tree/master/efficientnetv2).
 Paper [arXiv 2104.00298 EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298) by Mingxing Tan, Quoc V. Le.
 """
 
 __tail_doc__ = """  input_shape: it should have exactly 3 inputs channels. Set `(None, None, 3)` for dynamic.
@@ -51,15 +57,14 @@
       Note that different preprocessing will applied for different `rescale_mode`. It depends on `pretained` and `model_type`.
       - For all V1 models, `rescale_mode`s are "torch".
       - For "21k" pretrained V2 models, `rescale_mode`s are all "tf"
       - For "imagenet" pretrained V2 models, "bx" models are all "torch", ["s", "m", "l", "xl"] are "tf".
       Default `False`.
   pretrained: value in {pretrained}.
       Will try to download and load pre-trained model weights if not None.
-      Save path is `~/.keras/models/efficientnetv2/`.
 
 Returns:
     A `keras.Model` instance.
 """
 
 EfficientNetV2.__doc__ = __v2_head_doc__ + """
 Args:
@@ -134,15 +139,14 @@
 __v1_head_doc__ = """
 Keras implementation of [Github tensorflow/tpu/efficientnet](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet).
 Paper [PDF 1911.04252 Self-training with Noisy Student improves ImageNet classification](https://arxiv.org/pdf/1911.04252.pdf).
 """
 
 EfficientNetV1.__doc__ = __v1_head_doc__ + """
 Args:
-  model_type: is the pre-defined model, value in ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "l2"].
   model_name: string, model name.
 """ + __tail_doc__.format(pretrained=[None, "imagenet", "noisy_student"]) + """
 Model architectures:
   | V1 Model                       | Params | FLOPs   | Input | Top1 Acc |
   | ------------------------------ | ------ | ------- | ----- | -------- |
   | EfficientNetV1B0               | 5.3M   | 0.39G   | 224   | 77.6     |
   | - NoisyStudent                 | 5.3M   | 0.39G   | 224   | 78.8     |
@@ -205,7 +209,43 @@
   {train_config}
 """
 EfficientNetV1Lite0.__doc__ = __v1_lite_default_doc__.format(train_config="| EfficientNetV1Lite0 | 320              | 0.2     | 0.2               |")
 EfficientNetV1Lite1.__doc__ = __v1_lite_default_doc__.format(train_config="| EfficientNetV1Lite1 | 384              | 0.2     | 0.2               |")
 EfficientNetV1Lite2.__doc__ = __v1_lite_default_doc__.format(train_config="| EfficientNetV1Lite2 | 448              | 0.3     | 0.2               |")
 EfficientNetV1Lite3.__doc__ = __v1_lite_default_doc__.format(train_config="| EfficientNetV1Lite3 | 512              | 0.3     | 0.2               |")
 EfficientNetV1Lite4.__doc__ = __v1_lite_default_doc__.format(train_config="| EfficientNetV1Lite4 | 640              | 0.4     | 0.2               |")
+
+__edge_tpu_head_doc__ = """
+Keras implementation of [Github tensorflow/tpu/efficientnet/edgetpu](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/edgetpu).
+"""
+
+EfficientNetEdgeTPU.__doc__ = __edge_tpu_head_doc__ + """
+Args:
+  model_name: string, model name.
+""" + __tail_doc__.format(pretrained=[None, "imagenet"]) + """
+Model architectures:
+  | Edge TPU Model              | Params | FLOPs   | Input | Top1 Acc |
+  | --------------------------- | ------ | ------- | ----- | -------- |
+  | EfficientNetEdgeTPUSmall    | 5.49M  | 1.79G   | 224   | 78.07    |
+  | EfficientNetEdgeTPUMedium   | 6.90M  | 3.01G   | 240   | 79.25    |
+  | EfficientNetEdgeTPULarge    | 10.59M | 7.94G   | 300   | 81.32    |
+
+Training configures:
+  | Model                     | Input resolution | Dropout | Drop connect rate |
+  | ------------------------- | ---------------- | ------- | ----------------- |
+  | EfficientNetEdgeTPUSmall  | 224              | 0.2     | 0.2               |
+  | EfficientNetEdgeTPUMedium | 240              | 0.2     | 0.2               |
+  | EfficientNetEdgeTPULarge  | 300              | 0.3     | 0.2               |
+"""
+
+__edge_tpu_default_doc__ = __edge_tpu_head_doc__ + """
+Args:
+""" + __tail_doc__.format(pretrained=[None, "imagenet"]) + """
+Training configures: `Eval size` is used as the default model `input_shape`.
+  | Model                     | Input resolution | Dropout | Drop connect rate |
+  | ------------------------- | ---------------- | ------- | ----------------- |
+  {train_config}
+"""
+
+EfficientNetEdgeTPUSmall.__doc__ = __edge_tpu_default_doc__.format(train_config="| EfficientNetEdgeTPUSmall  | 224              | 0.2     | 0.2               |")
+EfficientNetEdgeTPUMedium.__doc__ = __edge_tpu_default_doc__.format(train_config="| EfficientNetEdgeTPUMedium | 240              | 0.2     | 0.2               |")
+EfficientNetEdgeTPULarge.__doc__ = __edge_tpu_default_doc__.format(train_config="| EfficientNetEdgeTPULarge  | 300              | 0.2     | 0.3               |")
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientnet/convert_effnetv2_model.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/convert_effnetv2_model.py`

 * *Files 1% similar despite different names*

```diff
@@ -122,15 +122,15 @@
         self._se_expand = tf.keras.layers.Conv2D(
             output_filters, kernel_size=1, strides=1, kernel_initializer=conv_kernel_initializer, padding="same", data_format=self._data_format, use_bias=True
         )
 
     def call(self, inputs):
         h_axis, w_axis = [2, 3] if self._data_format == "channels_first" else [1, 2]
         if self._local_pooling:
-            se_tensor = tf.nn.avg_pool(inputs, ksize=[1, inputs.shape[h_axis], inputs.shape[w_axis], 1], strides=[1, 1, 1, 1], padding="VALID")
+            se_tensor = tf.nn.avg_pool(inputs, ksize=[1, inputs.shape[h_axis], inputs.shape[w_axis], 1], strides=[1, 1, 1, 1], padding="valid")
         else:
             se_tensor = tf.reduce_mean(inputs, [h_axis, w_axis], keepdims=True)
         se_tensor = self._se_expand(self._act(self._se_reduce(se_tensor)))
         logging.info("Built SE %s : %s", self.name, se_tensor.shape)
         return tf.sigmoid(se_tensor) * inputs
 
 
@@ -440,15 +440,15 @@
         """Call the layer."""
         outputs = self._act(self._norm(self._conv_head(inputs), training=training))
         self.endpoints["head_1x1"] = outputs
 
         if self._mconfig.local_pooling:
             shape = outputs.get_shape().as_list()
             kernel_size = [1, shape[self.h_axis], shape[self.w_axis], 1]
-            outputs = tf.nn.avg_pool(outputs, ksize=kernel_size, strides=[1, 1, 1, 1], padding="VALID")
+            outputs = tf.nn.avg_pool(outputs, ksize=kernel_size, strides=[1, 1, 1, 1], padding="valid")
             self.endpoints["pooled_features"] = outputs
             if self._dropout:
                 outputs = self._dropout(outputs, training=training)
             self.endpoints["global_pool"] = outputs
             if self._fc:
                 outputs = tf.squeeze(outputs, [self.h_axis, self.w_axis])
                 outputs = self._fc(outputs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientnet/efficientnet_v1.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/efficientnet_v1.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """
 Creates a EfficientNetV1 Model as defined in: EfficientNetV1: Self-training with Noisy Student improves ImageNet classification.
 arXiv preprint arXiv:1911.04252.
 """
-import tensorflow as tf
+import math
 from keras_cv_attention_models.efficientnet.efficientnet_v2 import EfficientNetV2
+from keras_cv_attention_models.models import register_model
 
 
 def get_expanded_width_depth(width, depth, fix_head_stem=False):
     out_channels = [ii * width for ii in [16, 24, 40, 80, 112, 192, 320]]
-    depthes = [int(tf.math.ceil(ii * depth)) for ii in [1, 2, 2, 3, 3, 4, 1]]
+    depthes = [int(math.ceil(ii * depth)) for ii in [1, 2, 2, 3, 3, 4, 1]]
     if fix_head_stem:
         depthes[0], depthes[-1] = 1, 1
         first_conv_filter, output_conv_filter = 32, 1280
     else:
         first_conv_filter = 32 * width
         output_conv_filter = 1280 * width
     return out_channels, depthes, first_conv_filter, output_conv_filter
@@ -32,99 +33,116 @@
     model_name="EfficientNetV1",
     **kwargs,
 ):
     kwargs.pop("kwargs", None)
     return EfficientNetV2(**locals(), **kwargs)
 
 
+@register_model
 def EfficientNetV1B0(input_shape=(224, 224, 3), num_classes=1000, dropout=0.2, classifier_activation="softmax", pretrained="noisy_student", **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.0, 1.0)
     first_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter)
     output_conv_filter = kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), model_name="efficientnet_v1-b0", **kwargs)
 
 
+@register_model
 def EfficientNetV1B1(input_shape=(240, 240, 3), num_classes=1000, dropout=0.2, classifier_activation="softmax", pretrained="noisy_student", **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.0, 1.1)
     first_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter)
     output_conv_filter = kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), model_name="efficientnet_v1-b1", **kwargs)
 
 
+@register_model
 def EfficientNetV1B2(input_shape=(260, 260, 3), num_classes=1000, dropout=0.3, classifier_activation="softmax", pretrained="noisy_student", **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.1, 1.2)
     first_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter)
     output_conv_filter = kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), model_name="efficientnet_v1-b2", **kwargs)
 
 
+@register_model
 def EfficientNetV1B3(input_shape=(300, 300, 3), num_classes=1000, dropout=0.3, classifier_activation="softmax", pretrained="noisy_student", **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.2, 1.4)
     first_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter)
     output_conv_filter = kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), model_name="efficientnet_v1-b3", **kwargs)
 
 
+@register_model
 def EfficientNetV1B4(input_shape=(380, 380, 3), num_classes=1000, dropout=0.4, classifier_activation="softmax", pretrained="noisy_student", **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.4, 1.8)
     first_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter)
     output_conv_filter = kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), model_name="efficientnet_v1-b4", **kwargs)
 
 
+@register_model
 def EfficientNetV1B5(input_shape=(456, 456, 3), num_classes=1000, dropout=0.4, classifier_activation="softmax", pretrained="noisy_student", **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.6, 2.2)
     first_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter)
     output_conv_filter = kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), model_name="efficientnet_v1-b5", **kwargs)
 
 
+@register_model
 def EfficientNetV1B6(input_shape=(528, 528, 3), num_classes=1000, dropout=0.5, classifier_activation="softmax", pretrained="noisy_student", **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.8, 2.6)
     first_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter)
     output_conv_filter = kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), model_name="efficientnet_v1-b6", **kwargs)
 
 
+@register_model
 def EfficientNetV1B7(input_shape=(600, 600, 3), num_classes=1000, dropout=0.5, classifier_activation="softmax", pretrained="noisy_student", **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(2.0, 3.1)
     first_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter)
     output_conv_filter = kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), model_name="efficientnet_v1-b7", **kwargs)
 
 
+@register_model
 def EfficientNetV1L2(input_shape=(800, 800, 3), num_classes=1000, dropout=0.5, classifier_activation="softmax", pretrained="noisy_student", **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(4.3, 5.3)
     first_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter)
     output_conv_filter = kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), model_name="efficientnet_v1-l2", **kwargs)
 
 
+""" EfficientNetV1Lite models from EfficientDet """
+
+
 # https://github.com/google/automl/tree/master/efficientdet/backbone/efficientnet_lite_builder.py
+@register_model
 def EfficientNetV1Lite0(input_shape=(320, 320, 3), num_classes=1000, dropout=0.2, classifier_activation="softmax", pretrained=None, **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.0, 1.0, fix_head_stem=True)
     first_conv_filter, output_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter), kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), se_ratios=[0] * len(depthes), is_fused=False, model_name="efficientnet_v1-lite0", **kwargs)
 
 
+@register_model
 def EfficientNetV1Lite1(input_shape=(384, 384, 3), num_classes=1000, dropout=0.2, classifier_activation="softmax", pretrained=None, **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.0, 1.1, fix_head_stem=True)
     first_conv_filter, output_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter), kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), se_ratios=[0] * len(depthes), is_fused=False, model_name="efficientnet_v1-lite1", **kwargs)
 
 
+@register_model
 def EfficientNetV1Lite2(input_shape=(448, 448, 3), num_classes=1000, dropout=0.3, classifier_activation="softmax", pretrained=None, **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.1, 1.2, fix_head_stem=True)
     first_conv_filter, output_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter), kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), se_ratios=[0] * len(depthes), is_fused=False, model_name="efficientnet_v1-lite2", **kwargs)
 
 
+@register_model
 def EfficientNetV1Lite3(input_shape=(512, 512, 3), num_classes=1000, dropout=0.3, classifier_activation="softmax", pretrained=None, **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.2, 1.4, fix_head_stem=True)
     first_conv_filter, output_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter), kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), se_ratios=[0] * len(depthes), is_fused=False, model_name="efficientnet_v1-lite3", **kwargs)
 
 
+@register_model
 def EfficientNetV1Lite4(input_shape=(640, 640, 3), num_classes=1000, dropout=0.4, classifier_activation="softmax", pretrained=None, **kwargs):
     out_channels, depthes, first_conv_filter, output_conv_filter = get_expanded_width_depth(1.4, 1.8, fix_head_stem=True)
     first_conv_filter, output_conv_filter = kwargs.pop("first_conv_filter", first_conv_filter), kwargs.pop("output_conv_filter", output_conv_filter)
     return EfficientNetV1(**locals(), se_ratios=[0] * len(depthes), is_fused=False, model_name="efficientnet_v1-lite4", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/efficientnet/efficientnet_v2.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientnet/efficientnet_v2.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """
 Creates a EfficientNetV2 Model as defined in: Mingxing Tan, Quoc V. Le. (2021). arXiv preprint arXiv:2104.00298.
 EfficientNetV2: Smaller Models and Faster Training.
 """
-import os
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, models, is_channels_last
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     batchnorm_with_activation,
     conv2d_no_bias,
     drop_block,
     global_context_module,
     make_divisible,
     output_block,
@@ -37,14 +37,17 @@
     "v1-b2": {"noisy_student": "b4ffed8b9262df4facc5e20557983ef8", "imagenet": "6c8d1d3699275c7d1867d08e219e00a7"},
     "v1-b3": {"noisy_student": "9d696365378a1ebf987d0e46a9d26ddd", "imagenet": "d78edb3dc7007721eda781c04bd4af62"},
     "v1-b4": {"noisy_student": "a0f61b977544493e6926186463d26294", "imagenet": "4c83aa5c86d58746a56675565d4f2051"},
     "v1-b5": {"noisy_student": "c3b6eb3f1f7a1e9de6d9a93e474455b1", "imagenet": "0bda50943b8e8d0fadcbad82c17c40f5"},
     "v1-b6": {"noisy_student": "20dd18b0df60cd7c0387c8af47bd96f8", "imagenet": "da13735af8209f675d7d7d03a54bfa27"},
     "v1-b7": {"noisy_student": "7f6f6dd4e8105e32432607ad28cfad0f", "imagenet": "d9c22b5b030d1e4f4c3a96dbf5f21ce6"},
     "v1-l2": {"noisy_student": "5fedc721febfca4b08b03d1f18a4a3ca"},
+    "edgetpu-large": {"imagenet": "e92d2a9f12611d10e1b8843ab43d578c"},
+    "edgetpu-medium": {"imagenet": "40a682a333496501d49f060c51e9858f"},
+    "edgetpu-small": {"imagenet": "89ec826f3682f52ec4ba73775dfb963b"},
 }
 
 
 def inverted_residual_block(
     inputs,
     output_channel,
     stride=1,
@@ -59,35 +62,36 @@
     se_divisor=1,  # 8 for mobilenetv3
     se_limit_round_down=0.9,  # 0.95 for fbnet
     use_global_context_instead_of_se=False,
     use_last_bn_zero_gamma=False,
     activation="swish",
     name=None,
 ):
-    channel_axis = 1 if K.image_data_format() == "channels_first" else -1
-    input_channel = inputs.shape[channel_axis]
+    input_channel = inputs.shape[-1 if is_channels_last() else 1]
     bn_eps = TORCH_BATCH_NORM_EPSILON if is_torch_mode else TF_BATCH_NORM_EPSILON
     hidden_channel = make_divisible(input_channel * expand, 8)
 
     if is_fused and expand != 1:
         nn = conv2d_no_bias(inputs, hidden_channel, 3, stride, padding="same", use_torch_padding=is_torch_mode, name=name and name + "sortcut_")
         nn = batchnorm_with_activation(nn, activation=activation, epsilon=bn_eps, name=name and name + "sortcut_")
     elif expand != 1:
         nn = conv2d_no_bias(inputs, hidden_channel, 1, strides=1, padding="valid", use_torch_padding=is_torch_mode, name=name and name + "sortcut_")
         nn = batchnorm_with_activation(nn, activation=activation, epsilon=bn_eps, name=name and name + "sortcut_")
     else:
         nn = inputs
 
     if not is_fused:
-        if is_torch_mode and kernel_size // 2 > 0:
-            nn = keras.layers.ZeroPadding2D(padding=kernel_size // 2, name=name and name + "pad")(nn)
-            padding = "VALID"
+        if is_torch_mode and backend.is_torch_backend and kernel_size // 2 > 0:
+            padding = kernel_size // 2
+        elif is_torch_mode and kernel_size // 2 > 0:
+            nn = layers.ZeroPadding2D(padding=kernel_size // 2, name=name and name + "pad")(nn)
+            padding = "valid"
         else:
-            padding = "SAME"
-        nn = keras.layers.DepthwiseConv2D(kernel_size, padding=padding, strides=stride, use_bias=False, name=name and name + "MB_dw_")(nn)
+            padding = "same"
+        nn = layers.DepthwiseConv2D(kernel_size, padding=padding, strides=stride, use_bias=False, name=name and name + "MB_dw_")(nn)
         nn = batchnorm_with_activation(nn, activation=activation, epsilon=bn_eps, name=name and name + "MB_dw_")
 
     if se_ratio > 0:
         se_activation = activation if se_activation is None else se_activation
         se_ratio = se_ratio / expand
         if use_global_context_instead_of_se:
             nn = global_context_module(nn, use_attn=True, ratio=se_ratio, divisor=1, activation=se_activation, use_bias=True, name=name and name + "gc_")
@@ -100,26 +104,27 @@
         nn = batchnorm_with_activation(nn, activation=activation, zero_gamma=use_last_bn_zero_gamma, epsilon=bn_eps, name=name and name + "fu_")
     else:
         nn = conv2d_no_bias(nn, output_channel, 1, strides=1, padding="valid", use_torch_padding=is_torch_mode, name=name and name + "MB_pw_")
         nn = batchnorm_with_activation(nn, activation=None, zero_gamma=use_last_bn_zero_gamma, epsilon=bn_eps, name=name and name + "MB_pw_")
 
     if shortcut:
         nn = drop_block(nn, drop_rate, name=name and name + "drop")
-        return keras.layers.Add(name=name and name + "output")([inputs, nn])
+        return layers.Add(name=name and name + "output")([inputs, nn])
     else:
-        return keras.layers.Activation("linear", name=name and name + "output")(nn)  # Identity, Just need a name here
+        return layers.Activation("linear", name=name and name + "output")(nn)  # Identity, Just need a name here
 
 
 def EfficientNetV2(
     expands=[1, 4, 4, 4, 6, 6],
     out_channels=[16, 32, 48, 96, 112, 192],
     depthes=[1, 2, 2, 3, 5, 8],
     strides=[1, 2, 2, 2, 1, 2],
     se_ratios=[0, 0, 0, 0.25, 0.25, 0.25],
     is_fused="auto",  # True if se_ratio == 0 else False
+    use_shortcuts=True,  # Force shortcut usage for each stack
     first_conv_filter=32,
     output_conv_filter=1280,
     kernel_sizes=3,
     input_shape=(None, None, 3),
     num_classes=1000,
     dropout=0.2,
     first_strides=2,
@@ -134,25 +139,28 @@
     rescale_mode="torch",
     kwargs=None,
 ):
     # "torch" for all V1 models
     # for V2 models, "21k" pretrained are all "tf", "imagenet" pretrained "bx" models are all "torch", ["s", "m", "l", "xl"] are "tf"
     rescale_mode = "tf" if pretrained is not None and pretrained.startswith("imagenet21k") else rescale_mode
 
-    inputs = keras.layers.Input(shape=input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(shape=input_shape)
     bn_eps = TORCH_BATCH_NORM_EPSILON if is_torch_mode else TF_BATCH_NORM_EPSILON
 
     if include_preprocessing and rescale_mode == "torch":
-        channel_axis = 1 if K.image_data_format() == "channels_first" else -1
-        Normalization = keras.layers.Normalization if hasattr(keras.layers, "Normalization") else keras.layers.experimental.preprocessing.Normalization
-        mean = tf.constant([0.485, 0.456, 0.406]) * 255.0
-        std = (tf.constant([0.229, 0.224, 0.225]) * 255.0) ** 2
+        channel_axis = 1 if backend.image_data_format() == "channels_first" else -1
+        Normalization = layers.Normalization if hasattr(layers, "Normalization") else layers.experimental.preprocessing.Normalization
+        mean = np.array([0.485, 0.456, 0.406], dtype="float32") * 255.0
+        std = (np.array([0.229, 0.224, 0.225], dtype="float32") * 255.0) ** 2
         nn = Normalization(mean=mean, variance=std, axis=channel_axis)(inputs)
     elif include_preprocessing and rescale_mode == "tf":
-        Rescaling = keras.layers.Rescaling if hasattr(keras.layers, "Rescaling") else keras.layers.experimental.preprocessing.Rescaling
+        Rescaling = layers.Rescaling if hasattr(layers, "Rescaling") else layers.experimental.preprocessing.Rescaling
         nn = Rescaling(scale=1.0 / 128.0, offset=-1)(inputs)
     else:
         nn = inputs
     stem_width = make_divisible(first_conv_filter, 8)
     nn = conv2d_no_bias(nn, stem_width, 3, strides=first_strides, padding="same", use_torch_padding=is_torch_mode, name="stem_")
     nn = batchnorm_with_activation(nn, activation=activation, epsilon=bn_eps, name="stem_")
 
@@ -160,45 +168,49 @@
         "is_torch_mode": is_torch_mode,
         "use_global_context_instead_of_se": use_global_context_instead_of_se,
     }
 
     pre_out = stem_width
     global_block_id = 0
     total_blocks = sum(depthes)
+    se_ratios = se_ratios if isinstance(se_ratios, (list, tuple)) else ([se_ratios] * len(depthes))
     kernel_sizes = kernel_sizes if isinstance(kernel_sizes, (list, tuple)) else ([kernel_sizes] * len(depthes))
     for id, (expand, out_channel, depth, stride, se_ratio, kernel_size) in enumerate(zip(expands, out_channels, depthes, strides, se_ratios, kernel_sizes)):
         out = make_divisible(out_channel, 8)
         if is_fused == "auto":
             cur_is_fused = True if se_ratio == 0 else False
         else:
             cur_is_fused = is_fused[id] if isinstance(is_fused, (list, tuple)) else is_fused
+        cur_use_shortcuts = use_shortcuts[id] if isinstance(use_shortcuts, (list, tuple)) else use_shortcuts
         for block_id in range(depth):
             name = "stack_{}_block{}_".format(id, block_id)
             stride = stride if block_id == 0 else 1
-            shortcut = True if out == pre_out and stride == 1 else False
+            shortcut = cur_use_shortcuts if out == pre_out and stride == 1 else False
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             nn = inverted_residual_block(
                 nn, out, stride, expand, shortcut, kernel_size, block_drop_rate, se_ratio, cur_is_fused, **blocks_kwargs, activation=activation, name=name
             )
             pre_out = out
             global_block_id += 1
 
     if output_conv_filter > 0:
         output_conv_filter = make_divisible(output_conv_filter, 8)
         nn = conv2d_no_bias(nn, output_conv_filter, 1, strides=1, padding="valid", use_torch_padding=is_torch_mode, name="post_")
         nn = batchnorm_with_activation(nn, activation=activation, epsilon=bn_eps, name="post_")
     nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
 
-    model = keras.models.Model(inputs=inputs, outputs=nn, name=model_name)
+    model = models.Model(inputs=inputs, outputs=nn, name=model_name)
     add_pre_post_process(model, rescale_mode="raw" if include_preprocessing else rescale_mode)
     reload_model_weights(model, pretrained)
     return model
 
 
 def reload_model_weights(model, pretrained="imagenet"):
+    import os
+
     if pretrained is None:
         return
     if isinstance(pretrained, str) and pretrained.endswith(".h5"):
         print(">>>> Load pretrained from:", pretrained)
         model.load_weights(pretrained, by_name=True, skip_mismatch=True)
         return
 
@@ -208,125 +220,135 @@
         return
     pre_tt = pretrained_dd[pretrained]
     model_type = model.name.split("_")[-1]
     if model_type not in FILE_HASH_DICT or pre_tt not in FILE_HASH_DICT[model_type]:
         print(">>>> No pretrained available, model will be randomly initialized")
         return
 
-    if model_type.startswith("v1"):
+    if model_type.startswith("v1") or model_type.startswith("edgetpu"):
         pre_url = "https://github.com/leondgarse/keras_efficientnet_v2/releases/download/effnetv1_pretrained/efficientnet{}-{}.h5"
     else:
         pre_url = "https://github.com/leondgarse/keras_efficientnet_v2/releases/download/effnetv2_pretrained/efficientnet{}-{}.h5"
     url = pre_url.format(model_type, pre_tt)
     file_name = os.path.basename(url)
     file_hash = FILE_HASH_DICT[model_type][pre_tt]
 
     try:
-        pretrained_model = keras.utils.get_file(file_name, url, cache_subdir="models/efficientnetv2", file_hash=file_hash)
+        pretrained_model = backend.get_file(file_name, url, cache_subdir="models", file_hash=file_hash)
     except:
         print("[Error] will not load weights, url not found or download failed:", url)
         return
     else:
         print(">>>> Load pretrained from:", pretrained_model)
         model.load_weights(pretrained_model, by_name=True, skip_mismatch=True)
 
 
+@register_model
 def EfficientNetV2B0(input_shape=(224, 224, 3), num_classes=1000, dropout=0.2, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # width 1.0, depth 1.0
     out_channels = [16, 32, 48, 96, 112, 192]
     depthes = [1, 2, 2, 3, 5, 8]
     first_conv_filter = kwargs.pop("first_conv_filter", 32)
     output_conv_filter = kwargs.pop("output_conv_filter", 1280)
     return EfficientNetV2(**locals(), model_name="efficientnet_v2-b0", **kwargs)
 
 
+@register_model
 def EfficientNetV2B1(input_shape=(240, 240, 3), num_classes=1000, dropout=0.2, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # width 1.0, depth 1.1
     out_channels = [16, 32, 48, 96, 112, 192]
     depthes = [2, 3, 3, 4, 6, 9]
     first_conv_filter = kwargs.pop("first_conv_filter", 32)
     output_conv_filter = kwargs.pop("output_conv_filter", 1280)
     return EfficientNetV2(**locals(), model_name="efficientnet_v2-b1", **kwargs)
 
 
+@register_model
 def EfficientNetV2B2(input_shape=(260, 260, 3), num_classes=1000, dropout=0.3, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # width 1.1, depth 1.2
     out_channels = [16, 32, 56, 104, 120, 208]
     depthes = [2, 3, 3, 4, 6, 10]
     first_conv_filter = kwargs.pop("first_conv_filter", 32)
     output_conv_filter = kwargs.pop("output_conv_filter", 1408)
     return EfficientNetV2(**locals(), model_name="efficientnet_v2-b2", **kwargs)
 
 
+@register_model
 def EfficientNetV2B3(input_shape=(300, 300, 3), num_classes=1000, dropout=0.3, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # width 1.2, depth 1.4
     out_channels = [16, 40, 56, 112, 136, 232]
     depthes = [2, 3, 3, 5, 7, 12]
     first_conv_filter = kwargs.pop("first_conv_filter", 40)
     output_conv_filter = kwargs.pop("output_conv_filter", 1536)
     return EfficientNetV2(**locals(), model_name="efficientnet_v2-b3", **kwargs)
 
 
+@register_model
 def EfficientNetV2T(input_shape=(288, 288, 3), num_classes=1000, dropout=0.2, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # width 1.4 * 0.8, depth 1.8 * 0.9, from timm
     is_torch_mode = True
     out_channels = [24, 40, 48, 104, 128, 208]
     depthes = [2, 4, 4, 6, 9, 14]
     first_conv_filter = kwargs.pop("first_conv_filter", 24)
     output_conv_filter = kwargs.pop("output_conv_filter", 1024)
     return EfficientNetV2(**locals(), model_name="efficientnet_v2-t", **kwargs)
 
 
+@register_model
 def EfficientNetV2T_GC(input_shape=(288, 288, 3), num_classes=1000, dropout=0.2, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # width 1.4 * 0.8, depth 1.8 * 0.9, from timm
     is_torch_mode = True
     use_global_context_instead_of_se = True
     out_channels = [24, 40, 48, 104, 128, 208]
     depthes = [2, 4, 4, 6, 9, 14]
     first_conv_filter = kwargs.pop("first_conv_filter", 24)
     output_conv_filter = kwargs.pop("output_conv_filter", 1024)
     return EfficientNetV2(**locals(), model_name="efficientnet_v2-t-gc", **kwargs)
 
 
+@register_model
 def EfficientNetV2S(input_shape=(384, 384, 3), num_classes=1000, dropout=0.2, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # width 1.4, depth 1.8
     out_channels = [24, 48, 64, 128, 160, 256]
     depthes = [2, 4, 4, 6, 9, 15]
     first_conv_filter = kwargs.pop("first_conv_filter", 24)
     output_conv_filter = kwargs.pop("output_conv_filter", 1280)
     rescale_mode = kwargs.pop("rescale_mode", "tf")
     return EfficientNetV2(**locals(), model_name="efficientnet_v2-s", **kwargs)
 
 
+@register_model
 def EfficientNetV2M(input_shape=(480, 480, 3), num_classes=1000, dropout=0.3, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # width 1.6, depth 2.2
     out_channels = [24, 48, 80, 160, 176, 304, 512]
     depthes = [3, 5, 5, 7, 14, 18, 5]
     expands = [1, 4, 4, 4, 6, 6, 6]
     strides = [1, 2, 2, 2, 1, 2, 1]
     se_ratios = [0, 0, 0, 0.25, 0.25, 0.25, 0.25]
     first_conv_filter = kwargs.pop("first_conv_filter", 24)
     output_conv_filter = kwargs.pop("output_conv_filter", 1280)
     rescale_mode = kwargs.pop("rescale_mode", "tf")
     return EfficientNetV2(**locals(), model_name="efficientnet_v2-m", **kwargs)
 
 
+@register_model
 def EfficientNetV2L(input_shape=(480, 480, 3), num_classes=1000, dropout=0.4, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # width 1.6, depth 2.2
     out_channels = [32, 64, 96, 192, 224, 384, 640]
     depthes = [4, 7, 7, 10, 19, 25, 7]
     expands = [1, 4, 4, 4, 6, 6, 6]
     strides = [1, 2, 2, 2, 1, 2, 1]
     se_ratios = [0, 0, 0, 0.25, 0.25, 0.25, 0.25]
     first_conv_filter = kwargs.pop("first_conv_filter", 32)
     output_conv_filter = kwargs.pop("output_conv_filter", 1280)
     rescale_mode = kwargs.pop("rescale_mode", "tf")
     return EfficientNetV2(**locals(), model_name="efficientnet_v2-l", **kwargs)
 
 
+@register_model
 def EfficientNetV2XL(input_shape=(512, 512, 3), num_classes=1000, dropout=0.4, classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
     out_channels = [32, 64, 96, 192, 256, 512, 640]
     depthes = [4, 8, 8, 16, 24, 32, 8]
     expands = [1, 4, 4, 4, 6, 6, 6]
     strides = [1, 2, 2, 2, 1, 2, 1]
     se_ratios = [0, 0, 0, 0.25, 0.25, 0.25, 0.25]
     first_conv_filter = kwargs.pop("first_conv_filter", 32)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/gcvit/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/gcvit/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from keras_cv_attention_models.gcvit.gcvit import GCViT, GCViT_XXTiny, GCViT_XTiny, GCViT_Tiny, GCViT_Small, GCViT_Base
+from keras_cv_attention_models.gcvit.gcvit import GCViT, GCViT_XXTiny, GCViT_XTiny, GCViT_Tiny, GCViT_Tiny2, GCViT_Small, GCViT_Small2, GCViT_Base, GCViT_Large
 
 __head_doc__ = """
 Keras implementation of [Github NVlabs/GCVit](https://github.com/NVlabs/GCVit).
 Paper [PDF 2206.09959 Global Context Vision Transformers](https://arxiv.org/pdf/2206.09959.pdf).
 """
 
 __tail_doc__ = """  window_ratios: window split ratio. Each stack will calculate `window_size = (height // window_ratio, width // window_ratio)` .
@@ -14,15 +14,15 @@
   drop_connect_rate: is used for [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382).
       Can be value like `0.2`, indicates the drop probability linearly changes from `0 --> 0.2` for `top --> bottom` layers.
       A higher value means a higher probability will drop the deep branch.
       or `0` to disable (default).
   dropout: dropout rate if top layers is included.
   classifier_activation: A `str` or callable. The activation function to use on the "top" layer if `num_classes > 0`.
       Set `classifier_activation=None` to return the logits of the "top" layer.
-  pretrained: one of None or "imagenet".
+  pretrained: None or one of ["imagenet", "imagenet21k-ft1k"].
       Will try to download and load pre-trained model weights if not None.
 
 Returns:
     A `keras.Model` instance.
 """
 
 GCViT.__doc__ = __head_doc__ + """
@@ -30,24 +30,33 @@
   num_blocks: number of blocks in each stack.
   num_heads: num heads for each stack.
   embed_dim: basic hidden dims, expand * 2 for each stack.
   mlp_ratio: expand ratio for mlp blocks hidden channel.
   model_name: string, model name.
 """ + __tail_doc__ + """
 Model architectures:
-  | Model        | Params | FLOPs | Input | Top1 Acc |
-  | ------------ | ------ | ----- | ----- | -------- |
-  | GCViT_XXTiny | 12.0M  | 2.15G | 224   | 79.8     |
-  | GCViT_XTiny  | 20.0M  | 2.96G | 224   | 82.04    |
-  | GCViT_Tiny   | 28.2M  | 4.83G | 224   | 83.4     |
-  | GCViT_Small  | 51.1M  | 8.63G | 224   | 83.95    |
-  | GCViT_Base   | 90.3M  | 14.9G | 224   | 84.47    |
+  | Model           | Params | FLOPs  | Input | Top1 Acc |
+  | --------------- | ------ | ------ | ----- | -------- |
+  | GCViT_XXTiny    | 12.0M  | 2.15G  | 224   | 79.9     |
+  | GCViT_XTiny     | 20.0M  | 2.96G  | 224   | 82.0     |
+  | GCViT_Tiny      | 28.2M  | 4.83G  | 224   | 83.5     |
+  | GCViT_Tiny2     | 34.5M  | 6.28G  | 224   | 83.7     |
+  | GCViT_Small     | 51.1M  | 8.63G  | 224   | 84.3     |
+  | GCViT_Small2    | 68.6M  | 11.7G  | 224   | 84.8     |
+  | GCViT_Base      | 90.3M  | 14.9G  | 224   | 85.0     |
+  | GCViT_Large     | 202.1M | 32.8G  | 224   | 85.7     |
+  | - 21k_ft1k      | 202.1M | 32.8G  | 224   | 86.6     |
+  | - 21k_ft1k, 384 | 202.9M | 105.1G | 384   | 87.4     |
+  | - 21k_ft1k, 512 | 203.8M | 205.1G | 512   | 87.6     |
 """
 
 GCViT_XXTiny.__doc__ = __head_doc__ + """
 Args:
 """ + __tail_doc__
 
 GCViT_XTiny.__doc__ = GCViT_XXTiny.__doc__
 GCViT_Tiny.__doc__ = GCViT_XXTiny.__doc__
+GCViT_Tiny2.__doc__ = GCViT_XXTiny.__doc__
 GCViT_Small.__doc__ = GCViT_XXTiny.__doc__
+GCViT_Small2.__doc__ = GCViT_XXTiny.__doc__
 GCViT_Base.__doc__ = GCViT_XXTiny.__doc__
+GCViT_Large.__doc__ = GCViT_XXTiny.__doc__
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/gcvit/gcvit.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/moganet/moganet.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,170 +1,168 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
-    ChannelAffine,
-    MultiHeadRelativePositionalEmbedding,
     activation_by_name,
+    add_with_layer_scale_and_drop_block,
+    batchnorm_with_activation,
     conv2d_no_bias,
+    ChannelAffine,
     depthwise_conv2d_no_bias,
-    drop_block,
-    layer_norm,
-    mhsa_with_multi_head_relative_position_embedding,
-    mlp_block,
     output_block,
-    se_module,
-    window_attention,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
-    "gcvit_xx_tiny": {"imagenet": {224: "ff516a5a1d3dfdda0c0b2e0051206c00"}},
-    "gcvit_x_tiny": {"imagenet": {224: "723155237e083716bb3df904c80711c4"}},
-    "gcvit_tiny": {"imagenet": {224: "0e6ecf576b649f7077f4f2f8122b420e"}},
-    "gcvit_small": {"imagenet": {224: "ac0cfb4240ae85a40a88691c2329edab"}},
-    "gcvit_base": {"imagenet": {224: "ef6e4015239f68dcabbb8ae9cb799d76"}},
+    "moganet_xtiny": {"imagenet": "b1a7a1b77777cd8fdc8b5b5333e49215"},
+    "moganet_tiny": {"imagenet": {224: "2ecd6f4552fb5bdbd0845c6a49bb67a9", 256: "6be0f1b79d00ba535412c1d1d3c1a71f"}},
+    "moganet_small": {"imagenet": "8b090a8058304bdbacdc70896ecb25cd"},
+    "moganet_base": {"imagenet": "3aa0d6fccc312bb7674495b7c0c8153e"},
+    "moganet_large": {"imagenet": "1714094064c837afa3c6f0304563b496"},
 }
 
 
-def gcvit_block(inputs, window_size, num_heads=4, global_query=None, mlp_ratio=4, layer_scale=0, drop_rate=0, activation="gelu", name=""):
-    # print(global_query)
-    input_channel = inputs.shape[-1]
-    attn = layer_norm(inputs, name=name + "attn_")
-    attention_block = lambda inputs, num_heads, name: mhsa_with_multi_head_relative_position_embedding(
-        inputs, num_heads=num_heads, global_query=global_query, qkv_bias=True, out_bias=True, name=name
-    )
-    attn = window_attention(attn, window_size=window_size, num_heads=num_heads, attention_block=attention_block, name=name + "window_mhsa_")
-    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "1_gamma")(attn) if layer_scale >= 0 else attn
-    attn = drop_block(attn, drop_rate=drop_rate, name=name + "attn_")
-    attn_out = keras.layers.Add(name=name + "attn_out")([inputs, attn])
-
-    mlp = layer_norm(attn_out, name=name + "mlp_")
-    mlp = mlp_block(mlp, int(input_channel * mlp_ratio), use_conv=False, activation=activation, name=name + "mlp_")
-    mlp = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "2_gamma")(mlp) if layer_scale >= 0 else mlp
-    mlp = drop_block(mlp, drop_rate=drop_rate, name=name + "mlp_")
-    return keras.layers.Add(name=name + "output")([attn_out, mlp])
-
-
-def to_global_query(inputs, window_ratio, num_heads=4, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
-    query = inputs
-    num_window = 1
-    if window_ratio == 1:
-        query = extract_feature(query, strides=1, activation=activation, name=name + "down1_")
+def feature_decompose(inputs, use_pool=False, activation="gelu", name=""):
+    channel_axis = -1 if backend.image_data_format() == "channels_last" else 1
+    if use_pool:
+        decomposed = layers.GlobalAveragePooling2D(keepdims=True, name=name + "pool")(inputs)
     else:
-        while num_window < window_ratio:
-            num_window *= 2
-            query = extract_feature(query, strides=2, activation=activation, name=name + "down{}_".format(num_window))
-
-    # print(f"{inputs.shape = }, {query.shape = }, {num_window = }, {window_ratio = }")
-    query = tf.reshape(query, [-1, query.shape[1] * query.shape[2], num_heads, input_channel // num_heads])
-    query = tf.transpose(query, [0, 2, 1, 3])
-    query = tf.repeat(query, num_window * num_window, axis=0)
-    # print(f"{query.shape = }")
-    return query
-
-
-def down_sample(inputs, out_channels=-1, activation="gelu", name=""):
-    out_channels = out_channels if out_channels > 0 else inputs.shape[-1]
-    nn = layer_norm(inputs, name=name + "down_1_")
-    nn = extract_feature(nn, strides=1, activation=activation, name=name + "down_")
-    nn = conv2d_no_bias(nn, out_channels, kernel_size=3, strides=2, padding="same", name=name + "down_")
-    nn = layer_norm(nn, name=name + "down_2_")
+        decomposed = conv2d_no_bias(inputs, 1, use_bias=True, name=name)
+    decomposed = activation_by_name(decomposed, activation=activation, name=name)
+    decomposed = ChannelAffine(use_bias=False, weight_init_value=1e-5, axis=channel_axis, name=name + "affine")(inputs - decomposed)
+    return inputs + decomposed
+
+
+def multi_order_depthwise_conv2d(inputs, dilations=[1, 2, 3], channel_split=[1, 3, 4], name=""):
+    channel_axis = -1 if backend.image_data_format() == "channels_last" else 1
+    input_channel = inputs.shape[channel_axis]
+    channel_split = [-1] + [int(input_channel * ii / sum(channel_split)) for ii in channel_split[1:]]
+    paddings = [(1 + 4 * dilations[0]) // 2, (1 + 4 * dilations[1]) // 2, (1 + 6 * dilations[2]) // 2]
+
+    # print(f"{inputs.shape = }, {input_channel = }, {channel_split = }")
+    first = depthwise_conv2d_no_bias(inputs, kernel_size=5, padding=paddings[0], use_bias=True, dilation_rate=dilations[0], name=name + "first_")
+    first, second, third = functional.split(first, channel_split, axis=channel_axis)
+    second = depthwise_conv2d_no_bias(second, kernel_size=5, padding=paddings[1], use_bias=True, dilation_rate=dilations[1], name=name + "second_")
+    third = depthwise_conv2d_no_bias(third, kernel_size=7, padding=paddings[2], use_bias=True, dilation_rate=dilations[2], name=name + "third_")
+    # print(f"{first.shape = }, {second.shape = }, {third.shape = }")
+
+    out = functional.concat([first, second, third], axis=channel_axis)
+    out = conv2d_no_bias(out, input_channel, use_bias=True, name=name + "out_")
+    return out
+
+
+def moga_block(inputs, mlp_ratio=4, layer_scale=0, drop_rate=0, attn_activation="swish", activation="gelu", name=""):
+    input_channel = inputs.shape[-1 if backend.image_data_format() == "channels_last" else 1]
+
+    """ attention swish """
+    pre_norm = batchnorm_with_activation(inputs, activation=None, name=name + "pre_attn_")
+    nn = conv2d_no_bias(pre_norm, input_channel, use_bias=True, name=name + "pre_attn_")  # proj_1
+    nn = feature_decompose(nn, use_pool=True, activation=None, name=name + "pre_attn_decompose_")
+    nn = activation_by_name(nn, activation=attn_activation, name=name + "pre_attn_")
+
+    gate = conv2d_no_bias(nn, input_channel, use_bias=True, name=name + "attn_gate_")
+    gate = activation_by_name(gate, activation=attn_activation, name=name + "attn_gate_")
+    value = multi_order_depthwise_conv2d(nn, name=name + "attn_value_")  # MultiOrderDWConv
+    value = activation_by_name(value, activation=attn_activation, name=name + "attn_value_")
+    gate_value = layers.Multiply(name=name + "attn_gate_value")([gate, value])
+    gate_value = conv2d_no_bias(gate_value, input_channel, use_bias=True, name=name + "attn_gate_value_")  # proj_2
+    nn = layers.Add(name=name + "attn_out")([pre_norm, gate_value])
+
+    attn_out = add_with_layer_scale_and_drop_block(inputs, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "attn_")
+
+    """ MLP proj 1 """
+    nn = batchnorm_with_activation(attn_out, activation=None, name=name + "mlp_")
+    nn = conv2d_no_bias(nn, input_channel * mlp_ratio, use_bias=True, name=name + "mlp_1_")
+    nn = depthwise_conv2d_no_bias(nn, kernel_size=3, padding="same", use_bias=True, name=name + "mlp_2_")
+    nn = activation_by_name(nn, activation=activation, name=name + "mlp_")
+    # drop
+
+    """ MLP proj 2 """
+    nn = feature_decompose(nn, activation=activation, name=name + "mlp_decompose_")
+    nn = conv2d_no_bias(nn, input_channel, use_bias=True, name=name + "mlp_3_")
+    nn = add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "mlp_")
     return nn
 
 
-def extract_feature(inputs, strides=2, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
-    nn = depthwise_conv2d_no_bias(inputs, kernel_size=3, padding="same", name=name + "extract_")
-    nn = activation_by_name(nn, activation=activation, name=name + "extract_")
-    nn = se_module(nn, divisor=1, use_bias=False, activation=activation, use_conv=False, name=name + "extract_se_")
-    nn = conv2d_no_bias(nn, input_channel, kernel_size=1, name=name + "extract_")
-    nn = inputs + nn
-    return keras.layers.MaxPool2D(pool_size=3, strides=strides, padding="SAME", name=name + "extract_maxpool")(nn) if strides > 1 else nn
-    # if strides > 1:
-    #     nn = tf.pad(nn, [[0, 0], [1, 1], [1, 1], [0, 0]])
-    #     nn = keras.layers.MaxPool2D(pool_size=3, strides=strides, padding="VALID", name=name + "extract_maxpool")(nn)
-    # return nn
-
-
-def GCViT(
-    num_blocks=[2, 2, 6, 2],
-    num_heads=[2, 4, 8, 16],
-    # window_size=[7, 7, 14, 7],
-    window_ratios=[8, 4, 1, 1],
-    embed_dim=64,
-    mlp_ratio=3,
-    layer_scale=-1,
+def MogaNet(
+    num_blocks=[3, 3, 10, 2],
+    out_channels=[32, 64, 96, 192],
+    mlp_ratios=[8, 8, 4, 4],
     input_shape=(224, 224, 3),
     num_classes=1000,
     activation="gelu",
+    attn_activation="swish",
     drop_connect_rate=0,
-    classifier_activation="softmax",
     dropout=0,
+    layer_scale=1e-5,
+    classifier_activation="softmax",
     pretrained=None,
-    model_name="gcvit",
+    model_name="moganet",
     kwargs=None,
 ):
-    """Patch stem"""
-    inputs = keras.layers.Input(input_shape)
-    nn = conv2d_no_bias(inputs, embed_dim, kernel_size=3, strides=2, use_bias=True, padding="SAME", name="stem_conv")
-    nn = down_sample(nn, name="stem_")
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+
+    """ Stem """
+    nn = conv2d_no_bias(inputs, out_channels[0] // 2, kernel_size=3, strides=2, padding="same", use_bias=True, name="stem_1_")
+    nn = batchnorm_with_activation(nn, activation=activation, name="stem_1_")
+    nn = conv2d_no_bias(nn, out_channels[0], kernel_size=3, strides=2, padding="same", use_bias=True, name="stem_2_")
+    nn = batchnorm_with_activation(nn, activation=None, name="stem_2_")
 
-    """ stages """
+    """ stacks """
     total_blocks = sum(num_blocks)
     global_block_id = 0
-    num_stacks = len(num_blocks)
-    for stack_id, (num_block, num_head, window_ratio) in enumerate(zip(num_blocks, num_heads, window_ratios)):
+    for stack_id, (num_block, out_channel) in enumerate(zip(num_blocks, out_channels)):
         stack_name = "stack{}_".format(stack_id + 1)
         if stack_id > 0:
-            nn = down_sample(nn, out_channels=nn.shape[-1] * 2, name=stack_name)
-
-        window_size = (nn.shape[1] // window_ratio, nn.shape[2] // window_ratio)
-        # window_size = (int(tf.math.ceil(nn.shape[1] / window_ratio), int(tf.math.ceil(nn.shape[2] / window_ratio))
-        global_query = to_global_query(nn, window_ratio, num_head, activation=activation, name=stack_name + "q_global_")
+            nn = conv2d_no_bias(nn, out_channel, 3, strides=2, padding="same", use_bias=True, name=stack_name + "downsample_")
+            nn = batchnorm_with_activation(nn, activation=None, name=stack_name + "downsample_")
 
+        mlp_ratio = mlp_ratios[stack_id] if isinstance(mlp_ratios, (list, tuple)) else mlp_ratios
         for block_id in range(num_block):
-            block_name = stack_name + "block{}_".format(block_id + 1)
+            name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
-            cur_global_query = None if block_id % 2 == 0 else global_query
-            nn = gcvit_block(nn, window_size, num_head, cur_global_query, mlp_ratio, layer_scale, block_drop_rate, activation=activation, name=block_name)
+            nn = moga_block(nn, mlp_ratio, layer_scale, block_drop_rate, attn_activation=attn_activation, activation=activation, name=name)
             global_block_id += 1
-    nn = layer_norm(nn, name="pre_output_")
+        nn = batchnorm_with_activation(nn, activation=None, name=stack_name + "output_")
 
     nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
-    reload_model_weights(model, PRETRAINED_DICT, "gcvit", pretrained, MultiHeadRelativePositionalEmbedding)
+    reload_model_weights(model, PRETRAINED_DICT, "moganet", pretrained)
     return model
 
 
-def GCViT_XXTiny(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained=None, **kwargs):
-    return GCViT(**locals(), model_name="gcvit_xx_tiny", **kwargs)
+@register_model
+def MogaNetXtiny(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    return MogaNet(**locals(), model_name="moganet_xtiny", **kwargs)
 
 
-def GCViT_XTiny(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained=None, **kwargs):
-    num_blocks = [3, 4, 6, 5]
-    return GCViT(**locals(), model_name="gcvit_x_tiny", **kwargs)
+@register_model
+def MogaNetTiny(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [3, 3, 12, 2]
+    out_channels = [32, 64, 128, 256]
+    return MogaNet(**locals(), model_name="moganet_tiny", **kwargs)
 
 
-def GCViT_Tiny(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 4, 19, 5]
-    return GCViT(**locals(), model_name="gcvit_tiny", **kwargs)
+@register_model
+def MogaNetSmall(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [2, 3, 12, 2]
+    out_channels = [64, 128, 320, 512]
+    return MogaNet(**locals(), model_name="moganet_small", **kwargs)
 
 
-def GCViT_Small(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 4, 19, 5]
-    num_heads = [3, 6, 12, 24]
-    embed_dim = 96
-    mlp_ratio = 2
-    layer_scale = 1e-5
-    return GCViT(**locals(), model_name="gcvit_small", **kwargs)
+@register_model
+def MogaNetBase(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [4, 6, 22, 3]
+    out_channels = [64, 160, 320, 512]
+    return MogaNet(**locals(), model_name="moganet_base", **kwargs)
 
 
-def GCViT_Base(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 4, 19, 5]
-    num_heads = [4, 8, 16, 32]
-    embed_dim = 128
-    mlp_ratio = 2
-    layer_scale = 1e-5
-    return GCViT(**locals(), model_name="gcvit_base", **kwargs)
+@register_model
+def MogaNetLarge(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [4, 6, 44, 4]
+    out_channels = [64, 160, 320, 640]
+    return MogaNet(**locals(), model_name="moganet_large", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/ghostnet/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/ghostnet/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from keras_cv_attention_models.ghostnet.ghostnet_v2 import GhostNetV2, GhostNetV2_100
+from keras_cv_attention_models.ghostnet.ghostnet_v2 import GhostNetV2, GhostNetV2_100, GhostNetV2_130, GhostNetV2_160
 from keras_cv_attention_models.ghostnet.ghostnet import GhostNet, GhostNet_050, GhostNet_100, GhostNet_130
 
 __v2_head_doc__ = """
 Keras implementation of [Gitee mindspore/models/ghostnetv2](https://gitee.com/mindspore/models/tree/master/research/cv/ghostnetv2).
 Paper [PDF GhostNetV2: Enhance Cheap Operation with Long-Range Attention](https://openreview.net/pdf/6db544c65bbd0fa7d7349508454a433c112470e2.pdf).
 """
 
@@ -43,14 +43,17 @@
   | GhostNetV2 (1.6x) | 12.39M | 400.9M | 224   | 77.8     |
 """
 
 GhostNetV2_100.__doc__ = __v2_head_doc__ + """
 Args:
 """ + __tail_doc__
 
+GhostNetV2_130.__doc__ = GhostNetV2_100.__doc__
+GhostNetV2_160.__doc__ = GhostNetV2_100.__doc__
+
 __v1_head_doc__ = """
 Keras implementation of [Github huawei-noah/ghostnet_pytorch](https://github.com/huawei-noah/Efficient-AI-Backbones/tree/master/ghostnet_pytorch).
 Paper [PDF 1911.11907 GhostNet: More Features from Cheap Operations](https://arxiv.org/pdf/1911.11907.pdf).
 """
 
 GhostNet.__doc__ = __v1_head_doc__ + """
 Args:
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/ghostnet/ghostnet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/ghostnet/ghostnet.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from keras_cv_attention_models.ghostnet.ghostnet_v2 import GhostNetV2
+from keras_cv_attention_models.models import register_model
 
 
 def GhostNet(
     kernel_sizes=[3, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5],
     first_ghost_channels=[16, 48, 72, 72, 120, 240, 200, 184, 184, 480, 672, 672, 960, 960, 960, 960],
     out_channels=[16, 24, 24, 40, 40, 80, 80, 80, 80, 112, 112, 160, 160, 160, 160, 160],
     se_ratios=[0, 0, 0, 0.25, 0.25, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0, 0.25, 0, 0.25],
@@ -20,17 +21,20 @@
     pretrained=None,
     model_name="ghostnet",
     kwargs=None,
 ):
     return GhostNetV2(**locals())
 
 
+@register_model
 def GhostNet_050(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return GhostNet(**locals(), width_mul=0.5, model_name="ghostnet_050", **kwargs)
 
 
+@register_model
 def GhostNet_100(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return GhostNet(**locals(), model_name="ghostnet_100", **kwargs)
 
 
+@register_model
 def GhostNet_130(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return GhostNet(**locals(), width_mul=1.3, model_name="ghostnet_130", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/ghostnet/ghostnet_v2.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/ghostnet/ghostnet_v2.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,64 +1,68 @@
-import tensorflow as tf
-from tensorflow import keras
+import math
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     batchnorm_with_activation,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
     make_divisible,
     se_module,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
-    "ghostnetv2_100": {"imagenet": "4f28597d5f72731ed4ef4f69ec9c1799"},
+    "ghostnetv2_100": {"imagenet": "d9da39a786811d6c7755396cead539e6"},
+    "ghostnetv2_130": {"imagenet": "e69d07ba05ac171eb0c9e28b6348c477"},
+    "ghostnetv2_160": {"imagenet": "641c7a0ba319bf38261c50e3c9cd280d"},
     "ghostnet_050": {"imagenet": "dbb5a89e19fa78f2f35f38b4c1ae4351"},
     "ghostnet_100": {"imagenet": "19a0f0f03f20e4bd6c1736102b4d979d"},
     "ghostnet_130": {"imagenet": "3a73bc721765c516a894b567674fc60b", "ssld": "62571bb90d71a7487679ae97642d13fb"},
 }
 
 
 def decoupled_fully_connected_attention_block(inputs, out_channel, name=""):
-    # nn = keras.layers.AvgPool2D(pool_size=2, strides=2, padding="SAME")(inputs)
-    nn = keras.layers.AvgPool2D(pool_size=2, strides=2)(inputs)
+    # nn = layers.AvgPool2D(pool_size=2, strides=2, padding="same")(inputs)
+    nn = layers.AvgPool2D(pool_size=2, strides=2)(inputs)
     nn = conv2d_no_bias(nn, out_channel, name=name + "1_")
     nn = batchnorm_with_activation(nn, activation=None, name=name + "1_")
-    nn = depthwise_conv2d_no_bias(nn, (1, 5), padding="SAME", name=name + "2_")
+    nn = depthwise_conv2d_no_bias(nn, (1, 5), padding="same", name=name + "2_")
     nn = batchnorm_with_activation(nn, activation=None, name=name + "2_")
-    nn = depthwise_conv2d_no_bias(nn, (5, 1), padding="SAME", name=name + "3_")
+    nn = depthwise_conv2d_no_bias(nn, (5, 1), padding="same", name=name + "3_")
     nn = batchnorm_with_activation(nn, activation=None, name=name + "3_")
     nn = activation_by_name(nn, "sigmoid", name=name)
     # print(f"{nn.shape = }, {nn.shape = }")
-    nn = tf.image.resize(nn, tf.shape(inputs)[1:-1], antialias=False, method="bilinear")
-    # nn = keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(nn)
+    size = functional.shape(inputs)[1:-1] if image_data_format() == "channels_last" else functional.shape(inputs)[2:]  # For dynamic shape
+    nn = functional.resize(nn, size, antialias=False, method="nearest")
+    # nn = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(nn)
     # if int(nn.shape[1] - nn.shape[1]) > 0 or int(nn.shape[2] - nn.shape[2]) > 0:
     #     nn = nn[:, :nn.shape[1], :nn.shape[2]]
-    # nn = keras.layers.Resizing(nn.shape[1], nn.shape[2])(nn)
+    # nn = layers.Resizing(nn.shape[1], nn.shape[2])(nn)
     return nn
 
 
 def ghost_module(inputs, out_channel, use_dfc_block=False, activation="relu", name=""):
     ratio = 2
-    hidden_channel = int(tf.math.ceil(float(out_channel) / ratio))
-    in_channels = inputs.shape[-1]
+    hidden_channel = int(math.ceil(float(out_channel) / ratio))
     # print("[ghost_module] out_channel:", out_channel, "conv_out_channel:", conv_out_channel)
     primary_conv = conv2d_no_bias(inputs, hidden_channel, name=name + "prim_")
     primary_conv = batchnorm_with_activation(primary_conv, activation=activation, name=name + "prim_")
 
     # hidden_channel_cheap = int(out_channel - hidden_channel_prim)
-    # cheap_conv = conv2d_no_bias(primary_conv, hidden_channel_cheap, kernel_size=3, padding="SAME", groups=hidden_channel_prim, name=name + "cheap_")
-    cheap_conv = depthwise_conv2d_no_bias(primary_conv, kernel_size=3, padding="SAME", name=name + "cheap_")
+    # cheap_conv = conv2d_no_bias(primary_conv, hidden_channel_cheap, kernel_size=3, padding="same", groups=hidden_channel_prim, name=name + "cheap_")
+    cheap_conv = depthwise_conv2d_no_bias(primary_conv, kernel_size=3, padding="same", name=name + "cheap_")
     cheap_conv = batchnorm_with_activation(cheap_conv, activation=activation, name=name + "cheap_")
-    ghost_out = keras.layers.Concatenate()([primary_conv, cheap_conv])
+    ghost_out = layers.Concatenate()([primary_conv, cheap_conv])
 
     if use_dfc_block:
         shortcut = decoupled_fully_connected_attention_block(inputs, out_channel, name=name + "short_")
-        ghost_out = keras.layers.Multiply()([shortcut, ghost_out])
+        ghost_out = layers.Multiply()([shortcut, ghost_out])
     return ghost_out
 
 
 def ghost_bottleneck(
     inputs, out_channel, first_ghost_channel, kernel_size=3, strides=1, se_ratio=0, shortcut=True, use_dfc_block=False, activation="relu", name=""
 ):
     if shortcut:
@@ -76,15 +80,15 @@
         nn = batchnorm_with_activation(nn, activation=None, name=name + "down_")
 
     if se_ratio > 0:
         nn = se_module(nn, se_ratio=se_ratio, divisor=4, activation=(activation, "hard_sigmoid_torch"), name=name + "se_")
 
     nn = ghost_module(nn, out_channel, activation=None, name=name + "ghost_2_")
     # print(f">>>> {strides = }, {inputs.shape = }, {shortcut.shape = }, {nn.shape = }")
-    return keras.layers.Add(name=name + "output")([shortcut, nn])
+    return layers.Add(name=name + "output")([shortcut, nn])
 
 
 def GhostNetV2(
     kernel_sizes=[3, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5],
     first_ghost_channels=[16, 48, 72, 72, 120, 240, 200, 184, 184, 480, 672, 672, 960, 960, 960, 960],
     out_channels=[16, 24, 24, 40, 40, 80, 80, 80, 80, 112, 112, 160, 160, 160, 160, 160],
     se_ratios=[0, 0, 0, 0.25, 0.25, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0, 0.25, 0, 0.25],
@@ -99,42 +103,58 @@
     activation="relu",
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="ghostnetv2",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+
     stem_width = make_divisible(stem_width * width_mul, divisor=4)
     nn = conv2d_no_bias(inputs, stem_width, 3, strides=stem_strides, padding="same", name="stem_")
     nn = batchnorm_with_activation(nn, activation=activation, name="stem_")
 
     """ stages """
     for stack_id, (kernel, stride, first_ghost, out_channel, se_ratio) in enumerate(zip(kernel_sizes, strides, first_ghost_channels, out_channels, se_ratios)):
         stack_name = "stack{}_".format(stack_id + 1)
         out_channel = make_divisible(out_channel * width_mul, 4)
         first_ghost_channel = make_divisible(first_ghost * width_mul, 4)
-        shortcut = False if out_channel == nn.shape[-1] and stride == 1 else True
+        shortcut = False if out_channel == nn.shape[channel_axis] and stride == 1 else True
         use_dfc_block = True if num_ghost_module_v1_stacks >= 0 and stack_id >= num_ghost_module_v1_stacks else False
         nn = ghost_bottleneck(nn, out_channel, first_ghost_channel, kernel, stride, se_ratio, shortcut, use_dfc_block, activation=activation, name=stack_name)
 
     output_conv_filter = output_conv_filter if output_conv_filter > 0 else make_divisible(first_ghost_channels[-1] * width_mul, 4)
     nn = conv2d_no_bias(nn, output_conv_filter, 1, strides=1, name="pre_")
     nn = batchnorm_with_activation(nn, activation=activation, name="pre_")
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(keepdims=True)(nn)
+        nn = layers.GlobalAveragePooling2D(keepdims=True)(nn)
         nn = conv2d_no_bias(nn, 1280, 1, strides=1, use_bias=True, name="features_")
         nn = activation_by_name(nn, activation, name="features_")
-        nn = keras.layers.Flatten()(nn)
+        nn = layers.Flatten()(nn)
         if dropout > 0 and dropout < 1:
-            nn = keras.layers.Dropout(dropout)(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(nn)
+            nn = layers.Dropout(dropout)(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(nn)
 
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "ghostnetv2", pretrained)
     return model
 
 
+@register_model
 def GhostNetV2_100(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return GhostNetV2(**locals(), model_name="ghostnetv2_100", **kwargs)
+
+
+@register_model
+def GhostNetV2_130(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    return GhostNetV2(**locals(), width_mul=1.3, model_name="ghostnetv2_130", **kwargs)
+
+
+@register_model
+def GhostNetV2_160(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    return GhostNetV2(**locals(), width_mul=1.6, model_name="ghostnetv2_160", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/gpvit/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/gpvit/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/gpvit/gpvit.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/gpvit/gpvit.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,19 +1,21 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, initializers, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     add_with_layer_scale_and_drop_block,
     batchnorm_with_activation,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
     layer_norm,
     mlp_block,
     mlp_mixer_block,
     output_block,
     PositionalEmbedding,
+    qkv_to_multi_head_channels_last_format,
     scaled_dot_product_attention,
     window_attention,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 LAYER_NORM_EPSILON = 1e-6
@@ -22,151 +24,157 @@
     "gpvit_l1": {"imagenet": {224: "4c2b124acdb20cc9ef33a32a85a2cd4e"}},
     "gpvit_l2": {"imagenet": {224: "ce5e72b80cfcb9a9567e8e52d42b4e15"}},
     "gpvit_l3": {"imagenet": {224: "a378001b60878ea8851ebe78a28bcfbe"}},
     "gpvit_l4": {"imagenet": {224: "0cde8fcea39794ea0ce1ffdf7c49eef0"}},
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="kecam")
-class PureWeigths(keras.layers.Layer):
+@backend.register_keras_serializable(package="kecam")
+class PureWeigths(layers.Layer):
     """Just return a weights with specific shape"""
 
-    def __init__(self, shape, **kwargs):
+    def __init__(self, shape, initializer="glorot_uniform", **kwargs):
         super().__init__(**kwargs)
-        self.shape = shape
+        self.shape, self.initializer = shape, initializer
 
     def build(self, input_shape):
-        self.gain = self.add_weight(name="gain", shape=self.shape, dtype=self.dtype, trainable=True)
+        self.gain = self.add_weight(name="gain", initializer=self.initializer, shape=self.shape, trainable=True)
+        super().build(input_shape)
 
     def call(self, inputs, **kwargs):
         return self.gain
 
     def get_config(self):
         config = super().get_config()
         config.update({"shape": self.shape})
         return config
 
+    def compute_output_shape(self, input_shape):
+        return self.shape
+
 
 def lepe_attention(inputs, num_heads=4, dropout=0, name=""):
-    query, key, value = tf.split(inputs, 3, axis=-1)
+    query, key, value = functional.split(inputs, 3, axis=-1)
     _, hh, ww, input_channel = query.shape
     blocks = hh * ww
     key_dim = input_channel // num_heads
     # print(f"{input_channel = }, {num_heads = }, {key_dim = }")
 
-    lepe = depthwise_conv2d_no_bias(value, kernel_size=3, use_bias=True, padding="same", name=name + "lepe_")
+    lepe = value if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(value)  # channels_last -> channels_first
+    lepe = depthwise_conv2d_no_bias(lepe, kernel_size=3, use_bias=True, padding="same", name=name + "lepe_")
+    lepe = lepe if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(lepe)  # channels_first -> channels_last
 
-    query = tf.transpose(tf.reshape(query, [-1, blocks, num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
-    key = tf.transpose(tf.reshape(key, [-1, blocks, num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-    value = tf.transpose(tf.reshape(value, [-1, blocks, num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
+    query, key, value = qkv_to_multi_head_channels_last_format(query, key, value, num_heads=num_heads, data_format="channels_last")
     # print(f"{query.shape = }, {key.shape = }, {value.shape = }, {lepe.shape = }")
 
     output_shape = (hh, ww, input_channel)
     attn_out = scaled_dot_product_attention(query, key, value, output_shape=output_shape, out_weight=False, dropout=dropout, name=name)
-    return keras.layers.Add()([attn_out, lepe])
+    return layers.Add()([attn_out, lepe])
 
 
 def window_lepe_attention(inputs, num_heads=4, window_size=2, key_dim=0, qkv_bias=True, dropout=0, name=""):
     _, hh, ww, input_channel = inputs.shape
     key_dim = key_dim if key_dim > 0 else input_channel // num_heads
     # out_shape = input_channel if out_shape is None or not out_weight else out_shape
     emb_dim = num_heads * key_dim
 
     # qkv = self.qkv(img).reshape(B, -1, 3, C).permute(2, 0, 1, 3).contiguous()
     # split 3 -> split 2
-    qkv = keras.layers.Dense(emb_dim * 3, use_bias=qkv_bias, name=name and name + "qkv")(inputs)
-    qkv = tf.reshape(qkv, [-1, hh * ww * 3, emb_dim])
-    qkv_left, qkv_right = tf.split(qkv, 2, axis=-1)
-    qkv_left = tf.reshape(qkv_left, [-1, hh, ww, 3 * qkv_left.shape[-1]])
-    qkv_right = tf.reshape(qkv_right, [-1, hh, ww, 3 * qkv_right.shape[-1]])
+    qkv = layers.Dense(emb_dim * 3, use_bias=qkv_bias, name=name and name + "qkv")(inputs)
+    qkv = functional.reshape(qkv, [-1, hh, ww, 3, emb_dim])
+    qkv_left, qkv_right = functional.split(qkv, 2, axis=-1)
+    qkv_left = functional.reshape(qkv_left, [-1, hh, ww, 3 * qkv_left.shape[-1]])
+    qkv_right = functional.reshape(qkv_right, [-1, hh, ww, 3 * qkv_right.shape[-1]])
 
     attn_out_left = window_attention(qkv_left, window_size=(hh, window_size), num_heads=num_heads // 2, attention_block=lepe_attention, name=name + "left_")
     attn_out_right = window_attention(qkv_right, window_size=(window_size, ww), num_heads=num_heads // 2, attention_block=lepe_attention, name=name + "right_")
-    attn_out = tf.concat([attn_out_left, attn_out_right], axis=-1)
-    attn_out = keras.layers.Dense(attn_out.shape[-1], use_bias=True, name=name and name + "attn_out")(attn_out)
+    attn_out = functional.concat([attn_out_left, attn_out_right], axis=-1)
+    # print(f"{inputs.shape = }, {attn_out_left.shape = }, {attn_out_right.shape = }, {attn_out.shape = }")
+    attn_out = layers.Dense(attn_out.shape[-1], use_bias=True, name=name and name + "attn_out")(attn_out)
     return attn_out
 
 
 def window_lepe_attention_mlp_block(inputs, num_heads=4, window_size=2, mlp_ratio=4, layer_scale=0.1, drop_rate=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
+    input_channel = inputs.shape[-1]  # Channels_last only
 
     """ attention """
-    nn = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, name=name + "attn_")
+    nn = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "attn_")
     nn = window_lepe_attention(nn, num_heads=num_heads, window_size=window_size, name=name + "attn_")
-    attn_out = add_with_layer_scale_and_drop_block(inputs, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "attn_")
+    attn_out = add_with_layer_scale_and_drop_block(inputs, nn, layer_scale=layer_scale, drop_rate=drop_rate, axis=-1, name=name + "attn_")
 
     """ MLP """
-    nn = layer_norm(attn_out, epsilon=LAYER_NORM_EPSILON, name=name + "mlp_")
+    nn = layer_norm(attn_out, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "mlp_")
     nn = mlp_block(nn, input_channel * mlp_ratio, activation=activation, name=name + "mlp_")
-    nn = add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "mlp_")
+    nn = add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, axis=-1, name=name + "mlp_")
 
     """ DepthWise """
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_last -> channels_first
     nn = depthwise_conv2d_no_bias(nn, kernel_size=3, padding="same", name=name + "output_")
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1], name=name + "output_perm")(nn)  # channels_first -> channels_last
     return nn
 
 
 def light_group_attention(inputs, num_heads=4, key_dim=0, num_group_token=0, use_key_value_norm=True, qkv_bias=True, dropout=0, name=""):
-    _, hh, ww, input_channel = inputs.shape
+    input_channel = inputs.shape[-1]
     key_dim = key_dim if key_dim > 0 else input_channel // num_heads
-    blocks = hh * ww
 
-    query = PureWeigths(shape=[1, num_group_token, input_channel], name=name + "query")(inputs)
-    query = layer_norm(query, epsilon=LAYER_NORM_EPSILON, name=name + "query_")
-    key_value = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, name=name + "key_value_") if use_key_value_norm else inputs
-    key = keras.layers.Dense(key_value.shape[-1], use_bias=qkv_bias, name=name and name + "key")(key_value)
-
-    query = tf.transpose(tf.reshape(query, [-1, num_group_token, num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
-    key = tf.transpose(tf.reshape(key, [-1, blocks, num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-    value = tf.transpose(tf.reshape(key_value, [-1, blocks, num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
+    query = PureWeigths(shape=[1, num_group_token, input_channel], initializer=initializers.truncated_normal(stddev=0.2), name=name + "query")(inputs)
+    query = layer_norm(query, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "query_")
+    key_value = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "key_value_") if use_key_value_norm else inputs
+    key = layers.Dense(key_value.shape[-1], use_bias=qkv_bias, name=name and name + "key")(key_value)
+    # print(f"{inputs.shape = }, {query.shape = }, {key.shape = }, {key_value.shape = }, {num_heads = }")
+    query, key, value = qkv_to_multi_head_channels_last_format(query, key, key_value, num_heads=num_heads, data_format="channels_last")
 
     output_shape = [num_group_token, input_channel]
     return scaled_dot_product_attention(query, key, value, output_shape=output_shape, out_weight=False, dropout=dropout, name=name)
 
 
 def full_ungroup_attention(inputs, group_token, num_heads=4, key_dim=0, qkv_bias=True, dropout=0, name=""):
     # inputs: [batch, height, width, channel], group_token: [batch, num_group_token, channel]
     input_channel = inputs.shape[-1]
     key_dim = key_dim if key_dim > 0 else input_channel // num_heads
 
-    query = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, name=name + "query_")
-    key_value = layer_norm(group_token, epsilon=LAYER_NORM_EPSILON, name=name + "key_value_")
-
-    query = keras.layers.Dense(query.shape[-1], use_bias=qkv_bias, name=name and name + "query")(query)
-    key = keras.layers.Dense(key_value.shape[-1], use_bias=qkv_bias, name=name and name + "key")(key_value)
-    value = keras.layers.Dense(key_value.shape[-1], use_bias=qkv_bias, name=name and name + "value")(key_value)
+    query = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "query_")
+    key_value = layer_norm(group_token, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "key_value_")
 
-    query = tf.transpose(tf.reshape(query, [-1, query.shape[1] * query.shape[2], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
-    key = tf.transpose(tf.reshape(key, [-1, key_value.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-    value = tf.transpose(tf.reshape(value, [-1, key_value.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
+    query = layers.Dense(query.shape[-1], use_bias=qkv_bias, name=name and name + "query")(query)
+    key = layers.Dense(key_value.shape[-1], use_bias=qkv_bias, name=name and name + "key")(key_value)
+    value = layers.Dense(key_value.shape[-1], use_bias=qkv_bias, name=name and name + "value")(key_value)
+    query, key, value = qkv_to_multi_head_channels_last_format(query, key, value, num_heads=num_heads, data_format="channels_last")
 
-    attn = scaled_dot_product_attention(query, key, value, output_shape=inputs.shape, out_weight=True, out_bias=True, dropout=dropout, name=name)
+    attn = scaled_dot_product_attention(query, key, value, output_shape=inputs.shape, out_weight=False, dropout=dropout, name=name)
+    attn = layers.Dense(input_channel, use_bias=True, name=name and name + "out")(attn)
 
-    attn_out = tf.concat([inputs, attn], axis=-1)
-    attn_out = keras.layers.Dense(inputs.shape[-1], use_bias=True, name=name and name + "attn_out")(attn_out)
+    attn_out = functional.concat([inputs, attn], axis=-1)
+    attn_out = layers.Dense(input_channel, use_bias=True, name=name and name + "attn_out")(attn_out)
     return attn_out
 
 
 def group_attention(inputs, num_heads=4, num_group_token=0, mlp_ratio=4, layer_scale=0.1, drop_rate=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
+    input_channel = inputs.shape[-1]  # Channels_last only
     group_token = light_group_attention(inputs, num_heads=num_heads, num_group_token=num_group_token, name=name + "light_attn_")
 
-    tokens_mlp_dim, channels_mlp_dim = input_channel * 0.5, input_channel * 4  # all using embed_dims
-    group_token = mlp_mixer_block(group_token, tokens_mlp_dim, channels_mlp_dim, drop_rate=drop_rate, activation=activation, name=name)
+    tokens_mlp_dim, channels_mlp_dim = input_channel // 2, input_channel * 4  # all using embed_dims
+    group_token = mlp_mixer_block(
+        group_token, tokens_mlp_dim, channels_mlp_dim, drop_rate=drop_rate, activation=activation, data_format="channels_last", name=name
+    )
 
     attn_out = full_ungroup_attention(inputs, group_token, num_heads=num_heads, name=name + "full_attn_")
-    attn_out = keras.layers.Reshape(inputs.shape[1:])(attn_out)
+    attn_out = layers.Reshape(inputs.shape[1:])(attn_out)
 
     """ MLP """
-    nn = layer_norm(attn_out, epsilon=LAYER_NORM_EPSILON, name=name + "mlp_")
+    nn = layer_norm(attn_out, axis=-1, epsilon=LAYER_NORM_EPSILON, name=name + "mlp_")
     nn = mlp_block(nn, input_channel * mlp_ratio, activation=activation, name=name + "mlp_")
-    nn = add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "mlp_")
+    nn = add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, axis=-1, name=name + "mlp_")
 
     """ DepthWise """
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_last -> channels_first
     nn = depthwise_conv2d_no_bias(nn, kernel_size=3, padding="same", name=name)
     nn = batchnorm_with_activation(nn, activation="relu", name=name + "output_")
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1], name=name + "output_perm")(nn)  # channels_first -> channels_last
     return nn
 
 
 def GPViT(
     num_layers=12,
     embed_dims=216,
     stem_depth=1,
@@ -184,66 +192,76 @@
     dropout=0,
     layer_scale=0,
     classifier_activation="softmax",
     pretrained=None,
     model_name="gp_vit",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
 
     """ Stem """
     nn = conv2d_no_bias(inputs, 64, kernel_size=7, strides=2, padding="same", name="stem_")
     nn = batchnorm_with_activation(nn, activation="relu", name="stem_")
     for id in range(stem_depth - 1):
         nn = conv2d_no_bias(nn, 64, kernel_size=3, strides=1, padding="same", name="stem_{}_".format(id + 1))
         nn = batchnorm_with_activation(nn, activation="relu", name="stem_{}_".format(id + 1))
     ## nn = AdaptivePadding(nn) with padding='corner' --> use_torch_padding=False
     nn = conv2d_no_bias(nn, embed_dims, kernel_size=4, strides=4, padding="same", use_torch_padding=False, use_bias=True, name="stem_patch_")
-    nn = PositionalEmbedding(name="positional_embedding")(nn)
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1], name="stem_perm")(nn)  # channels_first -> channels_last
+    nn = PositionalEmbedding(name="positional_embedding")(nn)  # Needs to be channels_last for compitable with TF
 
     """ stacks """
     group_attention_layer_group_tokens = group_attention_layer_group_tokens.copy()
     global_block_id = 0
     for block_id in range(num_layers):
         block_name = "block{}_".format(block_id + 1)
         block_drop_rate = drop_connect_rate * global_block_id / num_layers
         if block_id in group_attention_layer_ids:
             num_group_token = group_attention_layer_group_tokens.pop(0)
             nn = group_attention(nn, num_group_heads, num_group_token, mlp_ratios, layer_scale, block_drop_rate, activation=activation, name=block_name)
         else:
             nn = window_lepe_attention_mlp_block(nn, num_window_heads, window_size, mlp_ratios, layer_scale, block_drop_rate, activation, name=block_name)
         global_block_id += 1
 
-    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="pre_out_")
+    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, axis=-1, name="pre_out_")
     if use_neck_attention_output:
         nn = light_group_attention(nn, num_heads=6, num_group_token=1, use_key_value_norm=False, qkv_bias=False, name="neck_")
-        nn = tf.squeeze(nn, axis=1)
+        nn = layers.Flatten()(nn)
+    else:
+        nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2], name="out_perm")(nn)  # channels_last -> channels_first
 
     nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
-    model = tf.keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "gpvit", pretrained, PositionalEmbedding)
     return model
 
 
+@register_model
 def GPViT_L1(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return GPViT(**locals(), model_name="gpvit_l1", **kwargs)
 
 
+@register_model
 def GPViT_L2(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     embed_dims = 348
     stem_depth = 2
     return GPViT(**locals(), model_name="gpvit_l2", **kwargs)
 
 
+@register_model
 def GPViT_L3(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     embed_dims = 432
     stem_depth = 2
     use_neck_attention_output = False
     return GPViT(**locals(), model_name="gpvit_l3", **kwargs)
 
 
+@register_model
 def GPViT_L4(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     embed_dims = 624
     stem_depth = 3
     use_neck_attention_output = False
     return GPViT(**locals(), model_name="gpvit_l4", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/halonet/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/halonet/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -53,23 +53,14 @@
   expansion: filter expansion for each block output channel.
   output_num_features: none `0` value to add another `conv2d + bn + activation` layers before `GlobalAveragePooling2D`.
   model_name: string, model name.
 """ + __tail_doc__ + """
 Model architectures:
   | Model          | Params | FLOPs   | Input | Top1 Acc |
   | -------------- | ------ | ------- | ----- | -------- |
-  | HaloNetH0      | 5.5M   | 2.40G   | 256   | 77.9     |
-  | HaloNetH1      | 8.1M   | 3.04G   | 256   | 79.9     |
-  | HaloNetH2      | 9.4M   | 3.37G   | 256   | 80.4     |
-  | HaloNetH3      | 11.8M  | 6.30G   | 320   | 81.9     |
-  | HaloNetH4      | 19.1M  | 12.17G  | 384   | 83.3     |
-  | - 21k          | 19.1M  | 12.17G  | 384   | 85.5     |
-  | HaloNetH5      | 30.7M  | 32.61G  | 448   | 84.0     |
-  | HaloNetH6      | 43.4M  | 53.20G  | 512   | 84.4     |
-  | HaloNetH7      | 67.4M  | 119.64G | 600   | 84.9     |
   | HaloNextECA26T | 10.7M  | 2.43G   | 256   | 79.50    |
   | HaloNet26T     | 12.5M  | 3.18G   | 256   | 79.13    |
   | HaloNetSE33T   | 13.7M  | 3.55G   | 256   | 80.99    |
   | HaloRegNetZB   | 11.68M | 1.97G   | 224   | 81.042   |
   | HaloNet50T     | 22.7M  | 5.29G   | 256   | 81.70    |
   | HaloBotNet50T  | 22.6M  | 5.02G   | 256   | 82.0     |
 
@@ -87,14 +78,15 @@
 HaloNetH6.__doc__ = HaloNetH0.__doc__
 HaloNetH7.__doc__ = HaloNetH0.__doc__
 
 HaloNet26T.__doc__ = __head_doc__ + """Model weights are reloaded from timm [Github rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models).
 
 Args:
 """ + __tail_doc__
+
 HaloNetSE33T.__doc__ = HaloNet26T.__doc__
 HaloNextECA26T.__doc__ = HaloNet26T.__doc__
 HaloNet50T.__doc__ = HaloNet26T.__doc__
 HaloRegNetZB.__doc__ = HaloNet26T.__doc__
 HaloBotNet50T.__doc__ = HaloNet26T.__doc__
 
 halo_attention.__doc__ = __head_doc__ + """
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/halonet/halonet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/halonet/halonet.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,12 +1,18 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, initializers, image_data_format
 from keras_cv_attention_models.aotnet import AotNet
-from keras_cv_attention_models.attention_layers import RelativePositionalEmbedding, conv2d_no_bias, CompatibleExtractPatches, make_divisible
+from keras_cv_attention_models.models import register_model
+from keras_cv_attention_models.attention_layers import (
+    RelativePositionalEmbedding,
+    conv2d_no_bias,
+    CompatibleExtractPatches,
+    make_divisible,
+    scaled_dot_product_attention,
+)
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
     "halonet26t": {"imagenet": {256: "6e8848ce6e98a13cd45f65dd68435d00"}},
     "halonet50t": {"imagenet": {256: "6c353144df942cb81a3eb7c140fc9791"}},
     "halonet_se33t": {"imagenet": {256: "7e0afb7f8fb6459491b8a46ad80bcd91"}},
     "halonext_eca26t": {"imagenet": {256: "630037a5c135bceacd0691a22855eb7e"}},
@@ -14,93 +20,105 @@
     "halobotnet50t": {"imagenet": {256: "0af1faad1a81e468d6e670e9fc253edc"}},
 }
 
 
 def halo_attention(
     inputs, num_heads=8, key_dim=0, block_size=4, halo_size=1, strides=1, out_shape=None, out_weight=True, out_bias=False, attn_dropout=0, name=None
 ):
-    _, hh, ww, cc = inputs.shape
+    height_axis, width_axis, channel_axis = (1, 2, 3) if image_data_format() == "channels_last" else (2, 3, 1)
+    filters = inputs.shape[channel_axis]
     if key_dim > 1:
         key_dim = key_dim  # Specified one
     elif key_dim > 0:
-        key_dim = make_divisible(cc * key_dim, divisor=8) // num_heads  # regard as key_dim_ratio
+        key_dim = make_divisible(filters * key_dim, divisor=8) // num_heads  # regard as key_dim_ratio
     else:
-        key_dim = cc // num_heads  # Default value
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
-    out_shape = cc if out_shape is None else out_shape
+        key_dim = filters // num_heads  # Default value
+    # qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
+    qk_scale = 1.0 / (float(key_dim) ** 0.5)
+    out_shape = filters if out_shape is None else out_shape
     emb_dim = num_heads * key_dim
     kv_kernel = block_size + halo_size * 2
     if block_size % strides != 0:
         strides = 1
         avg_pool_down = True
     else:
         avg_pool_down = False
     query_block = block_size // strides
 
     query = conv2d_no_bias(inputs, emb_dim, kernel_size=1, strides=strides, name=name and name + "query_")
-    _, hh, ww, cc = query.shape
     # print(f">>>> {inputs.shape = }, {query.shape = }, {block_size = }, {strides = }")
     # attn_query = rearrange(query, "B (h hb) (w wb) (hd c) -> B hd h w (hb wb) c", hb=query_block, wb=query_block, hd=num_heads)
     # pos_query = rearrange(attn_query, "B hd h w (hb wb) c -> B (hd h w) hb wb c", hb=query_block, wb=query_block)
-    hh_qq, ww_qq, cc_qq = hh // query_block, ww // query_block, cc // num_heads
-    query = tf.reshape(query, [-1, hh_qq, query_block, ww_qq, query_block, num_heads, cc_qq])
-    query = tf.transpose(query, [0, 5, 1, 3, 2, 4, 6])  # [batch, num_heads, hh, ww, query_block, query_block, key_dim]
+    query = query if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(query)  # channels_first -> channels_last
+    hh_qq, ww_qq, cc_qq = query.shape[1] // query_block, query.shape[2] // query_block, emb_dim // num_heads
+    query = functional.reshape(query, [-1, hh_qq, query_block, ww_qq, query_block, num_heads, cc_qq])
+    query = functional.transpose(query, [0, 5, 1, 3, 2, 4, 6])  # [batch, num_heads, hh, ww, query_block, query_block, key_dim]
     # attn_query = [batch, num_heads, hh, ww, query_block * query_block, key_dim]
-    attn_query = tf.reshape(query, [-1, num_heads, hh_qq, ww_qq, query_block * query_block, cc_qq]) * qk_scale  # [???] qk_scale NOT multiplied with pos_query
+    attn_query = functional.reshape(query, [-1, num_heads, hh_qq, ww_qq, query_block * query_block, cc_qq]) * qk_scale  # qk_scale NOT multiplied with pos_query
     # pos_query = [batch, num_heads * hh * ww, query_block, query_block, key_dim]
-    pos_query = tf.reshape(query, [-1, num_heads * hh_qq * ww_qq, query_block, query_block, cc_qq])
+    pos_query = functional.reshape(query, [-1, num_heads * hh_qq * ww_qq, query_block, query_block, cc_qq])
 
     # key_value = [batch, height, width, key_dim + out_shape]
     key_value = conv2d_no_bias(inputs, emb_dim + out_shape, kernel_size=1, use_bias=False, name=name and name + "key_value_")
-    kv_padded = tf.pad(key_value, [[0, 0], [halo_size, halo_size], [halo_size, halo_size], [0, 0]])
-    sizes, strides = [1, kv_kernel, kv_kernel, 1], [1, block_size, block_size, 1]
+    key_value = key_value if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(key_value)  # channels_first -> channels_last
+    kv_padded = functional.pad(key_value, [[0, 0], [halo_size, halo_size], [halo_size, halo_size], [0, 0]])
     # kv_inp = [batch, hh, ww, kv_kernel * kv_kernel * (key_dim + out_shape)]
-    # kv_inp = tf.image.extract_patches(kv_padded, sizes=sizes, strides=strides, rates=[1, 1, 1, 1], padding="VALID")
-    kv_inp = CompatibleExtractPatches(sizes=sizes, strides=strides, rates=[1, 1, 1, 1], padding="VALID")(kv_padded)
+    if backend.is_torch_backend:
+        kv_inp = functional.extract_patches(kv_padded, sizes=kv_kernel, strides=block_size, padding="valid")
+    else:
+        # kv_inp = tf.image.extract_patches(kv_padded, sizes=sizes, strides=strides, rates=[1, 1, 1, 1], padding="valid")
+        kv_inp = CompatibleExtractPatches(sizes=kv_kernel, strides=block_size, padding="valid")(kv_padded)
     # kv_inp = rearrange(kv_inp, "B h w (hb wb hd c) -> B hd h w (hb wb) c", hb=kv_kernel, wb=kv_kernel, hd=num_heads)
     _, hh_kk, ww_kk, cc = kv_inp.shape
     cc_kk = cc // num_heads // kv_kernel // kv_kernel
-    kv_inp = tf.reshape(kv_inp, [-1, hh_kk, ww_kk, kv_kernel, kv_kernel, num_heads, cc_kk])
-    kv_inp = tf.transpose(kv_inp, [0, 5, 1, 2, 3, 4, 6])
-    kv_inp = tf.reshape(kv_inp, [-1, num_heads, hh_kk, ww_kk, kv_kernel * kv_kernel, cc_kk])
+    kv_inp = functional.reshape(kv_inp, [-1, hh_kk, ww_kk, kv_kernel, kv_kernel, num_heads, cc_kk])
+    kv_inp = functional.transpose(kv_inp, [0, 5, 1, 2, 3, 4, 6])
+    kv_inp = functional.reshape(kv_inp, [-1, num_heads, hh_kk, ww_kk, kv_kernel * kv_kernel, cc_kk])
 
     # key = [batch, num_heads, hh, ww, kv_kernel * kv_kernel, key_dim]
     # value = [batch, num_heads, hh, ww, kv_kernel * kv_kernel, out_dim]
-    key, value = tf.split(kv_inp, [emb_dim // num_heads, out_shape // num_heads], axis=-1)
+    key, value = functional.split(kv_inp, [emb_dim // num_heads, out_shape // num_heads], axis=-1)
 
     # scaled_dot_product_attention
     # print(f">>>> {attn_query.shape = }, {key.shape = }, {value.shape = }, {kv_inp.shape = }, {pos_query.shape = }, {num_heads = }")
     # attention_scores = [batch, num_heads, hh, ww, query_block * query_block, kv_kernel * kv_kernel]
-    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1], transpose_b=True))([attn_query, key])
+    # attention_scores = layers.Lambda(lambda xx: functional.matmul(xx[0], xx[1], transpose_b=True))([attn_query, key])
+    attention_scores = attn_query @ functional.transpose(key, [0, 1, 2, 3, 5, 4])
     # pos = [batch, num_heads * hh * ww, query_block, query_block, kv_kernel, kv_kernel]
     pos = RelativePositionalEmbedding(position_height=kv_kernel, name=name and name + "pos_emb")(pos_query)
     # print(f">>>> {pos.shape = }, {attention_scores.shape = }")
-    pos = tf.reshape(pos, [-1, *attention_scores.shape[1:]])
-    attention_scores = keras.layers.Add()([attention_scores, pos])
+    pos = functional.reshape(pos, [-1, *attention_scores.shape[1:]])
+    attention_scores = layers.Add()([attention_scores, pos])
     # attention_scores = tf.nn.softmax(attention_scores, axis=-1)
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
+    attention_scores = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
 
     if attn_dropout > 0:
-        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
+        attention_scores = layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
 
     # attention_output = [batch, num_heads, hh, ww, query_block * query_block, out_dim]
-    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
+    # attention_output = layers.Lambda(lambda xx: functional.matmul(xx[0], xx[1]))([attention_scores, value])
+    attention_output = attention_scores @ value
     # attention_output = rearrange(attention_output, "B hd h w (hb wb) c -> B (h hb) (w wb) (hd c)", hb=query_block, wb=query_block)
     _, heads, hh_aa, ww_aa, patch, cc_aa = attention_output.shape
-    attention_output = tf.reshape(attention_output, [-1, heads, hh_aa, ww_aa, query_block, query_block, cc_aa])
-    attention_output = tf.transpose(attention_output, [0, 2, 4, 3, 5, 1, 6])
-    attention_output = tf.reshape(attention_output, [-1, hh_aa * query_block, ww_aa * query_block, heads * cc_aa])
+    attention_output = functional.reshape(attention_output, [-1, heads, hh_aa, ww_aa, query_block, query_block, cc_aa])
+    attention_output = functional.transpose(attention_output, [0, 2, 4, 3, 5, 1, 6])
+    attention_output = functional.reshape(attention_output, [-1, hh_aa * query_block, ww_aa * query_block, heads * cc_aa])
     # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
 
+    cur_data_format = "channels_last"
     if avg_pool_down:
-        attention_output = keras.layers.AvgPool2D(2, strides=2, name=name and name + "avg_pool")(attention_output)
+        attention_output = attention_output if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(attention_output)
+        attention_output = layers.AvgPool2D(2, strides=2, name=name and name + "avg_pool")(attention_output)
+        cur_data_format = image_data_format()
     if out_weight:
         # [batch, hh, ww, num_heads * out_dim] * [out, out] --> [batch, hh, ww, out]
-        attention_output = keras.layers.Dense(out_shape, use_bias=out_bias, name=name and name + "output")(attention_output)
-    return attention_output
+        attention_output = attention_output if cur_data_format == "channels_last" else layers.Permute([2, 3, 1])(attention_output)
+        attention_output = layers.Dense(out_shape, use_bias=out_bias, name=name and name + "output")(attention_output)
+        cur_data_format = "channels_last"
+    return attention_output if cur_data_format == image_data_format() else layers.Permute([3, 1, 2])(attention_output)
 
 
 BLOCK_CONFIGS = {
     "h0": {  # rv = 1, rb = 0.5
         "halo_block_size": 8,  # b
         "halo_halo_size": 3,  # h
         "halo_expansion": 1,  # rv
@@ -245,14 +263,15 @@
 
 
 def HaloNetH7(input_shape=(600, 600, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     # input_shape should be divisible by `int(tf.reduce_prod(strides) * halo_block_size)`, may using 640 here
     return HaloNet(**BLOCK_CONFIGS["h7"], model_name="haloneth7", request_resolution=600, **locals(), **kwargs)
 
 
+@register_model
 def HaloNet26T(input_shape=(256, 256, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 2, 2]
     attn_types = [None, None, [None, "halo"], "halo"]
     attn_params = [
         None,
         None,
         [None, {"block_size": 8, "halo_size": 2, "num_heads": 8, "out_weight": False}],
@@ -261,14 +280,15 @@
     # key_dim = 16
     stem_type = "tiered"
     model = AotNet(model_name="halonet26t", **locals(), **kwargs)
     reload_model_weights(model, PRETRAINED_DICT, "halonet", pretrained, RelativePositionalEmbedding)
     return model
 
 
+@register_model
 def HaloNet50T(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 6, 3]
     attn_types = [None, [None, None, None, "halo"], [None, "halo"] * 3, [None, "halo", None]]
     attn_params = [
         None,
         [None, None, None, {"block_size": 8, "halo_size": 3, "num_heads": 4, "out_weight": False}],
         [None, {"block_size": 8, "halo_size": 3, "num_heads": 8, "out_weight": False}] * 3,
@@ -277,14 +297,15 @@
     # key_dim = 16
     stem_type = "tiered"
     model = AotNet(model_name="halonet50t", **locals(), **kwargs)
     reload_model_weights(model, PRETRAINED_DICT, "halonet", pretrained, RelativePositionalEmbedding)
     return model
 
 
+@register_model
 def HaloNetSE33T(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 3, 3, 2]
     attn_types = [None, [None, None, "halo"], [None, None, "halo"], "halo"]
     attn_params = [
         None,
         [None, None, {"block_size": 8, "halo_size": 3, "num_heads": 8, "out_weight": False}],
         [None, None, {"block_size": 8, "halo_size": 3, "num_heads": 8, "out_weight": False}],
@@ -299,14 +320,15 @@
     stem_downsample = False
     output_num_features = 1280
     model = AotNet(model_name="halonet_se33t", **locals(), **kwargs)
     reload_model_weights(model, PRETRAINED_DICT, "halonet", pretrained, RelativePositionalEmbedding)
     return model
 
 
+@register_model
 def HaloNextECA26T(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 2, 2]
     attn_types = [None, None, [None, "halo"], "halo"]
     attn_params = [
         None,
         None,
         [None, {"block_size": 8, "halo_size": 2, "num_heads": 8, "key_dim": 16, "out_weight": False}],
@@ -316,14 +338,15 @@
     groups = [4, 8, 16, 32]
     stem_type = "tiered"
     model = AotNet(model_name="halonext_eca26t", **locals(), **kwargs)
     reload_model_weights(model, PRETRAINED_DICT, "halonet", pretrained, RelativePositionalEmbedding)
     return model
 
 
+@register_model
 def HaloRegNetZB(input_shape=(224, 224, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 6, 12, 2]
     strides = [2, 2, 2, 2]
     out_channels = [48, 96, 192, 288]
     # timm bottle_in=True mode, the first ratio in each stack is `ratio * previous_channel`
     hidden_channel_ratio = [[32 * 3 / 48, 3], [1.5] + [3] * 5, [1.5] + [3] * 11, [192 * 3 / 288, 3]]
     use_block_output_activation = False  # timm linear_out=True mode
@@ -338,14 +361,15 @@
     shortcut_type = None
     output_num_features = 1536
     model = AotNet(model_name="haloregnetz_b", **locals(), **kwargs)
     reload_model_weights(model, PRETRAINED_DICT, "halonet", pretrained, RelativePositionalEmbedding)
     return model
 
 
+@register_model
 def HaloBotNet50T(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 6, 3]
     attn_types = [None, [None, "halo"] * 2, [None, "halo"] * 3, [None, "bot", None]]
     attn_params = [
         None,
         [None, {"block_size": 8, "halo_size": 3, "num_heads": 8, "out_weight": False}] * 2,
         [None, {"block_size": 8, "halo_size": 3, "num_heads": 8, "out_weight": False}] * 3,
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/hornet/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/hornet/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/hornet/hornet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/hornet/hornet.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
-    ChannelAffine,
+    add_with_layer_scale_and_drop_block,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
-    drop_block,
     layer_norm,
     mlp_block,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
@@ -19,109 +19,107 @@
     "hornet_small_gf": {"imagenet": {224: "79ae2409e29b125ba9978227f353cd7e"}},
     "hornet_small": {"imagenet": {224: "4ac8b8552a750303d65cae5fa0ff8013"}},
     "hornet_tiny_gf": {"imagenet": {224: "49323a163df84196e9d40a65289e54a9"}},
     "hornet_tiny": {"imagenet": {224: "7bdafa31599cb863b42d960b9d7f5ae4"}},
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="kecam/hornet")
-class ComplexDense(keras.layers.Layer):
+@backend.register_keras_serializable(package="kecam/hornet")
+class ComplexDense(layers.Layer):
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
 
     def build(self, input_shape):
         _, input_height, input_width, channel = input_shape
 
         param_shape = (2, input_height, input_width, channel)  # 2 means `real, img` for converting to complex
-        initializer = tf.initializers.RandomNormal(stddev=0.02)
+        initializer = initializers.RandomNormal(stddev=0.02)
         self.complex_weight = self.add_weight(name="complex_weight", shape=param_shape, initializer=initializer, trainable=True)
         self.input_height, self.input_width = input_height, input_width
 
     def call(self, inputs):
-        complex_weight = tf.complex(self.complex_weight[0], self.complex_weight[1])
-        complex_weight = tf.cast(complex_weight, inputs.dtype)
+        complex_weight = functional.complex(self.complex_weight[0], self.complex_weight[1])
+        complex_weight = functional.cast(complex_weight, inputs.dtype)
         return inputs * complex_weight
 
     def load_resized_weights(self, source_layer, method="bilinear"):
         if isinstance(source_layer, dict):
             source_tt = source_layer["complex_weight:0"]  # weights
         else:
             source_tt = source_layer.complex_weight  # layer
-        tt = tf.image.resize(source_tt, (self.input_height, self.input_width), method=method, antialias=True)
+        tt = functional.resize(source_tt, (self.input_height, self.input_width), method=method, antialias=True)
         self.complex_weight.assign(tt)
 
 
 def global_local_filter(inputs, name=None):
     _, height, width, channel = inputs.shape
     nn = layer_norm(inputs, name=name and name + "pre_")
-    dw, fft = tf.split(nn, 2, axis=-1)
-    dw = depthwise_conv2d_no_bias(dw, 3, padding="SAME", use_bias=False, name=name)
+    dw, fft = functional.split(nn, 2, axis=-1)
+    dw = depthwise_conv2d_no_bias(dw, 3, padding="same", use_bias=False, name=name)
 
     # fft = tf.py_function(lambda xx: np.fft.rfft2(xx, axes=(1, 2), norm='ortho'), [fft], Tout=tf.complex128)
     # np.fft.rfft2(aa, axes=[1, 2]) ==> tf.transpose(tf.signal.rfft2d(tf.transpose(aa, [0, 3, 1, 2])), [0, 2, 3, 1])
     # ortho_norm = float(tf.sqrt(float(height * width)))
-    fft = tf.transpose(fft, [0, 3, 1, 2])
-    fft = keras.layers.Lambda(tf.signal.rfft2d)(fft)
-    fft = tf.transpose(fft, [0, 2, 3, 1])
+    fft = functional.transpose(fft, [0, 3, 1, 2])
+    fft = layers.Lambda(functional.rfft2d)(fft)
+    fft = functional.transpose(fft, [0, 2, 3, 1])
     # fft /= ortho_norm  # Means `norm='ortho'`, but will multiply back for `irfft2d` anyway, not affecting results.
     # fft.set_shape([None, height, (width + 2) // 2, channel // 2])
     # print(f">>>> {inputs.shape = }, {fft.shape = }, {fft.dtype = }")
     fft = ComplexDense(name=name and name + "complex_dense")(fft)
     # fft = tf.py_function(lambda xx: np.fft.irfft2(xx, s=[height, width], axes=(1, 2), norm='ortho'), [fft], Tout=inputs.dtype)
     # np.fft.irfft2(bb, s=[13, 14], axes=(1, 2)) ==> tf.transpose(tf.signal.irfft2d(tf.transpose(bb, [0, 3, 1, 2]), fft_length=[13, 14]), [0, 2, 3, 1])
-    fft = tf.transpose(fft, [0, 3, 1, 2])
-    fft = keras.layers.Lambda(lambda xx: tf.signal.irfft2d(xx, fft_length=[height, width]))(fft)
-    fft = tf.transpose(fft, [0, 2, 3, 1])
+    fft = functional.transpose(fft, [0, 3, 1, 2])
+    fft = layers.Lambda(lambda xx: functional.irfft2d(xx, fft_length=[height, width]))(fft)
+    fft = functional.transpose(fft, [0, 2, 3, 1])
     # fft *= ortho_norm
     # fft = tf.cast(fft, inputs.dtype)
     # fft.set_shape([None, height, width, channel // 2])
 
-    out = tf.concat([tf.expand_dims(dw, -1), tf.expand_dims(fft, -1)], axis=-1)
-    out = tf.reshape(out, [-1, height, width, channel])
+    out = functional.concat([functional.expand_dims(dw, -1), functional.expand_dims(fft, -1)], axis=-1)
+    out = functional.reshape(out, [-1, height, width, channel])
     out = layer_norm(out, name=name and name + "post_")
     return out
 
 
 def gnconv(inputs, use_global_local_filter=False, dw_kernel_size=7, gn_split=3, scale=0.3333333, name=None):
-    input_channel = inputs.shape[-1]
+    channel_axis = -1 if backend.image_data_format() == "channels_last" else 1
+    input_channel = inputs.shape[channel_axis]
     nn = conv2d_no_bias(inputs, input_channel * 2, kernel_size=1, use_bias=True, name=name and name + "pre_")
     split_dims = [input_channel // (2**ii) for ii in range(gn_split)][::-1]
     # print(f">>>> {nn.shape = }, {split_dims = }")
-    pw_first, dw_list = tf.split(nn, [split_dims[0], sum(split_dims)], axis=-1)
+    pw_first, dw_list = functional.split(nn, [split_dims[0], sum(split_dims)], axis=channel_axis)
 
     if use_global_local_filter:
         dw_list = global_local_filter(dw_list, name=name and name + "gf_")
     else:
-        dw_list = depthwise_conv2d_no_bias(dw_list, kernel_size=dw_kernel_size, padding="SAME", use_bias=True, name=name and name + "list_")
+        dw_list = depthwise_conv2d_no_bias(dw_list, kernel_size=dw_kernel_size, padding="same", use_bias=True, name=name and name + "list_")
     dw_list *= scale
 
-    dw_list = tf.split(dw_list, split_dims, axis=-1)
+    dw_list = functional.split(dw_list, split_dims, axis=channel_axis)
     nn = pw_first * dw_list[0]
     for id, dw in enumerate(dw_list[1:], start=1):
-        pw = conv2d_no_bias(nn, dw.shape[-1], kernel_size=1, use_bias=True, name=name and name + "pw{}_".format(id))
+        pw = conv2d_no_bias(nn, dw.shape[channel_axis], kernel_size=1, use_bias=True, name=name and name + "pw{}_".format(id))
         nn = pw * dw
 
     nn = conv2d_no_bias(nn, input_channel, kernel_size=1, use_bias=True, name=name and name + "output_")
     return nn
 
 
 def block(inputs, mlp_ratio=4, use_global_local_filter=False, gn_split=3, scale=0.3333333, layer_scale=0, drop_rate=0, activation="gelu", name=""):
     # print(global_query)
-    input_channel = inputs.shape[-1]
     attn = layer_norm(inputs, name=name + "attn_")
     attn = gnconv(attn, use_global_local_filter, gn_split=gn_split, scale=scale, name=name + "gnconv_")
-    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "1_gamma")(attn) if layer_scale >= 0 else attn
-    attn = drop_block(attn, drop_rate=drop_rate, name=name + "attn_")
-    attn_out = keras.layers.Add(name=name + "attn_out")([inputs, attn])
-
-    mlp = layer_norm(attn_out, name=name + "mlp_")
-    mlp = mlp_block(mlp, int(input_channel * mlp_ratio), use_conv=False, activation=activation, name=name + "mlp_")
-    mlp = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "2_gamma")(mlp) if layer_scale >= 0 else mlp
-    mlp = drop_block(mlp, drop_rate=drop_rate, name=name + "mlp_")
-    return keras.layers.Add(name=name + "output")([attn_out, mlp])
+    attn_out = add_with_layer_scale_and_drop_block(inputs, attn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "1_")
+
+    mlp = attn_out if backend.image_data_format() == "channels_last" else layers.Permute((2, 3, 1))(attn_out)
+    mlp = layer_norm(mlp, axis=-1, name=name + "mlp_")
+    mlp = mlp_block(mlp, int(mlp.shape[-1] * mlp_ratio), use_conv=False, activation=activation, name=name + "mlp_")
+    mlp = mlp if backend.image_data_format() == "channels_last" else layers.Permute((3, 1, 2))(mlp)
+    return add_with_layer_scale_and_drop_block(attn_out, mlp, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "2_")
 
 
 def HorNet(
     num_blocks=[2, 3, 18, 2],
     embed_dim=64,
     mlp_ratio=4,
     gn_split=[2, 3, 4, 5],
@@ -135,83 +133,95 @@
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="hornet",
     kwargs=None,
 ):
     """Patch stem"""
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
     nn = conv2d_no_bias(inputs, embed_dim, kernel_size=4, strides=4, use_bias=True, name="stem_")
     nn = layer_norm(nn, name="stem_")
 
     """ stages """
     total_blocks = sum(num_blocks)
     global_block_id = 0
     for stack_id, num_block in enumerate(num_blocks):
         stack_name = "stack{}_".format(stack_id + 1)
         if stack_id > 0:
             nn = layer_norm(nn, name=stack_name)
-            nn = conv2d_no_bias(nn, nn.shape[-1] * 2, kernel_size=2, strides=2, use_bias=True, name=stack_name)
+            input_channels = nn.shape[-1 if backend.image_data_format() == "channels_last" else 1]
+            nn = conv2d_no_bias(nn, input_channels * 2, kernel_size=2, strides=2, use_bias=True, name=stack_name)
 
         cur_use_global_local_filter = use_global_local_filter[stack_id] if isinstance(use_global_local_filter, (list, tuple)) else use_global_local_filter
         cur_gn_split = gn_split[stack_id] if isinstance(gn_split, (list, tuple)) else gn_split
         cur_scale = scale[stack_id] if isinstance(scale, (list, tuple)) else scale
 
         for block_id in range(num_block):
             block_name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             nn = block(nn, mlp_ratio, cur_use_global_local_filter, cur_gn_split, cur_scale, layer_scale, block_drop_rate, activation, name=block_name)
             global_block_id += 1
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
         if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
         nn = layer_norm(nn, name="pre_output_")
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "hornet", pretrained, mismatch_class=ComplexDense)
     return model
 
 
+@register_model
 def HorNetTiny(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return HorNet(**locals(), model_name="hornet_tiny", **kwargs)
 
 
+@register_model
 def HorNetTinyGF(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     use_global_local_filter = [False, False, True, True]
     return HorNet(**locals(), model_name="hornet_tiny_gf", **kwargs)
 
 
+@register_model
 def HorNetSmall(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     embed_dim = 96
     return HorNet(**locals(), model_name="hornet_small", **kwargs)
 
 
+@register_model
 def HorNetSmallGF(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     embed_dim = 96
     use_global_local_filter = [False, False, True, True]
     return HorNet(**locals(), model_name="hornet_small_gf", **kwargs)
 
 
+@register_model
 def HorNetBase(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     embed_dim = 128
     return HorNet(**locals(), model_name="hornet_base", **kwargs)
 
 
+@register_model
 def HorNetBaseGF(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     embed_dim = 128
     use_global_local_filter = [False, False, True, True]
     return HorNet(**locals(), model_name="hornet_base_gf", **kwargs)
 
 
+@register_model
 def HorNetLarge(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet22k", **kwargs):
     embed_dim = 192
     return HorNet(**locals(), model_name="hornet_large", **kwargs)
 
 
+@register_model
 def HorNetLargeGF(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet22k", **kwargs):
     embed_dim = 192
     use_global_local_filter = [False, False, True, True]
     return HorNet(**locals(), model_name="hornet_large_gf", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/iformer/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/iformer/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from keras_cv_attention_models.iformer.iformer import InceptionTransformer, IFormerSmall, IFormerBase, IFormerLarge
 
 __head_doc__ = """
-Keras implementation of [Github whai362/PVT](https://github.com/whai362/PVT/tree/v2/classification).
-Paper [PDF 2106.13797 PVTv2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/pdf/2106.13797.pdf).
+Keras implementation of [Github sail-sg/iFormer](https://github.com/sail-sg/iFormer).
+Paper [PDF 2205.12956 Inception Transformer](https://arxiv.org/pdf/2205.12956.pdf).
 """
 
 __tail_doc__ = """  input_shape: it should have exactly 3 inputs channels, like `(224, 224, 3)`.
   num_classes: number of classes to classify images into. Set `0` to exclude top layers.
   activation: activation used in whole model, default `gelu`.
   drop_connect_rate: is used for [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382).
       Can be a constant value like `0.2`,
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/iformer/iformer.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/efficientformer/efficientformer.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,165 +1,164 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
-    activation_by_name,
     add_with_layer_scale_and_drop_block,
+    ChannelAffine,
+    MultiHeadPositionalEmbedding,
     batchnorm_with_activation,
     conv2d_no_bias,
-    depthwise_conv2d_no_bias,
+    drop_block,
     layer_norm,
     mlp_block,
-    multi_head_self_attention,
-    output_block,
-    PositionalEmbedding,
+    mhsa_with_multi_head_position,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
-LAYER_NORM_EPSILON = 1e-6
-
 PRETRAINED_DICT = {
-    "iformer_base": {"imagenet": {224: "90c8fe1307bc8cbd73f5a7358e65956d", 384: "11d150c128a65f8dcc95871e87209491"}},
-    "iformer_large": {"imagenet": {224: "2a116d5780a551846761aa43b1d74395", 384: "b21ae4484d1f5da7f1425ac301248e29"}},
-    "iformer_small": {"imagenet": {224: "de2f8262da0cc1c6df43b32b827444f1", 384: "e1c7f4e52abf3514437138c62194d2d7"}},
+    "efficientformer_l1": {"imagenet": {224: "7698d40d502ccc548a7e2890fb33db34"}},
+    "efficientformer_l3": {"imagenet": {224: "ee3d11742d233bc2ec36648440cb5a0b"}},
+    "efficientformer_l7": {"imagenet": {224: "66c26fc1e0bd39bbf6886d570956d178"}},
 }
 
 
-def attention_low_frequency_mixer(inputs, num_heads=4, pool_size=1, dropout=0, name=""):
-    if pool_size > 1:
-        orign_height, orign_width = inputs.shape[1], inputs.shape[2]
-        inputs = keras.layers.AvgPool2D(pool_size, strides=pool_size, padding="same", name=name + "avg_down")(inputs)
-    nn = multi_head_self_attention(inputs, num_heads=num_heads, qkv_bias=True, out_weight=False, attn_dropout=dropout, name=name + "attn_")
-    if pool_size > 1:
-        nn = keras.layers.UpSampling2D(size=pool_size, name=name + "up")(nn)
-        if nn.shape[1] != orign_height or nn.shape[2] != orign_width:
-            nn = nn[:, :orign_height, :orign_width]
-    return nn
-
-
-def conv_high_frequency_mixer(inputs, activation="gelu", name=""):
-    nn = conv2d_no_bias(inputs, inputs.shape[-1] * 2, kernel_size=1, name=name)
-    nn = depthwise_conv2d_no_bias(nn, kernel_size=3, padding="same", name=name)
-    nn = activation_by_name(nn, activation=activation, name=name)
-    return nn
-
-
-def pool_high_frequency_mixer(inputs, activation="gelu", name=""):
-    nn = keras.layers.MaxPooling2D(3, strides=1, padding="SAME", name=name + "max")(inputs)
-    nn = conv2d_no_bias(nn, inputs.shape[-1] * 2, kernel_size=1, use_bias=True, name=name)
-    nn = activation_by_name(nn, activation=activation, name=name)
-    return nn
-
-
-def conv_pool_attention_mixer(inputs, num_heads=4, key_dim=0, num_attn_low_heads=1, pool_size=1, dropout=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
-    key_dim = key_dim if key_dim > 0 else input_channel // num_heads
-    attention_channels = num_attn_low_heads * key_dim
-    conv_channels = (input_channel - attention_channels) // 2
-    pool_channels = input_channel - attention_channels - conv_channels
-    conv_branch, pool_branch, attention_branch = tf.split(inputs, [conv_channels, pool_channels, attention_channels], axis=-1)
-    # print(f"{key_dim = }, {num_heads = }, {num_attn_low_heads = }, {attention_channels = }")
-
-    conv_branch = conv_high_frequency_mixer(conv_branch, activation=activation, name=name + "high_conv_branch_")
-    pool_branch = pool_high_frequency_mixer(pool_branch, activation=activation, name=name + "high_pool_branch_")
-    attention_branch = attention_low_frequency_mixer(attention_branch, num_heads=num_attn_low_heads, pool_size=pool_size, dropout=dropout, name=name + "low_")
-    high_low = tf.concat([conv_branch, pool_branch, attention_branch], axis=-1)
-    # print(f"{conv_branch.shape = }, {pool_branch.shape = }, {attention_branch.shape = }, {high_low.shape = }")
-
-    high_low_fused = depthwise_conv2d_no_bias(high_low, kernel_size=3, padding="same", name=name + "fuse_")
-    high_low_out = keras.layers.Add()([high_low, high_low_fused])
-
-    out = conv2d_no_bias(high_low_out, input_channel, kernel_size=1, use_bias=True, name=name + "output_")
-    if dropout > 0:
-        out = keras.layers.Dropout(dropout)(out)
-    return out
-
-
-def attention_mlp_block(inputs, num_heads=8, num_attn_low_heads=1, pool_size=1, mlp_ratio=4, layer_scale=0.1, drop_rate=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
-
-    """ attention """
-    nn = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, name=name + "attn_")
-    nn = conv_pool_attention_mixer(nn, num_heads, num_attn_low_heads=num_attn_low_heads, pool_size=pool_size, activation=activation, name=name + "attn_")
+def attn_block(inputs, num_heads=8, key_dim=32, attn_height=-1, attn_ratio=4, mlp_ratio=4, layer_scale=0, drop_rate=0, activation="gelu", name=""):
+    input_channel = inputs.shape[-1]  # also using -1 for channels_first
+
+    nn = layer_norm(inputs, axis=-1, name=name + "attn_")
+    nn = mhsa_with_multi_head_position(nn, num_heads, key_dim=key_dim, attn_height=attn_height, attn_ratio=attn_ratio, qkv_bias=True, out_bias=True, name=name)
+    attn_out = add_with_layer_scale_and_drop_block(inputs, nn, layer_scale=layer_scale, drop_rate=drop_rate, axis=-1, name=name + "attn_")
+
+    nn = layer_norm(attn_out, axis=-1, name=name + "mlp_")
+    nn = mlp_block(nn, input_channel * mlp_ratio, activation=activation, name=name)
+    return add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, axis=-1, name=name + "mlp_")
+
+
+def conv_block(inputs, mlp_ratio=4, layer_scale=0, drop_rate=0, activation="gelu", name=""):
+    input_channel = inputs.shape[-1 if image_data_format() == "channels_last" else 1]
+
+    nn = layers.AvgPool2D(pool_size=3, strides=1, padding="same")(inputs)  # count_include_pad=False [ ??? ]
+    nn = nn - inputs
     attn_out = add_with_layer_scale_and_drop_block(inputs, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "attn_")
 
-    """ MLP """
-    nn = layer_norm(attn_out, epsilon=LAYER_NORM_EPSILON, name=name + "mlp_")
-    nn = mlp_block(nn, input_channel * mlp_ratio, activation=activation, name=name + "mlp_")
-    nn = add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "mlp_")
-    return nn
+    nn = conv2d_no_bias(attn_out, input_channel * mlp_ratio, 1, strides=1, use_bias=True, name=name + "mlp_1_")
+    nn = batchnorm_with_activation(nn, activation=activation, name=name + "mlp_1_")
+    nn = conv2d_no_bias(nn, input_channel, 1, strides=1, use_bias=True, name=name + "mlp_2_")
+    nn = batchnorm_with_activation(nn, activation=None, name=name + "mlp_2_")
+    return add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "mlp_")
 
 
-def InceptionTransformer(
-    num_blocks=[3, 3, 9, 3],
-    embed_dims=[96, 192, 320, 384],
-    num_heads=[3, 6, 10, 12],
-    num_attn_low_heads=[1, 3, [7] * 4 + [9] * 5, 11],
-    pool_sizes=[2, 2, 1, 1],
+def EfficientFormer(
+    num_blocks=[3, 2, 6, 4],
+    out_channels=[48, 96, 224, 448],
     mlp_ratios=4,
+    num_attn_blocks_each_stack=[0, 0, 0, 1],
+    stem_width=-1,
+    stem_activation="relu",
+    layer_scale=1e-5,
     input_shape=(224, 224, 3),
     num_classes=1000,
     activation="gelu",
     drop_connect_rate=0,
+    classifier_activation=None,
+    use_distillation=False,
     dropout=0,
-    layer_scales=[0, 0, 1e-6, 1e-6],
-    classifier_activation="softmax",
     pretrained=None,
-    model_name="iformer",
+    model_name="efficientformer",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+    stem_width = stem_width if stem_width > 0 else out_channels[0]
+    stem_activation = stem_activation if stem_activation is not None else activation
+    nn = conv2d_no_bias(inputs, stem_width // 2, 3, strides=2, use_bias=True, padding="same", name="stem_1_")
+    nn = batchnorm_with_activation(nn, activation=stem_activation, name="stem_1_")
+    nn = conv2d_no_bias(nn, stem_width, 3, strides=2, use_bias=True, padding="same", name="stem_2_")
+    nn = batchnorm_with_activation(nn, activation=stem_activation, name="stem_2_")
 
-    """ Stem """
-    nn = conv2d_no_bias(inputs, embed_dims[0] // 2, kernel_size=3, strides=2, padding="same", use_bias=True, name="stem_1_")
-    nn = batchnorm_with_activation(nn, activation=activation, name="stem_1_")
-    nn = conv2d_no_bias(nn, embed_dims[0], kernel_size=3, strides=2, padding="same", use_bias=True, name="stem_2_")
-    nn = batchnorm_with_activation(nn, activation=None, name="stem_2_")
-
-    """ stacks """
+    """ stages """
     total_blocks = sum(num_blocks)
     global_block_id = 0
-    for stack_id, (num_block, embed_dim) in enumerate(zip(num_blocks, embed_dims)):
+    for stack_id, (num_block, out_channel) in enumerate(zip(num_blocks, out_channels)):
         stack_name = "stack{}_".format(stack_id + 1)
         if stack_id > 0:
-            nn = conv2d_no_bias(nn, embed_dim, 3, strides=2, padding="same", use_bias=True, name=stack_name + "downsample_")
-            nn = batchnorm_with_activation(nn, activation=None, name=stack_name + "downsample_")  # Using epsilon=1e-5
-        nn = PositionalEmbedding(name=stack_name + "positional_embedding")(nn)
-
-        stack_num_attn_low_head = num_attn_low_heads[stack_id] if isinstance(num_attn_low_heads, (list, tuple)) else num_attn_low_heads
-        num_head = num_heads[stack_id] if isinstance(num_heads, (list, tuple)) else num_heads
-        mlp_ratio = mlp_ratios[stack_id] if isinstance(mlp_ratios, (list, tuple)) else mlp_ratios
-        pool_size = pool_sizes[stack_id] if isinstance(pool_sizes, (list, tuple)) else pool_sizes
-        layer_scale = layer_scales[stack_id] if isinstance(layer_scales, (list, tuple)) else layer_scales
+            ds_name = stack_name + "downsample_"
+            nn = conv2d_no_bias(nn, out_channel, kernel_size=3, strides=2, use_bias=True, padding="same", name=ds_name)
+            nn = batchnorm_with_activation(nn, activation=None, name=ds_name)
+
+        cur_mlp_ratios = mlp_ratios[stack_id] if isinstance(mlp_ratios, (list, tuple)) else mlp_ratios
+        cur_num_attn_blocks = num_attn_blocks_each_stack[stack_id] if isinstance(num_attn_blocks_each_stack, (list, tuple)) else num_attn_blocks_each_stack
+        attn_block_start_id = num_block - cur_num_attn_blocks
         for block_id in range(num_block):
-            name = stack_name + "block{}_".format(block_id + 1)
-            num_attn_low_head = stack_num_attn_low_head[block_id] if isinstance(stack_num_attn_low_head, (list, tuple)) else stack_num_attn_low_head
+            cur_name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
-            nn = attention_mlp_block(nn, num_head, num_attn_low_head, pool_size, mlp_ratio, layer_scale, block_drop_rate, activation=activation, name=name)
+            mlp_ratio = cur_mlp_ratios[block_id] if isinstance(cur_mlp_ratios, (list, tuple)) else cur_mlp_ratios
+
+            if block_id >= attn_block_start_id:
+                if block_id == attn_block_start_id:
+                    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)
+                    block_height, block_width = nn.shape[1:-1]
+                    nn = functional.reshape(nn, [-1, block_height * block_width, nn.shape[-1]])  # Using 3D for attention inputs
+
+                nn = attn_block(
+                    nn, attn_height=block_height, mlp_ratio=mlp_ratio, layer_scale=layer_scale, drop_rate=block_drop_rate, activation=activation, name=cur_name
+                )
+
+                if block_id == num_block - 1:
+                    nn = functional.reshape(nn, [-1, block_height, block_width, nn.shape[-1]])  # Revert 3D to 4D
+                    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)
+            else:
+                nn = conv_block(nn, mlp_ratio=mlp_ratio, layer_scale=layer_scale, drop_rate=block_drop_rate, activation=activation, name=cur_name)
             global_block_id += 1
-    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="pre_output_")
 
-    nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
-    model = tf.keras.models.Model(inputs, nn, name=model_name)
+    """ output """
+    if num_classes > 0:
+        nn = layer_norm(nn, name="pre_output_")
+        nn = layers.GlobalAveragePooling2D()(nn)  # tf.reduce_mean(nn, axis=1)
+        if dropout > 0 and dropout < 1:
+            nn = layers.Dropout(dropout)(nn)
+        out = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(nn)
+
+        if use_distillation:
+            distill = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="distill_head")(nn)
+            out = [out, distill]
+    else:
+        out = nn
+
+    model = models.Model(inputs, out, name=model_name)
+    reload_model_weights(model, PRETRAINED_DICT, "efficientformer", pretrained, MultiHeadPositionalEmbedding)
+
     add_pre_post_process(model, rescale_mode="torch")
-    reload_model_weights(model, PRETRAINED_DICT, "iformer", pretrained, PositionalEmbedding)
+    model.switch_to_deploy = lambda: switch_to_deploy(model)
     return model
 
 
-def IFormerSmall(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    return InceptionTransformer(**locals(), model_name="iformer_small", **kwargs)
+def switch_to_deploy(model):
+    from keras_cv_attention_models.model_surgery.model_surgery import fuse_distill_head
+
+    new_model = fuse_distill_head(model, head_bn=None, distill_head_bn=None) if "head" in model.output_names else model
+    add_pre_post_process(new_model, rescale_mode=model.preprocess_input.rescale_mode, post_process=model.decode_predictions)
+    return new_model
+
+
+@register_model
+def EfficientFormerL1(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=False, pretrained="imagenet", **kwargs):
+    return EfficientFormer(**locals(), model_name="efficientformer_l1", **kwargs)
+
+
+@register_model
+def EfficientFormerL3(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=False, pretrained="imagenet", **kwargs):
+    num_blocks = [4, 4, 12, 6]
+    out_channels = [64, 128, 320, 512]
+    num_attn_blocks_each_stack = [0, 0, 0, 4]
+    return EfficientFormer(**locals(), model_name="efficientformer_l3", **kwargs)
 
 
-def IFormerBase(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [4, 6, 14, 6]
-    embed_dims = [96, 192, 384, 512]
-    num_heads = [3, 6, 12, 16]
-    num_attn_low_heads = [1, 3, [8] * 7 + [10] * 7, 15]
-    return InceptionTransformer(**locals(), model_name="iformer_base", **kwargs)
-
-
-def IFormerLarge(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [4, 6, 18, 8]
-    embed_dims = [96, 192, 448, 640]
-    num_heads = [3, 6, 14, 20]
-    num_attn_low_heads = [1, 3, [10] * 9 + [12] * 9, 19]
-    return InceptionTransformer(**locals(), model_name="iformer_large", **kwargs)
+@register_model
+def EfficientFormerL7(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", use_distillation=False, pretrained="imagenet", **kwargs):
+    num_blocks = [6, 6, 18, 8]
+    out_channels = [96, 192, 384, 768]
+    num_attn_blocks_each_stack = [0, 0, 0, 8]
+    return EfficientFormer(**locals(), model_name="efficientformer_l7", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/augment.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/augment.py`

 * *Files 0% similar despite different names*

```diff
@@ -23,15 +23,18 @@
 """
 
 import math
 import tensorflow as tf
 from typing import Any, Dict, List, Optional, Text, Tuple, Union
 
 # from tensorflow.python.keras.layers.preprocessing import image_preprocessing as image_ops
-from keras.layers.preprocessing import image_preprocessing as image_ops
+try:
+    from keras.layers.preprocessing import image_preprocessing as image_ops
+except:
+    from keras.src.layers.preprocessing import image_preprocessing as image_ops  # TF >= 2.13.0
 
 # This signifies the max integer that the controller RNN could predict for the
 # augmentation scheme.
 _MAX_LEVEL = 10.0
 
 
 def to_4d(image: tf.Tensor) -> tf.Tensor:
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/data.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/data.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,27 +1,11 @@
+import os
 import tensorflow as tf
 from tensorflow import keras
-
-
-def init_mean_std_by_rescale_mode(rescale_mode):
-    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std
-        mean, std = rescale_mode
-    elif rescale_mode == "torch":
-        mean = tf.constant([0.485, 0.456, 0.406]) * 255.0
-        std = tf.constant([0.229, 0.224, 0.225]) * 255.0
-    elif rescale_mode == "tf":  # [0, 255] -> [-1, 1]
-        mean, std = 127.5, 127.5
-        # mean, std = 127.5, 128.0
-    elif rescale_mode == "tf128":  # [0, 255] -> [-1, 1]
-        mean, std = 128.0, 128.0
-    elif rescale_mode == "raw01":
-        mean, std = 0, 255.0  # [0, 255] -> [0, 1]
-    else:
-        mean, std = 0, 1  # raw inputs [0, 255]
-    return mean, std
+from keras_cv_attention_models.common_layers import init_mean_std_by_rescale_mode
 
 
 def tf_imread(file_path):
     # tf.print('Reading file:', file_path)
     img = tf.io.read_file(file_path)
     # img = tf.image.decode_jpeg(img, channels=3)  # [0, 255]
     img = tf.image.decode_image(img, channels=3, expand_animations=False)  # [0, 255]
@@ -210,15 +194,15 @@
         print(">>>> cutmix_alpha provided:", cutmix_alpha)
         mix_func = lambda *args: cutmix(*args, alpha=cutmix_alpha)
     else:
         return train_dataset
     return train_dataset.map(mix_func, num_parallel_calls=tf.data.AUTOTUNE)
 
 
-class RandomProcessDatapoint:
+class RandomProcessImage:
     def __init__(
         self,
         target_shape=(224, 224),
         central_crop=1.0,
         random_crop_min=1.0,
         resize_method="bilinear",
         resize_antialias=False,
@@ -265,16 +249,16 @@
             )
 
         if use_token_label:
             from keras_cv_attention_models.imagenet import token_label
 
             self.token_label_align = token_label.TokenLabelAlign(num_classes=num_classes, target_num_pathes=token_label_target_patches)
 
-    def __call__(self, datapoint, token_label=None):
-        image = datapoint["image"]
+    def __call__(self, image, token_label=None):
+        # image = datapoint["image"]
         if len(image.shape) < 2:
             image = tf_imread(image)
         channel = image.shape[-1]
 
         flip_left_right, scale_hh, scale_ww, crop_hh, crop_ww = tf.cast(False, tf.bool), 1, 1, 0, 0  # Init value
         if self.random_crop_min > 0 and self.random_crop_min < 1:
             image, scale_hh, scale_ww, crop_hh, crop_ww = random_crop_and_resize_image(
@@ -293,71 +277,116 @@
             image = self.randaug(image)
         if self.random_erasing_prob > 0:
             image = self.random_erasing(image)
 
         image = tf.cast(image, tf.float32)
         image.set_shape([*self.target_shape[:2], channel])
 
-        label = datapoint["label"]
+        # label = datapoint["label"]
         if self.use_token_label and token_label is not None:
             token_label = self.token_label_align(token_label, flip_left_right, scale_hh, scale_ww, crop_hh, crop_ww)
-            return image, label, token_label
+            return image, token_label
         else:
-            return image, label
+            return image
 
 
-def evaluation_process_crop_resize(datapoint, target_shape=(224, 224), central_crop=1.0, resize_method="bilinear", antialias=False):
-    image = datapoint["image"]
+def evaluation_process_crop_resize(image, target_shape=(224, 224), central_crop=1.0, resize_method="bilinear", antialias=False):
+    # image = datapoint["image"]
     if len(image.shape) < 3:
         image = tf_imread(image)
     if central_crop > 0:  # Do not crop if central_crop == -1
         shape = tf.shape(image)
         height, width = shape[0], shape[1]
         crop_size = tf.cast((central_crop * tf.cast(tf.minimum(height, width), tf.float32)), tf.int32)
         y, x = (height - crop_size) // 2, (width - crop_size) // 2
         image = tf.image.crop_to_bounding_box(image, y, x, crop_size, crop_size)
     image = tf.image.resize(image, target_shape, method=resize_method, antialias=antialias)
-    label = datapoint["label"]
-    return image, label
+    return image
 
 
 # Not using
-def evaluation_process_resize_crop(datapoint, target_shape=(224, 224), central_crop=1.0, resize_method="bilinear", antialias=False):
-    image = datapoint["image"]
+def evaluation_process_resize_crop(image, target_shape=(224, 224), central_crop=1.0, resize_method="bilinear", antialias=False):
+    # image = datapoint["image"]
     if len(image.shape) < 3:
         image = tf_imread(image)
     shape = tf.shape(image)
     height, width = shape[0], shape[1]
     min_border = tf.cast(tf.minimum(height, width), tf.float32)
     scale_size = tf.cast(tf.minimum(*target_shape), tf.float32) / central_crop
     hh_scale = tf.cast(tf.floor(tf.cast(height, tf.float32) * scale_size / min_border), tf.int32)
     ww_scale = tf.cast(tf.floor(tf.cast(width, tf.float32) * scale_size / min_border), tf.int32)
     image = tf.image.resize(image, (hh_scale, ww_scale), method=resize_method, antialias=antialias)
 
     y, x = (hh_scale - target_shape[0]) // 2, (ww_scale - target_shape[1]) // 2
     image = tf.image.crop_to_bounding_box(image, y, x, target_shape[0], target_shape[1])
+    return image
 
-    label = datapoint["label"]
-    return image, label
-
-
-def recognition_dataset_from_custom_json(data_path, with_info=False):
-    import json
-
-    with open(data_path, "r") as ff:
-        aa = json.load(ff)
 
-    test_key = "validation" if "validation" in aa else "test"
-    train, test, info = aa["train"], aa[test_key], aa["info"]
-    total_images, num_classes = len(train), info["num_classes"]
-    num_channels = tf_imread(aa["train"][0]["image"]).shape[-1]
+def init_from_json_or_csv_or_tsv(data_path, is_caption=False):
+    if data_path.endswith(".json"):
+        import json
+
+        with open(data_path, "r") as ff:
+            aa = json.load(ff)
+        test_key = "validation" if "validation" in aa else "test"
+        train, test, info = aa["train"], aa[test_key], aa.get("info", {})
+    else:
+        import csv
 
-    output_signature = {"image": tf.TensorSpec(shape=(), dtype=tf.string), "label": tf.TensorSpec(shape=(), dtype=tf.int64)}
-    train_ds = tf.data.Dataset.from_generator(lambda: (ii for ii in train), output_signature=output_signature)
-    test_ds = tf.data.Dataset.from_generator(lambda: (ii for ii in test), output_signature=output_signature)
+        delimiter = "\t" if data_path.endswith(".tsv") else ","
+        label_key = "caption" if is_caption else "label"
+        train, test, info, is_train = [], [], {}, True
+        with open(data_path) as ff:
+            for ii in csv.reader(ff, delimiter=delimiter):
+                if ii[0] in ["base_path", "num_classes"]:  # special keys for info
+                    info[ii[0]] = ii[1]
+                    continue
+
+                if ii[0] == "TEST":  # Use this as indicator for start of test set
+                    is_train = False
+                elif is_train:
+                    train.append({"image": ii[0], label_key: ii[1]})
+                else:
+                    test.append({"image": ii[0], label_key: ii[1]})
+        test_key = "test"
+
+    """ Construct more info """
+    total_images, num_classes = len(train), info.get("num_classes", 0)
+    if not is_caption and num_classes <= 0 and "label" in train[0] and isinstance(train[0]["label"], int):
+        num_classes = max([ii["label"] for ii in train]) + 1
+        print(">>>> Using max value from train as num_classes:", num_classes)
+
+    if "base_path" in info and len(info["base_path"]) > 0:
+        base_path = os.path.expanduser(info["base_path"])
+        for ii in train:
+            ii["image"] = os.path.join(base_path, ii["image"])
+        for ii in test:
+            ii["image"] = os.path.join(base_path, ii["image"])
+    num_channels = tf_imread(train[0]["image"]).shape[-1]
+    return (train, test, info, test_key), (total_images, num_classes, num_channels)
+
+
+def build_custom_dataset(data_path, with_info=False, info_only=False, caption_tokenizer=None):
+    is_caption = False if caption_tokenizer is None else True
+    (train, test, info, test_key), (total_images, num_classes, num_channels) = init_from_json_or_csv_or_tsv(data_path, is_caption)
+
+    if is_caption:
+        # from tqdm import tqdm
+        # train = [{"image": ii["image"], "caption": caption_tokenizer(ii["caption"])} for ii in tqdm(train, "Tokenizing train caption")]
+        # test = [{"image": ii["image"], "caption": caption_tokenizer(ii["caption"])} for ii in tqdm(test, "Tokenizing test caption")]
+        context_length = caption_tokenizer.context_length
+        output_signature = {"image": tf.TensorSpec(shape=(), dtype=tf.string), "caption": tf.TensorSpec(shape=(context_length,), dtype=tf.int64)}
+        train_gen = lambda: ({"image": ii["image"], "caption": caption_tokenizer(ii["caption"])} for ii in train)
+        test_gen = lambda: ({"image": ii["image"], "caption": caption_tokenizer(ii["caption"])} for ii in test)
+    else:
+        output_signature = {"image": tf.TensorSpec(shape=(), dtype=tf.string), "label": tf.TensorSpec(shape=(), dtype=tf.int64)}
+        train_gen = lambda: (ii for ii in train)
+        test_gen = lambda: (ii for ii in test)
+    train_ds = tf.data.Dataset.from_generator(train_gen, output_signature=output_signature)
+    test_ds = tf.data.Dataset.from_generator(test_gen, output_signature=output_signature)
 
     options = tf.data.Options()
     options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
     train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(len(train))).with_options(options)
     test_ds = test_ds.apply(tf.data.experimental.assert_cardinality(len(test))).with_options(options)
     dataset = {"train": train_ds, test_key: test_ds}
     return (dataset, total_images, num_classes, num_channels) if with_info else dataset
@@ -421,65 +450,72 @@
     seed=None,
     token_label_file=None,
     token_label_target_patches=-1,
     teacher_model=None,
     teacher_model_input_shape=-1,  # -1 means same with input_shape
     **augment_kwargs,  # Too many...
 ):
-    import tensorflow_datasets as tfds
-
     # print(">>>> Dataset args:", locals())
     is_tpu = True if len(tf.config.list_logical_devices("TPU")) > 0 else False  # Set True for try_gcs and drop_remainder
+    try_gcs, drop_remainder = is_tpu, is_tpu
     use_token_label = False if token_label_file is None else True
     use_distill = False if teacher_model is None else True
     teacher_model_input_shape = input_shape if teacher_model_input_shape == -1 else teacher_model_input_shape
 
-    if data_name.endswith(".json"):
-        dataset, total_images, num_classes, num_channels = recognition_dataset_from_custom_json(data_name, with_info=True)
+    if data_name.endswith(".json") or data_name.endswith(".tsv"):
+        if info_only:
+            _, (total_images, num_classes, num_channels) = init_from_json_or_csv_or_tsv(data_name)
+        else:
+            dataset, total_images, num_classes, num_channels = build_custom_dataset(data_name, with_info=True)
     else:
-        dataset, info = tfds.load(data_name, with_info=True, try_gcs=is_tpu)
+        import tensorflow_datasets as tfds
+
+        dataset, info = tfds.load(data_name, with_info=True, try_gcs=try_gcs)
         num_classes = info.features["label"].num_classes
         num_channels = info.features["image"].shape[-1]
         total_images = info.splits["train"].num_examples
     steps_per_epoch = int(tf.math.ceil(total_images / float(batch_size)))
-
     if info_only:
         return total_images, num_classes, steps_per_epoch, num_channels  # return num_channels in case it's not 3
 
-    """ Train dataset """
-    train_dataset = dataset["train"]
-    if use_token_label:
-        train_dataset = build_token_label_dataset(train_dataset, token_label_file)
-
+    mean, std = init_mean_std_by_rescale_mode(rescale_mode)
     AUTOTUNE = tf.data.AUTOTUNE
-    train_pre_batch = RandomProcessDatapoint(
+
+    """ Train dataset functions """
+    train_image_func = RandomProcessImage(
         target_shape=teacher_model_input_shape if use_distill else input_shape,
         central_crop=-1,  # Resize directly w/o crop, if random_crop_min not in (0, 1)
         random_crop_min=random_crop_min,
         resize_method=resize_method,
         resize_antialias=resize_antialias,
         random_erasing_prob=random_erasing_prob,
         magnitude=magnitude,
         num_layers=num_layers,
         use_positional_related_ops=use_positional_related_ops,
         use_token_label=use_token_label,
         token_label_target_patches=token_label_target_patches,
         num_classes=num_classes,
         **augment_kwargs,
     )
-    if use_shuffle:
-        train_dataset = train_dataset.shuffle(buffer_size, seed=seed)
-    train_dataset = train_dataset.map(train_pre_batch, num_parallel_calls=AUTOTUNE).batch(batch_size, drop_remainder=is_tpu)
 
-    mean, std = init_mean_std_by_rescale_mode(rescale_mode)
     if use_token_label:
-        train_post_batch = lambda xx, yy, token_label: ((xx - mean) / std, tf.one_hot(yy, num_classes), token_label)
+        train_pre_batch = lambda data_point: (*train_image_func(data_point["image"]), data_point["label"])
+        train_post_batch = lambda xx, token_label, yy: ((xx - mean) / std, tf.one_hot(yy, num_classes), token_label)
     else:
+        train_pre_batch = lambda data_point: (train_image_func(data_point["image"]), data_point["label"])
         train_post_batch = lambda xx, yy: ((xx - mean) / std, tf.one_hot(yy, num_classes))
-    train_dataset = train_dataset.map(train_post_batch, num_parallel_calls=AUTOTUNE)
+
+    """ Train dataset """
+    train_dataset = dataset["train"]
+    if use_token_label:
+        train_dataset = build_token_label_dataset(train_dataset, token_label_file)
+    if use_shuffle:
+        train_dataset = train_dataset.shuffle(buffer_size, seed=seed)
+    train_dataset = train_dataset.map(train_pre_batch, num_parallel_calls=AUTOTUNE)
+    train_dataset = train_dataset.batch(batch_size, drop_remainder=drop_remainder).map(train_post_batch, num_parallel_calls=AUTOTUNE)
     train_dataset = apply_mixup_cutmix(train_dataset, mixup_alpha, cutmix_alpha, switch_prob=0.5)
 
     if use_token_label:
         train_dataset = train_dataset.map(lambda xx, yy, token_label: (xx, (yy, token_label)))
     elif use_distill:
         print(">>>> KLDivergence teacher model provided.")
         train_dataset = build_distillation_dataset(train_dataset, teacher_model, input_shape)
@@ -487,54 +523,70 @@
     train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
     # return train_dataset
 
     """ Test dataset """
     test_dataset = dataset.get("validation", dataset.get("test", None))
     if test_dataset is not None:
         # test_pre_batch = lambda xx: evaluation_process_resize_crop(xx, input_shape[:2], eval_central_crop, resize_method, resize_antialias)  # timm
-        test_pre_batch = lambda xx: evaluation_process_crop_resize(xx, input_shape[:2], eval_central_crop, resize_method, resize_antialias)
-        test_dataset = test_dataset.map(test_pre_batch, num_parallel_calls=AUTOTUNE)
-        # Have to drop_remainder also for test set...
-        test_dataset = test_dataset.batch(batch_size, drop_remainder=is_tpu)
+        # test_image_func = lambda xx: evaluation_process_crop_resize(xx, input_shape[:2], eval_central_crop, resize_method, resize_antialias)
+        test_pre_batch = lambda data_point: (
+            evaluation_process_crop_resize(data_point["image"], input_shape[:2], eval_central_crop, resize_method, resize_antialias),
+            data_point["label"],
+        )
         if use_token_label:
             test_post_batch = lambda xx, yy: ((xx - mean) / std, (tf.one_hot(yy, num_classes), None))  # just give None on token_label data position
         elif use_distill:
             test_post_batch = lambda xx, yy: ((xx - mean) / std, (tf.one_hot(yy, num_classes), None))
         else:
             test_post_batch = lambda xx, yy: ((xx - mean) / std, tf.one_hot(yy, num_classes))
-        test_dataset = test_dataset.map(test_post_batch)
+
+        test_dataset = test_dataset.map(test_pre_batch, num_parallel_calls=AUTOTUNE)
+        # Have to drop_remainder also for test set...
+        test_dataset = test_dataset.batch(batch_size, drop_remainder=drop_remainder).map(test_post_batch)
     return train_dataset, test_dataset, total_images, num_classes, steps_per_epoch
 
 
 """ Show """
 
 
-def show_batch_sample(dataset, rescale_mode="tf", rows=-1, base_size=3):
-    from keras_cv_attention_models import visualizing
+def show_batch_sample(dataset, rescale_mode="tf", rows=-1, caption_tokenizer=None, base_size=3, indices_2_labels=None):
+    from keras_cv_attention_models import plot_func
+    from keras_cv_attention_models.imagenet.eval_func import decode_predictions
 
     if isinstance(dataset, (list, tuple)):
         images, labels = dataset
-    elif isinstance(dataset.element_spec[1], tuple):
+    elif isinstance(dataset.element_spec[0], tuple):  # caption datasets
+        (images, labels), _ = dataset.as_numpy_iterator().next()
+    elif isinstance(dataset.element_spec[1], tuple):  # token_label datasets
         images, (labels, token_label) = dataset.as_numpy_iterator().next()
     else:
         images, labels = dataset.as_numpy_iterator().next()
+
+    if caption_tokenizer is not None:
+        labels = [caption_tokenizer(ii) for ii in labels]
+
     mean, std = init_mean_std_by_rescale_mode(rescale_mode)
     mean, std = (mean.numpy(), std.numpy()) if hasattr(mean, "numpy") else (mean, std)
     images = (images * std + mean) / 255
 
-    if tf.shape(labels)[-1] == 1000:
-        labels = [ii[0][1] for ii in keras.applications.imagenet_utils.decode_predictions(labels, top=1)]
+    if isinstance(labels[0], str):
+        pass  # caption datasets
+    elif tf.shape(labels)[-1] == 1000:
+        labels = [ii[0][1] for ii in decode_predictions(labels, top=1)]
     elif tf.rank(labels[0]) == 1:
         labels = tf.argmax(labels, axis=-1).numpy()  # If 2 dimension
-    ax, _ = visualizing.stack_and_plot_images(images, texts=labels, rows=rows, ax=None, base_size=base_size)
+
+    if not isinstance(labels[0], str) and indices_2_labels is not None:
+        labels = [indices_2_labels.get(label, indices_2_labels.get(str(label), str(label))) for label in labels]
+    ax, _ = plot_func.stack_and_plot_images(images, texts=labels, rows=rows, ax=None, base_size=base_size)
     return ax
 
 
 def show_token_label_patches_single(image, token_label, rescale_mode="tf", top_k=3, resize_patch_shape=(160, 160)):
-    from keras_cv_attention_models import visualizing
+    from keras_cv_attention_models import plot_func
 
     mean, std = init_mean_std_by_rescale_mode(rescale_mode)
     mean, std = (mean.numpy(), std.numpy()) if hasattr(mean, "numpy") else (mean, std)
     image = (image * std + mean) / 255
 
     height, width = image.shape[:2]
     num_height_patch, num_width_patch = token_label.shape[0], token_label.shape[1]
@@ -548,8 +600,8 @@
         hh_image = image[hh_id * height_patch : (hh_id + 1) * height_patch]
         for ww_id in range(num_width_patch):
             image_patch = hh_image[:, ww_id * width_patch : (ww_id + 1) * width_patch]
             image_pathes.append(tf.image.resize(image_patch, resize_patch_shape).numpy())
             scores = ",".join(["{:.1f}".format(ii * 100) for ii in token_label_scores[hh_id, ww_id]])
             classes = ",".join(["{:d}".format(ii) for ii in token_label_classes[hh_id, ww_id].astype("int")])
             labels.append(classes + "\n" + scores)
-    visualizing.stack_and_plot_images(image_pathes, labels)
+    plot_func.stack_and_plot_images(image_pathes, labels)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/losses.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/losses.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/token_label.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/token_label.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/imagenet/train_func.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/imagenet/train_func.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,47 @@
 import os
 import time
 import keras_cv_attention_models
-import tensorflow as tf
-from tensorflow import keras
-from keras_cv_attention_models.imagenet import callbacks, losses
-from keras_cv_attention_models import model_surgery
+from keras_cv_attention_models import backend, model_surgery
+from keras_cv_attention_models.backend import layers, models
+from keras_cv_attention_models.imagenet import callbacks
 
 GLOBAL_STRATEGY = None
 
 
+def set_random_seed(seed):
+    import random
+    import numpy as np
+
+    print(">>>> Set random seed:", seed)
+    random.seed(seed)
+    np.random.seed(seed)
+    os.environ["PYTHONHASHSEED"] = str(seed)  # Set a fixed value for the hash seed
+
+    if backend.is_torch_backend:
+        import torch
+
+        torch.manual_seed(seed)
+        torch.cuda.manual_seed(seed)
+        # When running on the CuDNN backend, two further options must be set
+        torch.backends.cudnn.deterministic = True
+        torch.backends.cudnn.benchmark = False
+    else:
+        import tensorflow as tf
+
+        tf.random.set_seed(seed)
+        tf.experimental.numpy.random.seed(seed)
+        # When running on the CuDNN backend, two further options must be set
+        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
+        os.environ['TF_DETERMINISTIC_OPS'] = '1'
+
+
 def init_global_strategy(enable_float16=True, seed=0, TPU=False):
+    import tensorflow as tf
+
     global GLOBAL_STRATEGY
     if GLOBAL_STRATEGY is not None:
         return GLOBAL_STRATEGY
 
     gpus = tf.config.experimental.get_visible_devices("GPU")
     for gpu in gpus:
         tf.config.experimental.set_memory_growth(gpu, True)
@@ -29,80 +57,112 @@
         strategy = tf.distribute.MirroredStrategy()
     else:
         strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0")
     GLOBAL_STRATEGY = strategy
 
     if enable_float16:
         policy = "mixed_bfloat16" if TPU else "mixed_float16"
-        keras.mixed_precision.set_global_policy(policy)
+        tf.keras.mixed_precision.set_global_policy(policy)
 
     if seed is not None:
-        print(">>>> Set random seed:", seed)
-        tf.random.set_seed(seed)
+        set_random_seed(seed)
     return strategy
 
 
 def init_lr_scheduler(lr_base, lr_decay_steps, lr_min=1e-5, lr_decay_on_batch=False, lr_warmup=1e-4, warmup_steps=0, cooldown_steps=0, t_mul=2, m_mul=0.5):
     if isinstance(lr_decay_steps, list):
         constant_lr_sch = lambda epoch: callbacks.constant_scheduler(epoch, lr_base=lr_base, lr_decay_steps=lr_decay_steps, warmup_steps=warmup_steps)
-        lr_scheduler = keras.callbacks.LearningRateScheduler(constant_lr_sch)
+        lr_scheduler = callbacks.LearningRateScheduler(constant_lr_sch)
         lr_total_epochs = lr_decay_steps[-1] + cooldown_steps  # 120 for lr_decay_steps=[30, 60, 90], warmup_steps=4, cooldown_steps=30
     elif lr_decay_on_batch:
         lr_scheduler = callbacks.CosineLrScheduler(
             lr_base, lr_decay_steps, m_mul=m_mul, t_mul=t_mul, lr_min=lr_min, lr_warmup=lr_warmup, warmup_steps=warmup_steps, cooldown_steps=cooldown_steps
         )
         lr_total_epochs = lr_decay_steps + cooldown_steps  # 105 for lr_decay_steps=100, warmup_steps=4, cooldown_steps=5
     else:
         lr_scheduler = callbacks.CosineLrSchedulerEpoch(
             lr_base, lr_decay_steps, m_mul=m_mul, t_mul=t_mul, lr_min=lr_min, lr_warmup=lr_warmup, warmup_steps=warmup_steps, cooldown_steps=cooldown_steps
         )
         lr_total_epochs = lr_decay_steps + cooldown_steps  # 105 for lr_decay_steps=100, warmup_steps=4, cooldown_steps=5
     return lr_scheduler, lr_total_epochs
 
 
-def init_optimizer(optimizer, lr_base, weight_decay, momentum=0.9):
-    import tensorflow_addons as tfa
+def _init_tf_buildin_optimizer_(optimizer_class, lr_base, weight_decay, no_weight_decay, momentum=0.9, **kwargs):
+    import inspect
+
+    is_weight_decay_supported = "weight_decay" in inspect.signature(optimizer_class).parameters  # > TF1.11
+    if is_weight_decay_supported:
+        kwargs.update({"weight_decay": weight_decay})
+    if "momentum" in inspect.signature(optimizer_class).parameters:  # SGD / RMSprop
+        kwargs.update({"momentum": momentum})
+    print(">>>> optimizer kwargs:", kwargs)
+    optimizer = optimizer_class(learning_rate=lr_base, **kwargs)
+
+    if is_weight_decay_supported:
+        optimizer.exclude_from_weight_decay(var_names=no_weight_decay)
+    return optimizer
+
 
+def init_optimizer(optimizer, lr_base, weight_decay, momentum=0.9):
     optimizer = optimizer.lower()
+    buildin_optimizers = {
+        # key: (class, kwargs)
+        "sgd": (backend.optimizers.SGD, {}),
+        "rmsprop": (backend.optimizers.RMSprop, {}),
+        "adam": (backend.optimizers.Adam, {}),
+        "custom": (getattr(backend.optimizers, "AdamW", None), {"beta_1": 0.9, "beta_2": 0.98, "epsilon": 1e-6, "global_clipnorm": 10.0}),  # For clip
+    }
+    if hasattr(backend.optimizers, "AdamW"):
+        buildin_optimizers.update({"adamw": (backend.optimizers.AdamW, {})})
     # norm_weights = ["bn/gamma", "bn/beta", "ln/gamma", "ln/beta", "/positional_embedding", "/bias"]  # ["bn/moving_mean", "bn/moving_variance"] not in weights
     no_weight_decay = ["/gamma", "/beta", "/bias", "/positional_embedding", "/no_weight_decay"]  # ["bn/moving_mean", "bn/moving_variance"] not in weights
-    if optimizer == "sgd":
-        optimizer = keras.optimizers.SGD(learning_rate=lr_base, momentum=momentum)
-    elif optimizer == "rmsprop":
-        optimizer = keras.optimizers.RMSprop(learning_rate=lr_base, momentum=momentum)
+
+    if optimizer in buildin_optimizers:
+        optimizer_class, kwargs = buildin_optimizers[optimizer]
+        optimizer = _init_tf_buildin_optimizer_(optimizer_class, lr_base, weight_decay, no_weight_decay, momentum, **kwargs)
     elif optimizer == "lamb":
+        import tensorflow_addons as tfa
+
         optimizer = tfa.optimizers.LAMB(learning_rate=lr_base, weight_decay_rate=weight_decay, exclude_from_weight_decay=no_weight_decay, global_clipnorm=1.0)
     elif optimizer == "adamw":
+        import tensorflow_addons as tfa
+
         optimizer = tfa.optimizers.AdamW(learning_rate=lr_base, weight_decay=lr_base * weight_decay, global_clipnorm=1.0)
         if hasattr(optimizer, "exclude_from_weight_decay"):
             setattr(optimizer, "exclude_from_weight_decay", no_weight_decay)
     elif optimizer == "sgdw":
+        import tensorflow_addons as tfa
+
         optimizer = tfa.optimizers.SGDW(learning_rate=lr_base, momentum=momentum, weight_decay=lr_base * weight_decay)
         if hasattr(optimizer, "exclude_from_weight_decay"):
             setattr(optimizer, "exclude_from_weight_decay", no_weight_decay)
     else:
-        optimizer = getattr(keras.optimizers, optimizer.capitalize())(learning_rate=lr_base)
+        optimizer = getattr(backend.optimizers, optimizer.capitalize())(learning_rate=lr_base)
+
     return optimizer
 
 
 def is_decoupled_weight_decay(optimizer):
-    import tensorflow_addons as tfa
+    # import tensorflow_addons as tfa
 
-    optimizer = optimizer.inner_optimizer if isinstance(optimizer, keras.mixed_precision.LossScaleOptimizer) else optimizer
-    return isinstance(optimizer, tfa.optimizers.weight_decay_optimizers.DecoupledWeightDecayExtension)
+    optimizer = optimizer.inner_optimizer if hasattr(optimizer, "inner_optimizer") else optimizer
+    # return isinstance(optimizer.__module__, tfa.optimizers.weight_decay_optimizers.DecoupledWeightDecayExtension)
+    return optimizer.__module__ == "tensorflow_addons.optimizers.weight_decay_optimizers"
 
 
 def init_loss(bce_threshold=1.0, label_smoothing=0, token_label_loss_weight=0, distill_loss_weight=0, distill_temperature=10, model_output_names=[]):
+    from keras_cv_attention_models.imagenet import losses
+
     from_logits = True if distill_loss_weight > 0 else False  # classifier_activation is set to None for distill model, set from_logits=True for distill
     if bce_threshold >= 0 and bce_threshold < 1:
         cls_loss = losses.BinaryCrossEntropyTimm(target_threshold=bce_threshold, label_smoothing=label_smoothing, from_logits=from_logits)
         aux_loss = losses.BinaryCrossEntropyTimm(target_threshold=bce_threshold, label_smoothing=label_smoothing) if token_label_loss_weight > 0 else None
     else:
-        cls_loss = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing, from_logits=from_logits)
-        aux_loss = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing) if token_label_loss_weight > 0 else None
+        cls_loss = backend.losses.CategoricalCrossentropy(label_smoothing=label_smoothing, from_logits=from_logits)
+        aux_loss = backend.losses.CategoricalCrossentropy(label_smoothing=label_smoothing) if token_label_loss_weight > 0 else None
 
     if token_label_loss_weight > 0:
         loss = [cls_loss, aux_loss]
         loss_weights = [1, token_label_loss_weight]
         metrics = {model_output_names[0]: "acc", model_output_names[1]: None}
     elif distill_loss_weight > 0:
         distill_loss = losses.DistillKLDivergenceLoss(temperature=distill_temperature)
@@ -117,44 +177,51 @@
 
 def init_model(model=None, input_shape=(224, 224, 3), num_classes=1000, pretrained=None, reload_compile=True, **kwargs):
     """model Could be:
     1. Saved h5 model path.
     2. Model name defined in this repo, format [sub_dir].[model_name] like regnet.RegNetZD8.
     3. timm model like timm.models.resmlp_12_224
     """
-    if isinstance(model, keras.models.Model):
-        print(">>> Got a keras.models.Model: {}, do nothing with it.".format(model.name))
+    if isinstance(model, models.Model):
+        print(">>> Got a models.Model: {}, do nothing with it.".format(model.name))
         return model
 
     if model.startswith("timm."):  # model like: timm.models.resmlp_12_224
         import timm
         from keras_cv_attention_models.imagenet.eval_func import TorchModelInterf
 
         print(">>>> Timm model provided:", model)
         timm_model_name = ".".join(model.split(".")[2:])
         model = getattr(timm.models, timm_model_name)(pretrained=True, img_size=input_shape[:2], num_classes=num_classes)
         return TorchModelInterf(model)
 
     if model.endswith(".h5"):
-        import tensorflow_addons as tfa
+        try:
+            import tensorflow_addons as tfa
+        except:
+            pass
 
         print(">>>> Restore model from:", model)
-        model = keras.models.load_model(model, compile=reload_compile)
+        model = models.load_model(model, compile=reload_compile)
         return model
 
     if input_shape != -1:
         kwargs.update({"input_shape": input_shape})  # Use model default input_shape if not specified
-    print(">>>> init_model kwargs:", kwargs)
+    if num_classes != -1:
+        kwargs.update({"num_classes": num_classes})  # Use model default input_shape if not specified
+    if pretrained != "default":
+        kwargs.update({"pretrained": pretrained})  # Use model default input_shape if not specified
+    print(">>>> init_model kwargs:", {kk: getattr(vv, "name", vv) if isinstance(vv, models.Model) else vv for kk, vv in kwargs.items()})
 
     model_name = model.strip().split(".")
     if len(model_name) == 1:
-        model = getattr(keras.applications, model_name[0])(classes=num_classes, weights=pretrained, **kwargs)
+        model = getattr(keras_cv_attention_models.models, model_name[0])(**kwargs)
     else:
         model_class = getattr(getattr(keras_cv_attention_models, model_name[0]), model_name[1])
-        model = model_class(num_classes=num_classes, pretrained=pretrained, **kwargs)
+        model = model_class(**kwargs)
     print(">>>> Built model name:", model.name)
 
     if model_name[0] == "aotnet" and pretrained is not None and pretrained.endswith(".h5"):
         # Currently aotnet not loading from pretrained...
         print(">>>> Load pretrained from:", pretrained)
         model.load_weights(pretrained, by_name=True, skip_mismatch=True)
     return model
@@ -164,48 +231,54 @@
     if freeze_backbone:
         pool_layer_id = model_surgery.get_global_avg_pool_layer_id(model)
         for id in range(pool_layer_id):
             model.layers[id].trainable = False
 
     if freeze_norm_layers:
         for ii in model.layers:
-            if isinstance(ii, keras.layers.BatchNormalization) or isinstance(ii, keras.layers.LayerNormalization):
+            if isinstance(ii, layers.BatchNormalization) or isinstance(ii, layers.LayerNormalization):
                 ii.trainable = False
 
     if use_token_label and model.optimizer is None:  # model.optimizer is not None if restored from h5
         model = model_surgery.convert_to_token_label_model(model)
     return model
 
 
 def init_distill_model(model, teacher_model):
     if hasattr(teacher_model, "layers") and hasattr(teacher_model.layers[-1], "activation"):
         teacher_model.layers[-1].activation = None  # Set output activation softmax to linear
     teacher_model.trainable = False
 
     model.layers[-1].activation = None  # Also set model output activation softmax to linear
     if model.optimizer is None:  # model.optimizer is not None if restored from h5
-        model = keras.models.Model(model.inputs[0], [model.output, model.output])
+        model = models.Model(model.inputs[0], [model.output, model.output])
         model.output_names[1] = "distill"
     return model, teacher_model
 
 
 def compile_model(model, optimizer, lr_base, weight_decay, loss, loss_weights=None, metrics=["acc"], momentum=0.9):
     if isinstance(optimizer, str):
         optimizer = optimizer.lower()
-        if optimizer == "sgd" and weight_decay > 0:
-            # Add L2 regularizer
-            model = model_surgery.add_l2_regularizer_2_model(model, weight_decay=weight_decay, apply_to_batch_normal=False)
+        # if optimizer == "sgd" and weight_decay > 0:
+        #     Add L2 regularizer
+        #     model = model_surgery.add_l2_regularizer_2_model(model, weight_decay=weight_decay, apply_to_batch_normal=False)
         optimizer = init_optimizer(optimizer, lr_base, weight_decay, momentum=momentum)
     print(">>>> Loss: {}, Optimizer: {}".format(loss.__class__.__name__, optimizer.__class__.__name__))
     model.compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights, metrics=metrics)
+
+    if not hasattr(model.optimizer, "_variables") and hasattr(model.optimizer, "_optimizer") and hasattr(model.optimizer._optimizer, "_variables"):
+        # Bypassing TF 2.11 error AttributeError: 'LossScaleOptimizerV3' object has no attribute '_variables'
+        setattr(model.optimizer, "_variables", model.optimizer._optimizer._variables)
     return model
 
 
 # @tf.function(jit_compile=True)
-def train(compiled_model, epochs, train_dataset, test_dataset=None, initial_epoch=0, lr_scheduler=None, basic_save_name=None, init_callbacks=[], logs="auto"):
+def train(
+    compiled_model, epochs, train_dataset, test_dataset=None, initial_epoch=0, lr_scheduler=None, basic_save_name=None, init_callbacks=[], logs="auto", **kwargs
+):
     if compiled_model.compiled_loss is None:
         print(">>>> Error: Model NOT compiled.")
         return None
 
     steps_per_epoch = len(train_dataset)
     if hasattr(lr_scheduler, "steps_per_epoch") and lr_scheduler.steps_per_epoch == -1:
         lr_scheduler.build(steps_per_epoch)
@@ -219,18 +292,18 @@
     checkpoint_callback = callbacks.MyCheckpoint(basic_save_name, monitor="val_acc")
     cur_callbacks = [checkpoint_callback] + init_callbacks
     hist_file = os.path.join("checkpoints", basic_save_name + "_hist.json")
     if initial_epoch == 0 and os.path.exists(hist_file):
         # os.remove(hist_file)
         os.rename(hist_file, hist_file + ".bak")
     cur_callbacks.append(callbacks.MyHistory(initial_file=hist_file))
-    cur_callbacks.append(keras.callbacks.TerminateOnNaN())
+    cur_callbacks.append(backend.callbacks.TerminateOnNaN())
     if logs is not None:
         logs = "logs/" + basic_save_name + "_" + time.strftime("%Y%m%d-%H%M%S") if logs == "auto" else logs
-        cur_callbacks.append(keras.callbacks.TensorBoard(log_dir=logs, histogram_freq=1))
+        cur_callbacks.append(backend.callbacks.TensorBoard(log_dir=logs, histogram_freq=1))
         print(">>>> TensorBoard log path:", logs)
 
     if lr_scheduler is not None:
         cur_callbacks.append(lr_scheduler)
 
     if lr_scheduler is not None and is_decoupled_weight_decay(compiled_model.optimizer):
         print(">>>> Append weight decay callback...")
@@ -244,12 +317,13 @@
         verbose=1,
         callbacks=cur_callbacks,
         initial_epoch=initial_epoch,
         steps_per_epoch=steps_per_epoch,
         validation_data=test_dataset,
         use_multiprocessing=True,
         workers=8,
+        **kwargs,
     )
 
     if logs is not None:
         print(">>>> TensorBoard log path:", logs)
     return checkpoint_callback.latest_save, hist
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/levit/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/levit/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -4,14 +4,15 @@
     LeViT128,
     LeViT192,
     LeViT256,
     LeViT384,
     MultiHeadPositionalEmbedding,
     mhsa_with_multi_head_position_and_strides,
     mhsa_with_multi_head_position,
+    switch_to_deploy,
 )
 
 
 __head_doc__ = """
 Keras implementation of [Github facebookresearch/LeViT](https://github.com/facebookresearch/LeViT).
 Paper [PDF 2104.01136 LeViT: a Vision Transformer in ConvNets Clothing for Faster Inference](https://arxiv.org/pdf/2104.01136.pdf).
 """
@@ -24,15 +25,15 @@
       or a tuple value like `(0, 0.2)` indicates the drop probability linearly changes from `0 --> 0.2` for `top --> bottom` layers.
       A higher value means a higher probability will drop the deep branch.
       or `0` to disable (default).
   dropout: top dropout rate if top layers is included. Default 0.
   classifier_activation: A `str` or callable. The activation function to use on the "top" layer if `num_classes > 0`.
       Set `classifier_activation=None` to return the logits of the "top" layer.
       Default is `None`.
-  use_distillation: Boolean value if output `distill_head`. Default `True`.
+  use_distillation: Boolean value if output `distill_head`. Default `False`.
   pretrained: one of `None` (random initialization) or 'imagenet' (pre-training on ImageNet).
       Will try to download and load pre-trained model weights if not None.
   **kwargs: other parameters if available.
 
 Returns:
     A `keras.Model` instance.
 """
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/levit/levit.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/levit/levit.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,196 +1,208 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import math
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
-from keras_cv_attention_models.attention_layers import batchnorm_with_activation, conv2d_no_bias, activation_by_name, add_pre_post_process
+from keras_cv_attention_models.attention_layers import (
+    activation_by_name,
+    batchnorm_with_activation,
+    conv2d_no_bias,
+    scaled_dot_product_attention,
+    add_pre_post_process,
+)
 
 
 PRETRAINED_DICT = {
     "levit128s": {"imagenet": "5e35073bb6079491fb0a1adff833da23"},
     "levit128": {"imagenet": "730c100fa4d5a10cf48fb923bb7da5c3"},
     "levit192": {"imagenet": "b078d2fe27857d0bdb26e101703210e2"},
     "levit256": {"imagenet": "9ada767ba2798c94aa1c894a00ae40fd"},
     "levit384": {"imagenet": "520f207f7f4c626b83e21564dd0c92a3"},
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="levit")
-class MultiHeadPositionalEmbedding(keras.layers.Layer):
+@backend.register_keras_serializable(package="levit")
+class MultiHeadPositionalEmbedding(layers.Layer):
     def __init__(self, query_height=-1, key_height=-1, **kwargs):
         super(MultiHeadPositionalEmbedding, self).__init__(**kwargs)
         self.query_height, self.key_height = query_height, key_height
 
     def build(self, input_shape, **kwargs):
-        _, num_heads, qq_blocks, kk_blocks = input_shape
-        self.bb = self.add_weight(name="positional_embedding", shape=(kk_blocks, num_heads), initializer="zeros", trainable=True)
+        if len(input_shape) == 3:
+            _, qq_blocks, kk_blocks = input_shape
+            self.bb = self.add_weight(name="positional_embedding", shape=(kk_blocks,), initializer="zeros", trainable=True)
+            self.output_perm = None
+        else:
+            _, num_heads, qq_blocks, kk_blocks = input_shape
+            self.bb = self.add_weight(name="positional_embedding", shape=(kk_blocks, num_heads), initializer="zeros", trainable=True)
+            self.output_perm = [2, 0, 1]
 
         if self.query_height == -1:
-            q_blocks_h = q_blocks_w = int(tf.math.sqrt(float(qq_blocks)))  # hh == ww
+            q_blocks_h = q_blocks_w = int(float(qq_blocks) ** 0.5)  # hh == ww
         else:
             q_blocks_h, q_blocks_w = self.query_height, int(qq_blocks / self.query_height)
 
-        strides = int(tf.math.ceil(tf.math.sqrt(float(kk_blocks / qq_blocks))))
+        strides = int(math.ceil(float(kk_blocks / qq_blocks) ** 0.5))
         if self.key_height == -1:
             k_blocks_h = q_blocks_h * strides
             while kk_blocks % k_blocks_h != 0:
                 k_blocks_h -= 1
             k_blocks_w = int(kk_blocks / k_blocks_h)
         else:
             k_blocks_h, k_blocks_w = self.key_height, int(kk_blocks / self.key_height)
         self.k_blocks_h, self.k_blocks_w = k_blocks_h, k_blocks_w
         # print(f"{q_blocks_h = }, {q_blocks_w = }, {k_blocks_h = }, {k_blocks_w = }, {strides = }")
 
-        x1, y1 = tf.meshgrid(range(q_blocks_h), range(q_blocks_w))
-        x2, y2 = tf.meshgrid(range(k_blocks_h), range(k_blocks_w))
-        aa = tf.concat([tf.reshape(x1, (-1, 1)), tf.reshape(y1, (-1, 1))], axis=-1)
-        bb = tf.concat([tf.reshape(x2, (-1, 1)), tf.reshape(y2, (-1, 1))], axis=-1)
+        x1, y1 = np.meshgrid(range(q_blocks_h), range(q_blocks_w))
+        x2, y2 = np.meshgrid(range(k_blocks_h), range(k_blocks_w))
+        aa = np.concatenate([np.reshape(x1, (-1, 1)), np.reshape(y1, (-1, 1))], axis=-1)
+        bb = np.concatenate([np.reshape(x2, (-1, 1)), np.reshape(y2, (-1, 1))], axis=-1)
         # print(f">>>> {aa.shape = }, {bb.shape = }") # aa.shape = (16, 2), bb.shape = (49, 2)
-        cc = [tf.math.abs(bb - ii * strides) for ii in aa]
-        self.bb_pos = tf.stack([ii[:, 0] + ii[:, 1] * k_blocks_h for ii in cc])
+        cc = [np.abs(bb - ii * strides) for ii in aa]
+        bb_pos = np.stack([ii[:, 0] + ii[:, 1] * k_blocks_h for ii in cc])
+
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("bb_pos", functional.convert_to_tensor(bb_pos, dtype="int64"), persistent=False)
+        else:
+            self.bb_pos = functional.convert_to_tensor(bb_pos, dtype="int64")
         # print(f">>>> {self.bb_pos.shape = }")    # self.bb_pos.shape = (16, 49)
 
         super(MultiHeadPositionalEmbedding, self).build(input_shape)
 
     def call(self, inputs, **kwargs):
-        pos_bias = tf.gather(self.bb, self.bb_pos)
-        pos_bias = tf.transpose(pos_bias, [2, 0, 1])
-        return inputs + pos_bias
+        pos_bias = functional.gather(self.bb, self.bb_pos)
+        return inputs + (pos_bias if self.output_perm is None else functional.transpose(pos_bias, self.output_perm))
 
     def get_config(self):
         base_config = super().get_config()
         base_config.update({"query_height": self.query_height, "key_height": self.key_height})
         return base_config
 
-    def load_resized_weights(self, source_layer, method="nearest"):
+    def load_resized_weights(self, source_layer, method="bilinear"):
         if isinstance(source_layer, dict):
-            source_bb = source_layer["positional_embedding:0"]  # weights
+            source_bb = list(source_layer.values())[0]  # weights
         else:
             source_bb = source_layer.bb  # layer
-        hh = ww = int(tf.math.sqrt(float(source_bb.shape[0])))
-        ss = tf.reshape(source_bb, (hh, ww, source_bb.shape[-1]))  # [hh, ww, num_heads]
-        # target_hh = target_ww = int(tf.math.sqrt(float(self.bb.shape[0])))
-        tt = tf.image.resize(ss, [self.k_blocks_h, self.k_blocks_w], method=method)  # [target_hh, target_ww, num_heads]
-        tt = tf.reshape(tt, (self.bb.shape))  # [target_hh * target_ww, num_heads]
-        self.bb.assign(tt)
+        source_bb = np.array(source_bb.detach() if hasattr(source_bb, "detach") else source_bb).astype("float32")
+        hh = ww = int(float(source_bb.shape[0]) ** 0.5)
+        if hh == self.k_blocks_h and ww == self.k_blocks_w:
+            self.set_weights([source_bb])
+            return
+
+        num_heads = source_bb.shape[-1] if len(source_bb.shape) == 2 else 1
+        ss = source_bb.reshape((hh, ww, num_heads))  # [hh, ww, num_heads]
+        # target_hh = target_ww = int(float(self.bb.shape[0]) ** 0.5)
+        tt = backend.numpy_image_resize(ss, target_shape=[self.k_blocks_h, self.k_blocks_w], method=method)  # [target_hh, target_ww, num_heads]
+        tt = tt.reshape((self.bb.shape))  # [target_hh * target_ww, num_heads]
+        self.set_weights([tt])
 
     def show_pos_emb(self, rows=1, base_size=2):
         import matplotlib.pyplot as plt
 
-        hh = ww = int(tf.math.sqrt(float(self.bb.shape[0])))
-        ss = tf.reshape(self.bb, (hh, ww, -1)).numpy()
-        cols = int(tf.math.ceil(ss.shape[-1] / rows))
+        hh = ww = int(float(self.bb.shape[0]) ** 0.5)
+        ss = np.array(self.bb.detach() if hasattr(self.bb, "detach") else self.bb)
+        ss = ss.reshape((hh, ww, -1))
+        cols = int(math.ceil(ss.shape[-1] / rows))
         fig, axes = plt.subplots(rows, cols, figsize=(base_size * cols, base_size * rows))
         for id, ax in enumerate(axes.flatten()):
             ax.imshow(ss[:, :, id])
             ax.set_axis_off()
         fig.tight_layout()
         return fig
 
 
-def scaled_dot_product_attention(query, key, value, output_shape, pos_emb=None, out_weight=True, out_bias=False, dropout=0, activation=None, name=None):
-    output_dim = output_shape[-1]
-    blocks = output_shape[1:-1] if output_shape[0] is None or output_shape[0] < 1 else output_shape[:-1]
-    # query, value: [batch, num_heads, blocks, key_dim], key: [batch, num_heads, key_dim, blocks]
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(query.shape[-1], "float32")))
-    # print(f"{query.shape = }, {key.shape = }")
-    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale  # [batch, num_heads, q_blocks, k_blocks]
-    # print(f"{attention_scores.shape = }")
-    if pos_emb is not None:
-        # attention_scores = MultiHeadPositionalEmbedding(query_height=height, name=name and name + "attn_pos")(attention_scores)
-        attention_scores = pos_emb(attention_scores)
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
-    if dropout > 0:
-        attention_scores = keras.layers.Dropout(dropout, name=name and name + "attn_drop")(attention_scores)
-
-    # output = tf.matmul(attention_scores, value)    # [batch, num_heads, q_blocks, key_dim * attn_ratio]
-    output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
-    output = tf.transpose(output, perm=[0, 2, 1, 3])  # [batch, q_blocks, num_heads, key_dim * attn_ratio]
-    output = tf.reshape(output, [-1, *blocks, output.shape[2] * output.shape[3]])  # [batch, q_blocks, channel * attn_ratio]
-    if activation:
-        output = activation_by_name(output, activation=activation, name=name)
-    if out_weight:
-        # [batch, hh, ww, num_heads * key_dim] * [num_heads * key_dim, out] --> [batch, hh, ww, out]
-        output = keras.layers.Dense(output_dim, use_bias=out_bias, name=name and name + "out")(output)
-    return output
-
-
 def mhsa_with_multi_head_position(
-    inputs, num_heads, key_dim=-1, output_dim=-1, attn_ratio=1, use_bn=True, qkv_bias=False, out_bias=False, activation=None, name=None
+    inputs, num_heads, key_dim=-1, attn_height=-1, output_dim=-1, attn_ratio=1, use_bn=False, qkv_bias=False, out_bias=False, activation=None, name=None
 ):
-    _, height, width, input_channels = inputs.shape
+    input_channels = inputs.shape[-1]
+    input_blocks = inputs.shape[1:-1]
     key_dim = key_dim if key_dim > 0 else input_channels // num_heads
     output_dim = output_dim if output_dim > 0 else input_channels
     embed_dim = key_dim * num_heads
+    if len(inputs.shape) == 4:
+        attn_height, attn_width = inputs.shape[1:-1]
+    else:
+        attn_height = attn_height if attn_height > 0 else int(inputs.shape[1] ** 0.5)  # inputs is square shape, attn_height == attn_width
+        attn_width = inputs.shape[1] // attn_height
 
     qkv_dim = (attn_ratio + 1 + 1) * embed_dim
-    qkv = keras.layers.Dense(qkv_dim, use_bias=qkv_bias, name=name and name + "qkv")(inputs)
-    qkv = batchnorm_with_activation(qkv, activation=None, name=name and name + "qkv_") if use_bn else qkv
-    qkv = tf.reshape(qkv, (-1, qkv.shape[1] * qkv.shape[2], num_heads, qkv_dim // num_heads))
-    qq, kk, vv = tf.split(qkv, [key_dim, key_dim, key_dim * attn_ratio], axis=-1)
-    qq, kk, vv = tf.transpose(qq, [0, 2, 1, 3]), tf.transpose(kk, [0, 2, 3, 1]), tf.transpose(vv, [0, 2, 1, 3])
+    qkv = layers.Dense(qkv_dim, use_bias=qkv_bias, name=name and name + "qkv")(inputs)
+    qkv = batchnorm_with_activation(qkv, activation=None, axis=-1, name=name and name + "qkv_") if use_bn else qkv
+    qkv = functional.reshape(qkv, (-1, int(np.prod(qkv.shape[1:-1])), num_heads, qkv_dim // num_heads))
+    qq, kk, vv = functional.split(qkv, [key_dim, key_dim, key_dim * attn_ratio], axis=-1)
+    qq, kk, vv = functional.transpose(qq, [0, 2, 1, 3]), functional.transpose(kk, [0, 2, 3, 1]), functional.transpose(vv, [0, 2, 1, 3])
+
+    output_shape = (*input_blocks, output_dim)
+    pos_emb = MultiHeadPositionalEmbedding(query_height=attn_height, name=name and name + "attn_pos")
+    output = scaled_dot_product_attention(qq, kk, vv, output_shape, pos_emb=pos_emb, out_weight=False, name=name)
 
-    output_shape = (height, width, output_dim)
-    pos_emb = MultiHeadPositionalEmbedding(query_height=height, name=name and name + "attn_pos")
-    output = scaled_dot_product_attention(qq, kk, vv, output_shape, pos_emb=pos_emb, out_bias=out_bias, activation=activation, name=name)
+    if activation:
+        output = activation_by_name(output, activation=activation, name=name)
+    # [batch, cls_token + hh * ww, num_heads * key_dim] * [num_heads * key_dim, out] --> [batch, cls_token + hh * ww, out]
+    output = layers.Dense(output_dim, use_bias=out_bias, name=name and name + "out")(output)
     if use_bn:
-        output = batchnorm_with_activation(output, activation=None, zero_gamma=True, name=name and name + "out_")
+        output = batchnorm_with_activation(output, activation=None, zero_gamma=True, axis=-1, name=name and name + "out_")
     return output
 
 
 def mhsa_with_multi_head_position_and_strides(
     inputs, num_heads, key_dim=-1, output_dim=-1, attn_ratio=1, strides=1, use_bn=True, qkv_bias=False, out_bias=False, activation=None, name=None
 ):
     _, _, _, input_channels = inputs.shape
     key_dim = key_dim if key_dim > 0 else input_channels // num_heads
     output_dim = output_dim if output_dim > 0 else input_channels
-    emded_dim = num_heads * key_dim
-
-    embed_dim = key_dim * num_heads
+    embed_dim = num_heads * key_dim
 
     qq = inputs[:, ::strides, ::strides, :] if strides > 1 else inputs
     height, width = qq.shape[1], qq.shape[2]
     # print(f"{height = }, {width = }, {strides = }, {inputs.shape = }")
-    qq = keras.layers.Dense(embed_dim, use_bias=qkv_bias, name=name and name + "q")(qq)
-    qq = batchnorm_with_activation(qq, activation=None, name=name and name + "q_") if use_bn else qq
-    qq = tf.reshape(qq, [-1, qq.shape[1] * qq.shape[2], num_heads, key_dim])
-    qq = tf.transpose(qq, [0, 2, 1, 3])
+    qq = layers.Dense(embed_dim, use_bias=qkv_bias, name=name and name + "q")(qq)
+    qq = batchnorm_with_activation(qq, activation=None, axis=-1, name=name and name + "q_") if use_bn else qq
+    qq = functional.reshape(qq, [-1, qq.shape[1] * qq.shape[2], num_heads, key_dim])
+    qq = functional.transpose(qq, [0, 2, 1, 3])
 
     kv_dim = (attn_ratio + 1) * embed_dim
-    kv = keras.layers.Dense(kv_dim, use_bias=qkv_bias, name=name and name + "kv")(inputs)
-    kv = batchnorm_with_activation(kv, activation=None, name=name and name + "kv_") if use_bn else kv
-    kv = tf.reshape(kv, (-1, kv.shape[1] * kv.shape[2], num_heads, kv_dim // num_heads))
-    kk, vv = tf.split(kv, [key_dim, key_dim * attn_ratio], axis=-1)
-    kk, vv = tf.transpose(kk, [0, 2, 3, 1]), tf.transpose(vv, [0, 2, 1, 3])
+    kv = layers.Dense(kv_dim, use_bias=qkv_bias, name=name and name + "kv")(inputs)
+    kv = batchnorm_with_activation(kv, activation=None, axis=-1, name=name and name + "kv_") if use_bn else kv
+    kv = functional.reshape(kv, (-1, kv.shape[1] * kv.shape[2], num_heads, kv_dim // num_heads))
+    kk, vv = functional.split(kv, [key_dim, key_dim * attn_ratio], axis=-1)
+    kk, vv = functional.transpose(kk, [0, 2, 3, 1]), functional.transpose(vv, [0, 2, 1, 3])
 
     output_shape = (height, width, output_dim)
     # print(f"{qq.shape = }, {kk.shape = }, {vv.shape = }, {output_shape = }")
     pos_emb = MultiHeadPositionalEmbedding(query_height=height, name=name and name + "attn_pos")
-    output = scaled_dot_product_attention(qq, kk, vv, output_shape, pos_emb=pos_emb, out_bias=out_bias, activation=activation, name=name)
+    output = scaled_dot_product_attention(qq, kk, vv, output_shape, pos_emb=pos_emb, out_weight=False, name=name)
+
+    if activation:
+        output = activation_by_name(output, activation=activation, name=name)
+    # [batch, cls_token + hh * ww, num_heads * key_dim] * [num_heads * key_dim, out] --> [batch, cls_token + hh * ww, out]
+    output = layers.Dense(output_dim, use_bias=out_bias, name=name and name + "out")(output)
     if use_bn:
-        output = batchnorm_with_activation(output, activation=None, zero_gamma=True, name=name and name + "out_")
+        output = batchnorm_with_activation(output, activation=None, zero_gamma=True, axis=-1, name=name and name + "out_")
     return output
 
 
 def res_mhsa_with_multi_head_position(inputs, embed_dim, num_heads, key_dim, attn_ratio, drop_rate=0, activation="hard_swish", name=""):
-    nn = mhsa_with_multi_head_position(inputs, num_heads, key_dim, embed_dim, attn_ratio, activation=activation, name=name)
+    nn = mhsa_with_multi_head_position(inputs, num_heads, key_dim, output_dim=embed_dim, attn_ratio=attn_ratio, use_bn=True, activation=activation, name=name)
     if drop_rate > 0:
-        nn = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop")(nn)
-    return keras.layers.Add(name=name + "add")([inputs, nn])
+        nn = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop")(nn)
+    return layers.Add(name=name + "add")([inputs, nn])
 
 
 def res_mlp_block(inputs, mlp_ratio, drop_rate=0, use_bias=False, activation="hard_swish", name=""):
     in_channels = inputs.shape[-1]
-    nn = keras.layers.Dense(in_channels * mlp_ratio, use_bias=use_bias, name=name + "1_dense")(inputs)
-    nn = batchnorm_with_activation(nn, activation=activation, name=name + "1_")
-    nn = keras.layers.Dense(in_channels, use_bias=use_bias, name=name + "2_dense")(nn)
-    nn = batchnorm_with_activation(nn, activation=None, name=name + "2_")
+    nn = layers.Dense(in_channels * mlp_ratio, use_bias=use_bias, name=name + "1_dense")(inputs)
+    nn = batchnorm_with_activation(nn, activation=activation, axis=-1, name=name + "1_")  # "channels_first" also using axis=-1
+    nn = layers.Dense(in_channels, use_bias=use_bias, name=name + "2_dense")(nn)
+    nn = batchnorm_with_activation(nn, activation=None, axis=-1, name=name + "2_")  # "channels_first" also using axis=-1
     if drop_rate > 0:
-        nn = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop")(nn)
-    return keras.layers.Add(name=name + "add")([inputs, nn])
+        nn = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop")(nn)
+    return layers.Add(name=name + "add")([inputs, nn])
 
 
 def attention_mlp_stack(inputs, out_channel, num_heads, depth, key_dim, attn_ratio, mlp_ratio, strides, stack_drop=0, activation="hard_swish", name=""):
     nn = inputs
     embed_dim = nn.shape[-1]
     stack_drop_s, stack_drop_e = stack_drop if isinstance(stack_drop, (list, tuple)) else [stack_drop, stack_drop]
     for ii in range(depth):
@@ -202,15 +214,15 @@
     if embed_dim != out_channel:
         block_name = name + "downsample_"
         ds_num_heads = embed_dim // key_dim
         ds_attn_ratio = attn_ratio * strides
         nn = mhsa_with_multi_head_position_and_strides(nn, ds_num_heads, key_dim, out_channel, ds_attn_ratio, strides, activation=activation, name=block_name)
         if mlp_ratio > 0:
             nn = res_mlp_block(nn, mlp_ratio, drop_rate, activation=activation, name=block_name + "mlp_")
-    return keras.layers.Activation("linear", name=name + "output")(nn)  # Identity, Just need a name here
+    return layers.Activation("linear", name=name + "output")(nn)  # Identity, Just need a name here
 
 
 def patch_stem(inputs, stem_width, activation="hard_swish", name=""):
     nn = conv2d_no_bias(inputs, stem_width // 8, 3, strides=2, padding="same", name=name + "1_")
     nn = batchnorm_with_activation(nn, activation=activation, name=name + "1_")
     nn = conv2d_no_bias(nn, stem_width // 4, 3, strides=2, padding="same", name=name + "2_")
     nn = batchnorm_with_activation(nn, activation=activation, name=name + "2_")
@@ -232,84 +244,104 @@
     strides=[2, 2, 0],  # down_ops, strides
     input_shape=(224, 224, 3),
     num_classes=1000,
     activation="hard_swish",
     drop_connect_rate=0,
     dropout=0,
     classifier_activation=None,
-    use_distillation=True,
+    use_distillation=False,
     pretrained="imagenet",
     model_name="levit",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
     nn = patch_stem(inputs, patch_channel, activation=activation, name="stem_")
-    # nn = tf.reshape(nn, [-1, nn.shape[1] * nn.shape[2], patch_channel])
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)  # channels_first -> channels_last
 
     global_block_id = 0
     total_blocks = sum(depthes)
     drop_connect_s, drop_connect_e = drop_connect_rate if isinstance(drop_connect_rate, (list, tuple)) else (drop_connect_rate, drop_connect_rate)
     for id, (out_channel, num_head, depth, key_dim, attn_ratio, mlp_ratio, stride) in enumerate(
         zip(out_channels, num_heads, depthes, key_dims, attn_ratios, mlp_ratios, strides)
     ):
         name = "stack{}_".format(id + 1)
         stack_drop_s = drop_connect_s + (drop_connect_e - drop_connect_s) * global_block_id / total_blocks
         stack_drop_e = drop_connect_s + (drop_connect_e - drop_connect_s) * (global_block_id + depth) / total_blocks
         stack_drop = (stack_drop_s, stack_drop_e)
         nn = attention_mlp_stack(nn, out_channel, num_head, depth, key_dim, attn_ratio, mlp_ratio, stride, stack_drop, activation, name=name)
         global_block_id += depth
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_first -> channels_last
 
     if num_classes == 0:
         out = nn
     else:
-        nn = keras.layers.GlobalAveragePooling2D()(nn)  # tf.reduce_mean(nn, axis=1)
+        nn = layers.GlobalAveragePooling2D(keepdims=True)(nn)
         if dropout > 0 and dropout < 1:
-            nn = keras.layers.Dropout(dropout)(nn)
+            nn = layers.Dropout(dropout)(nn)
         out = batchnorm_with_activation(nn, activation=None, name="head_")
-        out = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(out)
+        out = layers.Flatten()(out)
+        out = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(out)
 
         if use_distillation:
             distill = batchnorm_with_activation(nn, activation=None, name="distill_head_")
-            distill = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="distill_head")(distill)
+            distill = layers.Flatten()(distill)
+            distill = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="distill_head")(distill)
             out = [out, distill]
 
-    model = keras.models.Model(inputs, out, name=model_name)
-    add_pre_post_process(model, rescale_mode="torch")
+    model = models.Model(inputs, out, name=model_name)
     reload_model_weights(model, PRETRAINED_DICT, "levit", pretrained, MultiHeadPositionalEmbedding)
+    add_pre_post_process(model, rescale_mode="torch")
+    model.switch_to_deploy = lambda: switch_to_deploy(model)
     return model
 
 
-def LeViT128S(input_shape=(224, 224, 3), num_classes=1000, use_distillation=True, classifier_activation=None, pretrained="imagenet", **kwargs):
+def switch_to_deploy(model):
+    from keras_cv_attention_models.model_surgery.model_surgery import fuse_distill_head
+
+    new_model = fuse_distill_head(model) if "head" in model.output_names else model
+    add_pre_post_process(new_model, rescale_mode=model.preprocess_input.rescale_mode, post_process=model.decode_predictions)
+    return new_model
+
+
+@register_model
+def LeViT128S(input_shape=(224, 224, 3), num_classes=1000, use_distillation=False, classifier_activation=None, pretrained="imagenet", **kwargs):
     return LeViT(**locals(), model_name="levit128s", **kwargs)
 
 
-def LeViT128(input_shape=(224, 224, 3), num_classes=1000, use_distillation=True, classifier_activation=None, pretrained="imagenet", **kwargs):
+@register_model
+def LeViT128(input_shape=(224, 224, 3), num_classes=1000, use_distillation=False, classifier_activation=None, pretrained="imagenet", **kwargs):
     num_heads = [4, 8, 12]
     depthes = [4, 4, 4]
     return LeViT(**locals(), model_name="levit128", **kwargs)
 
 
-def LeViT192(input_shape=(224, 224, 3), num_classes=1000, use_distillation=True, classifier_activation=None, pretrained="imagenet", **kwargs):
+@register_model
+def LeViT192(input_shape=(224, 224, 3), num_classes=1000, use_distillation=False, classifier_activation=None, pretrained="imagenet", **kwargs):
     patch_channel = 192
     out_channels = [288, 384, 384]
     num_heads = [3, 5, 6]
     depthes = [4, 4, 4]
     key_dims = [32, 32, 32]
     return LeViT(**locals(), model_name="levit192", **kwargs)
 
 
+@register_model
 def LeViT256(input_shape=(224, 224, 3), num_classes=1000, use_distillation=True, classifier_activation=None, pretrained="imagenet", **kwargs):
     patch_channel = 256
     out_channels = [384, 512, 512]
     num_heads = [4, 6, 8]
     depthes = [4, 4, 4]
     key_dims = [32, 32, 32]
     return LeViT(**locals(), model_name="levit256", **kwargs)
 
 
+@register_model
 def LeViT384(input_shape=(224, 224, 3), num_classes=1000, use_distillation=True, classifier_activation=None, pretrained="imagenet", **kwargs):
     patch_channel = 384
     out_channels = [512, 768, 768]
     num_heads = [6, 9, 12]
     depthes = [4, 4, 4]
     key_dims = [32, 32, 32]
     return LeViT(**locals(), model_name="levit384", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/maxvit/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/maxvit/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/maxvit/maxvit.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/maxvit/maxvit.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,11 @@
-import tensorflow as tf
-from tensorflow import keras
+import math
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     batchnorm_with_activation,
     ChannelAffine,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
     drop_block,
@@ -40,53 +42,54 @@
 def res_MBConv(inputs, output_channel, conv_short_cut=True, strides=1, expansion=4, se_ratio=0, use_torch_mode=False, drop_rate=0, activation="gelu", name=""):
     if use_torch_mode:
         use_torch_padding, epsilon, momentum = True, 1e-5, 0.9
     else:
         use_torch_padding, epsilon, momentum = False, 0.001, 0.99
 
     if strides > 1:
-        shortcut = keras.layers.AvgPool2D(strides, strides=strides, padding="SAME", name=name + "shortcut_pool")(inputs)
+        shortcut = layers.AvgPool2D(strides, strides=strides, padding="same", name=name + "shortcut_pool")(inputs)
         shortcut = conv2d_no_bias(shortcut, output_channel, 1, strides=1, use_bias=True, name=name + "shortcut_") if conv_short_cut else shortcut
     else:
         shortcut = inputs
 
     # MBConv
     preact = batchnorm_with_activation(inputs, activation=None, zero_gamma=False, epsilon=epsilon, momentum=momentum, name=name + "preact_")
     nn = conv2d_no_bias(preact, output_channel * expansion, 1, strides=1, padding="same", name=name + "expand_")
     nn = batchnorm_with_activation(nn, activation=activation, epsilon=epsilon, momentum=momentum, name=name + "expand_")
-    nn = depthwise_conv2d_no_bias(nn, 3, strides=strides, padding="SAME", use_torch_padding=use_torch_padding, name=name + "MB_")
+    nn = depthwise_conv2d_no_bias(nn, 3, strides=strides, padding="same", use_torch_padding=use_torch_padding, name=name + "MB_")
     nn = batchnorm_with_activation(nn, activation=activation, zero_gamma=False, epsilon=epsilon, momentum=momentum, name=name + "MB_dw_")
     if se_ratio:
         nn = se_module(nn, se_ratio=se_ratio / expansion, activation="swish", name=name + "se/")
     nn = conv2d_no_bias(nn, output_channel, 1, strides=1, use_bias=True, padding="same", name=name + "MB_pw_")
     nn = drop_block(nn, drop_rate=drop_rate, name=name)
     # print(f"{shortcut.shape = }, {nn.shape = }, {strides = }")
-    return keras.layers.Add(name=name + "output")([shortcut, nn])
+    return layers.Add(name=name + "output")([shortcut, nn])
 
 
 def res_attn_ffn(inputs, output_channel, head_dimension=32, window_size=7, expansion=4, is_grid=False, drop_rate=0, layer_scale=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
-    attn = layer_norm(inputs, name=name + "attn_preact_")
+    input_channel = inputs.shape[-1]  # Channels_last only
+    attn = layer_norm(inputs, axis=-1, name=name + "attn_preact_")
     num_heads = attn.shape[-1] // head_dimension
+    # print(f"{inputs.shape = }, {num_heads = }, {window_size = }, {is_grid = }")
     attention_block = lambda inputs, num_heads, name: mhsa_with_multi_head_relative_position_embedding(
-        inputs, num_heads=num_heads, qkv_bias=True, out_bias=True, name=name
+        inputs, num_heads=num_heads, qkv_bias=True, out_bias=True, data_format="channels_last", name=name
     )
     attn = window_attention(attn, window_size=window_size, num_heads=num_heads, is_grid=is_grid, attention_block=attention_block, name=name + "window_mhsa/")
-    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "1_gamma")(attn) if layer_scale >= 0 else attn
+    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, axis=-1, name=name + "1_gamma")(attn) if layer_scale >= 0 else attn
     attn = drop_block(attn, drop_rate=drop_rate, name=name + "attn_")
-    # print(f"{name = }, {inputs.shape = }, {shortcut.shape = }, {attn.shape = }")
-    attn = keras.layers.Add(name=name + "attn_output")([inputs, attn])
+    # print(f"{name = }, {inputs.shape = }, {inputs.shape = }, {attn.shape = }")
+    attn = layers.Add(name=name + "attn_output")([inputs, attn])
 
-    ffn = layer_norm(attn, name=name + "ffn_preact_")
-    ffn = keras.layers.Dense(input_channel * expansion, name=name + "ffn/1_dense")(ffn)
+    ffn = layer_norm(attn, axis=-1, name=name + "ffn_preact_")
+    ffn = layers.Dense(input_channel * expansion, name=name + "ffn/1_dense")(ffn)
     ffn = activation_by_name(ffn, activation=activation, name=name)
-    ffn = keras.layers.Dense(input_channel, name=name + "ffn/2_dense")(ffn)
-    ffn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "2_gamma")(ffn) if layer_scale >= 0 else ffn
+    ffn = layers.Dense(input_channel, name=name + "ffn/2_dense")(ffn)
+    ffn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, axis=-1, name=name + "2_gamma")(ffn) if layer_scale >= 0 else ffn
     ffn = drop_block(ffn, drop_rate=drop_rate, name=name + "ffn_")
-    return keras.layers.Add(name=name + "ffn_output")([attn, ffn])
+    return layers.Add(name=name + "ffn_output")([attn, ffn])
 
 
 def MaxViT(
     num_blocks,
     out_channels,
     stem_width=64,
     strides=[2, 2, 2, 2],
@@ -103,25 +106,31 @@
     drop_connect_rate=0,
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="maxvit",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    height, width = inputs.shape[1:-1] if image_data_format() == "channels_last" else inputs.shape[2:]
     if use_torch_mode:
         use_torch_padding, epsilon, momentum = True, 1e-5, 0.9
     else:
         use_torch_padding, epsilon, momentum = False, 0.001, 0.99
 
     """ Stem """
     nn = conv2d_no_bias(inputs, stem_width, 3, strides=2, use_bias=True, padding="same", use_torch_padding=use_torch_padding, name="stem_1_")
     nn = batchnorm_with_activation(nn, activation=activation, epsilon=epsilon, momentum=momentum, name="stem_1_")
     nn = conv2d_no_bias(nn, stem_width, 3, strides=1, use_bias=True, padding="same", use_torch_padding=use_torch_padding, name="stem_2_")
-    window_size = [int(tf.math.ceil(input_shape[0] / window_ratio)), int(tf.math.ceil(input_shape[1] / window_ratio))]
+    window_size = [int(math.ceil(height / window_ratio)), int(math.ceil(width / window_ratio))]
+    # print(f"{window_size = }")
 
     attn_ffn_common_kwargs = {
         "head_dimension": head_dimension,
         "window_size": window_size,
         "expansion": expansion,
         "layer_scale": layer_scale,
         "activation": activation,
@@ -132,68 +141,75 @@
     global_block_id = 0
     for stack_id, (num_block, out_channel) in enumerate(zip(num_blocks, out_channels)):
         stack_se_ratio = se_ratio[stack_id] if isinstance(se_ratio, (list, tuple)) else se_ratio
         stack_strides = strides[stack_id] if isinstance(strides, (list, tuple)) else strides
         for block_id in range(num_block):
             name = "stack_{}_block_{}/".format(stack_id + 1, block_id + 1)
             stride = stack_strides if block_id == 0 else 1
-            conv_short_cut = True if block_id == 0 and nn.shape[-1] != out_channel else False
+            conv_short_cut = True if block_id == 0 and nn.shape[channel_axis] != out_channel else False
             block_se_ratio = stack_se_ratio[block_id] if isinstance(stack_se_ratio, (list, tuple)) else stack_se_ratio
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             global_block_id += 1
             nn = res_MBConv(
                 nn, out_channel, conv_short_cut, stride, expansion, block_se_ratio, use_torch_mode, block_drop_rate, activation, name=name + "mbconv/"
             )
+            nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)  # channels_first -> channels_last
             nn = res_attn_ffn(nn, out_channel, is_grid=False, drop_rate=block_drop_rate, name=name + "block_", **attn_ffn_common_kwargs)
             nn = res_attn_ffn(nn, out_channel, is_grid=True, drop_rate=block_drop_rate, name=name + "grid_", **attn_ffn_common_kwargs)
+            nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_last -> channels_first
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
         nn = layer_norm(nn, name="post_")
         output_filter = out_channels[-1] if output_filter == -1 else output_filter
         if output_filter > 0:
-            nn = keras.layers.Dense(output_filter, name="features")(nn)
+            nn = layers.Dense(output_filter, name="features")(nn)
             nn = activation_by_name(nn, "tanh", name="features_")
         if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     rescale_mode = "tf" if pretrained is not None and pretrained.startswith("imagenet21k") else "torch"  # For testing only
     add_pre_post_process(model, rescale_mode=rescale_mode)
     reload_model_weights(model, PRETRAINED_DICT, "maxvit", pretrained, MultiHeadRelativePositionalEmbedding)
     return model
 
 
+@register_model
 def MaxViT_Tiny(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 5, 2]
     out_channels = [64, 128, 256, 512]
     stem_width = 64
     return MaxViT(**locals(), model_name="maxvit_tiny", **kwargs)
 
 
+@register_model
 def MaxViT_Small(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 5, 2]
     out_channels = [96, 192, 384, 768]
     stem_width = 64
     return MaxViT(**locals(), model_name="maxvit_small", **kwargs)
 
 
+@register_model
 def MaxViT_Base(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 6, 14, 2]
     out_channels = [96, 192, 384, 768]
     stem_width = 64
     return MaxViT(**locals(), model_name="maxvit_base", **kwargs)
 
 
+@register_model
 def MaxViT_Large(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 6, 14, 2]
     out_channels = [128, 256, 512, 1024]
     stem_width = 128
     return MaxViT(**locals(), model_name="maxvit_large", **kwargs)
 
 
+@register_model
 def MaxViT_XLarge(input_shape=(224, 224, 3), num_classes=1000, drop_connect_rate=0, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 6, 14, 2]
     out_channels = [192, 384, 768, 1536]
     stem_width = 192
     return MaxViT(**locals(), model_name="maxvit_xlarge", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/__init__.py`

 * *Files 0% similar despite different names*

```diff
@@ -9,14 +9,15 @@
     MLPMixerH14,
     mlp_block,
     mlp_mixer_block,
 )
 from keras_cv_attention_models.mlp_family.res_mlp import ResMLP, ResMLP12, ResMLP24, ResMLP36, ResMLP_B24, ChannelAffine
 from keras_cv_attention_models.mlp_family.gated_mlp import GMLP, GMLPTiny16, GMLPS16, GMLPB16, spatial_gating_block
 from keras_cv_attention_models.mlp_family.wave_mlp import WaveMLP, WaveMLP_T, WaveMLP_S, WaveMLP_M, WaveMLP_B, phase_aware_token_mixing
+from keras_cv_attention_models.mlp_family import mlp_mixer, res_mlp, gated_mlp, wave_mlp
 
 __mlp_mixer_head_doc__ = """
 Github source [leondgarse/keras_cv_attention_models](https://github.com/leondgarse/keras_cv_attention_models).
 Keras implementation of [Github google-research/vision_transformer](https://github.com/google-research/vision_transformer#available-mixer-models).
 Paper [PDF 2105.01601 MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601.pdf).
 """
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/gated_mlp.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/gated_mlp.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,53 +1,59 @@
-from tensorflow import keras
-from tensorflow.keras import backend as K
-import tensorflow as tf
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
 from keras_cv_attention_models.attention_layers import activation_by_name, add_pre_post_process
 
 BATCH_NORM_EPSILON = 1e-5
 PRETRAINED_DICT = {
     "gmlp_s16": {"imagenet": "cc2d83bc0a7edd257aa6cd58397887e9"},
 }
 
 
 def layer_norm(inputs, name=None):
     """Typical LayerNormalization with epsilon=1e-5"""
-    norm_axis = -1 if K.image_data_format() == "channels_last" else 1
-    return keras.layers.LayerNormalization(axis=norm_axis, epsilon=BATCH_NORM_EPSILON, name=name)(inputs)
+    norm_axis = -1 if backend.image_data_format() == "channels_last" else 1
+    return layers.LayerNormalization(axis=norm_axis, epsilon=BATCH_NORM_EPSILON, name=name)(inputs)
 
 
 def spatial_gating_block(inputs, name=None):
-    uu, vv = tf.split(inputs, 2, axis=-1)
+    uu, vv = functional.split(inputs, 2, axis=-1 if backend.image_data_format() == "channels_last" else 1)
     # print(f">>>> {uu.shape = }, {vv.shape = }")
     vv = layer_norm(vv, name=name and name + "vv_ln")
-    vv = keras.layers.Permute((2, 1), name=name and name + "permute_1")(vv)
-    ww_init = keras.initializers.truncated_normal(stddev=1e-6)
-    vv = keras.layers.Dense(vv.shape[-1], kernel_initializer=ww_init, bias_initializer="ones", name=name and name + "vv_dense")(vv)
-    vv = keras.layers.Permute((2, 1), name=name and name + "permute_2")(vv)
+    vv = layers.Permute((2, 1), name=name and name + "permute_1")(vv) if backend.image_data_format() == "channels_last" else vv
+    ww_init = initializers.truncated_normal(stddev=1e-6)
+    vv = layers.Dense(vv.shape[-1], kernel_initializer=ww_init, bias_initializer="ones", name=name and name + "vv_dense")(vv)
+    vv = layers.Permute((2, 1), name=name and name + "permute_2")(vv) if backend.image_data_format() == "channels_last" else vv
     # print(f">>>> {uu.shape = }, {vv.shape = }")
-    gated_out = keras.layers.Multiply()([uu, vv])
+    gated_out = layers.Multiply()([uu, vv])
     return gated_out
 
 
 def res_gated_mlp_block(inputs, channels_mlp_dim, drop_rate=0, activation="gelu", name=None):
+    input_channel = inputs.shape[-1] if backend.image_data_format() == "channels_last" else inputs.shape[1]
     nn = layer_norm(inputs, name=name + "pre_ln")
-    nn = keras.layers.Dense(channels_mlp_dim, name=name + "pre_dense")(nn)
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute((2, 1), name=name and name + "pre_dense_permute_1")(nn)
+    nn = layers.Dense(channels_mlp_dim, name=name + "pre_dense")(nn)
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute((2, 1), name=name and name + "pre_dense_permute_2")(nn)
     nn = activation_by_name(nn, activation, name=name and name + activation)
     # Drop
 
+    # print(f">>>> {inputs.shape = }, {nn.shape = }")
     # SpatialGatingUnit
-    gated_out = spatial_gating_block(nn, name=name)
-    nn = keras.layers.Dense(inputs.shape[-1], name=name + "gated_dense")(gated_out)
+    nn = spatial_gating_block(nn, name=name)
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute((2, 1), name=name and name + "gated_dense_permute_1")(nn)
+    nn = layers.Dense(input_channel, name=name + "gated_dense")(nn)
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute((2, 1), name=name and name + "gated_dense_permute_2")(nn)
     # Drop
 
     # Drop path
     if drop_rate > 0:
-        nn = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop")(nn)
-    return keras.layers.Add(name=name + "output")([nn, inputs])
+        nn = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "drop")(nn)
+    return layers.Add(name=name + "output")([nn, inputs])
 
 
 def GMLP(
     num_blocks,
     patch_size,
     stem_width,
     channels_mlp_dim,
@@ -58,38 +64,42 @@
     dropout=0,
     drop_connect_rate=0,
     classifier_activation="softmax",
     pretrained="imagenet",
     model_name="gmlp",
     kwargs=None,
 ):
-    inputs = keras.Input(input_shape)
-    nn = keras.layers.Conv2D(stem_width, kernel_size=patch_size, strides=patch_size, padding="valid", name="stem")(inputs)
-    nn = keras.layers.Reshape([nn.shape[1] * nn.shape[2], stem_width])(nn)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+    nn = layers.Conv2D(stem_width, kernel_size=patch_size, strides=patch_size, padding="valid", name="stem")(inputs)
+    new_shape = [nn.shape[1] * nn.shape[2], stem_width] if backend.image_data_format() == "channels_last" else [stem_width, nn.shape[2] * nn.shape[3]]
+    nn = layers.Reshape(new_shape)(nn)
 
     drop_connect_s, drop_connect_e = drop_connect_rate if isinstance(drop_connect_rate, (list, tuple)) else [drop_connect_rate, drop_connect_rate]
     for ii in range(num_blocks):
         name = "{}_{}_".format("gmlp", str(ii + 1))
         block_drop_rate = drop_connect_s + (drop_connect_e - drop_connect_s) * ii / num_blocks
         nn = res_gated_mlp_block(nn, channels_mlp_dim=channels_mlp_dim, drop_rate=block_drop_rate, activation=activation, name=name)
     nn = layer_norm(nn, name="pre_head_norm")
 
     if num_classes > 0:
         # nn = tf.reduce_mean(nn, axis=1)
-        nn = keras.layers.GlobalAveragePooling1D()(nn)
+        nn = layers.GlobalAveragePooling1D()(nn)
         if dropout > 0 and dropout < 1:
-            nn = keras.layers.Dropout(dropout)(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(dropout)(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
     if sam_rho != 0:
         from keras_cv_attention_models.model_surgery import SAMModel
 
         model = SAMModel(inputs, nn, name=model_name)
     else:
-        model = keras.Model(inputs, nn, name=model_name)
+        model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="tf")
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="mlp_family", pretrained=pretrained)
     return model
 
 
 BLOCK_CONFIGS = {
     "tiny16": {
@@ -109,17 +119,20 @@
         "patch_size": 16,
         "stem_width": 512,
         "channels_mlp_dim": 512 * 6,
     },
 }
 
 
+@register_model
 def GMLPTiny16(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return GMLP(**BLOCK_CONFIGS["tiny16"], **locals(), model_name="gmlp_tiny16", **kwargs)
 
 
+@register_model
 def GMLPS16(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return GMLP(**BLOCK_CONFIGS["s16"], **locals(), model_name="gmlp_s16", **kwargs)
 
 
+@register_model
 def GMLPB16(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return GMLP(**BLOCK_CONFIGS["b16"], **locals(), model_name="gmlp_b16", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/mlp_mixer.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/mlp_mixer.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,61 +1,68 @@
-from tensorflow import keras
-from tensorflow.keras import backend as K
-from keras_cv_attention_models.download_and_load import reload_model_weights
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import activation_by_name, add_pre_post_process
+from keras_cv_attention_models.download_and_load import reload_model_weights
 
 BATCH_NORM_EPSILON = 1e-5
 
 PRETRAINED_DICT = {
     "mlp_mixer_b16": {
         "imagenet21k": "6353dffc590a2a7348a44cee2c784724",
         "imagenet": "abd04090063ba9ab0d49e2131cef9d64",
         "imagenet_sam": "d953ef41ffdb0ab9c3fa21493bf0982f",
     },
     "mlp_mixer_l16": {"imagenet": "fa91a74f1aa11ed610299d06d643ed45", "imagenet21k": "8dca5de1817112d9e717db6b2e9a7b0b"},
     "mlp_mixer_b32": {"imagenet_sam": "a6285750e55579fc68e7ba68a683c77d"},
 }
 
 
-def layer_norm(inputs, name=None):
+def layer_norm(inputs, axis="auto", name=None):
     """Typical LayerNormalization with epsilon=1e-5"""
-    norm_axis = -1 if K.image_data_format() == "channels_last" else 1
-    return keras.layers.LayerNormalization(axis=norm_axis, epsilon=BATCH_NORM_EPSILON, name=name)(inputs)
+    norm_axis = (-1 if backend.image_data_format() == "channels_last" else 1) if axis == "auto" else axis
+    return layers.LayerNormalization(axis=norm_axis, epsilon=BATCH_NORM_EPSILON, name=name)(inputs)
 
 
 def mlp_block(inputs, hidden_dim, output_channel=-1, drop_rate=0, use_conv=False, use_bias=True, activation="gelu", name=None):
-    output_channel = output_channel if output_channel > 0 else inputs.shape[-1]
+    channnel_axis = -1 if image_data_format() == "channels_last" else (1 if use_conv else -1)
+    output_channel = output_channel if output_channel > 0 else inputs.shape[channnel_axis]
     if use_conv:
-        nn = keras.layers.Conv2D(hidden_dim, kernel_size=1, use_bias=use_bias, name=name and name + "Conv_0")(inputs)
+        nn = layers.Conv2D(hidden_dim, kernel_size=1, use_bias=use_bias, name=name and name + "Conv_0")(inputs)
     else:
-        nn = keras.layers.Dense(hidden_dim, use_bias=use_bias, name=name and name + "Dense_0")(inputs)
+        nn = layers.Dense(hidden_dim, use_bias=use_bias, name=name and name + "Dense_0")(inputs)
     nn = activation_by_name(nn, activation, name=name)
-    nn = keras.layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
+    nn = layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
     if use_conv:
-        nn = keras.layers.Conv2D(output_channel, kernel_size=1, use_bias=use_bias, name=name and name + "Conv_1")(nn)
+        nn = layers.Conv2D(output_channel, kernel_size=1, use_bias=use_bias, name=name and name + "Conv_1")(nn)
     else:
-        nn = keras.layers.Dense(output_channel, use_bias=use_bias, name=name and name + "Dense_1")(nn)
-    nn = keras.layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
+        nn = layers.Dense(output_channel, use_bias=use_bias, name=name and name + "Dense_1")(nn)
+    nn = layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
     return nn
 
 
-def mlp_mixer_block(inputs, tokens_mlp_dim, channels_mlp_dim, use_bias=True, drop_rate=0, activation="gelu", name=None):
-    nn = layer_norm(inputs, name=name and name + "LayerNorm_0")
-    nn = keras.layers.Permute((2, 1), name=name and name + "permute_0")(nn)
+def mlp_mixer_block(inputs, tokens_mlp_dim, channels_mlp_dim, use_bias=True, drop_rate=0, activation="gelu", data_format=None, name=None):
+    data_format = backend.image_data_format() if data_format is None else data_format
+    norm_axis = -1 if data_format == "channels_last" else 1
+
+    nn = layer_norm(inputs, axis=norm_axis, name=name and name + "LayerNorm_0")
+    nn = layers.Permute((2, 1), name=name and name + "permute_0")(nn) if data_format == "channels_last" else nn
     nn = mlp_block(nn, tokens_mlp_dim, use_bias=use_bias, activation=activation, name=name and name + "token_mixing/")
-    nn = keras.layers.Permute((2, 1), name=name and name + "permute_1")(nn)
+    nn = layers.Permute((2, 1), name=name and name + "permute_1")(nn) if data_format == "channels_last" else nn
     if drop_rate > 0:
-        nn = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name and name + "token_drop")(nn)
-    token_out = keras.layers.Add(name=name and name + "add_0")([nn, inputs])
+        nn = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name and name + "token_drop")(nn)
+    token_out = layers.Add(name=name and name + "add_0")([nn, inputs])
 
-    nn = layer_norm(token_out, name=name and name + "LayerNorm_1")
-    channel_out = mlp_block(nn, channels_mlp_dim, use_bias=use_bias, activation=activation, name=name and name + "channel_mixing/")
+    nn = layer_norm(token_out, axis=norm_axis, name=name and name + "LayerNorm_1")
+    nn = nn if data_format == "channels_last" else layers.Permute((2, 1), name=name and name + "permute_2")(nn)
+    nn = mlp_block(nn, channels_mlp_dim, use_bias=use_bias, activation=activation, name=name and name + "channel_mixing/")
+    channel_out = nn if data_format == "channels_last" else layers.Permute((2, 1), name=name and name + "permute_3")(nn)
     if drop_rate > 0:
-        channel_out = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name and name + "channel_drop")(channel_out)
-    return keras.layers.Add(name=name and name + "output")([channel_out, token_out])
+        channel_out = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name and name + "channel_drop")(channel_out)
+    return layers.Add(name=name and name + "output")([channel_out, token_out])
 
 
 def MLPMixer(
     num_blocks,
     patch_size,
     stem_width,
     tokens_mlp_dim,
@@ -67,37 +74,42 @@
     dropout=0,
     drop_connect_rate=0,
     classifier_activation="softmax",
     pretrained="imagenet",
     model_name="mlp_mixer",
     kwargs=None,
 ):
-    inputs = keras.Input(input_shape)
-    nn = keras.layers.Conv2D(stem_width, kernel_size=patch_size, strides=patch_size, padding="same", name="stem")(inputs)
-    nn = keras.layers.Reshape([nn.shape[1] * nn.shape[2], stem_width])(nn)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+
+    nn = layers.Conv2D(stem_width, kernel_size=patch_size, strides=patch_size, padding="valid", name="stem")(inputs)
+    new_shape = [nn.shape[1] * nn.shape[2], stem_width] if backend.image_data_format() == "channels_last" else [stem_width, nn.shape[2] * nn.shape[3]]
+    nn = layers.Reshape(new_shape)(nn)
 
     drop_connect_s, drop_connect_e = drop_connect_rate if isinstance(drop_connect_rate, (list, tuple)) else [drop_connect_rate, drop_connect_rate]
     for ii in range(num_blocks):
         name = "{}_{}/".format("MixerBlock", str(ii))
         block_drop_rate = drop_connect_s + (drop_connect_e - drop_connect_s) * ii / num_blocks
         nn = mlp_mixer_block(nn, tokens_mlp_dim, channels_mlp_dim, drop_rate=block_drop_rate, activation=activation, name=name)
     nn = layer_norm(nn, name="pre_head_layer_norm")
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling1D()(nn)  # tf.reduce_mean(nn, axis=1)
+        nn = layers.GlobalAveragePooling1D()(nn)  # tf.reduce_mean(nn, axis=1)
         if dropout > 0 and dropout < 1:
-            nn = keras.layers.Dropout(dropout)(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(nn)
+            nn = layers.Dropout(dropout)(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="head")(nn)
 
     if sam_rho != 0:
         from keras_cv_attention_models.model_surgery import SAMModel
 
         model = SAMModel(inputs, nn, name=model_name)
     else:
-        model = keras.Model(inputs, nn, name=model_name)
+        model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="tf")
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="mlp_family", pretrained=pretrained)
     return model
 
 
 BLOCK_CONFIGS = {
     "s32": {
@@ -148,38 +160,45 @@
         "stem_width": 1280,
         "tokens_mlp_dim": 640,
         "channels_mlp_dim": 5120,
     },
 }
 
 
+@register_model
 def MLPMixerS32(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MLPMixer(**BLOCK_CONFIGS["s32"], **locals(), model_name="mlp_mixer_s32", **kwargs)
 
 
+@register_model
 def MLPMixerS16(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MLPMixer(**BLOCK_CONFIGS["s16"], **locals(), model_name="mlp_mixer_s16", **kwargs)
 
 
+@register_model
 def MLPMixerB32(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MLPMixer(**BLOCK_CONFIGS["b32"], **locals(), model_name="mlp_mixer_b32", **kwargs)
 
 
+@register_model
 def MLPMixerB16(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MLPMixer(**BLOCK_CONFIGS["b16"], **locals(), model_name="mlp_mixer_b16", **kwargs)
 
 
+@register_model
 def MLPMixerL32(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MLPMixer(**BLOCK_CONFIGS["l32"], **locals(), model_name="mlp_mixer_l32", **kwargs)
 
 
+@register_model
 def MLPMixerL16(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MLPMixer(**BLOCK_CONFIGS["l16"], **locals(), model_name="mlp_mixer_l16", **kwargs)
 
 
+@register_model
 def MLPMixerH14(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MLPMixer(**BLOCK_CONFIGS["h14"], **locals(), model_name="mlp_mixer_h14", **kwargs)
 
 
 if __name__ == "__convert__":
     aa = np.load("../models/imagenet1k_Mixer-B_16.npz")
     bb = {kk: vv for kk, vv in aa.items()}
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/res_mlp.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/res_mlp.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,25 +1,31 @@
-from tensorflow import keras
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import activation_by_name, add_pre_post_process
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
     "resmlp12": {"imagenet": "de6531fb461bcf52c25d3c36aa515583"},
     "resmlp24": {"imagenet": "f8127be7f8ba564fc59552c0cf6f3401"},
     "resmlp36": {"imagenet": "d0d3e6b09d7e975aaf46ff777c1fd73e"},
     "resmlp_b24": {"imagenet": "d7808ef59c06d2f1975ffddd28be82de", "imagenet22k": "8d3ae1abdac60b21ed1f2840b656b6bf"},
 }
 
 
-@keras.utils.register_keras_serializable(package="resmlp")
-class ChannelAffine(keras.layers.Layer):
+@backend.register_keras_serializable(package="resmlp")
+class ChannelAffine(layers.Layer):
     def __init__(self, use_bias=True, weight_init_value=1, axis=-1, **kwargs):
-        super(ChannelAffine, self).__init__(**kwargs)
+        super().__init__(**kwargs)
         self.use_bias, self.weight_init_value, self.axis = use_bias, weight_init_value, axis
-        self.ww_init = keras.initializers.Constant(weight_init_value) if weight_init_value != 1 else "ones"
+        if isinstance(weight_init_value, (int, float)):
+            self.ww_init = initializers.Constant(weight_init_value) if weight_init_value != 1 else "ones"
+        else:
+            self.ww_init = weight_init_value  # Regard as built initializer
         self.bb_init = "zeros"
         self.supports_masking = False
 
     def build(self, input_shape):
         if self.axis == -1 or self.axis == len(input_shape) - 1:
             ww_shape = (input_shape[-1],)
         else:
@@ -28,54 +34,73 @@
             for ii in axis:
                 ww_shape[ii] = input_shape[ii]
             ww_shape = ww_shape[1:]  # Exclude batch dimension
 
         self.ww = self.add_weight(name="weight", shape=ww_shape, initializer=self.ww_init, trainable=True)
         if self.use_bias:
             self.bb = self.add_weight(name="bias", shape=ww_shape, initializer=self.bb_init, trainable=True)
-        super(ChannelAffine, self).build(input_shape)
+        super().build(input_shape)
 
     def call(self, inputs, **kwargs):
         return inputs * self.ww + self.bb if self.use_bias else inputs * self.ww
 
     def compute_output_shape(self, input_shape):
         return input_shape
 
+    def get_weights_channels_last(self):
+        # channel_first -> channel_last
+        weights = self.get_weights()
+        if backend.image_data_format() != "channels_last" and self.axis == 1:
+            weights = [np.squeeze(ii) for ii in weights]
+        return weights
+
+    def set_weights_channels_last(self, weights):
+        # channel_last -> channel_first
+        if backend.image_data_format() != "channels_last" and self.axis == 1:
+            weights = [np.reshape(ii, self.ww.shape) for ii in weights]
+        return self.set_weights(weights)
+
     def get_config(self):
-        config = super(ChannelAffine, self).get_config()
-        config.update({"use_bias": self.use_bias, "weight_init_value": self.weight_init_value, "axis": self.axis})
+        config = super().get_config()
+        config.update({"use_bias": self.use_bias, "axis": self.axis})
         return config
 
 
 # NOT using
 def channel_affine(inputs, use_bias=True, weight_init_value=1, name=""):
-    ww_init = keras.initializers.Constant(weight_init_value) if weight_init_value != 1 else "ones"
-    nn = keras.backend.expand_dims(inputs, 1)
-    nn = keras.layers.DepthwiseConv2D(1, depthwise_initializer=ww_init, use_bias=use_bias, name=name)(nn)
-    return keras.backend.squeeze(nn, 1)
+    ww_init = initializers.Constant(weight_init_value) if weight_init_value != 1 else "ones"
+    nn = functional.expand_dims(inputs, 1)
+    nn = layers.DepthwiseConv2D(1, depthwise_initializer=ww_init, use_bias=use_bias, name=name)(nn)
+    return functional.squeeze(nn, 1)
 
 
 def res_mlp_block(inputs, channels_mlp_dim, drop_rate=0, activation="gelu", name=None):
-    nn = ChannelAffine(use_bias=True, name=name + "norm_1")(inputs)
-    nn = keras.layers.Permute((2, 1), name=name + "permute_1")(nn)
-    nn = keras.layers.Dense(nn.shape[-1], name=name + "token_mixing")(nn)
-    nn = keras.layers.Permute((2, 1), name=name + "permute_2")(nn)
-    nn = ChannelAffine(use_bias=False, name=name + "gamma_1")(nn)
+    channel_axis = -1 if backend.image_data_format() == "channels_last" else 1
+    input_channel = inputs.shape[-1] if backend.image_data_format() == "channels_last" else inputs.shape[1]
+
+    nn = ChannelAffine(use_bias=True, axis=channel_axis, name=name + "norm_1")(inputs)
+    nn = layers.Permute((2, 1), name=name + "permute_1")(nn) if backend.image_data_format() == "channels_last" else nn
+    nn = layers.Dense(nn.shape[-1], name=name + "token_mixing")(nn)
+    nn = layers.Permute((2, 1), name=name + "permute_2")(nn) if backend.image_data_format() == "channels_last" else nn
+    nn = ChannelAffine(use_bias=False, axis=channel_axis, name=name + "gamma_1")(nn)
     if drop_rate > 0:
-        nn = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "token_drop")(nn)
-    token_out = keras.layers.Add(name=name + "add_1")([inputs, nn])
+        nn = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "token_drop")(nn)
+    token_out = layers.Add(name=name + "add_1")([inputs, nn])
 
-    nn = ChannelAffine(use_bias=True, name=name + "norm_2")(token_out)
-    nn = keras.layers.Dense(channels_mlp_dim, name=name + "channel_mixing_1")(nn)
+    nn = ChannelAffine(use_bias=True, axis=channel_axis, name=name + "norm_2")(token_out)
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute((2, 1), name=name and name + "permute_3")(nn)
+    nn = layers.Dense(channels_mlp_dim, name=name + "channel_mixing_1")(nn)
     nn = activation_by_name(nn, activation, name=name + activation)
-    nn = keras.layers.Dense(inputs.shape[-1], name=name + "channel_mixing_2")(nn)
-    channel_out = ChannelAffine(use_bias=False, name=name + "gamma_2")(nn)
+    nn = layers.Dense(input_channel, name=name + "channel_mixing_2")(nn)
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute((2, 1), name=name and name + "permute_4")(nn)
+    channel_out = ChannelAffine(use_bias=False, axis=channel_axis, name=name + "gamma_2")(nn)
     if drop_rate > 0:
-        channel_out = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "channel_drop")(channel_out)
-    nn = keras.layers.Add(name=name + "output")([channel_out, token_out])
+        channel_out = layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + "channel_drop")(channel_out)
+    # print(f">>>> {inputs.shape = }, {channel_out.shape = }, {token_out.shape = }")
+    nn = layers.Add(name=name + "output")([channel_out, token_out])
     return nn
 
 
 def ResMLP(
     num_blocks,
     patch_size,
     stem_width,
@@ -87,38 +112,42 @@
     dropout=0,
     drop_connect_rate=0,
     classifier_activation="softmax",
     pretrained="imagenet",
     model_name="resmlp",
     kwargs=None,
 ):
-    inputs = keras.Input(input_shape)
-    nn = keras.layers.Conv2D(stem_width, kernel_size=patch_size, strides=patch_size, padding="valid", name="stem")(inputs)
-    nn = keras.layers.Reshape([nn.shape[1] * nn.shape[2], stem_width])(nn)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+    nn = layers.Conv2D(stem_width, kernel_size=patch_size, strides=patch_size, padding="valid", name="stem")(inputs)
+    new_shape = [nn.shape[1] * nn.shape[2], stem_width] if backend.image_data_format() == "channels_last" else [stem_width, nn.shape[2] * nn.shape[3]]
+    nn = layers.Reshape(new_shape)(nn)
 
     drop_connect_s, drop_connect_e = drop_connect_rate if isinstance(drop_connect_rate, (list, tuple)) else [drop_connect_rate, drop_connect_rate]
     for ii in range(num_blocks):
         name = "{}_{}_".format("ResMlpBlock", str(ii + 1))
         block_drop_rate = drop_connect_s + (drop_connect_e - drop_connect_s) * ii / num_blocks
         nn = res_mlp_block(nn, channels_mlp_dim=channels_mlp_dim, drop_rate=block_drop_rate, activation=activation, name=name)
-    nn = ChannelAffine(name="pre_head_norm")(nn)
+    nn = ChannelAffine(axis=-1 if backend.image_data_format() == "channels_last" else 1, name="pre_head_norm")(nn)
 
     if num_classes > 0:
         # nn = tf.reduce_mean(nn, axis=1)
-        nn = keras.layers.GlobalAveragePooling1D()(nn)
+        nn = layers.GlobalAveragePooling1D()(nn)
         if dropout > 0 and dropout < 1:
-            nn = keras.layers.Dropout(dropout)(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(dropout)(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
     if sam_rho != 0:
         from keras_cv_attention_models.model_surgery import SAMModel
 
         model = SAMModel(inputs, nn, name=model_name)
     else:
-        model = keras.Model(inputs, nn, name=model_name)
+        model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="mlp_family", pretrained=pretrained)
     return model
 
 
 BLOCK_CONFIGS = {
     "12": {
@@ -144,21 +173,25 @@
         "patch_size": 8,
         "stem_width": 768,
         "channels_mlp_dim": 768 * 4,
     },
 }
 
 
+@register_model
 def ResMLP12(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return ResMLP(**BLOCK_CONFIGS["12"], **locals(), model_name="resmlp12", **kwargs)
 
 
+@register_model
 def ResMLP24(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return ResMLP(**BLOCK_CONFIGS["24"], **locals(), model_name="resmlp24", **kwargs)
 
 
+@register_model
 def ResMLP36(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return ResMLP(**BLOCK_CONFIGS["36"], **locals(), model_name="resmlp36", **kwargs)
 
 
+@register_model
 def ResMLP_B24(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return ResMLP(**BLOCK_CONFIGS["b24"], **locals(), model_name="resmlp_b24", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mlp_family/wave_mlp.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mlp_family/wave_mlp.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,10 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     batchnorm_with_activation,
     conv2d_no_bias,
     drop_block,
     group_norm,
     # mlp_block, # cannot import name 'mlp_block' due to circular import
@@ -14,81 +15,78 @@
 PRETRAINED_DICT = {
     "wavemlp_t": {"imagenet": "c8fe3c22c129180c5cff7b734ede831c"},
     "wavemlp_s": {"imagenet": "b0375b030d1c3012232286cff76d301d"},
     "wavemlp_m": {"imagenet": "d2364156144ab259069026b417ca21da"},
 }
 
 
-def mlp_block(inputs, hidden_dim, output_channel=-1, drop_rate=0, use_conv=False, activation="gelu", name=None):
-    output_channel = output_channel if output_channel > 0 else inputs.shape[-1]
-    if use_conv:
-        nn = keras.layers.Conv2D(hidden_dim, kernel_size=1, use_bias=True, name=name and name + "Conv_0")(inputs)
-    else:
-        nn = keras.layers.Dense(hidden_dim, name=name and name + "Dense_0")(inputs)
+def mlp_block(inputs, hidden_dim, out_channel, drop_rate=0, activation="gelu", name=""):
+    nn = layers.Conv2D(hidden_dim, kernel_size=1, use_bias=True, name=name and name + "Conv_0")(inputs)
     nn = activation_by_name(nn, activation, name=name and name + activation)
-    nn = keras.layers.Dropout(drop_rate) if drop_rate > 0 else nn
-    if use_conv:
-        nn = keras.layers.Conv2D(output_channel, kernel_size=1, use_bias=True, name=name and name + "Conv_1")(nn)
-    else:
-        nn = keras.layers.Dense(output_channel, name=name and name + "Dense_1")(nn)
-    nn = keras.layers.Dropout(drop_rate) if drop_rate > 0 else nn
+    nn = layers.Dropout(drop_rate) if drop_rate > 0 else nn
+    nn = layers.Conv2D(out_channel, kernel_size=1, use_bias=True, name=name and name + "Conv_1")(nn)
+    nn = layers.Dropout(drop_rate) if drop_rate > 0 else nn
     return nn
 
 
 def phase_aware_token_mixing(inputs, out_channel=-1, qkv_bias=False, output_dropout=0, activation="gelu", name=None):
-    out_channel = out_channel if out_channel > 0 else inputs.shape[-1]
+    input_channel = inputs.shape[-1] if backend.image_data_format() == "channels_last" else inputs.shape[1]
+    out_channel = out_channel if out_channel > 0 else input_channel
 
     theta_h = conv2d_no_bias(inputs, out_channel, kernel_size=1, use_bias=True, name=name and name + "theta_h_")
     theta_h = batchnorm_with_activation(theta_h, activation="relu", name=name and name + "theta_h_")  # Fixed as relu [ ??? ]
     height = conv2d_no_bias(inputs, out_channel, kernel_size=1, use_bias=qkv_bias, name=name and name + "height_")
-    # height = keras.layers.Concatenate(axis=-1)([height * tf.cos(theta_h), height * tf.sin(theta_h)])
-    height_cos = keras.layers.Multiply()([height, tf.cos(theta_h)])
-    height_sin = keras.layers.Multiply()([height, tf.sin(theta_h)])
-    height = keras.layers.Concatenate(axis=-1)([height_cos, height_sin])
-    height = conv2d_no_bias(height, out_channel, kernel_size=(1, 7), padding="SAME", groups=out_channel, use_bias=False, name=name and name + "height_down_")
+    # height = layers.Concatenate(axis=-1)([height * functional.cos(theta_h), height * functional.sin(theta_h)])
+    height_cos = layers.Multiply()([height, functional.cos(theta_h)])
+    height_sin = layers.Multiply()([height, functional.sin(theta_h)])
+    height = layers.Concatenate(axis=-1 if backend.image_data_format() == "channels_last" else 1)([height_cos, height_sin])
+    height = conv2d_no_bias(height, out_channel, kernel_size=(1, 7), padding="same", groups=out_channel, use_bias=False, name=name and name + "height_down_")
 
     theta_w = conv2d_no_bias(inputs, out_channel, kernel_size=1, use_bias=True, name=name and name + "theta_w_")
     theta_w = batchnorm_with_activation(theta_w, activation="relu", name=name and name + "theta_w_")  # Fixed as relu [ ??? ]
     width = conv2d_no_bias(inputs, out_channel, kernel_size=1, use_bias=qkv_bias, name=name and name + "width_")
-    # width = keras.layers.Concatenate(axis=-1)([width * tf.cos(theta_w), width * tf.sin(theta_w)])
-    width_cos = keras.layers.Multiply()([width, tf.cos(theta_w)])
-    width_sin = keras.layers.Multiply()([width, tf.sin(theta_w)])
-    width = keras.layers.Concatenate(axis=-1)([width_cos, width_sin])
-    width = conv2d_no_bias(width, out_channel, kernel_size=(7, 1), padding="SAME", groups=out_channel, use_bias=False, name=name and name + "width_down_")
+    # width = layers.Concatenate(axis=-1)([width * functional.cos(theta_w), width * functional.sin(theta_w)])
+    width_cos = layers.Multiply()([width, functional.cos(theta_w)])
+    width_sin = layers.Multiply()([width, functional.sin(theta_w)])
+    width = layers.Concatenate(axis=-1 if backend.image_data_format() == "channels_last" else 1)([width_cos, width_sin])
+    width = conv2d_no_bias(width, out_channel, kernel_size=(7, 1), padding="same", groups=out_channel, use_bias=False, name=name and name + "width_down_")
 
     channel = conv2d_no_bias(inputs, out_channel, kernel_size=1, use_bias=qkv_bias, name=name and name + "channel_")
 
     # print(f"{height.shape = }, {width.shape = }, {channel.shape = }, {out_channel = }")
-    nn = keras.layers.Add(name=name and name + "combine")([height, width, channel])
-    nn = keras.layers.GlobalAveragePooling2D(keepdims=True)(nn)
-    nn = mlp_block(nn, out_channel // 4, output_channel=out_channel * 3, use_conv=True, activation=activation, name=name and name + "reweight_")
-    nn = keras.layers.Reshape([1, 1, out_channel, 3])(nn)
-    nn = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(nn)
-    attn_height, attn_width, attn_channel = tf.unstack(nn, axis=-1)
-    # attn = keras.layers.Add()([height * attn_height, width * attn_width, channel * attn_channel])
-    attn_height = keras.layers.Multiply()([height, attn_height])
-    attn_width = keras.layers.Multiply()([width, attn_width])
-    attn_channel = keras.layers.Multiply()([channel, attn_channel])
-    attn = keras.layers.Add()([attn_height, attn_width, attn_channel])
+    nn = layers.Add(name=name and name + "combine")([height, width, channel])
+    nn = layers.GlobalAveragePooling2D(keepdims=True)(nn)
+    nn = mlp_block(nn, out_channel // 4, out_channel=out_channel * 3, activation=activation, name=name and name + "reweight_")
+    nn = layers.Reshape([1, 1, out_channel, 3] if backend.image_data_format() == "channels_last" else [out_channel, 1, 1, 3])(nn)
+    nn = layers.Softmax(axis=-1, name=name and name + "attention_scores")(nn)
+    attn_height, attn_width, attn_channel = functional.unstack(nn, axis=-1)
+    # attn = layers.Add()([height * attn_height, width * attn_width, channel * attn_channel])
+    attn_height = layers.Multiply()([height, attn_height])
+    attn_width = layers.Multiply()([width, attn_width])
+    attn_channel = layers.Multiply()([channel, attn_channel])
+    # print(f"{attn_height.shape = }, {attn_width.shape = }, {attn_channel.shape = }")
+    attn = layers.Add()([attn_height, attn_width, attn_channel])
 
     out = conv2d_no_bias(attn, out_channel, kernel_size=1, use_bias=True, name=name and name + "out_")
-    out = keras.layers.Dropout(output_dropout, name=name and name + "out_drop")(out) if output_dropout > 0 else out
+    out = layers.Dropout(output_dropout, name=name and name + "out_drop")(out) if output_dropout > 0 else out
     return out
 
 
 def wave_block(inputs, qkv_bias=False, mlp_ratio=4, use_group_norm=False, drop_rate=0, activation="gelu", name=""):
+    input_channel = inputs.shape[-1] if backend.image_data_format() == "channels_last" else inputs.shape[1]
+
     attn = group_norm(inputs, groups=1, name=name + "attn_") if use_group_norm else batchnorm_with_activation(inputs, activation=None, name=name + "attn_")
-    attn = phase_aware_token_mixing(attn, qkv_bias=qkv_bias, activation=activation, name=name + "attn_")
+    attn = phase_aware_token_mixing(attn, out_channel=input_channel, qkv_bias=qkv_bias, activation=activation, name=name + "attn_")
     attn = drop_block(attn, drop_rate=drop_rate, name=name + "attn_")
-    attn_out = keras.layers.Add(name=name + "attn_out")([inputs, attn])
+    attn_out = layers.Add(name=name + "attn_out")([inputs, attn])
 
     mlp = group_norm(attn_out, groups=1, name=name + "mlp_") if use_group_norm else batchnorm_with_activation(attn_out, activation=None, name=name + "mlp_")
-    mlp = mlp_block(mlp, int(inputs.shape[-1] * mlp_ratio), use_conv=True, activation=activation, name=name + "mlp_")
+    mlp = mlp_block(mlp, int(input_channel * mlp_ratio), out_channel=input_channel, activation=activation, name=name + "mlp_")
     mlp = drop_block(mlp, name=name + "mlp_")
-    mlp_out = keras.layers.Add(name=name + "mlp_out")([attn_out, mlp])
+    mlp_out = layers.Add(name=name + "mlp_out")([attn_out, mlp])
     return mlp_out
 
 
 def WaveMLP(
     num_blocks=[2, 2, 4, 2],
     out_channels=[64, 128, 320, 512],
     stem_width=-1,
@@ -103,17 +101,20 @@
     drop_connect_rate=0,
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="wavemlp",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
     stem_width = stem_width if stem_width > 0 else out_channels[0]
-    nn = keras.layers.ZeroPadding2D(padding=2, name="stem_pad")(inputs)
+    nn = layers.ZeroPadding2D(padding=2, name="stem_pad")(inputs)
     nn = conv2d_no_bias(nn, stem_width, kernel_size=7, strides=4, padding="valid", use_bias=True, name="stem_")
     if use_downsample_norm:
         nn = group_norm(nn, groups=1, name="stem_") if use_group_norm else batchnorm_with_activation(nn, activation=None, name="stem_")
 
     """ stage [1, 2, 3, 4] """
     total_blocks = sum(num_blocks)
     global_block_id = 0
@@ -128,51 +129,55 @@
             name = stage_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             global_block_id += 1
             nn = wave_block(nn, qkv_bias, mlp_ratio, use_group_norm, block_drop_rate, activation=activation, name=name)
 
     if num_classes > 0:
         nn = group_norm(nn, groups=1, name="output_") if use_group_norm else batchnorm_with_activation(nn, activation=None, name="output_")
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
         if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
     if sam_rho != 0:
         from keras_cv_attention_models.model_surgery import SAMModel
 
         model = SAMModel(inputs, nn, name=model_name)
     else:
-        model = keras.models.Model(inputs, nn, name=model_name)
+        model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "mlp_family", pretrained)
     return model
 
 
+@register_model
 def WaveMLP_T(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 4, 2]
     out_channels = [64, 128, 320, 512]
     return WaveMLP(**locals(), model_name="wavemlp_t", **kwargs)
 
 
+@register_model
 def WaveMLP_S(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 3, 10, 3]
     out_channels = [64, 128, 320, 512]
     use_group_norm = True
     return WaveMLP(**locals(), model_name="wavemlp_s", **kwargs)
 
 
+@register_model
 def WaveMLP_M(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 18, 3]
     out_channels = [64, 128, 320, 512]
     mlp_ratios = [8, 8, 4, 4]
     use_group_norm = True
     use_downsample_norm = False
     return WaveMLP(**locals(), model_name="wavemlp_m", **kwargs)
 
 
+@register_model
 def WaveMLP_B(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained=None, **kwargs):
     num_blocks = [2, 2, 18, 2]
     out_channels = [96, 192, 384, 768]
     use_group_norm = True
     use_downsample_norm = False
     return WaveMLP(**locals(), model_name="wavemlp_b", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/fbnetv3.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/fbnetv3.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from keras_cv_attention_models.mobilenetv3_family.mobilenetv3 import MobileNetV3
+from keras_cv_attention_models.models import register_model
 
 
 def FBNetV3(
     num_blocks=[2, 4, 1, 4, 1, 4, 1, 5, 1, 5, 1],
     out_channels=[16, 24, 40, 40, 72, 72, 120, 120, 184, 184, 224],
     expands=[1, [4, 2, 2, 2], 5, 3, 5, 3, 5, 3, 6, 4, 6],
     kernel_sizes=[3, 5, 5, 5, 5, 3, 3, 5, 3, 5, 5],
@@ -17,27 +18,30 @@
     model_name="fbnetv3",
     **kwargs,
 ):
     kwargs.pop("kwargs", None)
     return MobileNetV3(**locals(), **kwargs)
 
 
+@register_model
 def FBNetV3B(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return FBNetV3(**locals(), model_name="fbnetv3_b", **kwargs)
 
 
+@register_model
 def FBNetV3D(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 6, 1, 4, 1, 4, 1, 6, 1, 5, 1]
     out_channels = [16, 24, 40, 40, 72, 72, 128, 128, 208, 208, 240]
     expands = [1, [5, 2, 2, 2, 2, 2], 4, 3, 5, 3, 5, 3, 6, 5, 6]
     kernel_sizes = [3, 3, 5, 3, 3, 3, 3, 5, 3, 5, 5]
     stem_width = 24
     return FBNetV3(**locals(), model_name="fbnetv3_d", **kwargs)
 
 
+@register_model
 def FBNetV3G(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 5, 1, 4, 1, 4, 1, 8, 1, 6, 2]
     out_channels = [24, 40, 56, 56, 104, 104, 160, 160, 264, 264, 288]
     expands = [1, [4, 2, 2, 2, 2], 4, 3, 5, 3, 5, 3, 6, 5, 6]
     kernel_sizes = [3, 5, 5, 5, 5, 3, 3, 5, 3, 5, 5]
     stem_width = 32
     return FBNetV3(**locals(), model_name="fbnetv3_g", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/lcnet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/lcnet.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from keras_cv_attention_models.mobilenetv3_family.mobilenetv3 import MobileNetV3
+from keras_cv_attention_models.models import register_model
 
 
 def LCNet(
     num_blocks=[1, 2, 2, 1, 5, 2],
     out_channels=[32, 64, 128, 256, 256, 512],
     expands=1,
     kernel_sizes=[3, 3, 3, 3, 5, 5],
@@ -16,32 +17,38 @@
     model_name="lcnet",
     **kwargs,
 ):
     kwargs.pop("kwargs", None)
     return MobileNetV3(**locals(), **kwargs)
 
 
+@register_model
 def LCNet050(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return LCNet(**locals(), width_ratio=0.5, model_name="lcnet_050", **kwargs)
 
 
+@register_model
 def LCNet075(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return LCNet(**locals(), width_ratio=0.75, model_name="lcnet_075", **kwargs)
 
 
+@register_model
 def LCNet100(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return LCNet(**locals(), model_name="lcnet_100", **kwargs)
 
 
+@register_model
 def LCNet150(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     use_output_feature_bias = False
     return LCNet(**locals(), width_ratio=1.5, model_name="lcnet_150", **kwargs)
 
 
+@register_model
 def LCNet200(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     use_output_feature_bias = False
     return LCNet(**locals(), width_ratio=2.0, model_name="lcnet_200", **kwargs)
 
 
+@register_model
 def LCNet250(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     use_output_feature_bias = False
     return LCNet(**locals(), width_ratio=2.5, model_name="lcnet_250", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/mobilenetv3.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/mobilenetv3.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     batchnorm_with_activation,
     conv2d_no_bias,
     inverted_residual_block,
     make_divisible,
     add_pre_post_process,
@@ -35,31 +35,31 @@
     "tinynet_c": {"imagenet": "29ce15979b8800176621780b0ea91eaa"},
     "tinynet_d": {"imagenet": "e08615d88e71f1548d040e697926514e"},
     "tinynet_e": {"imagenet": "514fbbcd582db6d4297a140efb84af9a"},
 }
 
 
 def avg_pool_conv_output(inputs, output_num_features=1280, use_output_feature_bias=True, activation="hard_swish"):
-    # nn = keras.layers.AveragePooling2D(pool_size=inputs.shape[1:3], name="avg_pool")(inputs)
-    # nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(inputs)[:, None, None, :]
-    h_axis, w_axis = [2, 3] if K.image_data_format() == "channels_first" else [1, 2]
-    nn = tf.reduce_mean(inputs, [h_axis, w_axis], keepdims=True)  # using AveragePooling2D cannot set dynamic input_shape
+    # nn = layers.AveragePooling2D(pool_size=inputs.shape[1:3], name="avg_pool")(inputs)
+    # nn = layers.GlobalAveragePooling2D(name="avg_pool")(inputs)[:, None, None, :]
+    h_axis, w_axis = [1, 2] if image_data_format() == "channels_last" else [2, 3]
+    nn = functional.reduce_mean(inputs, [h_axis, w_axis], keepdims=True)  # using AvgPool2D cannot set dynamic input_shape
     if output_num_features > 0:
         nn = conv2d_no_bias(nn, make_divisible(output_num_features, 8), use_bias=use_output_feature_bias, name="features_")
         nn = activation_by_name(nn, activation, name="features_")
-    nn = keras.layers.Flatten()(nn)
+    nn = layers.Flatten()(nn)
     return nn
 
 
 def conv_avg_pool_output(inputs, output_num_features=1280, use_output_feature_bias=True, activation="hard_swish"):
     nn = inputs
     if output_num_features > 0:
         nn = conv2d_no_bias(nn, make_divisible(output_num_features, 8), use_bias=use_output_feature_bias, name="features_")
         nn = batchnorm_with_activation(nn, activation=activation, name="features_")
-    nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+    nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
     return nn
 
 
 def MobileNetV3(
     num_blocks=[1, 2, 3, 4, 2, 3],  # [Stack parameters]
     out_channels=[16, 24, 40, 80, 112, 160],
     expands=[1, [4, 3], 3, [6, 2.5, 2.3, 2.3], 6, 6],
@@ -86,15 +86,18 @@
     drop_connect_rate=0,
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="mobilenetv3",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
     stem_width = stem_width if fix_stem else make_divisible(stem_width * width_ratio, 8)
     nn = conv2d_no_bias(inputs, stem_width, kernel_size=3, strides=2, padding="same", name="stem_")
     nn = batchnorm_with_activation(nn, activation=stem_feature_activation, name="stem_")
 
     block_kwargs = {  # Common parameters for all blocks
         "is_torch_mode": True,
         "se_activation": se_activation,
@@ -135,17 +138,17 @@
     if num_classes > 0:
         if use_avg_pool_conv_output:
             nn = avg_pool_conv_output(nn, output_num_features, use_output_feature_bias, stem_feature_activation)
         else:
             nn = conv_avg_pool_output(nn, output_num_features, use_output_feature_bias, stem_feature_activation)
 
         if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
-    model = keras.models.Model(inputs, nn, name=model_name)
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "mobilenetv3_family", pretrained)
     return model
 
 
 def MobileNetV3Large(model_name="mobilenetv3_large", **kwargs):
     kwargs.pop("kwargs", None)
@@ -164,26 +167,31 @@
     model_name="mobilenetv3_small",
     **kwargs,
 ):
     kwargs.pop("kwargs", None)
     return MobileNetV3(**locals(), **kwargs)
 
 
+@register_model
 def MobileNetV3Large075(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MobileNetV3Large(**locals(), width_ratio=0.75, model_name="mobilenetv3_large_075", **kwargs)
 
 
+@register_model
 def MobileNetV3Large100(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MobileNetV3Large(**locals(), model_name="mobilenetv3_large_100", **kwargs)
 
 
+@register_model
 def MobileNetV3Small050(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     fix_stem = True
     return MobileNetV3Small(**locals(), width_ratio=0.5, model_name="mobilenetv3_small_050", **kwargs)
 
 
+@register_model
 def MobileNetV3Small075(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MobileNetV3Small(**locals(), width_ratio=0.75, model_name="mobilenetv3_small_075", **kwargs)
 
 
+@register_model
 def MobileNetV3Small100(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MobileNetV3Small(**locals(), model_name="mobilenetv3_small_100", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilenetv3_family/tinynet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilenetv3_family/tinynet.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-import tensorflow as tf
 from keras_cv_attention_models.mobilenetv3_family.mobilenetv3 import MobileNetV3
+from keras_cv_attention_models.models import register_model
 
 
 def get_expanded_width_depth(width, depth):
     out_channels = [ii * width for ii in [16, 24, 40, 80, 112, 192, 320]]
-    num_blocks = [int(tf.math.round(ii * depth)) for ii in [1, 2, 2, 3, 3, 4, 1]]
+    num_blocks = [int(round(ii * depth)) for ii in [1, 2, 2, 3, 3, 4, 1]]
     return out_channels, num_blocks
 
 
 def TinyNet(
     num_blocks=[1, 2, 2, 3, 3, 4, 1],
     out_channels=[16, 24, 40, 80, 112, 192, 320],
     expands=[1, 6, 6, 6, 6, 6, 6],
@@ -29,30 +29,35 @@
     **kwargs,
 ):
     stem_feature_activation = activations
     kwargs.pop("kwargs", None)
     return MobileNetV3(**locals(), **kwargs)
 
 
+@register_model
 def TinyNetA(input_shape=(192, 192, 3), num_classes=1000, activations="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     out_channels, num_blocks = get_expanded_width_depth(1.0, 1.2)
     return TinyNet(**locals(), model_name="tinynet_a", **kwargs)
 
 
+@register_model
 def TinyNetB(input_shape=(188, 188, 3), num_classes=1000, activations="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     out_channels, num_blocks = get_expanded_width_depth(0.75, 1.1)
     return TinyNet(**locals(), model_name="tinynet_b", **kwargs)
 
 
+@register_model
 def TinyNetC(input_shape=(184, 184, 3), num_classes=1000, activations="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     out_channels, num_blocks = get_expanded_width_depth(0.54, 0.85)
     return TinyNet(**locals(), model_name="tinynet_c", **kwargs)
 
 
+@register_model
 def TinyNetD(input_shape=(152, 152, 3), num_classes=1000, activations="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     out_channels, num_blocks = get_expanded_width_depth(0.54, 0.695)
     return TinyNet(**locals(), model_name="tinynet_d", **kwargs)
 
 
+@register_model
 def TinyNetE(input_shape=(106, 106, 3), num_classes=1000, activations="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     out_channels, num_blocks = get_expanded_width_depth(0.51, 0.6)
     return TinyNet(**locals(), model_name="tinynet_e", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilevit/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilevit/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilevit/mobilevit.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilevit/mobilevit.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,14 @@
-import tensorflow as tf
-from tensorflow import keras
+import math
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
+    activation_by_name,
+    add_with_layer_scale_and_drop_block,
     batchnorm_with_activation,
     conv2d_no_bias,
     ChannelAffine,
     depthwise_conv2d_no_bias,
     drop_block,
     drop_connect_rates_split,
     group_norm,
@@ -38,232 +42,287 @@
         "imagenet": {256: "1fe59d8bb2662761084d1c04259a778d"},
         "imagenet22k": {256: "931f0be1761bcf8443359ec1661bb6a7", 384: "1dc6cdafb187611e5a4819272d64fba7"},
     },
 }
 
 
 def bottle_in_linear_out_block(inputs, out_channel, strides=1, expand_ratio=4, use_shortcut=False, drop_rate=0, activation="swish", name=""):
-    hidden_dim = int(inputs.shape[-1] * expand_ratio)
+    input_channel = inputs.shape[-1 if image_data_format() == "channels_last" else 1]
+    hidden_dim = int(input_channel * expand_ratio)
     deep = conv2d_no_bias(inputs, hidden_dim, kernel_size=1, strides=1, name=name + "deep_1_")
     deep = batchnorm_with_activation(deep, activation=activation, name=name + "deep_1_")
-    deep = depthwise_conv2d_no_bias(deep, kernel_size=3, strides=strides, padding="SAME", name=name + "deep_2_")
+    deep = depthwise_conv2d_no_bias(deep, kernel_size=3, strides=strides, padding="same", name=name + "deep_2_")
     deep = batchnorm_with_activation(deep, activation=activation, name=name + "deep_2_")
     deep = conv2d_no_bias(deep, out_channel, kernel_size=1, strides=1, name=name + "deep_3_")
     deep = batchnorm_with_activation(deep, activation=None, name=name + "deep_3_")
     deep = drop_block(deep, drop_rate=drop_rate, name=name + "deep_")
 
-    out = keras.layers.Add()([inputs, deep]) if use_shortcut else deep
+    out = layers.Add()([inputs, deep]) if use_shortcut else deep
     return out
 
 
 def linear_self_attention(inputs, qkv_bias=False, out_bias=False, attn_axis=2, attn_dropout=0, name=None):
-    input_channel = inputs.shape[-1]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    input_channel = inputs.shape[channel_axis]
     qkv = conv2d_no_bias(inputs, 1 + input_channel * 2, kernel_size=1, use_bias=qkv_bias, name=name and name + "qkv_")
-    query, key, value = tf.split(qkv, [1, input_channel, input_channel], axis=-1)
-    context_score = keras.layers.Softmax(axis=attn_axis, name=name and name + "attention_scores")(query)  # on patch_hh * patch_ww dimension
-    context_score = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(context_score) if attn_dropout > 0 else context_score
+    query, key, value = functional.split(qkv, [1, input_channel, input_channel], axis=channel_axis)
+    context_score = layers.Softmax(axis=attn_axis, name=name and name + "attention_scores")(query)  # on patch_hh * patch_ww dimension
+    context_score = layers.Dropout(attn_dropout, name=name and name + "attn_drop")(context_score) if attn_dropout > 0 else context_score
     # print(f"{query.shape = }, {key.shape = }, {value.shape = }, {context_score.shape = }")
 
-    context_vector = keras.layers.Multiply()([key, context_score])  # [batch, height, width, input_channel]
-    context_vector = tf.reduce_sum(context_vector, keepdims=True, axis=attn_axis)  # on patch_hh * patch_ww dimension
+    context_vector = layers.Multiply()([key, context_score])  # [batch, height, width, input_channel]
+    context_vector = functional.reduce_sum(context_vector, keepdims=True, axis=attn_axis)  # on patch_hh * patch_ww dimension
 
-    out = tf.nn.relu(value) * context_vector
+    out = functional.relu(value) * context_vector
     out = conv2d_no_bias(out, input_channel, kernel_size=1, use_bias=out_bias, name=name and name + "output")
     return out
 
 
 def mhsa_mlp_block(
     inputs,
     out_channel,
     num_heads=4,
     qkv_bias=True,
     mlp_ratio=4,
     num_norm_groups=-1,  # -1 or 0 for V1 using layer_norm, or 1 for V2 using group_norm
-    use_linear_attention=False,  # False for V1, True for V2
-    use_conv_mlp=False,  # False for V1, True for V2
+    mlp_drop_rate=0,
+    attn_drop_rate=0,
+    drop_rate=0,
+    layer_scale=-1,
+    activation="gelu",
+    name=None,
+):
+    # print(f"{inputs.shape = }")
+    attn = group_norm(inputs, groups=num_norm_groups, axis=-1, name=name + "attn_") if num_norm_groups > 0 else layer_norm(inputs, axis=-1, name=name + "attn_")
+    attn = multi_head_self_attention(attn, num_heads, qkv_bias=qkv_bias, out_bias=True, attn_dropout=attn_drop_rate, name=name and name + "attn_mhsa_")
+    attn = add_with_layer_scale_and_drop_block(inputs, attn, layer_scale=layer_scale, drop_rate=drop_rate, axis=-1, name=name and name + "1_")
+
+    mlp = group_norm(attn, groups=num_norm_groups, axis=-1, name=name + "mlp_") if num_norm_groups > 0 else layer_norm(attn, axis=-1, name=name + "mlp_")
+    mlp = mlp_block(mlp, int(out_channel * mlp_ratio), drop_rate=mlp_drop_rate, activation=activation, name=name and name + "mlp_")
+    return add_with_layer_scale_and_drop_block(attn, mlp, layer_scale=layer_scale, drop_rate=drop_rate, axis=-1, name=name and name + "2_")
+
+
+def linear_mhsa_mlp_block(
+    inputs,
+    out_channel,
+    num_heads=4,
+    qkv_bias=True,
+    mlp_ratio=4,
+    num_norm_groups=-1,  # -1 or 0 for V1 using layer_norm, or 1 for V2 using group_norm
     mlp_drop_rate=0,
     attn_drop_rate=0,
     drop_rate=0,
     layer_scale=-1,
     activation="gelu",
     name=None,
 ):
     attn = group_norm(inputs, groups=num_norm_groups, name=name + "attn_") if num_norm_groups > 0 else layer_norm(inputs, name=name + "attn_")
-    if use_linear_attention:  # V2
-        if num_norm_groups > 0:
-            attn = keras.layers.Reshape(attn.shape[1:])(attn)  # Or will throw error when converting tflite, if GroupNorm is followed by Conv2D
-        attn = linear_self_attention(attn, qkv_bias=qkv_bias, out_bias=True, attn_dropout=attn_drop_rate, name=name and name + "attn_mhsa_")
-    else:  # V1
-        attn = multi_head_self_attention(attn, num_heads, qkv_bias=qkv_bias, out_bias=True, attn_dropout=attn_drop_rate, name=name and name + "attn_mhsa_")
-    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name and name + "1_gamma")(attn) if layer_scale >= 0 else attn
-    attn = drop_block(attn, drop_rate=drop_rate, name=name and name + "attn_")
-    attn_out = keras.layers.Add(name=name and name + "attn_out")([inputs, attn])
-
-    mlp = group_norm(attn_out, groups=num_norm_groups, name=name + "mlp_") if num_norm_groups > 0 else layer_norm(attn_out, name=name + "mlp_")
-    if use_conv_mlp and num_norm_groups > 0:  # V2
-        mlp = keras.layers.Reshape(mlp.shape[1:])(mlp)  # Or will throw error when converting tflite, if GroupNorm is followed by Conv2D
-    mlp = mlp_block(mlp, int(out_channel * mlp_ratio), drop_rate=mlp_drop_rate, use_conv=use_conv_mlp, activation=activation, name=name and name + "mlp_")
-    mlp = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name and name + "2_gamma")(mlp) if layer_scale >= 0 else mlp
-    mlp = drop_block(mlp, drop_rate=drop_rate, name=name and name + "mlp_")
-    return keras.layers.Add(name=name and name + "out")([attn_out, mlp])
+    attn = layers.Reshape(attn.shape[1:])(attn)  # Or will throw error when converting tflite, if GroupNorm is followed by Conv2D
+    attn = linear_self_attention(attn, qkv_bias=qkv_bias, out_bias=True, attn_dropout=attn_drop_rate, name=name and name + "attn_mhsa_")
+    attn = add_with_layer_scale_and_drop_block(inputs, attn, layer_scale=layer_scale, drop_rate=drop_rate, name=name and name + "1_")
+
+    mlp = group_norm(attn, groups=num_norm_groups, name=name + "mlp_") if num_norm_groups > 0 else layer_norm(attn, name=name + "mlp_")
+    mlp = layers.Reshape(mlp.shape[1:])(mlp)  # Or will throw error when converting tflite, if GroupNorm is followed by Conv2D
+    mlp = mlp_block(mlp, int(out_channel * mlp_ratio), drop_rate=mlp_drop_rate, use_conv=True, activation=activation, name=name and name + "mlp_")
+    return add_with_layer_scale_and_drop_block(attn, mlp, layer_scale=layer_scale, drop_rate=drop_rate, name=name and name + "2_")
 
 
 def transformer_pre_process(inputs, out_channel, patch_size=2, resize_first=False, use_depthwise=False, patches_to_batch=True, activation="swish", name=""):
+    height_axis, width_axis, channel_axis = (1, 2, 3) if image_data_format() == "channels_last" else (2, 3, 1)
     nn = inputs
 
     if resize_first:  # V2
-        patch_hh, patch_ww = int(tf.math.ceil(nn.shape[1] / patch_size)), int(tf.math.ceil(nn.shape[2] / patch_size))
+        patch_hh, patch_ww = int(math.ceil(nn.shape[height_axis] / patch_size)), int(math.ceil(nn.shape[width_axis] / patch_size))
         # print(f"transformer_pre_process before resize: {nn.shape = }")
-        if patch_hh * patch_size != nn.shape[1] or patch_ww * patch_size != nn.shape[2]:
-            nn = tf.image.resize(nn, [patch_hh * patch_size, patch_ww * patch_size], method="bilinear")
+        if patch_hh * patch_size != nn.shape[height_axis] or patch_ww * patch_size != nn.shape[width_axis]:
+            nn = functional.resize(nn, [patch_hh * patch_size, patch_ww * patch_size], method="bilinear")
 
     if use_depthwise:  # V2
-        nn = depthwise_conv2d_no_bias(nn, kernel_size=3, strides=1, padding="SAME", name=name + "pre_1_")
+        nn = depthwise_conv2d_no_bias(nn, kernel_size=3, strides=1, padding="same", name=name + "pre_1_")
     else:  # V1
-        nn = conv2d_no_bias(nn, nn.shape[-1], kernel_size=3, strides=1, padding="SAME", name=name + "pre_1_")
+        nn = conv2d_no_bias(nn, nn.shape[channel_axis], kernel_size=3, strides=1, padding="same", name=name + "pre_1_")
     nn = batchnorm_with_activation(nn, activation=activation, name=name + "pre_1_")
     nn = conv2d_no_bias(nn, out_channel, kernel_size=1, strides=1, name=name + "pre_2_")
 
     if not resize_first:  # V1
-        patch_hh, patch_ww = int(tf.math.ceil(nn.shape[1] / patch_size)), int(tf.math.ceil(nn.shape[2] / patch_size))
+        patch_hh, patch_ww = int(math.ceil(nn.shape[height_axis] / patch_size)), int(math.ceil(nn.shape[width_axis] / patch_size))
         # print(f"transformer_pre_process before resize: {nn.shape = }")
-        if patch_hh * patch_size != nn.shape[1] or patch_ww * patch_size != nn.shape[2]:
-            nn = tf.image.resize(nn, [patch_hh * patch_size, patch_ww * patch_size], method="bilinear")
+        if patch_hh * patch_size != nn.shape[height_axis] or patch_ww * patch_size != nn.shape[width_axis]:
+            nn = functional.resize(nn, [patch_hh * patch_size, patch_ww * patch_size], method="bilinear")
 
     # Extract patchs, limit transpose permute length <= 4
     # [batch, height, width, channel] -> [batch, height // 2, 2, width // 2, 2, channel] -> [batch * 4, height // 2, width // 2, channel]
     # print(f"transformer_pre_process after resize: {nn.shape = }")
-    nn = tf.reshape(nn, [-1, patch_ww, patch_size, out_channel])  # [batch * patch_hh * h_patch_size, patch_ww, w_patch_size, channel]
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_hh * h_patch_size, w_patch_size, patch_ww, channel]
-    nn = tf.reshape(nn, [-1, patch_hh, patch_size * patch_size, patch_ww * out_channel])  # [batch, patch_hh, h_patch_size * w_patch_size, patch_ww * channel]
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch, h_patch_size * w_patch_size, patch_hh, patch_ww * channel]
-    extract_shape = [-1, patch_hh, patch_ww, out_channel] if patches_to_batch else [-1, patch_size * patch_size, patch_hh * patch_ww, out_channel]
-    nn = tf.reshape(nn, extract_shape)
-
+    if image_data_format() == "channels_last":
+        nn = functional.reshape(nn, [-1, patch_ww, patch_size, out_channel])  # [B * patch_hh * h_patch_size, patch_ww, w_patch_size, C]
+        nn = functional.transpose(nn, [0, 2, 1, 3])  # [B * patch_hh * h_patch_size, w_patch_size, patch_ww, C]
+        nn = functional.reshape(nn, [-1, patch_hh, patch_size * patch_size, patch_ww * out_channel])  # [B, patch_hh, h_patch_size * w_patch_size, patch_ww * C]
+        nn = functional.transpose(nn, [0, 2, 1, 3])  # [B, h_patch_size * w_patch_size, patch_hh, patch_ww * C]
+        extract_shape = [-1, patch_hh, patch_ww, out_channel] if patches_to_batch else [-1, patch_size * patch_size, patch_hh * patch_ww, out_channel]
+        nn = functional.reshape(nn, extract_shape)  # channels_last
+    else:  # [B, C, patch_hh * h_patch_size, patch_ww * w_patch_size]
+        nn = functional.reshape(nn, [-1, patch_size, patch_ww, patch_size])  # [B * C * patch_hh, h_patch_size, patch_ww, w_patch_size]
+        nn = functional.transpose(nn, [0, 2, 1, 3])  # [B * C * patch_hh, patch_ww, h_patch_size, w_patch_size]
+        nn = functional.reshape(nn, [-1, out_channel, patch_hh * patch_ww, patch_size * patch_size])  # [B, C, patch_hh * patch_ww, h_patch_size * w_patch_size]
+        if patches_to_batch:  # V1 -> channels_last, V2 keep channels_first, `patch_hh * patch_ww` -> attn_axis=2
+            nn = functional.transpose(nn, [0, 3, 2, 1])  # [B, h_patch_size * w_patch_size, patch_hh * patch_ww, C]
+            nn = functional.reshape(nn, [-1, patch_hh, patch_ww, out_channel])  # channels_last
     return nn
 
 
 def transformer_post_process(inputs, pre_attn, out_channel, patch_size=2, patch_height=-1, activation="swish", name=""):
-    if patch_height == -1:  # V1, [batch * 4, height // 2, width // 2, channel]
-        patch_hh, patch_ww, channel = inputs.shape[1], inputs.shape[2], inputs.shape[-1]
-    else:  # V2, [batch, 4, height // 2 * width // 2, channel]
-        patch_hh, patch_ww, channel = patch_height, inputs.shape[2] // patch_height, inputs.shape[-1]
+    height_axis, width_axis, channel_axis = (1, 2, 3) if image_data_format() == "channels_last" else (2, 3, 1)
+    patches_to_batch = patch_height == -1
+    if patches_to_batch:  # V1, [batch * 4, height // 2, width // 2, channel], channels_last
+        patch_hh, patch_ww, channel = inputs.shape[1], inputs.shape[2], inputs.shape[3]
+    else:  # V2, [batch, 4, height // 2 * width // 2, channel], channels_last or channels_first
+        patch_hh, patch_ww, channel = patch_height, inputs.shape[2] // patch_height, inputs.shape[channel_axis]
     # print(f"{patch_hh = }, {patch_ww = }, {channel = }, {inputs.shape = }")
 
-    # [batch * 4, height // 2, width // 2, channel] -> [batch, height // 2, 2, width // 2, 2, channel] -> [batch, height, width, channel]
-    nn = tf.reshape(inputs, [-1, patch_size * patch_size, patch_hh, patch_ww * channel])  # [batch, h_patch_size * w_patch_size, patch_hh, patch_ww * channel]
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch, patch_hh, h_patch_size * w_patch_size, patch_ww * channel]
-    nn = tf.reshape(nn, [-1, patch_size, patch_ww, channel])  # [batch * patch_hh * h_patch_size, w_patch_size, patch_ww, channel]
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_hh * h_patch_size, patch_ww, w_patch_size, channel]
-    nn = tf.reshape(nn, [-1, patch_hh * patch_size, patch_ww * patch_size, channel])
+    # [B * 4, height // 2, width // 2, C] -> [B, height // 2, 2, width // 2, 2, C] -> [B, height, width, C]
+    if image_data_format() == "channels_last":
+        nn = functional.reshape(inputs, [-1, patch_size * patch_size, patch_hh, patch_ww * channel])  # [B, h_patch_size * w_patch_size, patch_hh, patch_ww * C]
+        nn = functional.transpose(nn, [0, 2, 1, 3])  # [B, patch_hh, h_patch_size * w_patch_size, patch_ww * C]
+        nn = functional.reshape(nn, [-1, patch_size, patch_ww, channel])  # [B * patch_hh * h_patch_size, w_patch_size, patch_ww, C]
+        nn = functional.transpose(nn, [0, 2, 1, 3])  # [B * patch_hh * h_patch_size, patch_ww, w_patch_size, C]
+        nn = functional.reshape(nn, [-1, patch_hh * patch_size, patch_ww * patch_size, channel])
+    else:
+        nn = inputs
+        if patches_to_batch:
+            nn = functional.reshape(nn, [-1, patch_size * patch_size, patch_hh * patch_ww, channel])  # [B, h_patch_size * w_patch_size, patch_hh * patch_ww, C]
+            nn = functional.transpose(nn, [0, 3, 2, 1])  # [B, C, patch_hh * patch_ww, h_patch_size * w_patch_size]
+        nn = functional.reshape(nn, [-1, patch_ww, patch_size, patch_size])  # [B * C * patch_hh, patch_ww, h_patch_size, w_patch_size]
+        nn = functional.transpose(nn, [0, 2, 1, 3])  # [B * C * patch_hh, h_patch_size, patch_ww, w_patch_size]
+        nn = functional.reshape(nn, [-1, channel, patch_hh * patch_size, patch_ww * patch_size])
+
     # print(f"transformer_post_process before resize: {nn.shape = }")
-    if pre_attn is not None and (nn.shape[1] != pre_attn.shape[1] or nn.shape[2] != pre_attn.shape[2]):
-        nn = tf.image.resize(nn, [pre_attn.shape[1], pre_attn.shape[2]], method="bilinear")
+    if pre_attn is not None and (nn.shape[height_axis] != pre_attn.shape[height_axis] or nn.shape[width_axis] != pre_attn.shape[width_axis]):
+        nn = functional.resize(nn, [pre_attn.shape[height_axis], pre_attn.shape[width_axis]], method="bilinear")
     # print(f"transformer_post_process after resize: {nn.shape = }")
 
     nn = conv2d_no_bias(nn, out_channel, kernel_size=1, strides=1, name=name + "post_1_")
     nn = batchnorm_with_activation(nn, activation=activation, name=name + "post_1_")
     if pre_attn is not None:  # V1
-        nn = tf.concat([pre_attn, nn], axis=-1)
-        nn = conv2d_no_bias(nn, out_channel, kernel_size=3, strides=1, padding="SAME", name=name + "post_2_")
+        nn = functional.concat([pre_attn, nn], axis=channel_axis)
+        nn = conv2d_no_bias(nn, out_channel, kernel_size=3, strides=1, padding="same", name=name + "post_2_")
         nn = batchnorm_with_activation(nn, activation=activation, name=name + "post_2_")
-    return keras.layers.Activation("linear", name=name + "output")(nn)  # Identity, Just need a name here
+    return layers.Activation("linear", name=name + "output")(nn)  # Identity, Just need a name here
 
 
 def MobileViT(
     num_blocks=[1, 3, 3, 5, 4],
     out_channels=[32, 64, 96, 128, 160],
     attn_channels=[0, 0, 144, 192, 240],  # Can be a list matching out_channels, or a float number for expansion ratio of out_channels
     block_types=["conv", "conv", "transform", "transform", "transform"],
     strides=[1, 2, 2, 2, 2],
     expand_ratio=4,
     stem_width=16,
     patch_size=2,
-    patches_to_batch=True,  # True for V1, False for V2
     resize_first=False,  # False for V1, True for V2
     use_depthwise=False,  # False for V1, True for V2
     use_fusion=True,  # True for V1, False for V2
     num_norm_groups=-1,  # -1 or 0 for V1 using layer_norm, or 1 for V2 using group_norm
     use_linear_attention=False,  # False for V1, True for V2
-    use_conv_mlp=False,  # False for V1, True for V2
     output_num_features=640,
     layer_scale=-1,
     input_shape=(256, 256, 3),
     num_classes=1000,
     activation="swish",
     drop_connect_rate=0,
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="mobilevit",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
     nn = conv2d_no_bias(inputs, stem_width, kernel_size=3, strides=2, padding="same", name="stem_")
     nn = batchnorm_with_activation(nn, activation=activation, name="stem_")
+    height_axis, width_axis, channel_axis = (1, 2, -1) if image_data_format() == "channels_last" else (2, 3, 1)
 
     # Save line width
     mhsa_mlp_block_common_kwargs = {
         "num_heads": 4,
         "qkv_bias": True,
         "mlp_ratio": 2,
         "num_norm_groups": num_norm_groups,
-        "use_linear_attention": use_linear_attention,
-        "use_conv_mlp": use_conv_mlp,
         "activation": activation,
     }
 
     """ stages """
     total_blocks = sum(num_blocks)
     global_block_id = 0
     post_activation = activation if use_fusion else None
     for id, (num_block, out_channel, block_type, stride) in enumerate(zip(num_blocks, out_channels, block_types, strides)):
         stack_name = "stack{}_".format(id + 1)
         is_conv_block = True if block_type[0].lower() == "c" else False
         attn_channel = attn_channels[id] if isinstance(attn_channels, (list, tuple)) else make_divisible(attn_channels * out_channel, divisor=8)
         for block_id in range(num_block):
             name = stack_name + "block{}_".format(block_id + 1)
             stride = stride if block_id == 0 else 1
-            use_shortcut = False if stride != 1 or nn.shape[-1] != out_channel else True
+            use_shortcut = False if stride != 1 or nn.shape[channel_axis] != out_channel else True
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             global_block_id += 1
             if is_conv_block or block_id == 0:  # First transformer block is also a conv block .
                 nn = bottle_in_linear_out_block(nn, out_channel, stride, expand_ratio, use_shortcut, block_drop_rate, activation=activation, name=name)
             else:
                 if block_id == 1:  # pre
                     pre_attn = nn if use_fusion else None
-                    patch_height = -1 if patches_to_batch else int(tf.math.ceil(nn.shape[1] / patch_size))
+                    patches_to_batch = not use_linear_attention
+                    patch_height = -1 if patches_to_batch else int(math.ceil(nn.shape[height_axis] / patch_size))
                     nn = transformer_pre_process(nn, attn_channel, patch_size, resize_first, use_depthwise, patches_to_batch, activation=activation, name=name)
-                nn = mhsa_mlp_block(nn, attn_channel, layer_scale=layer_scale, **mhsa_mlp_block_common_kwargs, name=name)
+
+                if use_linear_attention:  # channels_last for Tensorflow, channels_first for PyTorch
+                    nn = linear_mhsa_mlp_block(nn, attn_channel, layer_scale=layer_scale, **mhsa_mlp_block_common_kwargs, name=name)
+                else:  # channels_last for both Tensorflow or PyTorch
+                    if block_id == 1:
+                        block_height, block_width = nn.shape[1:-1]
+                        channel_axis = -1
+                        nn = functional.reshape(nn, [-1, block_height * block_width, nn.shape[-1]])  # Using 3D for attention inputs
+                    nn = mhsa_mlp_block(nn, attn_channel, layer_scale=layer_scale, **mhsa_mlp_block_common_kwargs, name=name)
+                    if block_id == num_block - 1:
+                        channel_axis = -1 if image_data_format() == "channels_last" else 1
+                        nn = functional.reshape(nn, [-1, block_height, block_width, nn.shape[-1]])  # Revert 3D to 4D
+
                 if block_id == num_block - 1:  # post
-                    nn = group_norm(nn, groups=num_norm_groups, name=name + "post_") if num_norm_groups > 0 else layer_norm(nn, name=name + "post_")
+                    norm_axis = "auto" if use_linear_attention else -1
+                    if use_linear_attention:
+                        nn = group_norm(nn, groups=num_norm_groups, axis=norm_axis, name=name + "post_")
+                    else:
+                        nn = layer_norm(nn, axis=norm_axis, name=name + "post_")
                     nn = transformer_post_process(nn, pre_attn, out_channel, patch_size, patch_height, activation=post_activation, name=name)
 
     nn = output_block(nn, output_num_features, activation, num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="raw01")
     reload_model_weights(model, PRETRAINED_DICT, "mobilevit", pretrained)
     return model
 
 
+@register_model
 def MobileViT_XXS(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [1, 3, 3, 5, 4]
     out_channels = [16, 24, 48, 64, 80]
     attn_channels = [0, 0, 64, 80, 96]
     output_num_features = 320
     expand_ratio = 2
     return MobileViT(**locals(), model_name="mobilevit_xxs", **kwargs)
 
 
+@register_model
 def MobileViT_XS(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [1, 3, 3, 5, 4]
     out_channels = [32, 48, 64, 80, 96]
     attn_channels = 1.5
     output_num_features = 384
     return MobileViT(**locals(), model_name="mobilevit_xs", **kwargs)
 
 
+@register_model
 def MobileViT_S(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [1, 3, 3, 5, 4]
     out_channels = [32, 64, 96, 128, 160]
     attn_channels = 1.5
     return MobileViT(**locals(), model_name="mobilevit_s", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/mobilevit/mobilevit_v2.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/mobilevit/mobilevit_v2.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,60 +1,66 @@
 from keras_cv_attention_models.mobilevit.mobilevit import MobileViT
+from keras_cv_attention_models.models import register_model
 
 
 def MobileViT_V2(
     num_blocks=[1, 2, 3, 5, 4],
     out_channels=[64, 128, 256, 384, 512],
     attn_channels=0.5,  # Can be a list matching out_channels, or a float number for expansion ratio of out_channels
     expand_ratio=2,
     stem_width=32,
-    patches_to_batch=False,  # True for V1, False for V2
     resize_first=True,  # False for V1, True for V2
     use_depthwise=True,  # False for V1, True for V2
     use_fusion=False,  # True for V1, False for V2
     num_norm_groups=1,  # -1 or 0 for V1 using layer_norm, or 1 for V2 using group_norm
     use_linear_attention=True,  # False for V1, True for V2
-    use_conv_mlp=True,  # False for V1, True for V2
     output_num_features=0,
     model_name="mobilevit_v2",
     **kwargs,
 ):
     kwargs.pop("kwargs", None)
     return MobileViT(**locals(), **kwargs)
 
 
 def get_mobilevit_v2_width(multiplier=1.0):
     return int(32 * multiplier), [int(ii * multiplier) for ii in [64, 128, 256, 384, 512]]  # stem_width, out_channels
 
 
+@register_model
 def MobileViT_V2_050(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     stem_width, out_channels = get_mobilevit_v2_width(0.5)
     return MobileViT_V2(**locals(), model_name="mobilevit_v2_050", **kwargs)
 
 
+@register_model
 def MobileViT_V2_075(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     stem_width, out_channels = get_mobilevit_v2_width(0.75)
     return MobileViT_V2(**locals(), model_name="mobilevit_v2_075", **kwargs)
 
 
+@register_model
 def MobileViT_V2_100(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return MobileViT_V2(**locals(), model_name="mobilevit_v2_100", **kwargs)
 
 
+@register_model
 def MobileViT_V2_125(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     stem_width, out_channels = get_mobilevit_v2_width(1.25)
     return MobileViT_V2(**locals(), model_name="mobilevit_v2_125", **kwargs)
 
 
+@register_model
 def MobileViT_V2_150(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     stem_width, out_channels = get_mobilevit_v2_width(1.5)
     return MobileViT_V2(**locals(), model_name="mobilevit_v2_150", **kwargs)
 
 
+@register_model
 def MobileViT_V2_175(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     stem_width, out_channels = get_mobilevit_v2_width(1.75)
     return MobileViT_V2(**locals(), model_name="mobilevit_v2_175", **kwargs)
 
 
+@register_model
 def MobileViT_V2_200(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     stem_width, out_channels = get_mobilevit_v2_width(2.0)
     return MobileViT_V2(**locals(), model_name="mobilevit_v2_200", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/model_surgery/model_surgery.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/pytorch_backend/models.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,591 +1,557 @@
-import tensorflow as tf
-from tensorflow import keras
-import tensorflow.keras.backend as K
-
-""" Convert and replacing """
-
-
-class SAMModel(keras.models.Model):
-    """
-    Arxiv article: [Sharpness-Aware Minimization for Efficiently Improving Generalization](https://arxiv.org/pdf/2010.01412.pdf)
-    Implementation by: [Keras SAM (Sharpness-Aware Minimization)](https://qiita.com/T-STAR/items/8c3afe3a116a8fc08429)
-
-    Usage is same with `keras.modeols.Model`: `model = SAMModel(inputs, outputs, rho=sam_rho, name=name)`
-    """
-
-    def __init__(self, *args, rho=0.05, **kwargs):
-        super().__init__(*args, **kwargs)
-        self.rho = tf.constant(rho, dtype=tf.float32)
-
-    def train_step(self, data):
-        if len(data) == 3:
-            x, y, sample_weight = data
+import torch
+import numpy as np
+from tqdm.auto import tqdm
+from torch import nn
+from contextlib import nullcontext
+from collections import namedtuple
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.pytorch_backend import layers, callbacks, metrics
+
+
+class _Trainer_(object):
+    def init_metrics(self, cur_metrics=None):
+        if cur_metrics is None:
+            metrics_names, cur_metrics = [], []
+        elif isinstance(cur_metrics, str):
+            metrics_names, cur_metrics = [cur_metrics], [cur_metrics]
+        elif isinstance(cur_metrics, (list, tuple)):
+            metrics_names, cur_metrics = [ii if isinstance(ii, str) else ii.name for ii in cur_metrics], list(cur_metrics)
+        elif isinstance(cur_metrics, dict):
+            assert self.output_names is not None, "self.output_names cannot be None when using dict metrics, provide output_names or use list metrics"
+            assert len(self.output_names) == len(cur_metrics), "self.output_names={} not matching with metrics={}".format(self.output_names, metrics)
+            metrics_names, cur_metrics = self.output_names, [cur_metrics[ii] for ii in self.output_names]
+        else:
+            metrics_names, cur_metrics = [cur_metrics.name], [cur_metrics]
+        cur_metrics = [metrics.BUILDIN_METRICS[ii]() if isinstance(ii, str) else ii for ii in cur_metrics]
+        return metrics_names, cur_metrics
+
+    @property
+    def compute_dtype(self):
+        try:
+            return next(self.parameters()).dtype
+        except StopIteration:
+            return torch.get_default_dtype()
+
+    def train_compile(self, optimizer="RMSprop", loss=None, metrics=None, loss_weights=None, grad_accumulate=1, grad_max_norm=-1, **kwargs):
+        # works like kers `model.compile`, but `compile` is took by `nn.Module`, rename as `train_compile`
+        optimizer_kwargs = {"lr": 0.01} if isinstance(optimizer, str) and optimizer == "SGD" else {}  # lr is a required parameter for SGD
+        self.optimizer = getattr(torch.optim, optimizer)(self.parameters(), **optimizer_kwargs) if isinstance(optimizer, str) else optimizer
+        self.compiled_loss = self.loss = (lambda y_true, y_pred: torch.functional.F.cross_entropy(y_pred, y_true)) if loss is None else loss
+        self.loss_weights, self.grad_accumulate, self.grad_max_norm = loss_weights or 1.0, grad_accumulate, grad_max_norm
+        self.metrics_names, self.metrics = self.init_metrics(metrics)
+        self.eval_metrics = [metric.name for metric in self.metrics if metric.eval_only]  # Mark metric like acc5 for eval only
+
+        device = next(self.parameters()).device
+        device_type = device.type
+        if device_type == "cpu":
+            scaler = torch.cuda.amp.GradScaler(enabled=False)
+            global_context = nullcontext()
         else:
-            sample_weight = None
-            x, y = data
+            scaler = torch.cuda.amp.GradScaler(enabled=True)
+            global_context = torch.autocast(device_type=device_type, dtype=torch.float16)
+        self.device, self.device_type, self.scaler, self.global_context = device, device_type, scaler, global_context
+
+    def _convert_data_(self, data):
+        if isinstance(data, np.ndarray):
+            data = torch.from_numpy(data)
+        if self.device_type == "cuda":
+            # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)
+            data = data.pin_memory().to(self.device, non_blocking=True)
+        return data
+
+    def train_step(self, xx, yy):
+        # Split out for being able to overwrite
+        with self.global_context:
+            out = self(xx)
+            loss = self.loss(yy, out) * self.loss_weights
+        return out, loss
+
+    def fit(self, x=None, y=None, batch_size=32, epochs=1, callbacks=None, validation_data=None, initial_epoch=0, validation_batch_size=None, **kwargs):
+        callbacks = callbacks or []
+        [ii.set_model(self) for ii in callbacks if ii.model is None]
+        self.history = namedtuple('history', ['history', 'epoch'])({"loss": []}, [])  # Mimic of `keras.callbacks.History`
+        validation_batch_size = validation_batch_size or batch_size
+        self.batch_size, self.callbacks, self.validation_batch_size = batch_size, callbacks, validation_batch_size
+        self.accumulate_passed_batches = 0
+        self.optimizer.zero_grad()  # [TODO] should be inside or outside epochs loop?
+
+        bar_format = "{n_fmt}/{total_fmt} [{bar:30}] - ETA: {elapsed}<{remaining} {rate_fmt}{postfix}{desc}"
+        for epoch in range(initial_epoch, epochs):
+            print("Epoch {}/{}".format(epoch + 1, epochs))
+            epoch_logs = {}  # Can be used as global value between different callbacks
+            [ii.on_epoch_begin(epoch, epoch_logs) for ii in callbacks]
+            [ii.reset_state() for ii in self.metrics]
+
+            self.train()
+            avg_loss = 0.0
+            train_dataset, total = self._dataset_gen_(x, y, batch_size=batch_size)
+            process_bar = tqdm(enumerate(train_dataset), total=total, bar_format=bar_format, ascii=".>>=")
+            for batch, (xx, yy) in process_bar:
+                batch_logs = {}  # Can be used as global value between different callbacks
+                [ii.on_train_batch_begin(batch, batch_logs) for ii in callbacks]
+
+                xx = [self._convert_data_(ii) for ii in xx] if isinstance(xx, (list, tuple)) else self._convert_data_(xx)
+                yy = [self._convert_data_(ii) for ii in yy] if isinstance(yy, (list, tuple)) else self._convert_data_(yy)
+                out, loss = self.train_step(xx, yy)
+                self.scaler.scale(loss).backward()
+
+                self.accumulate_passed_batches += 1
+                if self.accumulate_passed_batches >= self.grad_accumulate:
+                    self.scaler.unscale_(self.optimizer)
+                    if self.grad_max_norm > 0:
+                        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=self.grad_max_norm)  # clip gradients
+                    self.scaler.step(self.optimizer)  # self.optimizer.step()
+                    self.scaler.update()
+                    self.optimizer.zero_grad()
+                    self.accumulate_passed_batches = 0
+                # print(">>>> Epoch {}, batch: {}, loss: {:.4f}".format(epoch, batch, loss.item()))
+                avg_loss += loss
+                process_bar.desc = " - loss: {:.4f}".format(avg_loss / (batch + 1))  # process_bar.set_description automatically add a : on the tail
+
+                # out = out.detach().cpu()
+                if isinstance(yy, (list, tuple)):
+                    [metric.update_state(cur_yy, cur_out) for cur_out, cur_yy, metric in zip(out, yy, self.metrics) if metric.name not in self.eval_metrics]
+                else:
+                    [metric.update_state(yy, out) for metric in self.metrics if metric.name not in self.eval_metrics]
+                metrics_results = {ii: metric.result().item() for ii, metric in zip(self.metrics_names, self.metrics) if metric.name not in self.eval_metrics}
+                process_bar.desc += "".join([" - {}: {:.4f}".format(name, metric) for name, metric in metrics_results.items()])
+
+                batch_logs["loss"] = loss.item()
+                batch_logs.update(metrics_results)
+                [ii.on_train_batch_end(batch, logs=batch_logs) for ii in callbacks]
+
+                """ Eval process, put inside for loop for a better display of process bar """
+                if batch == total - 1 and validation_data is not None:
+                    val_loss, val_metrics_results = self.evaluate(validation_data, batch_size=validation_batch_size, callbacks=callbacks)
+
+                    self.history.history.setdefault("val_loss", []).append(val_loss.item())
+                    for name, val_metric_result in val_metrics_results.items():
+                        self.history.history.setdefault(name, []).append(val_metric_result.item() if hasattr(val_metric_result, "item") else val_metric_result)
+
+                    process_bar.desc += " - val_loss: {:.4f}".format(val_loss)
+                    process_bar.desc += "".join([" - {}: {:.4f}".format(name, metric) for name, metric in val_metrics_results.items()])
+                    # process_bar.display()
+                process_bar.refresh()
+
+            loss = avg_loss / (batch + 1)
+            self.history.epoch.append(epoch)
+            self.history.history["loss"].append(loss.item() if hasattr(loss, "item") else loss)
+            for name, metric_result in metrics_results.items():
+                self.history.history.setdefault(name, []).append(metric_result)
+            epoch_logs = {kk: vv[-1] for kk, vv in self.history.history.items()}
+            with self.global_context, torch.no_grad():
+                [ii.on_epoch_end(epoch, epoch_logs) for ii in callbacks]
+            print()
+
+    def evaluate(self, x=None, y=None, batch_size=None, verbose="auto", callbacks=None, **kwargs):
+        callbacks = callbacks or []
+        epoch_logs = {}  # Can be used as global value between different callbacks
+        [ii.set_model(self) for ii in callbacks if ii.model is None]
+        [ii.on_test_begin(epoch_logs) for ii in callbacks]
+        [ii.reset_state() for ii in self.metrics]
+
+        self.eval()
+        avg_loss = 0.0
+        val_dataset, total = self._dataset_gen_(x, y, batch_size=batch_size)
+        for batch, (xx, yy) in enumerate(val_dataset):
+            batch_logs = {}  # Can be used as global value between different callbacks
+            [ii.on_test_batch_begin(batch, batch_logs) for ii in callbacks]
+
+            xx = [self._convert_data_(ii) for ii in xx] if isinstance(xx, (list, tuple)) else self._convert_data_(xx)
+            yy = [self._convert_data_(ii) for ii in yy] if isinstance(yy, (list, tuple)) else self._convert_data_(yy)
+            with torch.no_grad():
+                out, loss = self.train_step(xx, yy)
+            avg_loss += loss
+
+            # out = out.detach().cpu()
+            if isinstance(yy, (list, tuple)):
+                [metric.update_state(cur_yy, cur_out) for cur_out, cur_yy, metric in zip(out, yy, self.metrics)]
+            else:
+                [ii.update_state(yy, out) for ii in self.metrics]
+            batch_logs["val_loss"] = loss.item()
+            batch_logs.update({name: metric.result().item() for name, metric in zip(self.metrics_names, self.metrics)})
+            [ii.on_test_batch_end(batch, batch_logs) for ii in callbacks]
+
+        val_loss = avg_loss / (batch + 1)
+        metrics_results = {}
+        for name, metric in zip(self.metrics_names, self.metrics):
+            metrics_results["val_" + name] = metric.result().item()
+        epoch_logs["val_loss"] = val_loss.item()
+        epoch_logs.update(metrics_results)
+        [ii.on_test_end(epoch_logs) for ii in callbacks]
+        return val_loss, metrics_results
+
+    def _dataset_gen_(self, x=None, y=None, batch_size=32):
+        if isinstance(x, (list, tuple)) and len(x) == 2 and y is None:
+            xx, yy = x[0], x[1]
+        else:
+            xx, yy = x, y
 
-        # 1st step
-        with tf.GradientTape() as tape:
-            y_pred = self(x, training=True)
-            loss = self.compiled_loss(y, y_pred, sample_weight=sample_weight, regularization_losses=self.losses)
-
-        trainable_vars = self.trainable_variables
-        gradients = tape.gradient(loss, trainable_vars)
-
-        norm = tf.linalg.global_norm(gradients)
-        scale = self.rho / (norm + 1e-12)
-        e_w_list = []
-        for v, grad in zip(trainable_vars, gradients):
-            e_w = grad * scale
-            v.assign_add(e_w)
-            e_w_list.append(e_w)
-
-        # 2nd step
-        with tf.GradientTape() as tape:
-            y_pred_adv = self(x, training=True)
-            loss_adv = self.compiled_loss(y, y_pred_adv, sample_weight=sample_weight, regularization_losses=self.losses)
-        gradients_adv = tape.gradient(loss_adv, trainable_vars)
-        for v, e_w in zip(trainable_vars, e_w_list):
-            v.assign_sub(e_w)
-
-        # optimize
-        self.optimizer.apply_gradients(zip(gradients_adv, trainable_vars))
-
-        self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)
-        return_metrics = {}
-        for metric in self.metrics:
-            result = metric.result()
-            if isinstance(result, dict):
-                return_metrics.update(result)
+        if hasattr(xx, "element_spec"):  # TF datsets
+            data_shape = xx.element_spec[0].shape
+            if self.input_shape is not None and data_shape[-1] == self.input_shape[1]:
+                perm = [0, len(data_shape) - 1] + list(range(1, len(data_shape) - 1))  # [0, 3, 1, 2]
+                dataset = ((ii.transpose(perm), jj) for ii, jj in xx.as_numpy_iterator())
             else:
-                return_metrics[metric.name] = result
-        return return_metrics
+                dataset = xx.as_numpy_iterator()
+            total = len(xx)
+        elif isinstance(xx, np.ndarray) or isinstance(xx, torch.Tensor):
+            assert yy is not None
+            num_batches = xx.shape[0] if batch_size is None else int(np.ceil(xx.shape[0] / batch_size))
+
+            def _convert_tensor(data, id):
+                cur = data[id * batch_size : (id + 1) * batch_size] if batch_size is not None else data[id]
+                cur = torch.from_numpy(cur) if isinstance(cur, np.ndarray) else cur
+                cur = cur.float() if cur.dtype == torch.float64 else cur
+                cur = cur.long() if cur.dtype == torch.int32 else cur
+                return cur
+
+            dataset = ((_convert_tensor(xx, id), _convert_tensor(yy, id)) for id in range(num_batches))
+            total = num_batches
+        else:  # generator or torch.utils.data.DataLoader
+            dataset = xx
+            total = len(xx)
+        return dataset, total
+
+
+class _Exporter_(object):
+    def _create_fake_input_data_(self, input_shape=None, batch_size=1):
+        input_shapes = [ii.shape for ii in self.inputs]
+        input_dtypes = [ii.dtype for ii in self.inputs]
+
+        input_shape = self.input_shape if input_shape is None else input_shape
+        input_shapes = input_shape if isinstance(input_shape[0], (list, tuple)) else [input_shape]  # Convert to list of input_shpae
+        assert len(input_shapes) == len(self.inputs), "input_shape={} length not matching self.inputs={}".format(input_shape, self.inputs)
+
+        input_datas = []
+        for input_shape, model_input in zip(input_shapes, self.inputs):
+            input_shape = list(input_shape).copy()
+            if len(input_shape) == len(model_input.shape) - 1:
+                input_shape = [batch_size] + input_shape
+            assert len(input_shape) == len(model_input.shape), "input_shape={} rank not match with input={}".format(input_shape, model_input.shape)
+
+            if input_shape[0] is None or input_shape[0] == -1:
+                input_shape[0] = batch_size
+            if None in input_shape or -1 in input_shape:
+                print("[WARNING] dynamic shape value in input_shape={}, set to 32".format(input_shape))
+                input_shape = [32 if ii is None or ii == -1 else ii for ii in input_shape]
+
+            dtype = model_input.dtype or torch.get_default_dtype()
+            dtype = getattr(torch, dtype) if isinstance(dtype, str) else dtype
+            input_datas.append(torch.ones(input_shape, dtype=dtype))
+        print(">>>> input_shape: {}, dtype: {}".format([ii.shape for ii in input_datas], [ii.dtype for ii in input_datas]))
+        return input_datas
+
+    def summary(self, input_shape=None, **kwargs):
+        from torchinfo import summary
+
+        input_datas = self._create_fake_input_data_(input_shape)
+        summary(self, input_data=input_datas if len(self.inputs) == 1 else [input_datas], **kwargs)
+
+    def export_onnx(self, filepath=None, input_shape=None, batch_size=1, simplify=False, input_names=None, output_names=None, **kwargs):
+        input_datas = self._create_fake_input_data_(input_shape, batch_size=batch_size)
+
+        dynamic_axes = kwargs.pop("dynamic_axes", None)
+        input_names = input_names or self.input_names
+        output_names = output_names or self.output_names
+        if dynamic_axes is None and (batch_size is None or batch_size == -1):
+            print("Set dynamic batch size")
+            dynamic_axes = {ii: {0: "-1"} for ii in input_names}
+            dynamic_axes.update({ii: {0: "-1"} for ii in output_names})
+
+        filepath = (self.name + ".onnx") if filepath is None else (filepath if filepath.endswith(".onnx") else (filepath + ".onnx"))
+        torch.onnx.export(self, input_datas, filepath, input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes, **kwargs)
+        print("Exported onnx:", filepath)
+
+        if simplify:
+            import onnx, onnxsim
+
+            print("Running onnxsim.simplify...")
+            tt = onnx.load(filepath)
+            tt, check = onnxsim.simplify(tt)
+            if check:
+                with open(filepath, "wb") as ff:
+                    ff.write(tt.SerializeToString())
+                print("Exported simplified onnx:", filepath)
+            else:
+                print("[Error] failed to simplify onnx:", filepath)
 
+    def export_pth(self, filepath=None, input_shape=None, batch_size=1, **kwargs):
+        input_datas = self._create_fake_input_data_(input_shape, batch_size=batch_size)
 
-@keras.utils.register_keras_serializable(package="model_surgery")
-class DropConnect(keras.layers.Layer):
-    def __init__(self, rate=0, **kwargs):
-        super(DropConnect, self).__init__(**kwargs)
-        self.rate = rate
-        self.supports_masking = False
+        traced_cell = torch.jit.trace(self, example_inputs=input_datas if len(self.inputs) == 1 else [input_datas])
+        filepath = (self.name + ".pth") if filepath is None else (filepath if filepath.endswith(".pth") else (filepath + ".pth"))
+        torch.jit.save(traced_cell, filepath, **kwargs)
+        print("Exported pth:", filepath)
 
-    def build(self, input_shape):
-        if self.rate > 0:
-            noise_shape = [None] + [1] * (len(input_shape) - 1)  # [None, 1, 1, 1]
-            self.drop = keras.layers.Dropout(self.rate, noise_shape=noise_shape, name=self.name + "drop")
+    def load_weights(self, filepath, by_name=True, skip_mismatch=False, **kwargs):
+        if filepath.endswith("h5"):
+            from keras_cv_attention_models.download_and_load import load_weights_from_hdf5_file
+
+            load_weights_from_hdf5_file(filepath, self, skip_mismatch=skip_mismatch, debug=self.debug)
         else:
-            self.drop = lambda xx: xx
+            weights = torch.load(filepath, map_location=torch.device("cpu"), **kwargs)
+            weights = weights.state_dict() if hasattr(weights, "state_dict") else weights
+            self.load_state_dict(weights.get("state_dict", weights.get("model", weights)))
+            if hasattr(self, "optimizer") and "optimizer" in weights:
+                print(">>>> Reload optimizer state_dict")
+                self.optimizer.load_state_dict(weights["optimizer"])
+
+    def save_weights(self, filepath=None, **kwargs):
+        filepath = filepath if filepath else self.name + ".h5"
+        if filepath.endswith("h5"):
+            from keras_cv_attention_models.download_and_load import save_weights_to_hdf5_file
 
-    def call(self, inputs, **kwargs):
-        shortcut, deep = inputs
-        deep = self.drop(deep)
-        return keras.layers.Add()([shortcut, deep])
-
-    def get_config(self):
-        config = super(DropConnect, self).get_config()
-        config.update({"rate": self.rate})
-        return config
-
-
-def add_l2_regularizer_2_model(model, weight_decay, custom_objects={}, apply_to_batch_normal=False, apply_to_bias=False):
-    # https://github.com/keras-team/keras/issues/2717#issuecomment-456254176
-    if 0:
-        regularizers_type = {}
-        for layer in model.layers:
-            rrs = [kk for kk in layer.__dict__.keys() if "regularizer" in kk and not kk.startswith("_")]
-            if len(rrs) != 0:
-                # print(layer.name, layer.__class__.__name__, rrs)
-                if layer.__class__.__name__ not in regularizers_type:
-                    regularizers_type[layer.__class__.__name__] = rrs
-        print(regularizers_type)
-
-    for layer in model.layers:
-        attrs = []
-        if isinstance(layer, keras.layers.Dense) or isinstance(layer, keras.layers.Conv2D):
-            # print(">>>> Dense or Conv2D", layer.name, "use_bias:", layer.use_bias)
-            attrs = ["kernel_regularizer"]
-            if apply_to_bias and layer.use_bias:
-                attrs.append("bias_regularizer")
-        elif isinstance(layer, keras.layers.DepthwiseConv2D):
-            # print(">>>> DepthwiseConv2D", layer.name, "use_bias:", layer.use_bias)
-            attrs = ["depthwise_regularizer"]
-            if apply_to_bias and layer.use_bias:
-                attrs.append("bias_regularizer")
-        elif isinstance(layer, keras.layers.SeparableConv2D):
-            # print(">>>> SeparableConv2D", layer.name, "use_bias:", layer.use_bias)
-            attrs = ["pointwise_regularizer", "depthwise_regularizer"]
-            if apply_to_bias and layer.use_bias:
-                attrs.append("bias_regularizer")
-        elif apply_to_batch_normal and isinstance(layer, keras.layers.BatchNormalization):
-            # print(">>>> BatchNormalization", layer.name, "scale:", layer.scale, ", center:", layer.center)
-            if layer.center:
-                attrs.append("beta_regularizer")
-            if layer.scale:
-                attrs.append("gamma_regularizer")
-        elif apply_to_batch_normal and isinstance(layer, keras.layers.PReLU):
-            # print(">>>> PReLU", layer.name)
-            attrs = ["alpha_regularizer"]
-
-        for attr in attrs:
-            if hasattr(layer, attr) and layer.trainable:
-                setattr(layer, attr, keras.regularizers.L2(weight_decay / 2))
-
-    # So far, the regularizers only exist in the model config. We need to
-    # reload the model so that Keras adds them to each layer's losses.
-    # temp_weight_file = "tmp_weights.h5"
-    # model.save_weights(temp_weight_file)
-    # out_model = keras.models.model_from_json(model.to_json(), custom_objects=custom_objects)
-    # out_model.load_weights(temp_weight_file, by_name=True)
-    # os.remove(temp_weight_file)
-    # return out_model
-    return keras.models.clone_model(model)
-
-
-def replace_ReLU(model, target_activation="PReLU", **kwargs):
-    from tensorflow.keras.layers import ReLU, PReLU, Activation
-
-    def convert_ReLU(layer):
-        # print(layer.name)
-        if isinstance(layer, ReLU) or (isinstance(layer, Activation) and layer.activation == keras.activations.relu):
-            if target_activation == "PReLU":
-                layer_name = layer.name.replace("_relu", "_prelu")
-                print(">>>> Convert ReLU:", layer.name, "-->", layer_name)
-                # Default initial value in mxnet and pytorch is 0.25
-                return PReLU(shared_axes=[1, 2], alpha_initializer=tf.initializers.Constant(0.25), name=layer_name, **kwargs)
-            elif isinstance(target_activation, str):
-                layer_name = layer.name.replace("_relu", "_" + target_activation)
-                print(">>>> Convert ReLU:", layer.name, "-->", layer_name)
-                return Activation(activation=target_activation, name=layer_name, **kwargs)
-            else:
-                act_class_name = target_activation.__name__
-                layer_name = layer.name.replace("_relu", "_" + act_class_name)
-                print(">>>> Convert ReLU:", layer.name, "-->", layer_name)
-                return target_activation(**kwargs)
-        return layer
-
-    input_tensors = keras.layers.Input(model.input_shape[1:])
-    return keras.models.clone_model(model, input_tensors=input_tensors, clone_function=convert_ReLU)
-
-
-def change_model_input_shape(model, new_input_shape):
-    import json
-    import os
-
-    if model.input_shape[1:-1] == new_input_shape[:2]:
-        return model
-
-    aa = json.loads(model.to_json())
-    aa["config"]["layers"][0]["config"]["batch_input_shape"] = [None, *new_input_shape[:2], 3]
-    bb = tf.keras.models.model_from_json(json.dumps(aa))
-    temp_name = "__change_model_input_shape_temp__.h5"
-    model.save_weights(temp_name)
-    bb.load_weights(temp_name)
-    os.remove(temp_name)
-    print(">>>> Changed model input shape from {} to {}".format(model.input_shape, bb.input_shape))
-    return bb
+            save_weights_to_hdf5_file(filepath, self, **kwargs)
+        else:
+            save_items = {"state_dict": self.state_dict()}
+            if hasattr(self, "optimizer"):
+                save_items.update({"optimizer": self.optimizer.state_dict()})
+            torch.save(save_items, filepath, **kwargs)
+
+    def load(self, filepath, **kwargs):
+        self.load_weights(filepath, **kwargs)
+
+    def save(self, filepath=None, **kwargs):
+        self.save_weights(filepath, **kwargs)
+
+    def count_params(self):
+        total_params = sum([np.prod(ii.shape) for ii in self.state_dict().values() if len(ii.shape) != 0])
+        trainable_params = sum([np.prod(list(ii.shape)) for ii in self.parameters()])
+        non_trainable_params = total_params - trainable_params
+        print("Total params: {:,} | Trainable params: {:,} | Non-trainable params:{:,}".format(total_params, trainable_params, non_trainable_params))
+        return total_params
 
 
-def replace_add_with_stochastic_depth(model, survivals=(1, 0.8)):
+class Model(nn.Module, _Trainer_, _Exporter_):
     """
-    - [Deep Networks with Stochastic Depth](https://arxiv.org/pdf/1603.09382.pdf)
-    - [tfa.layers.StochasticDepth](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/StochasticDepth)
+    Examples:
+    # compile and fit on buildin models
+    >>> os.environ["KECAM_BACKEND"] = "torch"
+    >>> import torch
+    >>> from keras_cv_attention_models import aotnet
+    >>> mm = aotnet.AotNet50(num_classes=10, input_shape=(32, 32, 3))
+    >>> mm.compile(metrics='acc')  # Using default cross_entropy loss
+    >>> xx, yy = torch.rand([300, 3, 32, 32]), torch.functional.F.one_hot(torch.randint(0, 10, size=[300]), 10).float()
+    >>> mm.fit(xx[:256], yy[:256], epochs=2, validation_data=(xx[256:], yy[256:]))
+
+    # Build custom model
+    >>> os.environ["KECAM_BACKEND"] = "torch"
+    >>> import torch
+    >>> from keras_cv_attention_models.backend import layers, models
+    >>> from keras_cv_attention_models.imagenet.callbacks import MyCheckpoint  # Add a callback
+    >>> inputs = layers.Input([3, 32, 32])
+    >>> nn = layers.Conv2D(32, 3, 2, padding='same')(inputs)
+    >>> nn = layers.GlobalAveragePooling2D()(nn)
+    >>> nn = layers.Dense(10)(nn)
+    >>> mm = models.Model(inputs, nn)
+    >>> mm.summary()
+    >>> xx, yy = torch.rand([1128, 3, 32, 32]), torch.functional.F.one_hot(torch.randint(0, 10, size=[1128]), 10).float()
+    >>> loss = lambda y_true, y_pred: (y_true - y_pred.float()).abs().mean()
+    >>> mm.compile(optimizer="AdamW", loss=loss, metrics='acc')
+    >>> callbacks = [MyCheckpoint(basic_save_name="test")]
+    >>> mm.fit(xx[:1000], yy[:1000], epochs=2, callbacks=callbacks, validation_data=(xx[1000:], yy[1000:]))
+
+    # Prediction using buildin model
+    >>> from keras_cv_attention_models.mlp_family import mlp_mixer
+    >>> mm = mlp_mixer.MLPMixerB16(input_shape=(3, 224, 224))
+    >>> # >>>> Load pretrained from: /home/leondgarse/.keras/models/mlp_mixer_b16_imagenet.h5
+    >>> from PIL import Image
+    >>> from skimage.data import chelsea # Chelsea the cat
+    >>> from keras_cv_attention_models.imagenet import decode_predictions
+    >>> imm = Image.fromarray(chelsea()).resize(mm.input_shape[2:])
+    >>> pred = mm(torch.from_numpy(np.array(imm)).permute([2, 0, 1])[None] / 255)
+    >>> print(decode_predictions(pred.detach()))
+    >>> # or just use preset preprocess_input
+    >>> print(mm.decode_predictions(mm(mm.preprocess_input(chelsea()))))
     """
-    from tensorflow_addons.layers import StochasticDepth
 
-    add_layers = [ii.name for ii in model.layers if isinstance(ii, keras.layers.Add)]
-    total_adds = len(add_layers)
-    if isinstance(survivals, float):
-        survivals = [survivals] * total_adds
-    elif isinstance(survivals, (list, tuple)) and len(survivals) == 2:
-        start, end = survivals
-        survivals = [start - (1 - end) * float(ii) / total_adds for ii in range(total_adds)]
-    survivals_dict = dict(zip(add_layers, survivals))
-
-    def __replace_add_with_stochastic_depth__(layer):
-        if isinstance(layer, keras.layers.Add):
-            layer_name = layer.name
-            new_layer_name = layer_name.replace("_add", "_stochastic_depth")
-            new_layer_name = new_layer_name.replace("add_", "stochastic_depth_")
-            survival_probability = survivals_dict[layer_name]
-            if survival_probability < 1:
-                print("Converting:", layer_name, "-->", new_layer_name, ", survival_probability:", survival_probability)
-                return StochasticDepth(survival_probability, name=new_layer_name)
-            else:
-                return layer
-        return layer
+    num_instances = 0  # Count instances
 
-    input_tensors = keras.layers.Input(model.input_shape[1:])
-    return keras.models.clone_model(model, input_tensors=input_tensors, clone_function=__replace_add_with_stochastic_depth__)
+    @classmethod
+    def __count__(cls):
+        cls.num_instances += 1
+
+    def __init__(self, inputs=[], outputs=[], name=None, **kwargs):
+        super().__init__()
+        self.name = "model_{}".format(self.num_instances) if name == None else name
+        self.nodes = None
+
+        self.output = outputs
+        self.outputs = outputs if isinstance(outputs, (list, tuple)) else [outputs]
+        self.output_shape = [tuple(ii.shape) for ii in self.outputs] if isinstance(outputs, (list, tuple)) else tuple(outputs.shape)
+        self.output_names = [ii.name for ii in self.outputs]
+
+        self.input = inputs
+        self.inputs = inputs if isinstance(inputs, (list, tuple)) else [inputs]
+        self.input_shape = [tuple(ii.shape) for ii in self.inputs] if isinstance(inputs, (list, tuple)) else tuple(inputs.shape)
+        self.input_names = [ii.name for ii in self.inputs]
+        self.input_dtype_dict = {ii.name: ii.dtype for ii in self.inputs}
+
+        self.num_outputs = len(self.outputs)
+        self.create_forward_pipeline()
+        self.eval()  # Set eval mode by default
+        self.debug = False
+
+    def create_forward_pipeline(self, **kwargs):
+        forward_pipeline, all_layers, outputs = [], {}, []
+        dfs_queue = []
+        intra_nodes_ref = {}  # node names, and how many times they should be used
+        for ii in self.inputs:
+            dfs_queue.extend(list(set(ii.next_nodes)))
+            intra_nodes_ref[ii.name] = len(ii.next_nodes)
+        while len(dfs_queue) > 0:
+            cur_node = dfs_queue.pop(-1)
+            # print(cur_node.name, cur_node.next_nodes)
+            if len(cur_node.pre_nodes) > 1 and not all([ii.name in intra_nodes_ref for ii in cur_node.pre_nodes]):
+                continue
+            if cur_node.name in intra_nodes_ref:
+                raise ValueError("All nodes name should be unique: cur_node: {}, intra_nodes_ref: {}".format(cur_node.name, list(intra_nodes_ref.keys())))
+
+            # `set` is used here in case current node outputs multi times to next node, like `all_layers.Add()([inputs, inputs])`.
+            # dfs_queue.extend(list(set(cur_node.next_nodes)))
+            dfs_queue.extend([ii for ii in cur_node.next_nodes if ii not in dfs_queue])
+            # print(f"{dfs_queue = }")
+
+            forward_pipeline.append(cur_node)
+            setattr(self, cur_node.name.replace(".", "_"), cur_node.callable)
+            all_layers[cur_node.layer.name] = cur_node.layer
+
+            # print(f"{cur_node.name = }, {len(cur_node.next_nodes) = }")
+            intra_nodes_ref[cur_node.name] = len(cur_node.next_nodes)
+            if cur_node.name in self.output_names:
+                intra_nodes_ref[cur_node.name] = intra_nodes_ref.get(cur_node.name, 0) + 1
+            if all([ii in intra_nodes_ref for ii in self.output_names]):
+                break
+        self.forward_pipeline, self.intra_nodes_ref, self.__layers__ = forward_pipeline, intra_nodes_ref, all_layers
 
+    def input_to_tensor(self, inputs, dtype=torch.float32):
+        param = next(self.parameters())
+        device = param.device
+        dtype = param.dtype if dtype in (torch.float16, torch.float32) else dtype
+
+        if not isinstance(inputs, torch.Tensor):
+            inputs = torch.as_tensor(inputs, device=device)
+        if inputs.dtype != dtype:
+            inputs = inputs.to(dtype)
+        if inputs.device != device:
+            inputs = inputs.to(device)
+        return inputs
+
+    def forward(self, inputs, **kwargs):
+        if isinstance(inputs, layers.GraphNode) or (isinstance(inputs, (list, tuple)) and any([isinstance(ii, layers.GraphNode) for ii in inputs])):
+            return self.graphnode_forward(inputs)
+
+        # print(' -> '.join([ii.name for ii in self.forward_pipeline]))
+        if isinstance(inputs, (list, tuple)):  # Multi inputs in list or tuple format
+            intra_nodes = {kk: [vv] * self.intra_nodes_ref[kk] for kk, vv in zip(self.input_names, inputs)}
+        elif isinstance(inputs, dict):  # Multi inputs in dict format
+            intra_nodes = {kk: [vv] * self.intra_nodes_ref[kk] for kk, vv in inputs.items()}
+        else:  # Single input
+            intra_nodes = {self.input_names[0]: [inputs] * self.intra_nodes_ref[self.input_names[0]]}
+        intra_nodes = {kk: [self.input_to_tensor(ii, dtype=self.input_dtype_dict[kk]) for ii in vv] for kk, vv in intra_nodes.items()}
+
+        for node in self.forward_pipeline:
+            if self.debug:
+                print(">>>> [{}], pre_node_names: {}, next_node_names: {}".format(node.name, node.pre_node_names, node.next_node_names))
+                print("     intra_nodes:", {kk: len(vv) for kk, vv in intra_nodes.items() if len(vv) > 0})
 
-def replace_add_with_drop_connect(model, drop_rate=(0, 0.2)):
-    """
-    - [Deep Networks with Stochastic Depth](https://arxiv.org/pdf/1603.09382.pdf)
-    - [tfa.layers.StochasticDepth](https://www.tensorflow.org/addons/api_docs/python/tfa/layers/StochasticDepth)
-    """
-
-    add_layers = [ii.name for ii in model.layers if isinstance(ii, keras.layers.Add)]
-    total_adds = len(add_layers)
-    if isinstance(drop_rate, float):
-        drop_rates = [drop_rate] * total_adds
-    elif isinstance(drop_rate, (list, tuple)) and len(drop_rate) == 2:
-        start, end = drop_rate
-        drop_rates = [(end - start) * float(ii) / total_adds for ii in range(total_adds)]
-    drop_conn_rate_dict = dict(zip(add_layers, drop_rates))
-
-    def __replace_add_with_stochastic_depth__(layer):
-        if isinstance(layer, keras.layers.Add):
-            layer_name = layer.name
-            new_layer_name = layer_name.replace("_add", "_drop_conn")
-            new_layer_name = new_layer_name.replace("add_", "drop_conn_")
-            drop_conn_rate = drop_conn_rate_dict[layer_name]
-            if drop_conn_rate < 1:
-                print("Converting:", layer_name, "-->", new_layer_name, ", drop_conn_rate:", drop_conn_rate)
-                return DropConnect(drop_conn_rate, name=new_layer_name)
+            if len(node.pre_nodes) > 1:
+                cur_inputs = [intra_nodes[ii].pop() for ii in node.pre_node_names]
             else:
-                return layer
-        return layer
-
-    input_tensors = keras.layers.Input(model.input_shape[1:])
-    return keras.models.clone_model(model, input_tensors=input_tensors, clone_function=__replace_add_with_stochastic_depth__)
+                cur_inputs = intra_nodes[node.pre_node_names[0]].pop()
 
+            if self.debug:
+                print("     inputs.shape:", [np.shape(ii) for ii in cur_inputs] if len(node.pre_nodes) > 1 else np.shape(cur_inputs))
 
-def replace_stochastic_depth_with_add(model, drop_survival=False):
-    from tensorflow_addons.layers import StochasticDepth
-
-    def __replace_stochastic_depth_with_add__(layer):
-        if isinstance(layer, StochasticDepth):
-            layer_name = layer.name
-            new_layer_name = layer_name.replace("_stochastic_depth", "_lambda")
-            survival = layer.survival_probability
-            print("Converting:", layer_name, "-->", new_layer_name, ", survival_probability:", survival)
-            if drop_survival or not survival < 1:
-                return keras.layers.Add(name=new_layer_name)
-            else:
-                return keras.layers.Lambda(lambda xx: xx[0] + xx[1] * survival, name=new_layer_name)
-        return layer
+            output = node.callable(cur_inputs)
+            intra_nodes[node.name] = [output] * self.intra_nodes_ref[node.name]
+            if self.debug:
+                print("     output.shape:", np.shape(output))
+                print("     intra_nodes:", {kk: len(vv) for kk, vv in intra_nodes.items() if len(vv) > 0})
+        return [intra_nodes[ii][0] for ii in self.output_names] if self.num_outputs != 1 else intra_nodes[self.output_names[0]][0]
+
+    @torch.no_grad()
+    def predict(self, inputs, **kwargs):
+        return self.forward(inputs, **kwargs)
+
+    def graphnode_forward(self, inputs):
+        self.input_shape = [() if isinstance(ii, (int, float)) else ii.shape for ii in inputs] if isinstance(inputs, (list, tuple)) else inputs.shape
+        cur_node = layers.GraphNode(self.output_shape, name=self.name if self.nodes is None else (self.name + "_{}".format(len(self.nodes))))
+        cur_node.callable = self
+        cur_node.layer = self
+        cur_node.set_pre_nodes(inputs)
+
+        inputs = inputs if isinstance(inputs, (list, tuple)) else [inputs]
+        for ii in inputs:
+            if isinstance(ii, layers.GraphNode):
+                ii.set_next_nodes(cur_node)
+
+        if self.nodes is None:
+            self.nodes = [cur_node]
+            self.node = cur_node
+        else:
+            self.nodes.append(cur_node)
+        return cur_node
 
-    input_tensors = keras.layers.Input(model.input_shape[1:])
-    return keras.models.clone_model(model, input_tensors=input_tensors, clone_function=__replace_stochastic_depth_with_add__)
+    def compile(self, **kwargs):
+        # compile is tooken by nn.Module, checking by kwargs if calling `train_compile` or default `compile`
+        return self.train_compile(**kwargs) if "loss" in kwargs or "optimizer" in kwargs or "metrics" in kwargs else super().compile(**kwargs)
 
+    @property
+    def layers(self):
+        return list(self.__layers__.values())
 
-def convert_to_token_label_model(model, pool_layer_id="auto"):
-    # Search pool layer id
-    num_total_layers = len(model.layers)
-    if pool_layer_id == "auto":
-        for header_layer_id, layer in enumerate(model.layers[::-1]):
-            header_layer_id = num_total_layers - header_layer_id - 1
-            print("[Search pool layer] header_layer_id = {}, layer.name = {}".format(header_layer_id, layer.name))
-            if isinstance(layer, keras.layers.GlobalAveragePooling2D):
-                break
-        pool_layer_id = header_layer_id
+    @property
+    def weights(self):
+        skips = ["num_batches_tracked", "total_ops", "total_params"]
+        buffers = [layers.Weight(name=name, value=value) for name, value in self.named_buffers() if not name.split(".")[-1] in skips]
+        parameters = [layers.Weight(name=name, value=value) for name, value in self.named_parameters()]
+        return parameters + buffers
 
-    nn = model.layers[pool_layer_id - 1].output  # layer output before pool layer
+    def get_layer(self, layer_name):
+        return self.__layers__[layer_name]
 
-    # Add header layers w/o pool layer
-    for header_layer_id in range(pool_layer_id + 1, num_total_layers):
-        aa = model.layers[header_layer_id]
-        config = aa.get_config()
-        config["name"] = config["name"] + "_token_label"
-        if isinstance(aa, keras.layers.LayerNormalization) and config["axis"] == [1]:
-            config["axis"] = [-1]
-        # print(config)
-        print("[Build new layer] header_layer_id = {}, layer.name = {}".format(header_layer_id, config["name"]))
-
-        bb = aa.__class__.from_config(config)
-        bb.build(nn.shape)
-        bb.set_weights(aa.get_weights())
-        nn = bb(nn)
-    token_label_model = keras.models.Model(model.inputs[0], [*model.outputs, nn])
-    print("token_label_model.output_shape =", token_label_model.output_shape)
-    return token_label_model
-
-
-""" Get model info """
-
-
-def get_actual_survival_probabilities(model):
-    from tensorflow_addons.layers import StochasticDepth
-
-    return [ii.survival_probability for ii in model.layers if isinstance(ii, StochasticDepth)]
-
-
-def get_actual_drop_connect_rates(model):
-    return [ii.rate for ii in model.layers if isinstance(ii, keras.layers.Dropout) or isinstance(ii, DropConnect)]
-
-
-def get_pyramide_feature_layers(model, match_reg="^stack_?(\\d+).*output.*$"):
-    """Pick all stack output layers"""
-    import re
-
-    dd = {}
-    for ii in model.layers:
-        matched = re.match(match_reg, ii.name)
-        if matched is not None:
-            cur_stack = "stack_" + matched[1] + "_output"
-            dd.update({cur_stack: ii})
-
-    """ Filter those have same downsample rate """
-    ee = {str(vv.output_shape[1]): vv for kk, vv in dd.items()}
-    return list(ee.values())
-
-
-def get_global_avg_pool_layer_id(model):
-    """Search GlobalAveragePooling2D layer id"""
-    num_total_layers = len(model.layers)
-    for header_layer_id, layer in enumerate(model.layers[::-1]):
-        header_layer_id = num_total_layers - header_layer_id - 1
-        print("[Search pool layer] header_layer_id = {}, layer.name = {}".format(header_layer_id, layer.name))
-        if isinstance(layer, keras.layers.GlobalAveragePooling2D):
-            break
-    return header_layer_id
-
-
-def get_flops(model):
-    # https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-849439287
-    from tensorflow.python.profiler import model_analyzer, option_builder
-
-    input_signature = [tf.TensorSpec(shape=(1, *ii.shape[1:]), dtype=ii.dtype, name=ii.name) for ii in model.inputs]
-    forward_graph = tf.function(model, input_signature).get_concrete_function().graph
-    options = option_builder.ProfileOptionBuilder.float_operation()
-    graph_info = model_analyzer.profile(forward_graph, options=options)
-    flops = graph_info.total_float_ops // 2
-    print(">>>> FLOPs: {:,}, GFLOPs: {:.4f}G".format(flops, flops / 1e9))
-    return flops
-
-
-def print_model_params_count(model):
-    aa = []
-    model.summary(print_fn=lambda xx: aa.append(xx) if "params:" in xx else None)
-    print("\n".join(aa))
-    return {ii.split(":")[0]: int("".join(ii.split(":")[1].strip().split(","))) for ii in aa}
-
-
-""" Inference """
-
-
-def convert_to_mixed_float16(model, convert_batch_norm=False, policy_name="mixed_float16"):
-    policy = keras.mixed_precision.Policy(policy_name)
-    policy_config = keras.utils.serialize_keras_object(policy)
-    from tensorflow.keras.layers import InputLayer, Activation
-    from tensorflow.keras.activations import linear
-
-    def do_convert_to_mixed_float16(layer):
-        if not convert_batch_norm and isinstance(layer, keras.layers.BatchNormalization):
-            return layer
-        if not isinstance(layer, InputLayer) and not (isinstance(layer, Activation) and layer.activation == linear):
-            aa = layer.get_config()
-            aa.update({"dtype": policy_config})
-            bb = layer.__class__.from_config(aa)
-            bb.build(layer.input_shape)
-            bb.set_weights(layer.get_weights())
-            return bb
-        return layer
-
-    input_tensors = keras.layers.Input(model.input_shape[1:])
-    return keras.models.clone_model(model, input_tensors=input_tensors, clone_function=do_convert_to_mixed_float16)
-
-
-def convert_mixed_float16_to_float32(model):
-    from tensorflow.keras.layers import InputLayer, Activation
-    from tensorflow.keras.activations import linear
-
-    def do_convert_to_mixed_float16(layer):
-        if not isinstance(layer, InputLayer) and not (isinstance(layer, Activation) and layer.activation == linear):
-            aa = layer.get_config()
-            aa.update({"dtype": "float32"})
-            bb = layer.__class__.from_config(aa)
-            bb.build(layer.input_shape)
-            bb.set_weights(layer.get_weights())
-            return bb
-        return layer
-
-    input_tensors = keras.layers.Input(model.input_shape[1:])
-    return keras.models.clone_model(model, input_tensors=input_tensors, clone_function=do_convert_to_mixed_float16)
-
-
-def fuse_conv_bn(conv_layer, bn_layer):
-    # BatchNormalization returns: gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta
-    # --> conv_w_new = gamma * conv_w / np.sqrt(var + epsilon)
-    # --> conv_b_new = gamma * (conv_b - mean) / sqrt(var + epsilon) + beta
-    batch_std = tf.sqrt(bn_layer.moving_variance + bn_layer.epsilon)
-    if isinstance(conv_layer, keras.layers.DepthwiseConv2D):
-        ww = tf.transpose(conv_layer.depthwise_kernel, [0, 1, 3, 2]) * bn_layer.gamma / batch_std
-        ww = tf.transpose(ww, [0, 1, 3, 2])
-    else:
-        ww = conv_layer.kernel * bn_layer.gamma / batch_std
-
-    if conv_layer.use_bias:
-        bias = bn_layer.gamma * (conv_layer.bias - bn_layer.moving_mean) / batch_std + bn_layer.beta
-    else:
-        bias = bn_layer.gamma * (-1 * bn_layer.moving_mean) / batch_std + bn_layer.beta
-
-    cc = conv_layer.get_config()
-    cc["use_bias"] = True
-    fused_conv_bn = conv_layer.__class__.from_config(cc)
-    fused_conv_bn.build(conv_layer.input_shape)
-    fused_conv_bn.set_weights([ww, bias])
-    return fused_conv_bn
+    def set_debug(self, debug=True):
+        self.debug = debug
+        print(">>>> debug: {}".format(self.debug))
 
 
-def convert_to_fused_conv_bn_model(model):
+class Sequential(Model):
     """
-    Convert model by fusing Conv + batchnorm
-
-    Exampls:
-    >>> from keras_cv_attention_models import model_surgery
-    >>> mm = keras.applications.ResNet50()
-    >>> bb = model_surgery.convert_to_fused_conv_bn_model(mm)
+    >>> os.environ["KECAM_BACKEND"] = "torch"
+    >>> import torch
+    >>> from keras_cv_attention_models.pytorch_backend import layers, models, functional
+    >>> mm = models.Sequential([
+    >>>     layers.Input([3, 32, 32]),
+    >>>     layers.Conv2D(32, 3, 2, padding='same'),
+    >>>     layers.GlobalAveragePooling2D(),
+    >>>     layers.Dense(10),
+    >>>     functional.softmax,  # Can also be an functional callable
+    >>> ])
+    >>> mm.summary()
+    >>> print(mm(torch.ones([1, 3, 32, 32])).shape)
+    >>> # torch.Size([1, 10])
     """
-    import json
 
-    """ Check bn layers with conv layer input """
-    model_config = json.loads(model.to_json())
-    ee = {layer["name"]: layer for layer in model_config["config"]["layers"]}
-    fuse_convs, fuse_bns = [], []
-    conv_names = ["Conv2D", "DepthwiseConv2D"]
-    for layer in model_config["config"]["layers"]:
-        if layer["class_name"] == "BatchNormalization" and len(layer["inbound_nodes"]) == 1:
-            input_node = layer["inbound_nodes"][0][0]
-            if isinstance(input_node, list) and ee.get(input_node[0], {"class_name": None})["class_name"] in conv_names:
-                fuse_convs.append(input_node[0])
-                fuse_bns.append(layer["name"])
-    print(">>>> len(fuse_convs) =", len(fuse_convs), "len(fuse_bns) =", len(fuse_bns))
-    # len(fuse_convs) = 53, len(fuse_bns) = 53
-
-    """ Create new model config """
-    layers = []
-    fused_bn_dict = dict(zip(fuse_bns, fuse_convs))
-    fused_conv_dict = dict(zip(fuse_convs, fuse_bns))
-    is_inbound_elem = lambda xx: isinstance(xx, list) and isinstance(xx[0], str)
-    for layer in model_config["config"]["layers"]:
-        if layer["name"] in fuse_convs:
-            print(">>>> Fuse conv bn:", layer["name"])
-            layer["config"]["use_bias"] = True
-        elif layer["name"] in fuse_bns:
-            continue
-
-        for ii in layer["inbound_nodes"]:
-            # print(ii)
-            if is_inbound_elem(ii):
-                # print(">>>> Replace inbound_nodes: {}, {} --> {}".format(layer["name"], ii[0], fused_bn_dict[ii[0]]))
-                ii[0] = fused_bn_dict.get(ii[0], ii[0])
-                ii[3] = {kk: [fused_bn_dict.get(vv[0], vv[0]), *vv[1:]] if is_inbound_elem(vv) else vv for kk, vv in ii[3].items()}
-            elif isinstance(ii, list) and isinstance(ii[0], list):
-                for jj in ii:
-                    jj[0] = fused_bn_dict.get(jj[0], jj[0])
-                    jj[3] = {kk: [fused_bn_dict.get(vv[0], vv[0]), *vv[1:]] if is_inbound_elem(vv) else vv for kk, vv in jj[3].items()}
-
-        layers.append(layer)
-    model_config["config"]["layers"] = layers
-    new_model = keras.models.model_from_json(json.dumps(model_config))
-
-    """ New model set layer weights by layer names """
-    for layer in new_model.layers:
-        if layer.name in fuse_bns:  # This should not happen
-            continue
-
-        orign_layer = model.get_layer(layer.name)
-        if layer.name in fused_conv_dict:
-            orign_bn_layer = model.get_layer(fused_conv_dict[layer.name])
-            print(">>>> Fuse conv bn", layer.name, orign_bn_layer.name)
-            conv_bn = fuse_conv_bn(orign_layer, orign_bn_layer)
-            layer.set_weights(conv_bn.get_weights())
+    def __init__(self, sequence_layers=None, name=None, **kwargs):
+        self.sequence_layers, self.name, self.kwargs = sequence_layers, name, kwargs
+        if isinstance(sequence_layers[0], layers.Input):
+            self.build(sequence_layers[0].shape)
         else:
-            layer.set_weights(orign_layer.get_weights())
-    return new_model
-
-
-""" TFLite """
-
-
-@keras.utils.register_keras_serializable(package="model_surgery")
-class SplitConv2D(keras.layers.Conv2D):
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-        self.super_class = keras.layers.Conv2D
+            self.input_shape, self.built = None, False
 
     def build(self, input_shape):
-        cc = self.get_config().copy()
-        cc.update({"groups": 1, "filters": self.filters // self.groups})
-        grouped_input_shape = (*input_shape[:-1], input_shape[-1] // self.groups)
-        self.convs = []
-        for ii in range(self.groups):
-            name_scope = self.name + "_{}".format(ii)
-            with tf.name_scope(name_scope) as scope:
-                cc["name"] = self.name + "_{}".format(ii)
-                conv = self.super_class.from_config(cc)
-                conv.build(grouped_input_shape)
-            self.convs.append(conv)
-
-    def call(self, inputs, **kwargs):
-        return tf.concat([conv(ii) for conv, ii in zip(self.convs, tf.split(inputs, self.groups, axis=-1))], axis=-1)
-
-
-@keras.utils.register_keras_serializable(package="model_surgery")
-class SplitScaledStandardizedConv2D(SplitConv2D):
-    def __init__(self, gamma=1.0, eps=1e-5, **kwargs):
-        from keras_cv_attention_models import attention_layers
-
-        super().__init__(**kwargs)
-        self.super_class = attention_layers.ScaledStandardizedConv2D
-        self.eps, self.gamma = eps, gamma
-
-    def get_config(self):
-        base_config = super().get_config()
-        base_config.update({"eps": self.eps, "gamma": self.gamma})
-        return base_config
-
-
-def convert_groups_conv2d_2_split_conv2d(model):
-    from tensorflow.keras.layers import Conv2D
-
-    def __convert_groups_conv2d_2_split_conv2d__(layer):
-        if isinstance(layer, Conv2D) and not isinstance(layer, SplitConv2D) and layer.groups != 1:
-            aa = layer.get_config()
-            # Check if ScaledStandardizedConv2D or typical Conv2D
-            bb = SplitScaledStandardizedConv2D.from_config(aa) if hasattr(layer, "gain") else SplitConv2D.from_config(aa)
-            # bb.build(layer.input_shape)   # looks like build not working [ ??? ]
-            bb(tf.ones([1, *layer.input_shape[1:]]))
-            wws = tf.split(layer.get_weights()[0], bb.groups, axis=-1)
-            if bb.use_bias:
-                bbs = tf.split(layer.get_weights()[1], bb.groups, axis=-1)
-            if hasattr(layer, "gain"):
-                # ScaledStandardizedConv2D with gain from NFNets
-                ggs = tf.split(layer.get_weights()[-1], bb.groups, axis=-1)
-            for id in range(bb.groups):
-                sub_weights = [wws[id].numpy()]
-                if bb.use_bias:
-                    sub_weights.append(bbs[id].numpy())
-                if hasattr(layer, "gain"):
-                    sub_weights.append(ggs[id].numpy())
-                bb.convs[id].set_weights(sub_weights)
-            return bb
-        return layer
-
-    input_tensors = keras.layers.Input(model.input_shape[1:])
-    return keras.models.clone_model(model, input_tensors=input_tensors, clone_function=__convert_groups_conv2d_2_split_conv2d__)
-
-
-def convert_gelu_and_extract_patches_for_tflite(model):
-    from keras_cv_attention_models import attention_layers
-
-    def __convert_gelu_and_extract_patches_for_tflite__(layer):
-        if isinstance(layer, keras.layers.Activation) and layer.activation.__name__ == "gelu":
-            return keras.layers.Lambda(lambda xx: tf.nn.gelu(xx, approximate=True))
-        elif isinstance(layer, attention_layers.CompatibleExtractPatches):
-            aa = layer.get_config()
-            aa.update({"force_conv": True})
-            bb = attention_layers.CompatibleExtractPatches.from_config(aa)
-            bb.build(layer.input_shape)  # No weights for this layer
-            return bb
-        return layer
-
-    input_tensors = keras.layers.Input(model.input_shape[1:])
-    return keras.models.clone_model(model, input_tensors=input_tensors, clone_function=__convert_gelu_and_extract_patches_for_tflite__)
-
-
-def prepare_for_tflite(model):
-    model = convert_groups_conv2d_2_split_conv2d(model)
-    model = convert_gelu_and_extract_patches_for_tflite(model)
-    return model
+        inputs = layers.Input(input_shape[1:])
+        next_node = inputs
+        for layer in self.sequence_layers:
+            if isinstance(layer, layers.Input):
+                continue
+            next_node = layer(next_node)
+        super().__init__(inputs, next_node, name=self.name)
+        self.built = True
+
+    def add(self, layer):
+        self.sequence_layers.append(layer)
+        if self.input_shape is not None:
+            self.build(self.input_shape)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/nat/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/nat/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -3,36 +3,49 @@
     NAT_Mini,
     NAT_Tiny,
     NAT_Small,
     NAT_Base,
     MultiHeadRelativePositionalKernelBias,
     neighborhood_attention,
 )
+from keras_cv_attention_models.nat.dinat import (
+    DiNAT_Mini,
+    DiNAT_Tiny,
+    DiNAT_Small,
+    DiNAT_Base,
+    DiNAT_Large,
+    DiNAT_Large_K11,
+)
 
 __head_doc__ = """
 Keras implementation of [NAT](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer).
 Paper [PDF 2204.07143 Neighborhood Attention Transformer](https://arxiv.org/pdf/2204.07143.pdf).
+Paper [PDF 2209.15001 Dilated Neighborhood Attention Transformer](https://arxiv.org/pdf/2209.15001.pdf).
 """
 
 __tail_doc__ = """  stem_width: output dimension for stem block. Default -1 means using `out_channels[0]`
   attn_kernel_size: kernel_size for `neighborhood_attention` block, defualt 7.
+  use_every_other_dilations: True for DiNAT, False for others.
+      Using `dilation_rate=nn.shape[1] // attn_kernel_size` in every other attention blocks.
   mlp_ratio: channel expansion ratio for mlp hidden layers, default 3 for NAT_Mini and NAT_Tiny, 2 for NAT_Small and NAT_Base.
   layer_scale: layer scale init value. `-1` means not applying, any value `>=0` will add a scale value for each block output.
       [Going deeper with Image Transformers](https://arxiv.org/pdf/2103.17239.pdf).
       Default -1 for NAT_Mini and NAT_Tiny, 1e-5 for NAT_Small and NAT_Base
   input_shape: it should have exactly 3 inputs channels, like `(224, 224, 3)`.
   num_classes: number of classes to classify images into. Set `0` to exclude top layers.
+  activation: activation used in whole model, default `gelu`.
   drop_connect_rate: is used for [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382).
       Can be value like `0.2`, indicates the drop probability linearly changes from `0 --> 0.2` for `top --> bottom` layers.
       A higher value means a higher probability will drop the deep branch.
       or `0` to disable (default).
   dropout: dropout rate if top layers is included.
   classifier_activation: A `str` or callable. The activation function to use on the "top" layer if `num_classes > 0`.
       Set `classifier_activation=None` to return the logits of the "top" layer.
-  pretrained: None or one of ["imagenet", "token_label"].
+  pretrained: None or one of ["imagenet21k-ft1k", "imagenet21k"] for `DiNAT_Large`, or "imagenet21k-ft1k" for `DiNAT_Large_K11`,
+      or "imagenet" for others.
 
 Returns:
     A `keras.Model` instance.
 """
 
 NAT.__doc__ = __head_doc__ + """
 Args:
@@ -44,36 +57,55 @@
 Model architectures:
   | Model     | Params | FLOPs  | Input | Top1 Acc |
   | --------- | ------ | ------ | ----- | -------- |
   | NAT_Mini  | 20.0M  | 2.73G  | 224   | 81.8     |
   | NAT_Tiny  | 27.9M  | 4.34G  | 224   | 83.2     |
   | NAT_Small | 50.7M  | 7.84G  | 224   | 83.7     |
   | NAT_Base  | 89.8M  | 13.76G | 224   | 84.3     |
+
+  | Model                     | Params | FLOPs  | Input | Top1 Acc |
+  | ------------------------- | ------ | ------ | ----- | -------- |
+  | DiNAT_Mini                | 20.0M  | 2.73G  | 224   | 81.8     |
+  | DiNAT_Tiny                | 27.9M  | 4.34G  | 224   | 82.7     |
+  | DiNAT_Small               | 50.7M  | 7.84G  | 224   | 83.8     |
+  | DiNAT_Base                | 89.8M  | 13.76G | 224   | 84.4     |
+  | DiNAT_Large, 22k          | 200.9M | 30.58G | 224   | 86.6     |
+  | - 21k num_classes=21841   | 200.9M | 30.58G | 224   |          |
+  | - 22k, 384                | 200.9M | 89.86G | 384   | 87.4     |
+  | DiNAT_Large_K11, 22k, 384 | 201.1M | 92.57G | 384   | 87.5     |
 """
 
 NAT_Mini.__doc__ = __head_doc__ + """
 Args:
 """ + __tail_doc__
 
 NAT_Tiny.__doc__ = NAT_Mini.__doc__
 NAT_Small.__doc__ = NAT_Mini.__doc__
 NAT_Base.__doc__ = NAT_Mini.__doc__
 
+DiNAT_Mini.__doc__ = NAT_Mini.__doc__
+DiNAT_Tiny.__doc__ = NAT_Mini.__doc__
+DiNAT_Small.__doc__ = NAT_Mini.__doc__
+DiNAT_Base.__doc__ = NAT_Mini.__doc__
+DiNAT_Large.__doc__ = NAT_Mini.__doc__
+DiNAT_Large_K11.__doc__ = NAT_Mini.__doc__
+
 MultiHeadRelativePositionalKernelBias.__doc__ = __head_doc__ + """
 Multi Head Relative Positional Kernel Bias layer. Weights depends on `num_heads` and `kernel_size` not on `input_shape`.
 
 input (is_heads_first=False): `[batch, height * width, num_heads, ..., size * size]`
 input (is_heads_first=True): `[batch, num_heads, height * width, ..., size * size]`
 positional_bias: `[num_heads, (2 * size - 1) * (2 * size - 1)]`
 output: `input + gathered positional_bias`.
 condition: height >= size, width >= size
 
 Args:
   input_height: specify `height` for `input` if not square, default -1 assums `input_height == input_width`.
   is_heads_first: boolean value if input is `[batch, num_heads, height * width]` or `[batch, height * width, num_heads]`.
+  dilation_rate: dilation_rate used for last dimension `size * size`.
 
 Examples:
 
 # Basic
 >>> from keras_cv_attention_models import attention_layers
 >>> aa = attention_layers.MultiHeadRelativePositionalKernelBias()
 >>> print(f"{aa(tf.ones([1, 29 * 29, 8, 5 * 5])).shape = }")
@@ -105,14 +137,15 @@
   inputs: input tensor.
   kernel_size: extracting patch kernel_size for key and value.
   num_heads: Number of attention heads.
   key_dim: Size of each attention head for query and key. Default `0` for `key_dim = inputs.shape[-1] // num_heads`.
   out_weight: Boolean, whether use an ouput dense.
   qkv_bias: Boolean, whether the qkv dense layer use bias vectors/matrices.
   out_bias: Boolean, whether the ouput dense layer use bias vectors/matrices.
+  dilation_rate: dilation_rate for extracting `key` and `value` features.
   attn_dropout: Dropout probability for attention scores.
   output_dropout: Dropout probability for attention output.
 
 Examples:
 
 >>> from keras_cv_attention_models import attention_layers
 >>> inputs = keras.layers.Input([14, 16, 256])
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/nat/nat.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/edgenext/edgenext.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,207 +1,257 @@
-import tensorflow as tf
-from tensorflow import keras
+import math
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     ChannelAffine,
-    CompatibleExtractPatches,
     conv2d_no_bias,
+    depthwise_conv2d_no_bias,
     drop_block,
     layer_norm,
     mlp_block,
-    output_block,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
-    "nat_base": {"imagenet": "d221eaec4af71dd3522625333aa73d9e"},
-    "nat_mini": {"imagenet": "921de737ccebe4ab210dcd79ec0aed5f"},
-    "nat_small": {"imagenet": "5716d0d54f3abd582e586fbdea04b3db"},
-    "nat_tiny": {"imagenet": "91ed860ca950de181f433c189135070b"},
+    "edgenext_base": {
+        "imagenet": {256: "9492ab3b407aabe84cead0bded7f7dd7"},
+        "usi": {256: "9d93a7524eda4701b2364540a2a44c8a"},
+        "imagenet21k-ft1k": {256: "d82a6f8cfd1e85cb16b0d6ced189eb1e"},
+    },
+    "edgenext_small": {"imagenet": {256: "0234641a703283de1cb0d935bb0325e4"}, "usi": {256: "c237761b5bd5c32041d6b758186a0716"}},
+    "edgenext_x_small": {"imagenet": {256: "472df7659422c7feffbec8012a0f6fa4"}},
+    "edgenext_xx_small": {"imagenet": {256: "4190ba28c7caa2fe73215448f8abebd6"}},
 }
+LAYER_NORM_EPSILON = 1e-6
 
 
-@tf.keras.utils.register_keras_serializable(package="kecam/nat")
-class MultiHeadRelativePositionalKernelBias(tf.keras.layers.Layer):
-    def __init__(self, input_height=-1, is_heads_first=False, **kwargs):
+@backend.register_keras_serializable(package="kecam/edgenext")
+class PositionalEncodingFourier(layers.Layer):
+    def __init__(self, filters=32, temperature=1e4, **kwargs):
         super().__init__(**kwargs)
-        self.input_height, self.is_heads_first = input_height, is_heads_first
+        self.filters, self.temperature = filters, float(temperature)
+        self.epsilon = 1e-6
+        self.scale = 2 * math.acos(-1.0)  # 2 * pi
 
     def build(self, input_shape):
-        # input (is_heads_first=False): `[batch, height * width, num_heads, ..., size * size]`
-        # input (is_heads_first=True): `[batch, num_heads, height * width, ..., size * size]`
-        blocks, num_heads = (input_shape[2], input_shape[1]) if self.is_heads_first else (input_shape[1], input_shape[2])
-        size = int(tf.math.sqrt(float(input_shape[-1])))
-        height = self.input_height if self.input_height > 0 else int(tf.math.sqrt(float(blocks)))
-        width = blocks // height
-        pos_size = 2 * size - 1
-        initializer = tf.initializers.truncated_normal(stddev=0.02)
-        self.pos_bias = self.add_weight(name="positional_embedding", shape=(num_heads, pos_size * pos_size), initializer=initializer, trainable=True)
-
-        idx_hh, idx_ww = tf.range(0, size), tf.range(0, size)
-        coords = tf.reshape(tf.expand_dims(idx_hh, -1) * pos_size + idx_ww, [-1])
-        bias_hh = tf.concat([idx_hh[: size // 2], tf.repeat(idx_hh[size // 2], height - size + 1), idx_hh[size // 2 + 1 :]], axis=-1)
-        bias_ww = tf.concat([idx_ww[: size // 2], tf.repeat(idx_ww[size // 2], width - size + 1), idx_ww[size // 2 + 1 :]], axis=-1)
-        bias_hw = tf.expand_dims(bias_hh, -1) * pos_size + bias_ww
-        bias_coords = tf.expand_dims(bias_hw, -1) + coords
-        bias_coords = tf.reshape(bias_coords, [-1, size**2])[::-1]  # torch.flip(bias_coords, [0])
-
-        bias_coords_shape = [bias_coords.shape[0]] + [1] * (len(input_shape) - 4) + [bias_coords.shape[1]]
-        self.bias_coords = tf.reshape(bias_coords, bias_coords_shape)  # [height * width, 1 * n, size * size]
-        if not self.is_heads_first:
-            self.transpose_perm = [1, 0] + list(range(2, len(input_shape) - 1))  # transpose [num_heads, height * width] -> [height * width, num_heads]
-
-    def call(self, inputs):
-        if self.is_heads_first:
-            return inputs + tf.gather(self.pos_bias, self.bias_coords, axis=-1)
+        _, height, width, channels = input_shape  # ex: height, width, filters = 12, 27, 32
+        hh, ww = np.arange(height, dtype="float32"), np.arange(width, dtype="float32")
+        hh = (hh + 1) / (float(height) + self.epsilon) * self.scale
+        ww = (ww + 1) / (float(width) + self.epsilon) * self.scale
+
+        dim_t = self.temperature ** (2 * (np.arange(self.filters, dtype="float32") // 2) / self.filters)  # (filters,)
+        pos_hh, pos_ww = np.expand_dims(hh, -1) / dim_t, np.expand_dims(ww, -1) / dim_t  # pos_hh [12, 32], pos_ww [27, 32]
+        pos_hh = np.stack([np.sin(pos_hh[:, 0::2]), np.cos(pos_hh[:, 1::2])], axis=-1)  # pos_hh [12, 16, 2]
+        pos_ww = np.stack([np.sin(pos_ww[:, 0::2]), np.cos(pos_ww[:, 1::2])], axis=-1)  # pos_ww [27, 16, 2]
+        pos_hh = np.repeat(np.reshape(pos_hh, [height, 1, -1]), width, axis=1)  # [12, 27, 32]
+        pos_ww = np.repeat(np.reshape(pos_ww, [1, width, -1]), height, axis=0)  # [12, 27, 32]
+        positional_embedding = np.concatenate([pos_hh, pos_ww], axis=-1)  # [12, 27, 64]
+
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("positional_embedding", functional.convert_to_tensor(positional_embedding, dtype=self.compute_dtype), persistent=False)
         else:
-            return inputs + tf.transpose(tf.gather(self.pos_bias, self.bias_coords, axis=-1), self.transpose_perm)
+            self.positional_embedding = functional.convert_to_tensor(positional_embedding, dtype=self.compute_dtype)
+
+        self.token_projection_ww = self.add_weight(name="ww", initializer="glorot_uniform", shape=(self.filters * 2, channels), trainable=True)
+        self.token_projection_bb = self.add_weight(name="bb", initializer="zeros", shape=(channels,), trainable=True)
+        super().build(input_shape)
+
+    def call(self, inputs, **kwargs):
+        pos_emb = self.positional_embedding @ self.token_projection_ww + self.token_projection_bb
+        # tf.print(pos_emb.shape, attention_scores.shape)
+        return inputs + pos_emb
 
     def get_config(self):
         base_config = super().get_config()
-        base_config.update({"input_height": self.input_height, "is_heads_first": self.is_heads_first})
+        base_config.update({"filters": self.filters, "temperature": self.temperature})
         return base_config
 
 
-def neighborhood_attention(
-    inputs, kernel_size=7, num_heads=4, key_dim=0, out_weight=True, qkv_bias=True, out_bias=True, attn_dropout=0, output_dropout=0, name=None
-):
-    _, hh, ww, cc = inputs.shape
-    key_dim = key_dim if key_dim > 0 else cc // num_heads
-    qk_scale = 1.0 / (float(key_dim) ** 0.5)
-    out_shape = cc
-    qkv_out = num_heads * key_dim
-
-    should_pad_hh, should_pad_ww = max(0, kernel_size - hh), max(0, kernel_size - ww)
-    if should_pad_hh or should_pad_ww:
-        inputs = tf.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
-        _, hh, ww, cc = inputs.shape
-
-    qkv = keras.layers.Dense(qkv_out * 3, use_bias=qkv_bias, name=name and name + "qkv")(inputs)
-    query, key_value = tf.split(qkv, [qkv_out, qkv_out * 2], axis=-1)  # Matching weights from PyTorch
-    query = tf.expand_dims(tf.reshape(query, [-1, hh * ww, num_heads, key_dim]), -2)  # [batch, hh * ww, num_heads, 1, key_dim]
-
-    # key_value: [batch, height // kernel_size, width // kernel_size, kernel_size, kernel_size, key + value]
-    key_value = CompatibleExtractPatches(sizes=kernel_size, strides=1, padding="VALID", compressed=False)(key_value)
-    padded = (kernel_size - 1) // 2
-    # torch.pad 'replicate'
-    key_value = tf.concat([tf.repeat(key_value[:, :1], padded, axis=1), key_value, tf.repeat(key_value[:, -1:], padded, axis=1)], axis=1)
-    key_value = tf.concat([tf.repeat(key_value[:, :, :1], padded, axis=2), key_value, tf.repeat(key_value[:, :, -1:], padded, axis=2)], axis=2)
-
-    key_value = tf.reshape(key_value, [-1, kernel_size * kernel_size, key_value.shape[-1]])
-    key, value = tf.split(key_value, 2, axis=-1)  # [batch * block_height * block_width, kernel_size * kernel_size, key_dim]
-    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch * hh*ww, num_heads, key_dim, kernel_size * kernel_size]
-    key = tf.reshape(key, [-1, hh * ww, num_heads, key_dim, kernel_size * kernel_size])  # [batch, hh*ww, num_heads, key_dim, kernel_size * kernel_size]
-    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 1, 3])
-    value = tf.reshape(value, [-1, hh * ww, num_heads, kernel_size * kernel_size, key_dim])  # [batch, hh*ww, num_heads, kernel_size * kernel_size, key_dim]
-    # print(f">>>> {query.shape = }, {key.shape = }, {value.shape = }")
-
-    # [batch, hh * ww, num_heads, 1, kernel_size * kernel_size]
-    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale
-    attention_scores = MultiHeadRelativePositionalKernelBias(input_height=hh, name=name and name + "pos")(attention_scores)
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
-    attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores) if attn_dropout > 0 else attention_scores
-
-    # attention_output = [batch, block_height * block_width, num_heads, 1, key_dim]
-    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
-    attention_output = tf.reshape(attention_output, [-1, hh, ww, num_heads * key_dim])
+def norm_inverted_bottleneck(inputs, mlp_ratio=4, layer_scale=1e-6, drop_rate=0, activation="gelu", name=""):
+    input_channel = inputs.shape[-1]  # channels_last only, it should be permuted before entering this
+    nn = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name)
+    nn = mlp_block(nn, input_channel * mlp_ratio, activation=activation, name=name)
+    nn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, axis=-1, name=name + "gamma")(nn) if layer_scale >= 0 else nn
+    nn = drop_block(nn, drop_rate=drop_rate, name=name)
+    return nn
+
+
+def cross_covariance_attention(inputs, num_heads=4, key_dim=0, qkv_bias=True, out_bias=True, attn_dropout=0, out_dropout=0, name=None):
+    input_channel = inputs.shape[-1]  # channels_last only, it should be permuted before entering this
+    input_blocks = inputs.shape[1:-1]
+    key_dim = key_dim if key_dim > 0 else input_channel // num_heads
+    qk_out = key_dim * num_heads
+
+    qkv = functional.reshape(inputs, [-1, int(np.prod(input_blocks)), inputs.shape[-1]]) if len(inputs.shape) > 3 else inputs
+    qkv = layers.Dense(qk_out * 3, use_bias=True, name=name and name + "qkv")(qkv)
+    query, key, value = functional.split(qkv, 3, axis=-1)
+    query = functional.transpose(functional.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  #  [batch, num_heads, key_dim, hh * ww]
+    key = functional.transpose(functional.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
+    value = functional.transpose(functional.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
+
+    norm_query, norm_key = functional.l2_normalize(query, axis=-1, epsilon=1e-6), functional.l2_normalize(key, axis=-2, epsilon=1e-6)
+    attn = functional.matmul(norm_query, norm_key)  # [batch, num_heads, key_dim, key_dim]
+    attn = ChannelAffine(axis=1, use_bias=False, name=name and name + "temperature/no_weight_decay")(attn)  # axis=1 means on head dimension
+    attention_scores = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
+
+    if attn_dropout > 0:
+        attention_scores = layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
+    # [batch, num_heads, key_dim, key_dim] * [batch, num_heads, key_dim, hh * ww] -> [batch, num_heads, key_dim, hh * ww]
+    attention_output = functional.matmul(attention_scores, value)
+    attention_output = functional.transpose(attention_output, [0, 3, 1, 2])  # [batch, hh * ww, num_heads, key_dim]
+    attention_output = functional.reshape(attention_output, [-1, *input_blocks, num_heads * key_dim])  # [batch, hh, ww, num_heads * key_dim]
     # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
 
-    if should_pad_hh or should_pad_ww:
-        attention_output = attention_output[:, : hh - should_pad_hh, : ww - should_pad_ww, :]
-
-    if out_weight:
-        # [batch, hh, ww, num_heads * key_dim] * [num_heads * key_dim, out] --> [batch, hh, ww, out]
-        attention_output = keras.layers.Dense(out_shape, use_bias=out_bias, name=name and name + "output")(attention_output)
-    attention_output = keras.layers.Dropout(output_dropout, name=name and name + "out_drop")(attention_output) if output_dropout > 0 else attention_output
+    # [batch, hh, ww, num_heads * key_dim] * [num_heads * key_dim, out] --> [batch, hh, ww, out]
+    attention_output = layers.Dense(qk_out, use_bias=out_bias, name=name and name + "output")(attention_output)
+    attention_output = layers.Dropout(out_dropout, name=name and name + "out_drop")(attention_output) if out_dropout > 0 else attention_output
     return attention_output
 
 
-def nat_block(inputs, attn_kernel_size=7, num_heads=4, mlp_ratio=4, mlp_drop_rate=0, attn_drop_rate=0, drop_rate=0, layer_scale=-1, name=None):
-    input_channel = inputs.shape[-1]
-
-    attn = layer_norm(inputs, name=name + "attn_")
-    attn = neighborhood_attention(attn, attn_kernel_size, num_heads, attn_dropout=attn_drop_rate, name=name + "attn_")
-    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "1_gamma")(attn) if layer_scale >= 0 else attn
-    attn = drop_block(attn, drop_rate=drop_rate, name=name + "attn_")
-    attn_out = keras.layers.Add(name=name + "attn_out")([inputs, attn])
-
-    mlp = layer_norm(attn_out, name=name + "mlp_")
-    mlp = mlp_block(mlp, int(input_channel * mlp_ratio), drop_rate=mlp_drop_rate, use_conv=False, activation="gelu", name=name + "mlp_")
-    mlp = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "2_gamma")(mlp) if layer_scale >= 0 else mlp
-    mlp = drop_block(mlp, drop_rate=drop_rate, name=name + "mlp_")
-    return keras.layers.Add(name=name + "output")([attn_out, mlp])
-
-
-def NAT(
-    num_blocks=[3, 4, 6, 5],
-    out_channels=[64, 128, 256, 512],
-    num_heads=[2, 4, 8, 16],
+def split_depthwise_transpose_attention(
+    inputs, split=1, num_heads=4, mlp_ratio=4, use_pos_emb=False, layer_scale=1e-6, drop_rate=0, activation="gelu", name=""
+):
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    input_channel = inputs.shape[channel_axis]
+    sub_channels = int(math.ceil(input_channel / split))
+
+    if image_data_format() == "channels_last":
+        spx, remainder = inputs[:, :, :, : (split - 1) * sub_channels], inputs[:, :, :, (split - 1) * sub_channels :]
+    else:
+        spx, remainder = inputs[:, : (split - 1) * sub_channels], inputs[:, (split - 1) * sub_channels :]
+    spx = functional.split(spx, split - 1, axis=channel_axis)
+    gathered_result = []
+    for id, ii in enumerate(spx):
+        sp = ii if id == 0 else (sp + ii)
+        sp = depthwise_conv2d_no_bias(sp, kernel_size=3, padding="same", use_bias=True, name=name + "spx_{}_".format(id + 1))
+        gathered_result.append(sp)
+    gathered_result.append(remainder)
+    attn = functional.concat(gathered_result, axis=channel_axis)
+    # print(f"{inputs.shape = }, {attn.shape = }")
+
+    # XCA
+    attn = attn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(attn)  # channels_first -> channels_last
+    attn = PositionalEncodingFourier(name=name + "pos")(attn) if use_pos_emb else attn
+
+    attn_height, attn_width = attn.shape[1:-1]
+    attn = functional.reshape(attn, [-1, attn_height * attn_width, attn.shape[-1]])  # Using 3D for attention inputs
+
+    nn = layer_norm(attn, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "xca_")
+    nn = cross_covariance_attention(nn, num_heads, name=name + "xca_")
+    nn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, axis=-1, name=name + "xca_gamma")(nn) if layer_scale >= 0 else nn
+    nn = drop_block(nn, drop_rate=drop_rate, name=name + "xca_")
+    nn = layers.Add(name=name + "xca")([attn, nn])
+
+    # Inverted Bottleneck
+    nn = norm_inverted_bottleneck(nn, mlp_ratio, layer_scale, drop_rate, activation=activation, name=name + "ir_")
+    nn = functional.reshape(nn, [-1, attn_height, attn_width, nn.shape[-1]])  # Revert 3D to 4D
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_last -> channels_first
+    return layers.Add(name=name + "output")([inputs, nn])
+
+
+def conv_encoder(inputs, mlp_ratio=4, kernel_size=7, layer_scale=1e-6, drop_rate=0, activation="gelu", name=""):
+    nn = depthwise_conv2d_no_bias(inputs, kernel_size, use_bias=True, padding="same", name=name)
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)  # channels_first -> channels_last
+    nn = norm_inverted_bottleneck(nn, mlp_ratio, layer_scale, drop_rate, activation=activation, name=name)
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_last -> channels_first
+    # print(f"{nn.shape = }, {inputs.shape = }")
+    return layers.Add(name=name + "output")([inputs, nn])
+
+
+def EdgeNeXt(
+    num_blocks=[2, 2, 6, 2],
+    out_channels=[24, 48, 88, 168],
+    num_heads=4,
+    num_stda_layers=[0, 1, 1, 1],
+    stda_split=[2, 2, 3, 4],
+    stda_use_pos_emb=[False, True, False, False],
+    conv_kernel_size=[3, 5, 7, 9],
     stem_width=-1,
-    attn_kernel_size=7,
-    mlp_ratio=3,
-    layer_scale=-1,
+    mlp_ratio=4,
+    stem_patch_size=4,
+    layer_scale=1e-6,
     input_shape=(224, 224, 3),
     num_classes=1000,
+    activation="gelu",
     drop_connect_rate=0,
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
-    model_name="nat",
+    model_name="edgenext",
     kwargs=None,
 ):
-    """ConvTokenizer stem"""
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
     stem_width = stem_width if stem_width > 0 else out_channels[0]
-    nn = conv2d_no_bias(inputs, stem_width // 2, kernel_size=3, strides=2, use_bias=True, padding="SAME", name="stem_1_")
-    nn = conv2d_no_bias(nn, stem_width, kernel_size=3, strides=2, use_bias=True, padding="SAME", name="stem_2_")
-    nn = layer_norm(nn, name="stem_")
+    nn = conv2d_no_bias(inputs, stem_width, kernel_size=stem_patch_size, strides=stem_patch_size, use_bias=True, padding="valid", name="stem_")
+    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="stem_")
 
     """ stages """
     total_blocks = sum(num_blocks)
     global_block_id = 0
-    for stack_id, (num_block, out_channel, num_head) in enumerate(zip(num_blocks, out_channels, num_heads)):
+    for stack_id, (num_block, out_channel, num_stda_layer) in enumerate(zip(num_blocks, out_channels, num_stda_layers)):
         stack_name = "stack{}_".format(stack_id + 1)
         if stack_id > 0:
             ds_name = stack_name + "downsample_"
-            nn = conv2d_no_bias(nn, out_channel, kernel_size=3, strides=2, padding="SAME", name=ds_name)
-            nn = layer_norm(nn, name=ds_name)
+            nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name=ds_name)
+            # Set use_torch_padding=False, as kernel_size == 2, otherwise shape will be enlarged by 1
+            nn = conv2d_no_bias(nn, out_channel, kernel_size=2, strides=2, use_bias=True, padding="valid", name=ds_name)
         for block_id in range(num_block):
             block_name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
-            nn = nat_block(nn, attn_kernel_size, num_head, mlp_ratio, drop_rate=block_drop_rate, layer_scale=layer_scale, name=block_name)
+            if block_id > num_block - num_stda_layer - 1:
+                split = stda_split[stack_id]
+                use_pos_emb = stda_use_pos_emb[stack_id]
+                num_head = num_heads[stack_id] if isinstance(num_heads, (list, tuple)) else num_heads
+                nn = split_depthwise_transpose_attention(
+                    nn, split, num_head, mlp_ratio, use_pos_emb, layer_scale, block_drop_rate, activation, name=block_name + "stda_"
+                )
+            else:
+                kernel_size = conv_kernel_size[stack_id]
+                nn = conv_encoder(nn, mlp_ratio, kernel_size, layer_scale, block_drop_rate, activation=activation, name=block_name + "conv_")
             global_block_id += 1
-    nn = layer_norm(nn, name="pre_output_")
 
-    nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
-    model = keras.models.Model(inputs, nn, name=model_name)
+    """ output """
+    if num_classes > 0:
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="pre_output_")
+        if dropout > 0:
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
-    reload_model_weights(model, PRETRAINED_DICT, "nat", pretrained)
+    reload_model_weights(model, PRETRAINED_DICT, "edgenext", pretrained)
     return model
 
 
-def NAT_Mini(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 4, 6, 5]
-    return NAT(**locals(), model_name="nat_mini", **kwargs)
-
-
-def NAT_Tiny(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 4, 18, 5]
-    return NAT(**locals(), model_name="nat_tiny", **kwargs)
-
-
-def NAT_Small(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 4, 18, 5]
-    num_heads = [3, 6, 12, 24]
-    out_channels = [96, 192, 384, 768]
-    mlp_ratio = kwargs.pop("mlp_ratio", 2)
-    layer_scale = kwargs.pop("layer_scale", 1e-5)
-    return NAT(**locals(), model_name="nat_small", **kwargs)
-
-
-def NAT_Base(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
-    num_blocks = [3, 4, 18, 5]
-    num_heads = [4, 8, 16, 32]
-    out_channels = [128, 256, 512, 1024]
-    mlp_ratio = kwargs.pop("mlp_ratio", 2)
-    layer_scale = kwargs.pop("layer_scale", 1e-5)
-    return NAT(**locals(), model_name="nat_base", **kwargs)
+@register_model
+def EdgeNeXt_XX_Small(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    return EdgeNeXt(**locals(), model_name="edgenext_xx_small", **kwargs)
+
+
+@register_model
+def EdgeNeXt_X_Small(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [3, 3, 9, 3]
+    out_channels = [32, 64, 100, 192]
+    return EdgeNeXt(**locals(), model_name="edgenext_x_small", **kwargs)
+
+
+@register_model
+def EdgeNeXt_Small(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [3, 3, 9, 3]
+    out_channels = [48, 96, 160, 304]
+    num_heads = 8
+    return EdgeNeXt(**locals(), model_name="edgenext_small", **kwargs)
+
+
+@register_model
+def EdgeNeXt_Base(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    num_blocks = [3, 3, 9, 3]
+    out_channels = [80, 160, 288, 584]
+    num_heads = 8
+    return EdgeNeXt(**locals(), model_name="edgenext_base", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/nfnets/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/nfnets/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/nfnets/nfnets.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/nfnets/nfnets.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, initializers, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
 from keras_cv_attention_models.attention_layers import activation_by_name, drop_block, eca_module, se_module, make_divisible, add_pre_post_process
 
 PRETRAINED_DICT = {
     "nfnetf0": {"imagenet": "7f8ee8639d468597de41566ce1b481c7"},
     "nfnetf1": {"imagenet": "f5d298e50996f0a11a8b097e0f890fa2"},
     "nfnetf2": {"imagenet": "3b0f5d6ac33a2833d7d9ee0e02aae4bc"},
@@ -14,15 +15,15 @@
     "nfnetf6": {"imagenet": "ee4a06b4a543531d72ea5a8a101336ac"},
     "nfnetl0": {"imagenet": "6bd4d11037bf720506aa3d3e12ec4f53"},
     "eca_nfnetl0": {"imagenet": "7789af74226ffee28a0a68cdca6f3737"},
     "eca_nfnetl1": {"imagenet": "cd17a98175825258d32229bc82b744fd"},
     "eca_nfnetl2": {"imagenet": "ca7e0bba4f2d1945d881ffc6e36bed36"},
 }
 
-CONV_KERNEL_INITIALIZER = tf.keras.initializers.VarianceScaling(scale=2.0, mode="fan_out", distribution="truncated_normal")
+
 NON_LINEAR_GAMMA = dict(
     identity=1.0,
     celu=1.270926833152771,
     elu=1.2716004848480225,
     gelu=1.7015043497085571,
     leaky_relu=1.70590341091156,
     log_sigmoid=1.9193484783172607,
@@ -34,16 +35,16 @@
     swish=1.7881293296813965,  # silu
     softsign=2.338853120803833,
     softplus=1.9203323125839233,
     tanh=1.5939117670059204,
 )
 
 
-@tf.keras.utils.register_keras_serializable(package="nfnets")
-class ScaledStandardizedConv2D(tf.keras.layers.Conv2D):
+@backend.register_keras_serializable(package="nfnets")
+class ScaledStandardizedConv2D(layers.Conv2D):
     """
     Copied from https://github.com/google-research/big_transfer/blob/master/bit_tf2/models.py, Author: Lucas Beyer
     Modified reference: https://github.com/deepmind/deepmind-research/blob/master/nfnets/base.py#L121
     """
 
     def __init__(self, gamma=1.0, eps=1e-5, *args, **kwargs):
         super(ScaledStandardizedConv2D, self).__init__(*args, **kwargs)
@@ -52,74 +53,74 @@
     def build(self, input_shape):
         super(ScaledStandardizedConv2D, self).build(input_shape)
         # Wrap a standardization around the conv OP.
         if hasattr(self, "_convolution_op"):
             default_conv_op = self._convolution_op  # TF < 2.7.0
         else:
             default_conv_op = self.convolution_op  # TF 2.7.0
-        self.gain = self.add_weight(name="gain", shape=(self.filters,), initializer="ones", trainable=True, dtype=self.dtype)
-        self.fan_in = tf.cast(tf.reduce_prod(self.kernel.shape[:-1]), self._compute_dtype)
-        self.__eps__ = tf.cast(self.eps, self._compute_dtype)
-        self.__gamma__ = tf.cast(self.gamma, self._compute_dtype)
+        self.gain = self.add_weight(name="gain", shape=(self.filters,), initializer="ones", trainable=True)
+        self.fan_in = float(np.prod(self.kernel.shape[:-1]))
+        self.__eps__ = float(self.eps)
+        self.__gamma__ = float(self.gamma)
 
         def standardized_conv_op(inputs, kernel):
             # Kernel has shape HWIO, normalize over HWI
-            mean, var = tf.nn.moments(kernel, axes=[0, 1, 2], keepdims=True)
+            mean, var = functional.moments(kernel, axes=[0, 1, 2], keepdims=True)
             # Manually fused normalization, eq. to (w - mean) * gain / sqrt(N * var)
             # print(">>>>", mean.dtype, var.dtype, self.fan_in.dtype, self.__eps__.dtype, self.__gamma__.dtype, self.gain.dtype)
-            scale = tf.math.rsqrt(tf.math.maximum(var * self.fan_in, self.__eps__)) * (self.gain * self.__gamma__)
+            scale = functional.rsqrt(functional.maximum(var * self.fan_in, self.__eps__)) * (self.gain * self.__gamma__)
             return default_conv_op(inputs, (kernel - mean) * scale)
 
         if hasattr(self, "_convolution_op"):
             self._convolution_op = standardized_conv_op  # TF < 2.7.0
         else:
             self.convolution_op = standardized_conv_op  # TF 2.7.0
         self.built = True
 
     def get_config(self):
         base_config = super(ScaledStandardizedConv2D, self).get_config()
         base_config.update({"eps": self.eps, "gamma": self.gamma})
         return base_config
 
 
-@tf.keras.utils.register_keras_serializable(package="nfnets")
-class ZeroInitGain(tf.keras.layers.Layer):
+@backend.register_keras_serializable(package="nfnets")
+class ZeroInitGain(layers.Layer):
     def __init__(self, use_bias=False, weight_init_value=0, bias_init_value=0, **kwargs):
         super().__init__(**kwargs)
         self.use_bias = use_bias
-        self.ww_init = keras.initializers.Constant(weight_init_value) if weight_init_value != 0 else "zeros"
-        self.bb_init = keras.initializers.Constant(bias_init_value) if bias_init_value != 0 else "zeros"
+        self.ww_init = initializers.Constant(weight_init_value) if weight_init_value != 0 else "zeros"
+        self.bb_init = initializers.Constant(bias_init_value) if bias_init_value != 0 else "zeros"
 
     def build(self, input_shape):
-        self.gain = self.add_weight(name="gain", shape=(), initializer=self.ww_init, dtype=self.dtype, trainable=True)
+        self.gain = self.add_weight(name="gain", shape=(), initializer=self.ww_init, trainable=True)
         if self.use_bias:
-            self.bias = self.add_weight(name="bias", shape=(), initializer=self.bb_init, dtype=self.dtype, trainable=True)
+            self.bias = self.add_weight(name="bias", shape=(), initializer=self.bb_init, trainable=True)
+        super().build(input_shape)
 
     def call(self, inputs):
         return (inputs * self.gain + self.bias) if self.use_bias else (inputs * self.gain)
 
     def get_config(self):
         base_config = super().get_config()
         base_config.update({"use_bias": self.use_bias})
         return base_config
 
 
-def std_conv2d_with_init(inputs, filters, kernel_size, strides=1, padding="VALID", torch_padding=False, gamma=1.0, name=None, **kwargs):
+def std_conv2d_with_init(inputs, filters, kernel_size, strides=1, padding="valid", torch_padding=False, gamma=1.0, name=None, **kwargs):
     pad = max(kernel_size) // 2 if isinstance(kernel_size, (list, tuple)) else kernel_size // 2
-    if torch_padding and padding.upper() == "SAME" and pad != 0:
-        inputs = keras.layers.ZeroPadding2D(padding=pad, name=name and name + "pad")(inputs)
-        padding = "VALID"
+    if torch_padding and padding.lower() == "same" and pad != 0:
+        inputs = layers.ZeroPadding2D(padding=pad, name=name and name + "pad")(inputs)
+        padding = "valid"
 
     return ScaledStandardizedConv2D(
         filters=filters,
         kernel_size=kernel_size,
         strides=strides,
         padding=padding,
         gamma=gamma,
-        kernel_initializer=CONV_KERNEL_INITIALIZER,
         name=name and name + "conv",
         **kwargs,
     )(inputs)
 
 
 def activation_by_name_with_gamma(inputs, activation="gelu", gamma=1.0, name=None):
     nn = activation_by_name(inputs, activation=activation, name=name)
@@ -147,23 +148,23 @@
     hidden_filter = int(filters * channel_ratio)
     attn_gain = 2.0
     # print(f">>>> {beta = }")
     preact = activation_by_name_with_gamma(inputs, activation, gamma=act_gamma, name=name + "preact_") * beta
 
     if strides > 1 or inputs.shape[-1] != filters:
         if strides > 1:
-            shortcut = keras.layers.AvgPool2D(strides, strides=strides, padding="SAME", name=name + "shorcut_down")(preact)
+            shortcut = layers.AvgPool2D(strides, strides=strides, padding="same", name=name + "shorcut_down")(preact)
         else:
             shortcut = preact
         shortcut = std_conv2d_with_init(shortcut, filters, 1, strides=1, gamma=conv_gamma, name=name + "shortcut_")
     else:
         shortcut = inputs
 
     groups = hidden_filter // group_size
-    conv_params_3 = {"kernel_size": 3, "padding": "SAME", "torch_padding": torch_padding, "gamma": conv_gamma}
+    conv_params_3 = {"kernel_size": 3, "padding": "same", "torch_padding": torch_padding, "gamma": conv_gamma}
     deep = std_conv2d_with_init(preact, hidden_filter, 1, strides=1, gamma=conv_gamma, name=name + "deep_1_")
     deep = activation_by_name_with_gamma(deep, activation, gamma=act_gamma, name=name + "deep_1_")
     deep = std_conv2d_with_init(deep, hidden_filter, strides=strides, **conv_params_3, groups=groups, name=name + "deep_2_")
     deep = activation_by_name_with_gamma(deep, activation, gamma=act_gamma, name=name + "deep_2_")
     deep = std_conv2d_with_init(deep, hidden_filter, strides=1, **conv_params_3, groups=groups, name=name + "deep_3_")  # Extra conv
     deep = activation_by_name_with_gamma(deep, activation, gamma=act_gamma, name=name + "deep_3_")
     deep = std_conv2d_with_init(deep, filters, 1, strides=1, gamma=conv_gamma, name=name + "deep_4_")
@@ -174,30 +175,30 @@
         deep = eca_module(deep, name=name + "eca_")
         deep *= attn_gain
 
     deep = drop_block(deep, drop_rate)
     if use_zero_init_gain:
         deep = ZeroInitGain(name=name + "deep_gain")(deep)
     deep *= alpha
-    return keras.layers.Add(name=name + "output")([shortcut, deep])
+    return layers.Add(name=name + "output")([shortcut, deep])
 
 
 def stack(inputs, blocks, filters, betas=1.0, strides=2, stack_drop=0, block_params={}, name=""):
     nn = inputs
     for id in range(blocks):
         cur_strides = strides if id == 0 else 1
         block_name = name + "block{}_".format(id + 1)
         block_drop_rate = stack_drop[id] if isinstance(stack_drop, (list, tuple)) else stack_drop
         beta = betas[id] if isinstance(stack_drop, (list, tuple)) else betas
         nn = block(nn, filters, beta, cur_strides, block_drop_rate, name=block_name, **block_params)
     return nn
 
 
 def stem(inputs, stem_width, activation="gelu", torch_padding=False, conv_gamma=1.0, act_gamma=1.0, name=""):
-    conv_params = {"kernel_size": 3, "padding": "SAME", "torch_padding": torch_padding, "gamma": conv_gamma}
+    conv_params = {"kernel_size": 3, "padding": "same", "torch_padding": torch_padding, "gamma": conv_gamma}
     nn = std_conv2d_with_init(inputs, stem_width // 8, strides=2, **conv_params, name=name + "1_")
     nn = activation_by_name_with_gamma(nn, activation, gamma=act_gamma, name=name + "1_")
     nn = std_conv2d_with_init(nn, stem_width // 4, strides=1, **conv_params, name=name + "2_")
     nn = activation_by_name_with_gamma(nn, activation, gamma=act_gamma, name=name + "2_")
     nn = std_conv2d_with_init(nn, stem_width // 2, strides=1, **conv_params, name=name + "3_")
     nn = activation_by_name_with_gamma(nn, activation, gamma=act_gamma, name=name + "3_")
     nn = std_conv2d_with_init(nn, stem_width, strides=2, **conv_params, name=name + "4_")
@@ -231,15 +232,18 @@
 ):
     if gamma_in_act:
         # activation.split("/")[0] for supporting `gelu/app`
         conv_gamma, act_gamma = 1.0, NON_LINEAR_GAMMA.get(activation.split("/")[0], 1.0)
     else:
         act_gamma, conv_gamma = 1.0, NON_LINEAR_GAMMA.get(activation.split("/")[0], 1.0)
 
-    inputs = keras.layers.Input(shape=input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(shape=input_shape)
     stem_width = make_divisible(stem_width * width_factor, 8)
     nn = stem(inputs, stem_width, activation=activation, torch_padding=torch_padding, conv_gamma=conv_gamma, act_gamma=act_gamma, name="stem_")
 
     block_params = {  # params same for all blocks
         "alpha": alpha,
         "channel_ratio": channel_ratio,
         "se_ratio": se_ratio,
@@ -248,15 +252,15 @@
         "torch_padding": torch_padding,
         "attn_type": attn_type,
         "conv_gamma": conv_gamma,
         "act_gamma": act_gamma,
         "activation": activation,
     }
 
-    drop_connect_rates = tf.split(tf.linspace(0.0, drop_connect_rate, sum(num_blocks)), num_blocks)
+    drop_connect_rates = functional.split(functional.linspace(0.0, drop_connect_rate, sum(num_blocks)), num_blocks)
     drop_connect_rates = [ii.numpy().tolist() for ii in drop_connect_rates]
     beta_list = [(1 + alpha**2 * ii) ** -0.5 for ii in range(max(num_blocks) + 1)]
     pre_beta = 1.0
     for id, (num_block, out_channel, stride, drop_connect) in enumerate(zip(num_blocks, out_channels, strides, drop_connect_rates)):
         name = "stack{}_".format(id + 1)
         out_channel = make_divisible(out_channel * width_factor, 8)
         betas = beta_list[: num_block + 1]
@@ -266,85 +270,98 @@
 
     if num_features_factor > 0:
         output_conv_filter = make_divisible(num_features_factor * out_channels[-1] * width_factor, 8)
         nn = std_conv2d_with_init(nn, output_conv_filter, 1, gamma=conv_gamma, name="post_")
     nn = activation_by_name_with_gamma(nn, activation, gamma=act_gamma, name="post_")
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
         if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="nfnets", pretrained=pretrained)
     return model
 
 
+@register_model
 def NFNetF0(input_shape=(256, 256, 3), num_classes=1000, activation="gelu", dropout=0.2, pretrained="imagenet", **kwargs):
     return NormFreeNet(num_blocks=[1, 2, 6, 3], model_name="nfnetf0", **locals(), **kwargs)
 
 
+@register_model
 def NFNetF1(input_shape=(320, 320, 3), num_classes=1000, activation="gelu", dropout=0.3, pretrained="imagenet", **kwargs):
     return NormFreeNet(num_blocks=[2, 4, 12, 6], model_name="nfnetf1", **locals(), **kwargs)
 
 
+@register_model
 def NFNetF2(input_shape=(352, 352, 3), num_classes=1000, activation="gelu", dropout=0.4, pretrained="imagenet", **kwargs):
     return NormFreeNet(num_blocks=[3, 6, 18, 9], model_name="nfnetf2", **locals(), **kwargs)
 
 
+@register_model
 def NFNetF3(input_shape=(416, 416, 3), num_classes=1000, activation="gelu", dropout=0.4, pretrained="imagenet", **kwargs):
     return NormFreeNet(num_blocks=[4, 8, 24, 12], model_name="nfnetf3", **locals(), **kwargs)
 
 
+@register_model
 def NFNetF4(input_shape=(512, 512, 3), num_classes=1000, activation="gelu", dropout=0.5, pretrained="imagenet", **kwargs):
     return NormFreeNet(num_blocks=[5, 10, 30, 15], model_name="nfnetf4", **locals(), **kwargs)
 
 
+@register_model
 def NFNetF5(input_shape=(544, 544, 3), num_classes=1000, activation="gelu", dropout=0.5, pretrained="imagenet", **kwargs):
     return NormFreeNet(num_blocks=[6, 12, 36, 18], model_name="nfnetf5", **locals(), **kwargs)
 
 
+@register_model
 def NFNetF6(input_shape=(576, 576, 3), num_classes=1000, activation="gelu", dropout=0.5, pretrained="imagenet", **kwargs):
     return NormFreeNet(num_blocks=[7, 14, 42, 21], model_name="nfnetf6", **locals(), **kwargs)
 
 
+@register_model
 def NFNetF7(input_shape=(608, 608, 3), num_classes=1000, activation="gelu", dropout=0.5, pretrained=None, **kwargs):
     return NormFreeNet(num_blocks=[8, 16, 48, 24], model_name="nfnetf7", **locals(), **kwargs)
 
 
 def NormFreeNet_Light(channel_ratio=0.25, group_size=64, torch_padding=True, use_zero_init_gain=False, gamma_in_act=False, **kwargs):
     kwargs.pop("kwargs", None)
     return NormFreeNet(**locals(), **kwargs)
 
 
+@register_model
 def NFNetL0(input_shape=(288, 288, 3), num_classes=1000, activation="swish", dropout=0.2, pretrained="imagenet", **kwargs):
     num_blocks = [1, 2, 6, 3]
     num_features_factor = 1.5
     se_ratio = 0.25
     return NormFreeNet_Light(model_name="nfnetl0", **locals(), **kwargs)
 
 
+@register_model
 def ECA_NFNetL0(input_shape=(288, 288, 3), num_classes=1000, activation="swish", dropout=0.2, pretrained="imagenet", **kwargs):
     num_blocks = [1, 2, 6, 3]
     num_features_factor = 1.5
     attn_type = "eca"
     return NormFreeNet_Light(model_name="eca_nfnetl0", **locals(), **kwargs)
 
 
+@register_model
 def ECA_NFNetL1(input_shape=(320, 320, 3), num_classes=1000, activation="swish", dropout=0.2, pretrained="imagenet", **kwargs):
     num_blocks = [2, 4, 12, 6]
     attn_type = "eca"
     return NormFreeNet_Light(model_name="eca_nfnetl1", **locals(), **kwargs)
 
 
+@register_model
 def ECA_NFNetL2(input_shape=(384, 384, 3), num_classes=1000, activation="swish", dropout=0.2, pretrained="imagenet", **kwargs):
     num_blocks = [3, 6, 18, 9]
     attn_type = "eca"
     return NormFreeNet_Light(model_name="eca_nfnetl2", **locals(), **kwargs)
 
 
+@register_model
 def ECA_NFNetL3(input_shape=(448, 448, 3), num_classes=1000, activation="swish", dropout=0.2, pretrained=None, **kwargs):
     num_blocks = [4, 8, 24, 12]
     attn_type = "eca"
     return NormFreeNet_Light(model_name="eca_nfnetl3", **locals(), **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/pvt/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/pvt/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/pvt/pvt.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/pvt/pvt.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,10 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     add_with_layer_scale_and_drop_block,
     addaptive_pooling_2d,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
     layer_norm,
@@ -23,75 +24,86 @@
     "pvt_v2_b3": {"imagenet": "97d0e12d53898b0f2c66efb30a4b9de8"},
     "pvt_v2_b4": {"imagenet": "166f6323c6fe7a578970efc9cc8e5b17"},
     "pvt_v2_b5": {"imagenet": "43cf0deec87e4d7c25f3f7d57d49b917"},
 }
 
 
 def attention_block_with_conv_down(
-    inputs, num_heads=4, key_dim=0, sr_ratio=1, qkv_bias=True, out_weight=True, out_bias=True, use_linear=False, linear_activation="gelu", dropout=0, name=""
+    inputs, num_heads=4, key_dim=0, sr_ratio=1, qkv_bias=True, out_bias=True, use_linear=False, linear_activation="gelu", dropout=0, name=""
 ):
     _, hh, ww, input_channel = inputs.shape
     key_dim = key_dim if key_dim > 0 else input_channel // num_heads
     # out_shape = input_channel if out_shape is None or not out_weight else out_shape
     emb_dim = num_heads * key_dim
 
-    query = keras.layers.Dense(emb_dim, use_bias=qkv_bias, name=name and name + "query")(inputs)
+    query = layers.Dense(emb_dim, use_bias=qkv_bias, name=name and name + "query")(inputs)
     # print(f">>>> {inputs.shape = }, {query.shape = }, {sr_ratio = }")
-    query = tf.transpose(tf.reshape(query, [-1, inputs.shape[1] * inputs.shape[2], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
+    # [batch, num_heads, hh * ww, key_dim]
+    query = functional.transpose(functional.reshape(query, [-1, inputs.shape[1] * inputs.shape[2], num_heads, key_dim]), [0, 2, 1, 3])
 
     if use_linear:
-        key_value = addaptive_pooling_2d(inputs, output_size=7, reduce="mean")
+        key_value = inputs if image_data_format() == "channels_last" else layers.Permute([3, 1, 2], name=name + "permute_pre")(inputs)
+        key_value = addaptive_pooling_2d(key_value, output_size=7, reduce="mean")
         key_value = conv2d_no_bias(key_value, input_channel, kernel_size=1, use_bias=qkv_bias, name=name + "kv_sr_")
-        key_value = layer_norm(key_value, name=name + "kv_sr_")  # Using epsilon=1e-5
+        key_value = key_value if image_data_format() == "channels_last" else layers.Permute([2, 3, 1], name=name + "permute_post")(key_value)
+        key_value = layer_norm(key_value, axis=-1, name=name + "kv_sr_")  # Using epsilon=1e-5
         key_value = activation_by_name(key_value, activation=linear_activation, name=name + "kv_sr_")
     elif sr_ratio > 1:
-        key_value = conv2d_no_bias(inputs, input_channel, kernel_size=sr_ratio, strides=sr_ratio, use_bias=qkv_bias, name=name + "kv_sr_")
-        key_value = layer_norm(key_value, name=name + "kv_sr_")  # Using epsilon=1e-5
-        # key_value = keras.layers.AvgPool2D(sr_ratio, strides=sr_ratio, name=name + "kv_sr_")(inputs)
+        key_value = inputs if image_data_format() == "channels_last" else layers.Permute([3, 1, 2], name=name + "permute_pre")(inputs)
+        key_value = conv2d_no_bias(key_value, input_channel, kernel_size=sr_ratio, strides=sr_ratio, use_bias=qkv_bias, name=name + "kv_sr_")
+        key_value = key_value if image_data_format() == "channels_last" else layers.Permute([2, 3, 1], name=name + "permute_post")(key_value)
+        key_value = layer_norm(key_value, axis=-1, name=name + "kv_sr_")  # Using epsilon=1e-5
+        # key_value = layers.AvgPool2D(sr_ratio, strides=sr_ratio, name=name + "kv_sr_")(inputs)
     else:
         key_value = inputs
     _, kv_hh, kv_ww, _ = key_value.shape
     # key_value = [batch, num_heads, hh, ww, kv_kernel * kv_kernel, key_dim * 2]
-    key_value = keras.layers.Dense(emb_dim * 2, use_bias=qkv_bias, name=name and name + "key_value")(key_value)
-    key, value = tf.split(key_value, 2, axis=-1)
-    key = tf.transpose(tf.reshape(key, [-1, kv_hh * kv_ww, num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-    value = tf.transpose(tf.reshape(value, [-1, kv_hh * kv_ww, num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
+    key_value = layers.Dense(emb_dim * 2, use_bias=qkv_bias, name=name and name + "key_value")(key_value)
+    key, value = functional.split(key_value, 2, axis=-1)
+    key = functional.transpose(functional.reshape(key, [-1, kv_hh * kv_ww, num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
+    value = functional.transpose(functional.reshape(value, [-1, kv_hh * kv_ww, num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
 
     output_shape = (hh, ww, input_channel)
-    return scaled_dot_product_attention(query, key, value, output_shape=output_shape, out_weight=out_weight, out_bias=out_bias, dropout=dropout, name=name)
+    output = scaled_dot_product_attention(query, key, value, output_shape=output_shape, out_weight=False, dropout=dropout, name=name)
+    return layers.Dense(input_channel, use_bias=out_bias, name=name and name + "out")(output)
 
 
 def mlp_block_with_depthwise_conv(inputs, hidden_dim, kernel_size=3, use_bias=True, drop_rate=0, activation="gelu", name=""):
     input_channel = inputs.shape[-1]
     first_activation, middle_activation = activation if isinstance(activation, (list, tuple)) else (activation, activation)
-    nn = keras.layers.Dense(hidden_dim, use_bias=use_bias, name=name and name + "1_dense")(inputs)
+    nn = layers.Dense(hidden_dim, use_bias=use_bias, name=name and name + "1_dense")(inputs)
     nn = activation_by_name(nn, first_activation, name=name)
 
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2], name=name + "permute_pre")(nn)
     nn = depthwise_conv2d_no_bias(nn, use_bias=use_bias, kernel_size=kernel_size, strides=1, padding="same", name=name and name + "mid_")
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1], name=name + "permute_post")(nn)
     nn = activation_by_name(nn, middle_activation, name=name and name + "mid_")
-    nn = keras.layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
+    nn = layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
 
-    nn = keras.layers.Dense(input_channel, use_bias=use_bias, name=name and name + "2_dense")(nn)
-    nn = keras.layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
+    nn = layers.Dense(input_channel, use_bias=use_bias, name=name and name + "2_dense")(nn)
+    nn = layers.Dropout(drop_rate)(nn) if drop_rate > 0 else nn
     return nn
 
 
 def attention_mlp_block(inputs, embed_dim, num_heads=8, sr_ratio=1, mlp_ratio=4, use_linear=False, layer_scale=0.1, drop_rate=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
+    channnel_axis = -1 if image_data_format() == "channels_last" else 1
+    input_channel = inputs.shape[channnel_axis]
 
     """ attention """
-    nn = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, name=name + "attn_")
+    pre = inputs if image_data_format() == "channels_last" else layers.Permute([2, 3, 1], name=name + "permute_pre")(inputs)
+    nn = layer_norm(pre, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "attn_")
     nn = attention_block_with_conv_down(nn, num_heads=num_heads, sr_ratio=sr_ratio, use_linear=use_linear, linear_activation=activation, name=name + "attn_")
-    attn_out = add_with_layer_scale_and_drop_block(inputs, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "attn_")
+    attn_out = add_with_layer_scale_and_drop_block(pre, nn, layer_scale=layer_scale, drop_rate=drop_rate, axis=-1, name=name + "attn_")
 
     """ MLP """
-    nn = layer_norm(attn_out, epsilon=LAYER_NORM_EPSILON, name=name + "mlp_")
+    nn = layer_norm(attn_out, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "mlp_")
     mlp_activation = ("relu", activation) if use_linear else (None, activation)
     nn = mlp_block_with_depthwise_conv(nn, input_channel * mlp_ratio, activation=mlp_activation, name=name + "mlp_")
-    nn = add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "mlp_")
+    nn = add_with_layer_scale_and_drop_block(attn_out, nn, layer_scale=layer_scale, drop_rate=drop_rate, axis=-1, name=name + "mlp_")
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2], name=name + "permute_post")(nn)  # channels_first -> channels_last
     return nn
 
 
 def PyramidVisionTransformerV2(
     num_blocks=[2, 2, 2, 2],
     embed_dims=[64, 128, 320, 512],
     num_heads=[1, 2, 5, 8],
@@ -106,15 +118,18 @@
     dropout=0,
     layer_scale=0,
     classifier_activation="softmax",
     pretrained=None,
     model_name="pvt",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
 
     """ Stem """
     nn = conv2d_no_bias(inputs, embed_dims[0], stem_patch_size, strides=4, padding="same", use_bias=True, name="stem_")
     nn = layer_norm(nn, name="stem_")  # Using epsilon=1e-5
 
     """ stacks """
     total_blocks = sum(num_blocks)
@@ -132,47 +147,54 @@
             name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             nn = attention_mlp_block(nn, embed_dim, stack_num_head, stack_sr_ratio, stack_mlp_ratio, use_linear, layer_scale, block_drop_rate, activation, name)
             global_block_id += 1
         nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name=name + "output_")
 
     nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
-    model = tf.keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "pvt", pretrained)
     return model
 
 
+@register_model
 def PVT_V2B0(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     embed_dims = [32, 64, 160, 256]
     return PyramidVisionTransformerV2(**locals(), model_name="pvt_v2_b0", **kwargs)
 
 
+@register_model
 def PVT_V2B1(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     return PyramidVisionTransformerV2(**locals(), model_name="pvt_v2_b1", **kwargs)
 
 
+@register_model
 def PVT_V2B2(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 6, 3]
     return PyramidVisionTransformerV2(**locals(), model_name="pvt_v2_b2", **kwargs)
 
 
+@register_model
 def PVT_V2B2_linear(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 6, 3]
     use_linear = True
     return PyramidVisionTransformerV2(**locals(), model_name="pvt_v2_b2_linear", **kwargs)
 
 
+@register_model
 def PVT_V2B3(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 18, 3]
     return PyramidVisionTransformerV2(**locals(), model_name="pvt_v2_b3", **kwargs)
 
 
+@register_model
 def PVT_V2B4(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 8, 27, 3]
     return PyramidVisionTransformerV2(**locals(), model_name="pvt_v2_b4", **kwargs)
 
 
+@register_model
 def PVT_V2B5(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 6, 40, 3]
     mlp_ratios = 4
     return PyramidVisionTransformerV2(**locals(), model_name="pvt_v2_b5", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnest/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnest/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnest/resnest.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnest/resnest.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,89 +1,109 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.aotnet import AotNet
 from keras_cv_attention_models.download_and_load import reload_model_weights
 from keras_cv_attention_models.attention_layers import batchnorm_with_activation, conv2d_no_bias
 
 PRETRAINED_DICT = {
     "resnest101": {"imagenet": "63f9ebdcd32529cbc4b4fbbec3d1bb2f"},
     "resnest200": {"imagenet": "8e211dcb089b588e18d36ba7cdf92ef0"},
     "resnest269": {"imagenet": "4309ed1b0a8ae92f2b1143dc3512c5c7"},
     "resnest50": {"imagenet": "eee7b20a229821f730ab205b6afeb369"},
 }
 
 
 def rsoftmax(inputs, groups):
     if groups > 1:
-        nn = tf.reshape(inputs, [-1, 1, groups, inputs.shape[-1] // groups])
+        nn = functional.reshape(inputs, [-1, groups, int(np.prod(inputs.shape[1:])) // groups])
         # nn = tf.transpose(nn, [0, 2, 1, 3])
-        nn = tf.nn.softmax(nn, axis=2)
-        nn = tf.reshape(nn, [-1, 1, 1, inputs.shape[-1]])
+        nn = functional.softmax(nn, axis=1)
+        nn = functional.reshape(nn, [-1, *inputs.shape[1:]])
     else:
-        nn = keras.layers.Activation("sigmoid")(inputs)
+        nn = layers.Activation("sigmoid")(inputs)
     return nn
 
 
 def split_attention_conv2d(inputs, filters, kernel_size=3, strides=1, downsample_first=False, groups=2, activation="relu", name=""):
-    h_axis, w_axis = [2, 3] if K.image_data_format() == "channels_first" else [1, 2]
-    in_channels = inputs.shape[-1]
+    h_axis, w_axis = [1, 2] if image_data_format() == "channels_last" else [2, 3]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    in_channels = inputs.shape[channel_axis]
     conv_strides = strides if downsample_first else 1
     if groups == 1:
         logits = conv2d_no_bias(inputs, filters, kernel_size, strides=conv_strides, padding="same", name=name and name + "1_")
     else:
         # Using groups=2 is slow in `mixed_float16` policy
         # logits = conv2d_no_bias(inputs, filters * groups, kernel_size, padding="same", groups=groups, name=name and name + "1_")
         logits = []
-        splitted_inputs = tf.split(inputs, groups, axis=-1)
+        splitted_inputs = functional.split(inputs, groups, axis=channel_axis)
         for ii in range(groups):
             conv_name = name and name + "1_g{}_".format(ii + 1)
             logits.append(conv2d_no_bias(splitted_inputs[ii], filters, kernel_size, strides=conv_strides, padding="same", name=conv_name))
-        logits = tf.concat(logits, axis=-1)
+        logits = functional.concat(logits, axis=channel_axis)
     logits = batchnorm_with_activation(logits, activation=activation, name=name and name + "1_")
 
     if groups > 1:
-        splited = tf.split(logits, groups, axis=-1)
-        gap = tf.reduce_sum(splited, axis=0)
+        splited = functional.split(logits, groups, axis=channel_axis)
+        gap = functional.reduce_sum(splited, axis=0)
     else:
         gap = logits
-    gap = tf.reduce_mean(gap, [h_axis, w_axis], keepdims=True)
+    gap = functional.reduce_mean(gap, [h_axis, w_axis], keepdims=True)
 
     reduction_factor = 4
     inter_channels = max(in_channels * groups // reduction_factor, 32)
-    atten = keras.layers.Conv2D(inter_channels, kernel_size=1, name=name and name + "2_conv")(gap)
+    atten = layers.Conv2D(inter_channels, kernel_size=1, name=name and name + "2_conv")(gap)
     atten = batchnorm_with_activation(atten, activation=activation, name=name and name + "2_")
-    atten = keras.layers.Conv2D(filters * groups, kernel_size=1, name=name and name + "3_conv")(atten)
+    atten = layers.Conv2D(filters * groups, kernel_size=1, name=name and name + "3_conv")(atten)
     atten = rsoftmax(atten, groups)
-    out = keras.layers.Multiply()([atten, logits])
+    out = layers.Multiply()([atten, logits])
 
     if groups > 1:
-        out = tf.split(out, groups, axis=-1)
-        out = tf.reduce_sum(out, axis=0)
+        out = functional.split(out, groups, axis=channel_axis)
+        out = functional.reduce_sum(out, axis=0)
 
     if not downsample_first and strides > 1:
-        out = keras.layers.ZeroPadding2D(padding=1, name=name and name + "pool_pad")(out)
-        out = keras.layers.AveragePooling2D(3, strides=2, name=name and name + "pool")(out)
+        out = layers.ZeroPadding2D(padding=1, name=name and name + "pool_pad")(out)
+        out = layers.AvgPool2D(3, strides=2, name=name and name + "pool")(out)
     return out
 
 
-def ResNest(input_shape=(224, 224, 3), stem_type="deep", attn_types="sa", bn_after_attn=False, shortcut_type="avg", pretrained="imagenet", **kwargs):
+def ResNest(
+    num_blocks=[3, 4, 6, 3],
+    stem_width=64,
+    stem_type="deep",
+    attn_types="sa",
+    bn_after_attn=False,
+    shortcut_type="avg",
+    input_shape=(224, 224, 3),
+    num_classes=1000,
+    activation="relu",
+    classifier_activation="softmax",
+    pretrained="imagenet",
+    model_name="",
+    **kwargs
+):
     kwargs.pop("kwargs", None)
     model = AotNet(**locals(), **kwargs)
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="resnest", pretrained=pretrained)
     return model
 
 
+@register_model
 def ResNest50(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", groups=2, **kwargs):
     return ResNest(num_blocks=[3, 4, 6, 3], stem_width=64, model_name="resnest50", **locals(), **kwargs)
 
 
+@register_model
 def ResNest101(input_shape=(256, 256, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", groups=2, **kwargs):
     return ResNest(num_blocks=[3, 4, 23, 3], stem_width=128, model_name="resnest101", **locals(), **kwargs)
 
 
+@register_model
 def ResNest200(input_shape=(320, 320, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", groups=2, **kwargs):
     return ResNest(num_blocks=[3, 24, 36, 3], stem_width=128, model_name="resnest200", **locals(), **kwargs)
 
 
+@register_model
 def ResNest269(input_shape=(416, 416, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", groups=2, **kwargs):
     return ResNest(num_blocks=[3, 30, 48, 8], stem_width=128, model_name="resnest269", **locals(), **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/regnet.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/regnet.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from keras_cv_attention_models.aotnet import AotNet
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
     "regnety_040": {"imagenet": "35bde7c2f9e391c20d4aa20160902961"},
     "regnety_064": {"imagenet": "1b096c91cdcf5b3ea82900468cda829b"},
     "regnety_080": {"imagenet": "651b4e500556593695258dee604eecba"},
     "regnety_160": {"imagenet": "7a039bfb1e006571991a19b5e3fbd0d1"},
@@ -32,62 +33,69 @@
     attn_params = {"se_divisor": 1}
     kwargs.pop("kwargs", None)
     model = AotNet(**locals(), **kwargs)
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="resnet_family", pretrained=pretrained)
     return model
 
 
+@register_model
 def RegNetY032(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 5, 13, 1]
     out_channels = [72, 216, 576, 1512]
     group_size = 24
     model = RegNetY(**locals(), model_name="regnety_032", **kwargs)
     return model
 
 
+@register_model
 def RegNetY040(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 6, 12, 2]
     out_channels = [128, 192, 512, 1088]
     group_size = 64
     model = RegNetY(**locals(), model_name="regnety_040", **kwargs)
     return model
 
 
+@register_model
 def RegNetY064(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 7, 14, 2]
     out_channels = [144, 288, 576, 1296]
     group_size = 72
     model = RegNetY(**locals(), model_name="regnety_064", **kwargs)
     return model
 
 
+@register_model
 def RegNetY080(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 4, 10, 1]
     out_channels = [168, 448, 896, 2016]
     group_size = 56
     model = RegNetY(**locals(), model_name="regnety_080", **kwargs)
     return model
 
 
+@register_model
 def RegNetY160(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 4, 11, 1]
     out_channels = [224, 448, 1232, 3024]
     group_size = 112
     model = RegNetY(**locals(), model_name="regnety_160", **kwargs)
     return model
 
 
+@register_model
 def RegNetY320(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 5, 12, 1]
     out_channels = [232, 696, 1392, 3712]
     group_size = 232
     model = RegNetY(**locals(), model_name="regnety_320", **kwargs)
     return model
 
 
+@register_model
 def RegNetZB16(input_shape=(224, 224, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 6, 12, 2]
     strides = [2, 2, 2, 2]
     out_channels = [48, 96, 192, 288]
     # timm bottle_in=True mode, the first ratio in each stack is `ratio * previous_channel`
     hidden_channel_ratio = [[32 * 3 / 48, 3], [1.5] + [3] * 5, [1.5] + [3] * 11, [192 * 3 / 288, 3]]
     use_block_output_activation = False  # timm linear_out=True mode
@@ -101,14 +109,15 @@
     output_num_features = 1536
     kwargs.pop("kwargs", None)
     model = AotNet(**locals(), model_name=kwargs.pop("model_name", "regnetz_b16"), **kwargs)
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="resnet_family", pretrained=pretrained)
     return model
 
 
+@register_model
 def RegNetZC16(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 6, 12, 2]
     strides = [2, 2, 2, 2]
     out_channels = [48, 96, 192, 288]
     # timm bottle_in=True mode, the first ratio in each stack is `ratio * previous_channel`
     hidden_channel_ratio = [[32 * 4 / 48, 4], [2] + [4] * 5, [2] + [4] * 11, [192 * 4 / 288, 4]]
     use_block_output_activation = False  # timm linear_out=True mode
@@ -122,21 +131,23 @@
     output_num_features = 1536
     kwargs.pop("kwargs", None)
     model = AotNet(**locals(), model_name=kwargs.pop("model_name", "regnetz_c16"), **kwargs)
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="resnet_family", pretrained=pretrained)
     return model
 
 
+@register_model
 def RegNetZC16_EVO(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     use_evo_norm = True
     evo_norm_group_size = 16
     bn_epsilon = 1e-3
     return RegNetZC16(**locals(), model_name="regnetz_c16_evo", **kwargs)
 
 
+@register_model
 def RegNetZD32(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 6, 12, 3]
     strides = [1, 2, 2, 2]
     out_channels = [64, 128, 256, 384]
     # timm bottle_in=True mode, the first ratio in each stack is `ratio * previous_channel`
     hidden_channel_ratio = [[64 * 4 / 64, 4, 4], [2] + [4] * 5, [2] + [4] * 11, [256 * 4 / 384, 4, 4]]
     use_block_output_activation = False  # timm linear_out=True mode
@@ -151,14 +162,15 @@
     output_num_features = 1792
     kwargs.pop("kwargs", None)
     model = AotNet(**locals(), model_name=kwargs.pop("model_name", "regnetz_d32"), **kwargs)
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="resnet_family", pretrained=pretrained)
     return model
 
 
+@register_model
 def RegNetZD8(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 6, 12, 3]
     strides = [1, 2, 2, 2]
     out_channels = [64, 128, 256, 384]
     # timm bottle_in=True mode, the first ratio in each stack is `ratio * previous_channel`
     hidden_channel_ratio = [[64 * 4 / 64, 4, 4], [64 * 4 / 128] + [4] * 5, [128 * 4 / 256] + [4] * 11, [256 * 4 / 384, 4, 4]]
     use_block_output_activation = False  # timm linear_out=True mode
@@ -173,22 +185,24 @@
     output_num_features = 1792
     kwargs.pop("kwargs", None)
     model = AotNet(**locals(), model_name=kwargs.pop("model_name", "regnetz_d8"), **kwargs)
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="resnet_family", pretrained=pretrained)
     return model
 
 
+@register_model
 def RegNetZD8_EVO(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     stem_type = "deep2"
     use_evo_norm = True
     evo_norm_group_size = 16
     bn_epsilon = 1e-3
     return RegNetZD8(**locals(), model_name="regnetz_d8_evo", **kwargs)
 
 
+@register_model
 def RegNetZE8(input_shape=(256, 256, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 8, 16, 3]
     strides = [1, 2, 2, 2]
     out_channels = [96, 192, 384, 512]
     # timm bottle_in=True mode, the first ratio in each stack is `ratio * previous_channel`
     hidden_channel_ratio = [[64 * 4 / 96, 4, 4], [96 * 4 / 192] + [4] * 7, [192 * 4 / 384] + [4] * 15, [384 * 4 / 512, 4, 4]]
     use_block_output_activation = False  # timm linear_out=True mode
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/resnet_deep.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/resnet_deep.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from keras_cv_attention_models.aotnet import AotNet
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
     "resnet50d": {"imagenet": "1b71933a82b058ba1e605ee5c01f64b2"},
     "resnet101d": {"imagenet": "79b075be5cf222cff2bced7a5a117623"},
     "resnet152d": {"imagenet": "0a15299b9abe1fee3ae06d9a59d13a3f"},
     "resnet200d": {"imagenet": "b5961494e0072c342b838c77ef52ddc5"},
@@ -12,25 +13,29 @@
 def ResNetD(num_blocks, input_shape=(224, 224, 3), pretrained="imagenet", stem_type="deep", strides=2, shortcut_type="avg", **kwargs):
     strides = strides if isinstance(strides, (list, tuple)) else [1, 2, 2, strides]
     model = AotNet(num_blocks, input_shape=input_shape, stem_type=stem_type, strides=strides, shortcut_type=shortcut_type, **kwargs)
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="resnet_family", pretrained=pretrained)
     return model
 
 
+@register_model
 def ResNet50D(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 6, 3]
     return ResNetD(**locals(), model_name="resnet50d", **kwargs)
 
 
+@register_model
 def ResNet101D(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 23, 3]
     return ResNetD(**locals(), model_name="resnet101d", **kwargs)
 
 
+@register_model
 def ResNet152D(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 8, 36, 3]
     return ResNetD(**locals(), model_name="resnet152d", **kwargs)
 
 
+@register_model
 def ResNet200D(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 24, 36, 3]
     return ResNetD(**locals(), model_name="resnet200d", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/resnet_quad.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/resnet_quad.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
 from keras_cv_attention_models.attention_layers import batchnorm_with_activation, conv2d_no_bias, drop_block, quad_stem, add_pre_post_process
 
 
 PRETRAINED_DICT = {
     "resnet51q": {"imagenet": "2b54c5e252bd58f37454e6fb273716f7"},
 }
@@ -16,33 +16,33 @@
     if conv_shortcut:
         shortcut = conv2d_no_bias(inputs, expanded_filter, 1, strides=strides, name=name + "shortcut_")
         shortcut = batchnorm_with_activation(shortcut, activation=None, zero_gamma=False, name=name + "shortcut_")
     else:
         shortcut = inputs
 
     if groups != 1:  # Edge block
-        nn = conv2d_no_bias(inputs, filters, 1, strides=1, padding="VALID", name=name + "1_")
+        nn = conv2d_no_bias(inputs, filters, 1, strides=1, padding="valid", name=name + "1_")
         nn = batchnorm_with_activation(nn, activation=activation, zero_gamma=False, name=name + "1_")
     else:
         nn = inputs
 
-    nn = conv2d_no_bias(nn, filters, 3, strides=strides, padding="SAME", groups=groups, name=name + "groups_")
+    nn = conv2d_no_bias(nn, filters, 3, strides=strides, padding="same", groups=groups, name=name + "groups_")
     nn = batchnorm_with_activation(nn, activation=activation, zero_gamma=False, name=name + "2_")
 
     if extra_conv:
-        nn = conv2d_no_bias(nn, filters, 3, strides=1, padding="SAME", groups=groups, name=name + "extra_groups_")
+        nn = conv2d_no_bias(nn, filters, 3, strides=1, padding="same", groups=groups, name=name + "extra_groups_")
         nn = batchnorm_with_activation(nn, activation=activation, zero_gamma=False, name=name + "extra_2_")
 
-    nn = conv2d_no_bias(nn, expanded_filter, 1, strides=1, padding="VALID", name=name + "3_")
+    nn = conv2d_no_bias(nn, expanded_filter, 1, strides=1, padding="valid", name=name + "3_")
     nn = batchnorm_with_activation(nn, activation=None, zero_gamma=True, name=name + "3_")
 
     # print(">>>> shortcut:", shortcut.shape, "nn:", nn.shape)
     nn = drop_block(nn, drop_rate)
-    nn = keras.layers.Add(name=name + "add")([shortcut, nn])
-    return keras.layers.Activation(activation, name=name + "output")(nn)
+    nn = layers.Add(name=name + "add")([shortcut, nn])
+    return layers.Activation(activation, name=name + "output")(nn)
 
 
 def quad_stack(inputs, blocks, filters, groups_div, strides=2, expansion=4, extra_conv=False, stack_drop=0, activation="swish", name=""):
     nn = inputs
     stack_drop_s, stack_drop_e = stack_drop if isinstance(stack_drop, (list, tuple)) else [stack_drop, stack_drop]
     for id in range(blocks):
         conv_shortcut = True if id == 0 and (strides != 1 or inputs.shape[-1] != filters * expansion) else False
@@ -69,20 +69,23 @@
     activation="swish",
     drop_connect_rate=0,
     classifier_activation="softmax",
     pretrained="imagenet",
     model_name="resnetq",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(shape=input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(shape=input_shape)
     nn = quad_stem(inputs, stem_width, activation=activation, stem_act=stem_act, name="stem_")
     nn = batchnorm_with_activation(nn, activation=activation, name="stem_")
     if stem_downsample:
-        nn = keras.layers.ZeroPadding2D(padding=1, name="stem_pool_pad")(nn)
-        nn = keras.layers.MaxPooling2D(pool_size=3, strides=2, name="stem_pool")(nn)
+        nn = layers.ZeroPadding2D(padding=1, name="stem_pool_pad")(nn)
+        nn = layers.MaxPooling2D(pool_size=3, strides=2, name="stem_pool")(nn)
 
     total_blocks = sum(num_blocks)
     global_block_id = 0
     drop_connect_s, drop_connect_e = 0, drop_connect_rate
     strides = strides if isinstance(strides, (list, tuple)) else [1, 2, 2, strides]
     for id, (num_block, out_channel, stride) in enumerate(zip(num_blocks, out_channels, strides)):
         name = "stack{}_".format(id + 1)
@@ -96,35 +99,37 @@
         global_block_id += num_block
 
     if num_features != 0:  # efficientnet like
         nn = conv2d_no_bias(nn, num_features, 1, strides=1, name="features_")
         nn = batchnorm_with_activation(nn, activation=activation, name="features_")
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="resnet_family", pretrained=pretrained)
     return model
 
 
+@register_model
 def ResNet51Q(input_shape=(224, 224, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 4, 6, 4]
     out_channels = [64, 128, 384, 384 * 4]
     stem_width = 128
     stem_act = False
     expansion = [4, 4, 4, 1]
     groups_div = [32, 32, 32, 1]
     extra_conv = False
     num_features = 2048
     return ResNetQ(**locals(), model_name="resnet51q", **kwargs)
 
 
+@register_model
 def ResNet61Q(input_shape=(224, 224, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [1, 4, 6, 4]
     out_channels = [256, 128, 384, 384 * 4]
     stem_width = 128
     stem_act = True
     expansion = [1, 4, 4, 1]
     groups_div = [0, 32, 32, 1]
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/resnet_family/resnext.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/resnet_family/resnext.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 from keras_cv_attention_models.aotnet import AotNet
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 
 PRETRAINED_DICT = {
     "resnext50": {"imagenet": "cf65d988c38ba0335c97a046288b91f4", "swsl": "f1cf0cc3c49bb50e6949c50fcce3db8f"},
     "resnext101": {"imagenet": "1e58c0ecc31184bd6bfe4d6b568f4325", "swsl": "c2fe8eefcf9a55e0254d2b13055a4cbc"},
     "resnext101w": {"imagenet": "9a1b92145aeb922695c29a0f02b52188", "swsl": "58b7cf4a72b03171f50ed19789b20f3d"},
@@ -14,47 +15,53 @@
 def ResNeXt(num_blocks, input_shape=(224, 224, 3), pretrained="imagenet", strides=2, groups=32, **kwargs):
     strides = strides if isinstance(strides, (list, tuple)) else [1, 2, 2, strides]
     model = AotNet(num_blocks, input_shape=input_shape, strides=strides, groups=groups, **kwargs)
     reload_model_weights(model, pretrained_dict=PRETRAINED_DICT, sub_release="resnet_family", pretrained=pretrained)
     return model
 
 
+@register_model
 def ResNeXt50(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 6, 3]
     hidden_channel_ratio = 0.5
     return ResNeXt(**locals(), model_name="resnext50", **kwargs)
 
 
+@register_model
 def ResNeXt101(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 23, 3]
     hidden_channel_ratio = 0.5
     return ResNeXt(**locals(), model_name="resnext101", **kwargs)
 
 
+@register_model
 def ResNeXt50D(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 6, 3]
     hidden_channel_ratio = 0.5
     stem_type = "deep"
     shortcut_type = "avg"
     return ResNeXt(**locals(), model_name="resnext50d", **kwargs)
 
 
+@register_model
 def ResNeXt101W(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 23, 3]
     hidden_channel_ratio = 1
     return ResNeXt(**locals(), model_name="resnext101w", **kwargs)
 
 
+@register_model
 def ResNeXt101W_se(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained=None, **kwargs):
     # timm using an additional conv + bn before se_module
     num_blocks = [3, 4, 23, 3]
     hidden_channel_ratio = 1
     se_ratio = 0.25 / 4
     stem_type = "deep"
     return ResNeXt(**locals(), model_name="resnext101w", **kwargs)
 
 
+@register_model
 def ResNeXt101W_64(input_shape=(224, 224, 3), num_classes=1000, activation="relu", classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 23, 3]
     hidden_channel_ratio = 1
     groups = 64
     return ResNeXt(**locals(), model_name="resnext101w_64", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/swin_transformer_v2/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/swin_transformer_v2/__init__.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 from keras_cv_attention_models.swin_transformer_v2.swin_transformer_v2 import (
     ExpLogitScale,
-    PairWiseRelativePositionalEmbedding,
-    shifted_window_attention,
+    MlpPairwisePositionalEmbedding,
     WindowAttentionMask,
     window_mhsa_with_pair_wise_positional_embedding,
+    shifted_window_attention,
     SwinTransformerV2,
     SwinTransformerV2Tiny_window8,
     SwinTransformerV2Tiny_window16,
     SwinTransformerV2Small_window8,
     SwinTransformerV2Small_window16,
     SwinTransformerV2Base_window8,
     SwinTransformerV2Base_window12,
     SwinTransformerV2Base_window16,
     SwinTransformerV2Base_window24,
     SwinTransformerV2Large_window12,
     SwinTransformerV2Large_window16,
     SwinTransformerV2Large_window24,
+    switch_to_deploy,
 )
 from keras_cv_attention_models.swin_transformer_v2.swin_transformer_v2_timm import SwinTransformerV2Tiny_ns, SwinTransformerV2Small_ns
 
 __head_doc__ = """
 Keras implementation of [Github microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer).
 Paper [PDF 2111.09883 Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/pdf/2111.09883.pdf).
 """
@@ -89,15 +90,15 @@
 SwinTransformerV2Tiny_ns.__doc__ = __default_doc__.format(pretrained=[None, "imagenet"])
 SwinTransformerV2Small_ns.__doc__ = __default_doc__.format(pretrained=[None, "imagenet"])
 
 ExpLogitScale.__doc__ = __head_doc__ + """
 Apply `inputs / tf.maximum(scale, min_value)` on given axis.
 
 Args:
-  axis: list or int number, specific axis apply scaling.
+  axis: list or int number, specific axis apply scaling. Set `axis=None` for scaler `weight_shape=()`.
   init_value: weight init value. Actual using is `tf.math.log(init_value)`.
   max_value: limit scaled max value.
 
 Examples:
 >>> from keras_cv_attention_models import attention_layers
 >>> aa = attention_layers.ExpLogitScale()
 >>> print(f"{aa(tf.ones([1, 32, 32, 192])).shape = }")
@@ -108,35 +109,45 @@
 >>> bb = attention_layers.ExpLogitScale(axis=[1, 2])
 >>> print(f"{bb(tf.ones([1, 32, 32, 192])).shape = }")
 # bb(tf.ones([1, 32, 32, 192])).shape = TensorShape([1, 32, 32, 192])
 >>> print({ii.name:ii.shape for ii in bb.weights})
 # {'divide_scale_1/weight:0': TensorShape([1, 32, 32, 1])}
 """
 
-PairWiseRelativePositionalEmbedding.__doc__ = __head_doc__ + """
-Pair Wise Relative Positional Embedding layer.
-No weight, just need to wrapper a layer, or will not in model structure.
-
-input: `[batch * window_patch, window_height, window_width, channel]`.
-output:
-    relative_log_coords `[(2 * window_height - 1) * (2 * window_width - 1), 2]`.
-    relative_position_index `[window_height * window_width, window_height * window_width]`
+MlpPairwisePositionalEmbedding.__doc__ = __head_doc__ + """
+MLP Pair Wise Relative Positional Embedding layer.
+
+use_absolute_pos=True input: `[batch, height, width, channel]` or `[batch, height * width, channel]`.
+use_absolute_pos=True output: input + `[1, height, width, channel]` or `[1, height * width, channel]`.
+use_absolute_pos=False input: `[batch, num_heads, height * width, height * width]`.
+use_absolute_pos=False output: input + `[1, num_heads, height * width, height * width]`.
 
 Args:
+  hidden_dim: hidden dimension for innner MLP dense output.
+  attn_height: input attention height controling initialized coords height. Default `-1` for using
+      `inputs.shape[1]` if `use_absolute_pos=True and len(inputs.shape) == 4` else `sqrt(inputs.shape[-2])`.
+  attn_width: input attention height controling initialized coords height. Default `-1` for using
+      `inputs.shape[2]` if `use_absolute_pos=True and len(inputs.shape) == 4` else `inputs.shape[-2] // attn_height`.
   pos_scale: If pretrained weights are from different input_shape or window_size, pos_scale is previous actually using window_size.
-      Default -1 for using `[height, width]` from input_shape.
+      Default -1 for using `[height, width]`.
+  use_absolute_pos: boolena value if using absolute or relative positional embedding.
+      - `False` for using coordinates in shape `[2 * height - 1, 2 * width - 1]` and index in shape `[height * width, height * width]`.
+      - `True` for using coordinates in shape `[height, width, 2]` if `len(inputs.sahpe) == 4` else `[height * width, 2]`.
 
 Examples:
 >>> from keras_cv_attention_models import attention_layers
->>> aa = attention_layers.PairWiseRelativePositionalEmbedding()
->>> relative_log_coords, relative_position_index = aa(tf.ones([1 * 9, 4, 4, 192]))
->>> print(f"{relative_log_coords.shape = }, {relative_position_index.shape = }")
-# relative_log_coords.shape = TensorShape([49, 2]), relative_position_index.shape = TensorShape([16, 16])
->>> print(f"{tf.gather(relative_log_coords, relative_position_index).shape = }")
-# tf.gather(relative_log_coords, relative_position_index).shape = TensorShape([16, 16, 2])
+>>> aa = attention_layers.MlpPairwisePositionalEmbedding()
+>>> print(f"{aa(tf.ones([9, 4, 49, 49])).shape = }")
+# aa(tf.ones([9, 4, 49, 49])).shape = TensorShape([9, 4, 49, 49])
+>>> print({ii.name: ii.shape for ii in aa.weights})
+# {'mlp_pairwise_positional_embedding_2/hidden_weight:0': TensorShape([2, 512]),
+#  'mlp_pairwise_positional_embedding_2/hidden_bias:0': TensorShape([512]),
+#  'mlp_pairwise_positional_embedding_2/out:0': TensorShape([512, 4])}
+>>> print(f"{aa.coords.shape = }, {aa.relative_position_index.shape = }")
+# aa.coords.shape = TensorShape([169, 2]), aa.relative_position_index.shape = TensorShape([49, 49])
 """
 
 WindowAttentionMask.__doc__ = __head_doc__ + """
 Window Attention Mask layer.
 No weight, just need to wrapper a layer, or will meet some error in model saving or loading.
 
 query_blocks = `window_height * window_width`, blocks = `(height // window_height) * (width // window_width)`
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,154 +1,241 @@
-import tensorflow as tf
-from tensorflow import keras
+import math
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, models, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     BiasLayer,
-    # ChannelAffine,
     drop_block,
     layer_norm,
     mlp_block,
     output_block,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 PRETRAINED_DICT = {
-    "swin_transformer_v2_base_window12": {"imagenet21k": {192: "b68add0f06aeede5d4dd10cd26855f36"}},
-    "swin_transformer_v2_base_window16": {"imagenet22k": {256: "1caba96ed702d467e650465503033c85"}},
-    "swin_transformer_v2_base_window16": {"imagenet": {256: "a7bfb9ae0733807baa702d05eb8feab8"}},
-    "swin_transformer_v2_base_window24": {"imagenet22k": {384: "acd467e1d8555e15542c8254bdae1b72"}},
-    "swin_transformer_v2_base_window8": {"imagenet": {256: "15454d9f6ba2ccca940f9c45b6935af6"}},
-    "swin_transformer_v2_large_window12": {"imagenet21k": {192: "ace20b0d634eb92989ece52e300440d5"}},
-    "swin_transformer_v2_large_window16": {"imagenet22k": {256: "151bb82a138f956613ce4b9885bfdd18"}},
-    "swin_transformer_v2_large_window24": {"imagenet22k": {384: "04fa3b195e5201c2d1068d1e19c8a0c5"}},
-    "swin_transformer_v2_small_window16": {"imagenet": {256: "3b2ca43d1927cca1b414b60e1044a84d"}},
-    "swin_transformer_v2_small_window8": {"imagenet": {256: "0a8468bd9acdf2056fc401e9f5067f97"}},
-    "swin_transformer_v2_tiny_window16": {"imagenet": {256: "37ce8c5f514c2249ef10d9c3acc37d29"}},
-    "swin_transformer_v2_tiny_window8": {"imagenet": {256: "9317f155e37e4081a09d290ca99bf7cd"}},
+    "swin_transformer_v2_base_window12": {"imagenet21k": {192: "0747958b4a891370b8caf538b0c6cf1f"}},
+    "swin_transformer_v2_base_window16": {"imagenet": {256: "ac5c965069f4452da28169955e8b0444"}, "imagenet22k": {256: "059b9cc52d036b329345a46c82ee9077"}},
+    "swin_transformer_v2_base_window24": {"imagenet22k": {384: "809935e83475c252a96dc6107dccd84f"}},
+    "swin_transformer_v2_base_window8": {"imagenet": {256: "28ecbbcc6bfb539896bb9ef025df444c"}},
+    "swin_transformer_v2_large_window12": {"imagenet21k": {192: "7f3c92eea3295d61e9d5881a7a935da1"}},
+    "swin_transformer_v2_large_window16": {"imagenet22k": {256: "8a124dcd6104596bd8fc362f14b87088"}},
+    "swin_transformer_v2_large_window24": {"imagenet22k": {384: "c8782f2a1874ca2979c350a8c2c72c39"}},
+    "swin_transformer_v2_small_window16": {"imagenet": {256: "89be9ca9b104fb802331120700497bb0"}},
+    "swin_transformer_v2_small_window8": {"imagenet": {256: "2736f7ed872130ee59f86e4982d91de0"}},
+    "swin_transformer_v2_tiny_window16": {"imagenet": {256: "95d9754a574ff667aad929f1e49f4d1f"}},
+    "swin_transformer_v2_tiny_window8": {"imagenet": {256: "97ece5f8d8012d6d40797df063a5f02b"}},
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="kecam")
-class ExpLogitScale(keras.layers.Layer):
-    def __init__(self, axis=-1, init_value=10.0, max_value=100.0, **kwargs):
+@backend.register_keras_serializable(package="kecam")
+class ExpLogitScale(layers.Layer):
+    def __init__(self, axis=-1, init_value=math.log(10.0), max_value=math.log(100.0), **kwargs):
         super().__init__(**kwargs)
         self.axis, self.init_value, self.max_value = axis, init_value, max_value
 
     def build(self, input_shape):
-        if self.axis == -1 or self.axis == len(input_shape) - 1:
+        if self.axis is None:
+            weight_shape = (1,)
+        elif self.axis == -1 or self.axis == len(input_shape) - 1:
             weight_shape = (input_shape[-1],)
         else:
             weight_shape = [1] * len(input_shape)
             axis = self.axis if isinstance(self.axis, (list, tuple)) else [self.axis]
             for ii in axis:
                 weight_shape[ii] = input_shape[ii]
 
-        initializer = tf.initializers.constant(tf.math.log(self.init_value))
-        self.scale = self.add_weight(name="weight", shape=weight_shape, initializer=initializer, trainable=True, dtype=self.dtype)
-        self.__max_value__ = tf.cast(tf.math.log(self.max_value), self._compute_dtype)
+        initializer = initializers.constant(self.init_value)
+        self.scale = self.add_weight(name="gamma", shape=weight_shape, initializer=initializer, trainable=True)
+        # self.__max_value__ = functional.convert_to_tensor(float(math.log(self.max_value)))
+        self.__max_value__ = float(self.max_value)
         super().build(input_shape)
 
     def call(self, inputs, **kwargs):
-        return inputs * tf.math.exp(tf.minimum(self.scale, self.__max_value__))
+        return inputs * functional.exp(functional.minimum(self.scale, self.__max_value__))
 
     def get_config(self):
         config = super().get_config()
         config.update({"axis": self.axis, "max_value": self.max_value})
         return config
 
 
-@tf.keras.utils.register_keras_serializable(package="kecam")
-class PairWiseRelativePositionalEmbedding(keras.layers.Layer):
-    def __init__(self, pos_scale=-1, **kwargs):
+@backend.register_keras_serializable(package="kecam")
+class MlpPairwisePositionalEmbedding(layers.Layer):
+    def __init__(self, hidden_dim=512, attn_height=-1, attn_width=-1, pos_scale=-1, use_absolute_pos=False, is_deploy_mode=False, **kwargs):
         # No weight, just need to wrapper a layer, or will not in model structure
-        self.pos_scale = pos_scale
         super().__init__(**kwargs)
+        self.hidden_dim, self.attn_height, self.attn_width, self.pos_scale = hidden_dim, attn_height, attn_width, pos_scale
+        self.use_absolute_pos, self.is_deploy_mode = use_absolute_pos, is_deploy_mode
 
-    def __build_pairwise_relative_position_index__(self, input_shape):
-        # input_shape: [batch * window_patch, window_height, window_width, channel]
-        height, width = input_shape[1], input_shape[2]  # [12, 15]
-        hh, ww = tf.meshgrid(range(height), range(width))
-        coords = tf.stack([hh, ww], axis=-1)  # [15, 12, 2]
-        coords_flatten = tf.reshape(coords, [-1, 2])  # [180, 2]
+    def _build_absolute_coords_(self):
+        hh, ww = np.meshgrid(range(0, self.height), range(0, self.width), indexing="ij")
+        coords = np.stack([hh, ww], axis=-1).astype("float32")
+        coords = coords / [self.height // 2, self.width // 2] - 1
+        coords = np.reshape(coords, [-1, coords.shape[-1]]) if self.is_compressed else coords
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("coords", functional.convert_to_tensor(coords, dtype=self.compute_dtype), persistent=False)
+        else:
+            self.coords = functional.convert_to_tensor(coords, dtype=self.compute_dtype)
+
+    def _build_relative_index_(self):
+        hh, ww = np.meshgrid(range(self.height), range(self.width))
+        coords = np.stack([hh, ww], axis=-1).astype("float32")  # [15, 12, 2]
+        coords_flatten = np.reshape(coords, [-1, 2])  # [180, 2]
         relative_coords = coords_flatten[:, None, :] - coords_flatten[None, :, :]  # [180, 180, 2]
         # relative_coords = tf.reshape(relative_coords, [-1, 2])  # [196 * 196, 2]
 
-        relative_coords_hh = relative_coords[:, :, 0] + height - 1
-        relative_coords_ww = (relative_coords[:, :, 1] + width - 1) * (2 * height - 1)
-        relative_coords_hhww = tf.stack([relative_coords_hh, relative_coords_ww], axis=-1)
-        self.relative_position_index = tf.reduce_sum(relative_coords_hhww, axis=-1)  # [180, 180]
-
-    def __build_relative_coords_table__(self, input_shape):
-        # input_shape: [batch * window_patch, window_height, window_width, channel]
-        height, width = input_shape[1], input_shape[2]  # [12, 15]
-        hh, ww = tf.meshgrid(range(-height + 1, height), range(-width + 1, width), indexing="ij")
-        coords = tf.cast(tf.stack([hh, ww], axis=-1), self.dtype)
+        relative_coords_hh = relative_coords[:, :, 0] + self.height - 1
+        relative_coords_ww = (relative_coords[:, :, 1] + self.width - 1) * (2 * self.height - 1)
+        relative_coords_hhww = np.stack([relative_coords_hh, relative_coords_ww], axis=-1)
+        relative_position_index = np.sum(relative_coords_hhww, axis=-1)  # [180, 180]
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("relative_position_index", functional.convert_to_tensor(relative_position_index, dtype="int64"), persistent=False)
+        else:
+            self.relative_position_index = functional.convert_to_tensor(relative_position_index, dtype="int64")
+
+    def _build_relative_coords_(self):
+        hh, ww = np.meshgrid(range(-self.height + 1, self.height), range(-self.width + 1, self.width), indexing="ij")
+        coords = np.stack([hh, ww], axis=-1).astype("float32")
         if self.pos_scale == -1:
-            pos_scale = [height, width]
+            pos_scale = [self.height, self.width]
         else:
             # If pretrined weights are from different input_shape or window_size, pos_scale is previous actually using window_size
             pos_scale = self.pos_scale if isinstance(self.pos_scale, (list, tuple)) else [self.pos_scale, self.pos_scale]
         coords = coords * 8 / [float(pos_scale[0] - 1), float(pos_scale[1] - 1)]  # [23, 29, 2], normalize to -8, 8
         # torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / math.log2(8)
-        relative_log_coords = tf.sign(coords) * tf.math.log(1.0 + tf.abs(coords)) / (tf.math.log(2.0) * 3.0)
-        self.relative_log_coords = tf.reshape(relative_log_coords, [-1, 2])  # [23 * 29, 2]
-        self.height, self.width = height, width  # For reload with shape mismatched
+        coords = np.sign(coords) * np.log(1.0 + np.abs(coords)) / (np.log(2.0) * 3.0)
+        coords = np.reshape(coords, [-1, 2])  # [23 * 29, 2]
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("coords", functional.convert_to_tensor(coords, dtype=self.compute_dtype), persistent=False)
+        else:
+            self.coords = functional.convert_to_tensor(coords, dtype=self.compute_dtype)
 
     def build(self, input_shape):
-        # input_shape: [batch * window_patch, window_height, window_width, channel]
-        self.__build_relative_coords_table__(input_shape)
-        self.__build_pairwise_relative_position_index__(input_shape)
+        if self.is_deploy_mode:
+            self.deploy_bias = self.add_weight(name="deploy_bias", shape=[1, *input_shape[1:]], initializer="zeros", trainable=False)
+            super().build(input_shape)
+            return
+
+        if self.use_absolute_pos:
+            # input_shape: [batch, height, width, channel] or [batch, height * width, channel]
+            self.is_compressed = len(input_shape) == 3
+            if self.is_compressed:
+                self.height = int(float(input_shape[-2]) ** 0.5) if self.attn_height == -1 else self.attn_height
+                self.width = input_shape[-2] // self.height
+            else:
+                self.height, self.width = input_shape[1:-1]
+
+            self._build_absolute_coords_()
+            out_shape = [self.hidden_dim, input_shape[-1]]
+        else:
+            # input_shape: [batch, num_heads, hh * ww, hh * ww]
+            height = int(float(input_shape[-2]) ** 0.5) if self.attn_height == -1 else self.attn_height  # hh == ww, e.g. 14
+            width = (input_shape[-2] // height) if self.attn_width == -1 else self.attn_width  # hh == ww, e.g. 14
+            self.height, self.width, self.num_heads = height, width, input_shape[1]
+            padding = input_shape[-2] - height * width
+            self.padding = [[padding, 0], [padding, 0], [0, 0]] if padding > 0 else None
+
+            self._build_relative_coords_()
+            self._build_relative_index_()
+            out_shape = [self.hidden_dim, self.num_heads]
+
+        self.hidden_weight = self.add_weight(name="hidden_weight", initializer="glorot_uniform", shape=[2, self.hidden_dim], trainable=True)
+        self.hidden_bias = self.add_weight(name="hidden_bias", initializer="zeros", shape=[self.hidden_dim], trainable=True)
+        self.out = self.add_weight(name="out", initializer="glorot_uniform", shape=out_shape, trainable=True)
+
+        self.is_deploy_mode = False
         super().build(input_shape)
 
     def call(self, inputs, **kwargs):
-        return self.relative_log_coords, self.relative_position_index
+        if self.is_deploy_mode:
+            return inputs + self.deploy_bias
+
+        pos_bias = self.coords @ self.hidden_weight + self.hidden_bias
+        pos_bias = functional.relu(pos_bias)
+        pos_bias = pos_bias @ self.out
+
+        if not self.use_absolute_pos:
+            pos_bias = functional.gather(pos_bias, self.relative_position_index)  # [hh * ww, hh * ww, num_heads]
+            pos_bias = functional.sigmoid(pos_bias) * 16.0
+            pos_bias = functional.pad(pos_bias, self.padding) if self.padding else pos_bias
+            pos_bias = functional.transpose(pos_bias, [2, 0, 1])
+        return inputs + functional.expand_dims(pos_bias, 0)
 
     def get_config(self):
         base_config = super().get_config()
-        base_config.update({"pos_scale": self.pos_scale})
+        base_config.update(
+            {
+                "hidden_dim": self.hidden_dim,
+                "attn_height": self.attn_height,
+                "attn_width": self.attn_width,
+                "pos_scale": self.pos_scale,
+                "use_absolute_pos": self.use_absolute_pos,
+                "is_deploy_mode": self.is_deploy_mode,
+            }
+        )
         return base_config
 
+    def switch_to_deploy(self):
+        deploy_bias = self(initializers.zeros()([1, *self.input_shape[1:]]))
+        delattr(self, "hidden_weight")
+        delattr(self, "hidden_bias")
+        delattr(self, "out")
+
+        # add as weights so can be saved to h5 and loaded back
+        self.deploy_bias = self.add_weight(name="deploy_bias", shape=deploy_bias.shape, initializer=initializers.Constant(deploy_bias), trainable=False)
+        self.is_deploy_mode = True
 
-@tf.keras.utils.register_keras_serializable(package="kecam")
-class WindowAttentionMask(keras.layers.Layer):
+
+@backend.register_keras_serializable(package="kecam")
+class WindowAttentionMask(layers.Layer):
     def __init__(self, height, width, window_height, window_width, shift_height=0, shift_width=0, **kwargs):
         # No weight, just need to wrapper a layer, or will meet some error in model saving or loading...
-        # float_dtype = tf.keras.mixed_precision.global_policy().compute_dtype
         self.height, self.width, self.window_height, self.window_width = height, width, window_height, window_width
         self.shift_height, self.shift_width = shift_height, shift_width
         self.blocks = (self.height // self.window_height) * (self.width // self.window_width)
         super().__init__(**kwargs)
 
     def build(self, input_shape):
         hh_split = [0, self.height - self.window_height, self.height - self.shift_height, self.height]
         ww_split = [0, self.width - self.window_width, self.width - self.shift_width, self.width]
         mask_value, total_ww, mask = 0, len(ww_split) - 1, []
         for hh_id in range(len(hh_split) - 1):
             hh = hh_split[hh_id + 1] - hh_split[hh_id]
-            rr = [tf.zeros([hh, ww_split[id + 1] - ww_split[id]]) + (id + mask_value) for id in range(total_ww)]
-            mask.append(tf.concat(rr, axis=-1))
+            rr = [np.zeros([hh, ww_split[id + 1] - ww_split[id]], dtype="float32") + (id + mask_value) for id in range(total_ww)]
+            mask.append(np.concatenate(rr, axis=-1))
             mask_value += total_ww
-        mask = tf.concat(mask, axis=0)
+        mask = np.concatenate(mask, axis=0)
         # return mask
 
-        mask = tf.reshape(mask, [self.height // self.window_height, self.window_height, self.width // self.window_width, self.window_width])
-        mask = tf.transpose(mask, [0, 2, 1, 3])
-        mask = tf.reshape(mask, [-1, self.window_height * self.window_width])
-        attn_mask = tf.expand_dims(mask, 1) - tf.expand_dims(mask, 2)
-        attn_mask = tf.cast(tf.where(attn_mask != 0, -100, 0), self._compute_dtype)
-        self.attn_mask = tf.expand_dims(tf.expand_dims(attn_mask, 1), 0)  # expand dims on batch and num_heads
+        mask = np.reshape(mask, [self.height // self.window_height, self.window_height, self.width // self.window_width, self.window_width])
+        mask = np.transpose(mask, [0, 2, 1, 3])
+        mask = np.reshape(mask, [-1, self.window_height * self.window_width])
+        attn_mask = np.expand_dims(mask, 1) - np.expand_dims(mask, 2)
+        attn_mask = np.where(attn_mask != 0, -100, 0)
+        attn_mask = np.expand_dims(np.expand_dims(attn_mask, 1), 0)  # expand dims on batch and num_heads
+
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("attn_mask", functional.convert_to_tensor(attn_mask, dtype=self.compute_dtype), persistent=False)
+        else:
+            self.attn_mask = functional.convert_to_tensor(attn_mask, dtype=self.compute_dtype)
 
         self.num_heads, self.query_blocks = input_shape[1], input_shape[2]
+        super().build(input_shape)
 
     def call(self, inputs, **kwargs):
         # inputs: [batch_size * blocks, num_heads, query_blocks, query_blocks]
         # where query_blocks = `window_height * window_width`, blocks = `(height // window_height) * (width // window_width)`
-        nn = tf.reshape(inputs, [-1, self.blocks, self.num_heads, self.query_blocks, self.query_blocks])
+        nn = functional.reshape(inputs, [-1, self.blocks, self.num_heads, self.query_blocks, self.query_blocks])
         nn = nn + self.attn_mask
-        return tf.reshape(nn, [-1, self.num_heads, self.query_blocks, self.query_blocks])
+        return functional.reshape(nn, [-1, self.num_heads, self.query_blocks, self.query_blocks])
+
+    def compute_output_shape(self, input_shape):
+        return [None, self.num_heads, self.query_blocks, self.query_blocks]
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "height": self.height,
                 "width": self.width,
@@ -161,141 +248,132 @@
         return config
 
 
 def window_mhsa_with_pair_wise_positional_embedding(
     inputs, num_heads=4, key_dim=0, meta_hidden_dim=512, mask=None, pos_scale=-1, out_bias=True, qv_bias=True, attn_dropout=0, out_dropout=0, name=None
 ):
     input_channel = inputs.shape[-1]
+    height, width = inputs.shape[1:-1]
     key_dim = key_dim if key_dim > 0 else input_channel // num_heads
     qk_out = key_dim * num_heads
 
-    qkv = keras.layers.Dense(qk_out * 3, use_bias=False, name=name and name + "qkv")(inputs)
-    qkv = tf.reshape(qkv, [-1, qkv.shape[1] * qkv.shape[2], qkv.shape[-1]])
-    query, key, value = tf.split(qkv, 3, axis=-1)
+    qkv = functional.reshape(inputs, [-1, height * width, inputs.shape[-1]])
+    qkv = layers.Dense(qk_out * 3, use_bias=False, name=name and name + "qkv")(qkv)
+    query, key, value = functional.split(qkv, 3, axis=-1)
     if qv_bias:
         query = BiasLayer(name=name and name + "query_bias")(query)
         value = BiasLayer(name=name and name + "value_bias")(value)
-    query = tf.transpose(tf.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
-    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, vv_dim]
+    query = functional.transpose(functional.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, key_dim]
+    key = functional.transpose(functional.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
+    value = functional.transpose(functional.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, vv_dim]
 
     # cosine attention
-    norm_query, norm_key = tf.nn.l2_normalize(query, axis=-1, epsilon=1e-6), tf.nn.l2_normalize(key, axis=-2, epsilon=1e-6)
-    attn = tf.matmul(norm_query, norm_key)  # [batch, num_heads, hh * ww, hh * ww]
+    norm_query, norm_key = functional.l2_normalize(query, axis=-1, epsilon=1e-6), functional.l2_normalize(key, axis=-2, epsilon=1e-6)
+    attn = functional.matmul(norm_query, norm_key)  # [batch, num_heads, hh * ww, hh * ww]
     attn = ExpLogitScale(axis=1, name=name and name + "scale")(attn)  # axis=1 is head dimension
-
-    # PairWiseRelativePositionalEmbedding -> mlp -> add with attn
-    pos_coords, pos_index = PairWiseRelativePositionalEmbedding(pos_scale=pos_scale, name=name and name + "pos_emb")(inputs)
-    relative_position_bias = keras.layers.Dense(meta_hidden_dim, use_bias=True, name=name and name + "meta_dense_1")(pos_coords)
-    relative_position_bias = keras.layers.Activation("relu")(relative_position_bias)
-    relative_position_bias = keras.layers.Dense(num_heads, use_bias=False, name=name and name + "meta_dense_2")(relative_position_bias)
-
-    relative_position_bias = tf.gather(relative_position_bias, pos_index)  # [hh * ww, hh * ww, num_heads]
-    relative_position_bias = tf.nn.sigmoid(relative_position_bias) * 16.0
-    relative_position_bias = tf.expand_dims(tf.transpose(relative_position_bias, [2, 0, 1]), 0)  # [1, num_heads, hh * ww, hh * ww]
-    attn = attn + relative_position_bias
+    attn = MlpPairwisePositionalEmbedding(pos_scale=pos_scale, attn_height=height, name=name and name + "pos_emb")(attn)
 
     if mask is not None:
         attn = mask(attn)
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
+    attention_scores = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
 
     if attn_dropout > 0:
-        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
-    attention_output = tf.matmul(attention_scores, value)
-    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
-    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * key_dim])
+        attention_scores = layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
+    attention_output = functional.matmul(attention_scores, value)
+    attention_output = functional.transpose(attention_output, [0, 2, 1, 3])
+    attention_output = functional.reshape(attention_output, [-1, height, width, num_heads * key_dim])
     # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
 
     # [batch, hh, ww, num_heads * vv_dim] * [num_heads * vv_dim, out] --> [batch, hh, ww, out]
-    attention_output = keras.layers.Dense(qk_out, use_bias=out_bias, name=name and name + "output")(attention_output)
-    attention_output = keras.layers.Dropout(out_dropout, name=name and name + "out_drop")(attention_output) if out_dropout > 0 else attention_output
+    attention_output = layers.Dense(qk_out, use_bias=out_bias, name=name and name + "output")(attention_output)
+    attention_output = layers.Dropout(out_dropout, name=name and name + "out_drop")(attention_output) if out_dropout > 0 else attention_output
     return attention_output
 
 
 def shifted_window_attention(inputs, window_size, num_heads=4, shift_size=0, pos_scale=-1, name=None):
     input_channel = inputs.shape[-1]
     window_size = window_size if isinstance(window_size, (list, tuple)) else [window_size, window_size]
     window_height = window_size[0] if window_size[0] < inputs.shape[1] else inputs.shape[1]
     window_width = window_size[1] if window_size[1] < inputs.shape[2] else inputs.shape[2]
     shift_size = 0 if (window_height == inputs.shape[1] and window_width == inputs.shape[2]) else shift_size
     should_shift = shift_size > 0
 
     # window_partition, partition windows, ceil mode padding if not divisible by window_size
     # patch_height, patch_width = inputs.shape[1] // window_height, inputs.shape[2] // window_width
-    patch_height, patch_width = int(tf.math.ceil(inputs.shape[1] / window_height)), int(tf.math.ceil(inputs.shape[2] / window_width))
+    patch_height, patch_width = int(math.ceil(inputs.shape[1] / window_height)), int(math.ceil(inputs.shape[2] / window_width))
     should_pad_hh, should_pad_ww = patch_height * window_height - inputs.shape[1], patch_width * window_width - inputs.shape[2]
     # print(f">>>> shifted_window_attention {inputs.shape = }, {should_pad_hh = }, {should_pad_ww = }")
     if should_pad_hh or should_pad_ww:
-        inputs = tf.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
+        inputs = functional.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
 
     if should_shift:
         shift_height, shift_width = int(window_height * shift_size), int(window_width * shift_size)
-        # tf.roll is not supported by tflite
-        # inputs = tf.roll(inputs, shift=(shift_height * -1, shift_width * -1), axis=[1, 2])
-        inputs = tf.concat([inputs[:, shift_height:], inputs[:, :shift_height]], axis=1)
-        inputs = tf.concat([inputs[:, :, shift_width:], inputs[:, :, :shift_width]], axis=2)
+        # functional.roll is not supported by tflite
+        # inputs = functional.roll(inputs, shift=(shift_height * -1, shift_width * -1), axis=[1, 2])
+        inputs = functional.concat([inputs[:, shift_height:], inputs[:, :shift_height]], axis=1)
+        inputs = functional.concat([inputs[:, :, shift_width:], inputs[:, :, :shift_width]], axis=2)
 
     # print(f">>>> shifted_window_attention {inputs.shape = }, {patch_height = }, {patch_width = }, {window_height = }, {window_width = }")
-    # [batch * patch_height, window_height, patch_width, window_width * input_channel], limit transpose perm <= 4
-    nn = tf.reshape(inputs, [-1, window_height, patch_width, window_width * input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, patch_width, window_height, window_width * input_channel]
-    nn = tf.reshape(nn, [-1, window_height, window_width, input_channel])  # [batch * patch_height * patch_width, window_height, window_width, input_channel]
+    # [batch * patch_height, window_height, patch_width, window_width * channels], limit transpose perm <= 4
+    nn = functional.reshape(inputs, [-1, window_height, patch_width, window_width * input_channel])
+    nn = functional.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, patch_width, window_height, window_width * channels]
+    nn = functional.reshape(nn, [-1, window_height, window_width, input_channel])  # [batch * patch_height * patch_width, window_height, window_width, channels]
 
     mask = WindowAttentionMask(inputs.shape[1], inputs.shape[2], window_height, window_width, shift_height, shift_width) if should_shift else None
     nn = window_mhsa_with_pair_wise_positional_embedding(nn, num_heads=num_heads, mask=mask, pos_scale=pos_scale, name=name)
 
     # window_reverse, merge windows
     # [batch * patch_height, patch_width, window_height, window_width * input_channel], limit transpose perm <= 4
-    nn = tf.reshape(nn, [-1, patch_width, window_height, window_width * input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, window_height, patch_width, window_width * input_channel]
-    nn = tf.reshape(nn, [-1, patch_height * window_height, patch_width * window_width, input_channel])
+    nn = functional.reshape(nn, [-1, patch_width, window_height, window_width * input_channel])
+    nn = functional.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, window_height, patch_width, window_width * input_channel]
+    nn = functional.reshape(nn, [-1, patch_height * window_height, patch_width * window_width, input_channel])
 
     if should_shift:
-        # nn = tf.roll(nn, shift=(shift_height, shift_width), axis=[1, 2])
-        nn = tf.concat([nn[:, -shift_height:], nn[:, :-shift_height]], axis=1)
-        nn = tf.concat([nn[:, :, -shift_width:], nn[:, :, :-shift_width]], axis=2)
+        # nn = functional.roll(nn, shift=(shift_height, shift_width), axis=[1, 2])
+        nn = functional.concat([nn[:, -shift_height:], nn[:, :-shift_height]], axis=1)
+        nn = functional.concat([nn[:, :, -shift_width:], nn[:, :, :-shift_width]], axis=2)
 
     # print(f">>>> shifted_window_attention before: {nn.shape = }, {should_pad_hh = }, {should_pad_ww = }")
     if should_pad_hh or should_pad_ww:
         nn = nn[:, : nn.shape[1] - should_pad_hh, : nn.shape[2] - should_pad_ww, :]  # In case should_pad_hh or should_pad_ww is 0
     # print(f">>>> shifted_window_attention after: {nn.shape = }")
 
     return nn
 
 
 def swin_transformer_block(
     inputs, window_size, num_heads=4, shift_size=0, pos_scale=-1, mlp_ratio=4, mlp_drop_rate=0, attn_drop_rate=0, drop_rate=0, name=None
 ):
     input_channel = inputs.shape[-1]
     attn = shifted_window_attention(inputs, window_size, num_heads, shift_size, pos_scale=pos_scale, name=name + "attn_")
-    attn = layer_norm(attn, zero_gamma=True, name=name + "attn_")
+    attn = layer_norm(attn, zero_gamma=True, axis=-1, name=name + "attn_")
     # attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "1_gamma")(attn) if layer_scale >= 0 else attn
     attn = drop_block(attn, drop_rate=drop_rate, name=name + "attn_")
-    attn_out = keras.layers.Add(name=name + "attn_out")([inputs, attn])
+    attn_out = layers.Add(name=name + "attn_out")([inputs, attn])
 
     mlp = mlp_block(attn_out, int(input_channel * mlp_ratio), drop_rate=mlp_drop_rate, use_conv=False, activation="gelu", name=name + "mlp_")
-    mlp = layer_norm(mlp, zero_gamma=True, name=name + "mlp_")
+    mlp = layer_norm(mlp, zero_gamma=True, axis=-1, name=name + "mlp_")
     # mlp = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "2_gamma")(mlp) if layer_scale >= 0 else mlp
     mlp = drop_block(mlp, drop_rate=drop_rate, name=name + "mlp_")
-    return keras.layers.Add(name=name + "output")([attn_out, mlp])
+    return layers.Add(name=name + "output")([attn_out, mlp])
 
 
 def patch_merging(inputs, name=""):
     input_channel = inputs.shape[-1]
     should_pad_hh, should_pad_ww = inputs.shape[1] % 2, inputs.shape[2] % 2
     # print(f">>>> patch_merging {inputs.shape = }, {should_pad_hh = }, {should_pad_ww = }")
     if should_pad_hh or should_pad_ww:
-        inputs = tf.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
+        inputs = functional.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
 
     # limit transpose perm <= 4
-    nn = tf.reshape(inputs, [-1, 2, inputs.shape[2], input_channel])  # [batch * inputs.shape[1] // 2, height 2, inputs.shape[2], input_channel]
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * inputs.shape[1] // 2, inputs.shape[2], height 2, input_channel]
-    nn = tf.reshape(nn, [-1, inputs.shape[1] // 2, inputs.shape[2] // 2, 2 * 2 * input_channel])
-    nn = keras.layers.Dense(2 * input_channel, use_bias=False, name=name + "dense")(nn)
-    nn = layer_norm(nn, name=name)
+    nn = functional.reshape(inputs, [-1, 2, inputs.shape[2], input_channel])  # [batch * inputs.shape[1] // 2, height 2, inputs.shape[2], input_channel]
+    nn = functional.transpose(nn, [0, 2, 1, 3])  # [batch * inputs.shape[1] // 2, inputs.shape[2], height 2, input_channel]
+    nn = functional.reshape(nn, [-1, inputs.shape[1] // 2, inputs.shape[2] // 2, 2 * 2 * input_channel])
+    nn = layers.Dense(2 * input_channel, use_bias=False, name=name + "dense")(nn)
+    nn = layer_norm(nn, axis=-1, name=name)
     return nn
 
 
 def SwinTransformerV2(
     num_blocks=[2, 2, 6, 2],
     num_heads=[3, 6, 12, 24],
     embed_dim=96,
@@ -310,17 +388,21 @@
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="swin_transformer_v2",
     kwargs=None,
 ):
     """Patch stem"""
-    inputs = keras.layers.Input(input_shape)
-    nn = keras.layers.Conv2D(embed_dim, kernel_size=stem_patch_size, strides=stem_patch_size, use_bias=True, name="stem_conv")(inputs)
-    nn = layer_norm(nn, name="stem_")
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+    nn = layers.Conv2D(embed_dim, kernel_size=stem_patch_size, strides=stem_patch_size, use_bias=True, name="stem_conv")(inputs)
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)  # channels_first -> channels_last
+    nn = layer_norm(nn, axis=-1, name="stem_")
     window_size = window_size if isinstance(window_size, (list, tuple)) else [window_size, window_size]
 
     """ stages """
     total_blocks = sum(num_blocks)
     global_block_id = 0
     for stack_id, (num_block, num_head) in enumerate(zip(num_blocks, num_heads)):
         stack_name = "stack{}_".format(stack_id + 1)
@@ -331,99 +413,121 @@
         for block_id in range(num_block):
             block_name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             shift_size = 0 if block_id % 2 == 0 else 0.5
             nn = swin_transformer_block(nn, window_size, num_head, shift_size, cur_pos_scale, drop_rate=block_drop_rate, name=block_name)
             global_block_id += 1
             if extra_norm_period > 0 and (block_id + 1) % extra_norm_period == 0 and not (use_stack_norm and block_id == num_block - 1):
-                nn = layer_norm(nn, name=block_name + "output_")
+                nn = layer_norm(nn, axis=-1, name=block_name + "output_")
         if use_stack_norm and stack_id != len(num_blocks) - 1:  # Exclude last stack
-            nn = layer_norm(nn, name=stack_name + "output_")
-    nn = layer_norm(nn, name="pre_output_")
+            nn = layer_norm(nn, axis=-1, name=stack_name + "output_")
+    nn = layer_norm(nn, axis=-1, name="pre_output_")
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_last -> channels_first
 
     nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
-    model = keras.models.Model(inputs, nn, name=model_name)
-    add_pre_post_process(model, rescale_mode="torch")
+    model = models.Model(inputs, nn, name=model_name)
     reload_model_weights(model, PRETRAINED_DICT, "swin_transformer_v2", pretrained)
+
+    add_pre_post_process(model, rescale_mode="torch")
+    model.switch_to_deploy = lambda: switch_to_deploy(model)
     return model
 
 
+def switch_to_deploy(model):
+    from keras_cv_attention_models.model_surgery.model_surgery import convert_layers_to_deploy_inplace
+
+    new_model = convert_layers_to_deploy_inplace(model)
+    add_pre_post_process(new_model, rescale_mode=model.preprocess_input.rescale_mode, post_process=model.decode_predictions)
+    return new_model
+
+
+@register_model
 def SwinTransformerV2Tiny_window8(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     window_size = kwargs.pop("window_size", 8)
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_tiny_window8", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Tiny_window16(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     window_size = kwargs.pop("window_size", 16)
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_tiny_window16", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Small_window8(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 18, 2]
     window_size = kwargs.pop("window_size", 8)
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_small_window8", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Small_window16(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 18, 2]
     window_size = kwargs.pop("window_size", 16)
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_small_window16", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Base_window8(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 18, 2]
     num_heads = [4, 8, 16, 32]
     embed_dim = 128
     window_size = kwargs.pop("window_size", 8)
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_base_window8", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Base_window12(input_shape=(192, 192, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet21k", **kwargs):
     num_blocks = [2, 2, 18, 2]
     num_heads = [4, 8, 16, 32]
     embed_dim = 128
     window_size = kwargs.pop("window_size", 12)
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_base_window12", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Base_window16(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 18, 2]
     num_heads = [4, 8, 16, 32]
     embed_dim = 128
     window_size = kwargs.pop("window_size", 16)
     pos_scale = kwargs.pop("pos_scale", [12, 12, 12, 6] if pretrained == "imagenet22k" else -1)  # 22k model is fine-tuned from window12
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_base_window16", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Base_window24(input_shape=(384, 384, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet22k", **kwargs):
     num_blocks = [2, 2, 18, 2]
     num_heads = [4, 8, 16, 32]
     embed_dim = 128
     window_size = kwargs.pop("window_size", 24)
     pos_scale = kwargs.pop("pos_scale", [12, 12, 12, 6] if pretrained == "imagenet22k" else -1)  # 22k model is fine-tuned from window12
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_base_window24", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Large_window12(input_shape=(192, 192, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet21k", **kwargs):
     num_blocks = [2, 2, 18, 2]
     num_heads = [6, 12, 24, 48]
     embed_dim = 192
     window_size = kwargs.pop("window_size", 12)
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_large_window12", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Large_window16(input_shape=(256, 256, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet22k", **kwargs):
     num_blocks = [2, 2, 18, 2]
     num_heads = [6, 12, 24, 48]
     embed_dim = 192
     window_size = kwargs.pop("window_size", 16)
     pos_scale = kwargs.pop("pos_scale", [12, 12, 12, 6] if pretrained == "imagenet22k" else -1)  # 22k model is fine-tuned from window12
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_large_window16", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Large_window24(input_shape=(384, 384, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet22k", **kwargs):
     num_blocks = [2, 2, 18, 2]
     num_heads = [6, 12, 24, 48]
     embed_dim = 192
     window_size = kwargs.pop("window_size", 24)
     pos_scale = kwargs.pop("pos_scale", [12, 12, 12, 6] if pretrained == "imagenet22k" else -1)  # 22k model is fine-tuned from window12
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_large_window24", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2_timm.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/swin_transformer_v2/swin_transformer_v2_timm.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,12 @@
-import tensorflow as tf
-from tensorflow import keras
+import math
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, models, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     ChannelAffine,
     drop_block,
     layer_norm,
     mlp_block,
     output_block,
     add_pre_post_process,
@@ -12,99 +15,115 @@
 
 PRETRAINED_DICT = {
     "swin_transformer_v2_tiny_ns": {"imagenet": {224: "c3272af88ba0cf09c818ac558ca9970e"}},
     "swin_transformer_v2_small_ns": {"imagenet": {224: "89d5a63d528bbb88a4a287871e868414"}},
 }
 
 
-@tf.keras.utils.register_keras_serializable(package="swinv2")
-class DivideScale(keras.layers.Layer):
+@backend.register_keras_serializable(package="swinv2")
+class DivideScale(layers.Layer):
     def __init__(self, axis=-1, initializer="ones", min_value=0.01, **kwargs):
         super().__init__(**kwargs)
         self.axis, self.initializer, self.min_value = axis, initializer, min_value
 
     def build(self, input_shape):
         if self.axis == -1 or self.axis == len(input_shape) - 1:
             weight_shape = (input_shape[-1],)
         else:
             weight_shape = [1] * len(input_shape)
             axis = self.axis if isinstance(self.axis, (list, tuple)) else [self.axis]
             for ii in axis:
                 weight_shape[ii] = input_shape[ii]
-        self.scale = self.add_weight(name="weight", shape=weight_shape, initializer=self.initializer, trainable=True, dtype=self.dtype)
+        self.scale = self.add_weight(name="weight", shape=weight_shape, initializer=self.initializer, trainable=True)
         super().build(input_shape)
 
     def call(self, inputs, **kwargs):
-        return inputs / tf.maximum(self.scale, self.min_value)
+        return inputs / functional.maximum(self.scale, self.min_value)
 
     def get_config(self):
         config = super().get_config()
         config.update({"axis": self.axis, "min_value": self.min_value})  # Not saving initializer in config
         return config
 
 
-@tf.keras.utils.register_keras_serializable(package="swinv2")
-class PairWiseRelativePositionalEmbedding(keras.layers.Layer):
+@backend.register_keras_serializable(package="swinv2")
+class PairWiseRelativePositionalEmbedding(layers.Layer):
     def __init__(self, **kwargs):
         # No weight, just need to wrapper a layer, or will not in model structure
         super().__init__(**kwargs)
+        self.use_layer_as_module = True
 
     def build(self, input_shape):
         # input_shape: [batch * window_patch, window_height, window_width, channel]
         height, width = input_shape[1], input_shape[2]
-        xx, yy = tf.meshgrid(range(height), range(width))  # tf.meshgrid is same with np.meshgrid 'xy' mode, while torch.meshgrid 'ij' mode
-        coords = tf.stack([yy, xx], axis=-1)  # [14, 14, 2]
-        coords_flatten = tf.reshape(coords, [-1, 2])  # [196, 2]
+        xx, yy = np.meshgrid(range(height), range(width))  # tf.meshgrid is same with np.meshgrid 'xy' mode, while torch.meshgrid 'ij' mode
+        coords = np.stack([yy, xx], axis=-1).astype("float32")  # [14, 14, 2]
+        coords_flatten = np.reshape(coords, [-1, 2])  # [196, 2]
         relative_coords = coords_flatten[:, None, :] - coords_flatten[None, :, :]  # [196, 196, 2]
-        # relative_coords = tf.reshape(relative_coords, [-1, 2])  # [196 * 196, 2]
-        relative_coords = tf.cast(relative_coords, self.dtype)
-        self.relative_coords_log = tf.sign(relative_coords) * tf.math.log(1.0 + tf.abs(relative_coords))
+        relative_coords_log = np.sign(relative_coords) * np.log(1.0 + np.abs(relative_coords))
+
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("relative_coords_log", functional.convert_to_tensor(relative_coords_log, dtype=self.compute_dtype), persistent=False)
+        else:
+            self.relative_coords_log = functional.convert_to_tensor(relative_coords_log, dtype=self.compute_dtype)
+        self.height, self.width = height, width
         super().build(input_shape)
 
     def call(self, inputs, **kwargs):
         return self.relative_coords_log
 
+    def compute_output_shape(self, input_shape):
+        return [self.height * self.width, self.height * self.width, 2]
 
-@tf.keras.utils.register_keras_serializable(package="swinv2")
-class WindowAttentionMask(keras.layers.Layer):
+
+@backend.register_keras_serializable(package="swinv2")
+class WindowAttentionMask(layers.Layer):
     def __init__(self, height, width, window_height, window_width, shift_height=0, shift_width=0, **kwargs):
         # No weight, just need to wrapper a layer, or will meet some error in model saving or loading...
-        # float_dtype = tf.keras.mixed_precision.global_policy().compute_dtype
         self.height, self.width, self.window_height, self.window_width = height, width, window_height, window_width
         self.shift_height, self.shift_width = shift_height, shift_width
         self.blocks = (self.height // self.window_height) * (self.width // self.window_width)
         super().__init__(**kwargs)
 
     def build(self, input_shape):
         hh_split = [0, self.height - self.window_height, self.height - self.shift_height, self.height]
         ww_split = [0, self.width - self.window_width, self.width - self.shift_width, self.width]
         mask_value, total_ww, mask = 0, len(ww_split) - 1, []
         for hh_id in range(len(hh_split) - 1):
             hh = hh_split[hh_id + 1] - hh_split[hh_id]
-            rr = [tf.zeros([hh, ww_split[id + 1] - ww_split[id]]) + (id + mask_value) for id in range(total_ww)]
-            mask.append(tf.concat(rr, axis=-1))
+            rr = [np.zeros([hh, ww_split[id + 1] - ww_split[id]], dtype="float32") + (id + mask_value) for id in range(total_ww)]
+            mask.append(np.concatenate(rr, axis=-1))
             mask_value += total_ww
-        mask = tf.concat(mask, axis=0)
+        mask = np.concatenate(mask, axis=0)
         # return mask
 
-        mask = tf.reshape(mask, [self.height // self.window_height, self.window_height, self.width // self.window_width, self.window_width])
-        mask = tf.transpose(mask, [0, 2, 1, 3])
-        mask = tf.reshape(mask, [-1, self.window_height * self.window_width])
-        attn_mask = tf.expand_dims(mask, 1) - tf.expand_dims(mask, 2)
-        attn_mask = tf.cast(tf.where(attn_mask != 0, -100, 0), self._compute_dtype)
-        self.attn_mask = tf.expand_dims(tf.expand_dims(attn_mask, 1), 0)  # expand dims on batch and num_heads
+        mask = np.reshape(mask, [self.height // self.window_height, self.window_height, self.width // self.window_width, self.window_width])
+        mask = np.transpose(mask, [0, 2, 1, 3])
+        mask = np.reshape(mask, [-1, self.window_height * self.window_width])
+        attn_mask = np.expand_dims(mask, 1) - np.expand_dims(mask, 2)
+        attn_mask = np.where(attn_mask != 0, -100, 0)
+        attn_mask = np.expand_dims(np.expand_dims(attn_mask, 1), 0)  # expand dims on batch and num_heads
+
+        if hasattr(self, "register_buffer"):  # PyTorch
+            self.register_buffer("attn_mask", functional.convert_to_tensor(attn_mask, dtype=self.compute_dtype), persistent=False)
+        else:
+            self.attn_mask = functional.convert_to_tensor(attn_mask, dtype=self.compute_dtype)
 
         self.num_heads, self.query_blocks = input_shape[1], input_shape[2]
+        super().build(input_shape)
 
     def call(self, inputs, **kwargs):
         # inputs: [batch_size * blocks, num_heads, query_blocks, query_blocks]
         # where query_blocks = `window_height * window_width`, blocks = `(height // window_height) * (width // window_width)`
-        nn = tf.reshape(inputs, [-1, self.blocks, self.num_heads, self.query_blocks, self.query_blocks])
+        nn = functional.reshape(inputs, [-1, self.blocks, self.num_heads, self.query_blocks, self.query_blocks])
         nn = nn + self.attn_mask
-        return tf.reshape(nn, [-1, self.num_heads, self.query_blocks, self.query_blocks])
+        return functional.reshape(nn, [-1, self.num_heads, self.query_blocks, self.query_blocks])
+
+    def compute_output_shape(self, input_shape):
+        return [None, self.num_heads, self.query_blocks, self.query_blocks]
 
     def get_config(self):
         config = super().get_config()
         config.update(
             {
                 "height": self.height,
                 "width": self.width,
@@ -120,129 +139,129 @@
 def window_mhsa_with_pair_wise_positional_embedding(
     inputs, num_heads=4, key_dim=0, meta_hidden_dim=384, mask=None, out_bias=True, attn_dropout=0, out_dropout=0, name=None
 ):
     input_channel = inputs.shape[-1]
     key_dim = key_dim if key_dim > 0 else input_channel // num_heads
     qk_out = key_dim * num_heads
 
-    qkv = keras.layers.Dense(qk_out * 3, use_bias=True, name=name and name + "qkv")(inputs)
-    qkv = tf.reshape(qkv, [-1, qkv.shape[1] * qkv.shape[2], qkv.shape[-1]])
-    query, key, value = tf.split(qkv, 3, axis=-1)
-    query = tf.transpose(tf.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  #  [batch, num_heads, hh * ww, key_dim]
-    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, vv_dim]
+    qkv = layers.Dense(qk_out * 3, use_bias=True, name=name and name + "qkv")(inputs)
+    qkv = functional.reshape(qkv, [-1, qkv.shape[1] * qkv.shape[2], qkv.shape[-1]])
+    query, key, value = functional.split(qkv, 3, axis=-1)
+    query = functional.transpose(functional.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  #  [batch, num_heads, hh * ww, key_dim]
+    key = functional.transpose(functional.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
+    value = functional.transpose(functional.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, vv_dim]
 
-    norm_query, norm_key = tf.nn.l2_normalize(query, axis=-1, epsilon=1e-6), tf.nn.l2_normalize(key, axis=-2, epsilon=1e-6)
-    attn = tf.matmul(norm_query, norm_key)
+    norm_query, norm_key = functional.l2_normalize(query, axis=-1, epsilon=1e-6), functional.l2_normalize(key, axis=-2, epsilon=1e-6)
+    attn = functional.matmul(norm_query, norm_key)
     attn = DivideScale(axis=1, name=name and name + "scale")(attn)  # axis=1 means on head dimension
 
     # _relative_positional_encodings
     pos_coord = PairWiseRelativePositionalEmbedding(name=name and name + "pos_emb")(inputs)  # Wrapper a layer, or will not in model structure
     relative_position_bias = mlp_block(pos_coord, meta_hidden_dim, output_channel=num_heads, drop_rate=0.1, activation="relu", name=name and name + "meta_")
-    relative_position_bias = tf.expand_dims(tf.transpose(relative_position_bias, [2, 0, 1]), 0)
+    relative_position_bias = functional.expand_dims(functional.transpose(relative_position_bias, [2, 0, 1]), 0)
     attn = attn + relative_position_bias
 
     if mask is not None:
         attn = mask(attn)
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
+    attention_scores = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
 
     if attn_dropout > 0:
-        attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
-    attention_output = tf.matmul(attention_scores, value)
-    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
-    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * key_dim])
+        attention_scores = layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores)
+    attention_output = functional.matmul(attention_scores, value)
+    attention_output = functional.transpose(attention_output, [0, 2, 1, 3])
+    attention_output = functional.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * key_dim])
     # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
 
     # [batch, hh, ww, num_heads * vv_dim] * [num_heads * vv_dim, out] --> [batch, hh, ww, out]
-    attention_output = keras.layers.Dense(qk_out, use_bias=out_bias, name=name and name + "output")(attention_output)
-    attention_output = keras.layers.Dropout(out_dropout, name=name and name + "out_drop")(attention_output) if out_dropout > 0 else attention_output
+    attention_output = layers.Dense(qk_out, use_bias=out_bias, name=name and name + "output")(attention_output)
+    attention_output = layers.Dropout(out_dropout, name=name and name + "out_drop")(attention_output) if out_dropout > 0 else attention_output
     return attention_output
 
 
 def shifted_window_attention(inputs, window_size, num_heads=4, shift_size=0, name=None):
     input_channel = inputs.shape[-1]
     window_size = window_size if isinstance(window_size, (list, tuple)) else [window_size, window_size]
     window_height = window_size[0] if window_size[0] < inputs.shape[1] else inputs.shape[1]
     window_width = window_size[1] if window_size[1] < inputs.shape[2] else inputs.shape[2]
     shift_size = 0 if (window_height == inputs.shape[1] and window_width == inputs.shape[2]) else shift_size
     should_shift = shift_size > 0
 
     # window_partition, partition windows, ceil mode padding if not divisible by window_size
     # patch_height, patch_width = inputs.shape[1] // window_height, inputs.shape[2] // window_width
-    patch_height, patch_width = int(tf.math.ceil(inputs.shape[1] / window_height)), int(tf.math.ceil(inputs.shape[2] / window_width))
+    patch_height, patch_width = int(math.ceil(inputs.shape[1] / window_height)), int(math.ceil(inputs.shape[2] / window_width))
     should_pad_hh, should_pad_ww = patch_height * window_height - inputs.shape[1], patch_width * window_width - inputs.shape[2]
     # print(f">>>> shifted_window_attention {inputs.shape = }, {should_pad_hh = }, {should_pad_ww = }")
     if should_pad_hh or should_pad_ww:
-        inputs = tf.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
+        inputs = functional.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
 
     if should_shift:
         shift_height, shift_width = int(window_height * shift_size), int(window_width * shift_size)
         # tf.roll is not supported by tflite
         # inputs = tf.roll(inputs, shift=(shift_height * -1, shift_width * -1), axis=[1, 2])
-        inputs = tf.concat([inputs[:, shift_height:], inputs[:, :shift_height]], axis=1)
-        inputs = tf.concat([inputs[:, :, shift_width:], inputs[:, :, :shift_width]], axis=2)
+        inputs = functional.concat([inputs[:, shift_height:], inputs[:, :shift_height]], axis=1)
+        inputs = functional.concat([inputs[:, :, shift_width:], inputs[:, :, :shift_width]], axis=2)
 
     # print(f">>>> shifted_window_attention {inputs.shape = }, {patch_height = }, {patch_width = }, {window_height = }, {window_width = }")
-    # [batch * patch_height, window_height, patch_width, window_width * input_channel], limit transpose perm <= 4
-    nn = tf.reshape(inputs, [-1, window_height, patch_width, window_width * input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, patch_width, window_height, window_width * input_channel]
-    nn = tf.reshape(nn, [-1, window_height, window_width, input_channel])  # [batch * patch_height * patch_width, window_height, window_width, input_channel]
+    # [batch * patch_height, window_height, patch_width, window_width * channel], limit transpose perm <= 4
+    nn = functional.reshape(inputs, [-1, window_height, patch_width, window_width * input_channel])
+    nn = functional.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, patch_width, window_height, window_width * channel]
+    nn = functional.reshape(nn, [-1, window_height, window_width, input_channel])  # [batch * patch_height * patch_width, window_height, window_width, channel]
 
     mask = WindowAttentionMask(inputs.shape[1], inputs.shape[2], window_height, window_width, shift_height, shift_width) if should_shift else None
     nn = window_mhsa_with_pair_wise_positional_embedding(nn, num_heads=num_heads, mask=mask, name=name)
 
     # window_reverse, merge windows
     # [batch * patch_height, patch_width, window_height, window_width * input_channel], limit transpose perm <= 4
-    nn = tf.reshape(nn, [-1, patch_width, window_height, window_width * input_channel])
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, window_height, patch_width, window_width * input_channel]
-    nn = tf.reshape(nn, [-1, patch_height * window_height, patch_width * window_width, input_channel])
+    nn = functional.reshape(nn, [-1, patch_width, window_height, window_width * input_channel])
+    nn = functional.transpose(nn, [0, 2, 1, 3])  # [batch * patch_height, window_height, patch_width, window_width * input_channel]
+    nn = functional.reshape(nn, [-1, patch_height * window_height, patch_width * window_width, input_channel])
 
     if should_shift:
         # nn = tf.roll(nn, shift=(shift_height, shift_width), axis=[1, 2])
-        nn = tf.concat([nn[:, -shift_height:], nn[:, :-shift_height]], axis=1)
-        nn = tf.concat([nn[:, :, -shift_width:], nn[:, :, :-shift_width]], axis=2)
+        nn = functional.concat([nn[:, -shift_height:], nn[:, :-shift_height]], axis=1)
+        nn = functional.concat([nn[:, :, -shift_width:], nn[:, :, :-shift_width]], axis=2)
 
     # print(f">>>> shifted_window_attention before: {nn.shape = }, {should_pad_hh = }, {should_pad_ww = }")
     if should_pad_hh or should_pad_ww:
         nn = nn[:, : nn.shape[1] - should_pad_hh, : nn.shape[2] - should_pad_ww, :]  # In case should_pad_hh or should_pad_ww is 0
     # print(f">>>> shifted_window_attention after: {nn.shape = }")
 
     return nn
 
 
 def swin_transformer_block(
     inputs, window_size, num_heads=4, shift_size=0, mlp_ratio=4, mlp_drop_rate=0, attn_drop_rate=0, drop_rate=0, layer_scale=-1, name=None
 ):
     input_channel = inputs.shape[-1]
     attn = shifted_window_attention(inputs, window_size, num_heads, shift_size, name=name + "attn_")
-    attn = layer_norm(attn, zero_gamma=True, name=name + "attn_")
-    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "1_gamma")(attn) if layer_scale >= 0 else attn
+    attn = layer_norm(attn, zero_gamma=True, axis=-1, name=name + "attn_")
+    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, axis=-1, name=name + "1_gamma")(attn) if layer_scale >= 0 else attn
     attn = drop_block(attn, drop_rate=drop_rate, name=name + "attn_")
-    attn_out = keras.layers.Add(name=name + "attn_out")([inputs, attn])
+    attn_out = layers.Add(name=name + "attn_out")([inputs, attn])
 
-    mlp = mlp_block(attn_out, int(input_channel * mlp_ratio), drop_rate=mlp_drop_rate, use_conv=False, activation="gelu", name=name + "mlp_")
-    mlp = layer_norm(mlp, zero_gamma=True, name=name + "mlp_")
-    mlp = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + "2_gamma")(mlp) if layer_scale >= 0 else mlp
+    mlp = mlp_block(attn_out, int(input_channel * mlp_ratio), drop_rate=mlp_drop_rate, activation="gelu", name=name + "mlp_")
+    mlp = layer_norm(mlp, zero_gamma=True, axis=-1, name=name + "mlp_")
+    mlp = ChannelAffine(use_bias=False, weight_init_value=layer_scale, axis=-1, name=name + "2_gamma")(mlp) if layer_scale >= 0 else mlp
     mlp = drop_block(mlp, drop_rate=drop_rate, name=name + "mlp_")
-    return keras.layers.Add(name=name + "output")([attn_out, mlp])
+    return layers.Add(name=name + "output")([attn_out, mlp])
 
 
 def patch_merging(inputs, name=""):
     input_channel = inputs.shape[-1]
     should_pad_hh, should_pad_ww = inputs.shape[1] % 2, inputs.shape[2] % 2
     # print(f">>>> patch_merging {inputs.shape = }, {should_pad_hh = }, {should_pad_ww = }")
     if should_pad_hh or should_pad_ww:
-        inputs = tf.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
+        inputs = functional.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])
 
     # limit transpose perm <= 4
-    nn = tf.reshape(inputs, [-1, 2, inputs.shape[2], input_channel])  # [batch * inputs.shape[1] // 2, height 2, inputs.shape[2], input_channel]
-    nn = tf.transpose(nn, [0, 2, 1, 3])  # [batch * inputs.shape[1] // 2, inputs.shape[2], height 2, input_channel]
-    nn = tf.reshape(nn, [-1, inputs.shape[1] // 2, inputs.shape[2] // 2, 2 * 2 * input_channel])
-    nn = layer_norm(nn, name=name)
-    nn = keras.layers.Dense(2 * input_channel, use_bias=False, name=name + "dense")(nn)
+    nn = functional.reshape(inputs, [-1, 2, inputs.shape[2], input_channel])  # [batch * inputs.shape[1] // 2, height 2, inputs.shape[2], input_channel]
+    nn = functional.transpose(nn, [0, 2, 1, 3])  # [batch * inputs.shape[1] // 2, inputs.shape[2], height 2, input_channel]
+    nn = functional.reshape(nn, [-1, inputs.shape[1] // 2, inputs.shape[2] // 2, 2 * 2 * input_channel])
+    nn = layer_norm(nn, axis=-1, name=name)
+    nn = layers.Dense(2 * input_channel, use_bias=False, name=name + "dense")(nn)
     return nn
 
 
 def SwinTransformerV2(
     num_blocks=[2, 2, 6, 2],
     num_heads=[3, 6, 12, 24],
     embed_dim=96,
@@ -258,17 +277,21 @@
     classifier_activation="softmax",
     dropout=0,
     pretrained=None,
     model_name="swin_transformer_v2",
     kwargs=None,
 ):
     """Patch stem"""
-    inputs = keras.layers.Input(input_shape)
-    nn = keras.layers.Conv2D(embed_dim, kernel_size=stem_patch_size, strides=stem_patch_size, use_bias=True, name="stem_conv")(inputs)
-    nn = layer_norm(nn, name="stem_")
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
+    nn = layers.Conv2D(embed_dim, kernel_size=stem_patch_size, strides=stem_patch_size, use_bias=True, name="stem_conv")(inputs)
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)  # channels_first -> channels_last
+    nn = layer_norm(nn, axis=-1, name="stem_")
     # window_size = [input_shape[0] // window_ratio, input_shape[1] // window_ratio]
     # window_size = [int(tf.math.ceil(input_shape[0] / window_ratio)), int(tf.math.ceil(input_shape[1] / window_ratio))]
     window_size = window_size if isinstance(window_size, (list, tuple)) else [window_size, window_size]
 
     """ stages """
     total_blocks = sum(num_blocks)
     global_block_id = 0
@@ -280,28 +303,31 @@
         for block_id in range(num_block):
             block_name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             shift_size = 0 if block_id % 2 == 0 else 0.5
             nn = swin_transformer_block(nn, window_size, num_head, shift_size, drop_rate=block_drop_rate, layer_scale=layer_scale, name=block_name)
             global_block_id += 1
             if extra_norm_period > 0 and (block_id + 1) % extra_norm_period == 0 and not (use_stack_norm and block_id == num_block - 1):
-                nn = layer_norm(nn, name=block_name + "output_")
+                nn = layer_norm(nn, axis=-1, name=block_name + "output_")
         if use_stack_norm and stack_id != len(num_blocks) - 1:  # Exclude last stack
-            nn = layer_norm(nn, name=stack_name + "output_")
-    nn = layer_norm(nn, name="pre_output_")
+            nn = layer_norm(nn, axis=-1, name=stack_name + "output_")
+    nn = layer_norm(nn, axis=-1, name="pre_output_")
+    nn = nn if backend.image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_last -> channels_first
 
     nn = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "swin_transformer_v2", pretrained)
     return model
 
 
+@register_model
 def SwinTransformerV2Tiny_ns(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     use_stack_norm = True
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_tiny_ns", **kwargs)
 
 
+@register_model
 def SwinTransformerV2Small_ns(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", pretrained="imagenet", **kwargs):
     num_blocks = [2, 2, 18, 2]
     use_stack_norm = True
     return SwinTransformerV2(**locals(), model_name="swin_transformer_v2_small_ns", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/test_images.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/test_images.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,10 +13,10 @@
 
 def dog():
     return np.array(Image.open(io.BytesIO(__dog__)))
 
 
 __dog_cat__ = b"\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x00d\x00d\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x06\x05\x08\x07\x07\x07\t\t\x08\n\x0c\x14\r\x0c\x0b\x0b\x0c\x19\x12\x13\x0f\x14\x1d\x1a\x1f\x1e\x1d\x1a\x1c\x1c $.' \",#\x1c\x1c(7),01444\x1f'9=82<.342\xff\xdb\x00C\x01\t\t\t\x0c\x0b\x0c\x18\r\r\x182!\x1c!22222222222222222222222222222222222222222222222222\xff\xc0\x00\x11\x08\x02\x00\x02\x00\x03\x01\"\x00\x02\x11\x01\x03\x11\x01\xff\xc4\x00\x1f\x00\x00\x01\x05\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x10\x00\x02\x01\x03\x03\x02\x04\x03\x05\x05\x04\x04\x00\x00\x01}\x01\x02\x03\x00\x04\x11\x05\x12!1A\x06\x13Qa\x07\"q\x142\x81\x91\xa1\x08#B\xb1\xc1\x15R\xd1\xf0$3br\x82\t\n\x16\x17\x18\x19\x1a%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe1\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf1\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xc4\x00\x1f\x01\x00\x03\x01\x01\x01\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x11\x00\x02\x01\x02\x04\x04\x03\x04\x07\x05\x04\x04\x00\x01\x02w\x00\x01\x02\x03\x11\x04\x05!1\x06\x12AQ\x07aq\x13\"2\x81\x08\x14B\x91\xa1\xb1\xc1\t#3R\xf0\x15br\xd1\n\x16$4\xe1%\xf1\x17\x18\x19\x1a&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x82\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xda\x00\x0c\x03\x01\x00\x02\x11\x03\x11\x00?\x00\xc6\xa2\x8a*\x89\n(\xa2\x80\n(\xa2\x80\x15z\xd2\x93\x81M\xa2\x80\x14\x9c\x8aJ(\xa0\x02\x8a(\xa0\x02\x8a(\xa0\x02\x8a(\xa0\x02\x90)$\x01\xcd-[\xd3a\xf3\xaf\x10\x1e\x83\x93S&\xa2\x9beA9I$o[\xec\xb7\xb3U\xe8H\xc5B\x1b\x12\x13\xd0S5\x071\xedS\xc6j\x84w,e \x9c\xf1^M\x1b\xb6\xe4\xfa\x9e\xcdK$\x925w\xc7!\xc3g5N\xe9\x95\x07\xb54H\xe4\xe7\x14\xdb\x92LY\"\xba\xaecb\x81\x00>\xe5c\xcfcV&\x88\x1bG\xdf\xdds\x8a\xa3\xb0\x99H\x03\x91\xc8\x04\xf0kEN\xe8\x94\xb7L`\x8a\x99\xea\x89[\x99\x1a/&H\xfb9#\x15#)F \xd4zj\x94\xbc\x94\x0e\x00n>\x95b\xe5@\xb8j\xbc,\xfd\xe7\x13,\\}\xd8\xc8\x83\xa1\xa2\x8a+\xb4\xe1\n3\xc6(\xa2\x80\n(\xa2\x80\n(\xc7\x19\xa2\x80;\xdf\x00\xfd\xc3\xf5\xafK\x04,{\x98\x80\x00\xeak\xca\xbc\x1d{\x15\x95\xbb\xcb3m@i\x9e%\xf1\xcd\xcd\xeb\x1b;\x03\xb2.\x85\x81\xe4\xd6U*\xc6\x1b\xeemJ\x94\xa7\xb6\xc7M\xe2\x7f\x88\x16ZFm\xad\x19f\xb9<q\xd1k\xcc5}wV\xd7$\xddq3yc\xf8A\xc0\xaa\xc2\xcb3\x99\xa6b\xecy\xe6\xac\x17D_\xbb\xf3W\x1c\xe7*\x87u8F\x99\x9e\xb6\xc0\xfd\xe0rjCl\xbe^q\x83\xd8\nyl\xc8\x18\x9e*\xc5\xbco4\x98\xc6+)S\xd0\xde55 \xd3\x14\xc1w\x1c\x98\xc3\t\x06?:\xf7\x18N\xe8P\xfa\xa8\xaf\x1e\x16\xc67P\x01\xc8`k\xd7\xad?\xe3\xd2/\xf7\x05m\x81\xdeH\xc3\x1f\xb4Y=\x0bE(\xeak\xd0<\xe1i\xd4\xdauH\xc2\x9dM\xa0u\xaa\x01\xd4\xab\xd6\x92\x8a\x00}\x03\xad \xe9J:\xd0\x03\xa8\x1dh\xa0u\xa0\x07QE\x14\x00\xa3\x8aZh4\xea\x00(\xa2\x8a\x00r\xf4\xa5\xa4^\x94\xb4\x00QE(\xa0\x0f\x9fh\xa2\x8a\x0c\xc2\x8a(\xa0\x02\x8a\x07\x06\x8a\x00(\xa2\x8a\x00(\xa2\x8a\x00(\xa2\x8a\x00)\x0fJZ\x0fJ\x00h8\xa5\xddI\xd2\x8a\x00Q\xcdlhQo\x95\x9b<\x81\x8a\xc6\x15\xd3\xf8z\xd8\x94.x\x06\xb9\xf1O\x96\x93gF\x167\xaa\x8aZ\xcc\x85o\xc2c#\x1cU 6\x92\xfd\xf8\xab\x1a\xd6N\xa4y\xe8x\xaa\xcc\xf9\x88\xfbW\r'\xee\x9e\x8c\xd6\xa5\xd8\x1c\xb1\x185m\xe3Ia\xc1\xe4\x9a\xcf\xd3\x99_\x1d\xc5[\x9a_\xb3\xcd\xf3\x1c)\x15\xbcv\xb8\x99H\xda\x94\x97\x86\xc8\xcfCJ\xf9U$\x0c\x8fZ\xd0\xf2\xc4\xab\xb8\x0e\rA\"\xec\x8d\x97\xd2\xb2\x9c\xec\x821\xbb2-\x01\xfbd\x9cS\xefWd\xf8\xf6\xcd^\xd3\xad\x19\xd5\xe4#\x92x\xaa\xfa\x8aa\xf9\xea\xbcQ\x83\x95\xea\x93\x8d\x8d\xa9/S>\x8a\t\xa2\xbdC\xc9\n(\xa2\x80\n:\xd1E\x00\x18\xabVvmt\xf8#\x08\xbfy\x8dGm\x03\xdcJ\x11A>\xa6\xadj7\xabem\xf6h1\xbb\x18'\xb95\x85z\xca\x9a\xd3s\xa2\x85\x07Q\xdd\xec\x88uM@)K\x1b\\\x04^\xb8\xa6[\xdb.\xe0I\xe1FX\xfa\xd5\x18\xa1\xfd\xd8\x92Bs\x9c\x93\xebS=\xe1U*\x8b\xc0\x15\xc7\x15wvw7ed\\\x96x\xd5X/S\xc6j\x9a\xee\x95\x89<\x0fJ\xa2.\x1aF\xc9\xc6GE\x15\xabc\x0b\x11\x9e\xacke\x13;\x8e\x86\xd7\xcc\xda\xa0s\x9e\xf5\xb3kj#`O\x07\x14\xfb[`\xac\x1b\xb8\x15\xa1\x15\xa12\xee=\x05L\xf6*;\x94Z\x02.b]\xb9\xdc\xe0\x0f\xce\xbd&\x11\xb6$_@+\x85\x86\x16\x9bX\xb6@8\xdf\x9a\xef\x10`b\x96\x05i'\xe6N9\xeb\x15\xe4:\x85\xa2\x80q^\x81\xc0:\x94\x1aJQ\xd6\x90\xc5\xa0u\xa2\x81\xd6\x98\x0e\xa5^\xb4\x94\xab\xd6\x80\x1d@\xebE\x03\xad\x00:\x8c\xd1@\xeb@\x0b\x9e3H\xae\xacx\xe6\x95\x86T\x8a\xc7\x86\xf1\xa2\xd4\xbc\xa6\xe0f\x8e\xb6+\x95\xf2\xb9v5\xf9\rO\xcf5F{\x9f\xdf\x85\x07\x8c\xd4\xf1L\x1d\xf1\x91\x9a%h\xbb\x12\x93\xe5L\xb1EB\xf7(\x8f\xb4\x9a\x91d\r\xde\x95\xc7f\x87\x83\x8au2\x99-\xccp\xfd\xf3Cv\x04\x9bvD\xd4\x01\x9al\x13E7!\xaaF\xc0<sLM5\xa3>|\xa2\x8a(3\n(\xa2\x80\n(\xa4\xa0\x05\xa2\x8a(\x00\xa2\x8a(\x00\xa2\x8a(\x00\xa2\x92\x96\x80\x12\x93\xafJZ(\x01\xf1)g\x0b\x8c\x93]\x85\x84f\x1b%S\xc1\xc5`h\x96\xc6k\xf4$eT\xe6\xb75\xab\xd8\xed%P:\x95\xed^~:M\xc7\x91\x1e\x8e\x06\x1a\xf33\x07[\x8aE\x9f~\xec\xf3Us\xba\x16\xc7'\x15r\xeacu\x03\xfd:\xd6}\xaeJ\x11\xdf\xa5s\xd2~\xed\x99\xd75\xad\xcd+\x02\xb0\xdb\x86e\xc0\xf5\xaa:\xa5\xdb\xbc\x8a\x80dg\xadZ\x8a9d\x84\xaa7\x0b\xc9\x15<z\x7f\x9e\xc2f##\xb0\xae\x87%\x14f\xa3vY\xd2\xc3}\x9dw\xe7\x18\xe8j\x96\xab?\x972\xaa\x1e\\\xf0\x05h\xc98\xb7\xb5U\x8ce\xf0F\x00\xac\xab}:\xf6[\x93q<LG\xf0\xe4t\x15\xc3RNGLb\x93\xbb5\xb4\xe8e0\xe3\x1c\x0e\xf5\x91\xaa\x7f\xafe\xc6+\xa2\xb2\x9b\xca\x84\xa3|\xbe\xd5\xcfjn>\xd4\xf9\x19\xcdo\x81_\xbc9\xb1\xcd\xbaff2))\xd4\x84W\xaex\xe2QF(\xa0\x02\x95\x11\x98\x8c\x02iU\x0bV\xed\x9d\x80\xb5\x8f\xed2\x81\x8c\x02\xa0\xd6u*\xc6\x9cy\xa4mJ\x94\xaaK\x95\x0e\x8a\x15\xd3\xec\xc37\x0e\xe3$\xd7/3\xfd\xa3Pgc\xf2\xe6\xb6u[\xc6\xb8*\xaa{\xe2\xb3E\xa1^\x00\xc95\xe3{GR|\xcc\xf6}\x92\xa7\x0eTF\xf2\x01\xcb\xb6\x13\xb0\xf5\xac\xc9\xe4frT\x95_\xe7V\xeect\xc6\xf1\x82O\x00\xd3m\xed\x1aS\x97\xfb\xbf\xce\xbb \xecr\xc9j&\x9fd\xd22\x9cpy&\xba\xbb;uF\x1c\x7f\xf5\xaa\xbe\x9fe\x8c\x1c`zV\xe4V\xfbr1\xf8\xd6\x84\xd8!C\xb8\xf1\xc0\xad\x10\xca\x91n>\x95X\x0f+\x8a\x8e\xe2\xe4*m\x03&\xb1\xab+GCH+\xc9\"\xde\x86\xe2\xe7^\x18\x1f\xea\xd0\x9f\xc6\xbb:\xe4\xfc\x1fo\xb9\xae\xae\x9b\xab\x10\xa2\xba\xca\xe8\xc1F\xd4W\x99\xcb\x8d\x95\xea\xb5\xd8^\xd4Q@\xae\xb3\x94u\x03\xad\x14\x0e\xb4\x86:\x81\xd6\x8a\x07Z`:\x95z\xd2P\x0e\r\x00>\x94t\xa6\xf6\xa7\x0e\x94\x00\xb4QE\x00(\xfb\xb5\xcdj\xcf\xf6k\xaf?\xd2\xbaQ\xd2\xb25+Q4\x12\xe6\xaa*\xec\xec\xc2\xb4\x94\xb9\xb6\xb1\x8d\x0e\xb9\x15\xc4\xc8\x15\x81lr*\xdd\x86\xa1\xfb\xe6fn\xfcW%\xa0\xe9\x87\xfbb\xf0\xef;\x15\xc8\x1c\xd5\xcdVIm\xae\x168\xb3\xb7\x1c\xd7\xcf\xe2s)\xc7\x13*QW\xb1\xd7K\tN\xb7\"Z\\\xda\xb8\xd4\x1amH\"\x1e\x01\xe6\xb7\xa3.#S\xed\\n\x86\xad5\xc6X\x12\xc4\xf3]\xe4Q\x01\x10\x04\xf6\xae\xbc5I\xceI}\xe4c\xe1\nrq]4\"\x9er\xb0o^\xddk\x9f\xd4\x96\xf3R\x88\xad\xb9(\xf9\xe1\xab\xa1s\x14@\x86 \x83T\x1a\xf5!l\xc6\x07\x15\xe8T\x8cf\xb9N\x1c=gFjilc\xae\x8f\xe2\x18\x91L\x12\xa9\xce2\x0ek\xaa\xb0\xb7\xbcK_\xdf\xf3&*\xf6\x9b|\xb7p\x83\x8e{\xd5\xfe=)\xc2*1\xb2\"\xb5WV\xa3\x9b\xea|\xd9E\x14U\x18\x05\x14Q@\x05\x14Q@\x05\x14Q@\x05\x14Q@\x05\x14Q@\tG9\xa5\xa2\x80\n\x07QE\x03\x83\x9a\x10\x1df\x87fb\x85f*r\xdc\xd5\xbb\xbd\x10^\xbb\\J\xf88\xe1j\xde\x89\"\x1d> \xdc\x9d\xb4\xcdWy\x8f\x111\x07=\x01\xaf\x16\xa5G\xed[l\xf6\xa9\xc2\xd4\xd2FD\x961\xc5k!\xe8@\xc5s\xb6q2\xc8\xfct&\xb5\xe6\xb9\x91\x95\xa09\xe9\xcd\x16\xd6n\xcd\xf2\xaf\xde\xadae!\xbd\xb5+\xdb\xc6\xe1\xc8S\xd6\xba\x1d7I\x9an\x00;Oz\xb9\xa6\xf8tF\x82[\x93\x8e\xe0V\xdbJ\x91\xa6\xc8\x97\x03\x14\xaaj\xc9\xe7\xb6\x88\x86\xcfE\xb4\xb6\x04\xb8V\x7fz\xbe\x12\xdb;B\xae1Y\x84\xcd\xb8\x10\xa4\xd5+\x89o\xa3,V3\xc1\xfc\xea9\x15\x89\xe6osR\xf3K\xb5\x9d\x08\xc6\xd3\xea+\x89\xd6\xbc=4,\xd2\xc6\xfb\xc0\xf4\xeb]\x02_\xdd\t\t\x95\x1b`\x1d\xa8y\xe3\x9b\xef+.zf\x92\x9b\xa5.d[\x82\xa9\x1eVy\xd1R\x1b\x18\xe7\xd2\x90\xf1\xd6\xbd\x0e\xdfK\xb1\x9a]\xf2D\xa3\xd6\x9d{\xe1}6\xe22\xf0e[\xb6\rwC\x1b\tt8\xa7\x83\x94z\x9erzT\x91@\xf2\xb0\n\xa7\x9a\xeb4\xcf\x08\xac\x97\xc4\xdc>aN\xc3\xbdu\xd0\xe8\xfat\x04\x08\xe1PW\xa59\xe2\xe0\xb4\x8e\xa4\xc7\x0b/\xb5\xa1\xe7\xd6\xfaz[(\x92c\x829\x19\xa5\xd4/C\xc6\x123\xc6+\xba\xd4\xf4{{\x881\xb1G\x1d\xab\xcb5\xd8\xa6\xd3\xef\x00\x19*zW\x9fVr\xaa\xfd\xe3\xd2\xa3\x18\xd3^\xe8\x81X\xce\x8c\xc7\x9c\xf4\xad[hC\xabHFq\xd2\xb9\xfbK\xcd\xd7k\x1b\x9c\x9cWOj\xdbmX\x9e\x98\xac\xfe\x1d\re\xef+\x9c\xe6\xa2I\xb9\xda\xb8-\xdc\xfaV\x8e\x9b\n\"fL\x13Xr\\;^\xbe@\xc9j\xd1\x8c\\\x05\x07\x07\x15\xdf\x05\xa5\xceF\xce\x8e\xdeX\xf7|\x9c\xe3\xbd\\I\xdb8\xc7\x1d\xab:\xca\x13\x14\x00\x9eI\xab\xeaF\xd0\xc7\xb5S\xb96%r\x0c\x80\x93\xd0V[H\x1aWl\x92\x8b\xd4\xfb\xd4\xb3\xdc\xe1\x19\x97>\xc6\xa1\x816\xc0\xa8\xbc\xb4\xad\\\xb8\x87ucj+[\x9d\xa7\x85c+\xa5\xef#\x87l\x8a\xdd\xaa\xda}\xb8\xb5\xb1\x8a :/5f\xbdJQ\xe5\x82\x8fc\xca\xab.i\xb9w\x16\x81\x9e\xd4P\x0e+BGP:\xd1@\xebH\x07QE\x14\xc0^\xd4\xb8\xcd'jr\xf5\xa0\x05\x1d)\xc3\xa5%(\xe9@\x0bJ\x06M%><f\x93\xd8kq\xc2#\x8ak\xd9\xac\x8aT\xf7\xab\x01\x852I\x96>Y\x85B\x9b\xbd\xd1\xb2\\\xbb\x19P\xe81[3\xb4j2\xc7=+>\xe7B\x8eI\x8c\x8f\x9a\xd7\xb8\xd4\xd5A\x02\xb2\xe6\xbe\x92C\x85\xc9\xaew\x86\xa4\xe4\xe6\xe3\xab)b%\x1b8\xbd\x84\xb5\xb3\x83O;\x97\x00\xd4\x93jG\xa05\x02[\xcf9\xe785~\r1@\x05\xebxSQ\xf8Q\x95J\xb2\xa8\xef&f\xee\x9e\xe1\xb8\x07\x15n\r)\xdc\x83%k\xc7l\x91\xe3\n8\xa9\x80\xad\x122\xb8\xed6\xdc[\xae\xd5\xad\x1c\n\xa9m\xd6\xad\xd3\x11\xf3]\x14QA!E\x14P\x01E\x14\x94\x00\xb4QE\x00\x14QE\x00\x14QE\x00\x14RR\xd0\x01E\x14\x94\x01\xd7\xf8v\xe7\xce\x87\xcb\xfe\xe2\xd4\xba\x8e\xa1\x1c2\xed\xceNz\n\xcc\xd2\xee\xfc\xbd1\x91\x17\x0ex\xc8\xab\x16:\\\xd72\xf9\x93\x1d\xcaz\xe6\xbc,J^\xd9\xa4{\xb8o\xe1'&2(\r\xec\xcecRw\xf0\x08\x1d\x05uVv\xb0\xd9[\xa8p\x19\xc0\xe9I\x02\xc1g\x08\x86\xdd\x06\xfcu=\xaaA\x19f\xcb|\xcd\xd4\x9a\xd1h\x88\x93\xe6d\xc2W\x90\x86c\xf2zT\xa6Kx\x94\x12\xc2\xb9\xaf\x14\xeb\r\xa6\xe9\x92\xb5\xb0>`\x19\x07\xb1\xae7\xc3>*mJ\xe8A<\x8c]\xba\x8fJ\xaeY5r\x1b\x8av=\x8e\xdaH\xa6A\xb4\x03\xde\x9f,({Vf\x92\xca\xac\xbbG\x00w=kFg\xc4\x87\x15\x9d\xf4\xb9\\\xba\x94\xeea\x8d\x10\x9e\x07z\xc6\xf2\x1e\xe6p\x00\xc2\n\xda\x9a6\x9d\x87\xa5Mkh\xb1\xe4\xe3\x81Y\xbdY\xa2\x92\x8a(\xad\x96#\xda\x078\xaas[\\Z\x871\x9c\x93\xc0\x04\xfe\xb5\xd2yx\xebT\xee\xed>\xd0\xa7k\x11ZB6\xd4\x89J\xe8\xe7\xe1\xbdp\xc4t\xed\xf5\xa7\xbe\xa4\xfb\x81V\x1bGV=\xfd\x852\xf7Jf\x1c\xb6\xd5\x1d\x00\xef\xf5\xaaS\x91\x148#\xe6\xe9\x8cr?\n\xd5\xc53>n\x87C\x06\xa0\x97\x087\x11\x8a\xe7|[\xa7\x89,^X\xa3%\xc0\xc85\x14\x13\xb5\xb0\\\xefPNp1]\x024w\xd6dm\x03\x8crsR\xe0\x1c\xd6<>&t\xd4\x90\x92r:\xe6\xbbt'\xech?\xbd\xc9\xac\xed_@6Z\xc3H\xff\x00q\x8eT\xe2\xb6\"\x87}\xa8\x1f\xca\xa2q\xbc\x8e\x88\xca\xd19I\xec$3\x97Q\x9c\x9c\xd6\xb5\x9b\xcdlB\xcc\xbf/\xfb])\xf2\xca\x91\xdd\xa4 \x8c\x83\xc9\xad\xff\x00&)\xa0\xda\xd8*G&\xbb\xa0\xb49\xde\xe4\xb14\x12\xdb\x86\x87\xadV\xb8\x90D\x83'\x9a\x8a\x14Kd(\x8cqPK\x1f\x9f0\xde\xc4'\xa5MI$4\x89\xe2\x92;\x88\xf0\x07\xd2\xb6\xb4\r'\xcf\xd4\x16F\xff\x00W\x08\xcf\xe3Y\x0c`\xb6\x88\x1d\xca\xb8\xef]g\x83g\x8a\xebO\x96X\xc9'y\x06\xb9\xa8/iZ\xef\xa1u\xe5\xec\xe8\xbb\x1b\xc4c\x8a)_\xad6\xbd\x83\xc8\x16\x8e\xb4Q@\xc7P:\xd1J\x05\x00-\x14Q@\x00\xa7\xafZh\x14\xe5\xeb@\x0e\xa5\x1d)8\xef\xd2\xa3{\x94OCJ\xe8\t\xbf\x955\xa6H\xba\xd6t\xb7\xc4\x9f\x92\xabby\xcf<\nM\xdc\x11~}S\x1c.*\x83\xdcM9\xe35f\x1d4\x9eZ\xaf\xc5i\x1c}\xb3IDnW2\xa2\xb0\x92S\x97&\xb4 \xd3\xe3\x8f\xafZ\xba\xab\x8cc\xa5;\x15V&\xe3\x160\x9c(\xa7\xf1E-1\x05(\xefI@>\xb4\x01b\xdb\xef\n\xb7U-\xfe\xfd[\xa0\x0f\x9a\xe8\xa2\x8cPHQE\x14\x00RR\xd2P\x02\xd1E\x14\x00QE\x14\x00QE!\xe9@\x0bE \xe9K@\x05>4\xf3\x18.pi\x95\xa3\xa2\xdb}\xa6\xfdW\xd3\x9a\x99\xcb\x96-\x97\x08\xf3I#[J\xb0\x10\xa8$\xe7<\xfd+i\xa6u\\ \xeb\xc0\x03\xf9\xd5\x13\xfb\xbb\x86\x8c\x9d\xa8\xa3-T\xae\xb5\xb84\xe8^\xea~\\\xf1\x1a\x0e\xb5\xe1\xaej\x92\xbb\xdc\xf6\xe6\xa3\x08\xa5\xd1\x1bs\xeazv\x90\x84\xde\xddF\x8eFN\xe3\xcdb\xbf\xc4=\x11$\xd9\x1d\xc8\x91{m\xaf2\xd4\xa1\xd4<E\xa9\xc8\xf2>\xdd\xf2lP\xc6\xb4\xae\xbe\x1d\xfd\x93N{\xa5\xbf2yX\x0eB\xfc\xa1\xbd+\xd0\x86\x19=\xf5g\x0c\xeb\xcb\xec\xecnx\xa3\xc4\xd0j6\xbeU\xaa\xfc\xcc\x0fZ\xc3\xf0^\x91?\xf6\x89\xb8 \xec\x07\x03\xd2\xb1d\xdf\x01E$\x92\xc0(\xafM\xf0m\xb0\x86\xc1C.\xe6'p\x1e\x95U\xa2\xa3\x0eTM\x06\xe7>g\xd0\xed\xacA\x82\xdds\x8d\xd8\xe7\x15n6y%'\x1fJ\xa8\xbc\xa8P\xdd:\xd5\xb8\x8e\xd8\xd4w5\xe7m\xa1\xda\xcbjT0\x1csS\xa60*\xa2c\xe6c\xdf\xa0\xf4\xa9c\x94\x1e=)\xdd\"Zl\xb4\xc3#\xde\xab7\\\x1cb\xa1\xb9\xba\xd8\xa7\x0c\x01\x03\xbddO}\xe5\x11\xbaL\x86\xee\rT\xb6\xb8\xa2\xb5\xb1\xa9t\xa9\xb0\xb0\xc1'\x80+\x97\xbb\x80EpK.\xd6c\x90\xc4g5\xad\xf6\xb4\x922\xdb\xc6\x05d^\xcc\xd37\xc9/C\x9e\xbc}*\xa3 \x94L\xc9\xe3\xea@\x05\x97\xf3\"\xb4\xf4\x99\x11\x13s\xc8rOs\x8aF\x81\xbc\xaf\x99Tw<\xf5\xfaU?4Gy\xe4\xc9\x85\\n\xfa\x8a\xbb\x92\xd6\x965\xf5k\x08\xb5\x0bRB\xe5\x87 \xd7%yu\xf6\x1bs\x10?\xbc\xe83]\x96\x9dq\x14\xa8avVBp0zW-\xaa\xe91\x9dT\xa9\x91\x9a1\xce\x1b\x9f\xd6\x95\x92w\x1cn\xf49O\xb2O,\xdb\xb7\x12O$\x8a\xe9l<\xdf)ccWV\xd2\xda;I\x9a\x11\xf7F3\xebM\xd1\x8a7$e\xf3\x80+hM1\xf2\xd8\xb0\xb6FGU9\n9\xac\xef\x12H\xd60\x19!\x8c\xb3t\x00V\xe4\xa6er\xa8>j\xbbcc\x1c\xa8Z\xf5\x15\xc9\x1cq\\\xf5*E\xca\xd7.)\xadO3\xd1\xec\xef\xf5{\xb6{\x92\xc2\xdd\x06H\xcdz\xaf\x82\xad\r\xae\x90\xe4)\x08\xf2\x12\xbe\xe2\xb2\xc5\x94\x16ir\x90\x007\x9e+\xb5\xb1\x88A\xa6\xc0\x80`\x04\x15\xd1\x85\xb4\xe6\xda\xe8s\xe2\xdb\x8c\x14_Q\x97\x0e#\x8d\xdd\x8e\x02\x8c\x93\xe9\\\xf0\xf1D\x19m\xaaX\x03\xe9\x8a\xb3\xe2\xbb\xa6\x83L\xf2\x90\xe1\xa5l~\x03\xada\xe9\xfaRM\x12\xb9\xe4\xf7 \xd7MI\xcb\x9a\xd19\xa9\xd3\x8b\x8d\xe4u\x96w\xf6\xf7\xd1\xef\x85\xc1=\xd4\xf5\x15j\xb9)-\xe4\xd3dY\xa1\xc0\xdb\xd3\x1cc\xf0\xef]\x1e\x9fx\x97\xd6\xe1\xc6\x03\x8e\x19}\r:u.\xf9e\xb8\xaaS\xb2\xe6[\x17)F{\xd2S\xc0\xcdjd%\x00{PX'z\x82K\xa0\xb9\xc5\x17\x02\xcf\xca\xbdMC-\xd2\xa0\xe2\xa8Ir\xcd\x9a\xa9$\xa4\x9e\xb5.L\x0b\xb3_\x12x\xaa2\\\x96<\x9a\x85\x9b=\xea&?7\x157\x15\xce\x96\xc2\xd9$\x801\xab\xcb\n\xa0\xe0U}+\x9bQ\xf4\xab\x95\xa2\x10\xdd\xa6\x80\x0ey\xa7\xd2\x12\x00$\x9003\xcd0\x0e\x95\x04\xf7\xb6\xf6\xec\x16Y\x91\t\xe8\t\xaegW\xf1Qy^\xd7M9*v\xbc\xbd\xbf\n\xe3\xb5%\x9b\r$\xb3H\xe4\x9c\x92O\x0bX\xba\xda\xfb\xaa\xe6\xea\x8b\xb5\xe5\xa1\xebq\xcc\x92\xaeQ\x83\x0fj\x92\xb1<(]\xfc;j\xf2\x1c\xb3\x03\x92~\xb5\xb5Z\xa7us\x16\xac\xec)\xc7jJv\x05&\r1\x13\xdb\x91\xbe\xaeU+~\x1e\xae\xd0\x07\xcdtg4QA!E\x14P\x01IKE\x00\x14QE\x00\x14QE\x00\x14\x87\xa5-\x14\x00\x83\xa5-\x14P\x02V\xcf\x87$\xf2\xf5\x11\xef\xc5c\xf7\xab\x16\xd35\xbc\xea\xe9\xd4\x1e\x95\x15\x13qi\x17M\xf2\xcd6t\x1a\x94\xaf\xfe\x92\xeap\\\xed\xe3\xd2\xb9\xddGI\xb8\xba\xb0{\xec\xb7\xc8q\x1a\x1fA\xde\xbb]\x17K7\xd0\x9b\xc9\xc6S\xf8A\xad+\xbb\x08\xe4\xb4\xf2\xb7\xed_A\xdcW\x91E\xca\x0c\xf6*\xa8\xcd\x1e.\xf2g\xcb\x96\"\xa1\xd5\x89e'\xa9\xf5\xa7\xbd\xe4\xc2\xcaX\xdew\x10\xbb\t\x1a0\xdc;\x01\x81\x9f\xa5mj^\x1c\x85u\x16\xfb\x18\xdc:\xb1\xcfJ~\x9d\xe1\xa15\xd8i@*\xbdA\x1cW\xa2\xe7\x08\xfb\xfdO=Br\xf7:\x1c\xdd\x8e\x99u\xa9K\x13l`\x80\xe7q\xe3\x02\xbd_H\x89t\xeb$\x88\x90O\x075X,V\xaa\x11QF\xd1\xc6\x05W7FK\x8d\xac\x08W\xfc\xc5rN\xa3\x9e\xe7e:j:#\xaa\xb4\x95$\xdc\xdbz\xf3V\xbe\xd2\xa0c\x19=\xab\x99\x86\xf0\xc1\x10\xf2\x8eI\xeb\x9a\xb0oK\xa8,\xdc\x81\x91\\\xf6\xbb5vF\xac\xf7\xe1\x15\xb9\xc1\xe9\xd6\xa9I\xadG\x12\x05\xdf\x8c\x8c\x93\\\xde\xa3\xab\xc6\xa1\xb7\xbeI\xea\x01\xe9\x8e\xf5\xc4j\xfe%\x84\xb8T.@'98\xab\x8d'7\xa13\x9c`\xb5:\xaf\x16x\xdc[~\xee\x12\x18\xb7\x04\xfab\xb9m3\xc5\xf7\xd7\xb7\xab\x11|\xc6N\x0f\xbdp\x97w\xd2\xddL\xccX\xf2zg55\x84\xd7\x16\xa7\xed\t\x14\xae?\xbc\xa0\xe3\xf3\xae\xe8\xe1\xd2\x8d\x9e\xe7\x0b\xae\xf9\xee\x8f~\x8e\xc1\xae-\x83\xda\\\x06ld\xc6N\rE\x13?\xdcta\x83\x86&\xbc\xfb\xc3>6\x1er[\xcc\xde[\x13\x80\xc4\xf7\xfa\xd7\xa4Z^\xc7\x7f\x08\x93\x8d\xe3\x83\xcfz\xe1\xa9\x19R\x95\x9e\xc7m9\xaa\xb1\xba\"C#>\xd5#\xaf\xca_\xa0\xac\x8dbYD\x85\xd8,e\x0fns\xee+v\xea\x16h\x99c\\g\xd6\xb8\xfdB\xf2kI\xa3.\xa5\x91O#9\xc7\xf8\xd5\xc4\x993\x7fC\xbb\xf3\x93p#\xcc^\xa3\x1c\x1a5\xa9J\\\xab\x05\xfb\xeb\xc5d\xe8\xda\x9cOz\x1d\x06\x15\xb2\xbbk\xa1\xd5aY\xed\xd2d\xfb\xc9\xc8\xc5Eg\xee\x97G\xe23\xec\xa6&\x07\x88\x8f\x99\x8dY\xd1\x8aY\xfd\xa2vR\xc4\x1c/\xb7\xbdg\x1618\x91\x8e\x19\x87 zU\xbb\x0b\xc4\xfb\x14\xd1\xf1\x9c\xf7\xac#U\xc5ht:|\xcc\xdc\xb7\xb8\x8eX\xcc\xc4\xfb\x9a\xc9\xd45\xf1\x16QN\t\xe1Fj\x84\x17\xae\xef\xf6hH\xdc\xe7\x00f\xbb\xbb\x1f\x87zT\xb6\xc9.\xa2\x8d=\xc9\x19\xdc\x1c\x80\xbe\xc2\x8c=\tWm\x93^\xac(Y3\x99\xd0R\xe3S\xbeH\x8123\x10\\\x8eB\x8a\xf4\xb7\x8cG\x1a\xa0\xe8\x06)t\xdd\x1a\xc7I\x84\xc7i\x02\xc6\x0fS\xdc\xd3\xae1\xcd{\x18z\n\x8c|\xcf'\x11_\xda\xcb\xc8\xe2|]\xfb\xd9\xed\xa1\xcf\xdd\x05\x8f\xe2\x7f\xfa\xd5>\x93\xb8\xc6\xb1*\x03\x81\xf8Q\xe2\x18w\xea1\x1e\xa0\xa7\xf55j\xc2X\xed\x93f\xd0\xceGs\x80)\xff\x00\xcb\xc6_\xfc\xbaE\x8b\x98\x03\xc4cm\xad\x91\xe8\x05`$\xd2h\xb7\xc1\xf0LlpT\xfaWDn]\xf9\xf9\x15}\x00\x07\xf9U;\xfbx\xaeb!\x91\xb8\x1c|\xb55c}V\xe3\x84\xad\xa4\x8dX\xee#\x96\x15\x962\x19Xg5\x14\x97x\xe9\\\xde\x9fv\xfa]\xc7\x913\x03j\xe7 \x9e\n\xd7I<1\x18C\x86\\\x11\x90s\xc5]9\xb9\xaf3\x1a\x90\xe4ev\x9c\xb1\xe4\xf1P9\x19\xcej\x94\x97\xd6\xab\x90n#\x18\xf7\xa5[\x88\x9f*&C\x9e\x9f0\xaa2w',\x0fz\x89\x8f5^[\x84\x88nwU\x1e\xe6\xa37q6\n\xca\x84\x1e\x98j\x96\"\xc3\x9fCQ\x13\xcdBn\x14\x9c\x06\x07>\xf5\x1c\x93\xaa\x9c\x16\x00\xfb\x9a@w\x1aO\xfcz\x0f\xa5\\\xac\x0bM{O\xb0\xd3\xc3\xdc\xddD\x8a\x00$\x96\x1d\xce+\x0bV\xf8\x9f\xa7Z\xdd-\x96\x9d\x0c\x9a\x85\xd3\xe0*\xc7\xc0$\xf6\xcdk\xcc\x94u\x1a\x8b{\x1d\xd4\x92\xa41\xb4\x928UQ\x92I\xc5q\xda\xb6\xb7.\xa8\xcdid\xc4E\x9c3\x0e\xe2\xb3n5-CY\xd9\x1d\xcc\x89\x1f\x19\x928\xb2U}\xb3\xdc\xfb\xd6\xb5\x96\x9f\r\xba\xaf\xc9&q\x9e\xc3\xf9\xd7<\xa6\xea;-\x8e\x98\xd3\xf6j\xefr\x9d\xad\xa4V\x91\xf3\x19;\x7f\x88\x0c\x9f\xc6\xa8j\xc3\xccT\x10\xb8v\x94\xedQ\xd4\x13\xe9\xed]b\xa5\x98O\x9d\x8co\xeb\xbc\x02?*\xcb])_\xc4\xb6\x8f\x10\r\x0e\xff\x001\xf6\xe3\x07\x039\xc7nq\xedZr\xd9Y\x0b\x9a\xee\xec\xeat\xfbQc\xa7[\xda\x81\x8f*0\xa7\xeb\xdf\xf5\xab4R\xd6\xd6\xb1\xce\xc5\x14P)h\x11$\x1f\xeb*\xe9>\x95J\x0f\xf5\x95r\x80>l\xa2\x8a($(\xa2\x8a\x00))h\xa0\x02\x8a)\xb9\xa0\x07QI\x9e(\xa0\x05\xa2\x934\n\x00)h\xa2\x80\x14\x0e\xf5j\xc6\xd8\xdd^\xc5\x12\xf5c\x8a\xaa:WC\xe1\x18VMa\x1d\x86v\x0c\xd4T\x97,\x1b4\xa5\x1ei\xa4z\x1cV\xebi\xa6\xa4(\x07\xc8\xa0VC\xcav:\x8eN\x7f*\xd9\x9etHY\x99\x80\x18\xceI\xae\n\xf7Y\x96yd\x86\xccg$\xee\x90\xf4\x03\xda\xbc\x98\xa7)hz\x8a\xc9j\\\xb9\x92\xde\xd6-\x83j\xb1\xe5\x8fsY\xad\xaa\xb3\xa8\x8e\xd9J\x8e\xe7\x1dj\xa1\xb7O3t\xd33\xbez\xe7\x8a\x90\xb0Ncn=\x1a\xb7\xf6n\xf7ds\xe9dHw\x90Y\xc9=\xf85\x08\x93\x92X\xf3\xd0c\xbdT\xb9\xbbu\x1bx\x04\xf4#\xa5f=\xd4\xa1\xba\xf0;\xd2j\xe5)Y\x1b\xedt b\xa9&\t8<f\x9e\xb7-\"\x80\xcd\x82\x066\xfa\xd6E\xb3\xac\xf2\x00\xe0\x96\x03\xae1\xcdiF\xa0\xa9\xc8;@\xc6k);\x1aEsnr\xde$Ic\xde\xe8\xdf*\x8c\x00=+\x82}\xf2I\xf3\x12r{\xd7\xack\x11\xa3Y\xb2\x05\x03#\x80=+\x89\x97Li\x08eLm\xee\x05t\xe1\xaa+Y\x9c\xd8\xaam\xea\x8ck8\x84\x17\x91Iqk\xe6D\x87qO\xef{\x1a\xeau\x0f\x12\xdd]\xd8[\xda\xdb\xa4\x10A\x0e\xf2\"H\xc2\x8c\xb1\xc9\xacH\xad\xa5F\xdd\xcf'\x19>\xb5k\xc9\xbdb\x02\xdb\x86\x07\xb9\x15\xddv\x96\x8e\xc7\x12J\xfa\xab\x98\xda\x8a\x19<\x99R\x05Y\x89\xf9\x82\x0e>\xb5\xd9\xf8#]\x10N\xd6\xb7\x12`\x1e@&\xa3\xb3\xf0\x96\xa1\x7f\x19\x7f:%@\x01 u\x1f\x85U\xd6\xbc/&\x95\x19\x9d\\\xbe\xcc\x10\xdd\t\xaej\x9e\xce\xa2\xe4oS\xa2\x9f\xb4\xa7.t\xb4=d]\xc3%\xbe\xe8\xe4\xc3g\xa85\xc3x\x8c\xc7\n\x0c\x02p\xc4\x83\x9e\x9e\xb5CB\xd5JZ\xa8vf\xdb\xc6\xdfZf\xa5x\xd3\xbb#'\xcb\xce\x08\xfeF\xb9\x14y%\xcat\xf3)F\xe4Z>\xa3\x1a\xea\x89+\xb7\x06@0zW\xa1Iu\x1a\xdd\x88\x14\x90\x8e\xb9\xc5y\x8e\x99\x1cR\xdd\x0f3\x039\xc8\xf6\xf5\xae\xafQ\xbe\xf2c\xb6pyR\x01oQQ\x88\xde\xc8\xd3\x0e\xaf\xa8\xfb\xa2D\xee\x8c\xe4\x13\xcf\x06\xab\xd9\xcc>\xd0\xd13\xe1sW.\x7f}-\xbc\xe9\x8d\xae\xb8o\x7fz\x86]$E\xfb\xfd\xe7y<\n\xe2{\xd8\xef\x8bI\x0be\"[\xebk$\x0c\x0e\xd3\x9c\xfaW\xb6\xe8\xb7\xc6\xf6\xc5\x1c\x91\x9cs^A\xa7X*\x90H\xcb\x1eI\xc5z\x1f\x85\xfc\xc8\xce\xcc\x9d\xbe\x95\xdf\x82\x93\x8c\xf9V\xcc\xf3\xb1\xc9N7\xea\x8e\xbe\xa8\xdcu5xt\xaaS\x8e\xa6\xbdc\xc99\xfdn\x03$q\xc8\xa3\xe6RG\xe7Tb\x85\xe5\nH\x1b\x87b\x7f\xc2\xb7/\x80\xfb,\xcc\xcb\xb8*\x96\xc7\xd3\x9a\xc8\xd2/\xed\xef\xed\xfe\xd3j\xd9\x0b\xf2\x9e0A\xac\xa4\x92\x99\xd7I\xb7N\xc5\xe8t\xfb\xa27HW\xca\xf4(\x14~d\xff\x00J%\x89\xa3R\x0c\xab\xb4\xf1\x85n)\x92J\xccxY\x01=N\xec\x9f\xd6\xa2\x94\xdc\xa0't\x8c=\x1fn?\x90\xa4\xda\x15\x9d\xce\x7fU\x87\xef\xecn@=zf\xb0\xaf.\xae\x9a\xd1\x12+\xcf \xa7iA(\xc0\xf68\xf7\xe9]\x8c\xd6\x86h\xdb(\xa4\x91\x8d\xbe\xbf\xfdz\xc4\xbc\xb4\x16nQ\x80%\x89d\xdc@<\xf0A\xfav\xaev\xac\xee\x8dy\xae\xac\xce6\xd7O\xbf\xb9\x97b\xdb\xad\xbc\x85\xbegi2\x8c3\xd9\x87\x7fc\xf9\xd5\xa8\xed \xb3\x9cEspRb\x00\x11\xc8HV$\xe39\xc6Gz\xd97v\x81\x16+\x94H\xe2\x9d\x80\x019\x04\xf2\x08\xfc@?\x9dAt\xd6\xfa\x82\xc7\"C\xe7\xcc\xa4-\xb4\xbb\xb0\x8a\xa4\x86P{\xfa\x8fQ\x91ZFN\xda\x98\xc9.\x84\"\xc6\xce\xe2i\x12\xfa\xe2h.\x88|C\xbb!\x8a\xf6\x07\xf2?\x8dgL%Y\xdd\tX\xb6(\x7f*V\x19\xc61\x90{\x820>\xb5\xd4\x99\x90\xcbqmw4-1F\x11\xb2\xaf8#\x9c\x9f\\\x16\xfc\xbe\x95\x91e\xa2\\M\xa7\xb0\xd5#\x882M,V\xee\xee3\x83\xc2\x8fp\x0eN}\xaa\xefr,f\xc1tM\xfc_h\xb7q\x95b\x16>\x15\x8a\x83\x9cg\xa1\xc74\x82\xee\xde\xde\xfe-\xb2=\xc4\x17\x8a<\xb6g\xfb\xaes\x85>\x9cc#\xda\x9f\xa8\xe9\xb7:d\xf1\xdb<\x8d.c\xc3exl\x90F=3\xd2\x8b}/\xfbF7\xbd\xb3\x89\n\xa1\x11M\x03\x8d\xbb\xc0?{\xaf\x0e1\xd7\xa9\xa6\"=f\xc0\xdf@`\x89\xe2F\x85\x960\xa0`\x91\x9e\xbe\xfc\x12sU4\x9d-\xf4\x82'\x972\\J\xa5D\x8a8\x038\xc2\x9f\\rj\xc5\xfd\xb9\xb9\x8e\t\xadQ\xc4BB\xcd\xbb\x89\x14\xf203\xc1\x1f\xe1Wm\xae\xda+\x94\xb7\x9d\x99\xa2\x91\x83\xfc\xe9\xec\x0e\xd5\xf6$W=D\xde\x87E7mN\xa3B\x1b\xd3y|\x0589\xe3'\xd2\xb6\xdfI\x17\x7f<S\x85by%\x9b?\xfdj\xca\xd3.\xac\xa4\x99-\xe1A\x15\xca\xf5v\xe8\t\xeaG\xbfj\xd4\x9a\xde\xf8\xca>\xcf.\xf5^\xfeg\xcd\xf5\xc0\xaa\xa6\xd2\xd0%w\xa9\x9dy\xa3\xde\xd8D_\x05\x97?y_x?\xd6\xb7\xbc%`\xd0\xdaIw6K\xccp\xa0\xf6Q\xfe&\xaa\x06\xd4\xe1+\x10y\x99\xa48\xc3\x02G\xeb]T1\x88`\x8e0\x00\n\xa0`\x0e+x\xafz\xe6u[Q\xb1!\x1e\x94R\x8e\x94\x84b\xb59\x82\x96\x92\x94P\x04\xb0\x7f\xac\xab\x95J\x1f\xf5\x95v\x80>l\xa2\x92\x8a\t\x16\x8aJZ\x00))1\xcd-\x00!4QF8\xa0\x02\x8a)(\x01GZu \x1d\xe8\xa0\x02\x94RS\x94w\xa0\x05\x03\xb5u^\x1bht\xeb)\xb5\t\xdbh\x07\x00\xd7-V\xae\xaeA\xb3\x86\xcd\xba/\xcc\xcb\x9e\xa7\xdf\xfc+*\xd1r\x8f*6\xa3%\x19]\x8f\xd5|Ss\xa9\xca\xdb\x03,\x00\xfc\xb8\xefT\"\xd5\x18|\x80\x15\x1e\xd5dN\xc20\xbf*\xa8\xe800*\xabE\x13\xfc\xc5@'\xba\xf1U\x1aih\x81\xceM\xdd\xb2e\xb9V \xf9\xb8>\x84\xd1-\xd1\xf9An\x07\xa7CT^\xd9\x0b\x8d\xa4\x81P\xcc\x8f\x01\xe78\xa4\xe0\x86\xa6\xc9\xae&g\x1b\xb9\x18\xf7\xa8\x0c\x92\x0c\x107(\xeb\x9a\x83\xcc\x1bs\x9e*\xc4RF\xf1\x95\xe3\xe9\x9ek\x9eq\xb1\xd3NjF\x85\xad\xd8\xd8\n\x95R=\xbf\x9dj\x0b\xf4\x99\xd5B\xf5\x1c\x80zW+\x011\xcc\xfdq\x9ey\xab\xf1\xb6[h \x8c\xf0zW%E\xa9\xd5M\x9a\xb7H%^A?\xd2\x9dm\xa7\xa0\x8c9Q\xd3\xee\xb7CQDXapJ\x11\xc6OCZ\x96\x8f\x98v\x9c\x1e*#.R\xe5\x1b\xa2\xb5\xae\x8fh\xad9X\xd1\xd1\xce6\xb2\xf5\xad{-2\xda;S\x98\x80R1\x8e\xa3\xd8\x8a\xaeJ*\xa2\xafU\xc9\x04w\xa9%\xbe[[M\xbeg'\x9cg\xbdi*\xda\x19F\x8e\xa4\x93\xc7\r\xb0\x0c\x88\xa3\xd4\x01Y70-\xfby>X|\xf2w\x1c\x01U\x1a[\xcdF}\xb1\x96U\xfc\xb3]F\x97\xa5,\x08\x0br\xe7\x92\xc7\x9a\xe7\xe6\xbb\xb9\xd1\xc8\x928\x99<:m\x11\xcc#\x00\x1c\xf1\xda\xa8Mj\xcd)(\x84\x91\xf3}k\xd4\xefm\"X\xf9\x03\xdf\x15\xc9\xcf\xa7\x16yB0\xdb\xebIUi\xea\x1c\x91\x92\xd0\xe3\xadB%\xe0\x89S\x7f\xcd\xd4\x0c\x11Z\x9a\xd2n\xb1U?{\x15+YIor7\xaey\xfb\xe3\xb1\xf4\xa3V\x8d^\x1c\x97;\xfd1U9\xa6\xd0R\x85\x93,\xf8FQuh-\xe7`\xdeY\xf9ry\x15\xb1|\xc9=\xda\xc1\x18\xc9^\xb5\xc7i\x1ee\xbc\xc2H\xc1\xc9\xe0\x80k\xae\xd1\xc3\xdc_3\xb2\x11\xc5g-\xcd\x12\xd2\xe6\xed\x9d\xb8\x86\x10B\x02\xde\xf5\xd5\xf8whFf\x00\x1f\\\xd6:\xa2\xac|\xe0q\xde\xach\x865\xbea\x9e\xbd\x005\xd5\x85j5\x11\xc5\x88NPgh\x08\xdbU'\xefV\x97\xee\x8a\xab?z\xf5\xcf(\xcf\xb9\x1b\xa2u\xf5R?J\xe2\xf4\xa7\x92\x0f\x0f\x83\x08\x0b2;oLu\xae\xdaZ\xf2\xd9\xb5{\x8d;]\xbf\xb1\x9a\xdc\x10\xf2nY\x07\x1cg\xa7\xeb\\\xf8\x85\xa2g^\x16Vm\x1dv\x9dw{t^=\xecx\xc6\x17\x0b\x83\xed[\x10'\x97\x1f\xfaD}s\xce\xe3Y\xda@\xb5\xba\xb2\x13\xfc\xab/q\xdb\"\xabjW\xbb\xa3Kx\xdc\xc8\x18\x90T\x92\xacq\xcf\x06\xb3.N\xefB}SV\xb4\x82!\x14*\xc5\xd8\xf0@\xc8\x15\xccj\x06K\x97\x95M\xc2\xbc\xa62\xca\x0f\x04\xe4\xf3\xf9g?\x85^+\x05\x9aF\x144\x8a\xef\x95\x97<\x9c\x8e\x87\xdf\xad@\xb1\x12\xd3\xaf\x93\x86,\x06\x08\xcf\xcb\xc7C\xe9\xc1\xfdi$&\xd1\x9c\x96\xa5\xcf\xee\xad\xd4\x99\x02I\xe5\x1ey\xe8\xdf\x98?\x98\xa4w\xb4\xd3.\xb6H\xb9\x87\x90\xf1\xaer\xc4\xf4<{\x06\xad\x99\xadD+\xe5\xc6\xe5K\xb7\x03\xdf\x1d\xcf\xf2\xf5\xe9YP\xc6\xd6\xf3\xb4\x12\xde\xa1Y9\xc8\xea\x08\xfe!\x9fq\xd0\xfa\xf5\xaaH\x866{\xd6\x96q2\xc7\xb8H\x1f\x95\\d*\xb0?\xf0<p~\x80\xd5\x04\xb4\xbb\xf1\n\xdbK\xfd\xa1\xbeHP\xfc\x8a\xdc6\x14\x91\x9fs\xb7\xadt\xf6\xba;\xdc\xe93\xba\x98\xa4g\xf9\x90m\x18B2\t\\{s\x8fr+\x97\xf0U\xda\xdb\xf8\xca\xfe\xc1\xe3\nw\xa9\x00\x12z\xf0G=\xbaU=5BZ\xe8\xcd\xdb\t\xa1\xf1g\x86e\x17\x90$z\x84M\xcc,\xc4\xba\x10s\xf5\xeeqY~\x1c\xba\x8duo\xb3\xcc\xa1^v=F7\xb2\x0eO\xd7!s\xfe\xe9\xf5\xad\xbf\x12i\r\xa0\xeb\xa3\xc4\xb6_\xea\xd56\xdd\xc4\x07\x05y\xf9\x87\xbeNMsr\xc2t\xff\x00\x1d@<\xbd\xf0\xcd:\xc9\x10?2\x1d\xd9#\x07\xb79\x1f\x8d;\x89\xa2\xa4\xcd(\xf8\x95}\xa5\xc3n\x0c\x182\xaa\x8c\xe1AA\xd4zd\xfeu\xb5\x05\x85\xb8i^E\xf9\xad\xf3\xb4\xa1\xc8\x188V\x1f\x8e\xdf\xd6\xb4\xb4-\x19\xcf\xc4}z\xf9\xf9\x13F\x862H$\x02y\x03\xf1Z\xe5\xee\x9e\xe6\xd3\xc4\x97z^\xf2T\xa6\xd8\xd9\x8e6{\xfd\x00\x06\xa6J\xefB\xa3t\x8b\xc6\xd6m2Q(\x90\xcaC.e\xda\x08\x04\x8c~}\xab\xab\xd35\xb6k2f\xda\x87w\x0e\x07\xde__\xa7\xf9\x15\x90\xc9m\xa2\xa2+s\x13\xc4<\x94\x7fP:\x9c\xf6\x1f\xa9\xaa\xd6\x97\x8f\x1bA,\xd3\xb3,\x84\xe7h\xc6\x00\xc0\x19?Z\xcd\xab2\xd3\xbe\xe7m\xa6K=\xce\xa1\x13\xb2H\x91\x0c\x9c\x93\xb7w\x1cq]0\xe4p+\x9d\xf0\xf9\x91\x86\xe6b\xeaxS\x9e\xd5\xd2\x0e\x00\xae\x9a?\t\xcfU\xdeB\nZ;P+S \xc56\x9dF3@\x12C\xfe\xb2\xae\xd5(\x7f\xd6U\xda\x00\xf9\xb2\x8a(\xa0\x90\xa2\x8a(\x00\xa2\x8a(\x00\xa2\x8a(\x01\x0fJA\xd6\x82h\x1dh\x01\xd4Q@\xea(\x00\xa7/JZC\xd2\x80\x1b,\xc2\xde\x17\x97\x19*2>\xb5\xcb\x8dbh\xae7H\xeaY\xcf\xdd=\xebv\xf1\x94\xec\x88\xb0\xf9\xba\x9e\xc2\xb8\x8d~)`\xbb6\xc1\x1f(z\xe3\x8f\xa85,\xb5\xb1\xd4\rd\x94\xe8q\xfc\xabB\xd6\xf9%\x8fc0\x07\x1d\xf8\xae:)\x1b\xc9\x85\x9b;\xca\x8c\xfdin\xb5\x05\xb2@K\x1d\xd8\xe0P\xa4\x0c\xeb\xde\xe4\x02J\xb7z\x8ak\xd1\"mp\x08\xe9\x9a\xe3\xed\xb5\xa9.dT\x04\xe4t\x07\xa5k\xc1~\x8f\x11YG\xcc?\nm\x8f\xa0\xfb\x82\xd01\x04\x1e\rOh\x0c\x91\x89##9\xe4\x13Q\xea\xd7\n-QS\x1b\x9c\x82O\xa0\xa84\xe9\xbc\xb29\xfc\xff\x00\xc6\xb3\x9e\xc5\xd3\xdc\xd7b\x03\xfa7R\x01\xebO\x0c\xa1\x97'\xe8\x05Py\xc7\x99\xb8\x8e\xbd\xf3Oi\xf0\xd8<\x91\xc5q\xb4v\xa6nAu\x90\x10\x92\x05oZ\xc2\xcbd\\\x9022\xbe\xb5\xc8Y\xcc7d\xa9RF2zg\xda\xba\xcd>\xe0\xb5\xb0\x81\x88+\x8e3\xdb\xe9\\\xb5\x15\x8e\x9an\xe4\x1fh\x91\x98\x05R\xc4Tf\xce{\xb9#.\x08\x1b\xbb\xf7\x15ym&\x8aq\xe5\xf2\x8d\xd3\x8eqWT\xf9\x92,b2\n\xfaV\x17\xb9\xbd\xecO\xa7\xd8%\xb7\x0cq\xe8kz5+\xd0\xe3=\xaa\x85\xbd\xb4\x81\t\xdeG\x1fu\x85^\x8c\x94\x1f>\xd21\xceMZ\xd0\xc6N\xe3.b.\x9c\xf0+\x1e\xe2\xd9`\x98yg\xef\x0ex\xcdi\xcbs\x1b\xa9\xd8r+:Y\xc2\\\xc2\xe0\xfc\xa0\xf2z\xd6r\xb3e\xc2\xe9\x19\x17H\xden\xd6A\x83\xd3=+7P\xb3>I\x0e2>\xbd+\xb9\x9e\xde\xce\xf2\x0e\x19I?\xc4\xbd\xab*]%\x95Dk.\xf0Gz\x1a\xb1\xa4f\x99\xc8i\xf0F\xb8\xdd(\x03\xd4\xd7c\xa5,1\xba\xf9d\x1c\xf7\xf5\xac9\xf4yP\xb2\"\xed#\xd4U\xdd.3j\xeb\xe6\x13\xb8u\xa9l\xd1\xc6\xf1\xd0\xeb\x99C'5B\xde\xe1l\xef\x03\x02T\x03\x82j\xfa\xcc\x86\x00A\xc8\xf6\xacm@\xab+\x18\xc1\xdd\xef\xde\xb7\x83\xd54qJ/Tz]\xac\xa2Ku`r\x08\xebQOX>\n\xd6\x13R\xd3\x1e\x068\x9e\xdc\xed`O8\xed[\xd3\xd7\xb7\tsE3\xc8\x9cyd\xd1JC\x8c\x9fJ\xf2_\x15\x19\xae\xf5g\xfb6]\x8b|\xd2\x81\x90\xbe\xd5\xe8^ \xb9\x91a\x10\xc6\x0e\x1c\xfc\xc4\x1cq\xe9\x9a\xc0\x8e\xc5V-\xd2\x90\x1b\x93\x93\xd1Ec^W\xf7M\xa8F\xde\xf1,m\x1e\x9d\xa1C\x0c \xcd#&\xef\x9f\xe5\xfc:\xf5\xaa6\xd7,m\x84\x93\xa3H\x8ew#\x12\x01\x07\xd3\xd4\x1a\xbb\xf65x|\xc9<\xb9\xd5NT\x81\xb7\xbf\xd7\xa5M$btr\x88\xe4\x83\xbb\x12`\x85\x18\xea\x07^\xfe\xb5\x92\x91\xa5\xb5*Z\xc6\xb2\xa0\xf9X\x12\xd9\xc9\xea\xa3\xd4\x0f\xc4\xd6\x9c>Dg\xcad\x18\xed\x9c\x90@\xee+>%\xbbH\x8bmf\xda@\xfd\xda\x82\x07\xe0\x0e+&\xf7\xc4\x10\xd98k\x99E\xb8\x1ca\x98\x06\x1ct\xc5\x0eV+\x96\xe6\xd5\xd3$\xdb\xcc*\xca\x0e\x06A!O\xa7_\xe9Y\x17ZL27\x9f<\x80>\x7fv\xcd\x9d\xa5\xbbd\xe0~\x82\xae7\x8bb\xb7\xf0\xeb\xea\xd0Z=\xdaD:.2}O5V\xda\x7f\x11x\x86\x08o\xfe\xc7ie\x03\x8d\xc22\xc5\x9c\xfa\x12{~T\x94\x9d\xae\x16\xb3\xb1\xd1xc\xcd\x86\x1f.H\xf6 8\x1cq\xeb\xc1\xac\xdb\xdf\r=\x97\x8b\xd3S\xb5\x87|w\x05A\x00c\xcb#\xa9\xfc@\xac\xc7\xb9\xf1F\x9d{;0\x8a\xe9\x11\xb6\xc9\x1a\xb9\xca\xb7\xa6k\xad\xf0\xbf\x8a\xed\xb5\xd56\xef\x1bCw\x17\x12C'P}\xbdhm\xbd6\x12\xd3U\xa96\xafy\x1cV\x05.ab$]\xa4c\xf0\xfex\xaen\xe7N\xcc\xf0\xca\xd3#\xacRn\x08\xea\x7f\xbe\xcc\xc3\xa7\xaf\x03\xd0Wm\xab\xda\x03\x0e\xf2\x07\xca2\rr\xb2[\xc3o\x16\\9\x0c\xc6V\x05\xb3\xf3n\xe7'\xf0\xedV\x93\xbd\x88v\xb5\xc6iH\xf6\x9a\xcc7\xa2f\x91.\")&W\x0c\xaf\xd7\x06\xb3\xb5\x8d>;\xdf\x1c\xe9\xf2,\x04\xa9\xfb\xe4\xa6\x01\x1e\xe6\xb5\xad\xad\"\x9bU\x81W~\xe1\x86\xdd\xd3\xf0\xf7\xe8+\xa3\xbf\xb9\xb0\xd2\xe3\x17\x17L\x8aG\xdd\xc8\xc9\xfc\x07SQ$\xef\xa1j\xc9\x1c\xaf\x8ct\xaf\xb6I\x03(M\xc9\xc6H\xfb\xa0\x0ex\xf4\xac\xa8l\xd6\xde\xd3\xc9i\x00\xdd\xd7\xcb\x07\x9fl\x8e\x9e\xbd\xaa\xe6\xb3\xe2\xbb\xcb\xfbY\x13J\xd2\x9aE\x00\xfc\xd2\x1d\x85\xbd\x87\xa5s\xda\x0f\x8c#\xbc\xd3\xfc\xcb\xdd=\xad\x90\xb1\x8fw\x98H\xcf\xd4\xe2\x93\xee\x11\xbb\xd0\xeb\xbc3\xac}\x9a\xd4\xc2\xf0:yCr\xf5?.x\xcf\xbe+\xba\xb6\x9dg\x85\x1d\x7f\x88f\xbc\xafJ\x0f\xe7\xbd\xc2\x8d\xd1\xb9\xc0ebA_pzWe\xe1\xcb\x89\"&\xd5\x8a\x949+\xea+zS\xb3\xe5f5a\xa72:\x8aJ\x07JZ\xe99\xc2\x8a(\xa0\x07\xc4q \xab\xab\xd2\xa8\xc7\xfe\xb0U\xe1\xc7\x14\x01\xf3e\x14\x82\x823A\"g\xda\x9dM\xc5:\x80\nJZ(\x00\xa6\x9e\xb4\xa6\x92\x80\n(\xa2\x80\x14\x1e\xd4\xa3\x83M\xa5\x1d(\x01\xdb\xbd\xa9\t8\xc7L\xd0\xbc\x1aO\x98\xb9>\xddh\x1a1\xef\xdc\x1b\xed\x83$\x01\x8a\xcf\x9e\xee\xe7\x8bIU\n\x83\xc3\x15\x19\xf6\xe6\xad\xdf\x16MO\n\x0b\x06\xf5\xaa\x97a\x8b\x92x\xc7L\x8ej\r\x15\x8a\x13\x06\x17\x91\xaf\xa9\xac\xfb\xfb_:v$\xe0\x8e\x80\xd6\xab71\x96\xfb\xe1\xb85\x1c\xca\xb7K\xe6\x0f\x95\xc7\x0c\x05\x00\xd1\x8f`\x05\x95\xc1y\x94\x81\x8c\x02G\x19\xab\xedp\x1f\xe6\x00\x8c\xfbu\xa9\x05\xdc\x8b\x18\x85Q6\x8e2Fhi\x9eR\x03\x10@\xe8\x05\x0cC\xa5\x9c\xca\xa8\xa7\x8c\x0ejH\xce\x0e\x01\xe8*\xb3)\xc8n\xd5*\x03\x8c\xf6\"\x93*&\x89`a\xdd\xb4\x12z\xe4\xf7\xa8\xa3\x90\xb1_c\xf9UpYx\x07\x0e\x07\xe7SA\xb5X\x83\xf7\xbbV.:jl\xa5\xa9\xb7b\x1aB\xa8r@<b\xbakXK7,\xc0\x01\xc0\xc5s\xfad\xf1\xc2\xca\xe7?'Q\x8a\xd7:\xa9n` \xf1\x8e\xbc\xd7\x14\xe3vwBVGG\x15\xeaC\xe5\xe7-\xd8\x9fJ\x98\xb2\xe4<a\x8e=Er1\xdc\xcc\xf2\xef\x99\xf2\xa79\x02\xba[\x1b\xd4\x925!\xb8\xc61X\xb8X\xd7\x9a\xe6\xec7j\xaa\x03\xb6\t\xf4\xa8\xee\xa7\x0e\x85X\x12?!U\x05\xd8Q\xf2\x01\x8e\xd9\xa5\x16W7hY\xa5\xd8;\x01JWh\x12W\xb8\xdc\xacc\xe4\x04\x0fJ\x83\xec\xd3\xb3\x17GU\xe7\xee\xb0\xcejt\xb2\xb8\x84\xf2\x01\xf7\rV\xc3\x94\xe2@\xc3\xdf\x19\x15\x8bV5R\xec6\x08\x01 \xe0#\xfb\x0cf\xa41\xca\xf7\x001\x0c\xa3\xd3\xb5)o5s\x109\xfaS\xe3II\xcb\x1f\x9a\x96\xa4\xdc\x86EYI\x12@\xdb\x87F^\xf5\x02\xd93\x13\xf2\xfc\xbe\xa6\xb6\x96\x12\xc0\x1c\xe0Sn\xca\xc3\x0f*\x14z\x93M\xc7\xab)N\xce\xc8\xa1o40\x83\x19l\x91T5K\xa8\xd8\x1f)\xc6G\xa5N\x8a&\xdcc\\\xe3\xb8\xac+\xdd\xe9+d7^A\xedWL\x99\xee\x1e\x15\xd6\xbf\xb2\xfc]\x0b3b\x0b\x93\xe5?\xd4\xf45\xecS\xe3\x1ct\xaf\x9e5,\xc5t^\"C\x8f\x98c\xb1\x15\xed^\x19\xd6SZ\xf0\xf5\xb5\xde\xec\xbe\xc0\x1cw\x0c85\xeb\xe1\x9f\xbbc\xc9\xc4\xafz\xe1\xaa*\xefV\n7.Nk)\x97\xcd9s\xb9Gl\x0e\x7f\xfdUn\xee\xe1\xe4\x9e\xe2<\xed(0\xbd\x0ek4J^fUr\xed\xd1\xca\x81\xdb\x9cc\x1cTUw\x99T\x97\xb8,2\x1f4JD\x8c#]\xb1\xba/>\xe3\x1e\x94\xb3\xb4\x10+\xcf\xfb\xa8\xf7gqC\x86\xe7\xae{R\xcf\xa8\xda\xd8Y\xbd\xcd\xc3b5\x19h\xca\xf1\xf9\xfa\xd7\x01yu}\xe3=A\xed\xecD\xd6\xd6 \xaa\xc8\xc1\x06H=\xc7\xadO)M\x92\xeb\x9e0\x9e\xe6T\xd2\xf4H\x9eY\x8f\xcb\xbc.F?,\xd2\xe9\xfe\x04\xfd\xe2^\xea\xd3\xad\xcc\xe4\xeeps\x81\xf5\xcf\xa5u\xba?\x85\xadt\x8b\x15\x8e\xd8n\x93\xf8\\\x8e\xad\xea\x7f\xfa\xd5LM$\x17\xd2Y\xdc\x15yKe9\xda\x0f\x1e\xbc\x7f:Sm-\x07\x1dw6,,\xf4\xd9\xf4\xf9\xb4\xa4X\xe2\x12!VA\xd8\x91ZZ#y\x16\xc9ep\x82;\x9be\x08\xcb\xfd\xe0:0\xf65\xc1\xddZ\x1b\x1b\xc3)\xb9\x9a\x06`YY\xb8\x0cGA\xd7'\xff\x00\xad[6\x1e,e\x10\xc5\xab$s6B\xac\x90\x90\x1dI\xf7\xce?Z\x8e\x96\x1c\xa3wt[\xd3\xb49,\xf5-ZO6I\x8d\xfd\xe7\x9eKtA\x8c\x05\x1f\xaf\xe9\\\x86\xb1\xa8Cc\xf12\xd5\xec\xc9\xd9\n\x85\xb8h\xcf\x19'\x8c\xe3\xd2\xbd!b\xb1\xbf\x80\xa4z\xbd\xcc\x02L\xa9R\xca\xaf\xf4\xce3\xf9W/\xe2\r\x1fE\xd0\xe0\x92h\x968\x89\xc1\r\x9e$9\xe8\xc7\xfa\xd3\x8e\x8f\x9aLQ\x8b\xd9\x1e\x8b,\xf1^i\xf9V\xc8#\x9cs\\\xb6\xa9jY\x90E\x90\x8crB\xb6\x0ey?\x8fZ\xe5\xfc\x1d\xe2\x99\xf5+\xa6\xb5\xdeDJ\xe5\xb2;\xf3\xc0\xae\xf9\xca2\x8c\x90\x07\x18'\xb5R\xa9g\xa8\xdd;hP\xd3BG4R\xb2\x84d8\xf9{\x8f\xa5P\xbd\x88j>&\xbag\x97r@\x15\x10\x03\xf7F\x01\xe3\xf15\xa3s\x04\x8a\xcd*\x9d\xcb\x8ey\xe6\xb8d\xbd]?\xc6\xef\x05\xd8\x95b\xbc\xdaRt=?\x1a\x89O\x9bA\xaan\xda\x1az%\x89\xd0ty\x16\xf6\xe02\xdb\x99e\x92S\xd3ibG_lU]\x0f\xc3P\\\xf89%\xd4`\x08\xb3\xb3JP\x8ep\xccH\xfd\rt\x9a\xb7\xf6\x12\x08\xfe\xd2\xd3_\xe3\xe6X\x19\xff\x00vH\xee\xc0`\x1f\xc75\xcax\x83\xc42j3\xad\xb9\x95a\x80\r\xc8\xa9\x91\x93\xd8t\xfeT\xd6\xcf]IQh\xc9\xb6\xc7\x86\xb5\x85\xb5\x91\x1cX\xce\x7f\xd1\xce\xff\x00\xb8\xde\x86\xbbm>Y#\xd5!\x9a#\xc4\x9cc\x1cW/\x0c2^\x86\xb3\xbbq\xb6`IL|\xc0v\xfasR\xf8wT6\xb7\xc9\xa6_\xe6+\x88_\x00\x93\xcb.x\"\xb4\x8e\xb6d\xcbK\xa3\xd8c\xe5\x01\xcesO\xa8\xe2\xe60i\xf5\xdcq\x0bIKI@\x0f\x8f\xfd`\xab\xd5F?\xf5\x82\xae\xf6\xcd\x00|\xd8)i\x05-\x04\x85\x14Q@\x058t\xa6\xd2t\xa0\x05#$\xd3q\x8a(\xcf\x18\xa0\x00\x9cRg4\x13\xda\x92\x80\x1d@\xebH\x0fjZ\x00^\xd4\x8d\xc2\x13J:R\xe3%G\xa1\xcd\x03Ff\xaf\x19\x8e\x18\xae\x02\xf2\xad\x869\xe8\rg\xdc\xba8\xc9p\xc0s\x93\xde\xba9#I\xa2h\xddC+u\x06\xa8Z\xf8^+\xd9\xe2\x8f\xce+\x11\xfb\xc0\xf5\xfaTI\xdbSH\xab\xe8bIc;*M\xe4\x93\x0br\xacG\x07\xe9P\xcd\x10M\xd8\x038\xafH\xd7\xd6\xce\xd9l\xb4\xc8\xd4)\x03\x01\x7f\x0e\x05p\x9a\xc9K{\xc7\x8fnC\x1c\xb0\x1d\x14{TB\\\xd1\xbb4\x9cl\xec\x8c9\x01\x07\xa1\xf5\xa7\"\xedR\xd8\xc8\x15m\xa0YY|\x9e\xe7?J\x82\xe96!\x0c@`{w\xab2d\n\xeaX\x83\xde\xa5\x87\x7f\xcc\xa0\x8cv\xaa@\xe3<\xd4\xcb#\r\xa7#\xdf\x8aLi\x97\x9fyP\xeaW#\xa84\xf8\xf0\xce\t^\xbc\xd5h\x9d\xc9#p9\xedV\xd1\x95a\xf4#\x81\xf4\xa8f\x8bsE\\\x01\x92H\x18\xc6*H\x99Q\x19\x96^\x01\xe4b\xb1\xe4\xb8\x0b\x9c6GaU\x1e\xe9\xd7,\xbdMf\xe9\xdc\xd3\xdaX\xec\xa0\xbf\xce\xdcm\x189\xc9\xe2\xb5\xed'{\x97X\xe1a\x82wdu\xcdy\x82\xea\x12\x87\xc7j\xf4\x0f\n\xdcD#I\x8b\x92I;\xbd\xab\x1a\x94\xb9U\xcd\xa9\xd5\xe6v=\x1bJ\xd3\xfe@\xf2F3\xeay\x06\xb4\xd9\xa1Q\xb5\x06\xda\xe1\x93\xc7+$\x8dio\t\x12#|\xa0\xf45\xb7\r\xdc\xb3D^L|\xdc\xf0zV3\\\xa8\xd6\x1e\xf34.\xe5G\x8c\xe3\x07\x1e\x86\xb0\xe5\x92\xe6I\x02\xc5\xb8\xaf\xa0\xa9\xed\xed$\x9ef\xdf#m'\x81\x9a\xd7\xb7\xb6HG\xca\xa3\x8a\xe4z\x9dI\xf2\"\xa6\x9dis\xb7{\x97\xc1\xea+I-\x98\xb7Z\xb7\x08p\xbc\x03\xcfj\x90\xec\x817\xec\xe4\xf75j\x0bs77q\x9b\x0ch\t`\xa0\x0eNk\x9c\xd65+f\x98[$\xbb\xdc\xf5\xc7j\x8fY\xd7%\x99\xda\xda\xc72\xc9\xf7K/E\xa8\xb4\xdd\x15a_:\xe0\x16\x90\xf2w\x1a\x99>m\x11\xa4\x12\x8f\xbd#J\xd2?\"\xd8ma\x9fR+\x0bTS#\xb9.3\xef]\x11\x968\x93\x1c\x01\xefXz\x9c\xb6\xd2F\xca\xae7{t4\xe3\xb9\x1b\xeap:\x8b\x95\x9c\x96?6qZ\x1e\x0f\xf1L\xfe\x1d\xbf\xf2\x89g\xb4\x91\xb1,~\x9e\xe2\xb25\xc0\r\xc7\xcb\x9e\x0eEP\x81\xdf\xcc\x0e\xa4\x03\x8c\x11^\xad\r\xaey\xd8\x8d\xec{t\xb3\xdb\xdf\x934\x12\x90\x1b\x95\x07\x83\x8cv\xac\xfdV\xf2\x1b\x0bQ$A\x19\x9b\xa2?\x0c\xc7\xf05V;y\xa1\xd0\xac\xe5\x8cH\xec\xa8\xa5\x941\xe8{\xd5\x9b+5\x96\xe0\xcf4\xf0\xca\xca\xb9\x01\x00v\x1f\x87<\xd6rw\x9d\xca\x8a\xb4L\x0f\xec\xcdk\xc4\x1eY\xbe\xba\xf2\xa1\x91\xb2a\x8d\xba/`\x07\xff\x00\xae\xbb\x9d3G\x83D\xb3\xdbm\tq\x8d\xd8s\xf3g\xfc\xfd*\xcd\x8e\x91\x07\xdaE\xcc\x93<\xa4r\x036\x07\xe4:~u\xac\xb1\xacm\xe6\x10\x17\xd0\xe7\x8cU\xd8\x9b\xa3),\x9awW\xbae\x865\x19\x08\x9fx\x1fR\xdd1F\xb1\xe1\x8b\rf\xd9ZEu\x90\x0f\x92e\xfb\xc2\xac\xdd\xea\x91\xc0\xb24\x8d\x94Q\xb8\xfc\xbf\x97Z\xe0'\xf1v\xb3y\xac2h\xd6fP2\x1aY\xce\x14\x1f\xcf\xa7\xaf\xf4\xa9\xbaN\xc3\xb4\xb7F\x9d\xc6\x99\xe2\x8d1DL,u;0r\xa6bR@=\x01\xfe\xb5\x97.#\xf3'x\xd6S0+\xe5\xa1\xc9\x8cg\x81\xd3\x1f\x8fZ\xbd\x1e\x9d\xe3\x8b\xb1\xba\xeb[\xb6\xb6^\xa5b\x8b$\xfbv\x15\xab\xa7h\x17\x93\xfc\xf7\xf3\xc6\xf0\xb1\xf9\x91\"\xda$\xf7\xf4\x14\xa4\xbb\x15\x1d79\xeb+iR\x7f&f\x00\x0c\x10\x88\xe4\x95=\x80#\xb5K\xaa\xd9\xb5\xe2\x08\xa5s\"\x03\xf7O\\\xfb\xf4\xf5\x1d\xab\xa4\xba\xb7\x82\xc2-\x83*\xc7\xber\x1b\xdf\xaeMd\xe9v\xcdw\xa9\xc9n\xf6\x92\x94D\x0cn&\xc0V\xc9\xfb\xa3\xb9\xac\x9aOCH\xb7\xbfC\x02\xc2\xceM\r\x1ex\x14\xc7\xe6>\xe6y8\xd8\xb8\xed\xeak\xa9\xd3u\xd4\xba\xb3\x04\xc8\t\x03\x9c\xf1\xcdl\xea\xda?\xda\xf4\xe6HG\xcd\xb7!PW\x8e\xbe\xbd\x1d\x95\xe4\xb1N^\xd3n|\xc5\x99p\xcaA\xec;\xd4r\xcaR5R\x8f)\xeagZM\xc5I'\xa8\xeb\x91\\\x97\x88\x1dn\xeecd!\x8a|\xaa\x01\xe4\xf7\xe3\xfc\xf7\xac[=J\xd2\xf9\x05\xc47\xf7\x12F_\x83\x9cs\xf9T\xba\\\x83S\xd6\x96\xd6\xce\xe5\xe7\xda\xe0\xbf\x19\xd9\x8es\x9a-fW#J\xeffl$\x0fyc\xf7K\xc8\x18\x12J\xe1\xbe\x95\x9c\x0b\xad\xd6\xcd>\xc1\xe5\xba'\x1c\xe0*\x8e\xe4\x9a\xe8.\xac\xde\xdfSXvH\xb0I\xd2Q\xca\xb3\xe7\x04dt\xacMJ\xc7_\xb2\xbe\x96M.u\x10\x8e|\xa6\\\x93\xf8\xd6\x91\xd4\xe7\x96\x87[\xa4\xf8^+{s=\xd4\x8c\xf7r|\xccGA\xec=\x85b\xdeX\xdaG\xe3KYV\x08\x9ev!J\xba\xba\xe7\xdf \xe0\xd5\x8f\x0bx\x90]+[^\xcf$W\xc8p\xf1\xc8\x0eO\xb8\xadkX\xed\xef\xfc@7\xa0\xf9\x0eAu8\xfc\xbaV\xf1\xd1\x98N\xf6gs\x08\xdb\x12\x83\xc7\x1d\xaaJj(T\x00\x0c\x0fJZ\xec9\x05\xa2\x8aBp3@\x0f\x8f\xef\xd5\xd1\xd0Va\xb9H\xd8\x12j\x19\xf5R~X\xc5\x01c\xc1)sM\xcd\x19\xa0\x91\xd9\xa5\xa6~\x14P\x02\x93E!\xa6\xe7\xeb@\x0e\xcd&i3H\x7f\x1a\x07asE74g\xeb@\x87S\x81\xedM\x06\x94u\x14\x00\xe0{S\xc1\xedL\x1di\xc3\xa8\xa0\x07U\xbbF1\xc8\xac\x80\x1c\x1a\xabS\xd9\td\xb9\x8d!]\xce\xc7\x00z\xd4\xcbcH?y\x11\x98\xa6\xd5\xbcO5\xe9\xc0\xb6\xb5R\xa7\xdd\xb1\x80?\x9da\xea\xb6{\xe6c\x8eI\xafN\xfe\xcb\x8e\xc7O\xf2\xb07\x92ZB;\xb1\xeb\\\xae\xa1`\x06[mB[\x1d\xaa\x07\x9fM\x13\xdb\x9d\xcaH\xc5A,2K\x97\xe7\xe6\xe6\xba\xa4\xd2\x1a\xeaVv_\xdd)\xe3\xfd\xa3Uo4\xc2\x9d\x14\xd32t\xceRH\x99\t\x0e\x85{\n\x8cpy>\xd5\xb95\x93\xca6\xe3\x83\xc7J\xab.\x85r\xab\x95\xf9\x80\xe7\x1e\x83\xd4\xd2\xe6D{)t)F\xe59\x1djO?#\xe9]%\xb7\x85m\xad\xf4\xc6\xbd\xd4\xee\x84K\xb7(\x01\xc0\xfaW\ryp\xe5\xd9b$.H\x18\xf4\xa4\x9a\x93\xd0RN+S@\xdc&~\xf09\x1d\xa9\x04\xf9\xce\x13\x83\xde\xa9Y\xc4\xdc\xe3,\xdd\xebE\xa2\xf2\xd0\x07#=\xf1\xda\x9d\x88\xb9\x06C\xca\x07\x00\x9a\xf4o\t\xe9\xe4\xd8\xa0d\xf9\x9f\xbf\xb5p\x1am\xa9\xbc\xd4\x14\xe3\xe4\x07\x1czW\xb0h\x96\xabmj\t\xe8\x07\xcb\\\xb8\xba\x8a+\x94\xec\xc2A\xb6\xe4\x0f\xa5Y\xd9]\x89<\xb5\xc9\\\xfe5v\xd9\xe4\x99\xb6\xc4\xa5c\x1e\xddj,\xb4\xd7!\xb1\x9e{\xd6\xdc\x10\xacx\xdb\xd7\xe9^l\xa4\xe4z|\xb1\x82\xbd\x84\x86'\x00\xf4\x18\xf5\xad\x1b}\xe1~\xee\xefJ\x8c(\xc7\xef1\x8a\x86{\xb5\x88`\x1c\x0e\x9c\x1e\xb5\x17\xe5\xd4Z\xcbCI\xaf\x0c\x11\x9ezW7\xa8j\x97\x1a\xac\xbff\xb7\xdc#\x1fy\xc7\xf2\xa9\x96\x0b\xedE\xdb\x0c\"\x87\xa6;\x9a\xbd\x05\xac\x1ad8\x07\x00u\x01j\xee\xde\xe4.X\xfa\x90i\xdau\xbd\xacgb\x9c\x81\xc9\xcfZ'\xbe\x8e\x16 .\xef\xc35CQ\xd7\"\x8dJ))\xec\x07&\xb2$\xbf[\x84'\xce\n\x08\xe7-\xcdK\x97DTa}d\xcbW\xfa\x9a\x15e0\xb1'\xd4\xd6:0\xb9\x8a@\x8b\xb1\x86r\xa4\xf3Y\xf7\x93F%*\xb2\xbb\xfa\xbex\xa2\x1b\xa4\xb5\xba._\xe4\xdb\xc95\xad%\xd4\x9a\x9ah\x8cK\xf9eF\xdb\x90T\x00\x0e\xe0\rU\xb4\xb8v\x952bA\xdc\x88\xc7?\xa56\xe6\xe8\\4\xea\xe7n\xdc\x80}y\xa6i\x97\x0e\n\x81\x1c\x87\xcb<\x84]\xc7\xf5\xeb^\xad$\xd4u<\xaa\xad9\x1e\xd9oo\xe7\xe9V\xe8\xc1\xc81\xe0\xa6A\x07\xdc\x91\xd0V-\xdc\xf1X 11\xdd\x9f\x9aH\x90\x85\xc6\x7f_\xce\xb5\xf4{\xc7}\x1e\t\xf6I\x9c\x00\xeb'\xcb\x9f\xa8\x1d\x05E\xad\xf9W*7\x14M\xdd\x19F3\xed\xcfoz\xcaKFk\x17\xaa)\xd9x\xa6\xe3\xfd_\x90\xf3\x06\x18L\xf1\x9f\xa0\xc6\x07\xeai\x97^1\xbeI\xe5\xb7\xb6\xb3\xda[?19'\x1d\xb3\xf5\xac\x96\xb5\x10\x17\t:\x89\x1b\x96m\xc7\x18\xf48\x19\xfeUj\xd7PH\xa5\xf2\xde\x04!S&LrG\xb0\xff\x00\xeb\xd6Wf\xbc\xb1\xecQ\x96\x1ds\xc4\xb7\xabo\x7fr\xb1A\x8f\xba\xacp?>3^\x85\xa5hv\xbaE\x84I\x04h]G\x18\xc99\xf5\xac\x18.m.\xb6\xcb\x0c\xednq\x92\xc6\"\x80\x0fv5\xd1i\xf2H\xdf2\xc8\xb3!\xe7\x8c\xd6\xb0N\xfa\x99Tk\xa1f\xda\xdek\x89\t\xbb\x8c\xe0\x1c\x85\xdckJP\x91\xda\x9d\xea\xca\x8b\xc9*N\x7fLR)\x91U\x18(\xe9\xf7s\xfd*\xc7\x9b\x94\xdd0\x11)\xe0d|\xcd\xf4\xad\x149Q\x9b\x9d\xdd\xce^\xfbA\xd0o\x0c\xf3\xeaM4\xcc\xca\x0b*\xbb\x82\xab\xd8ps\x9a\xa6\x17\xc3Pj+-\xb3\xde\xc7qw\x01\x890\xae>Q\xe9\x9e\x86\xba\t\xb5k\xa6\xb9X4\xeb\x18f\\\xfc\xf2\x190\x17\xf4\xe4\xd3\xa3k\xc6r\xefkg\x01\x8d\xf1\xb9\x9bq\x03\xd7\xda\xb3P\xe64s\xb6\xe1\xe1\x99\xa0\x8bM\xb6\x86\xde\x1b\xe6A\x95\x0fp\xbf1\xe4\xf2\xd9\xabZ\xf7\x86\xb4o\x11Y\x98\xf5\x0b\x08]\x88\xf9dt\x1b\x94\xfa\x83S\xa5\xc4\xf8Pg\x89v\x9c\xb6\xd5\xea;\n\xc8\xd7<Gk\xa6[\xbb]\\\xae@,\x01\xeb\xc7\xa0\xae\x98G\x95X\xe7\x94\xb9\x9d\xcf\x13\x93I\xbe\xd3|H<5kf\xe6x\xe4+\x1a\xa8\xe1\xc1<9=\x869&\xbd\xd3H\xd0t\xdf\x0fi1\xdb\xdb\xda\xc6\xb2*\x01#\xa2\xf2\xed\x8eN{\xd7\x9fxWZ\xb5\xd4|Uuv\xe5\xbc\xc6\x1b\"-\x9d\xcc\x83\xff\x00\xaf^\x92.\x1bn#x\xfd\x95\xa8T\xe3\x1b\xdb\xa9s\xad9\xdb\x9b\xa1\xcd\\\xdb\xe8\xf7wO\x0c\x92\\\xda\xca\xce\xcc2\xc4\x0c\xf48\xcf\xe1U&\xb0\x96\x0b\xe9\x1c\xdc4\x80\x0c/\x19\x18\xfc1\x8a\xe9/.f\x86\x06\x96{Dp\x0f\xccW\x9c\x0f\\b\xb9k\xfdUa\x95\xda=\x8c\xac2\x9b_\x9f\xcb\xfc?Z\xe7pI\x9a)]hrW\xb3Go\xe2T3B\xef)\xfb\xb8\x93f?\x0cg\xf5\xae\xcf\xc33\x19\xf5\x16e\x8c\xb2/\x1d\x01\xc5y\xfe\xb9qwq\xa8\xc7q\x1a\xef\x03\x8d\xc4g\xf0\x07\xd7\xeb\x83]\xdf\x81\xd6^<\xdd\xb9s\xbb,0\x7f\x1fzq\xf8\xd0\xa7\xf0\xbb\x9e\x8c\x8e\x08\xfe\x94\xe2F3\xd2\xa1i\x15W\xe6\xc1\xaa\x93\\\x920\xad]\x87\"E\xb9'T\x1dj\x94\x97L\xc4\xe0\xf1U\x9eF\xc7<\xd4~g\xf9\xc5\x03H\x95\x9c\xb9\xe6\x99\x91L\xdd\xf5\xa6\x93\xcd\x05X\xf3\xcf\xec\x95\xf4\xa0\xe9\n{V\xf6\xda\x02g\xb5|\xbf\xd7+w=\xd7\x87\xa5\xd8\xc0\xfe\xc7_J?\xb1\xc7\xf7Mo\xf9t\xbb)\xfdv\xb7q}^\x97\xf2\x98\x03H\x03\xaa\x9a?\xb2\x17\xd3\xf4\xad\xfd\x82\x93m/\xaeV\xee\x1fW\xa7\xd8\xc0:B\xf6\x14\x87H\\t\xae\x83o\xb54\xa0\xcd\x1f\\\xad\xdc~\xc2\x97c\x9f\xfe\xc9_J\xa9w\xa7\xacK\x90+\xa9h\xc6++S\xe2#WO\x19W\x99]\x8aXjM=\x0e[8lS\xb3P\xb4\x9f\xbe4\xf0\xc3\xf1\xaf\xa4\x8b\xbaL\xf0&\xac\xda%\x06\x9d\x9a\x8c\x1ap4\xc9\xb1\xa1\xa3C\x1d\xd6\xb7c\x04\xc31I:+\x83\xdcg\xa5zT\xdaN\x9de\xa9\xdc]\xda@\xb1\xb4\x9c\x00\xa3\x85\xc7\\\x0e\xd5\xe7~\x1a\xb4k\xddv\xdc\x0c\x85\x88\xf9\xacGl\x7f\xf5\xeb\xd1\xe5,\xef\xedR\xce\xcc5=9\x99\x9fy\x96\x07=+\x02k&\xbd\x9cB\x9c\x0e\xac\xde\x82\xba;\xc0\x16#\xebQX\xda\x88a\xdc\xe3\xe7\x93\x93\xec;Pt\xd8\xc8\x93ND\x8cF\xa8\x02(\xc0\xe2\xb2/4\xd0\xc4\xfc\xbcWg$i\x8e\x95\x99{\x0e\xdbY\xee\x00]\x90\xe3vO\xa9\xc5'\xa2\x0e]\x0e-\xb4\xb8\xd04\x8d\x80\xa0g\x9a\xc7\x97\xc5\x9a5\xa5\xac\xb1\xaa\xb4\xf2\xca\xa5\x08Q\xd0t\xab\x1e1\xd5\xbf\xd0\xda\xc6\xd6@\xb2\xca>l\x1f\xba\xbf\xfdz\xe2!\xd1\xd1\xa4\x04H\x84c\xf25\x92\x8a\x91\x85i\xb5\xee\xa2\x1b\xcdF\xf6\xf9\x12\x19$w\x89s\xe5\xc7\x9c\xe0T\x10i\xe6TY\x19\xb0\xacp?\xfdu\xb4\xb6QB?z\xeav\xfd\xd285V\xe6d\x05\xc4_,dr=\xebM\x129\xec\xde\xe0\xa9\x1d\x9a\x95\x8f\x03?x\x9a\xaa\xecnf\x11\xc68\xcfn\xf5\x19i.\x18$`\xe3\xa5u\xbe\x1c\xf0\xebqqr\xa4(\xe75\x95J\xaa\n\xec\xbat]Gdh\xf8gBH\xa2\x12\xc8\xa5Ork\xae\x81^\xe4\xec\x81\xb6\xa0\xe0\xb5dG:\xdd\xca\xb6\xf6\xa3lJpO\xaduv6\xd1A\x12\xa9a\x91\xda\xbc\x8a\x93s\x95\xd9\xec\xc2*\x9cl\x89-\xad\xd62\x04q\x97lr\xc7\xa0\xad?\xb9\x10\x03\xaf\xadV\xfbj\xa2\x15\x00\x05\xe8*\xb4\xfa\xad\xbb\x15\x83c\xc9\x9e\xbb\x0e*.\xba\t\xa97\xa8\xeb\xcb\xf8\xa0!HiNq\xb5y\xe7\xde\x8b{d\x9e\xe4Mr\xfb@< \xe9N\xb6\xb2\x85\x08\x95#p\x1b\x9d\xacy\x15nx#\x009\x91\x90u\xe7\x14$\xafq\xb9tBO\xa8\xda\xdbF\xd8\x95A\x1f\xdd<\xd7=y\xe2\x08d\x0c\xa8\xcc\xcd\xd3\xadK\xa9J\x85v\xc5\x1a\xbb\x1f\xe2T\x02\xb9\xeb\xa4v\xf962\x06\xea\xd9\x1cS\xd2[\x82\x8d\x97\x99^\xe6q5\xc6\x04\xdc\x1e\xa1FMV\xbc\xbc\x82#\xe5y\xa0\xf6\xc2\xf6\xfa\x9aAu\x0co\xe5\xa1\x1cp\xd89&\xa8G\x1c\x17\x13\xb4I\x1e\x0e\t\xe3\x92kh\xc12%&\x85\xbd\x10\x88\xd1\xd8\xe4!\xca\xa8\xef\xefY\xf7\xads,\x0b\xd5|\xde\x7f#Z\xa6\xc8\xb9\x07\xcbf\\\xf1\xef\x8a\x86;i\\H[#\x01\xb6)\xe8\xa7\xb5u\xd3Q\x8a8\xea9H\xc8[qq,\xe8O\xdf;U\x8f\xf7\x85O\xa2\xa4\xd2\xca\xd1\xbaK\x1b\xfd\xd0\xeb\x81\xfa\x93H\xcd\x1d\x94&\xdeg\xdc\xe4e\x88\x1d\x0ey5N\x19`\xb8\x912nrX\x80\xb1\x9cg\xeak\xa2\xf79\x9cl{/\x87 \x16\xda\"\x82\x1f\x82K4\xad\x92\xdf\xce\xb5\xee#[\xebb\xd2\xc0\x92\xaa!#v8\xfc=k+@\xb3{O\x0fG\x1b\xc4\xc3?w\xcdr\xd8\x1f\xa7\xe5R\xdb\xdfK4\x86\xd5\xe4\xdc\x8391\xe5W\xf5\xcfO\xebY\xde\xce\xe6\x96\xd3B1e\x05\xacj\xb1\xc0XH\xfc\xa2\x8fn\x99=\x87sTu\x05U,\x11c\x96HpX\x93\xf2\x8c\x9e\xe7\xf2\xae\x8e%[\x95P@Hm\xe3\xe7\xe6'-\xd8{\xfa\xd6\x06\xa7\xa5\xcbn\x1e\x08\xb7\xc80\x08\xf4-\xd8\x9fS\x9ej'\x1d.\x8b\x84\xb5\xb3\x1biu\x1c\x8c1\x1c` \xc1\x92c\xd3\xdf\x1f\xe7\x15\xd9h\xea\xadn\x1aB\x08=\xc8\xdb\xb8z\xe3\xd3\xd3\xd6\xb8h\xed[J\xb32\\C\x1c\xa23\x85\x0c\xd8.\xd9\x07\x9f`kCM\xd5.\xaeY\xae\xee\x19\xc5\xb0\xc9\xdb\x827c\xa9\xc7S\xe9\xf9\n\xaar\xb6\xe2\xa9\x06\xf6=\x16\x04\x8dF\xf6#\xd9Gz\x87P\x80\\\xc5\xb9\xdb\x0b\xd8\x0e\xfe\xdfJ\xe6\xf4\xddy\xaf\xaee\x92\xedZ\x08a^Ww\xca\x83\xfa\x93\xd2\xae7\x88 \x9c\x1b\x85d\x11)\x08\x81\xb8\xdc\xdd\x80\xff\x00?\xce\xba\x14\xe2\xd1\xce\xe1(\xb2\xbf\xf6E\xec\x10H\xb0\xdd\xb8`\x0ep1\xb4\x7f\n\x0f\xa99&\x94-\xe4p\xacs)c\x1a\x80\xc4\x0e\x19\x8eI\xfc>\xe8\xfc\riE\xa8\xc2\xdeT/\"\x96\x90\x86o\x9b\xb7\xf9\xcdM6\xa3i\xe4\xf9\x8d\"\x85gl\x12j\x92\x8fB\x1b\x97S\x11\xe3\xd4.m\xc4k9\xb5\x8d@\xdcq\xcf\xe7N\xb7\xf0\xe5\xa8L\xdd*\xce\xc7\x9d\xf2|\xd5\xb0\x971I\x18\xc6\x02\xe7\x83\xebN{\x88\xdd|\xb5n:\xe75CL\xc0\xbf\xf0\xd5\xb1M\xf6\xf1yR\xa8\xf9Y0\x08\xa8\xed\xcd\xdeQf&F\x1c`\x8e\rm\xdd\xeaV\xf6\xf1\x92\xd2\xa0\x0b\xdf=s\\n\xa1\xe3\x1bX\x835\xb3\x17nx\x03\xdf\x152\x92@\x93f\xfd\xe4\xea\x8a\xb2}\xa5\xe2\xc6Q\xf9\xca\xaeq\x82\xc3\xd38\xfc\xcdr\x1a\xa6\x9e\xb3]\xf9\xb6\xe4+\x06\xdb$`\xe0\xa3{{\x1a\x81\xf5\x97\xba\xd6V\x19W\xfd\x1a\xe7\xf7M\x91\x8cg\x95?\x93\x01W\xec,n\xa6\x88\x96a\xe7\xda\x86\x8d\xc1\xff\x00\x96\xb1\x8e9\xf7__LzW<\x9f3\xb1\xbcW*\xb9>\x95l\xeb'\xef\x17k\x1e\n\xb8\xe0\x9f\x7fz\xd9\xd3\xcc)\xa8\x83l\xc3p\x1f4O\xd7\xfe\x02{\xfd:\xd5u\x1b,|\xbd\xf9\x98\x8cF\xcc8#\xfb\xac}}+\x1a\xd5Yn^p\x7fz\xc7\x0c\x0f\xf0\xe3\xa8\xfa\x8a#\xee\xeaL\xbd\xe3\xd0\xc4\xa5\xc6\x7fCLs\xc5V\xd3\xae~\xd1h\x8d!\xcf\x1c89\xff\x00\xf5\xd5\x89\x0e\xde\x0f>\x95\xd6\x8c\x16\xe3\x1b\xad&@\xa0\xb6{SM1\x8d'&\x86#\x14\xdc\xd1@\x18 g\xb5\x1bM8\x0fJv=\xab\xe3R>\x8a\xe4x4\xec\x0fJw\xe1F=\xa8hI\x8d#\xd2\x9b\xb7\xd8T\x84Rb\x84\x87r2)\xa4T\x84SH\xfc\xe8\x023\xd2\xb25a\xfb\x96\xad\x96\x1c\x1a\xc7\xd5\xff\x00\xe3\xdd\xbe\x94\xe2\xf5E#\x85\x92L\\0\xf7\xa9\x91\xf8\xaa3\xb1\x17o\xf5\xa9\xa3l\x8a\xfa\xeaO\xdcG\xcdT\xf8\xd9y_\x9a~\xea\xac\xad\x83R\xe7\x8a\xb2\x0e\xcf\xc0\x89\xfb\xeb\xc9\xcf\xf0\xaa\xae~\xb9\xae\xc4\xca\x03\x1er+\x93\xf0X\xd9\xa6\\7\xf7\xa4\xfeB\xb7\x99\xfej\x86\xf5=:\x11\xfd\xda$\x9f\x13N\xa9\xd8\x9ejBF\xeczU?7\xf7\x99\x1djU\x97\x8a.k\xcaI+\x04M\xcd^k\xe3\x9f\x10\xcboa\xf6\x1bY\x8aK4\x9ed\x9bz\x85\x1d\x07\xe7]\xfd\xfc\xc0@\xd9<\x01\x93^\x1b\xa84\xb7\xfa\x8d\xc5\xc3J\xbb\x99\xd8(=\x86x\xa8\x93\xbb3\xad.X[\xb9\x9b\x13<\x923\xcc\xe5\x98\x82Y\x8d4]\xa2d+\x8f\xce\xa7\x94Gl\x08iD\x8cG \x0e*\xb5\xa5\x83_\xdd*F\xb8R~b\x07AF\x89]\x9c:\xb6\x92\x13y\x90\x82\x0b>zb\xac\xc3\xa6\\\xde\xc8\x17aU\xf7\x15\xddi\xba.\x8f\x14I\x14R\xe6A\xd7\x7fs[0ip\xa4\x83\xe5\x15\xc3<W\xf2\x9d\xd0\xc2Y{\xec\xe6\xb4O\x0b$8\x92Q\x92+\xa4\x9a\xcfu\xbf\x92\x84\xaa\xfbV\xbaE\x1cJ\x08\xc0\xf6\xa5\x10\x07;\x8e\x14{\n\xe5\x9b\x94\xf5gT-\x15db\xdbY\x8b0\x1dF\x08\x18\x15\xa3\x1c\xcf\xe5\x93\xfa\xd2NP)\x07\xb56\t\x06\xf5\xfe :\x8cW>\xad\xd8\xe8N\xea\xe5\xa8\xa1\x92\xe5\x0b\xb3m\xc9\xda\x00?w\xde\xb5`\xd3\xe3\xb6\x87c\r\xcd\xd7>\xb5\x1d\xb0M\x91O\x10\r\x1b.\x1c\x01\xde\xa4k\xaf,\xb8\x0c\n\x83\x90\x1b\xb5m\xa4Q\x8c\x9c\xa4\xc7I(H\x82\tp\xde\xe3\xa5c\\\xde\xa4{\x84\xb7X\x1f\xed\x9e(\xbc\xd4\x9ef\xd8\xbbU}k\x9a\xbb{e&I\xa6.\xf9\xfb\xaa\xdc\x1a\xcdZL\xbb4\xb6.\\^\xa3e\xa1v\xf9}\xb85\xcd\xea\xda\x9d\xe4\xd3\xacrFpx\xda\xa7\x14\x97\xda\xbcq\x86\x11@Q\x88\xeb\xbb\x8a\xc5MF\xeddp[\xccf\x03anq]T\xa8\xbd\xecs\xd4\xaa\xbe\x14\xcdr\xbb\x95\x04)\x8c\x0c3g\x9c\xd5\xad>Hm\xa4a1\x01\x9b\xa1\xa84\xfb/&1;J\xcb\xf2\x96l\xf6\xf5\x07\xf3\xad8\xb4x\xae-\xf3\x10I\x11\xba\xedo\xf3\x8a\xb5\x1du&S\xd3B\xd3j0\xc6\x086\xc5YW\xb9\xe1\xab\x03Q\xd4\xd5\xb3\xe5\xe5G]\xeb\xd3>\xe2\xb4\x86\x8a\xa2\x16%\xe4V\x8c`#\xb6k&UUC\x1c\x96i\xf3|\xbed|7>\xb5\xb4 \xaea9\xc9\xae\xc7){\xa9O#\xb4m\"\xeen7'B+\xa2\xd1-.\x84\xf6\xf0-\xe2[\x97\xc2\x92\x01g\x04\xf6\x00w\xac\x8dGG\xfb>\xad\x141\x12A\x1b\xf7c\x8a\xef\xfe\x1a\xe8au6\xbe\x93s\xa2!\xfd\xe4\x8c\x06\x0f\xa2\xafZ\xe8vKC\x91_\x99\xdc\xef,\xe0k[?\xb1\xca\xed)\x1c\x17\x93$\xd6+\xecK\xa5\x81\xc6\xe4F\xde\x08'\x91\xe9\x81\xfe5\xab?\x9a\xf7\x1b\xa2A\xb07\xdef\xed\xec=j\x0b\xfbhn\xd6;\xab{\xad\xc5r\x1a6\x00\xfe\x1e\xb5\x9d\xael\x9d\x85\x8e\xe84\xa8\xd1\x83\x1cy\x1bT\x0e\x0bu'\xd3\x8a\xd8\xf3\x95`\x8eY\xc6I?.O,O\x7f\xe4+\x16\xde\xe6\xdcZ\xaaO#\xb5\xc1l}\xd0\xa3\x1d\xf0:\x01\xef\xd6\xa6\x92@^9'\x95\x82\xa8\xdd\x83\xef\xdf\xfc*\xe3r]\x8d\xab\xab\x18.\x15E\xcca\xfc\xb0X\xaat\r\xd0\x01\xef\xcd6\xdbJ\x82(\xa5u\x1b\xfc\xc6\xf2\x94v\x18\xf4\xf6\xe9Y0\xeb\x12*\x19#\x00\xc6\x18\xaa\xc6y\xe7\xd4\xfa\x9c\xf3\xf9U\x99\xf5k\x9f\xec\xe6\xf2\xd7\xf7\xca\xdbc\x04c\x9d\xab\xb8\xfe\x19\xfd}\xaa\xbd\xd7\xabD\xfb\xcbDI\xa8\xda$\xcdo\xa7[*\xa6g\x01\xdb\x1fy\xb1\x92\x7f \x7f/z\xab/\x84\xe2\x9f\xc8\xc4\xad\x8bc\x807\x7f\x1b\x1e_\xf3\xcf\xe4)l\xaf\xae\x1a\xe2ina\x11y\x1f4n\x0er\xc4\x9cq\xff\x00}~U~\xd9\xa5\x89e\x9c3lpwg\xb1\xce\x06=z\xe7\xf1\xa4\xa3\x1b\xde\xc0\xdc\xad\xb9\x9e\x9aB[\xeam6[\x11\x8f\x90\x13\xf2\x84\x03\x8f\xc4\x00*\x0b\x9b1qe\x04l\xc4\xb4a\x89U\xeb\xf7\xbfZ\xe8\xb6\x1f9Z)7\x16V\x1bJ\xfd\xee\x0f\x19\xf5\xac=R\xd4\xc9\x0crZ\xbf\x96\xf1\x97\x067L\x8cq\x9c~uOM\x84\xb5z\x9c\x9c\xb7\xf7\xd1^\xc6\x16f\xf2\x0b`(<\x8ey\x15b\x0b\xadBB\xe5.\x18\x8c\x1c\x03\xdc\n}\xd8\xdb*\\\xb2\xab\xc78\xc6\xd6'\x01\x87\x1d{\n#\x02\xd8\t\x16'\xda\xe0\x96V<\xaf\x1d\xbdsY\xdd\xb3M\x89\x97O\x9e\xf2\xdf72\xbbt=\xf0q\xd8\xd3\xe0\xf0\xed\xbe\xe9\x02\xa6\x15\xb9\xf9\x87\x1f\xe75$^l0\x16\x13H\x8c\xa4\x06\x1cr;dv8\xa8\x1b_\xb5\xb4%V\xe7\xef\x10v\x8e\x02\xb7L\xfb\x7f*|\xa8\x9ef[\x93G\x83`\x95\xc3\x10\x08\xc2\xa9\xe4`c\x8f\xca\x9f\xa8ki\xa6\xdc\xc7vG\xfa\xd5W\xe0\x06$\xf49\x03\xd4\x83\xd35\xcd_k\xca\xf9\x8a)\xda)\t\xe29\x060y9\x07\xa1\x04\x8a`\xd3\xbe\xdd\xa3\xc3-\xc2\xb2L\xb3:H#nNpr\x01\xe3\xae\x7f:|\xb6\x15\xee\xf5:\x1b\xa2\xd7\x13\xc7$\\\xd9\xb8\x12D\xa8\xd8e\x07\xef\x0ez\x90F0}=\xeaX\xe6\x9a\xef02bA\xf3\xc78\\\x19Tv\xff\x00{\xf9\xd5\x1d6\xd4\\\xd9K\x00\x99\xfc\xabw\x12l`y\\a\x94\x8e\xa3\xb1\xfc\x0f\xadU\xfbO\x9d\x7fm\r\xaco\x14\n\xc7a\xde3\x91\xe8\x07O\xc6\x94\xb6\x12wgm\xa0\xc5\"Za\x98\x11\x93\xd3\xeb[\x07\x85\xc3}\xdf\xe5\\\xd7\x87uHg\x9ah\xd1\x94\x1d\xf8q\x9e\x8f\x8eF=\xfa\xfe~\x95\xd1\xb1\xf7\xae\x88\xece\xd4c|\xbd\x7f1\xde\x99\x9a~w\x02\xbd\xff\x00\x86\xa0\xdcMP\xc5,\x00\xa6\x82}M9@#\xa5\x04g\x18\xa0\x0c~\x05/N\xb4Q_\x1e}\x00\x11F\r\x14s\xeb@\x05%-%\x00'za\xeai\xfd\xe9\x87\xad\x05\x0cn\xf5\x91\xab\xff\x00\xc7\xbbc\xd2\xb5\xdb\x8c\xd6F\xaa\x7fp\xff\x00J\x95\xba)\x1esrqx\xd5,m\xc5Cv@\xbdjr\x1a\xfa\xea_\xc3G\xcd\xd5\xfe#.!\xe6\xa6\r\xc5VF\xf7\xa9\x15\xbasZ\x19\x9d\xcf\x83n\x94\xdaO\x01 0p\xd8\xf6\xc5tNGQ^ca}-\x8d\xca\xcd\x13r8#\xb1\x15\xd2\x9f\x14[K\x17\xcd\xba6\xee1\x9a\x86\x99\xe8\xe1\xabC\x95E\xbb4t(\xc1\xa58\xedS3\x01X\x9aV\xa9mp\xa7d\xa0\x9c\xf4=j\xf9\x95\xe6`\x90\xc6\xee\xc7\xd0Vr\x92[\x9d\x8a\xcfT\xf4(x\x82\xf3\xec\xfa]\xcb\x8e[\xcb!~\xb8\xe2\xbca\xed\xe6H\xcb\xbf\x1c\xf2\r{u\xff\x00\x85nu$\xdbw9\x8a\x11\xc9\t\xd7?Z\xa8\xbe\x08\xd2`\xf9\x9a\x13+\x9e\t\x90\xe7>\xf5\x83\xc4F:\x9c\xd5\xe0\xea5m\x8f!\xd3\xf4\x1b\xddVlD\x84F:\xbbt\x15\xdcZi\x10\xe9\xd6\xc2\x18\xfa\xf7c\xd6\xba\x99t\xf4\xb5\x80$H\x17o@\xa2\xa8\xcb\x01U\xc0\x19c\\uq\x12\xa9\xa6\xc8\xd6\x8d\x08\xd3\xd7\xafs\x9c\x92\r\x92\xab(9\xcf\x18\xae\x92\xc8\xccPg\xa9\x19\xac\xfb\x9bs\xbdUG\xcd\x9a\xdf\xb3\x84E\x04l\xddq\x8a\xce*\xee\xc6\xd3vD\xe1\tEb0)\xb3L#Nq\x9e\xd8\xa8n\xae\xc4Q\x90I\x04v\x07\xade\xbd\xc9\x9d\xb2\xdd\xba\nS\x92\x8e\x88)\xc1\xcbV>wy\x9c\x85\x18\x07\xd2\x9f\x0c\x827\x10\xb08a\x8a\x8d$@@9\x07\xd2\x94\xde\xdbE)\x12\x0c:\xf2\x07\xd6\xb1I\xdc\xdd\xb4\x95\x8dm\xebem\xfb\xb6\xc1n\x8b\x9a\xa0/\x06\x1d\xa5\x90n\xc7v\xac\xedOZD\x87k2\x82G\x18\x15\xcbK\xa9\xc3\x18iC\x97\x94\xf6\x0b\x8a\xd64\xdc\x8c\x9c\xd4\x16\xa6\xdd\xf6\xad\x0b\xb1\x85\x10\xfc\xc7\xbbpk\x9f\xd6/\xd2[O%T\xf1\x9eA\xc1\x15\x9f}\xac\\M\xc3\x048\xee\xa3\xa5elg\x941\x95Is\x92\xa4\xf3]\xb4\xb0\xf6\xd5\x9cu\xb1WN(\x9c\xda\xc9\x1c\x02I\xe4;=3\xd6\xb4\xac\x15\x9e\xccN\x96\xe1Dchf\xe3p\xf4\x15[Q>h\x8dmQ\x8ch\x80\x16=\xc9\xab\xd6\xd6\xef5\xb4\x91\tI\xf2\x80]\x882\xc0{\n\xde\xdajs\xa6\xaf\xa1\xafcu!\xd2\xdbj\x82\xc0\xe5\xc9\xe4\x15\xcdg-\xe5\xcc~{\xc3\x13B\xc8\xdf\xc2H\x06\x9c.\xfe\xc7oqn\xd12.\x02l'\x923\xd6\xadY\xea\x12\xcfe\x05\xb4\x1bv\xab\x92L\xb1\xe7+\xd3\x91\xfdhP\xb6\xa2s\xb9\x1ax\xa5\x11$\x17\x02A!^1\xd6\xa9\xddj7\x13'\x96\xb9\xcb\x05\xea1\xc7\xaf\xb5i\xea\xbaU\xba\xcaU\x12&l\x80\xcc\xa3\x03\xd7\xf0\xaa\x91\xe9\xf7r\xce]\x9c2\x00T:\x0c\xe1\x87J\xd2)t!\xb7\xb33\xa7YmHwF2\x1c\x15,\x08\xe3\x1f\xadzW\xc3k\xa1\r\x95\xdf\xfa\x1c\xcd3\x1f\x95\xdd0?\x0c\xf6\xf75\xc0Ok7\x98d\xbd\x92Y6\x00G\xcc@\xc5w\x1e\x08\xbb\x11\xdaJP\xa9i\x1c\x04\r\xc8\xfcje\xb0-\xce\xa1\x16Rg\x88[\x87\x94\x9c\x92[8\xcf\xa7LVL\xba\x9a\xe9\xb1\xcf\x1c\xb0\xab2\x9c\x10\xc3\n?\x01\xd7\xf1\xad\xfb4\xb9{\x96i\xe4@\xb9%B\x9c\x02:s\xfesT5\xed\x169\xe3\xdf!\x0c\xa0\x8c\xb0C\xc7\xae\x05J\xb9^F9\xc2Z\xfd\xa0M\x14o\x92B\x13\x92I\xe7\x00t\x1f\x86~\xb5\x04K5\xe0\x8de\xb8e/ \x95\xdf~x\xc7\xca\x07\xe5X\xd2C\xf6)%y\x1c\xc3\x04\x87\xe6/\x96i\x14\x7f\x0f\xae?!Ph\xba\x8c\xd2k\xb1+4\x8d\x19c.d\\\x10\x14`\x0c}H\xc7\xd0Ur\xf6\x13o\xa9\xde\xd8\xc6\xa28\xd7+\xb5d\xf2\x80\xc7\x01\x89\xcf>\xff\x00tU\xdb\xcb\xc8\x19g\xb7\xc6\xc6\x8b!\x9b\x1c\x12I\xc9\xcfa\x93\xfaW?\xe1\x8dN\x06\x82\xe1\xafv\xa8\xb7f\x9f\x0cz\xb7\xca\xc0\xfel\xbf\xa5k\xe9\x17\x96\xb2\x1by& \xc6\xe5\x90\xef\xc6I;Y\x7fBs\xf4\xabI\xd8\x86\xd5\xc9n\xe3G\x82 %ex\xf3\xbf\x1d\x98\xe3*~\x9f\xce\xafExb\xd1\xf6\xcb\x08\x82I[\nz\xc6H\xfe@\xf1\xf9\xd5;\xeb\xa8\xe2\x96\xe1a\x05\x8a\x7f\xae^\xa5\x90\xfd\xd3\xef\x8fZ\x8d\xf5\x85\x82\xca\x1b;\xdbA\xe4\xc9\x033.{\xe4\xe7\x1e\x9f.\x08\xa6\x90JJ\xc5\xcd>\xfdf\x8aH\xcb\xab8%6\x9f\xbe3\x91\x8f~\xd8\xac\xe9\xeedhSv\xe9w0\xfd\xe61\xc1\x04\x1c\xfe\x9f\x8dc\xabE\x0e\xa8\x14I'\x9cf\x02'\xe9\xbc\x01\xb8\x12=q\xc7\xe3VouK4Y\xe3\xf3X\xacQ\x90\xca\xbdw\xb36\xdf\xafL\xfeT\xac\xfa\x85\xd2\x1b\xb5\x1e\xcc\xac\xcc\x81\xe3\x0b*\xef?+\x06\xe0c\xebT\xf5H\xccqB\x91K\xe6\x08\x94\xab\x16?20\x19Q\xe8x$f\xa4\xba\xd7,\xec\x11\x921\x1d\xc4B\x00\x91\xb0\xe8Xv?\x89\xae.\xf3Q\x97U\xd3d\x8d'_!LC\xc9\xce$(X\xf1\x9fP2\xa7\xe8)Y!96\xc8\xef\xfcI{*\x94H\xdd$Q\x92\xb9\xcf\xca:\x00{\x11\xcf\xe5Y\r%\xd5\xc6\xf9\x0cl\x0b\x80v\xed\xc9>\xe4\xf5\x15r(\xa3\x1edS\xa4\x89u\x19\xc0b\xbd\x03t$\x1e\xbc\x8c~>\xd5\xb1\xa7\xd8]\xcbr\x98\x91\x1b*\x100\x1bN\x07#'\xd7\x8e\xa2\x87d4\x9b!\xd3\xac\xad\xa5xb\x93\xccD\x98l;\x8e\x00~\x06G\xa1\xcf\xbe?:\xe8M\x9d\xdc:lpI$R\xc8\xb7\x1f$\x8d\xf2\xe4l\xe7=\xb2x\xe0\xf5\xa9-\x12\xceX\xfc\xa9\"r\xd0\xb1w\xf2\xd0\xe0\x9e\xe7\xd8\x8e\xf8\xfckJK\x96\x8a\xc2H\xda4\xf3b\xe5\x19\x1b\x823\x80\x0e:\x1f\xae)G]G-42Z\xf6=\x1dRx$\x0c\"m\xb3\xc4\xc0\xf4=p\xc7\xeb\xebZ:-\xbe\xf9$\x96\xe9\xbc\xd6\x8d\xce_\x00p\x0f\x19\xc7Q\x8cV^\x99g5\xfb\xc9s-\xa8-\x8d\xbc0\n\xc3<g\x9eH\xae\xbeYn\xe1\xb4\x8eV\xb5\xf2\xd8&\x03\xc8\x9cq\xc0\xe4\x12:b\x87\xa8-\x0eF\xeeai\xaf\xcc,\xe5T\x86`>r\xdc#\xf6\xc9\xfa\xfa\xf6&\xba\x8d\x03\xc5q^\xaf\xd9\xaeA\x8e\xe1\x0e\xd6\x04w\xae\x07Z\xbciu\xc2\xc65\x19\x18b\xa3\x83\xea\x08\xaa\xf7\xb7\x17vz\x847\xd6\xac|\xbb\x84\r\xb7\xd4\x8e\x1b>\x9d3\xf8\xd3\x8c\x9a\xd8\x1cU\xcfmP\x18n\x06\x87\x1b\x88a\xdf\xaf\xd6\xbc\xf7J\xd6\xb5K;\x7f>W2C\x91\xfb\xb3\xe9\xea\rt\xdaG\x89lu9\x1a\x14\x90\xac\xbd\xd5\xc60kX\xcdH\x89'\x1d\xcd\xb3\x85\x18\x1di\x01&\x90|\xc3 \xf1J\xa7\x15b1\xe8\xa4\xddF\xefz\xf8\xf3\xe8\x05\xa5\xa6\xee\xf7\xa06{\xd3\xb0\x0bA\xe9HXz\xd1\xba\x8b\x00\x94\xd3\xd4\xd0_\x06\x98\xcd\xdf\x14\x86\x84s\xc5dj\x99\xf2\x1b\xe9Z\x8e\xd5\x8d\xaaI\x88\xcd.\xa8\xb4\x8e\x06\xee\x067L\xdd\xa9\x00\xda\x06j\xed\xc3\xae\xf68\xc9\xacK\xab\xad\xb2`W\xd4P\x93\xf6h\xf0+\xc1{GcI\x18g\xadZ\x8e&~Ea$\xaf\x8c\x8a\xdb\xd3\xaeF\x06\xf1\xf5\xa9\xad\x88\x955t\x8a\xa1\x87\x8c\xdd\x9b'\xf2d\x03\x14\x86'\x15h\xddG\xda\xa3{\xa4\"\xb9?\xb4j\x7f)\xd8\xf2\xd8\x7f0\xba[\xbcW\xa0\x86\xc75\xeazE\xcbK\naNzt\xaf#\xb3\x9b}\xf0\xc7\xadz\xb7\x87\x1ftJ2I\x15\x95Z\xce\xa5U\xd0\xa8QT\xe9>\xa6\xdc\xa8Y@'\x81\xda\xa9L\x9c\x1cV\xbb\xa2\xe2\xa9<+\x92y\xaa\x94L\xe3#\x0e\xe2\x17e8\xac\xc7\x8b\xca\x0c\xcc9\xc5tr\xc5\x8e\x95ZkTe\xc3u\xac\x1c\x19\xb2\x9e\x875\x14H\x1b{\xfef\xa4\x9e\xe4\x08\x82'\xde\x1d\xc5^\x9bO\x1c\x01\xf7s\x9cUk\x8beFS\xb4\x0czP\x93E\xf3&c\xddnv\xc1<\x93SD\x12\xdc\x02\xe0q\xd4\x9al\xca<\xc6\x91\xce\xd0\x0f\xe7X:\xc6\xb0\xa8\xa6\x18\xf9\xa5\xca\xdb\xb1\xa76\x9eF\x96\xa3{\x033y/\xb5\xfdA\xe9\\\xe5\xee\xa0\xb0I\xbd\x19$$c\x93X\x97\xd3\xdc\xbf*\xcc\t\xe4\x81Q\xc5\tuL\xb6]\x94\xb0\x1e\x86\xbb!B\xca\xec\xe3\xa9\x88\xd6\xd1,]\xde\xcf1\x06Q\x82GN\x80\n\xceI^\xe9\xa4\x88?\xdd\x19\x18\x15nH\xc4\x8e7\x8c\xf4_\xa7\xbdPX\x9a\x1b\x87H\xc7\x04\xf2\xd8\xae\xb8E$q\xd4\x9c\x9b\x1f+\xc9\x19\x01\xa1\xca\xa8\xe7\xd4\xd3\xd0\xed-,\xe22\xc4ac\xf4\x1e\xb4M\x12.\xc0\xf33+\x0c\x12\xbdjq\xa5\x1f\x9c3\xab\xb0\x1f)\xc9\xaa[\x11+\xdfA\xcd;\x9d,/\xd9\xda5\xc8*\xc0\xf0qW!\x8ak\x83\x0c\x90\xc3\xe5\xbf\x04\xb6q\x90;\x93L\xbc\xb4\x91\x04H\xb3\r\xe1\x06\xd8\xc9\xc6?\x0fJ\xb4,d\x82o*[\xb500\x19\xcbd`\nN\xd6\xd0j\xf7\xb3-\xcfi-\xc6\xbe\x85%S\x94'\x9c\x15c\xb7\xb7\xadT\xd2\r\xc4\x13\xee\x91\x18\x9d\xc5A\xef\xd7\xa0\xa8n\x8aE5\xb5\xc4n\xc8\x9bHB\x9c\xb0>\xa75\xd0h\xd2\t\xadL\x8cwH\xcd\xca\x95\xe5\xfd\xc7\xbd\x1bDw\xbc\x89\x1a\xe6\xdff\xd6\x82ID\xb9\x0cd\x1bp3\xd2\xaaY\xc54S\x1b\x8d#\x98z\x98\xdf\xaf\\\x11Z\xcf \x96\xd9\xd6E?)\\s\x9d\xc3=y\xe9\xcfj\xcf\xb5\xd4\xed\xa1m\x8f\x04\x8c\xb31Ps\xb4\xa9\x07\xa5Ld9G\xb9\r\xba\xde\\^4s\x94b\xc4\x82\x9bq\xc5v\x96\x16\x8bg\xa27\x95g\x1a\xb6\xec\x19\x00\xc6k\x8f\x89\xd1n\x95\xf0\xee\x88\xd9\xda\xe7\xe7\x03\xdf\xd6\xbd3DQ\xa9h70\x00\x15\x13\x80\x07\x18\xe2\x9c\x96\x9a\t^\xfa\x90\xe9\x9a\xad\x84\xea\x88\xec\xab*\x0e\xeap\xa0}\x7f\x03Z\xf0\xeap\xdd\xa2\xc3\n5\xc7\xfbN@\xcf\xbdyN\xb1\x1d\xf5\xa4\xf2C\x1f\xee\x91\x8e\xd4!B\xe0g\x07\x9cg\x9af\x8d\xe2\xbb\xcd2\xfdm\xde&\x95\x98\x95\xf9P\xfey\xeb\xf9\xd3@\xf4=>\xff\x00@\xb3\xd4Y\xa4f\xfb\xc0\xab4`g\xf0\xaeb\xe7\xc3\x8d\xa7\xc7\x8b}\xef\xe6\xb9\x1b\x88\xf9\xb1\x8e\x7fO\xe7\xedZ\x9aw\x8b\xad\x92&\x17\xd2\xc3o\x93\x84B\xe0\xb1\xfdy\xfc*\xfb\xc9&\xad\x0cs[#\x8bv\xce\xec\xb7\x0c\xbe\x9e\xff\x00AG/pL\xe5-\xb4\xb8^\xc6h\xf2D\x97.\xb1/\xcb\xc8\x19\x04\x9f~\x15\x7f1]\x19\xd1 [\x94\x8e\x11\x88QN\xe1\xfe\xd1\xc7?\x96?*f\xf4\x1a\x85\xbcq\xa9\xcbn`;\xb3\x12\x00>\xc0\x01\xfeqZ;\x81\x9e\x04\r\xbb\xce\xc9lw\xdaG?O\x94\xd5-\x04\xd5\xcd\x13\xa2\xc2n\x96p\x01>XB?\x95r\xfe,\x12\xfd\xa6\x1f)B\xb4ry|\x8e\n7\xf8\x1a\xea\xed58\x84\x13\xb98\x8e7\x1b\x8bv\xce\x0f\xf5\xfd*\xbd\xc8K\x91\x86Ef\x1c\x93\xd7\xbf\x1f\xca\xad\xb5m\x0c\x94]\xf5<\x96Y\xb5\x06h\"\x8dwOk7\x99\x96\xe4\xb8\xc7\x0b\xfc\x85,i0\xd3\xa52\xa9w1\xa2\xb6\xe1\xc8d9R>\xa0g\xf0\xae\xea}*\xd6\x19\xe2\x9c\xa9\xf9\xd9\xcb\x04\\\xed*\xb9\n}\x88\x07\xf4\xabm\xa4\xd85\x9c\xb2N\x9ed\x05s\x92~e\x0b\xd3\xf2\xfe\xb5\x16\x934\xbc\x11\xe6\xad\xa5Os5\xd7\x95\x19e\x92\xe1$\xd8F\x17\x9e\x0f\xe1\xda\x997\x87\xa7\xb7\x8d\xda\x00\x1aF\x99\xb6D\xc3\x07r\xe0\xb2\xb7\xe1\x9c{\xd7\xa0\xdc}\x9ey\xbe\xcb\x00\x8e8\x84\\\xc8\xbc\x92\xad\x83\x1b}8n}EWu\x89\xee\xdc;\xee\x98fYx9NH$z\xe3\x90}\xb6\x9a\x12kq6\x9e\xa8\xe7 \xf0\xf5\xd5\xd5\xfcw2DA\x9f\t&\xfcf6\x07<{q\xc7\xb9\xad\xab\x81cj\x1dW\x0f\x12\x0c\xb8C\x87\x8f\x9c\x16\x04s\x8e\xa7\xf0?J\xc4\xbc\xd4\x91o/\xcd\xa5\xd6\xd9\xe2g\x91\x1c\xfc\xcb\xce\x08\xe3\xa0\\\xaa\x82>\xbd\xeb\x9b\xd7\xf5\xfb\xab\xd4_\xb3\x85\xb6\xb8e2\x89af\xe7\x80\x1e>}\x08\xe9\xf5\xcfZ|\xabps{\x1b\xfa\xc6\xaf6\x9b~Z\xde\xfeE\x8a|\x17U}\xa67\xc7\xdf\r\xc8*x\xed\x83\xda\x8b;)\xf5\t\xc4\xf3\x98\xf1*\xec\x04\xb7U\x07\xae@#\xb7J\xc3\xd0\xb4\x0b\xbdJ4k\xf0\xae\xa8\xd9\x8b\x8d\xcd\x9c\xf21\xe9\xc6\x7f\xc9\xaf^\xd0\xa1\xd34})\x0c\xad\x14l\xb9\r\xb5Jg\xd7\x83I\xb1\xa4\xcaV6\x89\xa3\xd9*\xd9\xa2\\B\xc3\x9d\xb9$\x1fS\x91\xc8\xaaw\xda\x92}\x82h\x8c>J\xab.\xfd\xb8t;\x81\xe8?\x01\xda\xb4\xefog\xbfGKD+`\xe3\x02@N\x7f\xa6+\x9e[(<\xcb\x94\x10<\x84G\x96H\xb0\x0b`\x83\x9eN\rH\xcej\xfd#\xb8\x11m\xf9$\xfb\xc3\t\x8e=V\x99\x12\xb4\x9ad\xcb4\x8c\xd3\xdb\xcb\xbdI\\p\xc3\xbf\xe4?:\xb5\xafi\xe8\xd7\xff\x00\xbbr\xac\xc7;O\x00\x0cs\x8cU\xcb\x1b\x19\x1aI\xecelL\xd6\xfb\xa3\xcf<u\x18?QR\x8b(\xe9\xba\x9cN\xaa\x97\xd2\xecD\x1bC\x0eQ~\xb4\xd0^-HK\x11]\x8e\xdf\xbb\x9d[\x8c\xfa\x9a\xaf\x0c\xeb\xa7j\x0b\xf6\x9bp\xf1\xb8\xc3(9V=3[3YE-\xb22\xda\x18\xcew\x05F\xfb\xdfJ\x12\xb0\x9e\xa5\xdbO\x15\xc9g|-\xa5\x7f6\x13\xc8`\xbc\x80y\x15\xd9[^\xc5s\x18t`s\\\x16\xadokokm!V$eN\x17\x0c}\x8dI\xa3K.\x9fv\x8d<\xad\x14,8C[F}\x19\x12\x8d\xb5GM\x9fz3\xefL\xcf\xad-|\xa1\xef\xb4;>\xf4\xa1\x88\xa6Q\x93N\xe2\xb0\xed\xde\xf4\x85\xbd\xe9)\x0f\x14\\i\x014\xd2\xde\xf4\x13Q\x93\x93H\x04\x91\xb8\xebX:\xc3\x11\x0b`\xd6\xdc\x86\xb15e\xcc\rI?y\x14\x8e1\x98\xb4\xac\t\xac\xab\xa8\xff\x00}\xcdkc\x17\rTo\x00\xf3+\xea\xe9\xff\x00\r\x1f;U\xbfh\xc2\x15\x18\x15e\x1f`\xc0\xaah\xd8#\x9a\x948=\xebD\x93Z\xa3>f\x9e\x85\x9f1\xb1\xd6\x98\xf2\x128j\x88\x13An){8\xf6\x1f\xb4\x9fsCJ\xe6\xe4\x13\xeb^\xb1\xe1\x89\x17\xcb\n:\x8e\xb5\xe4\xfa8-8\xc0\xc9\xcdzW\x87\x9c\xac\x80\x1c\xfa\xe2\xbclW\xbb\x89G\xad\x86\xf7\xb0\xfa\x9d\xcf\xcb\xb7=MT\x96]\xcc@\x1c\xd4\x85\xc9\x88v\xfaTq(\xceOZ\xd1\xbb\x99-\x04\xd8\x0f^\xb5V\xe7jn$\xf4\xab\xd22\xa2\x93\xd4\xd7?\xaa\\3\xab\x05\xa8\xa9%\x14T#\xcc\xcc\xfdGVD;c9\xac\xd3\xa8\xe5\x0b\x12\x0f\xd4\xf4\xaa\x93\xa3)f`1\xebPB\xb14\x87y\x18\xef\x9a\xe6\x8bm\xeawr\xc61\xd0\xadsxn%\xd8\x1d\x0f^\x05s\xb7\xd8Y\x99Tnv\xf6\xe75\xb5\xa9\x1b4}\xd0\x9cm=G\x15Y#2H\xd2E\x8e\x17\n\xcd\xeak\xb5C\x95\xa9\x1cN\xa72qG7yn\xb6\xaa<\xd9\x9b\xcc#\xa0\xaa\xd6\x8cD[\x8b\x16\xdaH\xc61[\x1a\xb4\x8b\x12\x06\x94,\x92?\x1c\x8a\xa5\x02\xc5\"\xa0\x90\x15@72\xa8\xe4\x9fJ\xea\x8c\xbd\xc3\x92Q|\xf6+:;K\xfb\x9c\xb9\xcfoz\xd0\x89\xa6H\x1a1\x17\xef\x1f\xe5\x198\xe3\xbdii:U\xcd\xef\x99(\x84C\xce\x15@\xc8\xc7\xbf\xbdl\xae\x84\xd1^@&P2H t\xfa\xd2\xf6\xa9hW\xb2{\x9c\x95\xa5\x84\xa8s$\x05\xb7\x92rz/\xd2\xad\xc7\x0f\x95k\xe4DUn\t\xc3;\x11\x81\xcek\xac}-\xa4\x91\x91[\x08W\x18\xe9\x8a\xad\x0e\x87\x02|\x93\x8f,#o\xdc\x07-\x9e\x82\x92\xab}\xc6\xe9X\xe5n,\x85\xbc\x08\xef\x11\x92\xe4\x8d\xe0\xf5\\{\xd6\x8a\xc7q-\xaa\xa4\x91#;\x8c\x9d\x8b\xf7W\xda\xb5c\xb9g\xb9\x82\xd4\xc2\x8e\x13v\xe2W\xb7A\x9f\xc2\xae.\x9ae\x80\xcd\x1b\xbc{r\x19\x01\xce\x07\xa8\xaar\xb2\xd4J:\x98\xb6\x89\x04\xe8\xb1\\DO\xd9\xc8\x07#\x82;\x9cz\xd6\xae\x87\x15\xb8\x8a8\xe2o\xba\xcc\xd0\xee\xed\x8a\xbb4am\xb2\xb1\xac\xb2\x029\x1d\x7f\x1ad\x16\x0b\r\xced\x18G\x1c\x11\xd0\x1a\x87Pj\x99\x9f\xa8\xdd\x03r\xd04\x82'pA\xf9x\xac\xb4\xb3\xba\x82\x7f5$G\xde3\xf3\r\xcaMo\xebZ_\xda#\x85\xa6\xe4\xa6H#\xa9\xa7i\xb6\xd2F\x05\x8d\xf4[\xede_\xddN\xbdG\xff\x00^\x9d;Z\xe2\x9aw\xb1\x99e4\xf3\xca$x\x84N\xbf,\x89\xfe\x15\xdfxAdK\xa9\x10\xc8\x12\xc1\xd7\"\x1c\x1e\x1b=I\xaefKCap\x8eJH\xad\xc2\xb0\xe8\xff\x00_CZ\xdav\xa4\xe8\xd1\t\tR\xb2\x0f-p2\xdf\xec\xd6\xbc\xc8\xcf\x95\x9d\x9d\xde\x8bgz\xed,\x96\xfb\xf1\xceH\xc0\xf6\xaf=\xd6|\x15uww!\xb3b\x88\xc4\xb3\xe1\x8bdzq\xd7\xf3\xafO\x8a\xed]F]rx\xc0\xe4\x0f\xa59Y\x07\n\x0f\xcb\xd7#\xfaS\x11\xe56?\x0b\xd2+Aw,\xc9\xe6\x95?\xeb~\xe8>\xfe\xde\xc2\x97\xec\xbe%\xd3\x99\xda\rS\xce\x89z\xaf\xdd@\xa3\xb0\x18\xc0\x1fJ\xf4\x8b\xcbX\xaebY\x19S~z\x9c\x03\xf8zV}\xd2\xda\xb2\x15\xb9\x8dH\x8c\x1c\x15\xcbd\xff\x00SI\xb6\x81$rQ\xf8\x8f\xec\xd6\xb6\xce`\x96i\x8a.\x04G\x1ek\x1e@\xc9\xe7\x19\xc7\xf8\x01V\xf5\r\\\xc0\xaf\xf6x\xb6\xdc\xbc^P\x0b\xce\xc6\xda@\x1fA\x9f\xd75v}\"\x16\x98O\x05\xac\xa1\xe1\x8bp`~\xe9\xc6\x00=\xb3\xec:Vm\xce4\xb3\xe5\xdc\x00\xcd\xe5\x8d\xd2m,\xc4\x9e\xf9\xf5\xff\x00\xebP\xa5\xdcv]\x00\xdf\xdd\xddZ\xba\x06#\xed \x90\t\xe4\x9c\x84\x1f\xa0&\xb4\x9fU\x8bH\x96r\tyR\xd5\x82\xc6N\xed\xce\xad\xbb\x8f\xe5\xf8V}\x95\xb5\xec\xf7,\xf0\xc6\xa1q\xcbw\x8c\x01\x95\x03\xf1\xceO\xbdf\xeap0\xb9\x86\xee)RT;\xd8\x0e\xa79\xc3\x02}\xd75JI\x11(\xb6?L\xf1\x1d\xfa\\\xb4\xc6=\x96\xcd)Gi\x17\"90\x06s\xd9x\x04\xe7\xb1\xcdT\xbd\xd7/\xac\xda\xe8J7YJXf?\xbc\x85\xf26\xfds\xc6>\x87\xda\xa5\x87\xc3\xd3\xdc\xda\x992\xc8\x92c`\x8c\xe0\x86\x03\x03\xafQ\x8e\x08>\xfe\xb5\x9b\xff\x00\x08M\xeb[N\x91\xdc\xbbD\xdbC'!O<\x10z\x8e\x83\x1e\x95\\\xe8\x9ff\xcaW\x1a\x9b\x19\xe3\xd9>\xd3\x1d\xb8X\xca\xb1\xcc\xaaF@\x1d\xf3\xd7\xf1\xa4\xbe\xd6\xee\xef\xa3\x8a\xed$\x86+\xd8#\xda\xe2\"s!\xc0\x19\xc1\xec\xcb\x8c\x8e\xa0\x83\xdb4\xba\x7f\x80\xae\xe0\xd5SJ\xbf\x95!\x99\xc0h\xdc\xbf\x0es\x9c\x03\xf5\x15\xbdq\xf0\xea\x0b+d\x9ai\x9d\xc3K\xb7'\xef#g\x81\xfc\xff\x00?z\x1c\x83\x90\xe0\xac,\xae'\xd5$\x99^IK\xbb\r\xf98d#\xee\xb7\xa6A\xc7=\x08\xae\xc7L\xf0Y\xb7\x11\xff\x00kp\x8b\xf3\xc6\xe5\x82\xe4\xe3\xa9#\xa1\xc69\xe9\xc0\xae\x97J\xd2\xa1\xd1\xae\x10%\x97\x9bi:m\x12\xa62\xb9\xf5\xecGl\x1fOb*\xde\xe1\x17\x9bg*\xa9\x8c6b\x8ec\xb7h\xfe\xe8<\xe3\xb8\xfd\rKm\x94\xa2\x8b\x96\xa9\xa5\xe8e6Z\xdc\xf93.\x1aO/\xcc\x8dO\xa9\xc6q\xf5\xac\xedSPFs\xbeQ<\x1f\xc1\x83\xb9q\xeb\xd3\xad>\xd7MP\x07\x9fw4eT,*d\xfb\xab\xd7o\xbf\xeb\xf8R\x1d:\xc6\xd6x\xeee\xb5\x92u^\x18\xc3) \xfapO\xe8qH\xbd\x8a\xd6\xb7w\x17\x16\xe2Khc\x8a\x15R\xb28\x199\xed\xc0?/\xe5Y\xd2\xebOmu\xf6q.\xc9e\x062\x81X\xaa\x92\x08\x1c\x8cc>\xe2\xba15\xb4\xd2\xed\xb3\xb6\x11\x81\xf3\x1e\x00\x01{\xf2\x0e\x0f\xe3P\xc7ig\x05\xc2\xdcH\x91nq\x95$\r\xe7\x07\x83\x83\xd4})\x88\xe7\x11\x04\xd6\xc2\xe6`X0\x191\xe0)?\x8f=*\xa2\\,\xb2\xb8\xb7\xfd\xc7\x94\x03\t\x1d\x8f\x18\xea\x07\xb5nkR\xc3\x14!\xd2\x07\xfd\xdeHx\x9bnG\xfb\xbc\xd7\x15\x08c3<\x9ee\xb8\x0e]\t eOPq\xdc\xd4X\xa2[\xbbx\xae7\xa3+:3\xfc\x8c\xa4u<\x83S\xe97m\x19k[\x90\xe6D\x1cy\x99S\xb7\xd6\xb5m\xa2\x12hn\xcd\x92\xc9\x8c\x11\xc6=\x0f\x1e\xc6\xb2\xe4\xd3c\xb8t\x96[\xf9\x12Q\xf2\xfc\x87\xf1\x14\xee\"\xdb]\xad\xa5\xc9$\x99\xa1f\xe0I\xfc\r\xe8*]JF\xbc\x8e;\xc8P)^X3g8\xac)\xed5+\x89\xc5\xbc\xd2~\xed\x8f\xca\xc7\x9f\xc4\xd6\xb6\x9fgy`\xca\x97\x90\xaa\"\xf06\xf7\xf7\xc50;\x1au&(\xaf\x98=\xc1h\xa5\x02\x96\x80\x1bI\xd6\x9d\x81I@\x0cja\x15)\x15\x1bw\xa0\x08$\xebYZ\x9a\xe6\x16\xadg\x15\x9b\xa8.`j]J8\x86\x18\xb9j\xce\xbd\x1f=iH1tk>\xfb\x87\xaf\xaa\xa3\xfc4|\xedo\xe22\xa8\xa7\xabt\xa6\x0e\x94\xb5\xaa1{\x93\x02qOX\xe4\x95\xb6\xa2\x16c\xd0\x01]\x17\x86\xbc#s\xac\xba\xcb\"\x94\x87\xb7\xbdz\x86\x99\xe1-?N\x88\x01\x12\x96\x03\x93\x8a\xab\x81\xe7~\x13\xd2\xa7\x8e\x7f2\xe6\x02\xab\xee+\xba\x81c\x8a@Pb\xb5n\xadbH\x8e\xc4\x03\x1e\x95\x90\x99YFzf\xa5\xc2.-\xb4L\xaaN\x13I3v&w@z\xd5\x8c\x1d\x9fw\x15\x15\xa0\xccB\xa6\x9b\x08\x9c\x9a\xf2e\xbb=u\xb1\x9ds!\\\x8d\xdcV\x1d\xec\xaaw\x00\x7fZ\xd1\xd4_ly^\xf5\xceL\xcd\xbf$t\xe9\xeek\x8aN\xf2:\xa1\x15\xcbr\xac\x84\xa2\x12\xc3\"\xa84\xca\xecG\x01\x07AZ\x170\xcfw\x1e\xd4%#\xfe\":\x9a\xc7xc\xb6u\x8c\x12\xce9'<WG\xb3I)\"UK\xbeR\x95\xe5\x90\xd8\xf2\x80\xc7?\x95S\xb4\x9aTVb\xa0\xc7\x1f\x1b}I\x15\xbb\x7f+-\xb9b\xbb\x94/\x0b\xfdk\x1a\xcd\x04\xc97\x94\t\x94\xe3*{\xd7g2\x9d3\x91E\xc6\xa1\x8d\x7f'\x999\xf3U\xb7\x8ey\xe9\xedW\xf4+g\xb8\xba\x8d\xc4;\xfe|\r\xdd1R\xcb\xa2O)\x0f.x?\x9duz]\x84\x9a|pG,Y\xda\t$Vr\xa8\xb9,\x8bT\xdf=\xd9\xb9\xa6Yy\x03\xec\xf1(\x00\x9e\x0f\xbe:\xd5\xdb\xad*C\x83\xe6\x0f\x91F\t\x1dM:\x1b\x98\x15W\x07nGC\xde\xae\xc7w\x1c\xe8T\xb0c\xed\xde\xb9`\xda\xdc\xdaz\xeccYC\xe6\x96\x8au\n\xcb\xf9\xfe\x15nkX\x95\x93\xcc\n\xe88\xc3.1Wg\xb7\xb7\xb9\x8c\x00\xa4:\x8f\x94\x8e\xa0\xfdj\x90\xd3\x97v%yr{\x9ekk\xdc\xc9\x91>\x99\xa6G:\xcb\x10\x01\xcfF<\x8a\x1b\xc8\xb1&Y\xa3Lm \xb8\xe8G\xbdW\x93M\x88;\xe2yb9\xe9\xd4\x1a\x8ak)-\xed\xc2\xad\xe1\n~\xee\xe1\xb9~\x94I\xb6\xb7\x04\x922\xa5\xb7\xb3iV[{\xb9\xa2\x8d\x9fpe\xe5W\xff\x00\xadW\r\xba\xc4\x8e\x12\xee9\xa2\x93\x07\x93\xc85\x8f\"\\i\x13\xb4\xee\xc9,NpUe\x00\x0f\xa5E-\xe5\xac\xe0\xe3\x82y\xe4\xf0\x7f*z\xda\xc5\x17\xb5;\xa7\xb6\x8c$\xc8^\x02\xc0\x87\xf4\xab6Z\x85\xb1\xb51\xbc\xb9\x87!\x91\xfb\xa3z\x1fj\xe6\xe4\xd5\x193\x1c2\xa3#\x0eQ\xfe`\xc2\xb2\x8e\xa4\xf0\xc9\x90\x8a\x03\r\xa5A\xe0\x8fJ\xa8\xa6\xb6%\xd9\xeev:\x8d\xe2\x95x\xf0\x08\xdaC\x8c\xf2\xa7\xb3W7o\xaa\xcf5\xce\xc6\xf3\x04\x8a\xdc\x08\xce\t\xf6\x1e\x99\xefY\x8fs3[\x96R\xdb\x93\x8eNr=*\xf6\x9b\x90c\x965\xda\xc7\xa9\xee}>\x82\xb4\x8b\xb6\xac\x99F\xfa#\xd84\x99\xe4\xb8\xd2a\x91\xce\xdb\x8d\xbc\xa0\xe7\xf2\xf6\xab\x06i<\xb2\x97\x03\xf7}\xc4d\x83\xfaV^\x88\xe6\x1d:6\x947\x19,\xed\xc6[\xd7\x9f\xff\x00P\xad9/\xe2\x8d\x13j\x97V?\xc1\xd3\xea}\x7f\xcf\x15\xba\xbb9FKs\x13F\xc6<o\x88|\xa3\x1c\n\xc1:\x96\xa5p\x0c\xef3C\n?\xcad@K\x1f@\x0f\x00{\xd7N\x92i\x97\x10JL\xd1\xaa\"\x90\xfb\x8e\x02\xf1\xcf?\xce\xa9\xdf\xe8\x16\xba\x84\x10\xef\x96Cn\xa4\x15\x8e5\xc0o\xf0\x14j\xb5\x04\xd6\xc6\x05\xc4\xda\x95\xde\x9a\x97\xb3\xde3&s\x12\xafL\x93\xd0\x0f\xe2>\xfd*Kk\x19\xf5\x08!m\xfb\x9b&Yd~\x8b\xce>\x99\xe2\xaf]\xc3\x1cq\xda\xc6\xa9\"\xa1;\x10\x15\xe4g\x8c\x81\xd8`u\xf4\xc7\xad]\xb1\xc2\t\xa5'\xcd\x8d?\xd5\xa0\xe1v\xafO\xaf?\x9d;_q\xde\xdb\x18\xc7W\xb7\xb3\xb96\xf1\x07X\xc6|\xc2\x14\x8d\xc4\x7f\x8dg]^X\xdd^\xc8`!a\x83\n\xc1z.F[\xeb\xc6G\xe7K\xab\xc0\xa9\x03\x18\xe2e\xbb\x9d\xbf}p\xef\xc2\x93\xcf\x1f\x95a[\xda$zU\xa3\xc4\xe8\x90K\x8c;\x1f\xberG?\x8a\x8a\x9b\xdb@\xe5\xbe\xa8\xeb\xed\xb5)^\xe2\x1b{[a\xf2\xba\xb6\x08#\xe4e\xe9\x9e\xff\x00Z\xb7gp\x82;\xb8\xdfsE\x1b:\x82~\xf2\xc89\xc7\xe4x\xfaV\x0e\x97\x15\xfa\xdf\xfd\xaek\x80\x16\x11\x1a1\\\x15V\x0b\xc8>\xc7\x8e}\xebk\\\x86{\xb5y\xa2\xb5O%\xb6\x96\x91{\x90\x0f-\xf4\xf5\xf45h\x92\xcd\xbc\x1fos=\xcaf\x16o3\xcbu\xe62Nr\x0f\xeb\xf9\xd5\xab\xbbX\xe5\xb7\x9a\x03+\xb0*\x826S\xbbc)\xf9I\x1f\xa7\xe1P\xd9Cy\xf6H\xd9.C|\x9bH8r\x87\xeb\xdcu\x14\xcbU)<\x9e^\x01?,\xf0\xabr\x1b\xfd\x9c\x9e\x9f\xfe\xaa.\x05\x11%\xce\x9f\x13B\xd0\x99-\x9erHo\x98\x8c\xf5\xdb\x8eq\x9f\xcb>\xd4\xa2K[\x0bqpLw\n\xb20\x01\xf8\xd8\xc4\xf4c\xfc'\xb7\xd0\xd5\x99\x9e\x04\xd4E\xd0w\x84\xaa\xb10L\x7fw)\xc78\xcfC\xc1\xae[Q\xb9mKQ\x96DSoo \xc4\xca\xcd\xf7\xbd\xc69\xfd=}(\xb7qz\x17d\xd4\xbc\xf2\xd0\x8bU\xdd3\x06\xf2\xe7$\x08\xcf\xfb,8\xedMmM-s8\x82\xe5\xd5N\x1aD>b\xa6=G\xf5\xab\xdav\x87m\x16\xd1\x1c\"h\x9c\x10%l\x9c\xf4\xe1\x86F:\xff\x00ZIg\x86\xc2)\x0b\xc7p3\xf2\xbcc\x90\xab\xd3\x80~\xf0\xf7\x04\xd3@\xccy\xf5\xd9nA63\xa1\x8c\x1c\xbc~R\xf2?\xd9\xc9\xdaz\xf23\x9a\xa5\x7f\xab\\y2\xb8i3\x81\x1b2\xe0F\x01\xe9\xc0\x1c~u\xa3<wy\x1f\xd9\xf0\x0bH\xe5o\x97\x9d\x8c\xc7\xa7C\xfc\xb9\xfa\xd5\xdb]&1\x08\x17)\x93\xff\x00-9#9\xed\x83C\x04b\xea\x16%\xf4\xa8\xa4y'9}\xe7`=\x0f\x1f^\xf4\xd7\xd1&\x82\x06\x88\xa0\x10\xb9\x05\xc9\xe4\x93\xfc\xebOZ\x86\xd6\xcaky\xf7\xce\xae\xad\x87BN\xd2\xbe\xcb\x9f\xcf\x15\x8b\xe2\x1b\xcf6dT\xbb\xdc\xee\x01EF\x1f?<\x13\xc7\xa7~*JCo\xef&\xb4\xb7_\xb2>\xf8\x99w6O\xca\xbe\xd8\x1d\x7f:\xab\xe7\\I\x1c\xa6\xe1\x95\x08\\q\xcb{\x12\x0f8\xac\xdd~\xef\xec\xb7\te\x1d\xb5\xc4HN\xe3\xe6Hp\xccy<\x1f\xe8j{\x8bM\x8bfd%\xc9\x05\xdb#\x8f\xf7s\xd7\xfc(\xb5\xc2\xe5\x8d.o\xed+\x88aYf{\xc00x\xc08\xed[\xf2N\xcf4\x9fi\xb92H\x83h^\xb8\xfcx\xaeqY\x8b\x1b\x9bx\x1e+\x98r\xdeX\xe4\x91\x8eJ\xfd+F\x1b\x98u\x7f+\xc8-\xf6\xe2\xa3{?\xdd\x03\xdb\x8aiXGc\xbe\x8d\xf4\xef+\xda\x8f+\xda\xbeG\x99\x9e\xf5\x84\xf3(\xdfN\x11z\n\x0cD\xf6\xa3\x99\x85\x88\xf7\xfb\xd1\xbf\xde\x9f\xe4\x9fJ_ \xd3\xbc\x87tBX\xe6\x90\x93S\xf9\x14y\"\x95\xe4\x1a\x15O<\xd5\x0b\xff\x00\xf5-[\x06*\xcf\xbf\x8c\x08\x1b\x8e\xd4\xe3'q\xa3\xcf\xe6\x18\xbb5\x99\xa8\x9ekV\xe7\x8b\xd2+*\xfdwJ\x17\xd4\xe2\xbe\xba\x83\xfd\xd2\xf4>v\xba\xfd\xec\x90\xcb;I.\xd8\x05\x1cW[\xa1xP\xdd]\xa1\x91N\xc0rsRx{NQ\n\xb1Q\x9cW\xa3h\xb6k\x0c!\xb6\x81\x9a\xe2\xa5\x89\x95j\xfc\x91\xd9\x1d\x93\xc3B\x8d\x1ein\xcd=>\xce++t\x8a5\x00\x01\xda\xac\xbb\xfaT`\x91H\xc4\xd7\xa6y\xf6\xd4\x82\xe8\xee\x89\xab\x03\xcd\x02C\x9e\x085\xbf/+\\\x1f\x88\xf5?\xec\xeb\x9e\xb8\xcfAT\xbb\x18\xd5[3\xba\xb2\x97l!\x8f<qH\xf2<\xaf\xc9\xe9T4\t\x9e\xebL\x8eV\xea\xc35t\xa9\x8c\x1f_Z\xf0\xea\xbfy\xa3\xda\xa7\xf0\xa6R\xbc*\x14\x9e\xa6\xb0.\"/p\x14\x1c\x13\xe9[\x92\xc2\xf7\x12\xfc\xdc\"t\x1e\xa6\xa9\xde\xe9\xf3yxPw\xb7S\xe9X\xb5ust\xf5\xb1\x9dp\\\xc4\"\x87\x04\x0e\xa0u\xac\xff\x00\xec)\xd8I(\x04\x823]^\x9f\xa4,q\xed#'\xab\x13Z\x92[\x01\x18@8\xef[\xc2o\x96\xc6SIJ\xe7\x00t\xf6\x96\x15\x8c\xa1f\xadM3\xc3\x08\x8aet9#\x9e+\xa8\x16j\xd2\x00\xaa85ua\xda1\xd8u\xa9\xa7{\xb1\xd4\x9a\xb2\xb1\xcc\xc3\xa2\xdb\xbb\xae#\xdb\x83\x9cV\xf4vQ\x01\xf3($\x00\rH\x90\xaa\x96\x7fZE\x97 0\xe8G5Q\x8f!\x9c\xa4\xe4\x8aWV0yC\n\x08\xdd\xe9YOa\x15\xac\x9ed3l\x07\xe6\x00\x8a\xd7\xbb|[\xcc7\x10W\xe6S\\\x8c\xfa\xab\xfd\xa3\xca\x90)\x85\xfe\xe0n\xc7\xd8\xd6\xa9&O3H\xb9w%\xecRn\x8e\xe0(\xfe\xe8\xe45cK\xe2K\xd8.\x84S;B\xad\xdf\xa8?QV\x9a\xf2&\x95\xac\xa5\x07v\x0bB\xc5\xb1\xff\x00\x015Rxm\xf5\xdd''b\\\xc0p\x1c\xff\x00Z\xb8\xc1-\xc5)2\xad\xef\x88nT\xa9\x8ehdI8+\xc8\xc8\xf4\xcf\xadK\x15\xc4W6e!\xb8h\xdb\x19{yX\xfe\x86\xb0E\x9eb\x92\xd2\xed<\xa6\x07\x02E9\x1b\x87B=\x8dd\xcf\x19\x84\xaa98\x1c$\xaa{z\x1a\xa7\x08\xdbB\x14\xdd\xf6.\xea-g\x1eT\xc6\xea\xa7\xbe\xec\xf3Y\xd09M\xfb_tO\xce=\rV\x16rH\xc57\x17\\\xf0G5z\x0bB\x13\xcb\x1fy\x0f+R\xdd\x95\x8d\"\xae\xca\xb2\x03,\x8a\xea\xbbw\x0c\x8fcV\xa2\xd3\xa5e\xf9\x81\xebZ1\xe9\xbb\xedK\x00A\x07\x8a\xd0\x86)cTVBW\x1dj\x1c\xec\xb4-C]L\x95\xb3\xc3\x98\x89\xc0#55\xaaIi*\x86\x0c\x139\x0e\xbd\xabf;x_\xf7d\xfd\xe1\x95'\xa85 \x80D\x0cn\xb9\xf7\x15*F\x8e'E\xa2\xc8oC\xefa\xb50w\xb9$`v\x15\xb5k5\xb4\xf6\xd2\xa2)i\xf2Wvz\x8fZ\xe7,-<\xb8Q\x81)\x10\xe7i=\xff\x00\xad^\xba\xfbM\x9c\x90\xbd\xa6\xd5\x87\xac\x84\x1e[\xfd\x9fq]T\xdd\xd1\xc5Qj%\xcf\x82\xe5\xbc\x8d].$F-\x92\xa0\xf1\xb4r\x17\x1f\xa9\xfc+\xa2\xb6i\xf4\xdb\x18\xad\x98\xc9<\x81p\xccN\x07\xe2};T\xda}\xd0\xba\xb4\xf3\x81\xd8\x14`\x821\x96\xeb\x8e}\x05Cw\xac\xe9\xf1\x94\xb7\xc0y\x9d\x82*\x01\x9f\xc7\x03\xfc\xf3[\xa4\xedc\x06\xc9'\xbd\xd3\xd67\xb8\xb9v\xf9W\x0cS\x9f\x7f\xca\xb9\x89<E\xa7\xb5\xccv\xb6~r\x9b\xa2!\x8cg\x94\\u\xf6\x1d*\xb6\xb7q\x15\xe4r\x14\xb9_\"\x02L\xa9\t\xdd\xb7\x1dF\x07\x19\xfe\xa4\n\xe7o\x96\xe0G\x05\xfc6\xcf\x0c\r\xca\xbc\xad\x86l\xf4\xc7\x7f\xf3\xedH\xa3\xb8\xb8\x92\xc9\xb4\xb9\x0cw1\xb7\x9b0XG\\\xe0\x00?\xa9\xac\xdbc\xa7Yiki<\"X\x94\x19b@rIf\xe9\xed\xd7\xf5\xaejD\xb9[kHg_)\x13\x0f!\x19\x1bFp\xa0\x0e\xe7\x9c\x0f\x7f\xa5hZ5\xee\xa6\xb2m\xb4\x11\xbc\xe0\"g\xf8\"\xc9\xea}x\xcf\xb5C\x1a5|=w\r\xa5\xb4\xf6\xd7\xd6\xe5\xa22yR&\xe0p\x0ex<\xd4\x96\xfa\x85\xc6\x9dd\xc3O\xb8{\xd4\x888e#,\xca1\xf2\xb2\xf5\xdc\x01\xea:\x8a\xe7\x96\xfd\x92\xe9\xee\xdb\t\xe5\xa8\x19\xceC\xe5\x8eX\x8e\xe7\x1bzz\xd5t\xbf\xb5\xb3\xb9\x02\xe6yV\x18\xee\x06J\x93\x85R\x1b##\xb7\xa7\xd2\xa9&\x89m\\\x9fN\xd5\xf5\x15\xbba\xa5\\\xab[\x0f\x9e(\xb1\xd3<\x88\xdb=\x86\x08\x04t\xe9Z\x12%\xf6\xbf7\x9ft\xedcr\xaa<\xc0\xa3tn3\xd7\x03\xeb\xcfQ\xf4\xc5TmGO\xb5\x13[\xaee\xb9\x8cnR\xa7\x1ebw`\xde\xa3\x93\xf8UK_\x13i\x81\x92k\xa9%[\x88x\x00\xabmb\t\x1b\xb7\x0eA\xf5#>\xbd2\x05Y\x8b\x99\x1b\xd6\xfe\x18\x9e\xf9\x19&\xbco!YY\x01bv\xe3\xb7'\x91\xfa\xf4\xad\xabH,mm\x9a\x18J\x9f-\xbc\xa7i\x17!A\xc9\x00\xe7\xb1\xe7\x06\xb2\xb4Mi<B\xf7Q_\x16\x8d\xe1`\xf1\xe4\r\xc5=\xf1\xc3\x8fB>\x95\xa3s\xa8\xe9Vs\xc6<\xe8\xe4\xdc\x02HCc\x1f_\xd3\xda\x93Lzt&\xbe\xbd\xb7KV\x81\xa56\xc5\x86\xd0~\xf7\x1d\xbas\xf8\xd6*\xd9\x16\x94%\xc6\xa1'\x920Z)\xe4,\x8e\xa7\xb8\xcd;[\xf1\x04\x16\xb07\xd8N\xf7Rv\x9d\x81\x80\xf5\x04w\xfc\xf3\\\xc5\xad\xc6\xad|\xf1M\xaa^\xf9I.@\x83\x01A_\\g\x83\xef\x8ak\xccGZn\xbc=\x1c\x9b\xe0\xbc>b\x10\xa6&}\xe3 \xf0py\xebT\xaeu(\xae\\\xcb\x1d\xd3\x90\xf9_*\x07\x1fx\x1f\xee\xb2\x82\xa7\xf1\xa8\xa1\xb1\x86VRt\x8f\xf5\x99X\xe6(\xac\xca1\xdd\xc0\xc7\xe0i\xba\x8f\x85\xae#\x11\x98\xed\xe3h\x18\xe4\xa8P\xa5\x07\xa9 \xf4\xf7\xc5&\xc6\x90\xf9o\x12]:5\xbdw\x91X\x90\x19\xe1\xdb\xb7=@=3\xf4\xc5b<D\xdf\xbcV7\to\x1cx\x90; $\xfa\xf0\x07B*I\xc1\xd3\x96{2e\x94\xf1\xf2+\x13$#=T\xe7\xe6\x1fZ\xc2\x87U\x9d\xb5\xef\xdd\xc9-\xdc\x119R\xf8\x00\x95\xeb\xc8\x1c\x01\xfaRc\x1d\xe2\x0bQ\xfd\xb2\nM\x1c\xc1\xf9f9n3\x90\x07\xb7pj;\xef\xdf\xb2\xc9\x04\x85\x90\x1d\xd9$\xe4\xb8\xe0\x8c~\x15\xb1q\x14\xd2\xc93\xc8\x82\x04\x92=\xc8\xa4`\xa9\xff\x00#\xa8\xaa\x8bx\x9b\x9f\xed\x05f'\x92\"\\\x9c\xe3\x91\x9f\xebE\xc0\x96\xdfWF\x8c\xcbm\x12I%\xba\xe1\xc9<\xb6\xee:\x1e\x86\xa3\x82\xf5\x85\xfaOe\x92\xf1\xfd\xf0\x0e\n\xfbb\xb9\xd7\x8e\t\xf5w\xf2g\x11\x1d\xc7\t\x90\xb8\x07\xae\t\xe2\xad\x89\xa3\xb7s\x1cn\xf2:\xc8~`yo\xadU\x89\xbd\xcfZ\xfbZQ\xf6\xb4\xae7\xfbh\x7fz\x90\xebJ\x7f\x8b\xf5\xaf\x98\xf63\xec{\xdc\xd1\xeevf\xee:i\xbcA\xde\xb8\xcf\xed\xb5\xfe\xf7\xebM\xfe\xda_\xef\xd3T\xa7\xd89\xe1\xdc\xed~\xda\x9e\xb4}\xb6?Z\xe2\x8e\xb6\x83\xf8\xff\x00ZO\xed\xc4\xfe\xff\x00\xebC\xa3S\xb0sC\xb9\xda\xfd\xbd;\x1aO\xb6\xaf\xadqG[O\xef\xfe\xb4\x7fn\xa7\xf7\xff\x00Z^\xc2\xa7`\xe7\x87s\xb27\xab\xcdf\xeaW\xab\xe46\x0fj\xe7\xbf\xb7\x13\xfb\xf5N\xfbXW\x88\x8d\xd4\xe3\x87\xa8\xe4\x95\x81\xd4\x82M\xdc\xa4\xd2\t/\x18\xf5\xaa\x17\xcc\x16d'\xd6\x96\xd2]\xf3\x93\x9e\xa6\xa1\xd5N\x1b>\xf5\xf4\xf0\x8f,\x14O\x9f\x9c\xefQ\xcb\xcc\xef|=8\x91#A\xdf\x02\xbd*\xd5vB\xa3\xda\xbcw\xc0\xf3\xb5\xce\xa9\x14y\xe0\n\xf6e\xe1\x00\xaeL\x1e\x1d\xd2\x94\xa4\xfa\x9d\x98\xaa\xea\xa4b\x90\xfc\xd21\xe2\x9b\xf8Px\x15\xdeq\x11=q>'\xd3\x12\xf2\xe1wc\xadv\x92\x1a\xe7\xf5\x08\x1e\xe2\xfa4\\u\xeai\xde\xca\xec\x89\xa7-\x11\xb1\xe1\xfbQ\x06\x97\x12)\xe0.9\xad\t-\xcb\x83\x8a\x9e\xce\xdfe\xba(\xe7\x03\xad[X\xb9\xf6\xaf\x1aJ\xed\xb4zjVV(\xdb\xe9\xea\x83t\x87'\xd2\xac=\xb2I\x8f\x94b\xad\x15^\xf4\xda\xb8\xc1%b\\\x9be_)#Rq\x8a`]\xc7q\x1fOz\x9a\xe3\x01@\xef\xda\xabG.\xe7bO\x03\x8a\x8bY\x95wb`#D$\xf6\xebP\xa3\xefQ\xcf\xde\xcf\x15\x14\xf3\x07R3\xf2\x93Ug\xbdDR\xc0\xe1W\xe5\x14\\\x12\xb9by6\xc50\x07\x18\x07\x15\x95\x15\xf7\x97\x12'\xb6G\xbf\xadekZ\xe9\xb7\x8e8FK\xc9\x9d\xf8\xec*9\xae\xa3\x10A8?(b\x8e=+T\xae\xb5\x11\xab}v\x82\x15\x946\x10\x80\xdf\x85sZ\x9a\xfe\xe2\xe4\xc7\xb1\x91\x86@\xeb\x8f\xa5:\xfe\xf4\x88\x96\x1c\xe56\xe0\x1f\xa8\xac\xab{\x92Q\xa3-\xb9\x1b\x8c\xf7SN*\xce\xe8\x1d\xadfT\x13\xc9\xa9\xdb\xa2d%\xdcL@'\xadQ\x9b\xed6rJm\xc9Y\x0b\x07\x1cw\xee*\xdb\xa4\x7flc\x80\t\xe42\x9f\xf3\xcd%\xe1x\xe4\x8aM\xa79\xc1'\xb8\xf5\xad\xdbKS(\xc5\xb2\x04\xbc\x87P\x84\xb4\xe1\xa1\x9c\x0c0\xed\x9fj\xa8\x88\x1d\xc5\xb3\x00\xc3\xaeGqZ\x12[\xe4\xee1\x82\n\x92G\xadVXe\xf9\x1e!\xb5\x87\x04\x0es\\\xf2\x9a{\x1d0\xa4\xf7d\xb1\xe9\xf1\xdb;\x14`0;\xf4\xc5YK8\x9c\x82\x07\xce;\x8a\xb1oo\xe6\xaa\xb3)\x19\xeb\x91W\xa3\xb2\xc0\xdd\xf2\x8e\xc0\x8e\x95\xcf\xce\xd9\xd3\xc8\x91B\xd6)!-\xe6}\xd6\xe8G\xadh\xa3,\x88Q\x94\x0c\xf4\"\xa7\x85\x19PG\"\x02\xbds\x8e\x94\xef't\xc0\xa4y\xff\x00dS\xdc\x9d\x8am\x01h\x82\xca\xb9*x#\xadXh&UY\x10\x87@>\xe9\x1c\x8a\xb5\xb4J\x99\x8d\xb0\x7f\xbai<\xe9\xa0L2\x06M\xbc\x8e\xf8\xf6\xabD\xc8\xaf\x1d\xcb\"\x9f.f\xcbq\xb4\xf65\xd1h3\xc1>\xdbgt\x92u\xe8\x07?\\\xd7\"\xc0H\xe5\xe3\x94)\x1d3\xda\xb44\xadN;{\xc8\xd4J\x04\xac\xdf>\xc1\x92\xde\xc7\xff\x00\xd7]4_s\x96\xba\xd3C\xb0\xd5--\xd6\x13+\x96\x01~\xe8S\xd3\xd7\x02\xb8\xe3u\x1c\x91\\\x13h\xd6\xd0\x82Sx|;\x03\xef\xd7&\xbaQ\xa9\xc3\xadiw0\xbf\xee\xa6\xc1\x0e\x1f\xe5\xc2\xf4,}\x05y\xf5\xfc\xd0\xd8\xdc/\x91p\xd3\xdb\xf4\x05\x1fv\x00=J\xf6\x06\xb6\xbb\xbe\x87=\x95\xb5:\x14\xb2\xb1\x02\x1bkYDPD\xb9\x16\xdbHB}X\xf7\xe7\xb7z\xb3\xfd\x95%\xf5\xc7\x9br\xec\xf1\xc06\xacJp\xbb\x87\x7f\xafol\xd6\x05\xb7\xd9\xb2\xd7r\xde\xc5,\xa1\x83 M\xca?\xe0_\xe0+Z\rb\x07\x85`\x92C\n\xb8\xdf \x891\xb8g\x85\x07\xb6p8\xf4\xaaZ\x89\xe8Oymi\x02E5\xee\xe0!\xfd\xe0\x03\xee\xee\xfe\x11\xee{~u\r\xbd\xfd\xf5\xe6c*-\xe2\x0c\x0b\xc6\xaa3\x8c\x80A=\x07~;\n\xa9\xe2\x1dT\xdd\xea\xe2\xd2\x00`\x82\xde3$\xb2\x91\x8d\xd8\xc7\x0b\xf8\xe0g\xf2\xa6\xde\x96\xb4\xd3\xe2\xb5\x8dU\xbc\xa6-0N\x01n\xbbG\xe2q\x9f\xad;\x13s&\xe2\xc2+\x8b\xc6\x98\xdcH\xcd\xbb\xcc3\x85\xda\x19\xb3\x85D\x1d\x80\xf5\xac\r{T\xbb\x92\xd8B\xac\xaedb\xa4\xf5\x0f\xf3d\x81\xed\xc8\x1e\xfc\xd6\xbe\xa1\x14\xf1Y\xb2\xdc\x86\x89\xe2\x8c,j\x1b\x80\xccH\xfc\xf0\x0e=\x86i\xfa\x16\x97owq\x1e\xa12+\xc5l\xd2lfo\x97q\xfb\xa7\xe9\xff\x00\xd7\xa7f&\xd7C.\xc7A\xbf\x8a\xc2;\xc5\xdcdQ\xbb\xcb|\xe6E\xdb\x93\x8f\xa0\xc8\x15bk(,mm\x96\xf4\x88\xd7;<\xc5\x04\x9d\xad\xd0\x1fOq\xdb5\xb1\xe2\x7f\x12}\x93O\x82\x0bWPf\x939\xc7\xdcRs\xfeEsZ<\x17\x1a\xfc\xee\xef'\x97\x0b\xcb\x1c[\x1f%Il\x9eG\xd7\x8c\xf5\x1cR\xb0\xb4\xd8\xeb-o!ky\x96\xc62\xb7$\xb9h\xc8?8\xe8q\xcfPpq\x9eA\xa8'\xd2_Z{k\x99\xad&\xf2\xcaa\xcc-\x86>\x80\x8cs\xfe}j\xc4\x8da\xa0}\x9a\xfe9Y\xa7\x85\xd4K\t`A\xf9J\xf0}q\xc7\xe1Z\x1a&\xb9\xa5n\xb8\x8bN\xbc\xcby\x81\n\xbb\xf0\xa4\xb1\x01\x87\xb18\xfaf\x8dX\xee\x91%\x97\x86\x96\x08\x19\x072q\x86f*s\xe8q\xdc\x8a\x9d\xb4\xa5\x84\t\nAt\xe0\x1c,\xc49\xcfq\x9e2>\xb5\x87u\xe2\t/<@\x90\xcfxc\xb7\x9a=\xe6\x15\xfe\x022\x181\xee2\x0e\r;V\xb9\x87O\x91d\x93[\x04*\x15\x8b\n\xbebzd\xf5a\xd4T\xda\xc5\xdf\xa9\xa0\xcd\x7f\xbd<\xeb\xab\x18l\x98\xec0[\xcc\xcaq\xec3\x8f\xd2\xadi\xfa\xbd\xa5\xd2\xcd\x00\x8c\xc4\xa8\xc4(gV\xcf\xd0\x9e\xb9\xae\x07T\xbb\xbd\xbcG\xb8\x8a4\x9bx\xc3\xcbl\xc7x\x1e\x98 \xe0V\x12K\xa8[\xc3\x16\xc5\xf9:\xac\x85\x11\xbb\xf4\x0c\x07Z\xbb\x11}OQ\xb9\xb3\xd3m^\xe2\xe1b\x96 \xea\x0b0\xce\xd4\xcf\\z\x0e\xfcq\\\xf2\xe9k\xa5NL\x1bZ\xe1KH\x8e\xaa?z\x84g\xa7\xb1\xfd*\x85\xbd\xa6\xb9|\xb6\xeb\r\xde\xc8\x98\x91\x1a\xb0\xdd\xf5\xc8\x03\xa7\xe1\x8a\xb7\xa9\x1b\xbd\x12\xd2\xd9~\xcc\xcb:e\x95\x9cu=2\x07\xe9\xc1\xa9z2\x93\xb8\xd9\xf5\x013}\x9e\"'\xb7hD\x8a\x8a\x081\xb6\xde@\r\xca\xf3\xd3\xf2\xaen\xddo.c\x9aC \xc2\x83\xb1pF\xe5\xeay\xeb\xc75\xbb\xa1\xe9\xa6\xf2F\xbc\xbfV\x8e5;ch\xf8Q\x93\xdf\xf1\xecj\x96\xa9v\xfan\xac\xc8\x85M\xac\x87\x8d\xa3\xa1\xee\xc3\xb7<\xf1B\x06c\xcdl\xdajy\xd2\xaa3\xb0\xc0\x0e\xa1\xd7\x1f\xd0\xd34\xc9\x0c\x97\xc8\xf3E\xbb\x1c\x1cp1\xebVg\xb9B\xd3G\xb1Z\x17\xe5Fz\x0fQ\xefL\xf2b\xdc\"\xb7R\xe7 \x81\x9e\x08\xebW\xd0\xcck\xca\xfdw\x1a\x8c\xcf'\xf7\x8dH\xeaFj\x06R1U\xca\xbb\x13\xce\xc43H\x7f\x8c\xd3|\xf9?\xbeh#\xde\x90\x83G*\xec.g\xdc<\xd9?\xbeh\xf3d\xfe\xf1\xa6\xe3\xde\x8c{\xd1\xca\xbb\x073\xee;\xcd\x7f\xef\x1a<\xc93\xf7\x8d7\x1e\xf4\xa0c\xbd\x16Av;\xcc\x7f\xef\x1a\x0b\xb1\x18-I\x8c\xf7\xa3\x1e\xf4Y\x05\xd9v\xc0\xfc\xd4\xddS\x91Ke\xc3Q\xa9\x0c\x8c\xd5\x08\xea~\x18\xdb\xf9\x9a\xb4\x92\xe3\xee\x80+\xd9\x07\x07\x9a\xf2\xcf\x85\x90|\xb3JGS^\xa4)\x17\x11\xc4\xf6\xa61\xc0\xa5s\xc5D\xccqM\x0c\x8eC\xc1\xaa\xf6v\xbev\xa0\x19\x86q\xd2\x9d+\x1a\xd4\xd2-\xf6\x82\xec\x06\xe3YW\x95\xa3b\xa9\xab\xc8\xd2\x8e-\xa8\x06)Jb\xa6\xed\xc55\x8e\x075\xc3\xca\x92:\x1bw f\xdbPI'\xca[8\x1d\xeaW;\x87<\n\xc9\xbe\xbb\xc01'o\xbd\x8aOB\xa2\xaeI\xe7\x07\x90\x16<\xf6\x15B{\xc1\x90\x01\xc0$\xe7\xf0\xa8\x1eICK/`\xb8QX\x93I4\x8a\xca\xb9\xc8\x1b\x81\xac\xb56H\xd6:\x8a5\xd4\x91\x83\x9d\xa3ma\xdfj;\xcc1\x10v\xcb+\x0c\x8fJ\xa7i$\xafu&\xf0K\x83\x9f\xa8\xc5T\xb8g\x88\x92s\x88\xd86i\xb5m\xcb[h,\xf7\t=\xd1/\xce\x08\r\x8e\xde\xf5+\\\xa4\x12\x1bi\xfe\xec\xbc.}Ef\xcc\x91[\x81(\x901\xcex=A\xa6\x16k\xb4U\xd8]\x97\xee\xd5s\xa4\x85\xec\xdbe\xb9\xa6\x11\xda\x9f1wg\xa6=*\x96\x9a<\xc9\x99\xf8L\xb6\x00=\xeaqg\x7f$\x91`|\x98*r;\x1fZ\x9e\xdfF\x9296\xed|\x9e\xfd\xaa}\xb5\xb6)Q\xbe\xac\xa6\xd1\"\\Hd$\r\xdd=j\xc3\xdb\t\x1f\x11\x96h\xca\xe5w\x0e\x95\xbbo\xa2y\xa4\x16e\x07\x1c\xee\xe6\xac\xa6\x9e\xb0M\xb0\x90W\x19\x04Vn\xa3e\xa8F'5mj\xcc\xa41\x1b\xd4\xe0)\xe3\x8a\x9d\xac\x95n\x11\x94\x95\xcfP\x05o}\x85K\x120y\xe3\xda\x96[vE\xc2\xa0'\xd4\xd4^\xe6\x973\x84\x12'+\xc8\xfau\xa7\x84\x07\xd5ry\x1d\xaa\xfa\x9d\xd1\x05\n\xa1\xc7@OQQG42)F\x01Yx4-A\xbb\x07\x90\x1dp\x9c0\xec\x7f\x88S\x94\x10\x14\xa7\x0e?\x84\xf6\xf6\xa7)\x11t\x0cH\xfeU\x1d\xc1\x0f\x86\x00\x829\x07\xd6\xb5\x8a3lcm\x97/\x82\x8f\xd7\x9a\x86I6\xa7\xcd\x8c\x9e?\xfdT\\\xdc\xb1^\x98 \xf5\xf6\xf7\xaaWRy\xf1\xe1\x8e\xc3\x8e\xd5D\x15/\x93\x04\xc8\x8c9\xe7\x8e\xc6\xaa\xc4%\x92\xea9buG<\x12\xed\xb4\x11\xdc\xf1\xcdU\x9d\x99AGb\xbd\x83\x03\xc1\xf4\xa8ln\x00\xbbV\xb8lF\xa7\xe6\x03\xa95q\xdc\x99\xbb\xa3\xb6\xb9\xb2\xb9\xbe\xb3\x16\xf6\xc5J\xb2\x80q\xf2\xab}}\x85`]\xe8&\xd2\xdeX\x9e\"\x8c\x17\x8d\xb2\xe7w\xa9\xc5v\x1a\x06\xb1gx\xbf*\x85\x9c\xfc\xaa\x1d\x81lz\x9cp>\x95_\\\xb4\xb9\xb8l\xd9\xaa\x95\xc6\x19\xc3u\xf6\xf7\xae\xc7-48m\xae\xa7\x9b\xdb,\xd6\x92!u\xdd\x01\xe1\xa3#\x07\xea9\xe2\xadI\"Mr\x97\x11y\xeaG\xdcVl\x0e\x06\x01\xf7\xaa\xba\x8b\xc9\xa5\xc8r\xeb*\x06\xc3\xf3\x92\xa7\xd0\xd4\x91\x9bi\xe1K\x97\x9c\xc5\xb4\xe4a\xc18\xfa\x0f\xebU\xe6&\xacu\x04,P[\xdd\xde\xc4<\xc4!\x92\x18\xc6|\xc6\x19\xc6}}\x7f\xfdU4\x96W\xed\xe4\xcc\xc0\x9bx\xdd\xe4\x9dq\x82\xecN\xe3\xff\x00\xc4\x81\xe9\xcdrI\xac\xcb\xf6\xd6h\xe6\x92u\xc6\xd0\xaf\xc6\xe3\xd8\x1c\xff\x00A]\xff\x00\xdb/?\xb3\xe3\x0e\xab\x1c\x861\x94\r\x8cg\x1d;\xf3\xcf'\x15IhD\xb705T\x89-\xcd\xce\xac\xc0\xee\xfd\xf3\x19\x0e\xd2\xee\x061\xf4\xc1\xc7\xd3\xebX\x96\xda\xc3\xea\x01-\xc3\xc7\x0c!^A\x11\x18Rs\x92H\xee:\x00=\xabOR\xd3\xa2\xbd\xbd\xb8\xbb\xba\xb8[\xd9\xfc\xc2\xb0\xc4\xbc\xae{\x01\xf4\xc15^\xe7L\xb3\xd1\xad\xa4\x92R\x93]\xc89@:\x0eN\x05;\x8a\xc7)|n5\x1dn8\xd4\xab\xbb\xbf\xcc\x1b\x1cz\xfb\x0e\xf5\xd4\xde\xfd\x83\xc3\xfaP-x\xb2M\xe6\t!(>o\x7fl\xf09\xf75\xcf\xdc\xac\xfal\ts#\xc3\x15\xd1r\xde_R3\xeb\xfe\x15J\xf2\xfe\xd2\xe2\x05\x06\x07\x92\xf6S\xb9\xdc\xb1\xc0\xf6\x02\xa4e\x86\xd4u\x0b\x9d2\xed\x99ckk\x86bRO\xbcI\x1c`\x9e\x9e\xbf\x8dP\xb1\xd3\xe6R\x12\xce\xe1VYS.\x1c\xf2\xac\x06\xf1\xf5\xe4~\x95f\x1c\xc9\x1aG3yH\xbc\xa8^w\x1e\x98>\x95\xad\xe1\xbd&\x1b\xfdHN\xf3\x88<\xa9\x00\xe3\xa1\x07\x8ai\x89\xc4\xb1r\xb2\xe8w\r;X,\xaf4\xcb)s\xd8\x1eYW\xd0\x13\x9f\xd6\xb9{\x9b;\xc6g\xf3\r\xbbFHp6\xe0r3\x90\x07#\xf0\xe2\xbdK[\xf0}\xec\xf2l\xb7c5\xb2E\xe6\"\xb1\xce\xe2zu\xfa\x7f:\xc6\xb2\xd2\r\x94\xff\x00c\xbe\xb7\t\x8f\x99\x1d\xc6P'\x7f\xc8\xf1I\xc9\xa6R\x8ah\xe44\x8dJ\xf2\xc2D\x89]\xb63`\"LT\x0fq\x9e\x9f\x88\xae\xb6\xda!\"\x19d\xb2\x95Al\xb3.\xd4\xc9\xe38 u\xe6\xad\xbe\x87\xa6\xc8\xb3C\x024\xb1\x890\x89\x14\x8a\x06\xe1\xc8\xeb\xc9\xfe\xa2\xba\x9bkv\x93M\x00[2*.\xc9\x91\xd5I8\x1c\x1c\x8e\xa3\x14\\V9\xa9\xae.\xec.w\xd8-\xbc\xf1\xed\xda\x8ef1?\xe7\x803Qj:\xad\xc4\xfay]RX\xd1\xc2\xee\xda\xc38\xe7\xbf^\xbe\xd5%\xd6\x9d}e\x13\xa6\x9b<\xf7\x10M\xf3\x1bf!\xc0_\xf6\t\xe6\xb9\xf8\xf4Xn/\x12\x1b\xc6\x92\x17,\x03#\x96Ry\xe8x\xeb\xf9\xd1\xb8\xfd\r\x88|U\r\x8am\x16\xf0\xf99,<\x87\xc8\x1e\xdf\x8d(\xba\xb0\xf1\x0e\x91<\xcfl\x88\xc4\xe1\x9c(\xc2\x9c\xe7 \x7f\x9f\xad\x16^\x19\xd3\xe2\x9c\xa8bv\xfc\xcc&\x1f8\xe4\x8e:\x02)5]>\xdfB\xba\xfb\\e\xe3\x0cB\x16,P\x02{\x91\xf5\xech\xb2\x0b\xb3\x93\xbaf\x9e\xee<F\xb0J\x8b\x8c\xff\x00{\x9e2>\x95\x05\xbbK\x16\xa9\x16\x14D\xd9\xda\xdc|\xa3\x9a\xe9u\xbd?\xed\x96_\xda\xd61F\xd1\xc6\xdf\xbd\xf2\xc0\xe4\xf4<\xe7\x8f\xc2\xb8\xf8&\x98\xea<\x92\xa1\x87Ls\xf9U\x99\xbd\x19\xa0\xeb\x93P\x95\xf6\xabn\xbd\xea\x16\x15v \xaeP\x1aa\\T\xf8>\x94\xdcR\x02\x02\xbe\xf4\x98\xf7\xa9\xb6\xd1\xb4P\x04 s\xd6\x97\x15.\xdf\xa5\x1bh\x02,{\xd1\x8fz\x94\x8cSq\xcd\x00Oj0sM\xbe;\xa9\xf6\xfcTw\xbd\xe9\x81\xea\x1f\x0c\xa1\xdb\xa4\x17\xc7\xde&\xbb\xfe\x95\xc7\xfc<\x87g\x87\xa28\xe4\x8a\xecI\x00R)\x0cs\x8a\xa5qw\x1c|\xb3\x8af\xa1z\x90FIj\xe2u\x1b\xf6\x9e\xe0\xecs\x8f\xad)MEj\\b\xe4\xf4:u\xd5#\x92\xe0 9\x19\xed]\x96\x9c\xa0[\xa9\nFGz\xf3\r\x18H\xd7\x88W\x9ey\xafM\xb5\xb8E\x8dT\xb8,\x07@k\x8aU\x1c\xd9\xd1\xec\xf9\x16\x85\xe2qQ\xb7\xcd\x96jO3p\xcdE4\xbf-K`\xb7!\xbb\x93lGi\xe4\xf4\xac\x98\xe1\xde\x8e\xcd\xc9'5~\\\xb2\xb3\x1fL\naP\x91m\x03\xa7\x06\xb3}\xcdV\xd62\xae\xc8V\x03\x1cc\x1fZ\xcex\x0b\x0f\xdd\xe1\x1f\x19\x00\xf7>\x95\xa3\xa8\x1d\x97\x11\x80p\x08\xac\xeb\x9b\x8c\x8d\xaa9\xf5\x1d\x88\xac%S\x94\xe8\x8d;\x992\xaa\xc5s\x1c\xd9(\xeaB\xb0\xf5\x15\x0e\xa3'\x9b.\xd8T|\xe9\xcej\xe4\xd6\xa6\xf1\x83d\x86\xee1O\x87L\x11)\x12\x9d\xdd\xc1\xc5d\xe6\xd9\xbca\x14s\x96\xfaA\xb8*\xac\x08 \xf1\xc7oJ\xe8\xect@\xa39!\xc7lU\xabu\x8dF\x02\x80\xf9\xc6@\xab\xcb\xbc63\xedR\xe4\xc6\xca\xeblT\x11\x81\x9f\xa5\"\xc3\xb6M\xacX}E_\x86\xdc\xaeT3\x03\xd4\x03S\xb0R\x9f2\x9e)\x91\xccS\xf2\xd4\x01\xc52HC(;H#\xa1\x15p\xabt\x08\x18\x1aa\x8aB\xbb\x17\x05s\xd1\xbb~4\xd2%\xb2\xa0\x8c\xed;\xb6\xfe?\xe3Q<NG\xdc\xc1\x03\xa19\x06\xae\xb4\x12\x10@\x1b}Fr\rW1\x8eA\x1bO\xa85W\xb0\xd32\xf6F$\x1eb\xed=\xb9\xe2\xa6[t$\x8c\x10\xc7\xbdO\xe5),\xbb\x83\x1e\xc1\xb9\xa8\xe4\xdc\x08f\x07\xe5\xee\xbc\x11M0a/\xeeB\x14\x03?t\xfa\x1a\xad(\x03\x92\xd8\x1dq\xe9\xf4\xa7\xca\x1aX\xd4\xa3\x03\xb4\xe7$PR)\x99w\x15\x04\x8e\x83\xd6\xb4Z\x92S\xb9\x08c\x05z\xe7\x04\xd6l\xb9x\x9d\x18s\xd8\xd5\xeb\xa2\xd0\x82\xac\xb9\x8f\xb1\x1d\xab\"[\x92>^\x81\xb8\xf6\xa6#\x0e\xe6\xe7hx\xa4S\x8c\xf4\xff\x00\n\xcb{\x96\x12\xe5pI\xee\xc35\x7fP\x90\x19J2\xe1\xc0\xe0\xfa\xd64\x8c\x01\xcf\xa1\xe9Z\xc0\xce]\x8e\xc7J\xbb\x94Z\xec\xb7V\xdc\xff\x00.#\xc0v\xcf'\xe9V\xf5_\x1d6\x91\x03Y\x98\xe1\x17J\xbbv\xa1\xce\xc1\x8e\xff\x00O\xd6\xb9\xcd\x0e\xe6T\xd4!\xf2\x97s\x93\xc6O\x03\xde\xb7<W\xe0\xf8\xee-\x12\xf3z\xb4\xb2\x9f\x9b\x03\x1b\x8f\xd7\xbduS\x92\xb1\xc9Ukt`\xd9N|W,\x90\xbcl\xd20\xcf\x9a\xaa\x14{q]L>\t\x83L\xd3\xd1e\x9b29\x19D<\x93\xf5\xff\x008\xaeS\xc3\xf1\x8d\x03T\x06x\x1c\x18\xdbj\xa8\\\x9f\xaf\xd6\xbd\x17L\xb8\x9e\xff\x00R>pTE\x03h?3/\xb9>\xbe\xc2\xb5I3\x16\xdfQm4M?D\x7f:B2\x06HQ\x80\x87\xdc\xf5\xfe\xb5\x89\xaa\xdd\xcf9\x91t\x98\xe4.\xe3{n\x1c\x05\xe8\t\xeeI\xed\x9e\xd8\xae\xde\xe2\x18\xef\xe5{4E\xf2\x93\xef\x12\xc3\x0b\xfe\xf1\xf5\xf6\xa6^Ka\xa5Y\x98\xa0T,\xd8\x0e\xea\x00\xde}\x056\x85s\xce\xe1\x8e\xfa\xce\xd2Ha\xb7u\x93\xa9\x9d\xb1\x90\x0f\\\x0e\xde\xd5R\xea\xe6\xda\xda[K5\x8f\xcf\xbc\x9d\xbf\x88\xee9\xee\xc7\xf9\x01\xed\x9a\xd1\xd4\x9c\xeb\x1a\x89\x8a\tH\x86 K\x88\x8e\x13\xdb\xeaj-\x07L\xb3\xd2\xee.u\x9dQ\xccF0DI\x9c\x94__\xad\x08\x0eoZ\xb5\xbdgp`%\xc9\xea\x01\xcdW\xb0\xb4[\rB8\xe6\xf2\xe4\x9d\xb1\x9c\xb08\xcfn\x87\x15\xbfk\x7f\xfd\xb3\xa8\xcfs\xa7\xa3\xe0\x86\x08\xf2\x9e\x9e\x98\x15Y,m\xe1\x9c\xa0\xf3&\x9b\xfeZ9\x03\xaf\xd4\xf4\xa4\xf4\x04>h,\xd29n\xa1\x90\xa3\xac\x87\xf7\x0e\x80\x0e\x9d\xb9\xebX\xf6\xba\xbc\xfa=\xcc\x84\xc5\x98\xa5\x07|du\xe7#\x9e\xdc\xe2\xba\x89\xadf\xf2\x942\xa8Lp\x98\xc6=\xf3\xd75\xca\xdf\xdb\xf9\xb7\x1c\x12\xc5\x98*\x93\xc7\xd6\xa1KSYGC\xd7\x8e\xbao\xbc;kyhX\xc4\xc8T\"\x8c\xb0b2\x07\xe03\xf9\xd7%\xe2\x0f\x18\xc3\x14V\xf0N \x96\xe9cia9\xda6\x93\xf7\x0f\xfbC\x1d\xfa\xf3\xedG\x845\x15\x8f\xc3\xed\x14\x8aB y\x17\x8c\xe3\x90\xa0\x0f\xa9\xe3\xe9^}qa>\xad\xac\xb8B\xd2\xa8\xceH\x1fw\xd3?\xa7\xe7ZYnb\x9b\xd8\xbd\x0f\x8coV\xddc\x8d!]\xa7\x03t`\x8e:\x1f\xf6Oo\xf0\xae\xbbI\xd6\xae\x8d\x87\xda%\xbb\x10:\x90B\t\t\xdc\x0f9\x03=>\x95\xcb\xe9^\x1f\n^9bi\x19\x08\xde\x89\xdcg\x18\xf4\xcd^\xd5\xa2\xbd\x8e\xd14\xf8lXG\x16wwb8\xe9\x9eT\x8c\x8aI\xa67tm\xc9\xe3\xdbh\xef\xd6\xde\xe5%\x92\x02\xfcJ\x00R>\xa0u\xe7\xbf\x06\xbag\xd74{\xcbU\x95\xc3N\x15\x01*\xea\t\x19\xf5\xe7\xa7\xe7^Ko\xa3\xdf\x07\x00\xc2J\xaf\xcc\x19\x88\\\x83\xc63Z1_k6R\x18\x8d\xacr\x83\x9e\x1e.FF\x08\xc8\xf5\x1d\xb9\xf5\xa7d\x17g\x7f\xff\x00\t\x05\xa5\xc5\x9a\xec\xf9\x88l\x00\x92.\xe8\xb3\xea\x0f?\xcf\xfaUk\xb6\x86\xf6\xd2e\x8agY\x00\xdd*M \xc3\xafF\xda=\x7f\x1a\xe4,\xfc/=\xe4\xb1\xee\x85\xe12d\xe1\xc1\xf2\xf3\xf5\xfc\xbf\xc2\xafZiZ\xbe\x8f*\xcb\x0b\xc9so\x92\x06\xe1\x91\x95\xfeT\xac4\xc6Y\xc8\xb1\xe9Z\x95\x81\x89Dh\xa5\x82\xc6\xff\x00;\x8e\xc5Oq\\\xe5\x92Cv\xd1\xb4N\xd9BX\xc7)\x18\x1fBk\xa2\xf1\r\xbcF8\xae\xe5E\x89\xe7\x05YcR\xbfA\x8f\xc7\xb5d\xe8\xf1`\xb7\xfa\x19\xbb\x8d\x19\x8c\x91\xb3|\xf8\xf5Zh\x97\xb96*6N\x98\xa9\xb1\x8amjfW+\x8aiQS\x11\x9aa\x14\x01\x11@i\xa4b\xa5#\x14\x84f\x80!\xc5\x18>\xb5%4\x8a\x00m4\xf5\xa7\x13M=h\x02k~\xb5\x1d\xef4\xf8z\x9a\x8e\xe8\x164\x98\x1e\xcf\xe0\xa2\xb1xr\x0c\x9c|\xa2\xacj\xde!\x86\xd1J\xef\x19\xfa\xd7#\xa4\xeb\x9eN\x87\x15\xb4*Z\\\x00\x02\x8c\x9aX\xbc7\xaa\xeb\x13y\xb3)\x89\t\xfe.\xb5\x94\xea\xc6\x06\xd4\xe9\xb9\x15/\xb5\xe9/e 1#\xb0\xa9-l\xae\xe5\x1e`\xb7\x91\x81\xf6\xae\xdfD\xf0e\x85\x89W\x95D\x92w&\xba\x95\xb7\x86%\xd8\x91\x00\xa3\xb0\x15\xc59\xb9\xb3\xb2)CDy\xee\x9f\xa2\xea\x12\x95u_-}\xeb\xad\xd3,nR@n$\x01\x14p\xa2\xb6\x921\x8c\x05\xc58B\x07z\xcd&\x8ar\xb8\x9b\xb8\xf9z\x01Q\x1c\x1c\x92i\xec\x85T\x81\x8a\x88\xe1F;\xd5\x19\xd8i\xe8\xab\xeai\x93\xb2\xc6\x8e\xc6\xac\x05\x18\xdcGJ\xc7\xd4&$2s\xc5D\xdd\x91\xa4\x15\xd9\x9f{\"\xcbs\xb5\xb8\x1b\x0e\rD\x96\xf1\xed,\x0e\xf2y\xfa\xfd)\x93\x19\x11\xd2R3\x8eq\x8e\xd4\x9eAw\xf3rG\xf7Ga\\\x87Z\xd0\xb9\x08\x89\xc8 \x81\xfdig\x8d\xd8\xed\x8b\x04\x1e\xd5\x04V\xc4\x13\xbd\x88\x1dO5af\x89HH\x89\xe9\xc1cHw\"\xf2U[\xe6;_\xa6{T\xa1VV\x19%H\xee:UK\xb2\xc6Ln\x01I\xceW\x9a\x9e\x05;N\xe7bON\xd5%4\xed\xa9\x7f,\xb8\x057/b*^H p}\xea\xbc2`dL\xc7\x1e\xabV\x96P\xea0\xdczV\x91\xd4\xc5\x8d\x05\xd7\xe6\x1bM*\xb6z\xe3\xe9He\x0c\x08\xc0\xe3\xd2\xa1.\xbfx\xab\x02=F*\xafaX\xb2\xc9\x9f\xbb\xc1\xfd*\x94\xd1\x91\x93\xb5Cz\x81S\xac\x82E\x18qA\x05z\xe1\xbd\xaa\xb7\x16\xcc\xc9\x92\x19c\x94y\x91\x86C\xd1\x97\x83Q\xc9\x1a\xb2\x8f\xde\xb7\xa0\x0c\xbc\xfeu\xad3\x06Q\xf2t\xf7\xaa7*\x14\x8f\x94\x95>\xdd)%b\xf9\xaeS\xdc\x80mn\x08\xea}j\xb4\xf0\xfc\xe5\xe2`\xad\xdf\x15w\x0b.r\xbd\xb03\xde\xa8\xdc\x07\x86,u\xc9\xc05I\xb4\r\"\x95\xcbL\xc9\xf3\x85c\x8e\n\x9e+\x02\xe2U\x8cH\xcc\xbd9\xe2\xb7K:\xcd\xe5?\xc8z\x86\xeb\xf9\xd6F\xa3lC2#\x02=\xaa\xac.\x87;y\"\xcf>\xf1\xf7\x08\xef\xd4Vj\xa0iI'\x8c\xf3\x8a\xd0\xbeB\xa0\x100{\xe2\xa9\xc4\xa5\x7fy\xb78\xea+h\xecg-\xcd=\x124YSt\x8a\x9b[\x9c\xff\x00\x17\xa0\xfaW\xa6^\xb2]i\x16\xed\x11Vh\x86G\x1bT\x1f\x7fj\xf3+\x08erZ$\x05\x01\xe5\x89\xc6\x07\xbdvVzt\xe7K\xfbt\x8e\xcd\x10\x93,\x0eN@\xf4\x15\xd1L\xe5\xaa,\xdas5\xc2+\xdcff\xc7\x99\"\xafC\x8c\xe0}=j9[\xfb(f+\x92\xd2\xb4\x85\x9ef;\x8ag\x8c\x01\xeb\x8f\xe7R>\xae\xaf\xf2y{\\'\xfa\xb5\x1d\t#\xaf\xbfJ\x8d\xbc\xab\xc9\xa4\x864\x8d\x8c,9>\xb8\xeb\x8e\xe6\xb6L\xc2F\xa6\x94~\xd6\x8bo\n\xbcV\xc8r\xcc\xc7.\xc7\xd5\xbd\xcf\xa7j\xbb6\x94\x9a\xae\xa0\xc1\x14>\xc5\xdb\x93\xf7#\x1f\xd4\xd6\x14\x97\xcf\xa2\xed\x86]\xde@\x19l\x1eec\xea}\x87\xa5n\xdak\x0e\xf6\xeb\xb3\xcb\xb7\x87\x19$\x0ej\xb4b)\xde\xf8xi\xb6\xeb\r\x9b\x08\xdaL\x06\x97nI\x1e\x8a\x07\xf3\xac}K\xc3\x90\\4)}+Ec\x1f\xccbO\xf5\x97\r\xef\x8f\xc2\xbaK\x9d\\\xcb\x19{dfq\xf2\xf9\xd2pq\xec:\xd3#\x9a\xce\xc6#s|\xc0L\xc9\x90\xbfy\x80\xee}\xa8\xdcF$\x96\xa9\xa7\xe9/\xf6[T\xb5yN\x02'\x1b#\x1e\xa7\xa9<\xf4\x1e\xb5\xcf\xe9\xc9$s\x19&\x89\x95\x15\x88\x01\x88\xcb\x1f\xa68\xad\xd9\xbc[ku:\xc7\x04{L\x9f(v\x1fqGSRYY\xd8\xcbn\xf7\x7f;\xc5\x9d\xaa[\xab\x81\xdf\xe8s\xc5\x0f`[\x89\x1bE%\xb3J\xb8\xc2\x81\xc7A\x93\xef\xdc\xd7\x1d\xaa\xa2L\xdbb\x01d\xdcs\xb7\xd0\xe3\xfc+\xaff\xf3\x03G\xe5\xaaD\x87\xe5U=>\xa7\xb9\xaev{\x17\x92R\xf1\xa7\x04\xe7=1\x8fz\xc1\xdf\x9bCu\xf0\xea:\xd5\xb1\xa6\xfd\x9e\xdeP\xf2\xbbm\xf9W\x01@$\x0c\x0e\xfd\x0f\xd7\xf0\xad\x1b\x0f\x0c\xc7\x17\xd9\xcc\xd2:\xbbI\xb2\xe1\x97\xa1\x18\x07\xf9\x9cT\xfe\x0f\xd1\xf6\xca\xf2\xc6C\x18\xber\xed\xd09\xe9\xf9\x0e\x7f\x11]\x05\xccv\xf1\xc5 \x9e\xf4\x06\xf3\x018<\x82H\x00~K\x9f\xadk4\xedc\x18h\xee`G\xa4\xc3\x0c\xecW\xcc\x16\xf3G\xe4\xcb\xb4u\xdcH\x04\xfe#\xaf\xadu\x16\x1a46\xd6o\x1d\xd2\xac\xaaP\x05,\t\xdd\xfe\x15\x9c\xe6\xdd\xa4\x9e\x07\xbb\x9a\x07]\xb8l`\x11\x90s\xed\xf3s\xf8\x9a\xbf\x1e\xb2\x8d\xa6\xdc9\x9e(\xfc\xa2\xc9 \x07\x94\xf4\xdc\x07oqH\xb6V\x8a!>\xc5[V\x1b\xff\x00w\xe6\xa0\r\x8e~\xeb\x8c~\xb4\xcb\x9b{g\xba\x06Hd\x84\xe5\x0eU\xc8S\x8e\x9d9\x04{W4<S\xa7\xa5\xfee\x12\xc7>\n\xf9\x9b\x88\xc7c\xdf\xe9\xcdj[j\xb6\xb3[\xbb\xb4\xf2\xcc\x06\x06\xe5\x983\xa9\x1e\xa3\x8c\xfe4\xd1%[\xbf\x105\xb5\xec\xf1\x06\x8a\xf2\xc6A\xe5\x99\xc0;\x90\xf6\x07\x1dq\xcfl\xd75s\xe2\x99,\xe1HP\xa4\xaa\x1f&D\x19Y\x06z7=\xbe\x82\xba\xfb\xc5\xb1\x99\xc3\xbd\x8eC\x90|\xf8\xc9F,\x07FRy?\xce\xb9\xb9m4\x15\x92h%i#\x12\x9c\xa9`\xbb}J\x83\xd5~\x95H\x96U\xd5\xb58uD\x8e(n|\xa8\xc9\x1f %\xf3\xf9\xf0+3M\xb4\xd4\x1a\xf8\"\xcc\x1cn!\xb0\x07 rio-\x04L\xb1Cu\x04\xb6\xa1\xc9F\x88\x13\x83\xef\x91\xd6\xb4t\xbb\x1b\xbb]&\xe1\xe3(\xef#o\x8dA\xc9\x93\x1dv\x9e\xcd\xed\xde\x9e\xc4\xf5*S\t\xc5Nm\xa6?\xc0i>\xc71\xfe\x03\xf9V\x84\x15\x89\xa6\x9ej\xd9\xb1\x9b\xfb\x87\xf2\xa4\xfb\x04\xc7\xf8O\xe5F\x80S=)\xb5\x7f\xfb:c\xfc'\xf2\xa6\x9d6oCJ\xe8\n\x07\xa9\xa6\x93\xda\xaf\x7ff\xcd\xe8h\x1a\\\xecxBi\xe8\x05\x03\xd2\x98q]&\x9f\xe0\xddSQ|G\tU\xf5a]\x86\x9b\xf0\xca\x18@{\xe7\xf3\x1b\xd3\xa0\xac\xa5V\x114\x8d)K\xa1\xe7zF\x99s\xaa\xde,\x16\xeaI'\x93\xd8W\xa5\xe9\xbf\r-\x11\x03\xdf9\x91\x88\xe8z\n\xdf\xb0\xd2\xac\xf4\xa2>\xcb\x02\x83\xeb\x8a\xd5\xf3$\x90\xf3\xc5rU\xae\xdb\xb2\xd0\xea\xa5B+W\xa9KN\xf0\xee\x99\xa6\x8f\xdc\xc0\xb9\x15\xa4\xd1\xee8U\xc0\xa7G\xb5FX\x8aYo-\xe2\\\xee\x15\x86\xfb\x9bm\xa2\x1e\x89\xe5r\xc7\x14\xf4\xb9\x8c\xb6\t\x04\xd6%\xe6\xb1\x18\x18\xcdr\x1a\xf7\x89\xa4\xb3E1\x1d\xa4\xb5\x0bWd6\xac\x9bgW\xe2\xdf\x11\xdc\xe96\xf1\xa5\x94\x0c\xf2\xc8q\xb8\x0c\x81WtML\xcb\xa7\xc5-\xdc\x9bda\xc85\x93\xa0\xf8\x9e\xc2\xf7OW\xbce\xdd\x900\xde\xb5\xd3\xb7\xd8\xbc\x90\xc9\x1at\xc8\x18\xae\xa5\x1eTs\xb6\xdb\t$\xdd\xca6V\xa2BZL\x11\xd2\x88\xaf\x9d\xc1H\xed\x8f\x1d\x0e*H\x16g\x9bt\x80(=\xab\x1b'\xaa/U\xa3$\x98~\xef\xd8\n\xc3\xb9$\x921[\x97\x05Q>f\xfc+1\xc2HN\x06)T\x85\xf4\x1d9X\xca@\xf9l\xf4\"\x9b\xbcgao\xbbZ2F\xb1\xa9\xc0\x15E\xd03\x1d\xa3\x93\\R\x8f)\xd7\x19\\\xa7\x83p\xb23pA\xc0\xc7qJaibU<d\xfe\x95hD\x01\x00\x8cb\xa4*\x03\xae8\x0b\x9a\x82\xf9\x8aF8\xe1\x8bl`\x93\xfcD\xf7\xa8\x9e@\xc87\x0f\xbb\xef\x8a}\xcb\x80\x81#\xfb\xe4\xf2Mf\xcbv\x1c\xec\x90\x8d\xa2\xa1\x9bGTk\xc3t6\x8c\x05\x03\xd0\x1a\xb8.#+\xf3\t\x10\xfd2\rs<o\x0c$\xdb\x9e\x9c\xf5\xad;{ydp\xcc\xfd\xbb\x1c\x8aqd\xce\x0bsN9\x10\xb6L\xb9\x07\xb0\\\x13R\xe1\x1d\xca\xb361\xdf8\x15S\xca\x98`\xfd\xa1S\x1f\xec\xe6\x96k\xbf (g\x0c\x7f\xd9\x18\xaaL\xcd\xae\xc4\xed\x02\x93\x959#\xf8wb\x98T\x9ce\xa4\x8f\x1e\xa39\xaa\xe3Q\xf3X\r\xc5}\x8f50\xb8\x83f\xc2\xceO\\\xfaU\xa9\"\\Y)\x0eP\x98\xc1\xcf\xadG\x1b\xaa#y\x90\xb0\xcfR?\xc2\xa3yv\xa2\x95\x0e\xc4w\xc1\xe6\x97\xed\xb2yy\x92)\x1dO\xf1\x01\xd2\xb5Vh\x96B\xf1F\xc7u\xb3\x93\xecj\xa3\x08\xfc\xdd\x97\r\x8c\x9c\x0c\xf454\xb7C\xfb\xbb\xd7\xdb\x82*\xbd\xc4\"x\xb7\x02\xdbq\xc8#\x91M!\x15.\xf4\xd0\xd9\x01\xb9\x03(\xc0\xfe\x95\x8bu\x0b\xc6\xbb\x94\xef+\xd5{\x8a\xd7x\xa6\x8e 3\x95#*A\xac\xe9\x1f\xf8N\x03\x8e\xff\x00\xd0\xd5\x01\xcf].\xf2F\x00\xcfcYr\x83\x13\xe4c\xd0\x8a\xe9.\xe2)\x9e\x01\x06\xb0oc\xda\xc4\x85%OQM\x03,\xe9R/\x9aQ\\(n\xde\xf5\xde\xe9\xda\xbf\x95l\x90\xdc\xc7\x8be\\\xef\x00\xe7#\xb7\xff\x00^\xbc\xea\xc65R\n\x9c\x8e\xe4\x1eEuV\x1e \x83L\xb7\x7f\xb7\xb0\x9e2\x08\x8e>\xa4\xd7M=\xceZ\xdb\x16\xee\xac\xac]\xe6\x92\xdar\xe8\x8e$\x99\xbf\xda\xea\x16\xab\xda\xdaN\x97\xdbc\xf9]\xce\x1d\xf1\xc6I\xe7\xf55\x1cZ\xcd\x8c\x12\xf9\xb1\xa0Tq\x98\xe2^\x9b\x8fs\xefR\xea\x9a\xe2Z\xda\x88\xe0h\xcd\xd9\x90\x85\xcbq\x80F\xe3\xf9\x8f\xd2\xb7\xe5}\x0e~e\xd4\xd3\xd5\xb4\xb7\xd4?q\xbft\x85\x89\xdcO\x0b\xc7\x1f\x856\xce&[)p\xfb\x9aV\x08\xaez$j9\"\xabi\xfa\xf5\xbc\xd6\x82V\x97s\xcd/\x92\xa0u\x1dx\xfc\x8e*\xf7\x9b\tU\x95\xf02\x08X\xd4\xf0\xaa\x0eI\xfe\x95\xa5\x89\xb9a\x16\xde\x1b\t\x94&\xf5\x8d\x86\x06pY\xbe\xbe\xfcW2m.\xf5\xbdE\xde8\xc8\xb5\xce\xc2\x15p\x18\x8e\xbfP+\xaa\xd0!{\xe8\xe4iS`-\x95_\xee\x8eI'\xdf\x9a\xb1\xaf\xcfk\xa1\xe9\xc5\xa3\x04,+\x84H\xf8\x04\x9f\xebR\xf7\x1fC\x98\xb8\xf0\xdc\x16\x11\x99\xd9D\x8c\xa0F\x91';\x98\xf4\x19\xad\x1b})\xe2\x8e\x05\x99\x94D\x01c\x8e\xecq\x91\xf4\xec>\x94\xdb{\xb8Z\xc0\x1b\xb1\xf6u\x8cnfc\x8d\x99\x19$\x9fZ\x97O\xd4\x06\xb3\x7f\x1c\xc0l\xb0\xb5\xda\x10\x7f}\xbb\x7f\xfa\xa9\xd8\x9b\x91\xea\xb6\xecd\x8a\xd2\xd6 \xb2;\x8c\x928Q\xff\x00\xea\xaa\xf7\xd6\x8am\xa4\x82\xd8\x12\xd9\xc2\xf6\xdf\x80G>\xdc\xd7G4l\x12k\xb9N\x19\xc9\n\x07\xf0\x8c\xe0~5A\xd4\xdb\xda\x82\xd8Y\xa4\x05cS\xdb\xdf\xf2\xc5E\xb54N\xe8\x8fO\xb5\x86=1#A\xf2\xc6\xd9\n\x0f2H?\x88\xfe5\x99\x7f\xa2\xdcy\xb1\x05V\xe6P]\x8f\x1d\x019\xc7\xe3[\xbak\xa2)\xdf\x1a\xa40\xaf\xdf'\xf4\xfck\n\xfb\xc5\xd1>\xa9l\xb1\x16);\x11\x19a\xd7\x86\x04\xfd9\\}*\xda\xb9\t\xd8\xb1\x1cKw\x14\x9et\xa1\x99_i\x03\x96#\x03\x8f\xc8~b\xb3\xbcgko%\xb2[\xc5r \x89\x14\xba\xc8\x8b\x96\xce8\x07\xbe=\xaa\xb4\xb3\xc9ysu\x1d\xb4E#\x95\x8f\xcc\xbdQ\x94\x82\x18\x0e\xfc\xe7\x8fAX\xf7\xde\x19\xd4\xa6xf\xfe\xd1\x12\xbb\xc6Q\xdd\x1c\x95 r0}}\x8fZIY\x83w\xd0\xe7e\xb57\x89\x12\xdbN\x8er\x02\xe3\xaa\xfd\x015\ni\x1a\xed\xa5\xc8\x96\x03\xb7#pea\xb5\xbf\x0e\xd5\xda\xda\xf8\x16\xd9 Ife\x92M\xdb\x8e\xc3\x82\x06:\xe3<\x8a\xd8M>\x0b{\x14\x88A\xbaH\xbef\xf3\x13p'\x1dr1\x81\xed\xcf\xb5>byO<\xb5\xf1>\xbd\xa3;\xc6\xf2\x90\xa7\xe5*\xc0\x1f\xd6\xba{=n\xdbY\xd3\xca\xdc\x08\xc4\xe9\xf3\x17\xde\x11A\xed\xc89\xab\x1a\xd7\x87\xed.\xed\xf7\xc3\x1c(\xed\x18%\xd5\xb0\xa0z\xe3\xb5r\xf6\x1a<\x9an\xa8\x8a\x0b\xac\xa5r\n\x10\xc3\xf3\xe9OAj\x86\xdf\x835\xc2\xac/\xfb\xf2~G\xe7\xaf\xd7\xd3\xf0\xad\x8f\xb5\xed\xd2a\x81\x9eu\xff\x00\x9e\xd1F@\xe7\xfb\xca?Z\x8eTX\xd6k\x92\xce%\x1c\xe5\xceK\x9fPO__\xc2\xabY\x91q3D\xf1\xba6\xf5\x93n\xec\x02\xbd\xc0>\xbc\xe6\x80\xdc\xee\x8d\xad\xbf\xaa\xd2}\x9e\xdczT\xed\xa5L3\xc1\xa6\x1d&|\x8c\xe4P\n\xc4>U\xb0\xee)\x0c0\x1e\x01\x19\xab\xb1xz\xe2r\x02\x83\x8fZ\xe8\xb4\xff\x00\r\xc1j\xa1\xa6\xe5\xbd\xeb)\xd5Q4\x8d7#\x9a\xb7\xd2$\xb9\xff\x00W\x11#\xd4\xd6\x9c^\x12w\\\xc8B\xfe\x15\xd5\xa8\x86\x05\xda\x81i\x93\\ax\xace^]\r\xe3F=N~/\x08\xdb\xee\xf9\xd8\x91Z\xd6\xda\x1e\x9bh\x06!R}\xc5=.X\x03\x91P\xcd\xa8*\xf65\x93\xab.\xac\xd1S\x8a\xd9\x1a\x06x`]\xb1\xa8\x15^K\xcd\xe2\xb2\xa4\xbfV<\x0ej\xe5\x9d\xb4\x97k\xb8\x9d\x8bX9\xb6\xf45QKp\xf3\xc2\x12I\xa8^\xf9\xf3\xf2#\x13\xec+PYD\x8b\xf7r{\x93I\x84N\x80S\x0b\xae\x86I\xfe\xd1\x9f\xfd\\D\x0fz\x85\xb4K\xfb\xa6\x1b\xe7\x08>\xb5\xb2\xf7!x\xcdFo\x00\xc6(\xb8\x14\x17\xc2r?\xdf\xbb\xfd)\xb7>\x02\xd2\xee\xb6\x1b\xc9d\x93o8\x07\x15\xb7\x1d\xc6Svp+>\xff\x00WXF7\x8c\xfaf\xad4\xb5D\xdaR\xd1\x8f\xb6\xd0\xb4\x1b\x05T\x8a\xd5I^\x84\xf3W\xda\xf2\xdd\x17\x01@\xc7J\xe4\xc6\xa6\xf3HH=\x0fZ\x98\\\x97\x1f3d\xd4\xba\xac\xa5I#u\xb5m\xb9\xda@\xa8\x0e\xae\xcb\x96\xdd\x8a\xc6c+\xa9\x11\xa1'\xb5F4\xcb\xf9\x86\x1d\xc2)\xa8\xf6\x92Z\x97\xec\xe3\xd4\xd8\xfe\xd8Y\x1f\x04\xa9\xfcj\xccR\x89\x17?-`\r\x14\xc6\xe0\xb4\xccO\xb5h\xc6Dk\x8c\xd5B\xb3n\xd2\"t\x92^\xe9vq\x94\xf7\xaa\x81\x14\xc8\x0e0~\xb4\xe7\x94\x15\xc6H\xa8\x04\xea>\xf1\xe6\x95Ewq\xd3\xd8\xb7\x91\xb7\x18\xe6\xa3a\x8c\xfa\x11L\x8eus\x8f\xe7N\x95\x94/&\xb0qf\x8bs&\xedJ\xb3d\xf1\xda\xb1%\xcf\x03 \x02y\xe2\xba\x19J\xca\x08\xe2\xb1\xee\xa09\x04t\xdd\xcdb\xf7:\xa9\xb5k\x10E*`m^G\xadM\x1d\xc4&N%\x05\x8fP\xa7\x15NEX\xb7\xa8\xea\xdcf\xb3\xdaC\x1bI\xe4\x81\x90y\xfaSL\xb6\x8e\xb6\x06/\x90\x8eW\x03\x82MG-\xbb\xe0\x16\xb8D\xf5\xcf9\xac;MZ@\xe67f8\xe8\xb9\xc0\xab\xc6\xfe)@ \xab\xb8\xed\x9c\nf|\xad\x13y\x89\xb9\x80\xde\xe4w\x04b\x91.\x95\\02)\x1d\x9b\xd6\xab\xbd\xf8FP\x90 _c\x91\x9a\x91\xc8\x98`J\x13\x8c\xf22*\xe1b$\x99~\xde\xea\\\x86\x12\xb6\xd3\xd0\x15\xe9Vd\xbc\x95\x8f\xc8\xea\xc0z\x02+\x9e\x8c\xc9\x1b\x93\x11\x89\x9f\xdd\x885\xad\x06\xa1\x1a\x95\xde\x99c\xd4n\x02\xb6\x8b2\x92\xf2\x01$R\xce3\x85\x94\xf4__\xa1\xa7K\x19P\x1e9\x18\xe3\x86\x06\xad\x1d\x8e\xff\x00&\xc0\xbdJ\x9c\x1c\xd4W3\xaa\xc6T\xe5G@q\xc5hd\xdb3\xa7\x94\xaamLc\xa1\x15\x93p\x16@\xee\xbfxp\xc2\xb4\xa5\x89fV\xfd\xdbc\xbb\xa9\xac\x89'1\xccb8#\xb1=H\xfa\xd1a\xdc\x86I\xc0\x84G'C\xc8l\xf4\xacK\xcc\xc7)\xdd\xc8>\x95\xb1;#)@\x80\x1a\xc7\xbd\x87t]FE42;\x04+p]%@\xa7\xef\x06\x1cWI\x17\x87\xa1\xd6\x90\xb4S\x89$U\xe25\x05A\xaeZ\x07\xf2J\x8e\x87\xa5z\x17\x82\xafc\x17*\x86M\xc4\xf0T\x0c\x00=\xcfz\xe8\xa6\xceZ\xbb\x1eu\xa8\xdb_\xd8\xdd\xb7\x9d\x11\x84D\x0e\xd6+\xf2\xa9\xf5\x1e\xa7\x1f\xe7\x8a\xe5\xa5\xbe\xb9\x92\xe0\xccY\x86\x14\xa2\xe7\xb0\xaf\xa7u\xdd*\xd2\xfe\xd8\xac\x88\x8e\x8e6\xe0\xaf\xf2\xaf8\xd4<%d\x96\xed\x17\x90\xaa\x8cO#\xf8}?:\xe9s\xb1\xce\xa9\xdc\xf3]/P\x9e\xce\t\x80f>[\xa4\xa9\xcf|\x81\xfdk\xb9\x87\xc4\x0bz\xb6\xd1\xa3\x00\xf2\x03\xd3\xb6G\xdd\xfc2k\x90\xd7<?q\xa7N\xefn\x1aKp9`+6\xc2\xeeHg\x8aE89 \x0fF<U\xc6JJ\xe8\xc9\xa7\x17f}\x19\xa3\xba\xb5\xb3\x08\x98\x17`\xb98\xe81Y\xbe!ey\xe1\x89U[i\x0e\x19\xb9\x0b\x8e\xe7\xf1?\xa5b\xf8C]\x8d\xe4k\x05$\xbc\x84(\xc1\xe8\x05t\x17\xf6\xf0\xdc]\xc1\t\x19*\xfd\xbe\xbf\xe3\x8a$8\x9cu\xfe\x9c\xf7\xa8\xe5\x99\xde\xdd\x18\x92\xbc\x80H\xeaO\xa9\xa7\xe8\xf7w\x03T\x8e\x04\x88Ak\x02\x99Y\x8e\x7f3\xeas]&\xb9,vv\xefi\x02\xaf\x99\x902G\xca\xbd\xf2}}qXV\x125\xc4\xef*\x86*\x83\x1c\x9e\xb8\xe0g\xf3\xcdg\xcffk\xcbtvO8\xbb\xbb\x8e\x15\xcf\x96\x8f\x96\xcf|\x0c\x7f\x8dd]\xcf\xe7\xeb\x92\xc7\x92R \xc5\x8e;\xf6\x02\x9d\xa5\x99Rswt\xd8.v\xc6\x0frO\\}*\xbd\xbbGs\xab[D\xa7\x0bp\xcc\xfc\x9eJ\x8c\x8c\xfe\x7f\xce\xaf}H\xdbC\x9a\xf1n\xaf>\x9c\xabl\xa4+I\x9e;c\x1c\x9f\xd7\x03\xf1\xae$_\\\xdfIhY\xd8; Rq\x81\xb8\xc8q\xfab\xbdb_\x0f\xd9k\x17\xc6\xfa\xf26\x92 \xfb\x151\xf7\xb9\xe9L>\x1d\xb0\x92\xea;[;`P\xb9\xe3\x1c\x008\xcf\xfd\xf5\xfc\xa9JV\x05\x0b\x9eWo\xa8]\xe4\xc1\xbeb\xdc\x87^\x84\x90{w\xdc2~\xb5\xd0\xe9\xda\xbc\xbaB\xab\\\xdc4\xb6\xe5w0\x00\x12\x80\xf4\xc8\xea\x0f\xe9\xe9]\xf3\xf8CF\x92\xe6If\xb4q0;d\xcb\xf7\xfe\xf1\xf45\x81\xabx$I|$\x81\xa6\xba\x8abp\xef\x80\x13\xf1<\xfa\x83J\xf7\x0e[lc_\xf8\xbfK\x8c\xf9\x90Dn\x8fUa\xf2\x9c\x1eO\x1c\xff\x00\x9e\xf4\xd8>$\xacNBZ\x90\x8c\xdc\xb0?0_NsZw>\x04\xb1\xb1\x87\xcex$P\xa7\r\x1cR\x06 \x7fx\x9a\xe6/\x97K\xd2\x9c\x98\xb4\xcb\x99\xe4\xfe\t%\x90\x05\xfa\xe0\x00MR\xb1-\xb3\xac\xb4\xd7\xec\xb5[qn\xf6\x92K\xbc\xe4\xb3\x8c\x0f\xfe\xb9\xaa3\xad\xdd\x94\xb2\x82c\x86\xd9FCI&\xe6\x1e\xde\x99\xf6\xaeV\xd7\xfbWT\xd4\x16\xe0\xb3\xa9\xcf\x01@UU\xf6\xae\x96\xf8%\x9cK\x10&I\x9b\xb0#\xe5\xfa\x02\x0f\xe7E\x85\xd0\xccV\x82\xebu\xcbHK\x12\xb1\x87e\xc0\xdd\x83\x8c\x83\xdb\xb5k\xd9\xc7m\x0c\x8bl\xc3\xcc\x97n\x1c/\x18\xe3<\x13\xd3#\xb5&\x9d\xa6\xc6\xe9,\xb71\xb9\x8ae\\\xabp\x0b\x0e\x7f\xcf\xd2\xae<\"\x0b;\x8b\xa3\tU\x03\xcabpK\x05\xcf\x0c>\x9d\xe8li\x1d;k\xf0\xf5$U\xcd.\xfa-JR\x8a\xf8\xc7\xadr\xfa\x15\x84z\x8d\xe6\xc7\xce\xda\xda\x97\xc3WZd\xcf-\x96[wA\x9a\xcas\xd2\xc6XUR~\xfc\xb6;_:\x0bx\x80\xca\xf1Y\x17z\x84\x92\xcd\xe5\xc4K\x13\xd8UM#E\xd4&\xfd\xe6\xa51\x03\xb2\x03[\"K-?\xee \xdc+\x95\xc5\xb7\xa9\xe9\xa9$\xb4\x12\xce\xd6p<\xcb\x83\x8fAD\xf2('\x9e*\xac\xfa\xda\xccJ\x82\x06;\n\xa3-\xea\xfa\xd6nKdh\xa2\xf7e\xa9.\x8a\x9d\xaa:\xd4e\x1aN_\xa5f\xcb\xa9$}94G|\xf3\x11\xd8f\x95\xaf\xb9[\x1a\x90[+8\xc2\x83\xcdt0\xa2\xa4@w\xac\xdb(\xf6D\x18\xf7\xa9\xde\xe0\x81\x80p*c\xdc\x1e\xa4\xf77\t\x0cD\xd7?>\xa9\x92q\xc2\xd4\x9a\xc4\x8f\xf6\"C`w\xaeFk\x93\xb3b\xb6\xe6\xe9JM\xde\xc3\x8cU\x8d\xf4\xbe\xf3\x1c\x82r{b\xad\xc7\"\xc67\xc9\xd3\xb0\xac\x1bL[D\x1e@L\x87\xb5:Cqt\xf8\xce\x07\xd6\x8e\x83\xb1\xa3}\xaem\xccq\xb0\xc7\xb5d,S\xdf\xc9\xb8\x83\x8fZ\xbb\x0e\x9d\x12\x1d\xd2|\xc7\xd6\xb4\xa1TQ\x84QR\xe4\xcaVE[}1UF\xe6\xc5h\xc1m\x1a\x0c\xe3'\xde\x9a[n\x00\x1di\xdev\xdc\x01\xd4\xd4\xdc\x1a\xb9mYTt\x02\xa2\x92\xe3\x8c\x0e\x07\xad@\xf2\xae\xdc\x13\x8a\xa7#;\x12\xd9\xe3\xb0\xa1\xcbA(\x96&\xbbU\xc8\x04\xe6\xa9\x1b\x96\r\xbb&\xa3-\x9f\xbd\xcd@\xc43c\x9aKR\xeddj\xdb\\\x99\x97\xe6\x1f\x8d,\x88\xb9\xce3T\xadf0\xb6\xd3\x132\xfa\x83W\xa4%\x93r\xe6\xba\x12\xba0z25\xde:\x13R\t\x0b.\xd6\xc1\xaa\xbe{Drjh\xae\x04\xeaCFO=ir\xdcwhc\xc7\xb4\xee^\x95^U\x0c\x0e;\xd6\x8b\xc4\xa5x$\x0fJ\xaa\xe8\x14\xe0\x9ek\x9et\xeco\t\x98S\xc2\xcb'\xadU\xfb:\xb3\x90\x14\xeeoJ\xd8\x92=\xf3\x11\xd0\xf6\x15j+U\x1f2\x0c\xb7\xb5s\xbd\x0e\xa8\xcfC\x9am%\xe6\xc8U \x91\x82I\xaaCM\xbb\x82\\\x83\x9c\xfa\xf6\xae\xe0\xda(\x19nI\xaa\xb3E\x1cd\x8cf\x97;E&\x99\xc9\x01s\x1b\xe0\xbe\x00\x18\xc3\x1c\n\xb5\x0e\xa0\xb0\xb2\x89P\xa3\x0e\x87\xb1\xad\xaf\xb0\tHl\x0c\x1e\x99\xa8\xeet\xd8!%]\x0b)\x1d\xfb\xd5\xa9\x11+l\x88\x83\xd9\\\xf5\xc7<\x92(\x16\xf1\x896\x86!G|\xe4~\xb5Q\xf4h\xd0\xee\xb2\xdc\x1f\x04\x94$\xd41\xcb$'d\xe8\xdb\xb1\xcf$\xd6\xf1k\xa1\x8c\x91\xa9,%H1\xb0`:\x91\xd6\xac\tV|\x1c\x00\x07\x07/\x8f\xe7\xc5V\xb7\xb8B7\x05V\\`\x80p\x7f#L\x9apTn\x8c\xe3<\x0c\x0c\xd6\xd1m\x98H\xb3$\x8bn\x8d\xb0\xc7\x92x\x00\xff\x00J\xe6\xefe2\xc8r\xbc\x03\xd4u\x15\xb4\xef\x1bDw*\xc8\x08\xec\xb8\"\xb0\xee#\"]\xc8\xa4\xaf\xb8\xe6\xb43+F$|\x81\x93\x8f\xce\xaa\xca\x92n}\x87q\x03\xa1\x15~\x07D\x9bsv\xedR\\\"\xc9\x97\xc0\xc7\xae?\x9d\"\x99\xcc]3.\xd7l\x81\xf4\xae\xcb\xc1\xd3\xef\xba\x84y\xdb@=\x0bm\xcf\xf8\xd7#\xaa\xee\x89N\x01#<V\xa7\x82\xe4\x8c\xdf\xc6\xd2\x00\xb2\x06\xca\xbbs\x8f\xa0\xae\x9akC\x9a\xa1\xec7\xcfu,l\"\x95#\x18\xc0\xde2\x00\xaengp\x12\x19\x1dgf\xc9R\x83\x81\xc6+\xac\xfb*Oa\xb2G\x95\x89^\\\r\xb5\xc7\xdc\xa2h\xa6UF\xf3\x1aL\x9d\xa7\xae=\xcfaW-\x8c\xa1\xb9\x97qj\xb1G1\x90n\xdc\xbb}k\x87\xbc\xf0\xc0%\xe4\x83\x80\xc7p\x18\xae\xcdo\x1e\xe5\xd6#\x08\x1bX\x93\x8fLTMt\x91H\x11\xd4\x0f\x96\xb3\x84\x9az\x1aN*KS'\xc2\x16\xf3i:\xc0\xb9\x98d\xaa1\x07\xf5\xae\xc6\xd6\xf4\xc6>\xd0p[\xa7&\xb9T\xba\\>\xcc\xe0\xe4\x1f\xa5\\\xb2\xbe\x17S\"\xb1\xdb\x1a\xb0g\xfc\xeb\xad;\xa3\x95\xab3\xaa\xd4\xad\x92X\"\x92\xe0\xe5\xdc\xf7\xe0\x0c\xfbV5\xcc\xd0Z2\xd8\xda\xa1\n\x14\x16?\xc4\xecO\x19\xad\xabFmBW\x92X\xcbna\x1c\nOOR\x7f\x0f\xe7XZ\xd5\xa0\x82\xe2\xe5\x91\xc3\xb6\xed\xcc}y\xe9\xf4\xac\xa5\x1dM#\"\xcd\xfd\xfcwz\xed\xa4\x16\xdf0\x85X\xbb\xff\x00\x08\x00c?\xce\xaa\xe8\x92\t<Au\xa8\xef\xd9m\x14-\x14 \xff\x00\xbd\x8c\x8f\xca\xa9Y\x19BH\xa0\x03#\xc5\xb9\xcfr\xc4\x9a\xafi\xbe\xc28\xad7\xf9\x93H@\x91\xb1\xf7FI\xc5h\x88g\xa6\xa4\xf8\xb4\xb2\xdb\x19\xd8Ny\x1f\x90\xac\xd2\xf2\xdcj\xb7W\xb6\xce\xa9\x0cr,aGdB\xbf\xcf,j]\x16W\xba\xd2\xe0\x96\xf1\xff\x00\xd2%\x0cbN\x98Q\xd3\x03\xb0\xe9Y\xe3\xc3\x93Gpn\xed\xf5\t\"v\xce\xec\x0c\xa9'\xa7\x1fZ\xceOR\xe3\xa255+\xd1o\x1c\xd7Q\x1f1\xb3\xcc\xb8\x03\x04u\xf75\xcc\xea>)\xbc\x9e\xdc\xc2\x0bo\x04#\x9589?\x87L{\xd6\xd6L\xac\xf6wM$\x9223+B\xbf,\x80\xf2\x08>\xa3\x9e+\x9d\xb8\xd3|Er\x8cm\xe5\n\xb0\xa01I\n\xecv\x1e\x8c(OPkB\x99\xd2e\xbf\xbd7\xb7j\xd1\x02\x7f\x86v!\x86y\xf9s\xc15bxl\x842G\x06\x9a$bs\xf3\xe1\xdf'\xeax\xfck\x0fT\xd1\xf5\xcb\xac\x99c;\xf1\x9d\xc9&\xee?\x0c\xf1W\xac\xbc;\xa8\xdbD\x93K\xaaM\x8e\xa6#\x11\x07\xf0\xad\x0c\xcc\xb7mV\xe3V\x03\xcd0\xc2\x1b>Lq\x8e\xde\xb5\x9f5\x95\xdc\xba\xb7\x9d!\x92R\xae\xa7\x95\xdb\x81\x9fJ\xeb\xb5\x11\x1e\x99j\"\x8e\x19\x16f\xfb\xd2\xbbe\x8f\xe2}j\x85\x94\xe8&\x1c\x10.#,3\xcf\"\x81\x16\xa4\xbcp\x91#FV\xd9\x9bn\xd7\x19\xf2\xd8\xe79\xf5\x15V\xfepm<\xa4\xf3c\xba\x8b\x02xs\x9f0v#\xd7\x8a-/]\xdd\xed\xef\x14\xb3\xc6\xdb\xa3\x919\xf5\xfd:\xfet\xeb\xc5\xb8{\xb8n\x02(h\xfe\xe9\xc6w \x1d?\x98\xfc\xa8)\x1d\x17\x86\"\xfb\x16\xae#\x97\xabt\xafP\x11\xc4\"\x0eq\x9a\xf3\xadI\x167\x8e\xea?\x96H\xc8\xc6+gN\xd5\xf5\x0b\xc8\x7fz\x15\x10\x7f\x16z\xd4I!R\\\xa9$k^\xdd\x0c\x90\xbd{VL\xb6\x12M\xf3\xb9\xda\xb4\xe7\xd4m\xe1\x97\xe6;\x9e\xa8\xeaZ\xf4a\x08\xde\x01\xc7\n+\x9d+\x9dW\xb0M\x1d\xad\xbcD'-\xdc\xd6\r\xc5\xce\xe9H_\xd2\x98\xb7\x13\xde\xcaNJ\xc7R$d\xb1\x08\xbfRj\x1d;\xbd\rT\xec\xb5 H\xd9\xa4\xcb\xe7\x15\xa9a\x0b\xbc\xa3\x0b\x84\x1e\xb5\x1aD\xb1\x90\\\x8c\xfaV\xa5\xa4\xa8\xe3\x081\x8a'M\xc2\x9b\x93&5\x94\xe6\xa2\x8d\x88\xa5\xdb\x06\t\xe4T^`f\xc1\xe9Ma\x95\x00\x1cTO\xf2\x15\xe7\xa5a\x1d\x8d\x99\x16\xb2|\xcb2\xa2\xb9\x9d>\xd4\x19y\x1c\xe6\xb7\xaf\xae\x94\xc4\xc3\xd6\xb0a\xbcXd/\xe9CWe-\x8b7EV\xe0F9\"\xa7\x8d\x82\xf1\xde\xb1\xfe\xd2g\xb9.\xbc\xb1\xe8+\x7fO\xb2\x12\xa7\x99&wg\x18\x1d\xaaeh\x8d]\x8a\t#5:J\xb1\xa1\xcf\x0c\x06p{\xd5\xff\x00\xb2 \xb6#\x1c)\xee2MU\x9cyq\xe4(b8\xdd\x8c\x92k'9\x1a(\xc4\x89o\x01_\x98`\x9e\x80\xf54*\x82C\xb0rs\xfc5\x90\x1eE\xb9f1\x96<\xf0\x01\x15\xa1k$\x88\x11X\x1d\xd9\xc9\xc3c\x15\x9an\xfa\x9a8\xabhYh\xd7x\x0f\xb8g\x903\xc8\xa4h\xe2\xda\x0b\x16\xc6q\xc1\xa7\xf9\xaa\xec\x15\x15r\xe3$\x9fZ\x8bv\x11\xbe\xfe\x14\xe5\x80\x19\xfc\xa9\xdc\x8b\x13\xa4\x16\xcc\xbd[\x151\xd2m\x08\xc9v\xe7\xa7<\x9a\xa0\xd78p\xa4g\xfb\xbb\xb8\xa9\xbc\xe7\xc0\xf9\x9dOq\xdb\xf0\xa6\xa5aJ&\x8c\x16v\xe8\x80\x078\xf4j\x98\xd9\xdb:\x91\x86\xfc\re}\xa9\x94\x8c\xeea\xfd\xe0\x7f\xc6\x9c5\x14\x1f)b\x0f\xb1\xabU\x19\x0e\x97R\xebh\xf6O\xc9\x12\x1f\xa1\xa14\xcb8\xf8\x1b\xf0z\x02\xc6\xab\x8b\xf6|*\xe0\xe7\xa5H/\x1b\x85U\xcb\x0e\xf9\xa7\xed<\xc3\x91\x97\x05\x8d\xb8\xfe\x0f\xcc\xd3E\xb4\x01\xbf\xd5\x8f\xa9\xa8\x85\xda\x95y2\xc4\x8e\x9b\xbb\x9aE\x9ft\xbf19\xc7#<\x02j\x1b\xb8$\xd1$\x90!\xe0\x05o|UyJG\xc2\xa8\x07\x1c\x9at\xb3m\xe0u~\x05SbP\x15bObk6\x8d#r)\xe6\xca\xe4\xe1\x86p9\xac\x89\xdc\xef g9\xe9Z\x13\xc6J\x16B\x02\xe7\x01\xcfsY\xedm;M\xe5\xa4lOf=\x0f\xa9\xa9\xe5:\"\xd2.YJ\xcc\xbbr\t\x00\xe2\xaco\x0e\xa4\x16$\x81\xd7\x15\x17\xd9\xcch\xe4.2\xa0\x0e\xd4\xd8\xe5\x96%\n\xe8\xab\xfe\xd0\x1c\xd1\xcaKi\x91\xb5\xbc\xab\xf3\xed\x04uc\xbb\x04\xd5[\x9b$\xbb\xc1X\x8a\x01\xd5\x83w\xf7\xad\x94\t<D:\xab\xe3\xa6j\xb4\x96\x88\xb2c\x8c\x1e\xdd\xabH\xae\xa6R\x91\xcf\x14\x92\xceM\xc5\xdaHz\x03\x9c\x8a\xbb\xe7\xdbH\x866\x84\x82y\xdc\x1b\x8f\xce\xad\xdd\xc7\x0e\xcf+\xcb8?\xdd5\x94#kiv\x88\xf7)\xe9\xce\x08\xae\x88\xbb#9+\x8f\x9e\x04Db\x91\xb3\x9e\xc3\xbf\xe7X\xc2N\x19Y\\\x12z8\xe4{V\xea\xde\"\xc2NN\xec\xf5#\xa7\xd6\xb3o\xa32\x1f1\x80e\xfe\xf2pkt\xee\x8c\x1e\xe6`Ef.\x8aC!\xe4w\xab\xea\xc6H\x9b\xe5\x07\x03\x9a`\x8c2n\r\x86\xc7\x04\x8ei\x10\xb2\x86\x00t\xebCCL\xe7\xb5X\x82\x96d\x90c\xb8\"\xa9xz\xe4\xc1\xaa\xaf$s\xc6\x1b\x18\xad]\\$\xa0\xf2\x15\xb1\xd4\x8e\xb5\xccF\xa2+\x80H\xc9\x07\xa05\xbd&s\xd6\xf2>\x87\xd3n\xee.\xb4\xc0\xd2m\x00\xaf\x18==\xc9\xack\x9f\x0e\xa4\xbb\xde{\xb6\x90n-\x8c\xf0I\xe8>\x957\x86g\x17\xbae\x82\xb3\x04\xda\x99e\xed\xf4\xab\xfa\xec1=\xbbE\xbbh\xe3\x95\xf5>\xbe\xf5L\xcd\x1c5\xde\xa7\xa7i\xf7,\x81\xc1\x93\x05\xb2(\xf3-/\xed|\xd2T\x12\x01\xe2\xb2uO\n\xc8\xf7.\xc6R\x15\xb9P9\xc0=+\x9f\xbb\xd3\xb5]-\x18\xc2\xcc\xe8:\x15=\x05\x11Qls\x94\x924\xf5X~\xcc\xac\xf1?^@\x1e\xb4\xcd:\xe5\x94onNA5\x97-\xdc\xb3F\x82V*\xcd\x83\xf3V\x95\x89E\x00\x0eOZ\xe8z\x18nvV\xfa\xbb\xc1e\x16\xd3\x82z\x03\xdc\x9eI>\xc0v\xaa\xd3_\x89c\xbaPw\x18\xd8\xf3\xea\xd8\xe3\xf5\xaap\xe1\x8a|\xdb\x99\x87\x00\xf4\xa9\xac\xa1\x8eMDD\xa4\x15\xff\x00Y)\xc7^p)7q\xa5b\xce\x9b\x17\xd9\xfc\xc9_\xf8b<\x1e\xf8\xc7\xff\x00^\xa3\xb7\x84F\x05\xd4\x8b\xe63.p\xdd\xc95\xa5%\xbby\x0e\xe0p\x00\\{\x9e\x94\xcb\xd5&1\x14*Y\xca\x80\xa0\x0e\xbd\xaao\xdc\xab\x1a\xb6\xd9\x95#\xb9!_j\x80{\x00\x0f@\xa3\xb0\xad{[\xf9Z\xd4\x0bx\xe2d3\x0c\x80\xd9 g\x19\xfc\xb2jM6\xc6\xdbO\xd3~\xcfp\xa1\xb27;\x0eKc\x9c~C\x150\xd2\xf4\xe7>d8\x8eV\x8c\x00\xeb\xc6\x07\xa0\xac\xde\xe5\"!<?\xbe\x8eI\xe3\xf9\x82\xb6\xc5\x03*z\x11\xf8\x11YwVrE\x0c$L\xd2\x10\xe5\x93\xaa1\xcf\xa9\x1d\xf3\x9f\xadS\xf1\x07\x86d\x92As\x1c\x8e\x92\xc6\x0b\x00x\x0c\xc5q\x83\xeck\x96\x97\xc4WV\xd6\xeb\x0c\xcf#F\xa3\x81 \xfb\xe0\x1c\x15'\xb1\x07\xa1\x15q\x8d\xc9\x93:\xd9\x16\x18\x7f\xd2#\x8e\x18\xd4\x8f\x9d\xa1\x19;\x8f\x1c\x8a@\xab$\x91\xc6\x92\xaf\x1c\x96l\x8c\x9e\xf5\xc6I\xe3h\xed\x90\x18\xe4T\x8d\x7f\xe5\x90R\xc7\x9e\xbc\x9a\x92\xd3\xc5\x9f\xda\xd7%m\xa3xWo\xef$\\\x03\xf9\x7f\xf5\xea\xac$\xcd\x1d~\xea\xcf\xed(\xb0\x80d\x1f($n\x07\xf3\xac\x00\xe9n\x96\xed\xff\x00-\x12\xe0\xe3\x1d0xe\xfc\x8ej+\xedId\xd5\x91a\xe1\x16@B\xe3\x04\xfa\xfdj\xd5\xdcP\xcc\xb1\xb2\x9c;I\xe6 \xf58\xff\x00\xebSZ\x12\xf5e\x85\xddc\xe6]2f6s\xf5$\x9eW\xdb\x9f\xe7Q\xdc]\xc47\\\xab\xbb[\xbe\x02\x809S\xef\xf8Uk{\xa6\xba\xd2\x9e9\x9f-$\x9b\x94\x1e\xcd\x9c\x0f\xcf\x8am\xbaJ\x9a[\x89I9\n\xbd?\x8f\x92O\xea(\x0b\x9dM\xed\xfc\x92FWh\x1e\x95\x9fey~\xcc\xcb<\xae\xa8\x0f\x00\x1e\xa2\xbbF\xd1a?\xc1\xfaR\x7fb\xc1\xfd\xc1\xf9Rj\xe0\x9d\x8e>\xf6\xfa\xe9\xa3+\x02\x9c\xfa\x9a\x83O\xb5c/\x9ft\xc5\x9b\xd0\xf6\xae\xe7\xfb\x16\x1f\xee\x0f\xca\x93\xfb\x1e\x11\xfc#\xf2\xa9\xe4Es\xb3\x91{\xa7I\x0f\x96\x9f-4j3\xa7D5\xd7\xff\x00c@\x7f\x80~T\x7fcA\xfd\xc1ME!96q\x92_\xdcH\xd9\xdak\xa3\xd0%g\xb4\xdd \xc3f\xb4?\xb1\xa0\xfe\xe0\xa0Z%\xb4\x8a\xaa0+\x1cW\xf0\xcd\xb0\xdf\x19i\xdd\xbb\x1a\xa7<\x8cz\x9a\xb8\x01-\xd2\x994\x04\x83\xc5q+\xd8\xec\xd8\xe65K\x93\x1e\x17<\x9a\xc6!\x98`}\xe65\xa5\xac\xa1F\xdcGJ\x9f\xc1\xbaKj\xf7\xcfq(\xc5\xad\xbf\\\xff\x00\x13zS*\xe8\xd2\xd0\xfc>Q\x04\x92\xf2\xcd\x8e}\x05u\x1fdH\xd3dc\x1c\xe4}*\xed\xb4(\xa8:`\x9e\x9e\xd4\xc9\x99\x83`\x01\xd7\x83Y8\xdfV\x1c\xfd\x88\x8cD\r\xcb\xc8\"\xa9\xdc\xaeF\xd0\x00a\xc88\xcd].\x11\x1b\xd4v\xefP\xef\x07'\x03\xa5\x1c\xaa\xd6\x05's\x0e{Y\x1e]\xdb\x8e\xd3\xc1\xe3\x15N@\xb6\xeeP\xb1${\xd6\xfc\xeb\x98\xff\x00\xd9=qY\xf3Z\xc6\xd2y\x989\x1cg=k)A\xa3u35'r\xa5\x94\x81\xc79\xa9\x04\xa8\xe0/\x98A\xeb\x8c\xf5\xa7Kj7\x10\xa3\xe5\xe9\x93PyN\xa9\x81\xc7<\x91\xe9Qf\x86\x9ae\x81\xb2#\xbb\x92O]\xdc\xd4\x8e\xd2\x85\xdc~\xf7\xfc\xb3\x19\xefU#bp\xad  \x9e\t\xe9R\xb4\x82D\xc2\xb2\xf1\x90I?\xca\x98\xda\x06Wt\x05\xa4\xc0\xeaFz\xd4.\xcc\xcd\x9f/\x81\xd9x\xa5S+*\x9d\xeavs\xc5,\x92\x03\xf2\x80@$\x92i65q#q\x10\x19%\x86~\xea\xf5\xfci\xd1\\I\x92\xd1\x8c\x10zS\"\x8c\xb3\x02\x83b\x8c\xe5q\x9f\xcc\xd4\xf6\xf1\x90\xad\x80A\x0b\xd8w\xa46\xc9E\xcc\xdeX-\x8c\x9e\x14\x9e\xb4%\xdb\xb7\xca\xc4\x87\x04\xf4\x15/\xd9\xd8<R\x00\x18\xa8\x1fZ\x8cY\xc8\xccdbK\x168\xf6\xaa\xd4\x9b\xa2\xf4~X\x01\x9d\xf9\x1c\x01\xe9PI\x90\xaeW\xe6,I9\xf4=\xeaUR\xbbw(\xc7ojI!\xf3\x18c\xe5f<\xe3\xa1\xa6H\x9bD\xb0\xedq\xf2\"\xe0m\xe8\xc6\x9d\x13(\\0+\xc7\x00\xd0\\\"\x88\xf0\xa0c\x1cz\xd3\x18\x90\xbb\xc1\xf9\xc7Q\xd8\xd0\x95\xc9l\x91\x96F]\xa4\x96\x07\x9ct\xc5B\xc3*\xdb\x93v9\xe3\x83N\x8af\xde\x04\x9f)#\x8ct\xcd8J0rJ5i\xcbq^\xc4!\xd5q\x80vu\xe9\xcd,\xd8\x91w(Q\xeesA\x95\xd1\x8e\xe3\x9c\xf3\x9cS'\x95G#\x8cu\x1d\xa9\xa8\xd8W\xb9Vb\xad\x84\x99Cz\x159\x02\xa9\xcc\xa1\x90F\x8e\x1b\xd8\xd5\x99e\x11\xb1\xcf\xf2\xc8\xa8\xbc\x94v,\x180>\xd5@TQ$@\xa1\x1c\x1e\xf8\xe2\xab\xcd\x11\x01\xc1\x03\xdbm^\x98\x9f$28,8dn\rW\x96e\x0c\xaa\x07^\xc7\xb5k\x1e\xc6rFz\xc0Y\x03\x06\x1b\xc7A\xeb\xecj+\x9f\x94\x82[i=s\xd2\xadI*\x896ci>\xb5R\xed\xb7A\x8e\xa4q\xf5\xad\x0c\xcc\rh\xec]\xd8\xc6\x7f*\xe5\xc9\xcb\x93\xdf5\xd9\xde\xa2\xbd\x99\xf3\x17\xe5\xf4\xf4\xae6U\x11\xca\xc1NV\xb6\xa4cT\xf5?\x877\xb0\xc5a1\x9aUy\xc1\xdc\x14\x1c\x90:W]\xe4\xcf|\xf2]\\\x92#S\xfb\xb4\x07\x8e\x9dk\x83\xf8O\x04l\xd7\xb7.8\x04 c\xd3\xd7\x15\xe8\xb7\xb7Q\x97H\x03\xf2\xc0\xe7\x03\xa2\xd6\x96M\x99^\xc7+\xa8\xdf\xd9\xc2\xd2*\x90\xcd\xb8\x13\xeb\\\xfe\xa3q\xbe\x161\xb6\xd2T\xed\xcfO\xa5tw\xda<b\xe2G\xd9\xfc9'\x15\xcd\xeaQl\x88\x85\x03i\x18\xdd\x8e\x9e\xf5\x11^\xf1\xa4\x9f\xbaq\x1a\xb5\xdb\xcd2\x8d\xa1X\x1c2\xf6\x07\xd6\xacZ^y@\x0c\xe4\x91\x8a\xa7\x7fn\xd7\x17\xbc\xb2\xabt$\x1a\xb5\x0e\x9e\xb0\xa0\xf9\xb3\xfd\xe6=\xeb\xa2M#\x9a)\xb3Z\xdfQ\xc3\xee\x07\xb6\x06=kKK\xbc\xf2.U]\xb2ep\xcf\xec\x05eY\xd9y\x8a]W\x08\xa3\xa9\xf5\xab1\xc1\xe4~\xfaN\x07oz\x9b\xa2\xac\xce\x9a\xe7_D\x89\xa2\x1dO?@\x0e\x07\xe9\x9a\x92\xcbPi\xafeH\xbeWP\x15O~\xbf\xe1\\K\\4\xf7*\x15q\x96\xc8\x1e\x80WM\xe1\xd8\xbc\xfdZ=\xccU\\\x95\xc8\xef@\xd1\xe9\xb2\xc0Z\xd1\r\xbb\x9f0\x003\x9e\t\xf7\xfdk:\xf0_Ec\xbe\xd8\x06\x91\x008#\xa8\x1c\x11\xf5\xf4\xad\x04\x95lL6\xf26\x15\x89Q\x81OS\xb6\xe4\x08_|L6\xb6\xe1\xc1\xff\x00&\xa2\xda\x95s\x13M\xd7d\xd4\xed^\xd6\xe1\xf6\xdd(!\x88\x00\xab`\xe0\xfec\xb7\xbdr^!\xf0\xcb\xde\\H\xf6\xf3F\xa8\xdc\xecc\xc6\ru\xb2\xe8\x96P\xddn\x86W\xb7l\x97 6\x01'\xadgKy\x12\xee\xdc\xad$\xd1\x9f\x9d\x02\xe0\xb6\x0e2;\x1a\xab\xdbbm}\xcf=\x93\xc1\x12\x19\x1bm\xc79\xc6\x0f \x8f\xadi\xc5\xa7\xae\x87k\x8bh\xc8c\xf2\xb4\xac\xa0\xd7O\x1b\xfd\xaa\xe1\x96;c\x14{~BF9\xf4\xa5\x9fFI\xedV\xd43\x19\x14\xe5\x8dRm\x93$\x91\xc0\xc1f\xf7\x97\xb73H\xec^6\xca\xbe1\xe9Z\x97vW\x97\x11@\xa8>dL\xa3\x0e\xcc\x0f\xf5\xc5v:\x7f\x86\x04%\xc6\xc2r\xc0\x8a\xdc\xb5\xd0\xd20>Z\xd1D\x8b\x9e{\xa7xrW\x99K\xc6@p\x0b\x01\xd0c\x9f\xe7]$\x1a1\x92\xdd#\xf2\xb1\x92X\x9frk\xb1\x83L\nx^\xd8\xab\xf1i\xea\xbf\xc2)\xf2\x8a\xe6ipi\xbe`\xa6l4\x85\rfQ/\x9a)\xa5\xf3L\tJ\x13\xde\x80\x14>)|\xcaO.\x9f\xe5\x9cP\x04{\xf9\xa6\xb2o!\xb1S\x08\xb9\xa9B\x00\xa0W>!^\x167\xc3\xbbH\x8a\x18pA\xa5\x9e<\x0e\x05N\xa7\r\x8cR\\\xab\x18\xce\xdeH\x15\xce\x95\x91\xd0\xe4\xdb8\xbdZ\xdd\xee%\x11 \xf9\x99\xb6\x8f\xc6\xbbm'M\x8bL\xd2\xa1\xb3\x8b\n\x00\xcb6>\xf1\xeek\x12\xc2\x03&\xb1\xb9\xd7\"!\x90=\xeb\xa9\xc8\x08\x06\xdeG'\xe9QmK\xb8H\xcb\x1a\x15\xcf#\x8a\xa3,\xa061\x8e\xf9\xa8\xee%\x02S\x93\x83\x9a\xac\xcc\xc0\xe79Z\xcaR[\x17\x18\x8a\xcd\x86\xc3\x12\xc3\xd6\xa2\x9a\xf5a\xcb;m\xe3\x8e)\xb3I\xb5H\xc8\xc63\x9fZ\xe5\xf5\x8b\xa9\x0cGsq\xdb\x06\x84\xda-+\x9a\x93\xeb\x88s\xb1\xc1>\xb9\xaa\x8b\xaa19<\xf3\xc8\xaeb\xdd\xdey\x0f\x04\x9e\xe7\xbdmCl\xb1\xa6\xf9\x1f\nFy\xa7{\x8d\xa4\x8d1\x7f\x1b\xa7,\x06\x0ex\xa7\xb4\xd0\xb2\xed,0I\xc6\rqz\xd5\xd9\xb5\r%\xbc\x84\xa9\xea\x07cUm5\x1b\xc9\xa2B\xd9^3\xcdR\xa7ur\x1c\xac\xecw\x05\"f\xc8a\xb4\x92@\xc7\xe9Q\xbc``\x001\xdf\x15\xca>\xa75\xb2\x83#\x0e\x99\x15\x1f\xfc%k\xb9\x14\xb8l\x0c\x1ei{\x16\xfa\x0b\xda[\xa9\xd7C\x18Ec\xbb\xaf\x1c\x1a\x910\xc9\xb0\x1c\xe7\xadrK\xe2\x8c\xe4\x04\x91\x81\x1d\x94\xd5\xd8\xfc@\x8c\xaa\x04\x13\x8c\xf0HCC\xc3\xbe\xc1\xed\xd7s\xa5,\xcb\xf2\x02W\xf8\xb3SEr\xc0a\xfa\x1c|\xc3\xd4W65\xa0\xccXC9\xc0\xe0l4\xe85\xa7\xde\xc1\xac\xee6\xe4\x10J\x1a>\xad.\xc1\xed\xe3\xdc\xebR\xe8\xe7\xa7\xd3\xde\xa4\x17\n\xca\x15\xba\xe3\xadr\xc3\\\x91\\n\xb7\xb8\x03o\x07\xcb<\x9a\x90\xeb\xd1\x84.b\x9dH\x19\xc1\x8c\x8eh\xf62]\x03\xda\xa7\xd4\xe9\xc6\xd7<7\x18\xa4\x03\x9e\xbcu\xfa\xd72<K\x01@\xa4\x9599\xc8\"\x9b\x1f\x89 \xf3U\x0c\xc0'\xb9\xa5\xec\x98\xf9\xce\x91\xf3\xe6t\x1e\xa2\xa3\x91\x19\x89\x05\xb2\x0f?J\xcc\x87\\\xb7\x96Q\xb6E8\xf7\xa9\x8e\xad\to\x91\xd4\xa9=\x8fJ=\x9bA\xcd\xd8\xb8x\xcey\xe7\x83MvVL\x11\xcf\xadg\xbe\xa7\x19,\t\xefM\x92\xf7\x8d\xad\xf8{\xd3*\xc5\xa7\x94\x85e9\xdb\x9a\x82W\xca\x1c60;\xd4&\xf4\x10\xc0\x9eqQ\x19\x83G\xb4\x9f|Q`$?\xbc\x8cn \x93\xc7\x14\xd2\xf9\x1b\x019\xc6\x0f\xa1\x15\x0f\x9e\x03\xe1;\xf6\xa5\x8c\x96]\xe0\x80}(C\x1e\n\xc8\xa5\x19y\xc7\x15J\xe5\t\xe5\xc1\xe3\x81W\xe3]\xc4\x92y\x03\xb7j\x85\xc1\x1c\x81\x9c\xf3\xf8\xd5\xc4\x86e\xb2\xa99l\x9ct\xcdV`Cml\xe0\x8c\xe6\xb4\xc4@\xeeW\xe0\x9e\x95X\xc3#\x0c*\xe4\xaf\x18\xf5\xaal\x8eS:\xfe6kR\xab\xc8\xc6Ep\xb7\xaaR\xe4\xf1\x82\xd5\xea\xf1\xe8s\xcd\x11!N\x08\xef\xc6\rr\xb7>\x14\xbe\xba\xd7-ahJ\xa3H2\xec8\x1fZ\xda\x8e\x9b\x98\xd6\xd5hw>\x07\xd1Z\xd7D\xb4U\xc2\x07_5\xcfv&\xb63\x08\xd5$P\xad\xfb\xb1\xf3\xb3u&\xad\xd9\xaa\xd8\xc3\xe5\xdd9\x1eZ\xe3\xcc\xc6\x06\x05Cq\x14\x12,\x97(\xd9\xde\x06T\x1e\xbd\xabX\xae\xe6\x0c\xc2\xd6\xaff\x89\x04\x90)pT\xfc\xbd\t\xcdq\x9a\xa6\xb13\x02\x8e\x9bp>PGz\xefu9#]\xa1\xd4\x07*@\x03\xb9\x035\xc7j\xd6\x90\x94s\"\x82q\x9c\x8a\xb5\xb8\x9d\xd28{K\x9f6\xfb\x88@%\xb9'\xb5m\xcd\x0f\xcc8\xc9\xf4\x15\x96\x90,2\x97=\x07Lw\xad\x19.Um\x82\xa7R9=\xea\xa4\xaeD\x1d\xb7-\xa6D*\x19\xfeQ\xd1\x16\x89\x03\xce\x9c\x96\xc7\xf2\x15J\xc8<\x87,H\xec+Sb\x08\x8a\x8e}Md\xf46Z\x99\x91G\xe5Lv\xf2zf\xba\xdd\n\x16[\xfbBz\x02\x0e\xde\xe6\xb1\x03\xa4\x03*\x80\xb6:\xe2\xb6\xbc\x1c\x92\xddx\x869\xa6,\xc1\x01#>\xb8\xab\x8e\xc4\xcbs\xd3\xee|\xbb\xa8\x90\xba0+\xdc~\xb5\x9c\xf6\xc2Q\x1cBr\x13`\x0b\x83\x8e\x01\xf5\xf5\xea*\xfe\x9a\t\x81\x84\x98-\xdc\xfe?\xe1R\xff\x00g\xe5\xb2\x8c89\xdaO\xafZj7!\xca\xc6\x1c\x963H<\xa9\xae\x03\xc6I\xc1#'\x07\xb7\xd2\xa1\x8bD\x926\xfd\xe4\xde`\x1d\x0e\x06\x7f>\xf5\xd3&\x9a\x83\r\xb3\r\x8epx\xfc\xaa\xd4VJ\xbd\x16\xb4Q!\xc9\xb3\x1a\xdbLR\x83(\xa4\x8e\xe4b\xa6\x8bJ\x1eib\xa3?J\xdd[p\x00\xe2\xa5X@\xedV\x92D6g\xc5f\x17\xf8EYKU\xf4\x15kh\x1d\xa8'\x14\xc3Q\x8b\n\x8e\xd5 U\x14\xdd\xc3\xd2\x8d\xc3\xd2\x809\x13)\xa6\xf9\x86\xa5\xda\x94\x9f'\xa5`ia\x81\xd8\xd2y\x8dR\x12\xa3\xa0\xa4\xde\x9e\x94\\,\";n\xab\x8b\xc8\xaa\x9b\xd0v\xa7}\xa1TQp\xb1d\x8a\x91P\xb2\x83\xe9T\xbe\xd4*\xecM\x94\xe6\xb1\xad\xb25\xa3\xbb\x10\xe1\xa6P\xc7h\xa7]\\\xc1\n\x1c0\xfc\xeb'\xc4\x17f\xdbM\x96T8u\x07\x15\xe5\xcb\xe2\x9b\x82\x8c\x8e\xec\xc7q\xefYF7F\xcft\x8fW\xd1\xa5\x17w\xb2\xba\x8f\x91H\x04\xfa\xd6\xf0\x00\xf7\xcf5\xcex*)\x13@\x8e\xe2e\x02Y\xbes\xec\x0fJ\xdef!ONzb\xb1z\x17\xbe\xc5\x1b\xbbe\x91\xd9\xb989\xaa3\xa9Pp\xea0;\xf45o\xcei_\x04\x8c\x03\x8c\x93\x8a\xadqn\xcc\xddJ\x83\xcda\xca\x9e\xc6\xe9\xd8\xc5\xbc\xba\x10(\x1b\xfa\xfa\xf3\x9f\xc6\xb9]F\xe9\\\x108\xc9\xe0\x8e\x95\xd0\xea\xb6;\xe3m\x99\x04rs\xc5p\xf3\x19~\xdc`9\xcej\xacR}M\xed\x1e \x8b\xe61\x1c\xfa\xd3\xb5\x1b\xf43\xa4\np\x085A\xefE\xacJ\x85\xb0:R\xe9\xba&\xa3\xae\\\x99U\x0cQ\xe0\x80\xed\xefZ\xd3\xa4\xe4\xf43\xa9YGs\x1a\xf6uy\x8aB\x86\\\x9f\xb8\xbc\xf3ZV\x9a.\xad\x7f\x1e\xd1\x18\x84\x1c`\x0eN+\xd04\x7f\x06\xd8i\xb0\xe5\xa3\xdd&r\xccy&\xba\x04\xb3EU\x11\xa8\x00{b\xbb\xe1B1Z\x9e|\xeb\xcaOC\xce-\xfc\x03\xb8\x03vZL\x9c\xe5\xcf\xf4\xad\xbbO\x05\xe9\xd0\x91\x88\xc1\xe3\xb2\x01\x8a\xecD)\xdc\x93\xf4\xa7\x18\xe3\xdb\x92\xb8=\xc5n\x95\xb62m\xb3\x9f\x8f\xc3\xd6\xb1`(9'\x9cU\xd4\xd0\xe1E\xf9F{\xe0\xd6\x8bD\t\xda\x1b\x19\xe9\x8a]\x84\x01\x96\x07\xbf\x07\x14\x07C4iPn\x04/#\xdb5et\xf8\xce>Q\x8f\xa6*\xd0Rpr\x14z\x1a\x01\xd8F~c\xec{P\x08\x86;8\x80 \xa2\x80=\xa9Z\xca,\xf2\xbf\x90\xa9\x8b\x05\x04\x8e\xbd\xc5\"\xbe\xd1\xd0\x00z\xf3@\xee\x88\x9a\xce\xdff\x0c(~\xaa\rW}2\xc1\xd7\x0fi\x01\xf5\xdd\x18\xab\xef\x92\xdc\x93\x83\xe8*=\x8eX\xf1\xb8{\xf4\xa2\xc1td\xcb\xe1\xcd\x1c1can3\xd3\t\x8f\xe5UO\x8446\xcf\xfa1@y\xf9$a\xfdk}\x94\x96\xeb\x8c\xf6\xa8\xfc\xa5\xf7\x079\xf5\xa9\xe5\x8fa\xf3\xbe\xe77'\x82\xb4\xc3\x9d\x93]&?\xbb&\x7f\x9dU\x97\xc1`\x90c\xd5g^\xe0H\xa0\xd7^\x00by\xe0~\x14\x187\x9e\x068\xee*](>\x85{Y.\xa7\n\xfe\x11\xd4\x15\xf7E\xa8D\xf8\xec\xc8Fj\xab\xf8w]\x85\x99\xb6\xc5&\x0f\x1b\x1f\xfck\xd0D[N\x02\x1e\x050\x86RF0=k7\x87\x83\xe8\\q3]O<k\x1dR\x10ZK)\xb8\xfe%\x19\xfeT\x91o\x8c\xe6X\xa4R=A\x15\xe8e\x1cs\xc5Fb\xde2Pc\xd0\x8c\xd6o\x08\xba3O\xadK\xaa8\x84\x9c(\xc8=O\"\x8c\x1c\x96\x1c\xaf\xa5usi\xb6\xb2\xaf\xef-\xd3\x93\xe9Ue\xd0a1\x9f)\x8ag\xd3\xa0\xa9xi-\xb5-b\xa0\xde\xa71\xb5\xa5r\x07s\xc5j\xe9Zk]\x13\xe6\x8f\x95\x0f8\xeajU\xd1n\"`F\x1c\x03\x9e:\xd5\xadf\xe6\xebJ\xd0\x8bX@]\xc3.\xf0\x01%\x81\xeb\xd3\x9a\x98\xd1\x977\xbc\x87:\xf1\xb7\xba\xc7\xdcZ)B|\xc9\t\x03\x01\x10\xe0(\xacK\x9ddi\xf7\xebm\x83+:\x82\x14\xf2~\xb5=\xc6\xb0\xd0\x88\x89\x8d\xa3\x05y\r\xc7\xf95\x8e\x9a\x85\x9d\xcd\xf4\xeeg\x8df\x90\xe3nrq[r\xdd\x98sht\xd1x\x8e\xd1\x95\"\xb9-\x1f8\"NU\xaa\xfaKj\xca\xd8\x00\x97\xe4\xba\x8e\t\xec\x00\xfaW\x9b\xeaZd\xd2;I\x0c\xf2l\x1drr?\xc2\xaaiz\xd5\xd6\x9b#\xdb\xc9)0\x1e\x8eNqF\xbd\x04u\xba\xad\xfc\x1er\xc7\x18\xdd\xe5\x8d\xaaO\xafz\xe6\xb5\x19\xdaX\xdb'\xe5n?\x11L{\x83+\x99\xdaF8\x04\xabg\x80;\xd7?s\xa8\xcdus\xb2\x06>X\xe3\xea}j\xe1\x1dH\x9c\xb4-\x1bda\x8c\x8c\x9a\x92\xdfKL\xe5\xba{\x9a\x9e\xc3N\x95\x86\xf9:\xfdkY4\xe9\\|\xb1\xb6:\n\xdf\x90\xca\xec\xcf\xfb\x1cA1\x19\x0bH\x96\xb21\xda\x9c\x8fj\xe8-|?,\x8d\xf3)\x02\xb7\xec\xbc<\x89\x8e*](\xbd\xcbU\x1a\xd8\xe5,\xf4Y\xe7`\x19@\x1d\xeb\xbc\xf0\xbe\x88\xb6\x934\xee\xa3\x85\xc0\xe2\xb4m4\xa8\xe3\x1c\xaekf\x08\x04i\x85\x18\xa6\xa0\x96\xc4\xb97\xb8\x9eR\x13\x801\x9a\x95m\x94`\xe2\xa6T\xf5\x15(\x02\xa9\"[#X@\xa9\x02\x81JN\r!<S\x04.E&i\xbb\xb8\xa6\x17\xc5\x03\x1c[\x1d\xe9\x9b\xcd4\x9ai4\x01&\xff\x00ZM\xd8\xeaj<\xd1\x9c\xf7\xa0\x0eokRl5h\x80)6\x8fJ\xe74\xb9\x08\x8c\x9aC\x1dX\xa2\x9d\x85r\xb1\x8b=)>\xcesV\xa9(\x0b\x95\xc4<\xf7\xab\x82U\x8e!Une\x11\xa8\xf54JI\x81y\xe7\x15\xc9Z~\xf7)\xd5F:s\x14\xf5i\x12{g\x8d\xb9\x04\x11^sc\xe1\xb2\xda\xd2\xc5\x8d\xe2I0\xa0v\xae\xd6\xf0\x92\xe7\x9a\xbb\xe1=?\xcc\xd4$\xbbe\xca\xc66\xa9\xf7=\x7fJ\xce2f\xb2Ks\xaa\x86\xd9m\xed#\x860UU\x00\x18\xf6\xaa\xd7NW\xe5\x07\x8e\x9f\xfdz\xd2\x94\xa8\x8fo\xb1\xe4V\\\x8a\x1d\xba\xf4\xc9\x02\xa2k\xb0@\x80\xa3\xdb\xa7\xcc\xbb\xf3Li\x04\xa3=\xeaYd(\x06\xdc\x15\x03\x18=\xea\xbb\x85T%\x86\x00\xf5\xedI+\x16\xdb2\xf5U\xdb\x11e`A\x15\xe7\x1a\xa5\xea\xa6\xa3\xfb\xb5\xdd6\x08U^\xa7\xd2\xbb\xcd_P\x86\x0bg,\xd8\\psY\x9e\x0e\xf0\xb8\x9a\xe5\xf5\x9b\xd5\xcb\xb9>J7e\xf5\xc5mF\x92\x9c\xb53\xabW\x92:\x07\x86|\x16\xf2\xec\xbf\xd5Ay?\x82.\xcb\xff\x00\xd7\xaf@\x8a\xd9!@\xa8\x15\x14\x0c|\xa2\xa4\x04\x01\x85\\\x00?*\x91Tc'\x1e\xd5\xe8\xa4\xa3\xa2<\xf6\xdb\xd5\x88\x80\x8f\x94\x0e\xdc\x1fZ\x95Fq\xb8\x00z\xf3\xd4\xd4!\xc2\xf0s\x9fQNY\x08\x0c\xde\x9e\x83\x9cS\x10\xf6#\x04|\xa3\x1e\xf4\x9b@\\\x8cc\xb6\xe3\xd2\x93s\xc8\xbd\xf0\x07\x04c$P\xc42c\xa0\xed\xc5\x00\x05\x898\x07\xa8\xec*6A !\x97p4\xe5 \xb6w\xb6}i\x08\n\xe7\x929\xa0\x06\x98\xf1\xd4\x90\x07QF\xe00=\xf3\xc5N[z\xae\x18\xf1\xea:\xd3\x08\xf9rO<\xd0\x03\x03\x8c\x94\xdaI'9\xdb\x9a\x7f\x039P?\x1c~\x94\xc6dV\r\xb8\x93\x92\x00\x1d)\xe4\xaf\x04\xfc\xde\xf4\x00\xbf\xc5\x90\x0f\x03\xb7j\x8d\xb7\x0c\x1c\x8cg<S\xbe\xf2\xe4\xb3\x03\x9e\r5A\x00\x92\x0e=I\xcd\x00F\xcc\xe4\xff\x00\xf5\xf3J\x06\xec\x80\x0eq\xd8\xd4\x8e\xbeb\xe4\x15\x03\xb6(\xf9\x818\x1c\xe3\x8eG4\x01\x1e\xc0\x00\xe0\x93\xe9\x9a\x1dK\xe0 \xe7\x1d3N\\1\xe7<\x8c\x10{S\xce\xdc`\x0e\x7f,\xd0\x04[K&y\xc8\xf7\xa8\xfc\xb7\x03\x9e>\x86\xac\xae\xd5\xc0\xea3\xdf\xad1\xf6\x00>\xb8\xc0\x1d\r\x00FC(#\x9cz\xf5\xa6\x02\xc5{\x00\x0fzz\x9c\xb3d\x0e)\xec\xbf/\xca\xa3?\xca\x80\xb9\x04\x81q\x93\xd0\xfaT`\x13\x8c\x0c\xe7\xda\xac\x98\xf7\x00v\xe0\xf3\xcfji\xb6\xc1\x04\x07\x18\x19\x1d\xa8\x02%\xe9\x82\xa3\xe9\x83AA\x9c\xe0\xe7\x1c\x8a\x7f\x00`d\x9cr8<\xd3\x8a\xb9\xe4\x0c{f\x80+\xb5\xbcR)\x0f\x1a\x90GB\x01\xac\x99\xfc)\xa2Kr'm2\xd8L\x0f\x0e\xab\xb4\xfe\x95\xd0\x05]\xa1\x83`\xfb\xfaST9\x1d\x0f\xa8 \xd2\xb0\\\xc2o\x0c\xd8\xed`\x91\xca\x80\x8c\x1c9?\xce\xb0\xaf\xfe\x1d\xda]s\x15\xe4\xd1\xa8\xe8\x00\x06\xbb\xd2\xa7\x92\xe1\x81\xfau\xa6d`\x06Q\xf9Q\xca\x87vp\x96?\x0f\xec\xedQ\xa3\xb8\xbb\xb9\xb8F\x18+\xc2\x8f\xd2\xb4\xac\xfc'\xa2X\x12\xd0i\xf1\x83\xea\xe0\xb1\xfdk\xa9h\xd7\x1d2\x0fLTB-\xac\x0e\xd0@\xe9\xcf\xe9BV\x11\x96\xb6\x96\xd1\x8c\x08\x15G\xfb\x952\xda\xc4\xc7j\xaf\x1fJ\xd1\x10\xee#\n\xa3=\x8fz\x9a(\x00\xe3\x14\xc0\xa7\x15\x92\x83\xc2\xd5\xd8-\x95Nq\xcdXX\x80\xedS\"\x81T\x02\xc5\x18Q\x9cT\xca\xa3\x14\xcc\x8aP\xc0\x1a\x00\x934\xb9\xa8\x8b\x02i7`u\xa0V$'\xbd4\xb8\xe9Q\x97\xf7\xa4-\x9a\x00yaM'4\xc2qM.\x05\x03\x1f\x9aJiz\x8c\xbd\x00HH=i\xa5\x80\xa8L\x98\xa8\xdaL\xe2\x80?\xff\xd9"
 
-__cat__ = b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x00d\x00d\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x06\x05\x08\x07\x07\x07\t\t\x08\n\x0c\x14\r\x0c\x0b\x0b\x0c\x19\x12\x13\x0f\x14\x1d\x1a\x1f\x1e\x1d\x1a\x1c\x1c $.\' ",#\x1c\x1c(7),01444\x1f\'9=82<.342\xff\xdb\x00C\x01\t\t\t\x0c\x0b\x0c\x18\r\r\x182!\x1c!22222222222222222222222222222222222222222222222222\xff\xc0\x00\x11\x08\x02\x00\x02\x00\x03\x01"\x00\x02\x11\x01\x03\x11\x01\xff\xc4\x00\x1f\x00\x00\x01\x05\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x10\x00\x02\x01\x03\x03\x02\x04\x03\x05\x05\x04\x04\x00\x00\x01}\x01\x02\x03\x00\x04\x11\x05\x12!1A\x06\x13Qa\x07"q\x142\x81\x91\xa1\x08#B\xb1\xc1\x15R\xd1\xf0$3br\x82\t\n\x16\x17\x18\x19\x1a%&\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe1\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf1\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xc4\x00\x1f\x01\x00\x03\x01\x01\x01\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x11\x00\x02\x01\x02\x04\x04\x03\x04\x07\x05\x04\x04\x00\x01\x02w\x00\x01\x02\x03\x11\x04\x05!1\x06\x12AQ\x07aq\x13"2\x81\x08\x14B\x91\xa1\xb1\xc1\t#3R\xf0\x15br\xd1\n\x16$4\xe1%\xf1\x17\x18\x19\x1a&\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x82\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xda\x00\x0c\x03\x01\x00\x02\x11\x03\x11\x00?\x00\xe7g\x01Eg2\x06l\xd5\xeb\xa6\xdc\xd8\x15O\xbdCfI\x0f\x8a1\x9a\xb0"\xc0\xa8c\xe0\xd5\xc4\xf9\x85ID\x05)\xa1H=*\xde\xce\xb4\xd5L\x9a`\xd8\xc4N*d\x8cu=i\xeb\x1f\x14\xc7l\x1ab\x8c\x88f`\rgNA5r\\\xb5T\x91*JlXc\r\xcdj\xd9D7\x8a\xcc\xb6l6\rl\xd9|\xce*\xe2C6\xa0\x80\x15\x1cTw\x16\xa39\xc5_\xb6Q\xb6\xa4\x99\x01S\xc5hI\xcf\xcf\x0f\xcb\xd2\xb2\xa4\x8b\x04\xf1]\r\xca\xe1Mb\xcd\x80\xc6\xa1\x96\x8ax\xc7\x14\x04\xc9\xa5\xeajx\xd3=\xaaJ\xb8\xd5\x8f\x1d\xa9Bd\xd4\xcc\xa7\xa58 \x1c\xd3\xb1\x9ba\x12b\xaeD\xc4w\xaa\xa0\xe0\xd4\xf1\xf3J\xc3E\xe8\x0f\xcd\xd6\xb4\xe3\x03\x02\xb2b\xc8\xe8j\xec3c\x82j$\x8db\xd1q\xf1P?\x15&\xf5a\xc1\xaa\xd7\x0f\xb4qP["\x96Q\x9a\xac\xceMD\xef\x96\xc1\xa9\x10\x82\xb5\xaa0dL3Q\x98\xf2sVB\x8c\x9ai\x00PI_n\x05\'\x96\r=\x8f4-!\x8e\x890j\xe4i\xc5V\x8c\xe0\xd5\xa4j`)AU\xa5\x879\xab\xc3\x9ak\xa7\x14\x08\xc9u\xdbU\xf7\x00\xd5\xa3<y\x15\x99(\xda\xc6\x90\xd0\xfc\xd4\x80\x82*\x159\x19\xa7\x03@\xc9B\xe6\x94/4\x80\xf1R/#\xa5\x001\x97\x8a\x85\x97\x9a\xb9\xb7"\xa2h\xcf<R\x19E\x86\rFG\x15i\xd6\xa0a\x81LeG\x03&\xa9\xcdZ\x0f\x19=j\xac\x91\x1c\xd02\x83T\x12\n\xba\xf1b\xab\xbaS\x1a(J\xbdj\xab\xadh:Ui\x16\x99E2\x94\xcd\xb5d\xad0\xad\x02 \xdbKO\xc5!\x14\x08f\x05!Zv)@\xa0dEi\xa5jR)\x08\xcd\x03\xb1\x16)\xc0S\xb6\xd2\x81H\x05QS(\xa8\x80\xc5J\x94\x98\xacv\x92\xc8\t\xa8\xc2\xe4\xe6\xa0\xf3r\xd8\xab(\xc3\x02\x99\x16&\x8e=\xdc\xd5\xa8\x97\x15\x04L\x08\xcd[R1HBI\xc0\xa5U\xe2\x98\x1b{\xe2\xaeC\x1eH\xa0\x892"0\xb5^A\x9ekBh\xf7p*\xb3@i\x8d+\x19\xd2\xfc\xa6\xab\xb3sW\xa7\x88\x8e\xd5I\x90\x83\xd2\x93(\x8d\x01\xf3Emi\xdfxVZ\'\xcd\x9cV\x9d\x8f\x12S\x88\x99\xd3\xdb\xb7\xca*fl\x83Y\xf1I\xb4b\xacy\x99Z\xd6\xe2\xb1R\xf0\xe1Ms\xf3\x02\xces\xd2\xba\x1b\x85\xca\x92k"X\xf2MK\x15\xca\x08\x9f5^\x89zqQ\xacxj\xb2\x8b\xd2\xa4\x97!\xa53\xcd4\x8cU\x82\xb8\x15]\xcf4\xc4\x86\x11S\xc3\xefP*\x92j\xca!\xa4h\x8b\nH\xc7\xa5?~\x0fZH\xd4\x91C\xae(\x1ad\x9e~:\x1a\x8d\xe4/\x91\x9a\xae\xc4\xf6\xa6\xee\xe2\x95\x86\xd88\xefJ\xb2m\x14\xd2\xd9\x15\x031\x06\x99%\xa5\x94f\x9e\xc75\x9c\xb2\x90\xddj\xe4\'w\x15"J\xe2m$\xe0\n\x9e\x18\x19\x8fJ\xb3\x15\xbe\xe0\r_\x86\xdf\x8e\x94#x\xd1l\xa0-Ni\xe2"\xbd\xabX@8\xe2\x87\x80zUXr\xa2\xd1\x9c\x82\x9c\xc354\x91\x85\xa8\xce1H\xc1\xab\x14\xe5N\x0ek2x\x81\'\x15\xae\xe3"\xa8\xca\xbdi\x05\x8c\xe0\xb8\x18\xa3\x90jW\x18\xce:\xd4D\x1a\x06J\xa4\xd4\xd1\x8a\xac\x8f\xce*\xdcF\x81\x16cL\xd2\xb4T\xe4\xa91\x9a\x00\xce\x9a\x1e\rV\xfb9\x1c\x9e\x95\xac\xd1\xe7\xadD\xe9\xc7J\n\xb9\x94\xf1\xfbUg\x8e\xb4&B\x18\xe6\xab2\xd3\x03>H\xea\x9c\xd1\xe2\xb5dZ\xa3:\xe2\x82\x91\x99"\xd5vJ\xbc\xc9\xcdB\xcbE\xc7r\x8b%D\xcbV\xdcsP\xb2\xf3@\x15\xca\xf3F\xde*]\x98\xa3m1\x15\xca\xd3q\x8a\xb2R\x98W\x8a\nD\x0c)6\xd4\xa5y\xa3m\x05\x11\xed\xa3mI\xb6\x8cR\x01\x98\xa9\x10Q\x8az\nL,m\xc6r\xdcU\xb4\'\xa5QC\xb4\x83VU\xf3\xc8\xa0\x86]\x8d\x8a\x8a\xb0\xb2\xe0{\xd58\x9f<\x1a\x95I&\x91\x9b/\xdb\xf2rkN\x1c\x04\xc9\xac\xe8\x07J\xb0\xd3m\x00\nd-Ym@.)\xcf\x18\xf4\xaa\xb1\xcb\x92\rY\xf3r)\xa2\xaeT\xb9\x8cl\xce9\xaa\x0f\x10=\xaa\xf4\xcf\xbb"\xab\x9f\x9a\x8b\x05\xca\xcb\x1d]\xb3_\x9e\x9a\xb0\x93\xce*\xcd\xb2m&\x98\xee]\x07\x18\xa9\x04\x98\x1dj\xbb\xca\x02\xf5\xe9P\x19\xc9\xa6D\xa4X\xb8\xb8\xe3\x15E\x9bw\x02\x9f\xb1\x9f&\x84Lv\xa2\xe6z\x8dT\xc5J\x8bO\x0b\xd3\xd6\xa4+\x81H\x08$8\x18\xaa\xe7\xadM\'sP\xe3&\x82\x91<H\t\xa9\xc0\xc5@\x84\xa8\x06\xa6W\xcd\x05\x96\x13\x00S\\\x8c\xd4f\\\x0e\xb4\xd2\xf9\xa0.6E\xa8z\x1a\x9c\xe5\xa9\xa6"h\x0b\x90\xb0\x18\xcdU\x90\xb6j\xe3&\x07Z\xab/\x06\x864UV!\xebJ\xd7\xa8\xac\xee7V\x85\xabt\xa4\x95\xcd\xe8\xc56o[`\xa8\xad[x\x81\x15\x8dl\xfd+r\xcd\x81\x03\x9a\xd5GC\xd5\xa7\x05bc\x08\xebLh\xc0\xabm\x8d\xb5VF\x19\xa9\x96\x84\xd4\x8a\xb1B\xe5\x00\xc9\xac\xe6~H\xad+\xa6\x1bNk\x0eY1!\xe6\xb2\xb9\xe6\xd5\x8e\xa3\xdd\xb1\xde\xab9\x06\x9c\xcf\x9a\x88\xf5\xa9\xb9\x9f)\x1b\xa8\xa8\x19qS\xbdB\xfc\xd3L\x96\x88\x80\xc1\xabP\x9e\x95_\x18\xa9bl\x1aw$\xd1\x8cdT\xc0T\x16\xec\t\x00\x9a\xb5\x8a`0\x8c\xd4n\xbcT\xacqP\xbbP\x05;\x84\xe35I\x85_\x94\xf5\xaa\x0cE\x05"\t\x05g\xcf\xc95ny{-Qpy4\\\xa4W|\n\xae\xe6\xac8\xebP\x95\xc9\xa5pefRi\x851V\x8a\xd4l\xb8\xa6M\xca\xe4\na\x03\xb5H\xc2\x9b\xb6\x98\x11\x91M\xc5HE4\x8ae\xa1\x98\xa4\xdbRu\xa4\xc5#B:LS\xc8\xa4\xc5\x00&)\xea(\x02\x9e\x06(\x03E\x0ei\xd9!\xb8\xa8\xa2&\xa7\x0bJ\xc6L\xb3\t\xce*\xec*GZ\xa5m\xc9\xc5j(\xf9)\xd8\xceD\xd1\xc8\x11)\xc1\x83sT\xa4r\x1b\x14\xb1\xcas\x82h\x04\x8b\xf1\x1f\x9b\xda\xae`\x95\xa8m\x15q\x93W\x86\xc0\xbdi\x92\xcc\xf7\x1c\xe3\x14$c\xbdJ\xcc\x85\xfa\xd4\xe9\x12c9\xa625Q\xb6\xa3yD`\x80i\xd3\xc8\x10aMgH\xce\xc4\x9c\x1a\x19\r\xd8\x92I\xcb\x1cf\xa4\x803\xf4\x14\xcbKG\x91\xb2\xe0\xd6\xba\xc2\xb1 \xc0\xa4\x95\xc4@\x17lx\xa6\xa7=)\xd26x\xa7D\x00\x14\x00\xa1y\xa7=("\x94\xf3L\x92\xa4\x82\xa1\x08j\xd4\xa4\x0e(U\x1bi2\x91\n\xe4u\xa9J\xfc\xb9\xa4+\x8an\xe2:\xd4\xdc\xab\x88\x0ez\x9a3\x86\xf6\xa8\xdd\xca\xb6hV\xdci\xdcW,\xab\x0cS\xb3\x9a\x8bi\xc6E3\xcc\xa3\x98\x10\xb2\x92s\x8a\xa8\xe8[\xad[##5\x0b\xa5C\x91h\xaac\xe7\x8a\xb5\x07\x18\xa6m\xa9QpETY\xad9\xd9\x9aP>+V\xdaR1\xce+\x1a\x1e\x9dj\xecrm\xady\xd5\x8fJ\x9d}\r\x91pq\xcdF\xf3\x01\x9c\xd55\x9b\x8c\x1al\xb2\x8c\x1ek)N\xe2\x9dk\x8c\xba\x9fvGj\xca\x97\x07\xa5M3\x96$-F\x90\x93\xebPp\xceWd*\x875&1VD<SL\'\xb5\x16#\x98\xa8W4\xc6\x8f\x035t@z\xe2\xa3\x923\xd8R\xb9\r\x99\xe5i\xe8\xbe\x94\xe7F\x1d\xa9\xd1\xa1\xa7r\x07\xaeA\x04U\xb1!"\xab\x85\xc0\xe6\xa6E\xe2\x9d\xc7q\xdb\xc9\xa8\xe4j{!\x1c\x8a\x88\xa9#&\x8b\x82+Jx\xac\xf9rI\xad)\x12\xaa<c&\x95\xcd\x11\x9e\xc9Q2U\xd7\x8f\x15\x19J.6\xca\r\x16j3\x16;V\x81\x8b\x8a\x89\xa3\xa2\xe6nE\x06Z\xae\xe0\xd6\x89\x8b\'\xa5D\xf0\xd5!\\\xce1\xd2\x14\x15m\xa3\xc5B\xe9\x8a\xab\x94\x99\\\xae\x053mNP\x9ahB;Qr\x93"\xd9M)VDy\xa0\xc7J\xe5\xa6T(i\xa5MY1\xe2\x93\xcb\xa6Q\x08ZpZ\x93g\x14\xe0\x94\xec2\xfa\xc1\xdf\x15"\xa5X\xdb\x8e\x94\xf8\xe3\xcf4\x1c\xeeBC\x160@\xab\x9b\xb6\xad,q\xf1\xd2\x99.\x056\x89\xdc\x84\xee\x90\x93\x8f\xc6\x84F\x0e\x00\xa9\xe3B\xd5\xa7\xa7i\xe2irjyF\xf4\x19o\x13\x04\xe6\xa3\xb8\xb8h\x869\xae\x89\xac\x14.+\x13Q\xb5\x01\xc8\xefCFe\x08\xe7%\xb2M]\x17\'f\x05W\x8e\xd0\x93\xec)\xf2&\xcf\x94sI\xbb\x0c\x91s\'\x07\x9a\xd0\x86\xc9v\x82\xc0Uk([p$V\xba\x90\x06\x08\xe6\xaa.\xea\xe4\x91\xc7\x06xU\xc0\xa7\xcbk"\xa9$f\xb5lm\xcc\x8c\x0e+k\xfb=Z<\x15\xed\\U\xb1\xd0\xa7.Tt\xd3\xc39+\x9eoq#$\xbb\\m\x14\x82\x7f\x97\x8a\xe8\xfcA\xa3f\xdd\xdd\x07\xcc\xbc\x8e+\x93\x8d\xf09\xad\xa9VU#teR\x9b\x83\xb3-\xac\xa4\x9e\xb58|\n\xa7\x11\x18\xf7\xa7\xbc\x98\x15\xaafB\xbc\x99|T\x8c\xdf/\x15Gyg\xe3\xadZL\xed\xf9\xa8\xb8\xc5\x12\x1c\xd0[<S[\x03\xa5=#,A4\x82\xe3\x1a2\xe4U\x88\xadO\x15n\xde\xdc0\xf5\xab\xf1\xdb\x0c\x0e)\xa4$g\xc7\x0e8\xc5E-\xb0\x0f\x9cV\xd9\xb6\xf9x\x15V\xe2\x03\xb7\x8a\x1a)\x19m\x0e\xc1U\xdc\x1c\xf1ZA\x069\xaa\xd2\xc5\xcf\x15\x93-\x94@\xe6\xad"q\xd2\x90F\x01\xe6\xacD\x9c\x8cSD\xa6I\x0cY\x15i"\xc7j\x96\xddF1V\x84@\x8e*%3\xb6\x96\xc5eA\x8e\x95\x14\xa9\xc7\x15p\xa6\rD\xeb\x9f\xa5B\x9e\xa5\xcfb\x92\xc1\x9a\x99`\xc7j\x95\n\xf6\xa9\x061[\xa3\x8d\xb2!\x17\xb5#AV\xd1;\xd4\xbe]6$\xca&\xdf\xe4\xe9Q4\x00\x0e\x95\xa8c\xe2\xab\xcc\xb8\x15\x84\xe4o\x08\xdc\xc5\x9a0[\x18\xebB\xc3\x8e\x82\xac:\xe6QV<\xb1\xb4dR\x84\xaeMH\x14\xb6T\xa9\x1f\xb5N#\xc9\xa9Dx\x1d+[\x9c\xed\x15\x8cc\x15]\x86\t\x15}\x86\x05P\x93\xef\x9aM\x8a\xe5y\x13 \xd5f^\xb5q\x85Wq\x8c\xd2\xb9|\xc57\x15\x03\x0ej\xd4\x82\xa0#\x9a\\\xc4\xb9\x11\x95\xa6\x98\xf3S\x84\xa7m\x02\x9a\x90\xaeU0\xd3L\x19\x1d*\xe6\xdc\xd0S\x8a\xb4\xc2\xe6T\xb6\xe3=*\xab\xdb\x12ki\xe2\xcdB`>\x946Rf9\x80\x8e\xd4\xdf+\xda\xb5L\x15\x19\x82\xa7\x98\xab\x99\xbb1\xda\x83\x1f\xb5^h}\xa9\x86>:SR)2\x83%7e[h\xbd\xa9\x86<V\x89\x9a\\\xab\xb6\x93\x15;/5\x19\\U\xa6;\x9a\xe0\x0e\xfdjE \x1a\x8a\x86<f\xa6\xe7!o\xce\x01q\x9a\xaf#\xefa\xcdS\x92\xe3\x1d\xe8\x12\xef\xe8j\xd35\x8ct4\xa1|\x0e\xbc\xd7K\xa3\x9d\xb1\xe7\xbds\x16\x91\x93\x8c\xd7Kf\xeb\x14C4\xc8\x935d\x94\x05-\x9a\xe7.\xe6\xdf19\xe9W//\x02\xc4@5\x85%\xc0\xc9\xc7SJR\xb1\x17\xd4\xb9\xf6\x8cp:\x9a\x9e\x08\xf7\x9c\x9eMR\xb6\x8c\xbbd\x8a\xd8\x81B\n\xc7Y0r,\xc5\x1a\xc6\x80\x9a\x9a\x03\xe7K\x81\xda\xb3\xae.p\x08\x07\x8a\xbb\xa2e\xdf\'\xd6\x8a\xd2q\xa6\xf9J\xa4\xaf%s\xac\xd3\x90ax\xad\xe0\x80\xa5d\xda\x05U\x15yg\x00c5\xf2U\xf9\xe3\'&{\x94\xedk"\xae\xa1\x02\xbcL\xb8\x1d+\xcdu\x8d?\xecw\x0c\xc9\xf7X\xe7\x15\xe97sd\x1ek\x89\xf1\x03\xab\x82;\xd7^Y\x88\x9b\x9d\xba\x18c)\xc7\x92\xefs\x9cW\xc0\xa9\x02\xb4\x9cv\xa8\x95=j\xc4R*q\x9a\xfa4x\xe4\xb0\xdb\x8fJ\x9f\xec\xac\xdd\x01\xa9\xad6\xbe3Z\x89\xe5\xaa\xf6\xadTEs$Z\x14\x19"\x9f\x1c`\x9a\xb5q(\xe7\x9e+4\xde\xacr`T\xb4\x90\x1a\xb6\x8a\x11\xb0zV\xa4J\rb\xd9\xb9\x92M\xc4\xd6\xccm\x81\xd6\x9a)\x16v\r\xb5\x9fr\xb8\x06\xae\x89\xc1\x15R\xe5\x81\x19\xa1\x94\x8c\xad\xbc\xd3dBV\xa5\x07\x93C\x11\xb6\xb2\x19D\x8ejxT\xd3\x1b\x86\xabPa\x97\x8a\x12#\xa9f\xdb\xae+A\x00\xc0\xac\xf4\xf9O\x15e%\xc7Z\xc6KS\xaa\x9c\xf4\xb14\xaa1\x9a\xa8\xfd\rX2\x02\xa7\x9a\xa8\xe7\x9a\x95\x1dG:\x9a\x0c\xc0\x02\xa4\x88d\x8a\x88\x9c\x1a\x96\x16\xc1\xcdl\x99\xcc\xd9yT`S\xf2\x07Z\x84I\x81Lyr\xb5M\x8a\xe4\xcf(\xf5\xaa\x92\xbej7~NMS\xb8\xb8\xda\x0e\rs\xcd\x1d\x10\xa8\x90\xe6 \xcdVT\xe4b\xb2\xa1\x98\xb3\xe4\xd5\xf8\xe4\x18\x154\xd0\xeaM2\xd0\x18\xa96\xe4T\x01\xeaA \xc5ms\x9d\xb1\x93p\xa6\xb3[\x92j\xec\xf2\r\xa7\x06\xa8\xee\x14\x99\x17\x1a\xc2\xa0qS;\x80*\xb3\xb1&\x90\xaeA \xa8\xca\xd4\xfbsM+\xedI\x8c\x84\x9eh\xcd9\x92\x9b\xb7\x157\x10\xa0\xfaR\x80M9@\xc5<\njC\xb8\xdf.\x98c\xe2\xa7\x02\x82\xbe\xd4\xf9\x8bE6\x8cv\x14\xcf/\xda\xae\xec\xcfjC\x1e*\x1c\x8a\xb9\x9e\xd1s\xd2\xa1h\xf1Zf<\xf6\xa8\x9e/j\x14\x85s1\xa3\xa8Y+I\xe1\xc1\xe9P<C\xb0\xad\xa3"\xd33\x9a.sP\xbaqZM\x1dWx\xf9\xadT\x8a\xb9!\xcezS$,G\x03\x8a\xb2c9\xc9\xa9\xad4\xdb\xbdF_.\xce\xdeI\x8f}\xa3\x81\xf5=\x05]\x89\x95\x91\x87$L\xe7\x15r\xce\xd9\x8f\xca\x01\'\xd8V\x9d\xc5\x8d\xb5\x81),\xcbst:\xc5\x03~\xed?\xde~\xe7\xd9A\xfa\xd5Q\x1f\x99\xfe\xb5\xce\xdc\xe7\xcbC\xb5\x7f\xc4\xfe4\xd2\xb0\x9c\xf44 \x8f\xca\x00\x9c`\x1cpA\xe6\xa6\x92\xeb\x03\x00\xd6w\x98\xb1\xae\xc8QU}\x14b\x960X\x8c\x9ei9\x19n\xc7\xca\xd2\xccp\xa6\x96\x1bF\xcf\xcc+B\xd6\xdb\x8c\x9a\xb7\xe5\xaa\xf5\x15\x1a\xb2\x9cREx\x93\xcbQ\xc5H\xf2\xe0c5\x1c\xd3\xa8\x04\n\xa6ebrzUh\x8c\xec[\xcf\x98}\xab[Mo#\x9a\xc4\x8euQV\xa3\xbc\xf9q\xd2\x89Y\xa2\xa3\xa3:\xe8\xb5EU\x034\xd95`:5rO;\x93\xf2\x9a\x11\xe5\xceI&\xbc\xfa\xd8ESdvC\x11\xc9\xb9\xd1\xcb\xaa\xbb!\xaev\xed\xde\xe1\xcb1\xe3\xd2\xa523.3PHI\xad0\xd8HQZ-LkW\x95B\x93#g\x8ahB\xa7$\xd4\x92J\x17\xebP\x19\t\xef]W0/\xc1>\xccsV\x1bP\n\xbdk$>\x05D\xee\xccq\xda\x9f3$\xb9q\xa8\x998\x19\xa8\x103\xb8cQE\x1f95r0\x00\xa4\xb5\x03N\xcem\xa0\n\xd3[\x9e:\xd7=\x1c\x81\x0f\'\x8a\xb5\x1d\xc6y\r\xc5U\xc6\x99\xae\xd7X\xefU\xe5\xba%O5E\xa5\xcfzcI\x9a\x1b)\x16\x16l\x93N\xf3s\xde\xa9\xb3\x103\x9aa\xb8\xc7Z\xce\xe3.\x93\x91\x9at3\x04=j\x8f\x9d\x91\xc1\xa6\xf9\x984\xeeK6Vl\x9c\x83N\x13\xe1\xab6)s\xde\xa7\xdex\xcdK\x1al\xbf\xe6\x96\x14\xcd\xc7=*\x05\x93\xa5J\x1b5\x05\xde\xe3\x8f&\xa5C\x8a\x83v9\xa6\x99qM2Ywx\xc5@\xf3d\xd4Fn:\xd5v\x93\x93\xcd\r\x90I$\xddj\x85\xc4\x99\xe0\x1a\x95\xdb\x8a\xaaAf\xcdD\x82\xe3\xe1j\xd0\x8d\xb8\x15B5\xe7\x8a\xbb\x10\xf5\xa4\x86\x99eZ\x91\xe4\xf4\xa6\x12ED\xcd\x8e\xb5@\xd8\xd9\x9c\xe4\x0c\xd4\x05\xe8\x95\xb2j\x02\xf4\x12=\x9b4\xca\x01\xcfZ\\\x8a\x06\x00P\xc3\x8aM\xdc\xf1A9\xa9b\x18W4\xa2:\x91W5&\xdfAR\xc6U+\x8aL\xb5X1\x9c\xd3J~u7\x19\x10b;S\xc3\x8a6\xfbR\x84\xc5+\x8d1\xc0\xd3\xb1\x91H\xa35 Z\x07q\x9b8\xa8\xde>j\xc0\x14\x85x\xa6\x82\xe5&\x8f5^H\xaa\xfb\x01Q2f\xaa\xe3L\xa0b\xaa\xef\x1f=+I\xa3\xe3\x8a\x81\xa3\xe6\xadL\xab\x9de\xd5\xd7\x86\xecr!\xd3`\xb8\x90t\n\x0b~lx\xae{S\xd7\xef\xaf\xa36\xc9\xb6\xda\xd3\xa7\xd9\xe0\xf9W\xf1=\xea\t\x1f\x15U\xc8\xc15\xd9)\xbb\t\xab\x15\x98\x15\x1d\x87\xb5G\xbd\xb3\xd6\xa4b\x18f\xa2\xc6\xe3Y\xdc\x96K\x1eX\xe2\xb4!\x84\x8ej\xbc\x10\xe3\x07\xbd_\x07\xe5\xaaB\xd8\x99.Z5 \xe2\xab\xdc_\xe0\x11\x9a\x86g `f\xa8I\x1c\x927\x00\x9a\x1b\x1d\xdb,\xc77\x9a\xf8\xad\x18\xecd\x9dp\x01\xaa\x9ae\x94\x9ez\xee\x1c\x13^\x87\xa5\xe9\x8b\xb1~Q\\X\x9cG\xb2\xdc\xde\x8d\x1es\x82\xfe\xce\xba\x89\xc8h\xce=@\xa9\xa2\xb2\x91\xce0Ez\xaa\xe8\xd1H\x83(?*\x82\xe7\xc3\x90\x91\xbd\x14+QC\x19\x19\xeeUL4\xa3\xb1\xc1\xc3\xa7\xedP[4K\x06\xde\x95\xbf{f\xf6\xc7\x04dz\xd6=\xd1\n=k\xbbKhr\xb5m\xca\x0ev\x82OJ\xcf\x9e\xe0\xb1\xc2\xf4\xab\x13\x96s\xdf\x15X\xc7\xb4\x1a\x8b\x92U\'\xd7\xad0\xbf\xa5H\xf1;\x9f\x94`R}\x92E\x1c\xd4\x80\x83\x9e\xf4\xa0~U<v\x92\x1e\xd4\xf6\xb2\x95Fx4\x01\x08`*Up*\xac\x8a\xf1\x9f\x98T\x06\xe3\x9csP\xe4\xd6\xc0^g\xdcx\xa9c$\x0e\xb5J9sV\xa2|\x9e\x94\x93\x93\x1d\x8b+\xbb\x14\xa4\x9at`\x9e\x82\x95\xe3#\x9a\xd2\xccde\xbeS\xebT\xd9\xcezU\xb6^\xa2\xa0h\xcei4\x17\x1a\x8ezsS\x0ej4B\x0fJ\xb0\x8a\t\xc6i\x88T%H\xabQ\xb1a\x8e\xb5Y\x86\x05Km \x13)=3\xcd!\x93\xa3\x80G50\x97\x8a\x92\xf2\xc4\xaa\t\xe3_\x97\xf8\x80\xed\xefY\xa2R\x0e\xd2{\xd4\xca..\xcc\xa7\xa1u\xa5\xc0\xa8\xdaOz\xaee\xe2\xa2irz\xd2\xb1-\x96\xbc\xd3Q\xb3\x92z\xd5\x7f7\xde\x9b\xe6\x1aB,n=;\xd3\xd13UU\xf2\xd5v"\x08\xa5a\xd8\x968\xf1V\x15p)\x89V\x10S\x0b\x10\xbf\x02\xa2d&"\xf9\xe8qWD[\xb3\xc19\xedLE\x12\xc8\xf6\x8a?y\xb0\x9e;\x91\xcf\xf8\xd6\x91\xa6\xe45\x06\xcc\xa9\rVc\x83Wd\x8b\xf1\xaa\xc6>\xbcVD\x11\xef\xa5\xdd\xc6i\xde^;PS\xb5\x000\x12j\xc2!4G\x16;U\xa4\x8f\x8e\x957\x0b\x08\x91\xd3\xfc\xba\x94\x0cS\x82\xd2c+\x98\xf2*6\x8f\x15{g\x15\x13%C\x02\x9f\x97J#\xcdY\xf2\xf2:Q\xe5\xd4\xdc\n\xa1pi\xde\x959\x8e\x9aS\x14\xd3\x01\x82\x9a}\xa9\xfbMF\xcd\x8a\xd1!\x918\xe6\xa3=jG\xe7\xa54-\x16\x18\x813Lhrj\xca\x8e)\xdb;T\xec\x06\x14\x8f\xb8u\xe6\xaa\xc8\xe7\xa0\xa9\\\xf1Qm\xc9\xae\xd9\x13}H\xceO\x15j\xde\x0c\x91I\x1c\x05\x885\xa7\x04;W\x9c\xd0\x85q\xab\t\x02\xacCn_\x8cT\x80\x00\x05X\x82E_J\xb1\xdc\x8b\xfb9@\xcb\n\x85m\x15_\x81W\xae.\x82\xafQY\xc9x$\x9c(\xf5\xa9\x93H\r\xab+L\xb2\x9cWi\xa5\xa6\xc4\x00\xd71\xa6\x91\x94\xae\xc6\xcdG\x96\x0f\xb5|\xfea[\x9d\x9e\xae\x1a6F\xac,\xa0S\xe6\xdaP\xd5\x07\x90 \x1f6*6\xbb\x00}\xfe+\xce\x861\xc5\xf2\xa3\xad\xd3\xbe\xa6F\xb4\xa3\xcbl\x0c\xd7\x0fp\xef\xbc\x82+\xb2\xd5n\x10\xa3\x1d\xc2\xb8[\xc9\xf6\xcc\xc7\x1cW\xd0\xe0j\xcep\xf7\x8f+\x19\x18\xa9h\x0eF2z\xd4\x01K\xb7L\x8a\x96\x12$5h@\xab\xc8\xeb]\xca,\xe2!\x8e%\xf4\xa7\xb4@\xb7A\x8a\x1aES\x83\xd6\xa1\x92\xe4\x80p)7a\xa2\xda"\x8cS\xe5P#\xe9Y"\xfd\xa3l\xb2\xe6\x92]_p\xc1B?\x1ai\xa1\x90\xde\xe3\x9e\re0\x1b\xea\xc4\xd7bC\xc6j\x01\x82rjX\x16b\x8fp\xfa\xd5\xa4\x8c\xaf5Z\xdd\xf6\xf1W\x03\xfc\xb5He\xa8[\x18\xedS0\xdc:\xd5$\x93\x9a\xb0\xaei\xdc\x05\xd9\x8a<\xb0GJy\xe6\x9c\x0f\x1cT\x88\x83\xcb\xa5\t\xcdO\x8e\xf4\x87\x14\x08\x8d\xd3\x8a\x8e5\xcc\x98\x0359\xe8i-\xe3\rr\x80\xf7a\xcf\xa5;\r\x1d=\xae\xa1\xb3Shg\xb7\x8aD\x8dT`\x8c\x7f\x08\xceMR\xf1&\x82\x90\xa3jzb\xb3Y1\xf9\xd0\xf2aoC\xec}h@\x7f\xb5\'q\xbc|\xe7\x0cN\xd1\xfa\xf1]\x05\x84\xb2BIh\x95\xa2\x90\x14\x96\x1e6\xca\xa7\xa8\xf4\x06\xb6q\xe6Z\x9ds\x82j\xc7\x9b\xee~\x84Q\x83\x9a\xe9|A\xe1\xf1\xa6\xdc\xac\xd6\xe4\xbd\x94\xe3t,G wS\xee+!b\x1d1Xr[C\x91\xa6\x9d\x99P!=i\xe2:\xb4\xd1\x01\xdb\x14\xc2\xb84X\x11\x1a\xc7\xcd[\x89\x07\x15\x12\x8ej\xc2\x8c\x00y\xe6\x95\x8aE\x98\xd4qVQTgq\x00z\x9e\xd5Z\x10\xc7\x04\xf0:\xd4\xe0\x19\xb0\x17\xee\xae\x08\x07\xb9\xf55\xa5:|\xcf\xc8\xd61\xb8\xb2\xdd(]\xb6\xe0\x823\x96a\xf7\xb8\xed\xe9P\xe8\x96\xee|K`#\x00\xbbN\xbf0\xf4\xee\x08\xfaf\x89B@\x8a\xa5\xb6\x9fBk\x7f\xc06"\xebY\x9fP*\x0cV\xa3\n}]\xbf\xfa\xd5\xd6\x92\x8a\xb25\xb2F6\xb9f,\xb5[\x88Uv\xa8rTz\x0c\xf1Y{\x07Z\xdd\xf1"\xb7\xf6\xbc\xec\xc3\x1f1\x00\xfa\xd6.8\xaf:j\xd2g\x1b\xdc\x85\x96\x9c\x88\r8\x8c\xf1R\xa0\xc5d\xd8\xd0"\x01V\x15E\n\xa0\xd4\xca\x00\x14\x86G\xe5\xd2\xe3\x15\'\x14\x84Rb\x1a\x055\x94\x9a\x90R\xf7\xa8`B\x10\xe2\x97mN\xab\x9aw\x97Y\xb1\x95v\xd4ej\xd9LS|\xa0E\n@Ra\x8a\xae\xcbZ\x0f\x1dW0\xe0\xd6\xaaB*m\xa7\x04\xcdX\xf2\xbd\xa9\xc2/j\xaea\x91*\x0cb\x9e#\xe3\xa5J#\xa9\x96:\xcd\xb08RK5O\x1az\x8abG\x8a\xb5\x1a\xf1\xcdw\xa2\x0b\x16\xf0\xee#\x8a\xd1\xf2|\xb4\xc8\xe9U\xa1!@53\xdc|\x98\x15j\xc0\x91Zy\x82t<U\x17\xd46g\x07\x9a.\xc9$\x81Y\xe2\x16g\xe6\xb2\x94\xed\xb0\xecI-\xf4\xb2\x9f\xbcqS\xd86\xd9A\xa8>\xcf\xde\xa6\x89J\x1e*\x1d\xd9KC\xaa\xb2\xbc\xda\xcb\xcfz\xec,\xb5%(9\xaf0Y]y\xdckR\xcbVu\xc05\xc5[\x07\xed\x11\xd3O\x13\xca\xce\xfa\xf7PU\x8f\x86\xe6\xb9\xdb\xcd^E$)5E\xb5\x037SM!d\xa5\x87\xcb)\xc1\xf3Kr\xaa\xe3%-"P\xbd\xd4\xa7\x97<\x9cVD\xd3\x9c\x12\xc4\x9a\xe8\xe4\xb3VN\x95\x97w`\xbbO\x06\xbd%\x05\x15dq\xb6\xde\xacm\x85\xccl\x07<\xd5\xe6\xb9\x03\x8cW>\xb6\xef\x1baI\xfa\xd6\x95\xbcr\x11\xcbf\xab\x9e\xc8\x92\xcb2\xb8\xe0\xe74\xd3\x1ejx\xed\xbb\x9as\xa8Q\x8a\xca\xf7w\x19\x97:qY\xb2\x82\t\x15\xb10\x075\x9f2no\xa56\xc0\xa2\x14\x9a\x08\xc5Y\xd9\x81PH\xb8\xe9J\xe3\x18$`\xc3\x06\xafE#2\x8a\xcf\xc7>\xf5r,\x85\xa1\xca\xc0ZBsVQ\xb8\xaa\xb1)&\xad*\xf1\x8aJ@?y\xf5\xa9T\x9cUbNqV\x13$b\xa8D\xa3$u\xa5\xd9\x9aDS\xda\xa7D=\xea\xd0\x88\x02\xe3\x83Wl"a0o\x99A\xef\xb8\x0f\xe7Q\xb2\x81\xdb?Ztjf\x7f\x9a\xe1c\x03\xb9\x18\xfe\x95Kq\xa3fHc7R;_[G\xb8\xe7\x18/\xf9\x8a\xd1\xb3\x96 \xab\xb2D\x9f\xd7\xca\x8f\x19\xfc++}\xaaH\xae\xda\x9b+\x15\x19\x11D\x1f=\xbd\ri\xdaM\x03\xc4J\xccI\xf5\x94l\xfc@\xc7\x15\xd7\x1dQ\xd7}\r\x84KMJ\xceKI3\xe579#i\x89\xc7BEr\xf3h\x92Gs2\x04\x08P\x80w\x1f\xba}+ZK\x99\xe2\xcb4\xc9*c\x87#\x18\xfe\x8c*\x0b\x99\x13\xc4\x1aR\\\x03\xe4_F\xa4\xba\xab\x7f\xacU\xcf\x1c\xf7\x18\xc8\xf6\xa9qM\xea\'\x15-\xcc{\xbd\x16\xee5\x0c"\x12\x009\xd8s\x8a\xcd\xfb4\x86FE\x8d\x9b\x1d@\x1c\xd7K\xa7\\\xa8\x89\xa6Yw\x07\x1dzdw\xado\xedh!\x928\xbc\xad\xa6PF\xed\xb8\x19\x14\x9d\x14\xf6!\xd29\x88t\x85(\x8ds:B\xad\x8f\x94\x8c\xb6?\xa5Z[m)\xa5h\x12G\x0c\xa7$\x9fJ\xd7\xb9\xb9\xb5\xbc@\xae\x8a$#*\xe3\x90M`^\xe8\xf6\xd77"\xe2\x13$2\xb9\xe4\x83\xc1`8\xc8\xf4<\x8f\xc2\xa9S\x8a\xe8Z\x82F\x89\xb4\xb0T\xc0t \xf7c\xd4\xd5;\x96Kr\xc4\x05\xce\x7f\x84\xe7\x02\xb1\'\x8e\xee\xd9QNdR\xd9`89\x1f\xe7\xf4\xab\x96\xb6\xca\xd0\xb4\x92\xb3\x97\xcf<\xf5\xaa\xd8\xb4\x8b\xf6:,\x9e W\x92F[x# <\xc7\x9c\xfb\x01^\x85\xa6ZY\xe9v\x1fe\xb1@#\\\x9c\x9eK\x9fS\xeb\\\x96\x8fp\x04km\x1a\xfe\xee0Tm=I\xef\x8f\xd6\xba\xb4d\x86\xdce\xb6\xa8^\xac\xd8\x03\xea:\xd4\xee)hp\x1e#\x0c\xda\x84\x92\xb29\xdc~\xfb\x9e\xbe\xc0t\xfc\xab\t\xab\xa7\xd6m\xc5\xed\xc3\xc8\x93\x99\t\xeb\xcf\x03\x1d\x80\xf4\xacW\xd2\xe5\x15\xc7R\x9c\xaft\x8eYFW3\xc1\xc1\xa9\xd1\x85+Y\xcb\x1f%I\xa6\x04+\xd4s\\\xee-nN\xa8\x9c6)\xe1\xbd\xea\x15\\\xd4\x81Oz\x81\xdc\x99Z\x9d\xd6\xa3QR\xa0\xcd&\x02\x81J\x17\x9a~\xcaP\xb5,\x04Q\x8aq4\xe0\xb4m\xac\xda\x19\x19\\\xd0\x16\xa5U\xcd<%C\xd0v+\x98\xf3L\xf2y\xab\xc13I\xe5\xf3\xd2\xa9Hv(\x18(\x10\xe3\xb5hy`R\x18\xe9\xf3\x07)K\xcb\xa7\x88\xfd\xaa\xcf\x97K\xb3\x8a\x97!\xf2\x9ey\x80)\xe1\xc0\x15\x0bds\xda\xa3G.\xdcf\xbd&\xcc\x8b\xf1\xbb\x93\xc7J\xb3\xb1\x9c\x0cf\xa0\x822q\xd6\xb5#\xc2\xaf=jy\xc6Q\x92\xd8*\x16j\xa6\xb0\x96~\x07\x15\xa7;\x16\x18\xa8UB\x9fzLW"[^9\xa6\xbc\x18\xab\xe3\x18\xcdD\xdf9\xc0\xe9R\x98\\\xaa"\xc8\xa9\xd6\x10\xa9\x929\xabQ\xc4\x00\xe9R\xf9;\xb8\xc5R\x9d\x80\xa4\x8e\xcay\xad\x1bi29\xc55mGB3V!\xb5\x11\xf2G\x14\xd5A\xd8\xb0\x00+\xedTn\xe2\x0c~Z}\xcb\x94\xfb\xa6\xa0\x87\xcc\x91\xb0Z\xad\xce\xe8DB\xcc\x1a\xb0\x96\xe1\x07\x02\xb4b\xb7!\x01"\x95\xa2\x18\xe9Y6\x05LazU+\x87\xe7\x00V\x94\x88@\xac\xd9\x97\x9c\xe2\x8b\x81E\xceE0B[\x9a\xb4#\xc9\xe6\xac\xc5m\x91J\xe0e5\xaf\xb5U\x9a\x02:\n\xe8^\xd8\xe3\x81U$\xb6,H\xc5+\x8c\xc1\xf2\xb0zU\xc8!,*\xdbY\xe0\xf4\xabV\xf6\xdd8\xac\xa71\x90\xc7o\xc7\x03\x9a\x98@@\xf7\xab\xf1[\xfbT\xc6\xdf\xda\x88\xc8V2<\x82[\xa5O\x1c\x04v\xad\x15\xb5\xa9\xd2\xd8zV\xeaB*\xc3n\x08\x04\x8a\x9c\xdb\xe1x\x15e#\xdaj}\x83mh\x98X\xc5\x91\n\xe6\xa9\xcb\x12\xb9;\x8b\x05\xf6\xad\x9b\x98\xfa\xe0Vi\xc0\x93,8\x1c\x9e(\xbd\xd8\xec]\xd3\xa0\x96\x18\xdakkug\x90\x81\xbf\x96$\x01\x8f\xce\xae\x16\xbc\x81Y\x9e\xe4*\x96?t\x8c\xaf\xd7\xa0\x1f\xadr3x\xaa\x7f=\xed\xe0\x0c\xb1\x17)\xb4r\xcd]6\x9f`\x97\x11$\xb3\xdfK\x00\xda\x01\xf3"\xe0\x1f\xc7\x91]\xd0\xd1X\xea[\x14u\x19\xf5\x07\x84\xa4s\x96\x8c\x9c\x02_\xbf\xa1\xf4\xac\x9b+\xdb\x8b{\xf1\x1c\x92\xb2\x05\xc8\xe7\xa3\x1fO\xe7[\xf7\xfa\xe6\x9d\xa4y\x90\x18\xcc\xc4\xfc\x8cH\x0c?\x00\x7f\xa9\xae\x0fT\xbf\x96\xe6W\x92\xcb\x12C\xb8\xe7\xcb\\4`t%z\xf7\xfaRe#\xb7\x1a\x85\xb5\xa4,\xb9 \xbe~R:s\xcf\x14\x7fm}\xa6Kg|\xefV-\xbc~\xa2\xb8\xc8d\x9em;{\x93\xe7[\x90\n\x83\xf7\x90\x9e?#\xfa\x1a\xd5\x86U\xb8\xd3\x97\x04\x06S\xf3m>\xbd1\xf9\x1a\xa4\xc3sY56\xf3&\xf2\xd4\xb7\xef\x0e\x14\x1fN\xe2\xac\x9dQ\x8b.\xe2T\'\xce9\xe0\x1e\x84W3\x05\xe3L\xd22\xf0\x00\xc0Q\xc1\xc7\xadl\xfd\x89ll\x15\xe7l\x19\x86[\x03\'\xe9E\xc6h[j6\xf7N\xe5\xc9\x89\x10n\xfc{\xff\x00\x9fz\xaf,\xad$\x88\x96\xe1\xf0\xdc\xc6\xc0g>\xdfN\xf5\x8a\xf6\xd2[\x8f)\x07\xca\xc7s63\xf8z\xe3\xf9\xd6\xb6\x8d\xa8\xc7ix\x9ed@\xc6O\\\x82W\xeb\xfe\x02\x93\x04w\xfa\x16\x8a-\xe1\x8eBr\x0f9>\xbd\xc9\xad}B\xd2O\xb2\x96\x88\x90;\x94 \xa9\xfa\x83U\xed5\x14\xb8\xb5V\x86h\xd4c\x03s\x02~\xa7\x1d\xe9\xdfn0I\xb4N\xd21\xeb\x81\xc7\xeb\xd6\x9a%\x9c\xac\xd3\xca%o7>`<\xe4u\xa6\xf9\xa1\xc5M\x7f\xfb\xdb\x83 \x01A\xea\xa3\xb1\xf6\xaa\xcb\x1eZ\xa8\x070\xc7\x05r=qUe\xb4IyA\xcd^Yv\xfc\xac\xb9\x15/\x97\x1b\r\xc9\xc5L\xa0\xa4\xac\xc4\xe2\x99\xcf\xb4\r\x13S\x95zV\xcc\xd6j\xeb\x9e\xf5E\xad\x8a6\ry\xb5\xa983\tB\xc5p\x9c\xd4\xc8\x94\xf1\x15J\x91s\xd2\xb9\xae\xc9\xb1\x18^\xd4\xa1*\xc0\x874\xbeV)\xd9\x8e\xcc\x80.)v\x93S\xf9Tl\xc5K\x1d\x88\x82\xd4\x80Q\x8c\x1ax\xac\x9b\x1a@\x17\x8a6\xd3\xb1O\xdb\xc5C(\x84\xad\x18\xa9v\xd1\x8a@G\xb2\x90\xadK\x8e)1\x9a\n<\xc2\xe8m\\\n[T\xc52f,\xe2\xae\xd9C\xbc\x0e+\xd3g)i:\x0c\n\xb2\x17\x8a\x96\xde\xd7$\x028\xab\xcdb\x15w\nJ#\xd4\xc5\xb8V\x185\x06\xe0*\xe6\xa0\xdbF\x00\xac\xb3\x92sJb.o\xca\x80\rK\x1af\xab\xc6\xb9\xc5_\x8a<u\x15\x9d\xc6O\x14g\x15m\x11GQQD=\xeal\xd4\xdch\x95a\x19\xcfj\x8ey\x02\xa9\xa6\xbd\xce\xc5 Vm\xdd\xe1\xc1\x0b\xd6\xa9j\r\x90\xcf6d\xc6kJ\xc22\xc01\xac\xabH\x1aY77L\xd7Cj\x9eX\xf6\xa6\xe5a"\xe2\x8c\'j\x81\xca\x83R4\xa0\x8e*\xab\xb6\xe3C\x9ac\x19 \xdd\xd2\xaa\xc9\x11=\xabB8\xeaV\xb7\x04t\xa5p\xb1\x8c\xb0\x10zU\xb8b8\xc6*\xc1\x83\xb59\x17a\x02\xae\xe2\x18 \xc8\xe4T2\xdb\x81\xce+@\x11\x8a\x8aQ\x9e*$\xca3\r\xbe\xe6\xe9V"\xb7\xc0\x1cU\xa4\x84\x13VR.\x95\x8b\x1aDP\xc0\x00\xe4T\x8d\x08\x1d\xaa\xd4i\x8a{\xa7\x14\xe2\r\x19\xe19\xc6*u\x88m\xa72\x81M\xdf\x83Z)\x08\x8c\xa6\x1a\x9e\x05.C\x1c\xd2\x9e;U\xa9\x01\x0c\xc9\x95\xack\xd8\xf2\xa5B\xe5\x8f\x00V\xf3)+X\x9a\xd4\xf2\xd9[\x19"\x00\x12pI\xad)k5r\xa2\xb58\x9dN\xce\xea\x16\xdf\x181>I,\x0e\x08\xfab\xae\xc9\xe2\x1b\xc94\xf8\xad\x8d\xd6\xf6(\x14\x9cc\xf4\xcf\x02\x9f<\xa2X\x99]\xb7;\x0e\xa0\xd5\x08t\xf2\x93\x89\x02\x02\x07<7#\xfc+\xd0:P\x9fd\x96I\x02\x92]\xcf\x0c\xdbI\xcf\xd3\xfc\xe2\xaf\xc5o\r\xb5\xdck\x12\x98\xa7^I\xc6\x7f\xfd}\xf8\xe8EKo<0\xdf\x86i%\x07\x82~}\xb9\xe3\xb8\xe9ZWZ\xdf\x86\xd1^\xce\xea\xfa\x00\xc7\x8c\xc6\xa5\xf0}\xf1\xd3\xebF\x88e-OE\x93K\x86\x0b\xc4eH\xa7b\x1d\x03q\xb5\x87 {g\x91\xe9T\x11\r\xb2\xdaJ\x8a\xdb.\x90)\x07\xb3z\xd6\x86\xb7\xacY]xD\xdb\xda]\xad\xd3B\xd9]\x8f\xb8\x81\xe8i\xd6v\xb0\xea\x9e\x17\xb7\x81\'A,x8\xeb\x86?\xfdj\x1bBF.\x94\x9bu\xf7\x8e\xe9\xd0\xc5\x16]\xcex\xc0\xf4\xf5\xae\xb0\xc55\xf5\x98\xbf\x9d\x82\xc0\xce\x040\x03\xc1\x19\xc6=\xb8\x15\xce7\x86\xee\x06\xa5s\x1a\xbbI\x14@\xf3\xbb\x1c\x1ep}\xeb\xb2\xf1\x11H\xfc;\xa5\xdaX\xba\xb3<\xc8\xa1Cc9\xfaP\x98\xc9[G\x96\x18#\xbc\xca\x8c)\xde\xcf\xd0\x03\xebY\xd7\x9ahB\x97hTE\x90\x00\x03\xb7\xe1\xd0WA\xaf\xdei\x10\xdbZ[j\x9a\x95\xbd\xa7\x92\xa0\xf9\x06p\xac\xc4\xf7#\xd3\xadY:}\xbc\xba8\x9a\xd7\xca\x92\'\x1f+F\xc0\xe4U0G9!\x9e9\xe1\xb4ILQJ\xa1\xbeU*q\xfd\x05z\x16\x9fal\xd6K\x19R\xc3\x1dI;\x81\xf5\xaeT\xd8\x16\x11+9\xdc\x80\xe3y$\x92}}k\xa6\xd0X\xc6v8\xdb\xcf4\x81\x99W\x10\x88\xa5u\x04\x90\x0f\x04\x9c\xe6\xa0\x00\x96\xce\xde\x95\xa7{\n\x9b\xc9\x90\x9ewv\xac\xe7GF\xd9\xebV!\xe9\x1ey sR\x08\x00\xe9D`*\xf3\xc5H\xc0\x05\x0e\xac~\x94\x00\xd0\x80\xfc\xa7\x8akZ\t>\xb56\xd0\xcb\x91\xd6\x84,XT\xca*J\xccM\\\xa3-\xaf\x95\xd6\x9a\xa9W\xee\x97\xe5\xc9\xac\xc7\xb9X\xfb\xd73\xc3\xc6\xe4r\x96\x15h**\x80\xbd\x19\xebO[\xc0OZ\xbfb\xac>R\xd3\x0cS@\xc8\xa6\xa3\xef\xe6\xa5\xe0\n\xf3\xeb\xd1kRZ!e\xe6\x95E9\xa8\x04W\x03\xd1\x88^\x94\xe0j"iU\xa8\xb8\x893\xc54\xf1K\x9e)\x8cj\x9a\x1d\xc7\nSLSJ^\x84\x87s\xcb\xd5<\xc6\xe9\xc5i\xdaf"\x05$0\x05\x18\xc5Lb8\xe0\xd7\xa2\xcesZ\xd6\\0=kFC\xba>\x95\xcf\xda\xc8\xd1\xb8\x07\x91[K.\xe8\xc5T]\xc6a\xeaJw\xe3\x15\x9c\x10\x93\xd2\xb6\xaf\xa3\xdc\xc2\xabGnGn*&\x89Ex\x94\xabsZP\xe0\xa8\xa6\xfd\x9c0\xf4\xa1\x11\xd1\xb0Ed\xe2\xca. \x1d\xa8\x90\x15\x1cS7\x15\x19\xc5C%\xc1\nr\r%\x16")\xe4\xda\x0eO5F42\xc9\xcei\xd29\x96N\x07\x15v\xd2/\x98qZ$"\xd5\x95\xb8\x00qZ\x0f\xfb\xb4\xa4\x85\x02\x8a\x98\xa6\xe1\x83Jp\xb9H\xa2e=\xa9\xa0\xb6\xec\xe2\xac\xb5\xa1\x07#\xa59a\xdb\xda\xb0\xb5\x9e\xa3\x08OL\xd5\xae\xa3\x8a\x81c \xe7\x15*\x1cV\xb1\x10\xe1\x19<\xd2\x182rju9\xa7\xe0V\xea(ES\x16:TL\x84\x1ej\xeb/\xa5@\xcbYN%\x0cL\n\xb5\x10\x07\xa5V\xd81\xc5M\x0bb\xb0\xb1H\xb5\xb7\x02\x98Oj\\\xe4f\xa3sI\xe86G+`U2Ij\xb6\xd1\x97\x14, u\xa9m\x93k\x91\xa0\xe3\x9a\x94.i\xc5@\x14+s\xc8\xaa\x8b*\xc2\x81\x81\xd2\xb9_\x15\xdc\xac6\xc4m]\xfdy\xeb][\xc9\xb22A\x03\x8a\xf3\x8f\x14\xde<\xcd\'\xces\x9cf\xbbh+\xbb\x8e+S\x9e\xfb{;n\xce\xd5\xce\t5\xa3\x16\xab\x1a\xc6\xa3p\x91\xc0\xc6\xce\x8a>\xbe\xb5\xcd13\x1f,\x1e\x9fz\xac%\xabF\xb9\\\x95#\x9e3]\x89\x9b\x1d\x9e\x99\xe1\xa9\xb5\xb0\xb2Kp\x80\x1c\xb0\x86>\xa7\x03\xa6k\x10\xe8\x84\xf8.\xe2\xf2\xca\x1d\xf7[\xe4I\x15FX:\xbf+\xeb\x9d\xbf\xa1\xa8\xb4\x1dr\xff\x00D\xbe\x8ex\xe7\xf3"\x07\xe6\x8c\x9c\x10+\xbb\x87OMfY\xb5o\x0b\xea\xc9\xa7_\\|\xf76s\xae`\x98\x8f\xe2#\xf8[\xdcTV\xa6\xe7\x14\xe1\xba4\x83[3\x8a\xd6\xac\xcah\xf6Z\x98\xb0\xb6\xd3\xf54\x8dD\x91[)U\x97\'\x85e\'\xae0*\xf8U\xb6\x97L\xd4m\xb2\xb0^&\xe6N\xca\xc3\xa8?\xca\xa7\xf1\x1f\x86|U\x042_\xea\xefh\xcc\xbcF\xcb\'\xca\xa4\xf1\x95@9lw=*\x0f\x0b\xa8\xd44\xbb{\x19:ZJ\xfeY\xef\xcf\xad\x14\xe3-\\\xde\xacRIi\x13\xbd\x11,\xd6L\xe0\x042.\xef\xbb\xe9\x92?*[\xdd!4\xb9t\xfd&\xdaP\xba\x9d\xdcMq\xf6\x822\xd1"\xe3q_~p?\x13[\x1a.\x90\xb0\xdb\xb0f\'\xaa\xe1\xab\x9f\xd6E\xdf\x88<P\xd2\xe9\xb7Q\xdb\xea:{*C\xbf\xb7\x1c\xe7\xb1S\x92\x085\xa4\xd3q|\xbb\x8e6\xbe\xa4>\x19K\x08\xbcU\xac\xe8\xb7^\x1eI\xc4q,\xb2^]7\x9b\xe6\x83\x8d\xa7\x9ery\xfc\xab\xa2\xf0\xfe\x97aq\x1e\xa9\xa6Y\xa7\xd9"\xb5\x9c\x84h_\xe5\x04\xf3\x8e\x7f\x97\xb5V{O\x1aI\x13%\xc4\x1a6\x9b\xbb\x89.\xe3r\xe4\x0fUS\xdf\xf3\xc5P\xbc\xd5\xa2\xf0\xd6\x90\xbaN\x8b\xe6M+\xb34\x97\x12.ZG=MgETn\xf3V\xfe\xb7\x1c\x94b\xac\x9d\xcbwQ\xdc\xc7)\x88\xdcy\x92\xc7\xc1=\xc8\xfc+oE\xb8.\x14:\x95q\xed\\\xe7\x83\xedn\xaf\x0c\x93\xdf6rz\xb7z\xec\xed-\x06\x1bh\x1ct\xf7\x15\xd2f\xd9WS\\]\xbc\x88\xa4\xe7\x1d\xea\x9c\x87p\x19\x035\xa5x\x01\xba*N2*\x9bB\x18\x9ezt8\xa0\n\xdb\xd1N\xdc\xe7\xebMf\xec8#\xbdXt\xc2\x03\xb4\x12*\xbb\x95\xdaN1L\x06\t\x9e\x17\x0c\xe3\x8a\xbb\tY\xb0\xe8pj\x08\xd5$B\xa4v\xefIn\xa6\x190:P\x01\xa9\x92\x96\xed\x93\xd0W\tu\xa9\x95\x95\x93w\x00\xd7\xa1_\xc0\'\xb4l\x1eq^C\xad$\x96\x97\xd2)\'\x19\xa4\xc4k\xae\xa5\xc7\xde\xabv\xf7\xbb\x8f&\xb9\x08\xae2z\xd6\x9d\xbd\xc7#\x9a\x9b\x94v\x96\xb7[\x88\x02\xb4\x94\xe5s\\\xd6\x9a\xe5\x88\xeb]\x14y\xd9\xcda]\xae]I\x90\xacj2\xd8\xa7\x91\xcdF\xc35\xe1Tw\x96\x86lv\xec\x8c\xd0\xa7\x9a`\x06\x9e\xa3\x06\x9cId\xa0\xe6\x9a\xd4\x80\xe2\x97\xad;\x8d!\x85\x88\xa8\xcb\x1e\xd5#-DA\xcd.q\xf2\x9c\x99m\x8c\x14\x8cb\x8f=\x015\x9d4\xf3D\xdb\x99IZ#\xb9\x8ea\xf2\x9c\x1fJ\xefoS\x9c\xbd\xf6\x94\x0e1Z0\\\x12\xa0\xd6\x1cq\x96\x98\x03\xf8V\xcc\x10\xb6\xd0*\xe2&\xc4\x9al\x91\x9e\xb4\xf5pE\x13\xdb\x9cg\x15P\xb3#b\x94\x98#A\x1c\x02E?\x82r+/\xcfl\xe3\x06\xac\xa5\xc1\x00qII\x01t\xf3\xd2\xabJ\x99\xa8\xcd\xe6\xdc\x8aD\x98Hz\x1ehsC\xb0\xe8\xad\xf2s\x8a\xbf\x0c>X\xc9\x1d)`@\x14\x1cQ4\xb9\xf9W\xf4\xa8sC\xb0\xf8\xe7\xf9\xf0kF\x02\xa5y\x15\x93\x14l\xcc+J\x10T`\xd3\x84\xef\xb8\x17\xd0F@\xc8\xa1\xa2LdUm\xc4s\xe9OY\x83\x0cf\xa6ve&#\xa8\x00\xd4\x05\x86x\xabD\x06\xa65\xba\x93\x91\xc1\xf5\xa8I\xa1\xda\xe2GSs\xe9LE\xdb\xefR\xe0\x9a\xd96M\x88\x98\x91\xda\xa0c\x9a\x9eC\x8ej\xabH7c5-\x80\xaa\x08\xa7\xab\xe0\xd2\tF)\xa7\x9eEE\xc6Y\xdc\x08\xe2\x9a\x06O5\x1cd\x93S\x81\x93CI\x80\xeczR\x1c\x8a\x95\x17"\x91\xd7\x14\x9cJD\x1c\xd3\t\xc59\x98\x0e;\xd4\x0c\xc4\xf4\xac\x9b\xb1v#\xbe\x9cGl\xc4c\xa5y\x86\xba\xe6P\xfb[\x8c\xf2k\xd0u\xa9\x16\x1d=\xc9 \x9cW\x9c\xdefT`q\x9fJ\xf40\xba\xc1\xb1\xc5\x19\xd6\xdff\x8c,`\xa9\x94\xf5>\xb5\xa0.\xedc\x90+\xc9\xd7\xae\xd1\x81\xf8\xd6+\xc4\xab\xec\xfd\xf3\xc0\x14\xf8 \x9aYN\xc4\xdf\x81\x93\xb7\x90\x07\xaf\xb5t\xdc\xb2\xcd\xcc\x96\xce\xe5\xe3r9\xc1\x01j\xde\x97}-\xb3\x7f\xa2H]8\xc8s\x82~\xa35\xa7a\xa6I\xbd"\xb9\x8e\xda\x14\x90pf;\x7f#Zv\x9e\x1b\x8a\xd2\xe6i\xad\xcd\xa4\xd2\x84%vJN\xcf|b\xaa,\x0bw\xda\xa4\xf7Ztp\xdf\xb4\x8c\xeb\x86]\xfd\x14{\x0e\xf5G\xc3\xa8-fdC\xb1K\x87a\x9eFy\xe9Ioi=\xb3;\xdd\xc9\xf2\xa2\x86RT\x95\xc9\xf7\xe9L\xb9\xd3u];PMkM\xb1\x97P\xb2T\x0f\x88NHa\xd7#\xd2\xad\x94\x8f_\xb6\xbaH\xb4\xd0yi\x02\xf1\x81\xd6\xb9&E\xb4\xf14\x9a\x96\xd6\xc4\xe0o\xdb\xfd\xec\x7fZ\xa1g\xe3\x0b\xad]`\x1aE\xab\xde]K\xb7\x16\xc9\x0e\x0eNrI\xce\x17\x80s\x9fJ\xec[F\x9e\xda\xc5\xbe\xdc\xd1G5\xc4\x99\xf2\xc7\xce1\xc7\xcaq\xdf\xdcR\x8bL\xa9+\x1cg\x89//\xae.w4\x8e\xe8A(\xc5OO\xadE\xa0[[\xde\x06\x96k\xc8Qz32\x9d\xe7\xdb\x9a\xecm\xb4cx\xb2\xdbI\x10x$\x04\x1f1pUzw\xa7\xcb\xa3Zh\xb6{-\x84j\xee8}\xb5d6;O\xd5\xad#\tmm\x1b$+\xc6\xee\x08?_z\xda\x82\xe0\xccF\xd1\x85\x1c\xf1\xcdq\x10\xd9]\x99\xf7\xc9:\xabg\x80HP\xc3\xf2\x1c\xd7W\xa5E0x\xc3\xa9\n\x07\x07\xd6\x82K\xb7\xeb\x89\x15\xb6\xe4\x1f\xe2\x15I\xfec\x9f\xe1\xec3Z\xda\x94k\xe5\xa9\xce\x00\xac\xa1\x16\x14\x90\xd9\xcf"\x98\xd1\x0b\xc6\xe4p2\xb5P\x8d\xb9\xe3\x8fCZ[\x9c\xa8V^ET\xbb\x88\xbc%\xbb\x8fJ\x06@\xb2\x8f\xf0\xa9\x90\x17\xc6:\xd6<\x17j\xaeb\x90w\xebZ\xf6\x98r\x1850-\xa1\xfd\xd9R9\xc5y\x97\x8d\xad\xc0\xb8,\x05z\x80\xda\xd9\xae\'\xc5\xd6\x8b0>\xb4\x84y\x9c\x1b\xb7V\xf6\x9dl\xd2\xb2\xe6\xa9Gfc\x9b\x91\xc6k\xa3\xd3Sn8\xa8\xb0\xcd\xed6\xc9cQ\x91Z\xe0\x000*\xa5\xa0;A\xab\xa0b\xb91+B\x18\xcd\xb5\x1b-NF\x051\xb1^K\x8e\xa4\xb2\x00\xb5 L\x8a1\xcdJ\x82\x98\x88\x82sO\tO<\x1aa5\x0c\xa45\xc0\xc5C\xb6\xa7$\x11M+PY\xc7I\x1a>T\x8a\xe7o\xac\xe4\xb5\x98\xc9\x0ev\x93\xd2\xb7%\x91\xd0\xe7\x19\x15\x08\xfd\xff\x00\x04d\x1a\xf4\xef\xa9\xc4U\xd3\xaf\x96GU\x93\x83\xef]U\xb1\x05\x01\x185\xcc\xc9\xa5\x10w\'\x06\xac\xda]\\Z\x90\x18\x12\xa3\xadk\x15\xd8\x0e\x82|\x90\x05P\x96\xdbwJ\xb6\x97\x0bq\x10e\xeb\xde\x91\x88\xa9\x92\xb8\x19\xa6\x17S\xd2\x946\xd1\xf3u\xabs8\xc5Q\xf9\xa6\x94(\xac\xda\xb0\xd92[y\xa7$\xf5\xab\xb6\xf6\x9b\x1c\x02*\xd5\xa5\xa8\xd8\t\x15mcP\xea)J:\r\x11\xb4ac\xc0\x1c\xd5a\x03\x03\x92+j\x1bC!\xce*g\xd3F\xdek?e&Q\x91\x06\xdd\xd5\xa3\x1a\xab\x0e\x95U\xed\x96"H\xa6\xc7r\x03\x15\xefDU\xb4b\xbd\x8b\xc64\x19\xaa\xf2F\xa82)\x1a|\xf1\x9a\x86ir\xbc\x1alw\x1c\xb7@\x1cf\xac\xa4\x80\x8e\xb5\x8a\xcaKU\x88\x03\x8e\xe7\x14\x93\xb8\xe3+\x1b)\x83R`\x01U &\xac7J\xd5\x03w \xb8#mfI\xb8\x9c\x8e+FU\'5X\xa78\xc5D\xd8\xacW\x8c6y\xab@qJ\x91{S\xc4x\xac\x93\x1d\x84\x19\x03\x8a\x999\xa6m\xf5\x14\xe4\xe0\xd6\x88\x92pqQK6)\x19\xca\xd4\x0c\xc3\'5C\x11\xf2\xe75\x11\xcdH\x18t\xa7u\xe9X8\x1aE\x9c\xf7\x89e\xd9c\x8c\xf2\xc7\x9fZ\xe1|\xc2$PS\x05\x8f\x19\xeb\x8a\xeb\xfc`\xe28S\xae\xe2\xd8\xcdq)#,\xe5\x94s\x8e\xbd\xf3\xed^\x8e\x19Z\x99h\xb56\x97\x14\x8e\x0c\xa4\x96\xeaQq\x9fa\x9e\xd5ZK\xfb\xa86\xdb\xd9\xc6Q3\x81\xe5/$\xfbw\'\xdc\xf3W\xed\xcc\x86]\xb1\xae\xec\x828\xee{\xf3\xfex\x15l,^YH\xc8,\xe3\r(\x1c\xb0\xee\x17\xd0~\xa6\xb7\xb8\xcc\x05\xba\xf2e"\xe5\x8c\xd3\x16\x19\x8e3\x91\x9fB\xdd\xfe\x83\'5\xdf\xe9w\x91\xdaF\x16\xee\xe5\xa1\xb8*3i\x06\x0c\x83=7\xb7\xf0\xff\x00\xbb\xc9\xf5\xc5`\xddi\x07E\xb7\x12[ \xfe\xd3\x90cv3\xf6e#\xa2\xff\x00\xb6GS\xd8V.\x8fm=\x95\xcc\xba\x85\xc6\xf3\r\xbf\xcf \x04\x83+\x13\xc2\x03\xeaMN\xcc\xa3\xb9\x95\xa6v-\x1cw\xac\x8e8y$\xc2\xe3=@<\x9c\xf4\xcf\xb5t\x1e\x1d\xd5\x8e\x9d\x98v\x9f/$\x8c\x91\x8a\xe2\xa5\xd5dy\xe0k\xc1\xbeI$\x01\xb6\xf4^\xa0\xe3\xdb\xe5 \x0e\xc3\xebVm\xe3\x86\xe0\t\x0c\xa4\x01\xc6\x18\xe3\xbf\xa59N\xccGK\xe1\x994+\x0f\x1a\xeb\x17\xdaZ\x85\x9e\xe3o\xca_\xe5L\xf2\xfbG\xbbWg{y\xf6\xb2>Ws\xd7\x11\xc82?\n\xf3\xa8\xec\xad\xa5]\xe3b\xb2\xf3\x90y\xfaU\xbbk\xb8Zs\x0cj\xcf\xb6\x07p\xce\xdd\xc7A\xed\x9a\xb5+\xb1&z\x16\x9b,P\x8cL\xd71\r\xbc4\xec0\xde\xf9\xf5\xae{W\x92\xe6\xf2\xf9\xe4\xb3\x9cI\x83\xb7\xca<\x11\xd7*{\x1e=}\xaa\xac\x0fq\xa8\x06\xd3\xee\xaeY\xed\xeeW\x82\xbc\x18\x9c}\xd6\x1e\xc7\x8e+wK\xd3\x841\xed\x94\x034c\x01\xff\x00\xbdZ\x0c\xcf\xd3\xb4\xeb\x87\x89\x0c\xd9OE#\xf45\xd2\xd9 \x88\xaa \xf9:\xfd\rC\x1c\x8a\xff\x00\xb8\xe8\x1b\xa1?\xc2kF\xce\x12\x91\xb3\xbf^\xe2\x816A|\xbelR.1\x8e}\xab\x16+\x9f\xdeyN\x08#\xa0>\x95\xae\xe7s\xb8F$u>\xa2\xb9\xe9[\xce\x91\x94p\xc8r\x1b\x1d*\x90\xd1\xa2\xd9o\x987\xb1\x15\x0e\xe0\xc4\xa9\x06\x88\x19\xca\xed~\xbe\xa2\x95\x91\xd1\xf7`\x1f\xa5\x039\xfdB\xd3\xca\xb82\x81\xf2\x1a\xb3\xa7HA\x0b\xd5j\xf5\xe2\x19\xa1`T}*\xa5\x95\xb1\x85\xbd=\xa9\x81\xb0\xa0\x01\xedX\x1a\xf5\x90\x9a&#\xadt+\x8d\x9c\xd6F\xa8\xd8\x8d\xb1HG\x9b\xc9\x1f\x959V\x1c\xe6\xb5l\x00$Tw\xb1+\xcaX\x0ej\xc5\x84g#\x8aLgGh>AV\xb0sU\xad\x86\x10U\xac\xd7\x15y#90a\xc5B\xd51\xe4Tl\xa7\xb5y\x93\x15\x88\xc7ZxlSJ\x903I\xba\xb1\xb8\x03\xb53y4\xafU\xdeLq@\x99!\x97o\xd6\x85\x90\xb7Z\xac\x01c\x9a\x9e55I\r6a\r4\x94\xcf\xadW\xfb\x14\x91K\xf2\xc6H\xf6\xadK\x0b\xd8\xa4\x90\xc6\xf8\xfck_\xec\xf1\x91\x95\xef^\x8a\xa7\xd5\x19F*G+*\xb8\xfe\x06\xfc\xaa8\xca\xf9\x81\\p}Eu\x7fe\x19\xe8\x0f\xe1I&\x99\x0c\xbc\x95\x19\xaaQ\x94]\xd1N\x93kC\x15,J\x1f2\x1eA\xec*;\x98\xd8.\xf5\x04\x11\xd4V\xb1\x88\xd8w\xdd\x1f\xa5X\x8a\x1b{\xd4\xdd\x1e3[\xa4\xa6\xbc\xccZh\xe4\x1a\xe0\xbf\xca8=\xc5i\xe9vd\xbe\xf6\x15\xa1s\xa2\x0f3z\xae\x18~\xb5n\xd2 \x8b\x8c`\x8e\xb5\x83\x83OP&X\x95\x138\xe7\x14\x91D\x0bn=i\xd2N\xa0b\xab=\xf2\xc7I\xb43n\xdd\x95GJ\x92Y\xd0)\xe4\n\xe7\x93T$\xe1jF\x9c\xc894\x9dT\xb4E&:\xf6t\x19\xda\xd5\x8c\xd2\x1d\xe4\xe7\x15b\xe8q\xefY\xec\xc785\xcf6\xdb\xb9,\xb3\xf6\x96\xc8\x02\xa7\r\x95\xe6\xa8\xa7QW\xe1L\xe2\xb2m\x94\x87\xaa\x87=*\xec0\xf1\x8d\xb4\x90\xc0\t\x07\x15y0\xbd\xabJW[\x8c\x89\x10\xa9\xe9R\xb7LT\x87\x06\x80\xa2\xb7`@P\x9a\x89\xa19\xce*\xe1\xe0\xf1@\x1b\xab\x19D\xa4WX\xf0(\x11\x9a\xb4#\x06\x94\xc7\x8e\xd5*6\x1b*\x94\xc7j\x8d\x86\x0ej\xdb/\x15]\xd7<\n\xb2Y]\xda\xa0bjI\x01\x07\x14\xccb\x8b\xd8V\x10S\xf3\xb7\x18\xa4\x03\'\x8aG=is!\xd8\xe2|o+4\x90(\xfb\xa0\xe4\n\xe3\xfc\xcd\xb3\x17\xf4\x19\xcf\xa5t\xde5\x94y\xb1F\x0eI9\xae^%\xde\xa1x%\xce\xdex\x1c\xd7\xa1G\xe0F\xd1\xd8\xbfe1E\x11\x93\xf3J\xbb\xdc\x93\xd1{(\xfa\xf5?\x85mi\x81cv\xbd#r\xc3\xd0c9n\xdf\x97_\xca\xb9\x0b\xb9\x1c\xde\xc8\xc0c/\x8e8\xe0\x7f\xfa\x85v:1F\xd3\xa3\x81\xc6Nr\xc4\x7f/\xe5V\xc6K\xf6\xa7\xb8\xdf#\x8ev\x93\xcfrz\x9a\xbb>\x9f\x13\xdb\xd8X\x80\x01}\xb7R\x9e\xfc\x92\x14~\x00\x13\xf8\xd4\x92X!\x8b\x08>g\xc0\x18\xad_\xb0\x83\xac\xc8\xe0\xf4E\x88}\x15v\xff\x00J\x94;\x94\x06\x8d\x14\x8a\xa4G\x80%Px\xe9\x84\x1f\xd4\xb1\xfcj\xdc\x9aB\x03\xe6\x05\x00\x15\r\xc7\xbf_\xd6\xb6\xa3\x89U\x0f\xf7_\x19\xf68\xe0\xd3\x11]\xf7DzrG\xb57\x11\\\xa3e`\xa5\xf2\x06\x08`\x01\xad{\x1d\x1a\x02\xf2\xce\x10\x00\xccA\xe3\xa8\x19\xc5En6\x84\x0b\xd8\xe4\x9f\xd2\xba\x08\xc7\x95`\xa7\xa1.\x14}*\xa0\x81\xb2\xbe\x9db\xb1\xa2\xc8\x078P}\xb0+x\xdb\x99\x15e\x03\xaa\xe1\x87\xf3\xff\x00\x1a\xa5f\xbb\x0e\xcf\xe1\xce?A[P\x9d\xb1\xf1\xd8\xd6\xa8W*\xdb\xd8\xaeU\x9b;\x97\xf8\xbdE[\xb9\x01-\x9bo;G+\x9cf\x9f\xbdU[\x1dTg\x1e\xd5\x87y}\x99\x86\xc688\x1b\xb3\x90}3M \x12\xde\xe3~\xfd\xd9\xc7@OZ\xe5\xee\x96K}U\x99X\x80y\xeb\x8a\xe8\xd2\x06\x8aBF\x02\xb79\x1c\x8a\xc7\xd4\x10\xbd\xd6\xd6L\x81\xed\xd2\xa9\x14\x8bv\xd3\x92\x17~\xdfb*s>e1\x13\xf8U8#\xf2\xa4\\\xed\x00\xf6\xa4\xb9\xdd\x15\xd2dd\x1e\x84\xd02\xd4\xe9\xb0\x07S\xd7\xb5$*\x18\x82\x05:\xe0\xe6\x10z\xd3\xa0M\xa9\xbb\xd6\x812F\x05N\x07J\xc2\xd5\\\xa8oJ\xdal\xf2s\xc5c\xea\x804M@#\x8f\x94f\xe0\x90j\xfd\x9e\x01\x1cUO/3\x11\xefZ0A\x80\re9XM\x9a\xb0\xbf\xcbV\x14\x8e\xf5J\x11\x81V\x93\x83\x9a\xf3+KS2\xca\xad8\xa0\xa4G\x14\xfc\x8a\xe6(\x82E\xe2\xab\xb0\xabR\x10\x05Sv\xc6i8\x92\xdd\x88\xe4p\xa3\xde\xab\x81\x93\xcd+1f\xf6\xa0\n\x8b\x12\xd9*\x81N\xdc\x05E\x92\x05\x00\x93Lw9\xe6m\xb7YU\x1e\xf8\xadX5\x03\x80\xa40\xf7\xaez\xd6\xfd%bx\xcf\xbd\\K\x90Oj\xed\xa7)&e\xe8m5\xf4\xa0nSN\x87W\xe7\x12q\x8a\xcc_1\x87\xcb\xcdV\x9ag\x8f;\x935\xd2\xe48\xceI\x9b\x97wKp\xa0)\xcdCc\xe6\xda\xdc\xee\x8c\xfc\xbe\x95CK\x06y\x08<\n\xd9\xb8\xff\x00G\xb7\xdc:\x8ek8\xcb\xde7\x95>xs\x9b\xf0\xc9\x1d\xcar\x00n\xe2\xa9\xea6\xaf\x14e\xe3\x1f\x95`E\xad\x05 \x83\x86\x15\xadg\xe2;{\x83\xf6{\x82\x01o^\x86\xba\x1c\xa35fs]3\x12K\xcc\xb6\t\xe6\x98\xd9\x93\xd6\x97\\\xb4[[\xbf\xb4Fs\x13\x9e}\x8d$\x0c\xf2\xa6\x14\n\xe1\x9d\xd3i\x89\r\x05\xa38\x02\xb4\xadQ\xe4Q\xda\xab%\x93\x97\x0c\xc0\xd6\xdd\x8c\x01\x00\xa9\x856\xde\xa5"\xab\xd8<\x84rj\x03\xa5\x06nrk\xa2\xda\xbd8\xa7*\xc6\x0fj\xddRC9\xe1\xa5\x01\xfc&\x9c\xb6\xc2#\xc1\xad\xb9\xe6\x89W\xa8\xac[\x9b\x95\x0f\xc61QR\x9cP\x16"|T\xbeb\xf75\x94.\xbd\r*\xce\xd2\x1c\x1a\xc90\xb9\xae\x92\x02x\xa9\xd4\x129\xaa\x10\x1e\x95\xa0\x9c\xadj\x98\xd0\xc7`\xb4\xd8\xe4\xcbu\x18\xa5\x9a\x1d\xc3\x83\xcd@\x91\x15\xf65\x9c\xdb\x1a4\x17\x00P\xcd\x81P#\xf65+\x11\x8aW.\xc3\x1a\xa0\x93\xda\xa6\x03q\xa7\x98A\x15\x9c\xa7a\xa8\xdc\xccd\xc9\xcd7\xcb\xad6\xb6\x04TF\x0cV\\\xf7\xd8|\x963\xcab\xa2\x90dsZRD\x00\x18\xa8\x0c\\\xe7\x1c\xd1\x1b\x8a\xc7\x95\xf8\xd0\xb0\xd4\xa3\xe3\x82;\xd6\x15\x8c\x83\xed\n1\xf7\x18\x1f\xa9\xcdt~:\xf95(\x93\xb9\x1dk\x909^\x15\xb0\x0fS\xd3\xf2\xaf^\x83\xf7\x11Q\xd8\xb9x\x02^\xcav\x93\xf3\xb7_\\\xff\x00*\xe8|=24\x11\x8c\xfc\xccN3\xdf\xa6+"\xde3{)iF\x19\xbe\xf0\xabP\xabX]+\x85\xc9RN{\x0c\x8a\xda\xd7\x1b;\xb8\x08\xf3\xe1\\\xff\x00\x1a\xd6\xb3(3\xa9\x1fx\xcaI?C\\\r\xa6\xad-\xc4\xe8\xe0\x15\xf9\x94\x80{\x0c\xd7yk(\x9ah\x9f\x82\x1f\x04~<\x9aJ$\x96\xeep\x8c#\x1d\x18\xf6\xa9\x1d6[\xb3c\xe7a\x81D\x8a&\x95}W\x83\xfd*\xce\xdf1\x95}:\xd54\x04v\xd1oT=\xabI\xe4$\xc7\x060\x00\xdd\xf8\xd4\x16i\x85)\xc0\xda\xd5h\x00\xf7\r\x8e\xd4\xd2\xd0\x0b\xb6\x98\x05\x94\xf5\xebZf\xe28W\xe6`\x00\x1c\xe7\xb5b\x9b\xa5\x85\xd5\xd8\xed\x07\xff\x00\xad\\\xf6\xb9\xe2t\x82\xfb\xcb\x1f<2\xa0\xc9\x1f\xc2\xc1\xbf\xc0~\xb5i\x01\xd1j:\x8f\x95rV)B\xc8p\xc8{}\x0f\xa85\x9d\x01\xfbX2\xc8\x866\x1fyG8\xe7\x19\x15\x8fe\x1c\x97\x12\xc6\x0b\x16\x89A\xd8I\xc9\xc7b?\x0e\xd5\xd0\xa3(\\\x80\x04\xb8\xe5s\xf7\xbe\x9f\xe1TU\x89K2\xc6\xb1\x96\x07h\xe9\xeb\xf4\xaa2b{\x90\xa1\x80#\xb1\xebU\xcd\xe3\x00\xe1\x01\x1e\xaa\xdd\xbe\x9e\x95b\xdeA\x1ca\x9f\x92{\xe34\xc0{\x00\xc7\xcb \xfc\xbc\x82jF\x83\xed\x08\x84t\x06\x98\xaf\xba`F@\xed\xde\xaf\xc6@\x8f\x00\xe4\xd02\xab\x8c\x90\xbcS\xe4\xfd\xdcC\x1c\x83\xfaR7\xdeg=\xaa8\xaeVBca\x8aB$\xc80\x9fZ\xc3\xd4O\xc8\xc0V\xc5\xca\x14L\x83\xc5b]\xb0e"\x81\x9c\xea\xf3)\xad8\x0e\x14U_ \tI\xab\xd1\xa6\x16\xb8\xb1\x12\xd0\x99\x13\xa3b\xad\xc2\xa6A\xedY\xcc\xc4V\x8d\x8beEy\xa9\xf3J\xcc\xce\xe5\xd4\xb7\x04Tr\xc0\xe8r\x06Ei@\x9f-:`6\xf23]>\xc1X\xa3\x9f\x96CU%~1\xebWoc\x08\xf9\x1d+>R\t\x02\xb9\'x\xbb36*\xaeE?n\xd1K\x08\x04\n|\x83\x8a\x8b\x8e\xc5f8\xa4V4\xf2\x99\xa6\x85 \xd4M\x95\x15\xa9\xcc\xd9h\xcbs\x16\xf8\xdco\x1d\xbdi\xff\x00`h\xdc\x06b\xbe\xb4\xb62\xc9gq\x8e\xd9\xe4V\x85\xea\x99W\xce\x8d\xb0q^\xcc\xd2Q\xbd\x8erK;\xb8\xed\x93\xc9tWv8\x0e;T\xf7V{\x80\xc2\xabg\x9c\x8e\xf5\x9fogt\xca\xac\x02\x1ew/\xf8RI\xabN\x97g|E\x02\xf1\xf2\xd74m\'\xcc=\x91\xa1gn\xd08%0*\xdd\xd8Y\xe3\xdb\x9cqUSS\x8ax\xb3\xb8~uZmC`;N\xe0=)\xcaI\x1bF\xabQ\xe52\xee\xac\x1a2J\xbfz\xa0\xd6\xb33\x7f\xac<zV\x94\xb7m09R?\n\xac\xb7H\xb2\x80\xc3\x83E\xd3G;-Cqq<k\x05\xc6[\x1cd\x9e\xb5\xb1`\x8b\x16\x01\xc7\x15\x9e\xea\x9eW\x98\xb8\xc8\x19\x18\xaa\x92j\xe1c\xc6p\xc2\xb3s|\xda\x9a\xa8\xa7\x1b\xf5:\xff\x00\xb4B\x16\x95n8\xf9\x08\x15\xc0\x8drV|d\xe3\xd6\xb6\xb4\xfb\xd4r\x19\xa4\xe4\xf6\xcdn\xa6\x9e\xc6w:\x07\xba\x90\x1c\x02~\xb4\xdf\xed\x02\xa3\xe6\xcei\x88\x12@\x08n\xbd\xa9Z\x15\x1c\x9c\x1a\xa6\x9fA\x90\xb5\xf3J\xdbv\xf5\xa4\xfb>\xfeM2H\x88l\xa8\xab\x10\xb7\xcb\x83X\xb9\'\xa4\x87fS\x92\xdc\xad"6\x0e\x05[\x94\xa9\xe9U\x9a\x06?2~"\xb3j\xdb\x01~\t1\x8c\xd6\x9cn\n\x83\x9a\xc3\x81\xf0p\xddkJ\x16\xc0\xebDYI\x9a\x03\x07\x14\xd7\x03\x1cT\x02R\x0e)\xad?8\xaa\x94\x93\x1a\x1d\x9c\x1a\x912z\xd4Hw\x9c\xd5\x88\xc6\x05b\xd34\x88\xf4\x8c\x13Vv\x80*\x05l\x1a\x9485\xc9V\xe8\xe8\x82B\x81\x9a\x8d\xd7\x9a\x9f\xe5\xdbU\xa5\x93\x06\x957\xa8Ob\xbb\x8c\xbe)$_\xdd\x90;\xd3\x89\xcb{\xd3\xc8\x1by\x1c\xd7RF\x07\x8f\xf8\xf2)WU\x1308e\xc0\'\xb0\xf6\xaeN\x00N7\x8c\x00z\x9a\xef\xfe$\x062\xc1\x95\xe0\x1c\xe6\xb8\xcby#\x88\x83:\x9c\x8e\x85OJ\xf50\xfa\xc1\r\x12\xdb\xc8a\x98|\xc4\x0c\x8c\xf1\xd6\xb5\xfc\xc8u\x08\xc8\x8d\xb9<m\x1d\x87\xbdc;\xc5|\xdeU\x98gnw\xbe\x0e\x14}i!/\xa6\x1e\x18\xb0\xed\x8e\xff\x00_\xf0\xad\xc6t\x16\xfaL\xc1L\xdb\xb8n\x9fA\xdcV\xd6\x95\xa9\xcdi\xb6)\x94\xed\x8e\\\xf3\xd9\x08\xff\x00\x1f\xe7YZN\xacf_\xde\x9d\xaa@E\xdcy9\xe8+\xa1\x8d"\xbc]\xe10\x1d\xc0\xf7\xdb\xdf\xfc\xfbU!\x1bv\xba\xac/|\x06p\x1dA9\xf7\x04\xe3\xf0\xc0\xfc\xebj\x15R$x\xceW f\xb9\xa5\xd1\xa3i\xcbG\xc6X(\xc5uzm\x9e\xcb}\xa0\xe7\x1c\x93\xebTM\x86\xc6\x0cnY\xc7\'\xaf\xd6\xa4\x8a`\xad&\xfc\x02\xdd\xea\xcc\xd1\x0e\xa3\x9c\x8c\xd6f\xa8Z-<\xcc\x8b\x92\xbdG\xd6\x8b\x0c\xa9\xaej1\x98\xde\x049(F\xf2;dpk\x99\xb2\xd2\xa6\xb8\x11\xb5\xc0,y\xcf=F{U\xd8 7\x0e\x92J6\xb3\x91\x9e\xfd\xb1\xcf\xe9Z\x96\xe8@X\xc0\xc7\x07\xf3\xa0\xa5\xa1v\xc6\x04\x8e\xd5$W8\x8f\xa3\'a\xee=\xa9\xb7\x17\r4\x868\xe4\x11\xb6r\x08\x1c\x1f\xa5V\xba\x92\xe6\xd2\xc3\xcd\x80\t\x19\x89\x0c\x14\xe3?\xfdz\x83J\x9e6^\xa49\xfb\xc9!\xc1\x14\xd0\x13,\xf2y\xa5\x98\x87e\xe1\x89\x18\xab+w!!\x14\x06B1\x801Y\xf7C2\x9d\xbb\xd5\xb9\x19Q\xc0\xfa\x8f\xea*\xdd\x94%N\x1b$\xe39\x07\x8a\xa0.\xcf:\xe9\xf6\xc2g\x03\x9e\xde\x95gN\x9d\xa4\xb632\xfd\xee\x95\x97un\xf7r\x84\x91\xb2\x83\xb5m\xda\xa0\xb7\xb2X\x9b\x04\x9a@F\xf2\x05F\xdd\xde\xaaDD\xd2e)\xd7L$\x7f/\xd2\x88\x9a8\x9f\xe5"\x80\x1dy1\x8e\x02\x0f\xa5s\x97\x13\x8c\x9c\x1a\xe85\x08\x8c\xf0\x16N\xa0W+:\x9d\xc4\x11\x82\rL\xe5d2hHa\xcfz\x99\x9c"\xd5(K)\xc6j\xcc\x8aX\x0fZ\xf2k\xd4\xbbf2\x96\xa2\x06\xdd\xf5\xab\x96\xf3\x18N{U8\xd4\xe7\x03\x93O+"\xe4\xe2\xb8T\xac\xee+3\xa7\xb3\xbeIc\xc7z\x96IA\x15\xcc\xc13/#\x8a\x99\xaf\x9f\x07\x9a\xee\x8d\x7fwP\xe6\'\xbe|\x9c\x03Y.\xdf74\xb2]\x13\x92MTi\xb3\\\xb5\x1f3\xb8\xaeh\xc2\xf5#6N*\xa5\xbb\xe5jd`[\x9aJ%\\\x98\x0c\n\x8d\xdbmJX\x01PK\xcfJ\x1cn\x86s\xa7\xcc\xe2Ln\x07\xadY\xcbK\x08\x898f\xabQK\x04\xf1\x81\x16\x0b\x86\xfc*\xdc\x16\xeb5\xd1\x9b`\n\x060\xb5\xe9s6\xf9\x0et\x8c\x99\x1a[,\x05fQ\x8c\x1fJtpf\x13)!\x89\xe7\x9a\xde\xb8\xd3\x92\xe6\x12\xa3\xafc\\\xb4\xeds\xa7\xcc`\x90\x1d\x87\xa1\xa9\xa9K\x93U\xb0\xfdJw8i\xb0\xa3i\xf5\x15~\xca\x0e\x07\x98A\xf7\xa4\x8e\xdc8\xde\xdc\xe6\xa6\x86\xceWpC\x10\x9e\x95\x82M\xbb\x8e\xc6\xb5\xbe\x9d\x14\xeb\xf2\xe0\x8a\xcf\xbd\xd1\x8cs\xa9T\xf9\t\xe9]&\x91\n\xc1\x1f"\x99\xa9L\x8d\xb9Wis\xc2\x8e\xf5\xbc\xd2P\x0b\x18\'N\x97\xc9!\x07j\xc0\xbf\xd2g9\xda\xa4\x10z\x9a\xefc(\x91(y\x06\xee\xfe\xf4H\x90\xc8\x98\xe0\xd1\nI\xc7P\xf4<\xb9-\'\x89\xf6\xb8\xfd+r\xc7O\x92P1\x91\xef[\xf7\x16\x113\x1c--\x96#gR>\xe1\xc6}\xfd+*\x8b\x91\\\xa8\xc56@\x91On\x9fx\x9aT\xbf}\xd8~kY\xb6:\xf4\x15\x9b-\xaa\xf9\xa5\x97\xf1\x15\x95,C\x93\xe5e\xd4\xa5\xca\xae\x8bQJ$\x03\xb5Jv\xa88\x19\xaa\xa8\xbb\x13\x8c\x8e*\x07\xbe\xd8p\xc7\x9a\xe8\x955#5&\x89$v\xf3:\x1cV\xad\xa4*\xf1\x83\xebYV\xf7qJ\xdc\xe2\xb5\xa0\x95b]\xc0\xfe\x14F\x9b\x8e\xacKQ\xb3\xd9s\xb9z\x8a \x1bN\x1a\xae\xc6\x92H\xac\xdf\xc2z\n\x82U\xf2\x9b\x81Q8[TZ&\xda\x19j&\xb7\xdd\xf3\n\xb5j\x9b\xd4g\xbdY\x92 \x83\x81^uj\x93\x83\xba:aN2F\\lT\x90z\x8a\x9cL:Sn#\n\xe1\x80\xc5e^\xde\x08\x0eA\xf6\xae\xba3\xe6\x8d\xd9\x84\xd7#\xb1\xac\xd7\x00w\xa9c\x9c7z\xe7\xa4\xbc\xdd\x17\\\x13W-$`\xb9\xea**EKb\xa3R\xc6\xc9\x97\x8e*\xbc\x8e\xc7\xa0\xa2\'\x12b\xac2\x01\x8e+:t\x1bw.U.\x86[\xc4[\x93\xd6\xa7x\xf0\xb4\xe8\x199\xe9N\x94\x82\xa7i\xae\xf5M$LY\xe7?\x11a\xf3,\x14\xaf\r\xbb\x8fj\xf3Gv\x81p\xc5\x9d\x87m\xb9\xaf^\xf1M\x9f\xdb,d\x8c\x02Xr\x00\xeek\xc9\xe63\xa3\xbc\x0fl\x80\xa9\xc1%\xb0+|;V\xb0\xd1\x1d\xb4\xb7\x08\xac\xe9.\xd1\x9f\x99B\xf1\xf8\xfb\xd4\x93\xdeJ-~U\xde\xc7\x81#t\xfa\xd3\xee\xbc\x9b{5\x96\xeaT]\xa3\xe5\x8dj\x9bEq\xa9[\xa3\x88\xccqg\xe5Q\xc7\x15\xd42\xd8Ia\xd8\xed7\xdd\xc1\x18\xf55\xd7i\xda\x8a\x98\x0cj\xe0;8\x0c}\x87\x1cW\x18\xc8#\xb7\x94\r\xd9\x03v\x0fs\xda\xadY\xb8\xd9\x0c\xecy\x1c\x91\xed\xd0\xd2Z\x01\xe9v\x1a\xa82\xc4A\x022\xc4\x83\xe8\x00\xc9?\xe7\xd2\xba\xad\x07UK\x8d9&\'\xe5c\xb4\x9f\xf3\xef^e\xa4^\xc5-\xbd\xc5\xb1m\xa7\xcb>[z\x1e\x9cU\xdd\x1bY{/\x0e\xbc\x0c3$nc+\x9e\xbd\xc1\x1f\xe7\xb5h\x98\x9a=I\xa7\\\xa7<\x0c\x8a\xa3\xa9N\x9fe\xd9\x91\xcf\x15\xcd\x8d]\x9e\x08n\xed\xe4\xf3"u\xda\xeb\x9eU\xba\x83\xf8\xd3n\xae\xa5\x96\x04.\xdbUX\xe7\x9e\xbe\x94\xee"K\x96-5\xb1G\xf9I\x01\xc2\xff\x00:\xbby}\x1e\x97f\xf3\xb0\xdd$\xae\x121\xeb\xcf5\x8e.\x96\tC1\x0c\x8b\x9eOn8\xaa\xf7z\x84S\xa2\xef`N\xe2\xca;\x82{\xd2\x1d\xce\x82\xc9dw\x0f+\xe41,\x01\xedR_\xdbAq\tE@T\x9e}V\xb1\xa1\xd5D\xb3\xaa\x87\x18\x00d\x7f:\xd5MF\xdc L\x80\xfb\xb0s\xfaSL\x02+i"T\x89\x98\xc8\x07\xddf\xeb\x8a\xe8,\xad\x14@w\x0f\x98\xd6$W\x89!\x1by\xc1\xc1\x19\xe4V\xfd\xb5\xc8T]\xc7r7\x00\x8e\xc6\x98\xca\xbff\xdft6e@8"\xa7\xbd)\x198~\x82\xadZE\x89\x99\x8f9<\x1a\xa1\xa9D\xf3\\:n\x00v\xe2\x81\x18\xb7\xb7\xe0E\x98\xfe\xfdP\xb6\xbd\x99\xa6\xc9\xc9\x06\xb5\xa0\xb3D\x99\x92U\x07\xebP=\xa4p^a>\xe9\xedLf\xbc,\xcdm\x9fj\xe6\xf5\x00\x16f$b\xbah\x86"\x18\xac]T\xa6\x0eTW=mb)ldD\xc0\x9a\xb8]B\x8fZ\xc5\xf3\xf6J@<U\x81s\xbb\xbdx\xd5\x1d\xd9\xcd\xccl\xdb\xc4\x07\xcdVv\x82\xa4\x9e\x95\x95m}\x85\xda\xdd\x07J\xb3-\xd1d\xc2W\x9a\xe173\xb23\x87)ZyB1\xdb\xd35]\xaeI>\xd4\xd9\xb7\x96\'\x9aH\xe1f5\xe8\xc2\xf6\xd4\xe4\x96\xafA\xb2>{\xd4\x01\xc9a\x9e\x95}\xed}\xaa\x03o\x83\x9cT\xcaB\xb3,[\x11\xb4\x8a\x9dN\x1b5\x0cQ\xe0U\xb8ay2\x14g\x03&\x9c]\xc6\x86\x06\xdc\xd4\xc7~\xd5+\xc6W9\x18>\x95RRW\x9a%t]\xec`\xe8\xb6\x13"\x16I\x88#\xadv:yQ\x00\x8f\x83\x8e\xf5\xcd\xd9M\xf2\x06\x8c\xe1H\xc7\xb1\xa2M^KI\xc0)\xd7\xb8\xe8k\xd1\xa7u.c3\xb0q\x93\x16\xd3\xf3o\x19\x1e\xb8\x04\xd5-V\xce;\xbbvb\x9f0\x1e\x9c\x8a\xce\xb0\xd6\xc4\x97\x90\x878\x01\x89\xe7\xe8\x7f\xc6\xba\x9b\x8bu\xb8\xb52\xc7\xd4\x8e\xd5\xd1\xcc\x9acJ\xe7\x03kpa\x94\xc1*\xf48\x15\xb5\x16\xd8\xd9U\x98)\x7f\xbaO\xafj\xc0\xd5c0j(\xa0\x10\xc5\xb95\xa7\xadn]>\x05\x8c~\xf1\x88\x02\xb8\xd7V\xba\x12\x8b\xe7Sq\x19P\n\x158#\xbde<\xed-\xde\xf4\x95J\xe7o&\xb3\xa6\xb8\xba@\xefp\xac\xac~\\\x11\xdf\xb9\xab\xf6\x90\xdb\xdc\xd8\x89#Be\x03\x07\xdc\xd7=I9?B\x92\xb9t\x19Y\xb6\x92\x0e8\xe0\xd4\xe8]W\xaf\xe4j\x9d\x9d\xccq\xc8\xd6\xf2c\xcdA\xf3\x0e\xc3\xda\xa6\x92\xe5\x11\x8b\x01\xef[Bz\\\x8b\x16\x95\xd8\xc0\xf2\xed\xc8E-\xf5\xc5OmbR\xce N[hf\'\xb9<\x93\xf9\x9ad2/\xf6<\xc3\x03"\x17\xff\x00\xd0MGs\xa8\x08\xad\xa1D\xfb\xec\x8b\xfc\x85\\\xa5\x17\xb9I\xd8\x9bc\x02\xc0\x12*\xbb0N\xbdj\xb1\xd4vF\xef1\xda\xa8\xa4\xb3{P\xf1\xbc\x8c\xe0\x96Fh\xa3|\x1e\n\xe4\x9e>\xb5\x8b\x8cV\xb1E\xf3\xb7\xa0\x8fpA5\x9bqn\xd7\x00\x91\x91Wvo\x93\x18\xe9V\xe3\x83=GZQ\x93&G6\x894-\xc3\x1e=kn\xd2\xed\x9a1\xbf\xb7\xad-\xcd\x9a\xbb\x8d\x9c15L\xac\x9eQ\n\xb8\xc8#>\x86\xaeShKC\xab\xb4\xd4C \x07\x03\x1cS\xe7\x99\x1fh#\x92p1X\x962\xa5\xc4\x11\xc9\xd3r\x83W\x16L\xdd\xed\x07\x88\x93\'\xea\xdd?@\x7f:\xd7\xda\xc6Q\xb3\x1e\xa6\xdd\xb1\n\xa3\x9am\xdd\xfa\xc6B\x93T\xd6\xec"\x91\x9eGZ\xcd\xbe\x9fs\x16\xddX\xd5\xa3\x19\xc4\xb5U\xc7b\xe4\xf7\xcb*\x15\x075\xcf\xce\xafqs\xb52\xc0\x1a\x93\xcf\t\x92O\xd2\xae\xe9\x11\xabe\xfa\x9c\xd4B\x9aJ\xc4JNOQ\x9fc\xc4j\xcc@\xees[0\xda\xaa\xc4\xb8\xf4\xaa\xd7P\xf9\xf7\x91@\xbfqF\xf91\xe9\xd8~&\xb4w\x04^\xbd*R\xb3\xb1I\x0f\x86 \x08\xf5\xa9\xa6\x1b\xa3\xdbU\xc4\xab\xd784\xe1p\x18\x11\x9a\xd6%hS\x134.A\xcfZ\xb6\xb3y\x91\xe6\xa0\x90\xa3\xb0\xc63S\xc7\xb7g\x18\xa9\xf7\xafa\xad\x0c\xbb\xf0YO\xe8k\xc7\xfcH\x86\xd7\\\x91\x94$\xa7\x19\xda~\xe8>\xb5\xees[,\xb17A\xc5x\xb7\x8c\xed6x\x8aH\xdc\xb3)\x19\xc0=}\xab\xa2\x84Z\x96\xa1\x1d\xcc}?L:\x9d\xca\xdc\xea3+E\x9f\x965\xe8~\xb5\xd9-\xb0\x92\x18\xd5#\x1bq\xf2\xa2\x8e\xbd\x86~\xa6\xb9\xcd>9P\x83"\xa8\x00`(\xe1Qk\xa3\xb5\xd4V\x08^C\x8c(\xdcN9\'\xb0\xf6\xafB6\xb1\xa1"h\xf1\xcf\'\x92\xca\x0e\x1bi\xc7s\x8a\xcd\xb2\xd1\x19\xb4\xd5t\x04\x80\xe67\x1d\xd4\x82Eji:\x98P\xa9&D\xb2\x96*q\xf7N2+OL\x91\x92\xe6\xee\xdd\x94*\xca\xc6O\xa3\x91\xd4}h\xb0\x1c\xe5\xb6\x9b.\x9e\x92\xac\xf1\x97\x80\xf3\xb9~\xf2\x1fQ\xf8\xd6u\xb5\xd1S-\xbb\xc8\x16a\xc7=\x1f\x1c\x83]\xd4P\x15\rk9#x>[\x1e\x84\x1e\x95\xcf\\h"\xefz\xacx\x91I\x18\xee\xac=\r\x0e ;\xc2\xe6g\xbf6\xa0\x9f-\x81*;`\xf6\xfc\xeb\xa3\xd6\xa2ad\n\x9cm\x97\x0c=x5\x83\xa2B\xd6w0\xbe\x0e\xe1\x9c\x13\xe9\xff\x00\xd65\xd1]\x93so)n\x00o3\xf4\xa4\x9e\x82g\x17\xa8jsf\x04\\\xe2L\xfeT\xb6\xc2\xe6Y\x15\xd81\xd8\xb9\x1e\xe2\xad\xbe\x9a%\x90J\xe3\xee\x8c(\xad\xfbam.\x9eD;V\xea\x04\xc0V\xfe1S\xab\x06\xce\x19\xaf\xa6\xb7>q\xc8\n\xd9\'\xdb5n\x1dFYeE.q\xbc/\xebK\xa9\xc6\x8c\x92\x16\x88\xaa99\xf6\xa8<7a.\xa1~\x14}\xd59$\xfb\n"3\xb5\xd3o\xe3\x11\xbfiQ\xb6\xb7\xbf\xa5t\xfa\\\xb2\xdd\xb6@1\x8c\xfc\xc0\xf4?J\xcf\xd3t[S\x97=X\xe4\x93\xeb],\x0b\x04\n#]\xbc\xff\x00\rmp5-S$\x100\x05r\xda\xfe\xa3-\xbd\xe1E\r\x90z\xe2\xba\xebE\xfd\xda\xe3\xee\xf6\xcfj\xc7\xbc\xb2\x86\xeaF9\x1b\xc1\xe8h\x11\xc7\rNy\'\x0e3\x9a\x9c\xcf<\x97\t#\x03\x8a\xb7\xa8\xdaC\n\x97E\x01\x96\xa0\xb3\x94J\xa3#\x8aoA\xa3d\xb3\x9b]\xc8{W5\xa9\xcd$\x80\xaeNkx\xdd,P\x95\xedY8\x8ey\xdb\x8ek\x83\x11>\x89\x99\xd49\xe6\x81\xc8\xddNPA\x19\xc8\xad\x8b\xb8V W\xa6y\x15\x90\xf2\x81\xf2\x91\xf8\xd7\x998\xb4b\xe2ZA\xc0\xc7\xe5Wa\xe19\x19\xcdf[\\\x04<\x9a\xd0I\xc3\x8a\xce\x17cZ\x16\x82\x06\xa4\x1bT\xfb\xd4->8\x14\xe4\x8a[\x83\x84\xefZ\xc6\xed\xd9\r\xb2f\x91p9\xa8%\x91H\xe2\xad\xdbh\x93\\6\xc2\xe5O\xadM7\x85\xee\x90\x1c1\'\xb5S\xc3Uz\xa4=ld\xc7p\xa5\xc2\x96\x03\xdc\xf6\xab\xf0Kql\xcb\xb1\x80$\xe4\x01\xc8oz\xa6\xb6/\x0c\xa2)\x94\x87\'\x1c\xf7\xab\xb2F\xc9pH\x00*\xb0U\xe7\xa0\xa9\x84d\xa5f\tu\x1ewN\xec\xce~c\xd6\xa8\xddD\xc3>\xa2\xac\t\x84s\x1d\xc7\xbf4L\xdevH\x15\xd1*jHM\xdc\xc2\x86G}:#n\x8aAo\x97\x1c\xf1X:\xcd\xec\x91L\x01S\x80y\x1e\x86\xab\xda[j\xda4\xe1$\x91\xd6\xdd\x9b\xe5\xc7 \xfb\xd4\xf7\xe8e\x97{\xb6Cu\xad\xa0\xad\xb3\xb9Ik\xa9\xa7\x1a\x99\x162\x9f)t\x0c\xad\xed]\x97\x87\xf5\x96h\xd6\xd5\x94\x97\xce\x08\'\x80=k\x8c\xd3gY,\x12\x0c\x13$$\xed\'\xba\x1e\xd5p\xea1\xd9\xba2\x1c\x127l\'\x1b\xbd\xaae$\xae\x89^\xec\xb4:\x9b\xcd2=Y>\xd5\x12\x14\x08\xe4|\xc3\xae+\x0bY\x0c\xd2Eo\xd0\x00I>\x98\xebV\xd7]\xbb\xbf\xb6\xff\x00BH\xad\x14 \xf3\x06s\xb4}+6\xe24\xb7\xbf\x9a;\x99\x9a\\\xc1\xb9\x98\xf59\xe7\x15\x9d(\xbaq\xb2w*\xa4\xd4\x9ad0=\xa1\xb5\x98\\K\xbd\x95\xb1\x1c{\xb9\xcdA&\xa1g\x15\xab\xbd\xa11\xc8F\x15\x03g\x9a\xcdK\x9bX!\xde\xe0\xe4\xca\x1b\x8eH\xf4\x1f\\T\xba\x84\x96\xc9{\x14\x90[\xa1A\xf3\xbb\x8f\xba\xfd\xb8\x1f\xdd\xcf\xe7Sc>m\tV\xd2X\xd44r1i\x07\xceI\xea\xe7\xff\x00\xafS\xd9\xcd#H"\x94\x9c\xe3\x04\xd3%\xb9\xfb\r\xf4\x90#\xee\x88c\x1b\xc6z\x80\x7f\xad_\x86\xe2\xd2\xe1\r\xd7\xfa\xb6\xf9\xbc\xdc\xf4\xc8\xe4\x1f\xc7\x9f\xc6\x9c\xa2\x96\xc0\x8br]\xf9\x16\xd2\xa7r\x8c\x07\xe5P\xc7s\x1a\xc1\x0bJ\x9b\x89U\x03\x1dzR\xb5\xb4\xb3,\x85\x83E\x94.\xa9\xb7\x92\xbe\xb5\x85{\xbam*(\x96t\x88\xc8\xa8\xbb\xa4l\x0c`g\xdc\x9e\xd5\x8c\xdbL\r\xdb\xfb\xcbK\x98\xedt\xb5\xb7\x95&\x8e\xe0\t\xe6\n\x08}\xa7!z\xf3\xc7_J\xd0\x99\xa2}Fm\xb3\xae\xff\x00\xb3!\x11\xb2\x90H\xc9\xe4\x1e\x9d\xfaVZO\x1d\x9aXY\xa9\x85\xd2&\xd9!\xdaIG\x03<\x12\x01\xe7?\x8ejT\x7f\xb4x\x911\x81\xba\xd0\x8e}A\xcdj\xae\xa2\xb9\xb5`\xda\xbe\x85\x85%[\'\xe9V\xa4\x9b\xec\xf1#\xb6>a\x9cw\x1fZo\x92\x13\xe6\xdd\x8c\xf4\xcfz\xa3u\x9f\xb6,,q\xbe2\xc0z\x90\x7f\xfa\xf5\x9a\xe6\xb9[\x13\xbd\xda\xcbw\x0ch2pX\x9fOJ\x82\xdc\xc9\xe5\x18\xf02%p3\xf5\xa8\xf4\xe53]\\\xba\r\xdb\x08\\\x0fQ\xff\x00\xea5\xa5gep\xb0\x16\xb9\xd9\x11\x0c\xccL\x8c\x06\xd0NkG\x19I\xab\t36\xc1\x8c0H\x92p!\x91\xd4\xfd\x01\xcf\xf5\xabV.\xe6"\xef\xf7\xa5;\xcf\xe3\xd0~X\xa4\xfb\x05\x8b\xad\xca\x9dj\xdc\xa3\xcce\x9c\xa8\'\xcbC\xd7\'\xf0\xc7\xe3S\xa9\xd3Y\x8bG\xadY\x98\xfb\r\x8e?\xa55M\xdbP*Irc\x96\xe0g\x9c\x8f\xe5Y\x177R\xcb \xdaH\x1d\xeb^U\x88_L\x04\xb1L\x8c\xaa\xc1\xa3$\x8f\xe5J\xf614y\xda\x00\xf6\xa8Qz\x92\xcc\x81p\xdb\x00\'$\xd6\xad\x95\xe1\xb1\xb6.ArxU\x1f\xc4{\n\xc8t\x0bp\xc0}\xd1\xde\xa4\x82}\xce%#\xe5\x1f,@\xfe\xa7\xf1\xa9\x95\xe24u\x96SmB\xce\xc1\xa5~da\xdc\xfb{\x0e\x953J\\\xf0x\xac{\x16\xdc\x9dp\x07Z\xd0GQR\xae\xf54.\x801\xc9\xac\xfb\x9b\xa6\xb6\xb8\x8d\x01\xf9d8\xab\x06`\x13\x83X:\xdd\xc7\xc9\x1c\xe0\xff\x00\xaap\xc6\xa9\xcf\xa0\x9a5ei-\xe7W\'\xe557\xdb\x8cy\x00\xd6f\xa5\xa9+\xe9\xe8\xeb\x8c\xb0\xc8\xac\x8d3Q{\x97q#r\xb5K\xbba\'g\xa1\xd7\xc5~\xd2\r\xa4\x1e{\x8a\xf3\x7f\x1dA \xd6RU\xc0M\x9d}+\xab\xb3\xd4\x92k\xa7\x8dON\rdx\xd6\x04\x92\xda\x1b\x85_\xf5g\x07\xf1\xad(NJ\xa5\x99qh\xc8\xd0#\x17\x88\xa1\xd4m\x07\'<WW\x1e\x97j\xa8c\x91\x14)\xe4\xe0rMr:\x1a\x16\x9c\x10J\x90\xbf(\xcfB{\xd7gim2C\xe5FXI#|\xf37$\x0e\xf5\xebGb\xc8\xbf\xb0\x03\xa0\x920\x13i$\xfd:Tw:m\xdc\x01.\x14\x16 \x10G\xa8\x1d+^)^Dd\x883n\xc0\xff\x00td~u\xbd\x04e\xed\xa6B\xa0\xed\xe5A\xf5\x07\x9ac9%\xbaI-\n\\#\x03\x19\xf9H\xfb\xc0\x1e\x7fJ\xbd\x05\xa7\x9cD\xeb\xb4\xb9P\xc0\x8e\x8d\xff\x00\xeb\xab\x9a\x96\x99\x18u\x96$!e\x19\x1e\xcc:\x8a\x9a\xc6\x05\x10ax nQ\xe9\xea)\x85\x8ecU\xd9er\xcc\x83\x89\x0e\xe01\xd0\xf7\xa9\xec\xd8\xdd[J3\xce*\xd6\xad\x04R\xb8-\x8c\x8a\xad\x03\xc7n\x08C\x80k\x9b\xda\xc5N\xcc\xcd\xc8\xcc\x8d\x18;\x06\xf5\xfc\xaa\xad\xd3}\x9d\xf7d\xa9\x1d1\xdcV\xc4rF\xd7\x04\xe0a\xb84^\xe9\xabs\x199\x07\xd2\xa1\xd6\x8aBS2\xef\xee,\xe4\xf0\xfd\xc0p\x1aFL\x0f\\\xd5\x7f\n<vpHG\xdf\x03\x06\xa9]X\xc9o!\x8c\xb7\xc8O\x00\xd6\xa6\x89\xa4\xbc\xe5\xb6\x0cm\xe4\x9fZ\xda/\x9bTh\x8e\xafM\x9d%\x1c\xc9\xb7\x90\ru\x96\xb6\xd0\x98\x95\xc8\x0cH\xce\xea\xe1\x17M\xbb\xb4a\xb5\xb0\x1f\x9c\x9e\xd5\xbbaqs\t\x08d%\x0fn\xc2\xb5E\x1d\xa5\xb8\xe9\x83\x9c\x8e+\x9c\xbeh\xed\xaf\x9d\xe4gS\x9a\xdd\xb0b\xa8;q\xc8\xebT\xf5{x\x8c\x85\xf6\x82Xg\x14u$\xc2\xbb\x8a\xca\xea2\xe9p\t=\xb3Ul\xedDjW\xf8}j\x84\xb6\xff\x00j\xbf\xdb\x04l\x98<\x9e\xd5\xb2\xd6\xb2-\xb8E\xce@\xedXW\xc4\xd3\xa6\xad\'\xa9J2kDT\x9dT1\x01\xb3Y\xe8LwC\x8e3\x8a}\xd3=\xb9\xcbg#\xad,o\xe6a\x80\x06\xbcIV\xbc\xae\x8cX\xba\xb4lm\x95\xd7\x92*\x84:q\x96\x10XsZ\xb7M\xe6E\xb4\x02qO\xb4\x0c\x8b\xf3\x0c\x0cWD\x9f4\x8c\xcez\xefLxFV\x9bj\x93\x90\x01\\\n\xe9n\x1a)N\xd2*\x05EF\xc1\x03\x8e\xd5\xcf=]\x90\xecP\xf2\xca.z\xfdkOK\x9a%Vc\x80\xcb\xebP\xc9\x1eW\x82+%\x8b\xad\xde\xc0H\xf5\xa9\xa5)\xd2\x9e\xa1\xa2=&\xc0C",\xd1\xed#\x1c\xe2\xaeH\xd2\\\xc9\x1bZ\x04+\x19!\xcbW\x03\xa7_In\xc576q\xf2\xe0\xd6\xd0\xd4\x9e\x0b\x1c\xb0`z\xef\x07\x93^\x94qJ\xf6z\x1a)h[\xd7\xa0\x88\xcb\x11\x00o\xdc+\x96\xb8I\x04\xf2m\xc9\xf9\x8dX\xfbu\xdd\xfd\xdcS\x18\xdc\xc5\xbc\x0c\x81R\xa9V\x92G8\xf2\xf7\x1c\x1a\xc2\xb5E:\x97Dn`\xb4\xe7\xcd!\xc6y\xe4\x1a\xd2\x85\xc7\x93\xe9\xc5Awn\xae\xcdq\xd0g\x8a\xa9,\xf3<\x0c \x8d\x9b\x1dH\xe8+8Tn\xf6&\xce,\xcd\xd3|I\x15\xda\xdb\xc3=\xbf\xcfq(\xc4\xec\xb9\x8dW\xa1\x04z\xe7\xb8\xedUu\r*t\xd6\x9e\xd3\x1b\x00\xf9\x88f\xcf\xe0\xa7\xb8=\x8f\xf5\xaa\x92]\xa5\xb4\xcfo!\x11\xac\x8c\x1d\n\xae\x142\xf2:t\xff\x00\xeb\xd6\xa5\xfd\xc5\xce\xb1,Ykx\xd25"\x00[\x99\x07\\3v\xf6\x148\xca-\xb8\x8dN\xea\xcc\x99\xb4\xd3\xa7\xd9\x1b\x98\xa5\x89\xf7\xc6\xe8\xa0\x1eC\x0e\xbf\x97\xad^\xb6\x92X\xad\xcd\xab\x1b9\x04P\x86O1F\xe8\xce\xde\xe7\xd4\xf6\xaeN\xfbR\xba\x82X\xac\xe2\x89c\xdc\x8e\xaeYrU\xc1\xeb\xee\xd8\xe3\xdf\x8a\xd2\xbf\x85m\x1c\xb4~iq\x10i\x19\xdbq\x90\x1e\xff\x00^\x07\xe1ZY\xca/\xfa\xee%+J\xe6\xce\xb3f4\x19l\xef\xe1q,\x1b\x16\xdep\x9f\xc6\xa4p\xdf\x9d`^H\x97o!\x17\xd0\xc3s\xb8(Y\xc9T\x19\x1c|\xc0\x1f\xd7\x8a\xbd\xaf\xea\x93\xdd\xd9\x0b3\x8c;\xfc\x8cz\xed\x0385\xcc3\x99\xf6m\xc6d`I<\xfd?\x95ra\xe7W\x97\xf7\x8b[\xfe\x1f"\xebrs{\x9b\x1aSY\xbcf\xc6\xc9\xed\x88\x89\x94\xb2\xdeFA\x1b\xcf\x07\xe6\x1f+\x0f\xd4dT!\xe4\x8aH\xa2\x99Ip\xd8nx\xda\xbd\x87\xe3\x8a\x86\xceg\x8a{\x85\x89\x8a(\xc1\x01X\xe0\xb6;\x8e\xfd3W\x1f\xcb\x11\x1b\xa6,\xd1\xb4eQ\xba\xed\'\x9c\x1f\xc7\xbf\xd2\xba\xe5%}\x0cz\x9b\x171\xc38,\xd8\xfd\xe8R\x0f\xa7\x02\xa8\x88\xa2\xfbP\x82\xda\xea9U\x01i\x11\xc1@\xc0v\xcf \x9a\x97F\xb9Y\xcd\xb4\x97!R4\x00"\xe7\x1b\x98\x0e\xa7\xd8\x1a\xa9\x12Gg{>\xe7\x0f\n\x86\x19\xce2\xa0\xd4)7vS\xd8\xe9\x18_j\xb6\x03m\xe3\xc5$yX\xfe@\xa1\x86>`\xc7\xaf\x1cV\n[\xde\r=\xa1x!\x95 \x9a3(\xdd\xfb\xd5\x03\x86\xda:\x85\xe4t\xf4\x06\xae\xe9\xf7\xf2\\[\x9byn\x04\x08\xcb\xcc\xcf\xc8^1\xd3\xf2\x15N\xf2\xfak\te\xb5\x82\r\x8e\xaa\xbee\xda\x1d\xea\xdb\x180db3\xd0\x91\x8c\x91\xcf\xb5Br\x94\xee\xc6\xb6\xb8\xfdF\x04\x1e$\x828\x95\x85\xb4\xe5"y\x9d\x89]\xcd\x9d\x87$\xf5\xea+N\xdbO\x97\xfbQn\x16X\xa5Dv\x88D\x8f\xfb\xc0q\xc0\xdb\xdf\xdf\x15\x15\xcd\xb5\x95\xe5\x84\xf6y\x10\xe1\x92kP\x0e\x03\x02r\x7f,\x1au\xae\xa9`\xd0^\x06\xb6\x90\xcc\xd2\x83m8\x01d\x8d\x82\xf4\xfc\xf3\xf5\xad%\x15\xd0\x95n\xa5\xc4\xbb\x95\xee|\xbc`\x0e\x0e\xfe1To\xef`D\xfbv\xdf8[\x92\x1c+\xe0\r\xc0\x81\xcfq\x91\x8e)\xdb/fA=\xeee@\xd9~\x063\x8e>\xa2\xa9\xb4\x8bx\x92\xc6P\x01u\x0b\xaa\x01\xc0\x0c\xbf7OL\x8a\xa8&\x95\xc6\xf4.\xe9WWWV\x16\xe4l\xb4\x136Lp\r\xa0 =I\xeaN2rMT\xb4\xb9\x9e[\x94%\x88\x92B\xcf\xbc\x8e\x02\xf5\xe7\xf0\xa94\xe8\xa5)zX7\x97o\x12\xc1\x1bc\xf8\x9f\x81\x8f\xc3uF"\x96\xc2\xf6Wx\xe4P\x10\x15G\\\x1fa\x8fA\x82M>g\xf1=\xc3\xd0K\xe4\xfb2$h\xc4\xc3#\x96l\x9c\xfc\xde\x87\xe9J\xb3\x05L)\xedV\'\xd1\xeen|;g$\x00\xbc\x8e\xbek\xaey.I?\xd6\xb2\xa3\xd2\xafm\xad\xbc\xf9\xd9\xb6w\x1e\x95\xc6\xa7\xce\xb9\xa3\xfdX\xb9A\xc7FX\xb1\x9d\xe3\xbd,\xfc\xc6\xdf+/\xb5kLf\xde"\x83.\xac2\xa7\xda\xb1\x81hJm\\\x97\xab\xd2_\xfd\x9a\x02\xaa\xdc\xed\xf9\x88\xef\xec+\xa66Q\xd5\x90\x91\r\xc4/$\xa2\xd9{\xfc\xd2\x1fA\xe9\xf8\xd2\xb5\xbc\xbeh\x93\x19\xc1\xe8)\xf0\xc5*\xd9\x1b\x869w\x7f\x9f\xd8\xe3\x8au\x94wRJ\x122\xce\xc7\xb6*U\x9b\xb6\xa5"\xf5\xac\x9bNv\xf1\xefI=\xe3\'\x00UI5!o!\x84\xb2\xca\xe3\xb2\x0e\xa7\xdb\xdb\xde\xaa\xc9s%\xc3\x10Yb\x1e\x83\x92j\xdc;\t\xb2\xff\x00\xdbe\x97+\x1a\x96>\xd5CP\x0e-$[\x8b\x88\xa2\x0c:\x13\xb9\xbf!\xfe53\xda\xcd\x1d\xbf\x98\xdb\x99@\xc6EP\xd4t\xd9.4\xa9\x1c8\x0c\x06\xe0*=\x9d\xde\x9a\x8c\x96\'ItUtr\xdb8\x1b\xb8?\x88\xaej=N\xe2+\xb2\xb0*\x97q\x8c\xf6\x15\x905K\x9bU*\xacv\xb8\xc3\x0fz\x92\xcaE\xf2\x1d\t\xfd\xe7\xadh\xe1p\xb1\xb5o-\xdaI\xe6\xad\xc6\x1d\xdb\xe6\xc0\xe2\xb4\xf5K\xa9\xcd\x8a\xc5+\x87G\xee\xddErb\xf3\xca\x941\'b\xf5\xab\xf6\x0eu\xab\xd6\x8eIs\x04\x0b\xb8s\x83U\x1bGV\\#q\xdae\xd3Grp\xfbT\x1f\x99\x8f\x15\xd3\xa6\xb5#\xa1\x86\x07\xc2\xed\xc6z\xb1\xcdrp\xc4\xcfv\xc9\x17\xdcV8\xf4\xae\xf3A\xd0M\xc5\x9f\x9a\xac\xa0t\xe9^\xac\x1d\xd1\xa9-\x96\xb0\xd0\xcf>\xe0\n\xfc\xa8\xb8\xe7\x9c`\x01\xf8\x92MuZ6\xa9\x1d\xdc\xf7\x88\xa4|\x93\x8d\xbc\xf5\x0c\xbd\x7f:\xc3O\n7\x12\x16\xe8\xdb\x80\xcf\x7f\xf2)b\xd0u\r6e\x92\x02N9 \x0e\xd5W\x1d\x8e\xd6X\x92xZ20O\xcc\xbe\xc4Vx\xb60\xa3\xb6q\x8e~\x95\x05\x8e\xb0]\x17\xcc\x1f26\x1b=F\x7f\xfa\xf5{U\x9f\xcb\xd3\x9ed\xfb\xc1rG\xa8\xefF\xda\x89\xe8\x8e\x07Z\xbbh\xee\x9dA\x1c\xf3\xd6\xb1\xe6\xbdc\xb5Q\xb95V\xee\xf7\xed\x1a\xc4\xaeX\x98\xc7\x0b\xfdEU\xb3\xbb\x8c\xdc\\3\xf2\x07\n\rx\xd5\xde\xaeH\xe6/Z\xcbs\xf6\x9c\x0eT\x9a\xea-b\x99\xd4.s\\\xc5\xa5\xd8iA\x03\xe5\x15\xbeu\x84\xb5\x8c\x15\x19\xe3\x9a\x88j\xb5\x1aF^\xbb\xfb\x9dB5q\xf2\x81\x9a\xdc\xd35+(a%\\\x0e9\xacmQ\xa5\xd6\xa3\x8a[XK\xca\xa7\xa7|V\\P\xecf\x8d\xb2\x8eF\n\x9e\x08\xafF\x8c\xec\x91\xb4v;\xe1\xaeC*(A\x92\xc3*j\xfd\x9c\xcb.\xc6^\x06y\x15\xc1\xdaB\xe8\xe8\xc1\xf2\xaa\xb8\x075\xd4\xe9 \xbe\x15\xa4\xdes\x9e:\x8a\xeaN\xe5\\\xeet\xf5f^\x1b\x82x\xf6\x14\x9a\x920p\xe3\xa7LV\x86\x8djV\x01#\xae\x01\x18Pj\xbe\xb3\x11\x13\xc7!lF\xbdE;\xab\x88\xe6n/-\xedK\x0f\x97\x7fSSY\xde\xc7rB\xab\x01\x9a\xe7\xf5\x89 \x9e\xf6B\x9f\xa5>\x18\xc5\xa5\xaco\x1bd\xb7_Z\xf9\xca\xd8T\xf1N\xabw4X\x89r\xf2\xa2\xfe\xa7\x12;\xb0\xc0oS\xebY1\xaf\x91\x91\x9e*qvf}\x84\x90}\xea\xa6\xa0\xea\xa3\x0b\xf7\x8dmR)\xbb\xa3\x99\xb3kO\xbb\x95\xe3\xca\xc2\x9bXmbGQO\x9dQ#$q\x8a\xc8\xb0\xba\x91\x11`@\xd9#\xa7\xbdm4\x02\xde\xd5\xa5\xbda\x83\xfc5\xac\x1d\xe2&g\xba+A\xe6s\xbf\xb0\x1d\xe8K\xf1\x01\x1f\xba\x1b\xfa\xeea\x9c\xfbS\xed\xef\xa1\xf3L\x01\xca\xc6\xff\x00s#\xee\x9e\xd5\x1d\xf7\x95\x1d\xac\x91\xb8\x0b8 \xc6O_\xc7\xda\xb8[\xe5\x9a\xba\xdc\xa6\x9d\xae\x84\x8c\xf9\x8e\xe4\x81\xb5\xb9\xcf\xa5R\xbb\x8c$\x88\xec7(?xRE<\xb1\xc8\xe8\xe4\x11\xdc\xafj\x9d&Y]Q\x17*\xddG\xadt^\xe8\xcb}\x07\xach\xc66\x8f\x9d\xc7\x07\x15kQ\x928\xedL~g\xce\xa3\x95\xf4\xa7*\xdb\xc1\x13H\x0e<\xcc\xa2{b\xb1\xb58$+\xbfst\xc9&\xb3Z]\xb3G\xb1\x0e\x93s8\xd5\xa3\xf2\xa5p\x999\\\xf0F)<\xd9U\xbeQ\xf2\x83\x97\xc7\xbdA\xe1\xd1\'\x9d,\xf2\x02\xaa\xb1\xbb)?\x95hZ\xa0\xf3r\xbc\xc7\x80\x1b\xdb\xd7\xf1\xa9Q\xe7Fih\x89\xeen\x12\xe3O*\x91\xe3<\x0cw5\x99\xa7[\xbd\xad\xcb,\xdc\xf3\xb8\xa9\xe9\xd2\xafM\x08\xb5\xb9YT\x9d\xb0\x9d\xc5{\x11E\xd6.\x1d\x8ar\xc5x"\xb4\xb2\x82\xb7Q\xeb}Nn\xcfK\x93[\xbb\x86E\xb3\x13\x14\x83\xcfx_\xa1^\xe3\xeb\xe9\xee+:}^\xd3L\x93\xcd\xb5\x85\xbc\x86\xca\x82\xed\xf3*\x9c\x11\xcf\xaf^+\xac\xf0\x8e\xaf\x1d\xa6\xbf\n\x93 \x8e\xe2%^\x98^\x87\x1b\x87c\xe9Wu\xbf\x04Y\xcf\xab\xcd\xabG4p[\x949\x84\xa6Alv\x1d\xab\n\xf8\xc8\xd3\xc4\xaa3\xeb\xb6\x86\xf4\xf0\xeet\xb9\xe3\xd0\xe1\xeen\x1e\xf6\xeem\xd0C\x1c\x91\xfc\xaeb\xe0I\xb7\xa4\x80v8 \x9cpkF\xea\xe4\xdeCs,hN\xd8L\x7f0\xe78o\xf1\x15WH\xb7\x8e\xe7T\x86\xda]\xd0\xdc@\xc6(\x9b\x19\xf3\x13?w\xd3x\x1c\x0c\xf0A\xc7P+f\xf6\xca{\tn\xe1\xb8\x81\xbc\x898\x82@\xb8W\x1e\x9f\xef\x0c\xf3\xea0k\xd2Qq\x8bf\x0e\x12\xe5\xe69-[1\x05es\xb9\xe0\r\xcf\xa1\x00\x7fZv\x98\x9et\xaa\xa0p\xb1\xe3>\x86\xa0\xb9\x96)\xb5\x19\x9c\x94D\x8e!\x10\x0cp7c\x9a\xb5\xe1\xb5v\xfbL \x8d\xf2DZ5\xcf%\x94\x7fZ\xe7QRh\x862\xe2!\x14.#\xf9]\xae0\xf8\xec@\xcf\xf45kK\xbe\x8d\xc3i\xf3G\x98\xe4,\xdef@\xf2\xfd\x8f\xaeO\xe5\x9a\xb9}\xa7\xca\xf6\xab2[\xbbI(\x05\x00\xfe\x16\xef\x93X\xd6\xfau\xda^\xaaKn\xc1\xa4|\xc6\xc7\xa1n9\xc7\xa5\r\xbe`\xb6\x87@4\xf5\xd3#\xfbC>\xfbh\xd7\x10\x83\xcf\xcc}~\x95\x97v\x0b\x14\x85\xe3d\x96I\x031~\x03)\xe5\x7f\x0c\xf3\x9a\xd6\x8e\xe62\xb2\xda\xc8\xc5\xd2#\xd3\x19\xdd\x8fo\xce\xb2$\x96\xe2\xfd\xcbO##$M\xb1\x82\x0e\x83;W\x8e\xe7\xa5\x15%\x15\x15\x18\x82#\xd4\xee\xde\xdf\x16\xb6m\xe66\xeco\x1d\n\x8e\xff\x00CR\xa5\xe3\xbe\x85\x1d\xaf\xcc\\\xc8v\xaer608\xfc\xaau\xbb\xfb\\7\x92^C\x04\xcc#HW`\xd9\xc6\x069\x1d\xc7^{\x8a\xa3m8\x05c\x8c\x1c \xc19\xe0\x8e\xa3\xe9\xdf\xf3\xa5)(\xabG\xa2\x19\xad\x05\xd3\xba9\x90\xb2\xfd\x9bb\xa8\xce>b\x00\x02\xa3K\x19\xac\xef`Y7 \xbbs2!]\xbbr6\x9e~\xb9\xa9#\xd4\xad\xd6\xce\xe26@\xa6B\x0e\xd5\x1927\xb9\xed\xd0\xf3S\xe9\xf7\x89z\xf0\xc3w\xbcI\x02\x92\x98\xe1\x13\xd4c\xdf\x8a|\xf7\x8a\x7f\xd6\xe0\x91\xb9/\x9f6\x89$\t\x03\x18\xa3;\x99\xc9\xe8\xa0~\xa6\xb0m\xe6Q\xa8i\xcd\x86\xe4!`F0\\\xe3\x1fLUv\xf1\x04\xce&\x8a\x06ibg\xe6\x16\xe1\x19z\x11\xf5=\xe9\xd6V\xec\xba\xa4\t$h\xc5\xa4\x88\xc6Q\xce\xd0\x08\xf9q\xeb\x81\xc5]\xd2I.\xe3n\xe6\xad\xe9{[\x0b-.3\x89\x0b4\xf3Hz\x802\xab\x9f\xc0\x1a\xbc\xf1\xde\xdc\xea\x8fk\x04\xae\xb1Ih\xb14\xa4\x1c\xf4\xe7\'\xde\x9d\xb4\xde\\\xdf\xef\x1b\x81\xbfdP\x07UU\xe9\xf4\xe3\xf5\xad&\x93P\xb13\x95\x8d@\xd8\xbb7s\x9e9\xac\xa5R\xee\xdd\x99j\x0fql\x92\xe6\xd5\x92\x17\xdb\xb5\x00R\x01\xeb\xee+;Y\x9b\xcaO\xb3\x07W\x13d\xf2\xb8 w\xfdx\xa7K~\xb3D\'\xb8]\xa7\xb1\'!Ms\xc3T\x97S\xd5\x97v\x18\x19DA\xbf\xbc;\xe3\xda\xb2\xf6J)%\xf7\x1a7\xa6\xa5\x99\xb6\xc1\xbd\xe4\x1cF\x99\x1e\xe7\x15C\xcfya[vT\n\x80H\xed\x8erO\x035vxn5\x0b\xb9#\x89wG\xbc\x8c\xf4P\xa3\xaeO\xd6\xa0\xddoi\xe6\xa5\xab,\xf3g/p\xe3\xf7k\xec\xa3\xf8\xb1Z\xc5h\xfb\x19\xdfCJ\xd0D\xf6\xefk#\xfc\xd3&\xe5S\xd4\x91\xcdg\t.LF%-k\x03\x8eUO\xef\x1f\xea{\x0fj\x8a\xceU]F)\x95\x9aY7\x00\xd2?_\xc3\xd2\xa1\xd5\xafM\xac\x82<\xfc\xf9-\xf8g\x8a\xd9I+\x06\xefR\xee\x94 \xf3%\x89\xe3\t\xeez\x9f\xa9\xa8e\xb0\x94\xcc\xed\x01\x07\x07\xa5d\xdbAq}\x0c\xf7"\xe1\x83G\xd20z\xd5\x88u\tm\xa3T\x8c\x96\x95\x87\xcc\x0fANU!Qr\xad\xd0%c\xb7\x82\xfa\xda\xebG{`\x02]l\xc0S^\x7f\xabj76h\xf0\tCg\xe5>\xdfJ\xb3y\xaf$VO\n\xc6>\xd0\xff\x00\xf2\xdb\xb2\xd7\x19=\xden\x18;y\x87\xd74\xd3\xe5^\xe8\xee\xba\x11^\x01\x11E\xce\xeesV\xe3\x87d\xa5{\x95\x04\x81T\xc2y\xf7*\xc7\xee\x8f\x98\xe7\xd0S\xe7\xba\xdf\x19\x08Hr\xdc\x9fj\x1d\xd8\xd5\xba\x91\xdd\xbe\xc2P\x1c\x93\xe9SY\x03c(iK*J1\x85\xa8L^@Y\x98g\x8c\xf3P\xb4\xcdt\xea]\xf0\xb9\xfc\xa8II\x15\xaaw:h\x19\x9et\n8\xc7\xdcQ\xfc\xcdz\x8f\x86\xe4XmCHB\xae8\xf75\xe5\xbac\xb6\xd5\xd8\x08\t\xfcl:\xd7y\xa1\xbf\x9d\x18;\xdd\xf6\x90\x0b\x1e\xfe\xb8\x15\xe9Q~\xee\xa5#\xbc\x17\xf0H\x9bA\xdc\xcd\xdb\xe9\xfc\xab^\xceD\x9d\x10\xe3$\x01\xcf\xf3\xaeq&\xb1\x87\x08\n\xef\x18c\xeb\xc75\xa9\xa4\xde\xf9\xaa\nFBd\xe1qZ2\xc7j^\x1f\x86Gk\x9bp\x15\xcf\xde\x03\xbdr\xde%\xbf\x92\xce\xc9\xa2|\x82\xab\xc9\xf4\x1e\xb5\xe8\xdb\xf7G\x90?\x03^s\xf1\x11E\xbcQ]*\x86\x88\xe5$\x1e\x99\xf5\xf6?\xe1Q6\xd4\x192\xd5\x1eW\xe72J\xcc\xe0\x06\x04\xb3b\xa4\x82\x1d\xf0\xab\x01\xcc\x875I\xe5\xdf\x1c\x90\xc7\xfb\xc6c\xc6?\xbbO\xb7\xbc\xf2\xca\xae~T9\x035\xe64\x9e\x879\xd0\xc5lm\xd4\x16\x8fn\x07>\xf5\x11\x90M*\x80MW\x9fZ[\x84\x11\x869<Ud\x98\xdb\xcb\xb8\x1c\x82x\xac\xd4yt5q\xd2\xe8\xef|>\x89\x112\xb9\x00(\xe2\xb5\xe5\xf0\xb5\xa7\x88\xff\x00y\x93\x0c\xa3\xa4\x89^cm\xae\xcd\x1d\xde\x15\x8e\xd2q\x8c\xd7\xa2\xe8^,\xb2\x86\xd3\xcb\x17q\x99\xb2\x01\\\xf3^\x959C\x91Dq/Y|(\x08\xfb\xa4\xd6&\xd9\xd3j\xa0\xe4WY\xa4\xf8;L\xd2\x1c4FYXw\x95\xb3V4\xfdb\x19\xe2S\xb8r=j\xe9\xbdX\xdb\xf7\xb8\nz?o\xc7\xd2\xb5J\xdb\x15b\xe8\x00\x01\x8e+\x8b\xf1\xa6\xae\xb6\xec\x96\xaa~b\xa5\x8e;WOytV\xd8\xb4Dd\xf45\xe6:\xec\xb1K}!\x9b\xe7v\'8?\xca\xb0\xc4M\xc2\x1a\x11;\xda\xc8\xce_.\xef\x0e\x0e\xd6=3\xde\xa1\xd5g\xb8\xb5\x8a\x12\x84\xe0\x0e\xa3\xfa\xd3\xd5%`\xab\x18\xdf\xb3\xee\xa8\x1f1\xab6\xf1o\x9e\x18\xb5HYR90\xd1\x9e\xe0\xf49\xaf)TNWDFV)X^\xb4\xa4\xcd)\xcb\x11\xc6*\xecv\xe94\x82y\xa5\x01Cc\x15\x0c\xd6\xb1X\xdc\\G\x0c\x88\xf1\x192\x8e\xa3\x01\x87\xa0\xa1\xc0(b\x99J\x1e\x089\xe2\x9f5\x9b\x8b \xb4\xd7\xb0ZL\x16\xd9\x84\x8d\x8f\x99\x8fjf\xa5p\xf2\xc5\x1by\xa6B\xdc\x11\xe9U\x13O\x90p\x8d\xe6\x05\xe40\x1di\xd0\xca\x91\xc8\xa2T%\xd7\xef\x11\xd4\x1f\xebR\xea7\xa3\xd1\r.\xe4\xcd=\x91\xb5\x111q"\x8e\xe3\xbf\xb5jj\xd6/{\xa7\xc3<R\xa0>B\x97\x0cq\x9fz\xe5\xafQ\x8b\xb4\xc1YA\xc9U=\xab\xa6\xf3\xa3\x9bF\xb1\x0f)\x1f\xbb98\xa5\xcc\x9d\xd3\x1a{\xa2\x18\xd1\x16\x18\xa2[EW_\xbc\xe8I\xde=\xea\xe5\xbe\x9a%\x8d\x9dfX\xa4S\x95V\xf7\xe9\x8a\x8e\xce\xe4\xacY\x01\xb2\x99\nv\xe7#\xd6\xa6\xb6\xb8\x8e\xeba,:`\xb2\x8c\x05\x1e\x86\xa7\x96\xed6RHt\x91F\xb6"+\x82\x13\xc8-\x87o\xe2\xf7\xac\x1dZI\xcd\xb4k\x1c\x8a\xc1\xc7\xcb\x8fJ\xe8.\xa2\x85\xa1\xf9\xa7g\xe4\xfc\xae{q\xda\xb2\x1a\xd4\xcf0\x962\xa9\x08\x1c\x06\xf5\xa6\x94\x94ye\xb8J\xd7\xf7J\x86\xe2S\xa7\\d\x02 \xb5\t\x9e\x87$\xd2Y\xc4b\xb0\xb8\x9a\xda\xe0\xbf\xc9\xb8\x92:69\x153i\xb78\x95ee\x10\x80\xad C\x9d\xc7\xa8\xcf\xa7\xad%\x8a\xc7\x0e\x93t\x9en\x03n;\x9b\xd7\xd2\xb3\xe6j^\xf16{\x15\xed\xb5\x98\xee\xe21K\x0b\tx<r\x0e=*\xb8y\xe4\xbe\x8d"\x0e]\x86\xe0\x17\x8d\xbc\xd3\xb4\xdbe\xf3\xbc\xcd\x98]\xa1\xb0O*\xc3\xb7\xf8U\xab\xb8/n\xafG\xee\t,\x80\x81\x19\xf9\x86:\xe7\xd3\xe9Bs\x97\xc4\x84\x95\xd1\xca\xa5\x8c\xeb\xb9C\xb0\x92\x05\x03v\x08\\\x8e\xff\x00\xadj\xddx\xcfR\xb4\x10\'\x9b\xe6B\x8e#\x90\xb4c-\xdb\xf0\xe6\xa6\x1e"\x8fX\xbc\x92\xdfO\x8c\xad\x96\xcc\x82\xe9\x87\x07\'\x8e\xa7\x8e\xfcV<\x96lnei\x14\xba\x82\x01\r\xceOs\x9f\xc6\xb6\xf6Q\xad\x18\xd4\xa9\rW}\xd1nn\x9c\x9a\x83\xd0\xbf\xaa\xc3\t\xb9#\xe7\x11\xcc\xbb\xc9\x8d\xb6\x90s\x9e1\xdf\xde\x8b\x9di\xf5@\xb1\xcd\x98\x84P\xef\xd8\xceY\x80\xe8\x18\x9f^\xf9\xa8a\xb7{\x99\xd4\xb3\xe0A\x94Py\xca\x9e2}\xe9\'\xb7\xb1\xd1\xae\xae\xa6YZ\xe9\xee\xa3\xdb\xb1\xba\x8fO\xc2\xbb/u\xab\xd0\x9b\xcd\xad\xf63\xf5\r2;\xc4F\x85vJ\x02\x89\x89\xc1\x0c\x00\xc0pG\x07\xfc\xfaT\xbaU\xab\xda\xde\xc2\xf6\xf11O\xbb,\xa7\x92W\xbf\xf3\x15R\xd6[\x84\x93q\x8dZ\xdeBB\xecn3\xceW\xd8\xd5\x88\xe2\xbby\x12}>\xe2F\x89xx\xddv\xbcg\xdf\xb1\x04w\x15\x8bj,\x8b\x1dU\xc5\xcd\xe5\xb4\x1eb(1E\xca\x903\xfaV]\xbd\xf9\xbc\xb8\x96\xf1\xed\xcc-\n\x196\x8f\xe1$~\x95\xa9ct\xd76mo#\x04d\xc1)\x8e\x18t\xa8/,\xcc\x16\x17\x0bo\x19\xcd\xc1P\xe3\xba\xa8\xe6\xa7[\xdd\x07K\x19Q\xc8"D\x98 \xf3\x1f\xe6lu\x07\xb8\xa4\xb9\xba\x10X\xdc\x9bt*\xb7\x01K?\x04\xc6G\xa6zz~5v\xce\xdbm\x93$\xcb\x86\x07\'\x9c\x9c\xd5i`\xb6O:\x17\x08\xc9 \x03\x81\xd1\x86y\xfcj9n\xee\rX\xcb\xb1\x8cIfM\xa4\x8a\xdeg%$\xf9H\xec1\xebI:\xbd\x9c$L\x85Dy\xde{`{\xfb\xd3\xae\xe1\xfb\x15\xb0]\xbb\x9d~a\xf5\xecj\xd4>s\x08\xd2pe\x84\x85"\x1cu?Z"\xb7o\xa9&U\xbc\x93],\xb2\xc5\x1b4\x99Vs\xb3\xb0\xeb\xc7\xa7nkCJ\xb6"{\xcb\xc3+H^\x06(\x00\xe1}G\xb5l\xac\tkcq;\xa2\xfc\xe3n\xc5\x19-\x93\xc6\xf2}\xe9\xdaT*.\xda \xca\xabw\x1e\xcd\x98\xc0\x12\x01\xcf\xe7\x82h\x94=\xdd\xcaG1\x05\xc2Kt\xb0\xfd\xc8\x95\xb6+\x0cu\xf6\xad]\x0e+\x8b\xb6\x8f\xca\xf9d\x86R\x11\x1c\xf4\x19\xc09\xff\x00\xbe\x8f\xe3Y\xb7v\xd0\xdb\xde!\xb8\x04<\x04\x18\x9e5 \x93\x9f\x987\x189\xe7\x07\xadt\x16\xb0\xcbar\x8d\x1b\x9d\xab\x93\x8cr\xccN\x7fAD\xe4\xa3f\xc6\xa2t\x93\xd9\x1bg\x96[\x7f,I,\x84\xbf\x983\xd7\xd3\xde\x9e\xe6\xd7P\xbb\x11Mw,\x17\x10\xff\x00\x00l\t\x06?\xfa\xd5\x18\x98\x1b\xa5\x92nc?>\x0f\xf4\xfcj\x86\xa9aoo\xacM(\xb9m\xe5\xf7&\x17;N>\xedLb\x9br\xb1\xaavVf5\xec\xc9.\xb1sl\xf2L\xb1\xc3\x9c\x1c\x02?\x10*?\x0eX\xc9\x1c\xeb5\xc6\x01Ey\x11X\xfc\xc4\x1e\xf8\xec=\xe9`\xb4\x92[\xc9\xae#\xb6\x92\x14\x90m\x0crr\xde\xd9\xe6\xb4, \x8e(\xeeQ\x18H\xae\x11\x0c\xbc\x92\xe4\xf5\xcey\xcflU\xdf\xa9\x93m\xecW\xd5%v\xd9i\t!X\x0f\xdc\xc7\x9f\x9c\x93\xdc\xf7\xa6\xc7\x0cv\x8f\'\xdbb\xf9\xe3\x00\x08OR\xdd\x81\xfeg\xe9Z\x97\x11\x0bK\x88\xee\xa0_\xde\xb0\x0b\xc1\xe5F1\xf2\xfb\xd64\xd0\x11#\xbc\xee\x10\xa0\x1b\x8b\x1e\xae\xc7\xd7\xd6\xa6-9]\xea\rX\xab\nI-\xdcin\x8c\xee\x1c1\x03\xeb\xc9\xaa>#\xb2\xb8My\xb7\x06"\\y^\xfcWU\xa6Y\xcdc/\x9f#n\x99\xf2K\x0e\x98\xf4\xa7jWIl\xf1\x1b\xa8D\x93s\xb1\xb1\xf7\x07p\x0fj\xdaSQwEn\xceyc\x93J\xb6O-\xc2J\xe3\xe7\'\x9f\xc8U\x08\xbcA\x16\x91|\xc8-D\xf9\xe2Wc\xcekV-*\xfe\xe2)o\xaeZ8\xf2I\x8d\xa4\x95F\x07\xae?\x95d[\xae\x93*8\x90o`\xc5wc\x86>\xc7\xbd8Q\xba\xbb\xd2\xe1\xa9\x8b:\xb5\xec\x93\xdd+\x15F|\x05\xeb\x8c\xf6\x15f]\x05\xed\xf4\xef1m\x8b\xc8y,+\xae\xb0\xd3,^$\xb6X\x97*7\x0fj\xde\xb7\x10\xc1l\xe93\x88\xd0\x0e\x1c\x8c\xfe\x94\xf9\xb6\xb7Q\xa6y\x82i\x8c\x88\x90\xa8\xf3\x1c\xe2IOL \xeb\x9fJ\xcb\xd5\xe4\xb2i<\xfb_\xb4$\xae\xff\x00<L\x9f"\x8e\xd8n\xa6\xba\xfdB\xea\x1b\x8b\xcb\x9bKx\x94\xdb\x007\xa0l4\xddyf\xeb\xf9p+\x89o6I\xa5\x11\x86\x11\x16;c\x7f\x98\xa0\xf4\'\xfa\xd6\xb0\x85\x9b\xb8!\x1a\xe8\xca\x02\xb3\xfc\xa0p+[M\xf0\xc6\xa1\xabif\xf6\xc2?<\xab\xc8\xac\x8a1\xb0 \xdcK1\xe0q\xd0w\xach\xe1h.\x15\x1e00\x0bn=+\xd6<\x11\xadi\xf6z\x17\xf6E\xe5\xcaX\\\xb6\xf7t|\x03$m\xd0\xf3\xc6O\xa1\xe7\x15\xc5\x8f\xadS\x0fK\x9e\x94of\xbe\xe3\xa2\x84\x15Ir\xc9\x9c\x1e\x8fz."\xc9\x1c\x01\x85\xcfB}MzG\x87"-\x12\xc9\xbf\xa8\xe3\xd4\x81X\xd7\xda\x07\x87\xb4\xadf\xdaM.h\r\x9b(\x066\x99\x99\x99\x8fp1\x80\x00\xf45\xbb\x1d\xccV\xf3\x05\x81\x80N\x80 \xeb^\x86\x1b\x15\x1a\xd0R\xa7u~\xe4\xca./\x94\xdf\x8e\xce(\'\x0f \xdeX\xf21\xdc\xfb\xd7I\x04\xf1\xc3\x1a\xf90\x9cz\x01\\\xbd\xbc\xb2\xbci\xe5\x872\x0e\xe0g\x07\xda\xba{\x05\x91]|\xcc\xf0\xb9$\xd7zZj4j[\\y\xf0\x9c\xa9\x1cpk\xcb~\'\xdf\x18\xe4\x10\xe7tR.\xc6\x19\xc0\xf5\x07\xea\rz\xd2\xb2I\x13\x14\xc6\xe1\xe9^#\xe3\xc7\x8a\xe7V\x9a\xdeG\x08G^\xfcg\xae;\x8c\xfeU3v\x8b"oC\xce\xd6ym\xa7\x8aH\xb8\xf9\x86G\xaf=\rkk\x96\x89k22)\x1e`\xdc1\xd3\xe9O\xb1\xd0\xfc\xbb\xb6{\x8f\x90D\xd9e\x07(\xe7\xb1\x07\xd2\xb4\xda\xc6K\xad:D\x9aH\x9aEr\xd1\x84|\x90\x0fo\xc2\xbc\xbeh\xad\x99\x8d\xf5\xb9\x81g\xb5X+\x01\x9fZ\x9a\xf6\xda\xe5QJ\x91\x86\xe9\xf4\xa8\xad\xd28\xdbt\x910~@\xde\x08\x04\x8a\xd7\xd2b\x1a\x83N\xd3\xa3\xa2\xc27\xa8#\x86\x1d\xf1V\xe5\xcd\x12\xb9\xb4\x13E\xd1\x17Q\r\xb9\xf6D\x98S&q\x82z\x9ad\xfe\x13\x16\x97\xf2\xc9kz\xf2\x05\\\xc5\x1c}]\xbd\xcfaR[L,\xb4\xd6\xb0\xb9}\xb6\xef/\x98\xee\xdc\x16\x03\x91\xfaV\xde\x9c\xb6\xbfgk\x9bA"\xb8\xe5df\xe3>\x98\xa2\x9dK\xc6\xc9\x0e,\x97B\xd5uh\xe7{Y\xd1\xe3\x91\x14\x18\xc3\x0e\xb5\xde\xfd\xa6\xeam1\x96g)"\xf3\xc1\xaeoM\xb9I\xa4I"\xb8\x13](\xf9\x93\x1d\x0f\xb7\xadO\x7f\xab^\xda\xc7#\xdf\x04\x11\xe3\x86O\xe1\xfa\x8fZ\xd7\xdb$\xdc[\xf9\x16\xdd\x91\xa2\xba\x8b\xcf\n\xc0\xf3\x90\x99\xc6\xcd\xdd\x0f\xb5gOlmdm\x9f:\x8ew\x1a\xcf\xb5\xbd\x83R\x89\x94\x88\xd8\x1e\x99\x18\xe7\xfcj\xe5\xbb\xa3\xc2\xd1\xcb1\n8\x0b0\xe5O\xd7\xbdsJ|\xebS%&\x8a\xbb\x89\x9c4L\xcb\xb4n\x05?\xc6\xac[\xcdsz\x7f|\xaf"\xe0\x9d\xc4\xfd\xccR[\x81\xa6\xff\x00\xaf\x94\x98]\x8f\xce\xdc\x8c{\x7f\x85I\r\xe1\x0cZ$\xc1d\xdd\x1b\x0e\xc4z\xd7-D\xae\xb9D\xfc\xc6K\x1a\xac\x07\xcc\x04\xc4FA\x1dT\xd5)\xd4\xc2\xa4\xe1\xa4\xdcwg\xdb\xd2\xad\xfd\xa1\xde\xe5\xe7\x93\x0c\x1b\x89\x15G\x03\xe9Q\\,2\x97\x92\xcd\xc7\x92\xe7\x0c\xb9\xce8\xeb\xf4\xa4\xe5mm\xa8\xacj\xdb<K\x12(\xdb\x1c\x92\x0c\xbb)\xe1G\xf8\xd0\xb76\xd3\xea\x11\xdb-\xa8\x92\x00\xdf\xeb:s\\\xe7\xd9^9\x02FY$\xc6p\x99+V-Vh\xa0Y\xee\x0f(\xd9\xca\xe4g\xeb\x9a%\x88\xe6\x83\x8d\xacTn\x9d\xd9\xd7j\xba,\x17\x16\xf1\xb0\x8cez\x81\xdcVe\xdd\xa0H\xe3X\x81B\x13+\x17oz\xd3\xd3uT\x9a0\xb2\x9cdu\xcd7Y1\xc8\xd1\xb2\x1e\x80\x0e\xbd\xab\x83\t*\x91\xbcj-\x0e\xba\xd1\xa7%\xcd\x02\x85\x95\xb4ko\xe6\x17b\xdfZ\x8ai\xda\xd6\'\x056\xef$\x1c\xf4#\xd6\x9f\x1c\x1b\xd7bI\xb5H=\xfb\xd2\xdc\xc1$\x93\xc6\xceG\xcb\xda\xbd8-\x0ef\xec\x8a\xf2ye i\xf7\xabM\xd7o?0\xed\xf8\xd6v\xa3|\xf6\xd1y\x11\x12e\xdc\x1b8\xe3\x00\xf3ZW\x88\xc9\xfb\xb8\xdf\x0f!^\xbd\x01\x1d1U\xa5[yW\xed\x0f\x06\xc9\x02\x90\xe4\x1eOl\x11E\x94[\xd0\x86\xaeC3Mg\x02I\x07\xcd+@\x0c\xc1\xbf\x8b\xd8\xd4\xbaLQ]i\xb70\xc5\x99\x179@\xddy\x1d\x0f\xe3YO+\xdc\xde\xa2\xed\x9b\xc9U\xf9\xb6\x8f\xe1\x1cV\xae\x87\xb6\xd3W\x92\x13#ys&\x14\x91\x821J\xd7\xdci\xdeEm\x1e\xf4$\xe9j\xa1\xdc\xaf\x0b\x9c\x10\xbds\xf8V\xad\xe6\xa2\xb6\xdbg\xb3x\xe5\xceVE\x04s\xf8\xfbVm\xed\x9cz7\xda\xe6\x80\x12\xecB\xae9\xc6\x7f\xfdu\xcf\xc3mqi\xa7\xb4^A\x8d\x9eF]\xdf\xdea\xc9?\x91\x1c\xd5\xd3iJ\xcd\x9aS\x8aoS7\xc3\xf3\xdb\xc39\x9a\x02\xca\xeb\x90\xd9\xe8\xcaz~5\xd3\xde\xdb\x89\xad\xb7\xbe\xf5\x91\xb0c\xf2\xfd}\xc7z\xe5a\xb9I\x19\xc0\xb7X\\\x9f\x9bo|t\xae\xaa\xee\xf8\xc1\xa5\xda\\\xa1\\\x0e\x18\x11\x9c\x8cs\xf8\xfaV\xb1\xf7S\x89\x83Wf-\x9b\xbcW\x17q\xb1\xda\xc0c\xeak6\xcem\xf7\xe3\xcfl\x96\r\xcf\xb8\xa9-ZK\xa9^BI\x05\xc2\xe7\xd4Wg\xe1M\x06\xc6\xe2\xee\xe7\xcf\x883tM\xc3\xda\xb91X\xa8\xd1\xa7\xcc\xcb\xa3M\xd4\x97*8\xad\n\xca[\x0b\xbdB\x17p\xd1I),\xa7\x9e\xbd\x08\xf45\xb1\x15\xbbX\x97\xb9\xf3\x9d\x84\xce\x00\x8c\x0c\x00=j]sI\x97I\xd6d\x88\x7f\xa9\x91\x8bF\xc0c#\xde\x92\xd6V\xb8\x02\x17\xe1T\xf0qU\x1a\xde\xd7X\xec9\xa6\xa4\xd37\xe1\xb5\xb6VK\x94o\x9foJl\xd2\x1biwd\xb3\x122\x0fa\xe9Q\xc2\xf1**\x86b\xeb\xf3n\xa5h\xa3\xb9\xfd\xe9\xcb)\xc6ET[\x8a\x15\x86\xc7\xe4J\xed<jc\x19!\x91\x869\xf5\xa6\xdei\x8am\xe5\xb8U\x05\x80\xc8_Z\xb6\xd6\x8bxB\xf9\x85J\xf6\x07\x82~\x95~m-\x8d\x83&\xfcepp){h\'i\x17\xecd\xd5\xd1\xc1J\x82\xee8a\x9em\xb7\x0cAeNrs\xd3\xe9]E\x8cZ~\x99\x7foov\xc7\x0c\x03\x1c\x8e\x1b\xd4g\xfc\xe6\xb3\xed\xf4\x18\xad.VF\x1ec\x86\x07\x03\xda\xb5R3q0I~C\x9c\xee5r\xbc\x97*v\xbfS(\xaeYj\x8c{\xeb\x9bM\'W\x92+8\xee\xa6\x8av\x00E<\x9b\xcb\x02~\xf1\xf6\xf4\x15:\xe9\x0c\xb7q\xea?he\xb7W\x05\x02\x9c`\xfa\x1c\xfe5\xa9\xa8\xe8\xac`\x17\x11\x08\x1eh\x97\xf7.\xcb\xc6}\rah\xd1M\xe4\x18\xae#H\xd4\xff\x00\xacQ#\x9es\xd7\xe6\'\x9fzN.\x1e\xeb-osj\xff\x00O\xb6e8U\x93/\x9d\xac2\x06i\x8bg%\xc1\x11#\x0f\x95IQ\xdf51\xb9\x8d\xad\xd6(\xc9;z\xb0\xe4\x9a\xdb\xd2-\xa0\x863q#oc\xc4g\x1c\x81S\x1a~\xd2I1\xc67\xd8\xcb\xb9H\xff\x00\xb3\xfc\xf9m\xfc\xb7\x8a\x12\xbdz\x13\xda\xa8\xeb\x12B5iZve\x07i\xf9G?trMt\xda\xbd\x8bMl\x04#;\x8f"\xb0u?\x0f4\x97\xcb=\xcc\xc5\xe1h\xd5Y\x17\x826\x8f\xeb[N\x9f\xba\xd3\xeeKi\x95\xc5\xf4s\x98\x84J\xf8\xdc\xac\x0e\xd3\x82=\x8dZhL\x92\xba\xb2\x95,r\xc7\xbdKke,\x88Y\xa2\x08\x00\xda\x91\x8f\xe1\x1d\xa9\xda\x89\x92\xde\xd4H\x82C4C!\x00\xcf\x98{f\xb9\xd3\xe6\xde\xe0\xa3mJ\xf7K\r\x99\x0c\x0b\xbb(%w\x0f\xba\xdd\xb1\xebY\xb6zo\xdb\x19e\xbb\x90*\xc4\xfeg\x92T\xfe\xf0\xf69\xf6\xab\x93\xea\x02t7w\x18\xf9~UA\xc8\xc9\xebS\xc4\x93\x88\xfe\xe6\xe4nA-\xdb\xde\x8b\xdaV\x89Ri\xbb\x88\x19cq%\xdb(\xef\xb1O\xdd\x03\xa6k.[\xf4\xbd\x89\xe2\xbb\xb4Sld\xc0l\xe7\x1e\x83\xdc\xd6\x86\xa7m\ni\xe6H\x18\xb3;\x05 \x9c\xe2\xb1c\x82\xf2P\xa4F\xa2\x08$\xda\\\x9e\x0b\x91\x9e\xbfJ\x1f\xc4\xac\x89\xd5lV\xd6\xb4\x8f\xb5j\x0b%\x9d\xe2\x0by\x14\x19VF#\x1ct\x1c~\x95-\x8f\x86\xedm^\x052<\x89\xbfz\xc7\xd5Ct\xc8\x1d\xcd*iQ\xdd\x19/\xee\xe4t\x95p\x10\x19>_\xa0\x15\xb3nL~P\x8b\xee\x8ce\xbd\x05m)\xdd$\x99w\xb9\r\xec0\xe9\x92A\x14\x08\x00<\xee\xce~S\xff\x00\xd7\xab\x1a\xa5\xad\xad\xee\x8e\xe4K\xb6P\x84\xa3g\x1c\xfa\x1a\xa5\xab[\x16\xbb\x17\x01K\x101\xb7<U\t\xed~\xd9m WrJ\x11\xc98\x1e\xd4R\x83R\tK\xa9\x91\xa1x^\xe6\xf6)\xf5Cp\xac\xf1\xae\xd6\x88\x0f\xbc=\xbd\xebCE\xb0\xd2\xf4\x98\xe7K\xfd$\xdc\xfd\xb2S\xe5\xcd\x10%\x98c\x98\xcf\xa7<\x8aO\x0f\xcbw\xa3\xa3}\x981Dm\xae\xac3\xbb5\xa2of\x8a\x1b\x84\n\xab<\xb8 \x11\x91\t\xfe\xf0\xff\x00k\xf9U\xcaw\xba\x90\xaf\xae\xa7/5\x9cV72J\xd6\xe4\\\x16-\x143\x00~\xce;n\xec[\xdb\xb7z\xb36\x85\xa7\xc1d\x97r,\x93K)\xdc\xcb#e\x895<v\xf3\xcf$\x82\\\xca\xd9\xdc\xc5\xfa\x9a\xd8\x8e\x08\xb5+\x8b{q P\xab\xb8\x83\xd6\x9cf\xac\xe4$\xee\xec\x8ej\x1d\x1aIfF\xb6y\xa2\x84p\xf1\x96%O\xa1\xc5tzv\x9b"\xdc\x89$~zm\xc5n\xa5\xa5\xbcp~\xef\x96^8\xefKo\x03\xdb7\x992\xe1O|s\xf8\x8a\xd6\x85E\xce\xeei\x16\xed\xa91\x92H\xd4"\xb8\xdc\x01\xc9\x1c\x05\xedZZD\x97\x82W.\x01\x89@\n7g\x81\xdf\xf1\xac\xc12\\~\xedP\xa8S\x81\xc7QZzU\xc8\x8c\xb8d\x91\x88\xe7v\xda\xeeUc="Rw:K\x07y\xe6;F\xd5\xc75\xe7\xde5\xf0\xb9\x97R\x96\xe7l\x12#7\xdd\x91\x88\xc1>\x87\xb15\xdci\x9eh\x9d\x9c\xb01\xb7>\xf5\x81\xe2\x87{\xd8u-=\x95\xe4\x8eU\xdb\x1c\x91\x10\x19O\xa7\xe0{\xd65\x1d\xe9\xeaD\xf69\x0b_\rEq\xa7\x9bwg\x86El\xa8~Y=A\xf5\x15N\xe2\xde\xd3J\xb8\xc4s+O\x1f+\x1e\xd22s\xc1\xcf\xd2\xae\xd9\xe9\x97\x9eF\xf7\xb8\xb8\x8eT\x05s\'\xcc\xc7\x1d\x98\xd5=o\xc3\xb7io\x1e\xa0\xce\r\xae\xd0\xb7.2\xe63\xeb\xb7\xa9\x07\xd7\xb5y\xcdn\xc8\xe5V4\xa5\x85\xf5\x1d.{\xa8\x95g\x9a\x1e\x0c@\x06Y\x1b\xd8\x0e\x86\xb2a\xbd-:\xb4\xd1\xce\x92\xc7\x91%\xa1M\xae\x84\xf6#\xd3\xde\xb2\xf4\x1d/X\xd4/\x19t\xcdB\xd4\xa2v\x13\xfc\xe7\x07\x82S\xafj\xeb\xaf.\x12\xe3P\x86\xebR\xb3\x89u\x18\x98\xa7\x98X!\x95W\xa1\xc1\xe4\x8aMMGO\xf8\x02z\x9c\xec\xd05\xfd\xdd\xba\xca\xa8\xd8\xcb:\xb0\xe5\x07l\x8a\xec-\xed,\x8d\x92[\xa4ab\x03\x1f*\xe3>\xf5\x9c\x86\xd6\xe6\xe8\xcd6\x99\x14\x12H\x00,\xacX\x00;\xd6\xc4K\x19\x88\x0bv\x12\x0fB\xbc\x0f\xa9\xac\xa1\x17\x17\xa3\x1aB\x8b\x0b(%[\x88\x15\xa2*\xa4.\xc3QAq\x04\xb0\xc8\x91[\xce\xc3\x9c\xb4\x84`\xd2\xcdos\x85\xf2\x9c)\xf4#?\xe4VD\x96\x18\xb9\xcaM-\xbc\xd9\xcb\xa0c\x83\xf4\xae\x9av\xde\xc0\xd6\x9b\x9a\x96\x9a.\x96\xe0\x98\xa5\x92\xdeb\xd9`G\xca\rO\xa8X\xb9UB\xeaO\xf0\xc8\xbc\x83\xf5\xac\x8cj\n\xdb\xcd\xcce\x07\x00\x13\x90G\xd6\xb5\xf4`_\xcdg\'\x8e\x839\x03\xe9D\xac\xdf/R/c\x1c\xe9w\xc9\x19\x17%\x0c \xfc\x9c\xe4\xa9\xff\x00\n\xd6\xd34\xd9n\xf4wfEYab\xbbA\xfb\xc0T\xd7\xc41_\xad:i\xd2\xda\xd8\xc9\x19!\x98a\x94\x1c\x03\xf5\xaez\x90\xb5\xf9J\x85\x9b\xf7\x8c\x9bh\xa3V\xe7\x98\xd4\x96*89\xf45l\xdbG1\x91\xe0\x8fnO\xca\xa0~u\x98\xd3\xf9\x93\x96E\xdaO\\V\xe6\x8bp\x85\x8aH8\x1d+\x96\xb4\xe5k\xad\xcdi\xa4\xdf+!\x8eQ\x0c\xf1\xe2%\x12E\xf3\x10\xdd\rh]^\xdb]\x0c\xf9ar\xb8#\x1d\xcdO=\xad\xb4\x934\xaa\x06\xea\xc8\xbc\x8b\xcb\x93!~S\xda\xa1\xd0U\xe2\xaau4\xe7\x95+\xc7tN\xb6\xd0$\x0c\xc8\xca\x00\x1d\t\xa8\x1d\x89\xd9\x96\xca\xe3\x8c\xf6\xa3j\x08\xb3\x8e\xd5\x03\xfc\xc0\x00H\xad\xe1I\xc1{\xce\xe6R\x9a{+\x13\x12|\xc4=;\xe6\xa7\x96RFT\xf3\xfc>\xd5O\x95@0~\xb5-\xbb\x82\xd8q\x90\x7fJ\xda3\xbe\xc6dk\xb2g062\xc7\x97\'\x81\xefP\xb6.%\xd8\x18er\xa5\x9b\x90\xfc\xf1W\x1d\x920cnS\xb5f\x95"L\x05\xc2\xe7\xa04\xe2\x9a\x93m\x84\xb6H\x92!\x86~\x00\xc7\x05GJ\x1e?&U\x9f\xe6\xe7\x9c\x8ex\xa7\x16(9\xf4\xe8+.\xe2\xfe\xe4]$q\xb1R\x1b \x8e\xd8\xefM\xc9=\xc1+\x1b\xb7\xb6\xcd5\xbcW\x16\xf2\xab4\xbcmn\xfe\xf5GRg\x88}\x9ax\xf33\x01\xfb\xd3\xd1\x06y\xc0\xa9\xf4\x9b\x86\xb8\x84\tN\x1dI#=z\xf5\xa9\xb5X\x05\xc4{\xb7\xfc\xde\xb5J\x11p\xbfcx\xc6\xf0rG\x98\xcf\x0c\xb1\xc6f\np\xe7\x00\xfbV\xd5\x8e\xa2\x92h\xedi\'.\x83\x82y\xab\xa9\x14W6\x02\x02\x06@\xefT,\xec\x85\xb5\xe1\xc8\xdc\x08\xc6*\xa5+;\x98>\xc6\x9f\x86\xe0\x82Gdu\x18on\xe0\xd7|\xf1\xc3ej\xf7\x10\xa8\r\xd4\x81\xd7\xa5y\xed\xad\xc3ZJV\x15\x00\x03\x9e\x95\xbf\x17\x88P \x8e|\x82\x7f*\xf1\xb1\xd4*JK\x97X\xbd\xce\xcc4\xe1\x15g\xb9N\xefS\xfe\xda\xcbO\x01U\x1c\x1c\xf55\x92\x1d<\xd7D\x18U8\x06\xb5\xf5K\x98nc\x06\xdc\xae}\x86+\x0e8\xdd[vy\xcf \xf7\xae\xdaMF\x9a\x8cU\xacr\xd4\xbb\x93m\xdc\xb3\x05\xcc\x8b#\x7f\xb4{\xfaV\x94R\xcd\xf7\x07\x00\xf6\x02\xb3\xc6\x19\xf7m\xe4\xd6\x84\x0e\x13\x03\xbf\xadm\x0f2\t\xd9\x1a\x12%\x0cr*\xc2\xeaSM\x11R\xf8\x02\xab\x82\xd2\x0eO\xcb\xde\xab\\\x8d\x90\xba\x83\x83\xd8\x8a\xbeH\xb7\xcc\xd1JrZ&6\xf3S\xf2\x9c\x08\x8er1\xb7\x1c\xe6\xae\xdaK,\xb0e\xf8\xc5`[L\x15\x8f\x9a2\xe0\xe75\xaf\x05\xf2<D\x1f\x96\xb4Vr\xb9\x0eF\xc6\x99t.\x16H\t9\x07\x02\xa9\xdc\xe9N\xac\xea\x8d\xc19\xebY\x06ym\xe72B\xd8\xcdX\x8bT\x92F>c\xed\'\xf5\xa5\'\x19h\xc5r\xd5\xb5\xa8\xb4\xe1\x98\x8fSR\xa7\x88,\xe1ci\xe7s\x9cd\x1e\x95\x04LfWG\x902\x9a\xa3m\xa6\xdb\xc7v\xfb\x906\xec\xd6*..\xf1)>\xc7kc+G\x08o3\xcc\x07\xa04J\x93\xc97\x9e\xeb\xfb\xb5\xe8=k:\xc1\x8d\xbe\xc0\x06S\xa0\x04\xd5\xfb\xbd@\xcd\tH\xc8\xf48\xae\xa6\xf9\xa3a\xa5\xd4\xa9\r\xf4\x97\x17o\xe5\x95X\xd3\xaf\xb9\xab2\xdf\xc6 >p\x19<c\xb9\xac\x1b\xab\xa8"\x924\xe67<\x1cqW\xa3\x92\xca\xfd<\x91\xfe\xb0\x0f\xd6\xb3\xa7\'\xacz\x96\xd77\xc2X\x16v\x8ff\xf8!C\x0c\xe2\xb0\x92\x19v\xc9n\xa6W`x]\xdcV\x8b\xc7\xe4\xe1X\x92\xab\xd2\xb3\xaf.\'\xd3\x9f\xed1\r\xd9?t\x8a\xceROuff\xd3E\xe2\x93C\x12G=\xab\x14\x0b\xd7\xd0\xd5e\xb6\x96fX\xcb\xff\x00\xa3\x12\t@;\xfa\x9a\xeat\xbb\x81\xa8\xe9\xea\xd2\x81\x929\xf6\xac\xfb\xe8\xe2\xb5s\xb7\x03<U\xd6\xa4\xdaN;\x14\x8cy\xada\x86TV#\'\x8eO\x02\xb4\x97N\x89\x04\x8d\x0c\xdb\xe0c\xc7\xaa\xf1\xceG\xd6\xb3e\x82\x1b\xa9D\x8f \'\xa7\xae*\xe4{\xe0\x8c\xa26A\xeb\x8fJ\x84\xade\x16\\l\xb4c\x14,\x80\x87\n\xdbO\r\xd35j\xda\x186|\xaa\x0bu\xc7\xadb\xdc]\x18\x0b\xc0\x0e\xd6q\x9c\xb7j\xa5g\xa9\xcbes\xf6y\x0f\'\x90I\xe1\x87\xa85\xd3%kH\x8d\xf47/!0\x9f2$\t\x9e\xa3\x15\xca_\xdc\xed\xbc\t\xd1\xb3\xcf\xbdk_j\x97W\x12,\t\x90OB\x0esY\xb3\xf8r\xed\x1dn\xe6}\xca>b\xb8\xedY\xd4\\\xd2\xd09Z\xd0\x94\xca\xb2D\x1d\x0e\xdc\x0cn\xff\x00=j\x03\xe5\xc1\x18\x93w\xcf\xedK=\xc4S\xc1\xe5[\x8d\xa4\x0c\x1c\x1cb\xa8\xdb\xab\xb5\xc2C(\xc8\x04f\x93\xb5\xadqZ\xc7Ok+\xc6\x13a\x1b\x98tn\x95\xd3Gc=\xdc),\xd81\xaf\\\x1cW\t{-\xccsB\xd0p\x13\x9cv5\xd9\xe8z\xd7\xdb\xec\xbc\x89Y\xa3a\xc3/\xbd*4\xa0\xa7\xef-z3Nv\xe3n\x85\xe4\x82\t~H\xc8U\x07\n\xd8\xe7>\x86\xb7\xb4\xfd:\xd0\xdb\x86\xc4\xaa\xd9\xcb1\xef\\\x95\xe5\xcc\x96q\x95E.3\xc6=kkJ\xf1*\xce\x82;\xb8\xda%\xc7%\xbb\xe6\xbb)U\x84e\xcb\'\xa8\xd3-3\xac7\x8e"m\xf1n\x19 p\rR\x8e;y\x1al\xfc\xb2\x969\xfa\xd4WZ\xf5\x9cwsZY\xa31,T\x9e\x8aO\xa85\x95s\xf6\xc3tgm\xaa\xad\xd7i\xe2\xb2\xad\x89\x8a\xdbP\xdd\\\x86\xfe{\xad6\xf1E\xc7\xcfl\xce\t\xdb\xf7\x8flV\xbb\xb4s\xe8\xb35\xb0i\x03FT\xaaq\xff\x00\xea\xaa\x12\\G4f90\xc4\x1e3\xd8\xd4\x9a|\xabh\x1dC\x13\x1b\xf6<\xd60\xad\x1b\xf92\x0f2\xbf\xd5\xa1\x92\xf3z\xda,r+m\x1b\x10\x07\xe3\xaf<s]%\xac\x93\xcdimlt\xfbcu6$f\x84neN\xc1\x89\xcf\xcd\xeb\x8a\xe8e\xd0\xb4\xe4g\xb9\x8e\x18\x96W;\x9b(\xa7\'\xd7\xa5gCj\xd6\xb3\xb3\xdb\xca\x06:\r\xbf\xd6\x9c\xdf.\x88,\xba\x1a\x0fke,\x11\xc5q\xb6\xdai\x01\n\xe9\xc9\x1e\xc74\xb6\xd6m\xa7\x93\x16C\xa1`\xc1\x8dZ\x87\xcb6\xe1d\n\xc7\x19\xe5GZ\x86Y\x80B\x81A\x03\x81CQ\xe5\xf3\x12\xdc\x8eY\x15e\xdc\xbb\x83\x1f~\x82\xa3y!\x9cm\x91wc\xdf\x9a\xaed\x02M\xc7\x83\xe9OYP\x11\xc0\x03\xda\xa2,M\xea\tk\x18\xe6\x16 wV\x15m.a\xb5\x8fkF\x14\xf7+U\x9e\xe9T\xfd\xdc\xfd*\xad\xc5\xd2:\xe3n\x0f\xd6\x93\x96\xban+\xdc\xbf)I\xb0\xd10\xc9\xaa\xf7\x113\xc6\x15\xb8\xc5T\xb6\xb9Dc\xd79\xe8j\xcd\xc5\xea4~\xf4\xde\xda\x81\r\xbd\xba\xf9\xc0\x1a\xd9\x10F\x91\xeeR\x01\xaez+\x9c\x1d\xc0\xd5\xc1xX\n\xca.*\xf7\x0b\x9a\x8b&\xcew\n\xa7us\xb9\xf0y\xa8\x96s\xeeEV\xb8\x93\x8c\x8e\xb4\xde\x8bA\xb9\\\xd1B$\x8f\xdc\xd3\xa2\x80n\xac\xcb{\xa2\x0e\x0f\xadh,\xe79\xcdRi\xee+\x92\xcc\xbeZ\x91\xb40\xfeUAX\xab\xf0*\xc4\x97;\x8e\x0fJ@\x8a\xe4v\xcd\rv\x19\x11M\xcc\t=\rL"\x18\xdd\xd0S\x8c`pj9\xa6\xd8\x9bGJAb P\xb7\xccAn\xd5RkDk\x8d\xff\x00.\xe3\xd4\xfaS\x83\xb3\x9d\xdbzt\xa8\x9aR\x1f\'\xa85\x9a\x97rYz\x0bI#m\xc8W\xea=*\xd5\xc6\x1a\xdc\xafBj\x8aj\'\x01r\x07\xd2\x92[\xa5\x93\x04\x12Tu\xae\xa4\xe3m\nRiX\xc8\x85\x11\x1c\x00\x08?Z\x9eKb?x\xa3\xb7"\xa1Yp\xe0\xf7\xad8\xd9]98\xcd\x1c\x97\xdcW3\x15c \x900I\xe6\x99\x1cI,\xff\x0078\xed\xebS\xdc\xc7\xb3\x84\xe9\x9c\xd6|\x92\xb4m\xb8q\x8a\xc6q\xbe\x83\xbe\xb75\xd6$\t\x8fn\x86\xaa\xb4K\xb9\xb8\xfc*\xa4\x1a\x90\x07\x0f\x9c\xd4\xa6\xed\\\x1085\x14\xe1$\xf5.m5\xa1<@\x01R+`\x16\xf4\xedU"\x94\xe0\x81S\xa3\x12\xbc\xd6\xad\x19\x96\x96V8\x03\xa1\xebQ\xdd\x8d\x88A<T\x0b#D\xf9\xe7\x14\xcb\xbb\xaf1hL\x1a*\x1c\x1694)*:\xd5W\x93/\xd7\xadX\x8d\x86\xdcT\xdfS2\xe8`@&\xa3\x90\xf0Ux\x06\xab\x99Jq\xda\xa3g$\xee\xc9\xfc*\xa4\xb4\x02\xdd\xb4\xe6)@$\xaf\xa9\xab\xa2\xfdB\xe4\x9c\x9c\xd6#\x13\x8c\x92i\xf6\xf2cvz\x0cu\xf7\xac\xd3kB\x91\xbau\x19\xe6\xb7!~Z\x8a\rZe`\xacs\x93\x8a}\xa7\x95\xb5C\xe0)\xad\x7f\xf8G\xednQY_i\xea0k\xba\x1a\xad\x0b\x8c\\\x8c\x9b\xfd6\xe2\xec\xa4\xe1\xb6\x8e\xb5Z9\x1e\xc2A!b\xa0u\xcdt\x17\x10\xbd\xb0T\x04\xb0\x1dy\xaaw\x96\x91]Y\xb7\xcb\xc9\x15\x9c\xe8\xaenn\xa5-\x0b\xb6\x17q_\xc3\xf2\xb0-\xdf\x14\xcb\xdbw\xc7\xcc\t\x8cW\x0f\x04\xf7\xba-\xe3\x04\x8d\xda<\xf1\x8e\xd5\xddi:\xba\xdf\xc0\x16U\xe7\xd1\x86\ri*wF\xb2N]\x06\xe9\xba\xd40J-\x95\xc0l\xf0\r]\xd4\x96\x1b\x9b~\xdb\x8f\xa1\xaecZ\xd0nSP\x8e\xf6\xcb$\x06\xcb\'\xa8\xab\x82\xf6H\xa3C$gr\x9eT\xf4\xc5)E\xab\x10\xe1e\xa1kN\xd3\xc9\'y \xf6=\x8d\x0e\xc6\xc3R]\xed\x98\x9b\x83U\x9fW\x938\x862Tv=\xa9\xcd\x9b\xd0\x1aO\xbdY\xd4\xe5\xb6\xda\x89\'\xd4\x93XH.d\x89\xe0`$\x18\xc7\x19\x04{\xd6\x89\xd2\xac\xae,U\xa4T\xde\x07C\xebY\x02\xdcF\xc4\x1c\xaf\xe3I\x1d\xbd\xd8c\xfb\xef\x93\xb6M\x11\x9bWLv]\r+K+x\xdb\xe5\x8c|\xa7\x8a\xd3\x9d\xd1\xad\x1a6Q\xc0\xc5d\xdb\x96\x8c|\xd8\xf7\xc5$\x97@\x1cn\x1f\x8d$\xd2[\x92c.\x80|\xd9eB\x14\x92y\x1d\xc59\xb4\xf5\xf2\xf1\xc0u={\x8a\xd3{\xd8\xe3@C\x00=\x05G\xe7\xc1(\x0c\xac\xb9\xac\xa6\xed\xaa@\xcaj\xca\xbf#`7OcH\xaa\xf6\xd7K:\x12\xbe\xab\x9e\xb5$\xb0\xa3\xb6\xe1\xcf\xae)\xcd\x19e\xdap\xc9\x8e\x87\xb5B\xaf\xd0\x11n}F9c\x19\x07wp{\xd6\x85\xa6\xa1i%\xb6\xd6\xc0 c\r\xd4V\x12G\x10\x1bs\xd7\xa6i\xc2\xd92X\xfeF\x9c+v\x1e\x9b2If\x85.\x89\x87\x8ez\x0e\x95\xa0\xda\x81\x92 \xa4\xe6\xb0n\xf6\x8cl\x18\xf7\xa4\x82r\x00\x0cy\xaey\xa9+\xb4Kv\xd1\x1a\x92FBo\x04\xfa\xd3\xedn>C\xbb\xa8\xaa\xe96P\x1f_ZQ\xcfJ"\x9b\xd5\x06\xc5\xc6\xbdy\x07\x96\x16\xa2\xdeQ\x87\xadF\x0f\x94\t5]\xee\x83\x13\x8a\xd96\xf7\x0b\x16\xa4\xbce<\x1e(I\xb7\x03\x93\xc9\xac\xf2\xfb\x87N\x94\xfd\xe7ha\xc1\xabz\x8fB\xc89l\xb0\xe2\x94\xca\xbc\xe0Uo9\x88\x1c\xd3s\xd7\xbdLt%\xb1\xb2\\|\xf8\xefL`eRA\xc1\xa6<Y\x9798\xabQ\xa2\x80\x00\xaa\xd9\xdc\x92\xach\xc1\xb0sN\x91\x1dE\\1g\x9a\x8d\xc6z\xd3n\xe0V\x8d\x0e*\xdcJ;\xf6\xa8\xd4\x0cR\xf9\x9bO\x15\x9d\xba\x81sxPz\x1a\xad)\r\x90:\xd2n\xe34\xcc\xf3K\x9a\xe3hf\n6jt\xb88\xc15\x1b\x9c\xa95\x07\x98\x07zM\xd8E\xe4\x93vA\xab6\xaf\xcf&\xb1\xc4\xf88\x07\xadY\x8ab8\x06\x9c\'vR5\xe5\x94c\xadf\xcf&3\x9esNi\xfe\\\x13T\xe6\x90\x1e\xf5\xa3w\x06\xc9\x04\xe8\xc7\x04\xe0\x0e\xd5^Y\x95\xcf\xca\x0f\xb5Dw7N*hm\x7f\x88\xfe\xb56l\x96C\x12\xc8\x1f\'\x81Vd\x91Dx\xef\x8e\x94\xef\x94\x0eG5\\\xed/\x8cV\xb1V\xd0FsH\xc0\x8a\xb3\r\xd3\x8c\x0c\xd3>\xceNMD\x01W\x19\xabL\xab\x1a[\xcb/\'5^x\xc6\xd3\xc0\xc5O\x10\xdc\xa0\xd2\xbcySR\x07:\xd9\x8eB;U\x84\r\x8c\xe6\xa7\xba\xb7\x1b\x81\x1d\xe8\x8e#\xb7\x9a\x1b\x02hF:\x8a\xba\xa3\x00b\xabFJ\xf1W\xe2P\xeb\xc5+\x94\x91Zv\x01y\xac;\xbb\x95B@n+z\xea\x1c\x83\xcdr\xfa\x95\xbb\x06b*\xadt\x16"\x17{\x9c\x0c\xd5\xf8\xae0+\x9bR\xcb\'\xe3Z\t)\xd8\x0eqY-\xc8q6\x0c\x81\xbb\xd3\x94\x8f^\xb5\x8d\xf6\xb28\xcdH\x97\xb8\xe35m\x93\xcak\x9c\x14\xe2\xa2\x1ft\xafL\xb6MB\x979\xfc\xa9\xfb\x83Vm\\\xa3R\t\t\x84.s\x8a\xb5\x15\xc5\xdc\x03\t3\x05\xeb\x8c\xd6LR`pj\xda\xbb8\x00U\xc3\x99\x0b\x9a\xc6\xa8\xd5e\x916H\xb9\xf7\xa7)c\xf3\x06;j\x95\xbcx\x1c\xf0j\xf2\x8c\x83\xf3c\x8a\xd7Y+2\x93/\xdb\xd9\xdbN\xb9p7V\x9d\xbe\x9fn\x14m\x03p\xef\\\xc5\xbd\xcb\xc5#\x00\xfcg\xbdh\xc3\xa82?/\xf2\x9a\xd6.\xda\x16\xa4\xd6\xc3\xef\'\x92\xdeq\x19\x07nz\xd5\x91\x047\x90\x8d\xe8\x0f\xbdf\xde\xbb\\\x00Pn\xc7=j\xde\x8e\xb2\x18r\x1f\x91\xd5M)FW\xb9\xaf\xbc\x96\xa3f\xd2J|\xd1\xaa\x1cw\xaa\xd1\xba\xda\xb1.q\xea+V[\xc5\x8ab\x8e\x075\x14\xf6\x82\xe8o\x18\x1cTJ/q;\x10\xab\xdb]\x0c\x800{\x8a\xb7\x14Qya@\xdc1Y\xf0\xda,R\xfd\xfe}*\xdb\x18\xe3\x00\x86?\x85B}\xc5\xe8L\x91\xc4\x87\x078\xaa\x97\xf1BG\xc8\xc3?\xca\x9e\xd2\x82:\xe6\xa9L\xd26N\xe1\x8f\xa5&\xae\xac\x81F\xe4\x7fb\x12`\x10\xdfQVb\xd1\x95S!\xb9\xf7\xac\xd7\xd4Z#\x8d\xe6\xb4-\xef\xcb\xc5\x9eM8\xc5\xc5Y\xa2\xbd\x98\xd7\xb3h\xb9\xc6\x00>\xb5^YD\\\x1a\xb8\xd7a\x90\xfc\xa7\x1e\xf5\x9dq\xfb\xdc\xedZ\x87\x18\xb7r\x1a\xb6\xc5w\x9c3\xe75/\xda\xf6\xa6\t\xcdT0\xbe\xec\x9c})\x16\x17\xc7#\x15\x94b\x933q\x1d,\xe1\x87\x1c\xd4q>[\x93I$\x04\x1c\x8c\xd3\x166\xce1Zr\xdc\x93E_r\x80\x0fCVQ\xc0Nz\xd5\x0b\x7f\x94\x90jm\xcc\t=*Tl4-\xc5\xc3\x0c\xf3PBrri\x93\xc8\xdb\xbaTJ\xe4\x1c\x9f\x96\xa7\x97[\x97}\r\x02\xc0\x8e8\xa0\xb0Q\xf5\xaa\xc8\xf9<\x9a{8\xc5ib\x07\x87\x1e\xb8\xa7\t\x00\xe2\xaa\x17\xe6\x9c\x87\xb95-\x08\xb4\x06\xe3\xc5XE\x08=\xea\xa4r\xa8\xe8j\xc0\x9dqZE\\\t\xcb\xe1j\xac\xaf\xde\x95\xa6\x18\xaa\x92\xca\x08\xe2\x94\x96\x808\xcf\x8e\xa6\xa2k\x80\x0fZ\xae\xe7\xd0\xd5i\x1c\x83X\xea\x06\xaa\xdd\x020*e\x90\x1a\xc7\x81\xcesWQ\xcd(\xadG{\x96\xe5|-gK&\x0e*\xcb\xbf\x1dj\x84\xad\x86\xf5\xa74"Tnj\xe27\x15\x9b\x1be\xaa\xea\x11\x8aP\x88\xcb%\xc9Z\x87is\xd6\x90\xc9\xc55$\x1b\xabP-\xa4@\n\xb2\xa7\x0b\x8fj\xae\x92\x0c\x0ejF \xafZ\xa1\x10\xc8\xf8\'5Y\x0f\xef\x0f=jY\xb2j\xa7(\xdc\xd5E\xd9\x88\xbc\x91\x83\x19\xa8\x1a\xdbvx\xfaU\x8b6\xdf\x18\x07\xaf\xa5N@\x19\xf5\xa9\xd8\xe8I4P\x81\x8a\x1d\xa7\xa8\xab$\xe5j+\x85\x08\xe2A\xf44\xa2PF)\xdc\xcd\xe8B\xf1\xef\x18\x1e\xb4\xd1\x16*\xdc{y\xf5\xa4\x91GQR\xc4Tu\xf4\xab6\xeeG\x19\xa6\x15\xe0\x8a\x8c>\xd1S\x17\xa9H\xb3;\xe4\x1a\xc4\xbb\x8by&\xb4$\x9c\x05\xe7\x9a\xcf\x9a\xe39\xc5n\x90\xd9\x96\xd6\x8a\x1f\'\xbd$\x96\xe0\x0c\x01V\xfe\xf1\xcd.3I\xa4g\xa9\x89<dt\xa8#\xded\xc5n\xcbl\x1cg\x15Q\xed\xb6\x9c\xe3\x9ai"\x93\x16\x07+\x8a\x9d\x9c\xee\x18\xe0T(\x00\x19<\x1a\x94\x10N*m\xa9,\xb3\x1c\x87\x03\x9a\xbd\x03\x90rO5\x9a\xa3\x18\xa9\x91\xdc5;X\x9b\x1d\x05\xbb\x83\x8d\xc7\x8a\xb13o\x19\r\xc7j\xc9\x82O\x97\x9a\xb8\xb2\x9e1U\x19\x15\x1b\'\xa8\xbfgbwg\x9a\xac\xd1\\\xab\x93\xbb\x8e\xc2\xaf\xa1\xc9\xe74\xff\x00,\xb1\xde?*\xd22\xb9\xd3\x14\xba\ti\x14\xec\xb89\x06\xaf\xc2\xf2\xdb.v\xe7\xd6\xa0\x82f\x84\xfc\xc3\x8a\xb7%\xc2\xba\xe5\x07>\xf5\xad\xecu5\x17\x1dJs7\x9d)c\xd6\xb4\xedn\x82\xc3\xb4\xfd\xea\xc7\x95drH\xc8&\x9fn\xce\xa3\x0e\r\x0eh\xc6I4X\x9c\xc8\xd2\x96^\x05@e}\xc0n\xce}j\xc3H\x08\xc5f\xca\xdf\xbc\xc0\xe2\xa2N6\'\x99$jFr\xbc\xe35:\xa2\xb0\xedY\xd1\xbf\xca\x00<\xd5\x98\xe4+\xde\xb9\xdbW3\xe6\xd4d\xfajJr@\xfc*XlB&\x00\xe6\xaeE\xf3\x8c\xd2\xbeT\x1fJ\xb5vS\x91D\xdb\x85\xf9GJ\x8b\xc9\x03 \xd5\xa6oJc\x91\xde\x9f*!\xb2\x93\xa0\x07\xa0\xa8\x1d\xc0\xcf<\xd4\xf3g&\xaa\xb4c?^\xb5\x9b\x8d\x8c\xa5-F\x03\xb8\x1ez\xd2\x05\\\xd3\x8c{O\x1d)\x0e@\xa5{"n $R3\x9294\xa7\xa1\xcdA+V2\x96\xa08\xf3\xc9\xaa\xd2\x1ex\x15 c\x8an7u\xfc\xa9\xc5\xdc.1\x19\xfa\xfaT\x81\x98\x83\x93H@\x02\x90\x109\xadR\x00-I\xe7b\x99#w\xaa\xaf&\x0fZm\x01pO\x83S}\xa8\x01X\xef?9\xcf\x15\x10\xb8r\xde\xd4\x96\x80m5\xd6\xee\x01\xc541=\rgG)&\xac$\xc3=j\xadp&\xce\x0e\r2E\x04\xd3\x8b\xab\n\x8c6[\x9a\x97\x00\x16/J\xb6\xad\x8czT*\xbcn\xa7\xb1\xc0\xa4\xa1a\x0e\x92O\x97\x8a\xa7#n\xe9R\xb1\xe0\xd4\nr\xd8\xa5(\x8c\x965 \n\xb0\xaeq\x8a\x8dG\x14\x99\xc1\xc5JV\x196\xee\xe4\xd4\x0c\xe7<P\xf9\xc51wP\xd02\xccs0\x1c\x9a\x9df$u\xe9U\x93\xa74\xe2\xbbz\x1a\xa4\x98\x8bJ\xfb\x8f4\x92\'\xcb\xd2\x92\xddI\xebW\xc4coL\xd5\xa8\xdc,E\x12\xec!\x85I!\xc2\xe6\x99\x1b\x02\xb4\xd9d\xc2\x91M\xc4\xb8\xca\xc5y\xe6\xf9H\xf6\xaaIq\x82A4\xf9\x9b$\x9a\xa4\xcb\xc95\x9c\x90I\xdc\xd3\x8a~s\x9a\xb4\x1bp5\x8d\x1b\xed\xefW"\x9f\xb5!\x16\x98\x90\xd8\xa62\x13\xcd*\xb0n\xbdjP\xb9\xa4\x96\xa3E\x19\x14w\xac\xf9c\xf9\xce+nHARk>H\xc2\xb95\xad\xec\x81\xb2\x9cp\x9e\xf4\xe6\x84\xa9\xe9\x9a\xb1\xb8\x03\xc0\xa9\x10\x06\x1c\x8a\x95{\x88\xa8\x17\x8c\x1a\x86H\x85_x\xfd\x0e*\x06\x8f<f\xb6\x8a\x16\xa6t\x91`f\xa0\r\x83\x8cV\x99\x80\x9e\xa2\xab\xc9k\x8ej\xf9\x11\xa4c}\xc6\xc4\xc7\x18\xabK\x9e8\xaa\xa8\x85NqV\xa3~:Q\xca7L\xb9\x0c\x80pzU\xe8\x827J\xcbV\x18\xa9\xa1\x93kpj\\\x05\xec\x99\xbd\x02\x81\x83V\xb0\x80g\x1c\xfbVD7\xbbx~\x95`\\+}\xd6\xfc)E\xb4\xce\x88\xc5\xa4_m\x8d\xd4\npTS\x91\x8a\xab\x13\x82>j\x97\xccLb\xba/t\x12\x1e\xc5I\xc8\xa4;\x08\xce0j \xc0\x1e\xb4\xc7\x98/SXI\x989\x0e8\xc9\x19\xaa\x93\x10\xa6\x95\xe7\xe7"\xaa\xcf&\xe1\x9e\xf5\x83\xd4\x86\xee<O\x83\xd6\xae[\xcf\x93\xd7\x8a\xc5iq\x8a\xb1\x04\xe0\x0c\x83R\x93\xb97:8\xa7"\xa5i\xb7\xadbEw\xd8\x9a\xb9\x1d\xc0 `\xd6\xd1+\x98\xb0sQ\xb3g\x8aC0\'\xad/\x07\xf1\xadn&\xc8Xs\xcdWa\xc9\xab2\x1e*\n\x89"\x19\x18\xce1PHpj\xd1Z\x82T\xc8\xacf\x80\xaeX\x93Q?"\xa4"\x98\xdd+\x1e[\x81\x18\x1c\xd2\x93\x83KLs\xefZ\xc6#\x14\x9c\xd4n\xc0\x0e\xb4\xd6r*\x07bMh\x86+\xbe\xe1\xc5Wzy\xe2\xa1v\xaa\xdcD,~jC\xc59\x88\x14\xdc\xe4\xf4\xa3\x94c\xd5\xc04\x19\xb1\xde\xa3u\xef\xde\xab\xb4\x84\x1ehV\x1a4\x04\xf9\x03\x9a\x95e\x04\x8ek)e#\x8a\xb3\x1b\x93\x82i\xb6\r\x1bP\xb8\xdbR7+\x9a\xa3o&\x00\x15`>N)\x8a\xc3d\xa6E\x19-\x9c\xd3\xc8%\xaahS \x0cVm\x0e\xc3\xd5p)\x8c\x999\xab\x1b;S\x08 Rh,VrF\x059H#\x8aV\\\x9aU\x8f\x06\xa9Dv$Q\xde\x9f\x8c\x9eh\x03 qR"f\xaf\x90|\xa3\xe3\xe3\xa5YI\x08\x15\x12\x00\xbc\xd3\xb7\n\xb5\x12\x94Jp\xddey\xa7\xb3\x86=k-\x18\xf6\xe2\xacF\xe4\xf7\xac\xd9\x9d\x89\xa4Pj\xac\x89\xc5M\xe6Tr0"\xa1\x8d\xa2\xa6\xec6\rX\x89\xb3\x8ej\xa4\xc7\x9a"r\xa7\xad`\xdd\x98\x8d\x98\x9b\xde\xac\x898\xac\xa8\xe59\xcd\\\x84<\x86\x85$\x8aJ\xe4\xcc\xe4\xd5i~n\xd5\xa2\x96\xac\xd50\xd3\xb7\xf5\x152\xc4An\xcd=\x8c\x99\x82"$\xd4\xc1p8\xadc\xa7\xed\xedQ\xb5\xa6\x05U:\xd1\x96\xc2t\x9a\xdc\xce\xdaOQQ\xb2\xed\xfa\xd5\xc7B\x9dEW\x93\xe6\x15\xdf\x1bX\x8d\x88\x19\xea#\xb6\x95\xd6\xab\xc8\xc4t4\\\xa8\xca\xc3\xd9@\xc9\x15\x10`O\xa5 \x90\xfb\xd3\x8cd\xfc\xc2\x99\xaad\xf1\x00N\rY\x10\xf7\x15\x04#\x1dj\xe2\xb8U\xebV\xacZh\x8c\xe4\x0cSU\x9d\x0f\\RK.[\x03\xad*\x82G<\xd3\xe5E\xf3\x16#\xba\xdb\xd4\x9a\x99\xaf\x00\x15Mc\xcfb*\tr\xbc\x1a\x89\x19NE\xd6\xbe>\xb4\xcf\xb6\x16\x1dk7\'4\xe1\x93X\xb4\xd9\x83\xd4\xbf\xf6\x9c\x8ak\xc9\x9e\x95T#S\x82\xb0\xa9q!\xa0c\xb8\xd2\xc7\xb9{\xf1@\x1e\xb4\xe1SbI\x15\xca\xf7\xab)pET\xed@404\xa3\x9c\x93\xd6\xacGq\xce\reF\xd8\xefV\x10\x90\xc0\xe6\xa5H\r2r)\xa0S\x15\xbeZ\x907\x15\xa2`\xc6\xb0\xa8\x9cqOf\xe6\xa2v\xe2\xa5\xa1\x15d#&\xa1-K#|\xd5\x189\xa8\xb0\x84f\xc5FrMHA=\x05&\xd3\xdcS\xb0\xc8\xd9\x0f\xadDc\xc5X \xd2m4\x0c\xa8\xebU\x98b\xb4\x1d8\xa8\xfc\x9c\xf6\xabLh\xcd*X\xe0\x83R$\x04\xf4\xab\xa6\xdf\x9e\x05O\x14\x1c\x0e*\xaeU\x8c\xc3\x03\xb7\x14\x1b!\xde\xb7\x16\xd8c\x9cR\x98\x00\x1c\x8aV\x03\x9d6eOL\xd3\xd2\x12\x0e\rl\xb4\x0b\x9e\x05B\xd0\x0c\xe7\x15\\\xa0A\x04f\xac\xaa\xe0\xd3\x84{G\x14`\x8e\xa2\x9d\x8aH]\xbc\xe6\xa7L\x00=j\x00y\xa9\x93\x9ejX\x13\xa8\xa4\x90|\xb4\xab\xd2\x91\xfe\xefZH\n\xf5"\x01Q\xe3\x14o\x03\xbdh\x98\xaeX\xe9\xf4\xa9\x03\x008\xaa\rq\x81\xd6\x98\xb7X<\x9a\xa4\xd0\xd4\x8d \xd9\xe7\xb5#H\x00\xaa\x8br6TR\\\x82p(r*\xe48\xa7)\xc0\xa4V\x04R\x1e;\xd6M\x12+\x1ex\xa6\xb1\xe2\x93\x06\xa1\x90\x9c\xfbT4\x03\x1cd\xd3\xa3Q\x9ei:\xd2\x8e+7\x01X\xb9\x0c{\x88\xad\xab(zVE\xa1\xc9\x15\xbfh1\x8a\xe0\xc4\xb9%dt\xd1\x8a\xb9~(\xc7\xa5ZT\xf6\xa6\xc2\x01QS\xe0\n\xf1jN\xa2g\xa5\x18+\x10\xba\x0csUdE5nW\x18\xac\xf9\xdc\x01\xc1\xae\xec%FcV\x9a)\\\xaa\xe0\xd6d\x84sVnn0H&\xb2f\xb8\x19\xe0\xd7\xd0R\xbd\x8f:p\xd4$bO\xb5BW\x93Q4\xf94\xf4rkC;Xw\x97\xcdH\x17\x0b@$\x8ap\xcd=JC\x93\x1d\xe9I\xc7C@\xe0SX\x0cqM6;\x913\x1d\xe3\x9a\xbb\x1f\xdd\x15\x9f \xee*h%#\x86\xab\xb8\xd4\xcd\x05l\xd47h\t\x04R\xac\xca\x05G#\xf9\x86\xa6\xe1)\\\x84 \xa9\xa3\x87=\xa9\x8a\xbc\xd5\xd8@#\x14\xaeJC\x16<v\xa51\xe4U\x9d\xa2\x9a\xc0Re4Tx\xfd\x05G\x8a\xb0\xec\x05V2\x0c\xd6l\xc9\xa0\xa4\xce\r5\x9cb\xa2g\xe7\xadC&\xc5\x95|\x1a\x9d%\xcb\n\xcf\xf3*H\xe4\xe9S`6\xa3~\x05K\xbf\x02\xa8E/\x02\xa5\xf3x\xebT\x98\x12\xb4\x9c\xd4/ \xa6\xee\xe6\xa2\x91\xb8\xa7qX\x85\xdcn4\xe4\x19\xaa\xec~j|ljz\x85\x8b<R\xe2\xa3\x06\x9e\x0e*\xae;\x08\xcbQ5LO\x15\x0b\x9eh\xb0\xd2\x1b\x8c\xf5\xa7\xaa\x8fZ\x88\xb6\r*\xc9\xea*\x92-D\xb0\x10S\xd5\x005\x12H:S\xf7\xf7\x06\xae0\x1f)aE+\x01U\r\xc1\x02\x98\xd7\x19\x15\xaa\x80X\x9eM\xa3\x8e\x95U\xda\xa3\x92r{\xd4E\xf3I\xc4V-+\x0cP\xcc*\xb2\xbd)oz\x86\x04\xe1\x85J\x8d\x8ej\xbaT\x85\xbe\\T0\'\x12\xfb\xd3$\x93\x81U\xcc\x80Tm6x\xa9\xb0\x89^N*\x07\x94\xd4O/=j&zd\xb2F\x90\x9a\x85\xde\x86q\x8a\x81\xdb\x8e\xb4\xaeI(\xb8e\xe34}\xa0\xd5R\xfc\xd3K\xe2\x8b\x8e\xe7\xff\xd9'
+__cat__ = b"\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x00d\x00d\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x06\x05\x08\x07\x07\x07\t\t\x08\n\x0c\x14\r\x0c\x0b\x0b\x0c\x19\x12\x13\x0f\x14\x1d\x1a\x1f\x1e\x1d\x1a\x1c\x1c $.' \",#\x1c\x1c(7),01444\x1f'9=82<.342\xff\xdb\x00C\x01\t\t\t\x0c\x0b\x0c\x18\r\r\x182!\x1c!22222222222222222222222222222222222222222222222222\xff\xc0\x00\x11\x08\x02\x00\x02\x00\x03\x01\"\x00\x02\x11\x01\x03\x11\x01\xff\xc4\x00\x1f\x00\x00\x01\x05\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x10\x00\x02\x01\x03\x03\x02\x04\x03\x05\x05\x04\x04\x00\x00\x01}\x01\x02\x03\x00\x04\x11\x05\x12!1A\x06\x13Qa\x07\"q\x142\x81\x91\xa1\x08#B\xb1\xc1\x15R\xd1\xf0$3br\x82\t\n\x16\x17\x18\x19\x1a%&'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe1\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf1\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xc4\x00\x1f\x01\x00\x03\x01\x01\x01\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x11\x00\x02\x01\x02\x04\x04\x03\x04\x07\x05\x04\x04\x00\x01\x02w\x00\x01\x02\x03\x11\x04\x05!1\x06\x12AQ\x07aq\x13\"2\x81\x08\x14B\x91\xa1\xb1\xc1\t#3R\xf0\x15br\xd1\n\x16$4\xe1%\xf1\x17\x18\x19\x1a&'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x82\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xda\x00\x0c\x03\x01\x00\x02\x11\x03\x11\x00?\x00\xf3\x85\x1f\x958u\xcd \xe4\nvNj\x069NjE<\xfa\xfdj!\x93\xceM<\x1ey\x074\x0c\xb0\xadS+s\xefU\x01\xf9\xba\xd4\xc0\xe3\xa1\xa0\x0b\x88\xff\x00\x9dL\x1f\x8fZ\xa6\xae\x005'\x98\t\xe0\xe6\x90\x16\x8b\xfb\xd2n\xf7\xa87\x9fZ\x0b\xf1@\xc9\xb7\xd3w\xf1P\xef'\xbd0\xbf\xa7\x14XD\xcc\xf9\xef\xd6\x98[\x9f\xea*\"\xfc\xf7\xa6\x97\xf7\xf6\xa6\x04\x86NqH_\x9e\xb5\x11|\x9c\xd3ws@\x13n\x00\xf6\xa4=j=\xde\xfc\xd2\x86\xe7\xff\x00\xafL\x07z\xfa\xd3G>\xdfJvA=\xa9\xa6\x80\x10\xe0\nLt4\xb9?\xfdzNh\x01\x0f?Ji\xc6})\xc74\x1a\x00\x8c\x8fjB0x\xa7\xb0\x04\xd2s\xc8\x02\x80\x19\xb4\x11A\xe7\x18\xa7c\xf2\xa4 \xf7\xa0CO8\xf5\xa4\xe3<\xd2\x9aN\xbe\xc6\x80\x0c\xff\x00\xfa\xa9\x05.1H1\xff\x00\xd7\xa6\x02\xf54\xee\xbd\xff\x00\nh<\xd2\x93\xc7z@)8\xef@<\xe7\x8afx\xe9\xc5\x04\xfaS\x02@E8\x1e*/\xa5\x19\xa0\t\xb3\x823Fj=\xfd\xe9sI\x81.\xe3N\r\x9e\xf5\x0e\xee:\xd2\x86\xc7\xf8P\x04\xa0\xe4\xf4\xfciA\xe75\x18n;\xd2\x83\xd0\xd0\x04\xa0\xd2\x83\xefQ\xff\x00:p\"\x81\x8f\xcf=;\xd1\x9e\xd4\xda\\\xd0\x02\x93\xc1\x02\x9aI4\x03\xff\x00\xd6\xa0\x8fj\x00C\xfeM\x00\x1c\xf5\xe2\x97o\xb59W\xa62(\x01\x02\xe0\xd4\x80c\x9csF\r8rh\x01\xbbx\xc7zp\x03\xd2\x9d\x80)B\xfa\xd2\x02<\x12i\xca\xbe\xd4\xf0\xb4`S\x016z\xd22{T\xa3\xa5\x0c3\xc8\xe0\xd0\x05fP\x07\xb5D\xcb\x8e\xf5e\x87\x15\x03\xf4\xf6\xa6\x04'\xe9Q\x93R04\xccs\xdcP\"\xa2\xf5\x14\xec\xfe\x19\xa6\x8ax \x9a\x91\x87L\x8c\xfe\x14\xbcz\xd2t\xeb\xda\x97\xfaP\x03\x94\xe7\xa8\xa9\x15\xc8\xe9Q\x0e\xbd\xa9wb\x81\x96\x15\xf8\xa9C\xfb\xd5@\xf8\xa7\x864\x80\xb2\x1f\x03\x9a\x0b\x8e\x87\xa5V\xdfK\xbf\xb5\x00L_=)\xbb\x8e\x06sP\xee\xf5?\xfdz7\x93\xd2\x98\x89Ks\xd7\xf3\xa6\x93\x9c\xf3Q\xee\x18\xe6\x90\x92z\xd1p\x1f\x9c\x93FO@i\xbf\xce\x8d\xdf\xfdz\x00\x7f4g\xebL\xcf\xe1\x9a\\\xf5\xf5\xfet\x01'>\xd4n\xfc\xa9\x85\xb1\xd2\x80\xdf\xa5\x00I\x9e(\xefL\x07\xdf\x9a]\xc3\x8eh\x01q\xf8\xd2R\xee\xcf\xe3IL\x04 \x8e\x9f\xad!\x19\xa7v\xedI\xce}(\x01\xbf\x874\x87\x04sN\xc6rqHq@\r\xedHE;\x04\xd2\x11\xc5\x02\x1b\xe9\xefF9\xa7\x11\xcfJi\xce\r\x03\x10\xfa\xd3\x0f^i\xe4dRc\x1d\xfa\xd0!3\xcf\xa9\xa3\xd2\x83\x92=(\xe8?\xa5\x00\x14~4\x84\xe3\xa7JO\xc8S\x01\xfd\xb1\xde\x93\x9e\x94\xdc\xf6\xa5\xcfz@?w\xa74\xa1\xaa,\x92zS\x81\x00s\x9a\x00\x9bp\x14\xa1\xbaqQ\x06\xe3\x1d\xa9Cs\xeb\xed@\x13\xee=\xa9\xdb\x87\xff\x00\xae\xa0\x07\x8a\x90R\x19/_\xa5/N*0{R\x83\x8c\xd0\x03\xf8\xfci\xfe\xfd\xea1\xedO\x07\xb50\x14/zU\xf4\xa34\x00\x7f:\x00p\xa7\x8e\x9cSA\xe6\x9c9\xee(\x01\xc0`\xd3\x85\x03\xa6;\xd3\xf8\xc5 \x10.i1\x8e\xdf\x95?\xa1\xa4<\xd010;P:Q\xc8\xa0\xf4\xfe\x94\xc4D\xf9\xc5@\xeax\xa9\xdc\x8f\xa5FG\x1c\xd3\x02\xbb\x0e\xfc\xd3\rJ\xd4\xccP\x80\xa0A\x1d\xe9\xc3\xafsM\x1e\x83\x14\xa3\x1e\xb9\xf7\xa8\x01\xdd)\xd8<\x1ah\xe8)h\x00\x07\x19\xa5\x18\xc1\xff\x00\n\x01\xc0\xa5\x03#\x9a\x10\x05\x03\xf1\xa0\x8e\x94\x94\xc0v{\xd2n\xfa\xd2v\xff\x00<\xd2w\xe7\xa8\xa2\xc0.\xec\xd3s\x8a\t\xee)\x0e?\x1a\x00]\xc0\x8e\x94\xa1\x86?\x953\xa0\xc5\x078\xc61@\x12\x06$\xf2isQ\x83K\xd7\xebE\x80x>\xfc\xd2\xee\xe2\x99\xf4\xfc\xe8\xcf\xe1@\x0e-\xc5h\xc7e%\xd2\xa4\x90/\xc9\x8c\x1e3\x8f\xaf\xadeg\xe6\xf5\xafM\xf0N\x88\xefd\xb2g}\xb3\x90\xe3=G\xb5R@pW:m\xdd\xae\xd2\xf1\x90\x1b\xa6j\xab\x87\x8d\xb0\xeaT\xfa\x1e\xf5\xee7\xcb\xa1\xc6\xa6\x1b\x89#a\x9d\xc5\x0f85G\xfb?\xc2z\x8cK\x13D\x9f(\xda\x0f\xf7s\xff\x00\xeb\xa3@\xb3<p>)\xc1\xf3^\x8b\xab\xfc/\xf3\x15\xae4[\xa4\x91J\xfc\xb0\xc8q\xcf\xb1\xff\x00\x1a\xf3\xfb\xed/P\xd3%1^\xdaK\x03\x02G\xce\xbc\x12=\x0fq\xefI\xa0#\xde:\xe6\x97#\x07\xde\xa0\xdd\xcf8\xc5<?J@Hs\x8e\x94\x9c\xd2\x06\xcd(4\x00\x1c\x7fJ1\xcf\x14\xb8\xe9\x8a)\x80\xdcc\x8aLS\xb1F8\xa0\x06\x11\x8aB;\n~:PGz\x00\x8c\x8ai\x18?\xca\xa4#9\xe2\x9as\x9a\x00a\x1di\xa7\x1c\xf6\xa7\xe2\x9aG9\xc74\x08Bx\xefE\x14\x87\x81E\xc02\x7f\x1a;\xfb\xd0\x07\x14\x11\x81\xeb@\xc7\x13\xda\x9c\x0ei\x83\xad;\xbd\x00J\x0f\xf2\xa7\x03\xc7Z\x88\x1e)\xfd\x0f8\xa4\x04\x80\xe6\x9d\x8f\xad1MH\x0f\xd7\x14\xec\x03\x87\xaeE=y\xa6\x0ei\xe3\x81@\x0b\xdf\x14\xeah\x00\x1c\xf6\xa5\xc8\xcd\x008c\xf0\xf5\xa7\x83\x8f\xa5G\xfc\xa9\xdb\xb1\xd6\x80&\x1fZx>\xf5\x08n84\xbb\xe8\xb0\x13g\xde\x93 \xfd)\xd6\xf6\xf3\xdd>\xd8by\x1b\xfd\x91\x9a\xe9\xac|\x07\xac](yU!\x8c\x8e\xac\xdc\xfeT$\xc0\xe5\xff\x00\x1ak\x1f\x94\x8c\xd7\xa3\x7f\xc2\xbe\xb6\xca\x01;\x16\x03\x9f\x9b\xa9\xac\xcb\x8f\x02K\x05\xc3\xb1u\xf2FO\xae)\xd8.q\x07$\xd22\xa2\xaf<\xb6:\x0e\xd5\xa5\xa8\xc0\xb6sI\x1cy)\xdd\xb1Ym\xf36z}(\x11\x11\x03\xb7zi\x18\xa9M7\x00\xf7\xe2\x81\x99T\xbf\x857\x8c\x8fjw\x1c\xe6\xa4\x05\xef\xf4\xa7q\xe9M\x07\x9c`R\xe7\x9a\x00vx\xff\x00\x1a>\x9c\xe2\x81\x83A\xfcq\xefH\x05\xeb\xc9\xe2\x8e\xa3\xd6\x8c\x8f\xfe\xb5!\xc7j`\x18\xa4\xed\xd2\x94\xf54\x878\xa0\x06\xfdy\xa0\xd2\x9aLt\xee(\x01\t\xf5\xe9E\x04\x80M'a\xebE\x80^\xde\xf4\xb9\xf7\xe4Rf\x8c\xd0\x02\xe7\xda\x9aXb\x82}\xea2\xd8\xf4\xa6\x06\x9e\x89bu-J\x1bu\x04\xbb8#\x07\xa0\x1dk\xd3\xf5\xfdz-\x03O\x8e\xc2\xd5@\x98\xa0\x0c\x17\xb5r\xde\x02\xb6\x92\xd2\x0b\xfdZH\xc8\x8a(\xf1\x1b0\xe0\xb7\xb1\xac\x1b\xbb\xc9\xb5\rBIdfb\xcd\xd6\xa6R\xb2\xd0\xb8F\xecK\xcb\xdb\x9b\x89\x0b\xbb\x10\x0f=M6\x0b\xab\xabw\xdd\x1c\xad\xc9\xce\t\xad+{\x10\xca;\xfdjq`\x0f@:\xfaV\x0e\xa1\xd2\xa9\x97\xf4\x9f\x18\xddY\xba+\xc9\x95\xee\x0eF?\x1a\xef\xed\xb5m\x1b\xc4\x96\"\xd7S\xb7\x86t~>n\xc7\xd8\xf5\x1f\x85y\x89\xd2\xd7k|\x84\x8f\xc8\x0f\xc6\xabZ\xb5\xe6\x99v\xaf\x10;\x07P\x0fZ\xa8\xd6\xe8\xc8\x95\x1b\xecv\x1e&\xf8v\x91\xaf\xdb\xb4\xe9\xa4\x9a\x01\x80\xd1\xed\xdc\xea\xbd2=q\\\r\xe6\x91qj\xc5YI\x00\x90\x18\x03\x86\xc7\xa5z_\x87|Y47)\x15\xc6\xf4W=\x18q\xf8\x1a\xeb\xb5\x1d\x1e\xc7\xc4v\xce\xcb\xe5\xa4\xa5s\x9fV\x1d\x0f\xd7\xb7\xbdT\xa2\xfe(\x18Z\xda3\xe7r\xac\x84\xe6\x80\xd5\xddk\x9e\x0f\x9a\xdbs\x08\xc8\xc1\xe7\xd8\xd7\x17ug%\xb3\x90\xcaF\x0fzT\xea\xa9h\xc1\xc4`l\xe7\xd6\x9d\xd7\xe9P\x83O\x075\xa9#\xfb\xe6\x94t\xcd0\x1cS\xa9\x80\xb8\xe3\xad7\xafJv9\x19\xa3\xe9H\x06c\x83HE?\xe9\xd7\xbd4\xf5\xa7`# \xe7\x8e\xb4\xd29\xa9q\xc74\xd2\x07\xa6(\x02&\xfdi\x0f\xbdI\xb7\x9fjL\n\x00\x8c\x8a)\xd8\xe9K\x8a\x00@03\x8aQ\xd7$\xe2\x97\x1e\x94\x0e\xb8\xa0B\x81\xc6qO\x18\xe7\x8ag^\xbdjE\x14\x0cx<\xd2\x81\xd0\xfa\xd3@\xa9\x00\xc51\n=\x7f\x9d;=\xe9;\xfaR\x1a\x00\x93\xf1\xe9\xc5/s\x81\xf8S>\x87\xf1\xa5\xdd\xe9@\x0b\x9cb\x90\xbf>\xd4\xc2\xde\xe6\xad\xe9:U\xde\xb5~\x96\x96\xa9\xb9\x8fV\xec\xa3\xd4\xd1`\"\x8f|\xb2,q\xa9gc\x80\x00\xc95\xddh>\x01\x92d\xfbF\xb4\xfeD]\xa3\x07\xe6?_J\xe84\x8f\x0c\xe9>\x17\x8cO.\xdb\x8b\xe0\xb9.\xfd\x17\xe9\xe9X^!\xf1=\xc5\xcb\x94\x84\xe4/`p(vE(\xb6uM\xa9\xe8~\x1f\xb5\xf2,\xd6$\x08;u?Z\xe4\xf5O\x1aMpH\x81\xc8\x1d\x82\x9e\xb5\xc9:\\\xdd\xca\xce\xe5\xb7\xb9\xcex#\x14\xe1b\xc8\xbc\xe7\xe9\x8a\xc5\xd5\xbe\x86\xf1\xa3\xd4\xba\xbe-\xd4a\x97>a\x18\xef]o\x87|g\x1d\xe8\xf2.\xdf,x\xf9\xb8\xcdp-i\xd7r\n\xac\xea\xf6\xd2y\x91\xe5H>\x94\x95@t\xce\xff\x00\xc7\x1a|kj\x970\x02!<\xe1z\x12zf\xbc\xf8\x9ex\xae\xfe\xcfP:\xcf\x84nm\xdcn\x924\xc8\xcf=+\x82 \xab\x10W\x15\xd1{\xab\x9c\xed\r8\xefG&\x83\xd7\x9e\xd4\xa3\x9e8\xa4\x06??\xad\x1d\x7f\xc2\x9cE\x01}\xbe\x95 4c\xb94\xe1\xd7\xf1\xa4\xe6\x83\xea\x05\x00;<sK\x9ah<})A\xe3\x8f\xa5\x00:\x8e\xfd3M\x18\xc0\xa5\xdd\xd3\x9a\x00(\xc1\xcd\x1e\xa0Pz{P\x01M>\xb4\xee\xbe\xb4\x87\x93\xd7\xf3\xa0\x04#\x02\x98x\xfci\xfc\xe2\x9az\xfa\xd0\x02u\xe0\xf3Hx\xa5\xe2\x9a\xc7\x19\xf5\xa6\x03Kt\xe9]_\x81\xfc\x1d7\x8a5\x0f2PR\xc2\x12<\xc7\xfe\xf1\xfe\xe8\xaesM\xb0\x9bU\xd4\xed\xecm\xc6\xe9fp\x8b\xf8\xf7\xaf\xa7<5\xa1[\xe8\xbaM\xbd\x8d\xba\x80\x91.\t\xc7\xden\xe4\xfe5\x9c\xe5w\xca\x86\x91\xc9\xf8\xe6\x0bm\x0b\xc2q\xdb[D#F\xf9B\x8e\x9d+\xc7\xedRFl\xf5\xe6\xbdo\xe3\x0c\xc2;KK|u\x0c\xc2\xbc\xa7M\x1f6\xd3\x9e\xbdA\xac\xea{\xa8\xe9\xa2\xaetv0\xe5A?\x95_\x16\xeb\x92\x7fAU\xed\x18\x04\xf4\xabh\xe0\x1ey\xac\x13:\x1a\x1a &6\x04|\xb9\xe9P\xcb\x12\x95\xd9\xb7\x1e\xe4U\xe5\x91I\xc9\xe3\x1d\x07\xad0g~y\xff\x00\x80\xf3N\xe4\xeacK\tL\xae\xe0s\xdc6\x08\xfc\xeb\xa2\xd0\xbcC6\x9a\xea\x92\xb9h\xf3\x8c\x9e\xa7\xf0\xac\xe9\xd6)\x1bk\x02\xa4\x9e\xa2\xa9]\xdaJ\xad\x88J\x9fC\x9e@\xad!6\xb6\"pR=\x8a\t,\xf5\xbb\x10\x0e\xdc\xbf;\xbf\xc6\xb8o\x15x4\xa8wH\xf0}\x85dh^\"\x9fM\x99c\x92]\xc1x*\xc7\xadz\xc5\x95\xed\xbe\xaff\x8b!\x05Yx>\x95\xa5JJ\xa2\xe6\x8e\xe76\xb1vg\xcd\x17\xf6RY\xce\xc8\xe3\x15]Mz\xc7\x8f\xbc%\xe4\xab\xcf\x12\xe4u\xc8\x15\xe52\xc3-\xb4\xbe\\\xc8Q\xb0\x0e\x0fq\xebJ\x8dG%g\xba&HP~\x94\xb9\xe7\x9a`<S\xb3\xc5nH\xf0G\xe9JO\xa50{R\xf4\xa4\x03\xb84\x84R\xe4v\xa44\xc0a\xe0\x9aL\x1c\x80H4\xfc\x0e\r8&#.G\x19\xc2\xff\x00Z@DG4\xd2)\xe4g\xd2\x8d\xb9=\xe8\x0202h=3Rm\x02\x8d\xb9\xea)\x81\x1e9\xcd/9\xf7\xa7\xe3\xf2\xa4\x0b\xc6h\x00PI\x03\xbd.1N\x8f\x97\x14\x01\x8ei\x80\xa0S\x87\xe7\xf8P\x01\xc5.;f\x81\x06y\xf6\xa5\x1di:w\xcd\x1c\xe3\xa50\x02p:\xd3Ks\x8aBy\xad\xcf\x0cxb\xe7\xc4\x17@\x90c\xb4C\xfb\xc9z~\x03\xde\x90\x15t=\x12\xf3_\xd4\x05\xb5\xb2\x1d\xa0\xfe\xf2Lp\x82\xbdb\xda\xd7M\xf0\x96\x9e \xb5\x0b\xe7c\xe7\x90\xf5cQM\xa8\xe9^\x1c\xb26Zr \xda\xb8;z\x93\xeek\x8c\x9a\xf2MN\xed\xe6\x95\xbe@p\x8az\x1f\x7f\xa5L\xa6\xa2i\nnA\xa9kWz\x8d\xc3\xa4r|\x84\xf5\xe9\x9a\xab\x06\x9a%9\x91\x99\x9b\xaf\xb5]\x82\xdd\x81\x19\x03\xe8\x07\x15} B2\x18\x83\xe9\x9a\xe5\x94\x9bg\\b\xa2\x8a\xb1\xd9\x08\x90`\x00:P\xd0\x0eMh\xf9*\xe9\xb89\xe3\xda\xab:\x90}=\xfdj\x1a-J\xe6|\xd6\xbb\x94\x9a\xc4\xbd\x0b\x18*\xd9\xc8\xae\xa4\xa8\xc1\xe3\x9a\xe7\xf5\x98\xc0S\xd74\x93\xd4m]\x1a\x9e\x0e\x97\xf7S\xc4\x87\xe6te\xe4{\x1a\xe6%,e;\xba\xe6\xb5\xfc\x1f#\xae\xa2\xa9\x9c\x9c\x81\xf5\xac\x9b\xa0\x16\xf2e\x1d\xa4 ~u\xdd\x07\xa1\xc15f3\xb7&\x97\xad \xc6)qTA\x97\x8e=\xe8#\xd7\xad(98\x1f\x8d(\xe9\xd3\x15 3\xd0\xd1\x80pr\x7f\x1a~=\xa9\xa4c\xd7\xf1\xa6\x031\xd6\x97\x1d\xe9\xc3\xe9@\xe6\x90\r\xa0f\x9d\xb7\xbd&8\xeb\xf8\xd0\x02sFh\x1e\xb4c\x9f\xa5\x00\x14\x9c\xd2\xf6\xa6\xe0\x03\xcd\x00/9\xfe\xb4\xd3\xefJy\xa3\xbd0\x18I\xa8\xdd\xaaF\xf5\xc5B\xc4f\x80=+\xe0\xf6\x92.5[\xadJE\xcf\x90\xbe\\y\x1d\x18\xf5?\x97\xf3\xaf{\xb6\x8c\"\n\xf2\xef\x84\x16\xc2/\r\x991\xf3I)&\xbdZ1\x85\x15\x8d=[~cz#\xc9~3\xc6\xc0\xd9I\x9f\x94\xa1\x1f\x8ek\xcbt\xd6f\x9b\x038\xcf5\xed?\x17\xac\x05\xc6\x87o>y\x8d\xca\xfec\xff\x00\xad^1\xa6\xe29\xb1\x90ib\x16\x97:p\xe7I\x0bm\\~\xb52\x93\x92Fj\xbcg U\xb5 u\x195\xc6\x8e\xb6=\x0f\xbf\xd4\x9a\x9dH*rJ\xa8\xa6D\xa0\xb7 \n|\x8e\xaev\xe0\xe0{\xd6\xb1\xb5\xaef\xef{\x0c*\x1b\xa0\x04z\xb7Zc\xa8e(\x01RGP)D{\x9fh\x07\x1e\xb9\xa9\n\r\xb8\xdc\xa4u\xe7\xa8\xaaZ\x92\xcc\x0b\x95x%\xdc@\x0c\x0es[\xde\x17\xf19\xb7\x95\xa12\x10z\x84=)\x84C2\x951\x16p:\xe3\x8a\xc6\xba\xd3\x04\x92+\xc4<\xb9S\x90G\x7fj\xd2\x17\x8b\xdc\xceiI\x1e\xdboqm\xae\xe9\x86\t\x8eC.\x01#\x90}+\xc3|s\xa3\\\xe9\x9e$k\x7f%\xb6\x88\x86\xc2\x07\x0c\x00\xc9\xfc\xb3]&\x81\xe2I\xb4\xc9U'\xce\xc0pN8\xc5v\x9a\xd5\x9d\xb7\x8a\xfc>\xe65\x8d\xae<\xb3\xe5H\xddA#\xa7\x1c\xfbV\x8e\t\xcb\x9dns;\xad\x0f\x02\x85\x94\xb8\x04u\xa9&\x01&`\xbc\x0c\xe4}:\xd3\xef\xf4\xcb\xad&\xfcAs\x13+u\x00\xf3Q\xdde%\\\xe7%{\xfbqZ\x12 \xa5'\x07\x81Q\x86\xe2\x9c\x0esH\x07g\x9f\xc3\xa5/n)\xa3\xdcf\x9c\x0f\x1di\x80\xa0\x16!@\x07<\x0cU\x8b\x9c\tD)\xf7\"\x1b\x01\x1d\xcfs\xf9\xe6\x8b%\xcd\xc9\x93\x1cD\xa6C\xf8\x0e?\\T9\xcbd\xf2OZ@\x18\xfc}i1\xea)\xfd\xf9\xa4\xc7\xb5\x003o\xebK\x8a~:\xe7\xa5\x18\xa6\x031\xcfJ\x00\xf6\xe6\x9f\x8aLP\x02\"\x80x\xa5\x0bRF\xbf{\x1d\x81\xa0\x0eh\x01\x00\xe6\x82:\xd3\xc0\xc5&\x00\xaa\x10\xd3\xc7^=\xaa3\xd7\x8ay\xcfj\xe8<5\xe1i\xb5\xe9\xf7\xb81\xda\xa1\xf9\xdf\xd7\xd8P\x04~\x15\xf0\xbc\xda\xfd\xe0\x92@R\xca3\xf3\xb9\xfe/a]\xe6\xaf\xab\xd8\xe8\x96_\xd9\xd6J\x11\x15q\xf2\xd5}WW\x83F\xb4\x1af\x9a\x15QW\x04\x8a\xe4\xa5f\xb9\x90\xb3\x1c\xb9\xe4\x96\xf5\xa8\x94\xeck\x08\\Cs\xf6\xf9[s\x83\x19\xceA\xefW-\xed\x84C\x00\xb6\xdfV\xa6\xdaXD\xaa]\x8e\x18\xf5\xf4\xadX\xed\xd3\xcb%\\\x1cw\x03\x15\xcc\xef'vu+GA\" \x002\x06{f\xac\xa2\xaep\xe3\x03\xd6\x98\xb6\xe3\x1b\xb7dT\x882q\x8c})r\xdbpn\xe2\xce\x98\x8b\xe5 \xe4\xe3=*\x8b\x12~V\xe0\xfb\xd4\xb3;|\xf1\x93\xf2\x9c\x11P\x86p0y\x15\x9c\xa5\xa9q\x8e\x82o\xd83\xe9X\xba\xbc\x8b/\x0cq\xe9[\x12\r\xcb\xe9\\\xfe\xae\xbb\x17'\xf0\xa8[\x95m\x0b^\x1a\x88Es$\x99\xfb\x80\xb6~\x82\xb0\x99\xdaGf'%\x8eMt^\x12\x8c<7\nN\x7fv\xe3?\x85s\x8c\xa5\x18\xa9\xecq^\x8c>\x14y\xf5>!E(a\x9af~\x94\xb9\xcf\xadQ\x06vA\xf4\xa5\x1cqJT\xe3\x8aL\x10*@PE ?\xe4Rw\xa3?OZ\x00(\x1c\x1e\x94\x0e\xb4w\x14\x00\xfe\xa2\x93\x19\x02\x94s\xdb\x8a\\g\x91\x9a\x00c\x0e)\xbb*S\x9e\xf4\x01@\x10\xe3\x02\x93\x198\x15)_\xf3\x9aa\x1f\xe4P\x03\t\xf7\xa6\x9c\xe0b\xa49\xa6\x1f\xd2\x80\x1a\xff\x00\x85@\xfdNj\xc1\xa8$Z\x00\xf7\x9f\x84\xd3+h\x0b\x1e~\xe9\xcdz\x92}\xd1^\x17\xf0\x8fS\x11\x96\xb6f\xef\xc5{\x9c-\xb9\x01\xach\xbd\xd7a\xc8\xe4\xfe$\xc0f\xf0\x9c\xa4rU\xc1\xaf\x9fm2\xb7\x9c\xf5\xcf5\xf4o\x8e\xe3\x12xJ\xf3\x81\x95\xc3\x0c\xfdk\xe7\x00\xd8\xbd>\xb9\xe4U\xd7\xf8Q\xbe\x1c\xe9\xa2n\x95u>ldVT/\xf2\xae+b\xdb\x1b{\x9a\xf3\xd9\xdc<#\x9f\x95T\xfe\x15*[>\xe1\xbc\x80\xbd\xc1\xa9Cv\x1c\x0fjk\x06\xfe\xf1\xc5\\Hw%\"-\x9bPn\xf6\x03\xa9\xa8^3\xb3,6\xaf\xa2\xd3\xd2ha\x03/\xcf\xb0\xff\x00\x1a\x98I\x01\xf9\x96Wf\xf4\xceq]\x0bS6\x99\x9a\xdb\x87\x08\x8f\x81\xeej\xab\x84\x0e\x1fs+\x0e\xc6\xb5\xa4{\x97\x05U\x00S\xdc/5N]:`\xa5\xdd\xb1\x9fl\xd4\xb4\xfa\x02k\xa9F\xe5TD\xdb\x89\n\xc3\xa8\xedW|%\xe2+\xad.\xf7\xc8\xb8m\xd6\xcf\xdf\xd2\xa9\xa3\xb4,\xf1I\x87\x8d\xbb0\xe4VCB\xd0^\x12\x0b\x15'#\x8e\x95\xac's*\x90G\xadk\xfe\x1c\xd3\xfcU\xa4\xb4\x89\x91:\xfc\xc9$g\r\x91\xce3\xf5\xaf\x1a\xf1-\x84\x96W\x8c\xb2\x1f\x9d\x1c\xab\x021\x81\xc6\x0f\xe8k\xd0|3\xe2&\xb4>T\xcf\xba2\xdbqZ>0\xf0\xd4\x1a\xe5\x8b^Z\xa82\xe0\x12\xd9\xea?\xc9\xadZ\xea\x8ev\xba\x1e(\xadR\x03K5\x94\xd6\xd2:8#i8\xcfq\x9cf\xa2S\x9e\x94\x12M\x9e\xd4\xf3\x90\x004\xd4\x19\x94)\xc7\xad=\xdb+\x9c\xf54\x01f\xdf\xe4\xd3\xae\xa4\xee\xc5#\x1f\x89,\x7f\xf4\x11P\x8ez\xd4\xceJ\xe96\xeb\xff\x00=%\x91\xff\x00\x00\x15G\xf5\xa8\x17\xa78\xa6\x03\xcd.08\xa3#\xd7\x14\xb4\x00zR\xd2z\xe7\xbd(\xa0\x04\xa2\x94\xd1\xdf\xd0P\x04\xf0/\xcb9=\x16?\xea\x07\xf5\xa8\xc0\xab\x10\x8cY\xdd9\xee\x15\x7f6\xcf\xf4\xa8\x06@\xa6\x02q\xff\x00\xea\xa4'4\xf0\x01\x07$\x0ct\x04u\xae\xa3\xc2~\x1a}B\xf5.\xaeS\xfd\x15\x0ey\xefLE\x9f\x07xDj*o/\x94\xac\x03\xa0=\xebw_\xd7`\xd3m\x1a\xc3L@\x9bF\xd2Tc\x15&\xbd\xafCin\xf6V,\xa8\xa86\x9d\xbd\xab\x80\x9d\xcd\xdc\x86C3}1\xc1\xa8\x94\xbb\x1a\xc2\x17\x1c\x90\xbd\xc3\xf9\xd7.\x0b\xb7$f\xb4\xed\xa2\x19\x00 \xeb\xda\xa0\xb1\x11\x9c\x00\x8d\x9fj\xd9Hd\n:\x80z\x1cW=\xdb\xd4\xea\xd1h9F\xc0\x01\x8c\x13\xee)\xf8W8e\n\x0f\xf7x\xa7\xaa:\x9d\xac\xca\xc3\xd2\x92[l\x8f\x92\x8fx\x12[\x02\xc2\xf0\x10P\x06_|\xd5\xbd\xb1O\x16H(\xdd2+(y\x89&70#\xa85v\tx\xc3w\xee*y\xfb\x96\xe9\xf5Eym\xe4R\x07P8\x18\xa6l\xe2\xae>\xee\xfcUy\x1f\x1c\x1a\xc5\x96\xaeV\x92\xb9\xddm\xc1\x8fgs[s>3\xd6\xb9\xad]\xb7\xcc\xbc\xf0\x0f<\xd3\x8a\xd5\x04\x9d\x91\xad\xa3H4\xfd\x16\xea\xe1\x98n\x11\x90>\xa6\xb9\xb2\xc4\x9d\xc7\xadoM\x19\xff\x00\x84i\xf1\xcf\xce\x99\x1e\x95\x8a\x91\x83\x8a\xf4\x16\xc8\xf3e\xab\x1a2jE\x8f#\x9a\x91c\xc7\xbd<-\x04\x99\xdb;\xd3\n\xd5\x8d\xb8\xa6\x15\xed\xcdH\x15\xca\xfau\xa6\x11\xf9U\x82\x80\xd3vb\x81\x90\xf7\xfaR\xf1\x8cu\xa7m\xcfo\xa5&\xdf\xc2\x9a\x10)\x1d)\xe0d\xe6\x99\x8apa\xf44\x00\xee\xc2\x828\xa7\x0c\x8f\xff\x00]\x18\xef\x8a\x00f\xdcsM+\xdf\"\xa5\xc6z\xd3H\xe6\x80#*:\xd3\x19Nx\xa9\x88\xf7\xa6\xe0s\xc6\x05\x00C\xb3\x8a\x8d\x92\xac\xf6\xe9\xc5F\xca(\x03S\xc2Z\xa9\xd2\xb5\xa8\xd8\xb1\n\xe7\x19\xcfz\xfacC\xd4\x12\xf2\xca6\x079\x15\xf2k)\r\x90y\xeck\xd6\xbe\x1b\xf8\xc8\x1d\x967O\xb6E\xe0d\xf5\x15\xcfS\xf7s\xe7\xe9\xd4\xa5\xaa\xb1\xea\xbe,\x84\xcf\xe1{\xf8\xd791\x121_1\\\xfe\xea\xf6A\xdd[\xf3\xaf\xab2\x97\xd6N\x99\xe2D*q\xee+\xe6O\x11\xda\xfd\x8f\\\x92\"07\x11\x91\xeckY\xb5(\\\xd2\x8b\xb3\xb0\xfb9KF\t5\xb3mq\x81\x82k\x0e0QW\x1d\x07\xa5[\x8d\xce3^|\x91\xe8D\xdak\xb0;\x8aQ)\x9b\xee\xf3\xeek\x15\x89lsZvav\xe0\x9c\xe7\xd2\xa5^\xe6\xb6I\\\x91\x96U9\x0e3\xed\xd6\x88\xa7\x9e7\xf9VL\xfd8\xfa{\xd5\x8f\xb0!9W|\x91\xd0\x11SEe2\x80\x00(?\xbc[$\xd7D\"\xcc\xa7R\"\xad\xcd\xf1\x1f\xeaO\xe3\xc51\x85\xf4\xb920E\xc7s\x8cU\xcf\xb3?\x01Y\xbd\xfbS\x99\x00<\x8c\xe3\xd7\xbdtr\xf79\x9c\xd7Ds\xd7\x91)~Y\x9d\xbdGJ\xa6\xea\x8a\xc0\xb1#\xd0\x13[7D\xb6I\x85I\x1c\n\xc7\xbb\x121\x04C\x86\x15\x16\xe5w\x1d\xee\x84\xb6\xc6\xe3$d\xefV\xafC\xf0\xd6\xa6d\xb7\xfb<\x87\xdb\x06\xbc\xf6\xc1\xc2\x02]~f\xe0\xfbV\xf6\x8bp\xa9y\xbdNx\xc8\xc5tE\x9c\xf2\x88\xef\x1cxc\xe67p 1\xbf\x0c\x13\xa8>\xb5\xe7\xf7V\"\x126g\xe5\xeb\x91\xde\xbd\xdd$K\xdbm\xacA\"\xb9\xfdCF\xd3\xae\x83F\xc8\x15\xbd1\xf9\xd3\xb7c=\xcf\"\xb7\xb7\x95\x91\xdc)\xe1x\xcd1\x81/\xb0\x0c\x85\x1c\x9a\xf4\xb9\xbc/o\x1d\xa3\xban\xe7\xdb\xa5s\xb7>\x1d\x9a)\xa4\n\xbc7<R\x0b\x1c\xfd\xdb\x15\x8a\xce/\xee\xc1\xbb\xfe\xfafo\xe4ED\xb5\xbb\xa9\xe9B;\xb2\xac\x87(\xa9\x1e3\xd7j\x81\xfd)-\xf4\x85;N;\xe3\x06\xa9+\x93c\x1f\x9e\xe2\x9e\x838\xad\x8b\x9d81\xf9\x06\x0f\xb59ti\x04\x1b\xb1\xcfj,\x06\x198'\xebN\xf7\xedZ\x07H\x93if\xeaz\nH\xf4\xf7bT\xa5\x16\x03<\xf3\xd7\x18\xa5\x03\x15z]=\xa1\xea?\x1a\xae c\xc6>\x94\x80\xb1l\x03i\xd3\xafv\x91\x00\xfc\x15\x8dV\xe4\x00qZV\x96\xcc \x84w7 \x7f\xe3\xb5\xafg\xe1\x7f>\x1c\xb3`\xe6\x98\x19\x1a6\x9b&\xa7\x7f\x141\xaeF\xef\x98\xe3\xa0\xafO\xd4&\x8bI\xd2\xc5\xb4@&\x13\x1cU\x7f\x0fh\xf1hv\xc6R\x01\x95\xc7Z\xca\xf1\x04\xe6\xe9\x9c\x06'\x03p\xe6\x86TQ\xca\xeaRa\x99C\x12\xd26MEkl]\xd4\xab.=2y\xa7<.c\x0f\xb4\xee\xef\x9a\xbf\xa7[.\x00en}k\x9eM\x9dQH\xb9k\x02\xa2\x0f,\x84\x7fJ\x98\xc92\x1cI\xc9\xedN\xf2\xcc@m\x1f\x9d=fv\x01d\x8c\x10=\xf9\xa5b\xc6Gt\xeapW&\xa7\x17\x83p\x04\x15\xfc)\xe9\xe4LB2\x1c\x9e\x00\xc6ig\xd3\xcer$<\x7f\n\xf2O\xe3\xd2\x9d\x9aZ\x02\xe5\xbd\x98\xe6\xc4\x80\x13\xb6D\xfdG\xd2\x93\xc8~\nFY\x7f\xd9\x15^;\x86\x88\xedX\x02{\xb7\xcc\x7f\xc3\xf4\xabF\xe1\xa4\x88\xab9#\xb7<\x8a\xc6R\x8c\x8dTe\x10nW\x9e\x08\xeck*\xe9\xd8>3R\x19\xfc\xb9H\xcf\xd6\xab\\J\x1d\x8f\xa5er\xdclA+|\xa6\xb9\xbb\xd2~\xd2I\xfaV\xec\xef\x95<\xf1\\\xfc\xd9\x9a\xe8'\xbdi\x05\xa9\x94\xf4F\xc3>4\x07S\xc9g\\Vj\xafN\x95\xa3z\xa6;X \xe3\x9f\x9c\xff\x00O\xebU\x15q\xfe\x15\xdcy\xcd\xea3\x14\xf5^\x9djE\x8c\x9a\x95c\x03\xeb@\x8c\x9ac\x01\xf5\xa5&\x98Z\x90\xc4\xfeT\xd3JN;\xfeT\x85\xb8\xe7\xa5\x02\x18@'4\xceA\xf7\xa7\x91HG\xa9\xa41\x98\xe7\xda\x81N\xc6Fi8\xc0\xa6!\xe3\xa5.qH\x0eNqK\x8a\x10\t\x9fz\x1a\x8e\x83\xad\x04\xe7\x1c\xd3\x01)\x08\x1c\x9a^@\xa44\x00\x87\xa6E4\x83O\xed\x83\x8aB\x07\xff\x00^\x80\"d\xcd6'\x96\tVHY\x96E\xe4\x15\xea*b?:\x8c\xaf\xe1I\xa4\xd6\xa0\x8f[\xf0'\xc4\xc4\xdd\x15\x86\xaa\xdb\\\x90\xa2Bx5\xce|N\xb3\xfb?\x88&\x91@\xda\xd2o\\\x7ft\xd7\x08\x0e\xc7\x0c\x008\xecz\x1a\xf4\x7f\x11\xba\xeb>\x13\xd2/|\x97\x8d\xbc\x8d\x87\xcc\x19$/\x00\xe7\xbfJ\xca4\xb9\x13I\xe9\xf9\x1aFZ\x9c\xde\x9c\xcb<\x00\x1a\xb5${\x07\xb5di\x8f\xe5\xbe\xdfC[\xc1\xbc\xc5\xc6k\x96J\xcc\xf4`\xee\xaeQRX\xe0\x10\x07\xa9\xad\xbb\x14\xb4\x0b\x99\xa7\xe3\xdb\x8a\xcax\xdfw_\xc9i\xf1\xab\x92\x17\xcac\xeeA\x02\x85\x1e\xb6)\xca\xea\xd7:H\xb5\x0b8\x98\xf9\n\x0e:\x93\xfe53^\x19\x1bqS\x8f\xf7\xbaV\n$\x9bw\x8cF\xb9\xe3\xe5\xeb\xefS\xc7i%\xcf\xceg\x93\x83\xe9\x8a\xde70\x94bk<\xf1\xc4\xbf4\xa0\x13\xd0\n\xab$\xd1?+#\x91Q\x7fgD0\xe0<\xa5\x7f\xbcsP\xb3\xedll\xdc=\x00\xad5\xeage\xd0.\x1d\x8cl\x13\x1b\x87 UH\xcbJ\xa4\xe0+\xafPj\xde\xcd\xe48OcVb\xb0\r\xb1\x80!\x97\x1d\xb9#\xd0\xd0\xa3v&\xd23\xed\"\x0c\xacX\x0ez{U\xcb+u\x87\xcc\x95I\x07\xa7\xe1V\xe4\xd3\xd66\x05d\xe2E\xf4\xe8{\x1a\xaf\xf6)P\xc8\xac\xf8\x18\xe2\xad+\x19\xb7sb\xc2\xfd\xa3\xe5\t \x0c\xf3W\xecI\xb8\xbb3H\xa3g\xf5\xaeb\xdeU\x8c\x9c\xb1\xc9\x18*+Hj^Z\x80\xa3n\xe2\x00\xfa\xd5\xa6f\xd1\xd4\xe5$r\x9bA\x00t5Jk(.\\\xbb\xa8\x19<\xe3\xf9U\x15\xd5\x95a\r\xe6~\xf6@\xd8\xfa\x03\x8c\xfe\x94C~.B\xaa\x11\x81\x92\x1b\x1cd\n\xab\x93c\x1f[\xb3H\xe7i%\x07\xf7\x87\x80?\x84V\x01-\x0c\xc5\x13&5\xe9\xcdtZ\xd4q\xe5\xe4\xbd\xb8b\xaa\x06\xd4S\xd4\x8a\xcd\xd0n\xad/\xef\xbc\x95\x85B\x86\xf9\x8b\x1aC\x1fmcux\x04\x91\xdb\xb0\x8c\x0e\xa7\x81L\xbd\x9aK\x1bg\x0e\xeb\x9c\x12\x14\x1ekW\xc4\xda\xccZ|\xd1Z,\xdb\x17\xba \xc9\xc7\xa9\xf4\xae&\xf2\xe26\xba@\xf2\xbb,\xcb\xb8\x12z\x0c\xf1I\xb1#~\xc8Kqi\x86\x8c\x19\x98ej\xb4\xb2\xc9i(Y\xa3(H\xc6\r7\xc2\x97\xdfh\xd4\x15Y\xb7*\xb9\x1fA\xd8\xfe\x95\xd0\xf8\x92(\x13V\x80\xca\xaacu\xe1\x8f'>\x94\xfa\x0c\xe5\xdesqp#Df\xcf\xa0\xad\x9bm$\\\xa2\x18\xe2\xce8j\xbd\xa7\xe9\xd1\xdbN.\x04(bc\xb5\x8a\x120{\x1cv\xad\xbb\x19b\x89\xe4\x84*\xa9?7\x1e\x87\xbd4&V\xb4\xf0\xecQ\xc4\xad*\xf4\x97x\xc7n*m\x82\xdeVA\x9d\x99\xebZ\xad)X\x94\x02\x08\xdd\xb4\xd5;\xa3\x19\xb7ga\x92O\x14\x98\";\xbb\xbcD\xab\x92x\xc0\xaevxdwc\x9eN{U\xcb\x96\"S\xe8O\x03\xd0UH\xdafb\xab\xcb\xe7\xadC5\x8a\xb1J\xe2?)\x15r\t\xefJ\x98|bM\xa0zU\x9b\xbb\tL%\x98\xe4\xfbS,\xf4\xb9duUS\xc7Z\xceI\xdc\xd6-X\xb7\tE\x18.\xcc=\xea\xdf\xd9c\n$~\x10\xf2\x00\x19'\xe9Uf\x8e;@>p\xd2\x0es\xd9\x7f\xc4\xd2\xdb_\xc7&rNs\xd7vi\xed\xa3\r^\xa8\xb6\xb1\x87R\xb0\xb0D\xc7#\xb9\xfa\x9e\xf4\xa2\x19P\x10\x01\xe3\xa1\x1d)2\x8ewD\xe1X\xfe\x15:K*p\xd8\xfa\xfa\xd0\xf5\x0b\xb4VwY\x14\xee]\xb2\x0e\xd5\x97q0L\x821\xf4\xad\xcb\x98\x96\xe1\tP\t\x1dy\xe4W1\xa8E<M\x80K\x81\xf9\x8a\xe6\xab\x06t\xd1\x92+\xcfp\x18\xf59\xa8\xb2\xd2qP\xedf9*\xc2\xad@\xa1y5\x9d\xacm'\xd8\x86t\xd9\x11c\xda\xb2,\x13\xed\x17\xe7\x03\xda\xb4uK\x8d\xb1\xb0OJO\x0e\xc0\x14=\xc3\xaf\x08\x0b\xd6\xf4U\xdd\xce:\xf2\xb2\xb0\xeb\xe5/v\xc0}\xd4\xc2\x0f\xc3\xff\x00\xafQ\x04\xc5Y*X\x96\xeeNiBz\xf5\xae\xa6\xce\"\x10\xbe\xd5\"\xa7\x19\xa9\x02{S\xb6\x8cf\xa5\xc8G*_\xff\x00\xd5L/\xebH[\x9e\xb5\x19&\x81\x8f/\x9a7f\xa2\xdd\xd8\xe6\x8d\xdc\xf5\xa0D\x99\xff\x00\xf5Q\x90j0\xf4\xbb\x851\x92g\x9aBE08\xa34!\x0f\x07\x1d:\xd3\xf3\x9a\x8b?\x859X\x1a`;\xfaQ\x9e\x7f\x9d\x1d~\x94w\xa0\x03\xa54\xfdi\xe0sF\x07\xa5 \x19\x9fZN=i\xe5i\xa4`\xd0\x02\nC\xebJz\xd250\x18\xac\x12Ub\x81\x80?t\xf45\xdb\xebW\xa4i\x1a-\x94\\B\xb6\xc0m'\x90OQ\xef\xf5\xae\x1d\xfd=k\xa4\xbf\xb8[\x8d/M\x91Nv\xc5\xb4\x920r\x0f9\xa8\xa9\xf0\xbb\x1aR\xf8\xd5\xcc\xff\x00%\xe0\xba\xc9C\xcf|t\xad{ll\xdcX\xe2\xae\xdb\xb4\x17\x16\xea\x18\x02\xd8\xebU5\x00\x90\xa3\x058\x03\xf5\xaeHK\xda=N\xd6\xbd\x9a$7Q(;X\xe7\xd34\xe8u\x14\x8d\x8f\x98r=+\x98\x8e\xe3|\xe4r\x07\xb5i\x04\x0c\x01S\xba\xa9\xddlTm-\xcd\xb3\xaf\xa0\x03\x11 \x1d\xb8\xab\x10j\x85\xca\xfd\xc0\x8d\xc61\\\xd6\xcd\xa4\x0f_\xc6\xad\xdab\t@rp\xfd\x0fj\xb8\xc9\xdc%\x08\xdbC\xb3\xb3\x96\x16\xceO\x07\xf4\xa9.4\xc8\xdb2\xc7\xf7\x8f<w\xac\xa8#X\xd8\x18\x9b\xe6<\x8c\xf7\xad\x1b;\xc2w\x03\x91\xb7\x82\rt/3\x8eZl3\xc8\n\xc1\xf0B\x91\xcf\xb5\\\x85U\xd0\x0c\x8c\x8e\xe3\xbd\x07d\xc1\x9b#\nj\xacO\xe5\xbb($\x0e\x80\xd3'r\xe4\x88\x02,DnS\xd1\x8fj\xa3t1\x16C\x00\xd1\x0c\x1e:\x8fZ\x99\xee\xc4Q\x02\xec\t\xe3\x07\xd6\xb2\xefn\xcf\x98\xdc\xfc\x92e\x0f~\r\x04\xd8byP\xdcd\xc6\xa7w\xf1\n\xab$\xe5v\x16\xc8Q\xb8\xe7<\xd4\xb7\x16\xed\xe6\xc2T\xe1\x03m$\x1e\xff\x00\xe4T\xb0i\xa1\xe1\x9aV*\xcd\x1a\x16\xc3\x9e\x9d\xc9?\x95\x00VE|Aq#\xedU\x8b\x1b;\x92X\xd5\xcb=B\x0bXe\x906p73\x1e\x83\x9e\x83\xf0\xcd3\xec\xd1\xdcX[\xc8\xcd\xb9\x8a\xe3\x0b\xe9\x92q\xf9V\x0e\xa0\xcacx&\x94$\x12N\x06\xd8\x86\xdc*\x8e@\xff\x00\xbe\xf9>\xc6\x98\x8aw\xba\xc3j:\xbb\x1f=\xf6)\xda\x8b\x9c\xee$\xf4\x02\xba\x0b;qg\x00\x11\x0f0\x8e\xea=\xbf\x9dp\xb6\xf0\x11\xa8(\xb6\x8fcJ\xed\xe5\x96n\x8b\xdc\xfe\x035\xdfim%\xbb\xc7\x0b(iL\xa40\xeb\xb1@\x1d\x7f\n\x10\x8c\xfd^\x05\xba\xd5c\x8eu\xf2\xe2\x11\x03/9m\xdd\x815\xcdM\x1b}\xb2x\xd8\r\xd1(\xdas\xfc'\xa5v\xfa\x83Ml\x97\x97w\xb6\xc1\x13'd d\x96\xf4>\xf8\xae]\x15g\xf12\xacq\x07y\x00i\x018\x19\xc0\xc8\xfa\x0e?*\x06A\xa2\xf9\xdau\xd4\x10\xb4\x86\x131\xf3U\xc8\xe0z\x0fo\xff\x00Ut\xfe$\xb9k\xaf\x0e\xf9\xf2HK\xc4\xd9\x19\xe0\x8e{{U\xbbk\x14\xd6\xae\xccb\xdc\x88\xe1\\\x83\x8eI\xc9\xc8\x07\xf0\xa5\xd4\xd5m\xa2K+\xe8URX\xceq\xf7H\x04r=\x0f\xf8\xd0#/\xc2:\xf2J\x93\xda\xdc\xc8G\x99\xc0\xc9\xef\xeb]\x14\xa6H\xe1\x0f\xe6\xe6X\xb2\xc1\x87\xf1\x1f\xfe\xbdp\x13h\xcf\xa6\xdd\xdb\xdd\xc6\xf8\xb7yHf\x1dT\x9e\x84\x8fC\xd7\xdb\x9a\xe9\xb4\xb9\x8bY\x1bI\x9bs( ?R1\xefN\xe0t0j\x0f$\x08K\x107\x96\x00\xf7\x18\x06\xa5\x82\xef\xed1\xb6\xe7!Cn\xc1\xed\xcf\x15R\x0by%\xb4\xb2q\xf38,\t\xfc\x08\xa4T1H\xc9\xc2\xab\x10>\xbd\xff\x00\xc2\x90\xc7L\xe4:n\x1c\x9c\xf1\xf4\xabV\xb6\xabnw\xb9;H\xc9>\xf4\x90\"\xcbq\x13\xed\x05\xf0\x7f\n\xd3H\xc4\xb17M\xa3\x80>\x9d\xe9Xw+\x18^\xf2uTLF\xa3\xbf\x1f\x9dH\xe4Z\xc4\xe2!\xb9\xcf\x04\xe3\xef\x1f\xf0\xab\xa4\x14\x88\xacg\x1b\xbb\xe3\x93U\xcc9 \x16;\x00\xea{\x9av\x0b\x9c\xf4\xf62^\x16\x04\x91\xdf#\xbdB\xfaY\x82?\xe2b;zWO\x84\xdb\xb5T\x01\xfc\xea\xa3\x15\xde\xc1@w\xfap\xb5.(\xd65\x1e\xc7;\xb6h0<\xc2K\x7f\x01\x15<s\xddD\xc3iR;\xab\x1a\x96\xfe\xe0\xc1\xb8\xaana\xc9oJ\xc37O0f\x04\xf2{t\xac%d\xce\xa8\xdeJ\xe6\xef\xdb.\x10\xf9\x89\x19e\xee\x01\xe4\x7f\x8dT\xbb\xbd\x8ebC\xc6W#\x86\xc69\xf4\xac\x8f>x\xcf\xdf#\xf1\xa7\xad\xd4\x92}\xf93\x81\xdf\x9a\x972\xd5;jLc+\xf3#\xe5j)\xe5e\x18\xa8\x1e\xf8F\xe1\x06\x08\xfa\xd5\xa2\xa9s\x00\xda>b8\xa8Q\xb9.i3\x0e\xed\xfc\xc96\x0es\xd6\xb7t\x96O\xb2\xbc+\xd5\xa3,~\x82\xb3\xbf\xb3X7\x98\xc7\x81\xf9\xd5\x8d\r\xc1\xbd\x98v(EiNih\x8ez\xd1v\xb9>\xde\x94\xe0\x95 Jr\xaekG#\x8e\xe4{\x0f\x02\x94!\xa9\xc2g\xfc)\xc2:\xcd\xc8W8\x02j&\xe9N-P\xb3w\x15\xd0P\xbb\xb1\xd6\x98_\x9anK6\x14d\x9a\xd7\xd3\xbc7\x7f\xa80\xdb\x13m>\xd52\x9cc\xab\x1d\x9b2w\xfaR\xe5\x8f s^\x85\xa7\xfc7\xba\x90\x03\"\x1a\xdc\x8b\xe1\x83\xed\xe5+\x07\x88]\x11\\\xbd\xcf\"\xcb\x0e\xa3\x8aM\xfc\xd7\xad\\|2\x95P\x95C\\\xde\xa3\xe0[\xbbl\x9f-\xbf*K\x15\x1b\xd9\x87'c\x8d\x0fOV\xf45f\xefI\xb8\xb5$2\x1e*\x87\xcc\xad\x82+\xa23R\xd8\x96\xacZ\x07\x8aw\xe5U\xd5\xf3R\x86\xaa\x11(\xc5/jfp)\xc0\xd3\x01p)1N\x14\xb9\xf6\xa0\x08\xca\xfbS\x08\xf5\xa9\xb1\xc6z\x1aB\xa0\xd0\x05F\x1e\xd5\xbdn\xf1\xc9\xe1\xb5\n\xe5\x8cs\x91\xb4\xf5\\\x8e\x9fJ\xc6u\"\xb6th\x12\xe7K\xbe\x8c`\xc8\x81d\xc6y\xa5-QQ\xd2E\xcd)Y\xe2S\xdcsY\xda\xc4\xb2\xcb9Q\x90\x07\x15\x7fFs\xe4\xce\x17\xaa\xf4\xac\xe9ei&\x91_\x19\xeek\x96\x94-\x1b\x9d\x95'\xcc\xd1\x9f\x14.\xad\x95\xe6\xaeE!\x00g \xd4QG\"\xb1\xda\x0f^\xdd\xea\xca\x9e\xbeb\x02;\x90)=Mc\xa0\xa9.\xe9\x90?\x1c\xf5\xad\xc3d\x19C/)\xd4\x81\xda\xa8Y\xc1\x14\xc1S z\x1fZ\xe8--\xfc\xb2\x00'\x18\xe4\x1a\xd2\x10\xb2\"\xa4\xfb\x11\xc2\xdb\xe0]\x99\xde\x9c\xf3\xe9W\xa1fa\x95\x00?|\xf7\xa8\x92\x1f*bTarM^\xb6\x8a9\x88\xc0\xf9\x97\xf2&\xb6G4\x99\n<\x8b1=\x11\xc7C\xebK*\x93 \nz\x8a\xb5t\x84\x85\x18\xdb\x87\x19a\xd8\xd2=\xb4\xebn\xb2\x05%\xa1l\x10\x06r?\xc3\xad;\x12Q\x86\x19\x12w\xb7\x95I\x86L\xc8\x87\xd0{~5a\xb4\x8f0C\x97\x05pNq\x8e?\xcf\xf2\xad\x98 I\xe0\xdd\x19\x0c\xa3\xfdY?J\x9d\x15n#M\xa0\x80@V\x04t#\x8c\x7f*v%\xb3\x0e\xe2\xc0\xc6\x96\x81\x86\x0e\xec\xb7\xa0a\xd2\x9fgj\xd3ir\x97\x8c\x81:\xe4\x01\xcf\x0c\x06\x17\xf5\xadim\xa4\x95R&\x03\xe6\x93\xafl\xe3\x9f\xeb\xf8\xd3\xaf\x91\xad\xa1C\t!~_\x97\x19\xc8\xe3\x02\x98\xaeaA\xa6\x18#\x1avDQ\xabn'\xbe\xd3\x96<\xfe_\x96+\x1f\xc4\x9aKM\x04\x89m\xb4\xc6\x91\xab7\xcb\x96l\xb7\x03\xd8p\x0f\xd0WP\xd6\xec\xce\xe6i\xb7\xb9\x90\x03\xb7\xbe\x14\x01\x9f\xa7\xf5\xa5\x9a!u9\x88\r\x91Fp\x0ff`0X\xfb\x0e\xdfA\xebLG\x13\xa6xu\xfe\xd5\x1d\xcd\xd9*|\xc5i\x17\xae#RH_\xc4\x8c\x91\xf5\xae\xa3B\x9e\xc9\x1e\xe4\x95\xfd\xea\x9d\xac\x9dq\x83\x93\xf8\xe4\xfe\x95zX#\x94\xa2\x1f\xf8\xf6\x8cy\xf3g\x80@9\xe7\xd8\xe7\xa7\xa05\xc2G,\xf7\xbe#\xd5-\xac\x9c\xeep\x00#\x03\x82\xc0\x13\x8e\xdf\xc6\x7f\x11\xeb@\x1d\x84\xf2i\xda\x84\x8c\xb71\xa9Q\xf2\xa4d\xe5\x89?1'\xd3\x80z\xfa\x9a\xe1\xa5\x86K\x8f\x15\xb0P\x902\x90\xb1\xaf?*\xee\xda9\x1e\xbc\xd7]\xae]G\xa5XY\xc1l\xaa\x0c\xd2\x04\x0e\xa9\xcb\x1c\x16f\xf7\xe0g\xf0_Z\xe3\xf4\xb9\x1a\x0b\xbb\x89/\x17\x93p\x06\xd5\xe4\x83\xc6\x14\x9f\xa0o\xc4\xd2{\x82=\n\xd2\xddlt\xf0\xd3\x82\xb3\xbbl\xde:t\x1b\x7f\x0e\xa3\xebM\xd54\xf85m%\xe0b\x19\x9c0W\x07\x1b\x0fc\xf5\xe3\xe8H\xab\x1a\xeaJt\xc8\xe2\xb1pd,6\xb3\x1e\x181 ~Y?\xa5s>\x18\xf1'\xda5\x9b\xbd:M\xc28dc\x17L\x1c1\xc8\xfd2>\x95@bj\x16\xd7s\x03?\x94V\xc7jK6z\x068\xdd\x8fO\x987\xe5\xef[\x9au\xbe\xdb]\x8aD\x92\x9b\x86M\x83\x1f:\xe7\x19\x1f\x95u\x1a\xce\x96.t{\x94\x85\x16ID,\x91\x86\x1f{\xe6\xdc\x14\xfbq\xfc\xeb/C\x8e\x16\x84\xf9\xa8\xa0\xa5\xc0(\xd8\xe8J\xe3\x8fL\xe3\xf5\xa5`\xb9z\xde\xdf,\xd0\xa1\xda\xd1\xa7\xdeQ\xc6r\x7f^*\x84\xb19\x9e0\xa0\x92\x01\xdd\x8e\x9c\xe4\xe7\xf4\xfdkl\x87\x9a\xf5\x81$(\x8c\xfc\xc0c'\xa6\xef\xd2\xa11\xbci,\xe0eC|\xdcv\xc0\xf4\xa2\xc3 \x861f\x9ef\t,\xa0\xaa\x81\xceO\xff\x00^\x9e.\x0cp\xc5\x009-&\x18\xfa\xf7\xfc\xaa\xe1\x81\xbc\xc0\xb8\xcb:\xe7i\xec)\x93Y\x90\xa1\x91~r\xdbPc\xb9<\x93\xf8R\xb0\\\x94\xca\x0c\x81\x14\xe5\x80\xe7\xda\xa1|\xbc\xca\x07\xdc\xc75\x12\x01\x04\xacCo<\xef#\xa6GaM\x0e\xe4\xb1\x19\xdc\xdcc\xd2\x81\x8e\x9d\xf1\x9cg\x03\xf3\xaa\xcb\x032\xf9\x8cH\xc9\xfb\xa3\xbdY@PnnI\xed\x8aa\x9f\x03n9>\x9d\x85!\xa7c2\xea\xd6+\x8969\xf9\x17\xa0\x1e\xbe\xb5\x13\xe9\xb1E\x1e\xd8\xd3b\x8e\xbe\xa6\xb60U\x02\xaa\x02\xc7\xa94\xd9v\xc2\x85\x9d\xb2}\x07ZN&\x8ar\xb1\xc8\xde\xd9\x84\xc7\x9a\xc53\xd0\n\xa4c\n\xa7h#\xebZW\xa2V\x95\xa6\xb8|d\xe0(\x1c\xfd+\x1ak\x89:\x01\xb4~\xb5\xc95\xef\x1d\xb0w\x89\x9bxZ9\xbe\xa6\xb7t[\x94\x90\xaa\x1eI\xe3>\xb5\x958\xde\x01\xc75j\xc1\x04N\xae89\xc7\x1d\xea\xe1\xbd\x8c*w5u`\xd1n\\\xfc\xa4q\x8a\xce\xd03\xf6\xc6\xff\x00t\xe6\xae\xea\xd7!\xec\x03\x908\xe0S<1\x16\xe5\xb8\x9c\x8c\x80\xb8\x06\xa1G\x96\xa3\"\xb4\xafM\x1a\n\x86\x9c\"\xe2\xa7X\xfd\xaaT\x8f\xda\x9bg\t\nG\xc5H#\xa9\x961O\xd9P\xd8\x1eB\xd2\x108\xa5\xb7\x86[\xbb\x84\x86\x04/#\x9c\x00;\xd5R\xc5\x98\x01\xd4\xf6\x15\xec\xbe\x00\xf0x\xb1\xb7K\xbb\xa8\xc1\xba\x90\x03\x82>\xe0=\xab\xaaRkDZE\x7f\n|:T\t=\xf0\xdf!\xe7oa^\xa7\xa6\xe8\x16\xd6\xc8\xa1bP\x07\xb5]\xb0\xb2X\xd4\x129\xad \x00\xa9\x8d$\xf5z\x8d\xc8\x8a;x\xe3\x1c(\xa96\x8fAK\xda\x8a\xd9E\".!P{\n\x82{(.\x14\x87\x8dN}\xaa\xc5-)B2\xdd\x0e\xec\xe2\xb5\xdf\x05Z\xdd\xc6\xcd\x1a\x00O\xb5x\xff\x00\x89<#=\x84\xae\xcb\x19\x03>\x95\xf4\xa1\x00\xf5\xac=oE\x86\xfe\xdd\x95\x90\x1c\xfbW4\xe8\xb8>h\x16\xa5}\xcf\x95\x984NU\x81\x04S\xd5\x81\xc5v\x1e3\xf0\xb4\x9at\xcf,hvg\xf2\xae$\x12\xa4\x82+ZsRBh\xb6\xad\xd8\xd3\xc1\xe9\xcdVF5*\x9ekA\x13\x83N\xcf\xe5Q\x8avGj`?\x9aon\xbe\xd4g4\xb4\x01\x1b\x0c\xd6\x8f\x86\xe4H\xf5\xb8\xa3\x93\xeeN\x0cd\xfdEg\x9e\x9f\xd6\x9a\x03+\x06^\x089\x074\x02:\x1b\\i\xba\xdc\xd0K\xf7e\xf9A\xedPj\xf6\x7fg\xb8i\x94\xe1\x0f5nim\xf5\xabH\xdd\xdbe\xda\x80\x1cg\xa9\x1d\xc5.\xadi$\x96(\xb9'\x8es\xde\xa1E%c[\xdc\xca\xb4\xb8\xb72\xfc\xaf\x9c\xf2\x01\xfe\x95\xb9\x12[I\x87d\xf9O\x06\xb8\xc3\x13\xdb\xb7\xcc\x8d\x8c\xf6\x1c\xd6\xfe\x91\x15\xdcG\xcd\x8e_2>\xa5[\xd2\xa6\xda\x9a\xf3;\x1b\xb1h\xb1\xa3\x13\x13~\xed\xb9P{U\xd8\xf7$\xa8\xbbI\xecjKvV\x8d$M\xcaq\x86C\xda\xaeB\xa1\x88`\xb9\x07\xa8\xe8G\xbdZD9\\T\x88F\x8a\x00\xceN=A\xa5\x82\xd1\x8aHT\xed`s\xb7\xdb\xe9W\xa0\xb5\x01\x97c\xb6\x14\xe4\x06\x1d?\x1fJ\xb8\xd1\xae\xd0%\x8bp<\xe4&\xec{\x8e\x86\xaa\xc6m\x94\xed\xd5\x1d|\xbb\x8f\x92C\xf2\x829V\xff\x00\xeb\xfbU\xb0|\xb8\xd7v\x10\xe7\x0c\xbdC\xfd\x0fc\x8a\x9dc\xb6\x90\x1c\x04\x93#\x05wr\xdf\x81\xeb\xf5\xedQ\xcdsh\xa7a\x07z\xf5G\x05\t\xfag\xbd1\x10H\xf1+\x06\x84`\x1e\x0e\xd1\xc2\x9e\x9c\x8fO\xe5Z\xda|_,\x83\x82\xbb\xb0A\xed\xea=\x88\xff\x00\n\xaa\xb6\xe8\xcf\xcb\xe1\xdb\xee\xb7\xf8\xfb\xe3\xfc\xf6\xadKU\x11\xe0g$\x81\x93\xde\x98\x98\x93\xc4#d%N7v\xf5\xff\x00?\xa8\x15\x8f\x7f(X\xb7\x1ca~R=\x8f#\x15\xbb9\r\x19Bq\x9e\x87\xd0\xf5\xfd+\x95\xb9\xb8iT\xc4T\xab\x896\xb0\x03\x8e:\x8f\xa0 \xfe\x14\x022c\xd67\xbc\x924\x9bql&'\x1dN\xd7\x1f\xe1[\x8b\xb2XbH\xf9g\xe8G\x19\x1cs\\\xf5\x9e\x95\x05\xc6\xab \x93/\x14k\xf2\xee\xeew!\x03\xe9\xfe5\xd7ZZ\xaaM\xe6\x16,\x01\n=\xff\x00\xc9\xfeT\r\x89u\n\xa4,\x18y\x8as\xbdzo\x18\x03\xfck\xcbt\xcd\xb6\xbe&\xbc\x86{\x84i\xdc\x01.\xcf\xba\t\xe4\x8f\xa8\x07\x1f\xf0\x1f^\x9e\xab\xa8\xb2\x88$\x90p|\xb2\xa3\xbfZ\xf2\x94\xd2Z\xde\xfcI2\x90\x92\x15\x12\x81\x80\xddK\xb7N\xe7\n\xbf\x9d\x17\x15\x8d\x8dy\xde\xce6\xd4d\x97z\xab1\x8c\x9f\xe0$\x0f\x95Gn\xc3\xeaO\xd2\xb9\x8d\x1aq-\xe4\xad&V\x112;\x19\x07\x01\xb03\xf9\xe1\xc6=\xebk\xc57\xd6\xb76\xe1\x15\xc9\x8642*\xe3\xef9l\x03\xfa\xf0~\xa6\xa8\xf8v\xe5da!\xc0\x17\x0e\xe9<d`\xef\r\x90A\xf7\xcfJC=\x1bRt\x8bN2.\x00\x18e$r\xbd\x08\x1f\x9f\xf3\x15\xc4\xf8[Nk\xfdE\xee\xdaM\x91\xa4\xd2:\x05=A`\xd8=\xf8%\xbf\xef\xa6\xf4\xad\x8f\x11_\x94\xd2.eu*\x8d\n\xa9BpP\x8d\xdf\xd4~\x95\x97\xf0\xb6\xe3\xcfMF\xdc\xa1\xf9%\x12\xc4O\xf7I#\xf4\xe7\xf34\xc4z+H\x12t\xc9\xf9s\xb4\xfb\x02?\xfa\xf5\x8e-\r\x9c\xc1\xd1\x81\x8ds\xbc\x0e>l\xe7&\xb6%\\C\x9c\xe0g\xf2\xf45\x87\xad}\xa4\xd8\xcf\x1d\xb1\\\xca\xac\x8d\x90x8<\xfd?\xc2\x80E\xbd>m\xd0\xb0s\xf3\xaaF\xa4g\xb9P\x7f\xf6z\xd4\x8a\xd66ED\xc0\x89H\xc8\xc7R0\rqZu\xcd\xcc\xb3\xbb\x07\x0b\x19`\xe7i\xe9\x83\x85\x03\xd7 \n\xec\xec\x9c\xacA0@^\x0eOS\xdc\x9a\x01\x92\xca\x866f\xce%p@a\xfc+\xec=j\xa92F\x89o\x18\xdb\xd0\x16\xcf#\xd0\x7fSZE\x07\x97\xe6\x91\x96a\xc0\xe9\xc7\xbda]\xc9\xe7L\xca\x90H\xca$ \x02\xb8V\xe3\xa9\xf6\xa0\x11`\x84\x91\xd3\xc8\xdam\xe3\x18/\x8f\xbes\xdb\xd7\x9aj\xc0F\xec\x80\xbb\x81$\x9e\x80S\xe0\x1ez,\x8e\x8d\xc0\xe1\x00\xc7n\xb5d\xef\x98\x90!c\x8e\xff\x00\xc2?\x1a\x06f\x05E@\x8cI,0=ME$b6\x1b9brk@\xdb\xec,\xec\xb8o\xe1\x03\x9cUe\x8c\xaa\x96't\x9e\xa7\xd6\x90\xeeC\x1cg?1\xdcGZI\xd7\xf7\\\x0c\x93\xd4\xd5\xa5\x8fdE\x8eI<\x9a\x8c\x81\xb7\xa5!\xa3\x99\xbd\x8e\x18\xc9i\\\x16\xf7\xe8+\x9f\x99\xe3i8\x1b\x8f\xbf\x02\xb65\xfd\x91\xbbI\xc6z\x02\xdd\x01\xff\x00\x1a\xe6Y\xc6x\xf3\x19\x8f\xfb5\x84\xd5\xd9\xd7\x06\x92\xbb\x1dp\x17\xcc\xc8 \x0cU\x9b4\x12\xc8\x175\x93q&\x01%Xzd\xd6\xc7\x87\x15\xa4\x93q\x19\x1e\xf4\xe1\x1dneRwC\xbcG\x98\xec\xa1\xb6C\xf3\xbbt\xf6\xad\xbd\x1e\xcc\xd9\xe9(\xac0\xd2\x1d\xc7\xe9N\xb8\xd0\xfe\xd7\xaa%\xc4\x9fq\x13\x9c\xf4\x15\xa6\xe03\x00\x06\x11F\x00\xf6\xa8\xabd\xcei\xce\xea\xc4!3\xd3\xadJ\xa9\xc58/5.+#;\x91\x85\xc5(Z\x90/\xb58%!\x1ee\xe0M\x14j\xda\xfa\xcb*\xe6\x0bl;\x03\xdd\xbb\n\xfa\x1fI\xb4\x01W\x8a\xf2\xdf\x85\xf6\x02=(\xccF\x1aY3\xf8\n\xf6M:0#\x06\xba\xd6\xae\xe6\xbd\x0b\xe8\xbbW\x14\xea(\xad\x91\x01E'j(\x01h\xa4\xa5\xa0\x02\x9a\xeb\xb9qKE&\x80\xe4<M\xa4G{l\xea\xca\x0eA\xaf\x9e5\xfd5\xf4\xcdNH\x98`g\"\xbe\xa5\xd4c\r\x19\xaf\x14\xf8\x95\xa5\x81\x12\xdd\xa0\xe5N\t\xaev\xb9et^\xe8\xf3e<\xd4\xaaj\x14\xa9\xd7\x8a\xd8C\xd4\xf3O\x07\xdc\xfe4\xce\xf4\xb4\x00\xf0E.}*\"ph\xce~\xb4\x01!9\xefM<\xd2n\xf7\xa4,\x01\xe4\xfeT\xc0\xbb\xa6\xdb\xb4\xb7J\xbb\x99Aa\xd0\xd7w\xafX\x85\xb2\x1b\x0e\x0e\xc1\xd3\xe9\\\xb7\x85m\xcd\xce\xab\n\x01\xf2\x83\x92z\xd7e\xe2\xbb\x98\xe2\xb7*\xa7\xe6\xc60)=\x8a\x89\xe7p3\xfd\xb3c\x16e\xcfB2k\xae\xb2\xb4g\xda\xe3h#\xfb\xbc~\x95\x87\xa2[\xcdw~$\xfb\x16\x15O.Ev\xa4\x88\x14f0H\xf5\x14\x92.\xe3\x16-\xe1\xb1\xc1S\x93\xc5iZ\xaa\x981\xb7<z\xf4\xa8\xed\x8c\x8d(\x7f$\xa0a\x8c\xf5\x06\xb5m\xd5VO\xb8\xa0\x9e2\x8d\xfd*\x91-\x8e\xb4\x8c\x98\xc8$c\xb1&\xadl`\xbbJ\xef\xc7#\x04\x7f:z\xee\xea\xa8K\x0e\x98\xff\x00\xeb\xd3\x1c:\xbe\xf9\x12B\xc7\x8c '\xff\x00e\xa6\"\xbb[6Y\x8a\x1022\xa3o'\xd7\xa9\xa8\xe5\xdap\x01Y\x07?$\xa3\x9f\xc3\xa8\xa7Or\x88J\x832\xbf\xf7\x15N\x7f\x11\x8a\xae\xd3\x8b\x90bx\xc0\x90\x0c\xaf\x9d\x19Nh\xb8\x16,\x9bt\x8c#fS\x81\x88\xe49\x03\xe9\x91\x90?1Z \x90>\x7f\x95\xab\x9a\x9a\xe2{t\x1b\xd3~\x1b9.2>\x8c\x0f\xf3\xc5ZmX\x98\x81%\xb3\xfe\xdf\x19\xfcz\x1a.\x165\xa7\xb9*\x0e\x08$s\x83\\\xb6\xa8\xe4\xdc,\xe9!D'\x07\x1c\xe7#\xa5Y\xb9\xbf\xc0\xc1>\xea\x7f\xa1\xac\xa6\x9c;\xee?w\xa9_JW\x1d\x8d+\x14Cs\x15\xc9-\xfe\xac.:\x7f\x9eEl[\\\xe5P\x10>\xeb7_~+\x94\xfe\xd1h\xca\xba\xfd\xdf\xbb\x82}\xea\xca\xdf\xf9\x11\xee99\x01G=@\x1f\xe2h\xb8X\xd4\xd5o\x17\xca\x925\xef\xef\xcf\xbdy\x9f\x8a\xa7o\xedKuRFdfl7N\x06\xdfn\x9f\xce\xba\xcb\xab\x977-!\xca\xa6\xecdv\x1d\xeb\x9a\xd6\xca\xdd\xcb2Y\xfe\xf6GU\xf2\x9b\x1c\x82{\xfe\x8bE\xc4\xd1\x99\xe2)c\x86\xe6)\"\x91\xd4J\xc2\x15\x1c`\x00y?^*\x9d\x8cs\xe9\xfa\xbd\xb3\xc6\xfedQH\x1c\x91\xd3\xefrO\xeb\xf9U\xaf\x10\xdb\xc7uil\xf0\xb9\xda\x87\x03\xe6\xc8\x0ey9\xef\xe9\xfa\xd3\x91\xbe\xca\xb7\xea\x08\x7f5\xe3 \x8ep\t\xed\xf8\x1f\xd6\x81\x1dg\x8b$i\xb4\xbb\x98\xc0.\xce\x8a\x17\x03\x9c\xa9\xe3\xf9\x9f\xfb\xea\xac\xf8:#e\xe6'\x93\xe4;LX(\x1c\x7f\x9e+3W\xb9\xf2a\xb3=UaX\xdc\x1eps\x8c\xff\x00*\xdf\x85\xa4[\xb1+\xb1\n\xdbeP\x07~\x87\xfa\x1aw\x19\xd6\xbb\x17\x8c\xec\xe4\xf3\x8eq\xcdW\xbd\x81\x8c,\xd1\xe0\x16\xfd\t?\xe3U\x96\xf8*,n\xd9\xf9\xf21\xe8i^\xeb\xcd\x86@\x8c\n\x82\x0f\xe1\x9a.\x169\xe8\xdc\xd9\xc2I\xf2\xfc\xc8\xd7h'\xa2\x81\xc0\xff\x00>\xde\xf5\xb9\xa43\xdc\xb0-\xe6,i\xf2\xedn\xa4\xf5\xe7\xdf\xb9\xf7\xe3\xb5b]\"^j1\xa9U+\x102\x1e8'\xb0\xf7\xc7\xf4\x15\xbfc!X\x80\x00\x85\xed\x9e\xad\xeah\x1b77\xe0pC1<\x13\xda\xaa\\\x879!F\x07$\x9f\xfe\xbf\xf2\xa6%\xcbgb+\x13\xd3 S\x9b\xcc<\x87<{\xff\x00\xf5\xc1\xa2\xe2\xb1\x1e2r\xc8v\x8cr\xd9\xc6~\x9f\xfe\xa1S4\x8b4\x9bD\xcc\xe0u\xdb\x85Q\xf8\xff\x00\x86j\xa4\xec\xaf\x9d\xe4\x85\x1c\x1c\xb7_aO\xb6\xbc\x89@\x89$TA\xf7D@\xbf\xf2\xa6\x03\xe4\x89\xa6l,\xe1\x10s\x859&\x98\xf1*\xa9\x03qa\xdc\xd6\x80\x02`\x1b\x0f#\x7f\xb5\x1bq\xf9\xd4R\xae\xc5\xc0\x80\x06\xf7\x18\xfe\x94\x01\x9e\x19\x8a\x10x\xfe\x95Ni\x88W\x00`'V5y\xe2i\x17\x1b\xb0;\x9c\x9e?J\xae\xe8\x88Uc\xc3\x01\xd6\x90\xcej\xf6\xc5%B\xe03\xca\xdd7\x0c\xe3\xe8+\x9a\xd5\xb3l\x84\x02\x0e\xd1\x82{\x0fj\xf4;\x88\xff\x00vB \xe4u\xae\x17_\xd2a\x07\xcc\x9e\xeb\x07\xb2-CF\x91\x91\xcb\xc3\x0b\xdd\xce2I;\xab\xd0|;\xa7\xac+\xb8\xaf\x1dk\x8f\xd3V5\xb8\n\x88\xc3'\xab\x0cW\xa5\xe9q\xa9\xb4\x07\xbe*b)\x84\x8cI#\xb5G\xb6\xa7)\x92h\x11\xd7,\x9e\xba\x9c\xcc\x88'\x15\"\xa75 \x8e\x9e\xa9S`#\t\xcd<%J\xa9R\xac~\xd5I\x0c\xca\xf8x\xa0h\xb6\xea;\n\xf5K\x1f\xf5B\xbc\x7f\xe1\xdd\xd86K\x11<\xa9\xaf]\xd3\x9c\x18\xc0\xae\xb8\x96\xf6/QKIZ\x90\x1d\xa8\xa5\xa4\xa0\x02\x96\x92\x96\x80\x12\x8a(\xa0\n\x97\xa3\xe45\xe6~9\x80M\xa2\xdd\xaf\xa2\x16\x1f\x85zU\xf3a\ry\xc7\x8c\xe5\x0b\xa3^\x93\xff\x00<\xcdaSb\xd6\xc7\x87'Z\x9dz\x7f\x85VF\xcb{\xd5\x95<V\x82\x1f\x8f\xca\x94\x0f\xad\x19\xe3\x91\xcd(\x1e\xbf\xce\x80\x10\x8e3\x8ei\x87\x8a\x90\x8f\xd2\x9aGS@\x11\x93\x8e\xb4\xb1De`\x17\x92N)B\x96\xe83]\x17\x87t\xaf\xb4\\\xab\x120\x0fB(\x1d\x8e\x9f\xc2z8\xd3\xec\x9e\xeaP\x03\x91\xf2\xf3\xd2\xa9\xea\xcc&\xbb\xf9\x94\xb95\xb3\xa8\\yH \\\x80\xa3\x00\x0e\xf5F4\xcc\xbf\xbeN\xdd\r2\x96\x85\x9d2\x08\xe1\x88;aA\x1c\x8a\x99\xee-%\x9ba\xdcq\xd3\x07\xadW\x92VE\t\x19\x1e\x80\x15\xcdKagr%\xf3\x99x\xfe\x1f\x97\x8a5\x03V\xd2\x12\xd2\tb\x8d\xdc\x8e\x9f(\x1f\xd6\xb4\x91\x0bd\x9d\xf1\xf6 w\xfc\x85W\x86\xe4\x04 6@\xfe\x14\xebL\xb8\xbcX\xd7\xe5F\x07\xae\x0bS\xb8\x1a\x01\xb8\n\x1fi\xf5\xcf?\x90\xaa\xd7\x12G\xca\xca\xec\xca\x06Y\xdd\x8e\x07\xd7\x1f\xd7\x15\x85{\xac}\x96\x16\x95\x80\xe3\xbb\xb98\xfc\x17\x15\xe6~$\xf1\x9c\xf2\xdcl\x8d\x11\xd4}\xd1\xb7\x0b\x9f\xa5K\x92\xd8\x14[\xd4\xed\xfcA\xe3}?K]\xb1\x96\x90\x91\xc2\x80\x146=\x0e\t\xfc\xeb\x84\xbc\xf8\x9b{6\xe8\xd2\xd6\x13\x1e~P\xfc\xb2\xfd1\x8a\xe4U/u{\xb6\x92Ww'\xef7Z\xdb\x8bCX0\x82-\xcf\xb0\xc8H \xe0\x01\x9c\xfe_\x8dL\xa5a\\\x99\xbc\x7f\xa9\xb6w\xdaD\xe3\xa8,\xbf0?\\t\xab\xd6\x1f\x12dF1\xdc\xdb0C\xd4\xa3\xe7\xf4\xacv\x85I\xda\x83\x1d\xb9=O\xe3Te\x89_9E#\xd7\x1d))wB\xbb=6\xcf\xc4V\x97\xf0\xa3A8;\xbb\x13\xcf\xd0\xd5\x95\x9de\x07k\x129\xef^D\xa9-\xa4\xa2kg*\xe3\x9c\x13\xd6\xbb\x8d\x03V\xfe\xd0\x897p\xeb\xf7\xc1=\xea\x8aR\xb9\xd0;\xf0\x15s\xd3\x9e?\xcf\xad*\xc9\xbc(\x91\x98\xed\\\x8cv&\x9e\xc9\xf3\x0f\x97\r\x8e\xc7\xadU`\xe3y\x1c0\x18\x1cq\xcf\xf9\x14\xc6Z{\xa9\x02\xe25\xdcX\xed\xeb\xd3=\x7f\xa5A\xf6O\xb2(\x8a\x1c\x12\xac]\xe4\x1c\x92\xbcd\x03\xeb\x9a\x8a\x17w\xb9VU-\x12\xb9\xeb\xf4\xab\xa02\xbcj\x84a\x94+\x16=\x00\xcf\xf3?\xca\x81\x1c\xc0o2\xd3\n\xbc;\x9c\xa9\xe4\xee\xc9\xcf\xe8qR\xc0\x9b]L\x84F\xca\xa7 t\xeb\x91\xfa\xe2\xb4\xa1\xb3\xb6\xb4\xbc\x85\xd4\x16\x18\xe7w\x19n\xa6\xa4\x1ao\x99p\xd3\xbb\x12\x85\x02\xed\x03\x87\x00\xe7'\xf0\xa6\x05\x85\xd3\x1e\xf6!\x1c\xf90l\x88\xef\xcf$\x8f\x98\x7f,WA<\xa0C$h\xc1\xb7\xc7\xba3\x9cr85\x9b\xa6\xb4\xab\x11f'\xc8\x8d3\xb7\xd3\x8f\xe5\x8ac\x0c\xdb\x84RZH\xf3\x93\x8e\x87\xbd\x0c\x11<\x97EX\xbe\x0e\x00\x03\xf2\xe7\xfcj\xc5\xa4\xed\xb7\xccf\x05\x0f\x04g\xa8\xaaO\x11e\xdc:\x11\xcez\x9c\xff\x00\xfa\xa9b\xc4j\xb1\x12\x01\xc6H\x06\xa4f\x91\x8e??~\x17\x9e\xd8\xe8*\xda\xdfy\\\xb6\x0b\x1f\xba3\xd0{\xd6T\xf3-\xbc,\xef\xf7\x9b\x8c\n\xe4\xb5_\x16A\xa6.\xd2\xe2I\x88\xff\x00V9#\xeb\xe9Cc\xb1\xe9#WM\x81IP\xa0s\xb7\xfc{T\x11\xea\xebs\xbd<\xb9]:\x1c\x16<~\x03\xf5\xaf\x12\x9b\xc6:\xad\xcc\x85\xa1M\xa3\xf8y\xaa\xff\x00\xdb\xba\xdb1\xc3F\x01\xea\xa1W\x1f\x8f\xaf\xe3E\xfb\x93t{\xecr\xdb\xa2\x13\x13(a\xc6%\x0cG\xe0O\x15r\xde\xf6q\x84\x98~!v\xfe\xa0\xd7\x83Y\xf8\xe3[\xd3\xc8\xfbDQ\\'@\xb2 \xda?\x01\xc5v:\x07\xc4xo\x98A,b\xdaRxA U?\xee\x9cc\xf0'\xe9\xe9T\xa4#\xd7RF=7\x90;\x9al\xa0J2\xc7\xa7\xa7\xff\x00^\xb2\xb4\xdb\xc3w\x16VP\xc0v\xc0V_\xadh\x89p\xb9\x90\xc8\x0f\xb8\xeb\xf8\xd3\x11NX\x08c\xb0\xab}j\xb4\x91\x18\xc8\xe3?\x87J\xbe\xc6=\xdeau\xfcG\xf5\xa8ed\xdb\x9d\xb9\x07\xa8\n(\x19JU-\x80\xccq\xd7\x8a\xa1\xa8\xe9\"XK\xc6\xaa\xcd\xd7\xe6\xe6\xb4\x0c\x96\xc4\xe4\x96\x18\xfe\xf7AS\x94\x8e\xe2-\xa0\xe3\x8fZ\x07s\xcd\xa6\x81\xe1\xba\xc1E\xf9O\xf0\xd7W\xa1L\xcd\x1e\xd3\xd2\xaaj:x\x86W;z\xf7\xf5\xaa\xdav\xa2-\xe7\xf2\x98\xf2\x0e0*l;\xdd\x1dcE\x874\xa2:Ky\x84\xea\x085qc\xaeW\x1dL\xacV\xf2\xfd\xa9\xeb\x1f=*\xcf\x95N\x11\xd3Q\x0b\x10\xac~\xd5(\x8e\xa4\tR\x04\xabQ\x19\xe4~\x0c\xd4~\xc5|\xb1\xb1\xc0&\xbd\xd7G\xbbY#R\x0f\x06\xbel\x85\xda\xdau\x91I\xc8<\xe2\xbdk\xc1\xde\"YcH\xdd\xf9\xf7\xab~\xeb\x1e\xe7\xab\x83\x91\x9aZ\xa9ip\xb2\xc60j\xddj\x9d\xc8\x13\xb5\x14\xb4\x94\xc0Z))h\x00\xa4'\x14T7\x12\x84CI\xbb\x01\x9d\xa9L\x02\x9ek\xca>\"_\x88tYc\r\xf3JB\x01\xfa\x9a\xf4-R\xec*\xb1-^\x13\xe3\x8d`jZ\xa8\x866\xccP\xe7\xa7\xada-]\x8d6G.\x9d\xff\x00\xc6\xac!\xc5B\x83\xa5J\x07\xd6\xb5$\x94\x1c\xd4\x80\xf0*\x1c\xe0S\x81\xe3\xde\x8b\x81.\x7f\x1aC\xc8\xa4\xdf\xda\x90\xb6?\x1a`K\x021\x95z`\xd7q\xe1\x90\xca\xd9\x07\x81\xeb\xd2\xb8XK\tA\x07\xbdz\x17\x86By$\xed \xfbt\xa1\x14\x8d;\x82\xb2M\xf3)\xc8=\x85Q\xbdw\xc8\xdb\x90W\x8c\x1fJ\xd6y\xd1A\xf9>n\xc6\xb3.f\x8d\xdb.\xa7p\xefM\x8c\xab\x18\x95eWx\x99\xd78\xc7S[\xea\xe8\x91o\x91\xf2\xc0p\xac\xbd+\x9er\xb2\xcaYfe\x19\x18\xc1\xcf\xe5V\xa0\x91\xb5\x16xL\xa5\xa3^\x19F3\xff\x00\xd6\xa0\nskZ\x8c\xba\x8a\xc5g\x13M\x08\xc8b\x91\x9e?\x13\x8a\xd4\x81/\n\xf9\x8c\xcc\xa0\x0eA\x00\xff\x00\"kN\xdfM\xb7\x8dT\"\xc8\xaa\xa3\xa2\xe0\x1f\xd0U\xd4Q\xb0\xb2\x14~\xc3\x9eiX.rz\xc5\xa6\xfbFi\x19\xb7\x11\x93\xf2\xe2\xbcgWq\xfd\xa4\xd1\xc6\x07\\\x0cz\x9a\xf7\x8di\x1c\xd9J\x0cd\xb9^6\xc7\x8c\xfe$\xd7\x84^\xc7\xff\x00\x13\xd0\xaf\xc7\xef\x08'\x18\xc757\xd6\xe3\x7f\t\xd9i\x1a+\xd9\xe8\xf1\\\xb4DG(*\x1c\xaeA#\xae=\xc5\x17+\x18\x04\xb8\\\xe4g\xe5\x00d\x0e1\xe9\xces\x8e\xb5u\x9d\xfc\x98\xc2\xc8v*\x8c.x'\xe9T/\xd8\xc5n\xfc\x1c\xe0\x9c\x7f\xf5\xeb\x923nDH\xe7\xafn\xa2\x9aG\x81\xa4L(e\x1c\x90\xab\x92\x0f\xca;c-\xeb\x9a\x81\nF \"5\xc4n\x15\xd7\xb3\xae~\xf1?\xde\xe7\x9e\x9cc\x15\x94x9\xe7=N}\xe9\xc6L\x0c\xe4\xf1\xc75\xdb\xb6\x84\x1d\r\xfd\x9cf\xe2\xe5\xed\xc2\x8bu\x98\xaas\x8c\x8c\x9e\x99$\x9c~8\xc8\xc9\xaaV\x936\x9b\xa9E*\xb6\x11\x9c\x06\xc1\xe3\xebZ6R\x0b\x8b\x05P\x98!\x17\xdf\xf9\xd5\x1b\xd8w\xc2\xff\x00.\x0e3\x93\\\xeev\x91Q=/O\x9e)!Ww\xdcB\x8f\xcc\xe2\xad\x1b3\"\xc8p0W'\xea3\xfe5\xcex:\xfdn\xac\xe0\xdf\x92\xd8\xc3\x93\x8e\xa2\xbb\xf3i\xba\xc4u\xe5r\t\xea+\xa1\x16r\xb0\xc6\xea\xce\xa1~^\x13\xf0\xff\x008\xab+n\xd7{\x10\xe5R<\xc7\x93\x80\x1b\xfc\xf1S\xc6\n\xb9\x8dS*\x18\xe7<s\xefS5\xaa\x92\x8a\x19\x97n_#\xeb@\x15\xe4\x8br\x9d\xe1D\x91\xb1Q\xe8\xc3\xb0\xf6\xebU\xda0e\xca\xb3\x01\x80B\xe3;\x0e~\xef\xe5[6\xc8\xb2\xe3r\xa9\xda2\t\xc7&\x98\xf0\x85\x99\xa4X\xc2\xee\x00\x13\x8e\xb8\xa0\n\xd6\xa4G\x13\xc6\x14\xf3\x9e\x87 \x83\xfe\x7fJW\x8dL\xa07\x0ep\t\xed\x9e9\xab\x96\xf6\xeb\x0c2\x08\xd8\x10\xcd\xbbvs\x8c\xf5\xa9#\x85\x1e\xec\xc6v\x9c\x0cc\xbd \x18-\tUg9\x0c\x07\x18\xc0\x18\xaaw(S\x0e{\xf2~\x95\xd2\xb4*\xb1\xa6Fy\xeb\\\x8f\x8b.\xd2\xd2\xc2i]\xca\x85M\xc4\x0f\xcb\x14\x98\xd1\xc9\xf8\x93]|\xf9\x16\x8c\x1aR0\x84\x9c\xed\xf5c\xfc\xbf:\xe4c\xd3\xf7\xcb\x93\x99ec\xcb7%\x8dhZ\xa7\x9a\x8ds/\x0f7?\xee\x8e\xc0U\xd6\x89a@\xec\xbc\x9e3\xcd`\xeajD\x9d\xcc\xa1d\xc1\xe2\x8fa\xdd(&5<n\xc6s\x8c\xfd\rIi\x12]\xdb\x19b\x8d\xcch\xc1^L\x1c\x02G\x00\xf6\x1d\rgO~.\xa5\x90\xba\x80\xa4\xe0}\x01\xc8\xfdMOi\x7f\xb6Y\x80Q\x9b\x82\x82F\xdb\xc8\xc1\xe4\x8fN?\x95m\xc8\xacC6\x06\x94$\xb43\xb1\x88\xc7\xbc!]\xe3~H';z\x91\xc7Z\xc3\xd54i,\xd7\xcf\x8b\xee\xf7\xf6\xae\xddaI\x82m@\xaa\x83j\x80\x00$z\x92:\x9f\x7f\xa7\xa5>\xfe\xd6\xdeHX,2\xec(\x00/\xcf\xcf\x8e{t\xcfA\xd6\xb0\xe6\xe5\x96\x9b\x17\x17s\x07\xc2\x9e7\xbe\xd2Y-\xd8y\x91\xe4cs\x13\x81\xe8+\xd7\xb4\xbdy\xefcV\xf2\xca\xabs\xf7\xb7~U\xf3\xfd\xa4b\x1d]c' I\x8czW\xb7hvL-\x15\xe3\x08\xb9\x19\xc9\xe35\xba\xbd\xf44\xd1\xc7\xcc\xeba\x97q\xc7\xca=\xf1S\x9d\xb8;J\xe4\xfaqX\xb1\xdcM\x1f\xde\x11\x06\x1c\xb0\xcfoj\x9e+\xfbfc\xca\x12}\xbfJ\xd1\x10\xd1Z\xf4:\xc8F\xd6s\xd4\x00EW[\xb9\xe1O\x9a<\x1a\xd0\x93\xcbx\xcb\xc7\xe5 '\x9d\xb9\xcdf2\xe5\xb3\x90y\xee(\x02\xa5\xdd\xd1\xbb\x89\xd1\xb3\xb8\xfaW)\xf6i\"\xbd\x01\x81\xeb]\xc3\xc5\x02\xf2\xa8\x0bw\xc0\xac\xeb\x9b/6\xe5\x18\x11\xc1\xe9CC7th\xc0\xb5^Mm*V~\x9d\x19\x8e\x15_j\xd6E8\x15\x93Z\x92\xc6\x04\xa5\xd9S\x05\xa3m\x16\x10\xc0\x94\xe0\xb4\xecR\x81\xc5U\x80\xf9\xf9\x94U\x8d;P\x97N\x9c:\x93\xb7<\x8a\x89\x851\x97\"\x9c\x92h\x0fb\xf0\xaf\x8b\xe2\xb9DC \xdd\xe8z\xd7\xa1Z\xdfG:\x02\x08\xaf\x96\xe1\x9a[Y\x04\x90\xbb+\x0e\xe2\xbbM\x07\xe2$\xf6Ec\xbdR\xca?\x8cVW\x94<\xd0\xec\x99\xef`\x83Eq\xfa?\x8c\xf4\xfdA\x17\xca\xb9BOby\xae\x86=N\x19\x06C\x0f\xce\xae5c-\x84\xe2\xd1~\x96\xa9\x9dB 2XUy\xb5xP}\xf0?\x1a\xa7Q!Y\x97\xe4\x94 \xebX\x9a\x85\xfa\xaa\x9c\xb0\x02\xb3u?\x12\xdb[\xa1-*\xfeu\xe6>%\xf1\xdbK\xbe+f\xeb\xc6k\tVOE\xa9j%\xdf\x1a\xf8\xb5`\x8d\xed\xed\xdf2\x1c\x8e;W\x95\x12\xd29v$\x93\xc95$\xf3Ku)\x92V,X\xd0\xa9\xfaV\x90\x8d\xb5{\x89\xbb\x82\x83\xf9T\x98\xe3\xa5(\x14\xbbz\xd5\x88n)q\xc5;\x1f\xa1\xa0\x1fz\x00o4\x84\xe6\x9d\x8fZ\x8d\xf9<P\x04\xf6\xe03\xf7\xeb\x9e\xb5\xe9\x1e\x18\x01-\xfe\xe99\x1f\x85y\xc5\xa4e\xa5\x01z\x93\xcdzf\x83\x0f\x95h\x0b\x03\x9cz\xd5!\xa2\xfd\xdce\x88!N+\x1ai\n\x99#\x95UA\xfa\x9cV\xac\xd7\xbb\xa5\x08\xb9>\xc0U;\xa2\xb9*\xf8\x1e\xb4\x8a2\xdex\xa3\xc7\x96U\xf21\xc2\xe4\n\xd3\xb1\x81-\xd1e\xfb\x9b\xb9\xdf\xc8\xcf\xf8W9\x7f+\xc5/\xfa+\x00A\xfe\x01\x9c\xfdE[\xb3\xd6\xe0\xdc \x98\x08\xc1\x19>[\x91\x93\xf4c@\x1dC]* a3\x12:`\xe0\x8f\xc4U\xbb=F9x\x90\x91&p\xc3<\xfe5\xc9\xcd\xa8<\x00\xba2\xba\xe3\xe5\x081\x9f\xd4\xfe\x86\xa0\x83[\x8a_+m\xbc\xab\"vS\xc9\x1f\xd4Qp;\x8b\x98\x05\xc42`\x92\x08\xe5O?\xe7\xf1\xaf\x08\xf1\x8d\x89\xd3|B\xe4)\x03v\xeeE{>\x9d\xa9$\xaa\xab\x85enA\xc61\xedX\xde6\xf0\x9f\xf6\xf6\x9a\xc6\xd6=\xb7Q\xfc\xca6\x8f\x9b\xdb\"\xa6K\xa8t\xb1\xce\xd8\xbcWZ%\xbc\xe0\x00J\x80q\xce;\x1a\xce\xd4#\x91\xe0\x0b\xf7\xd4\x7f\x18\xf4\xac\x8f\r\xea\xcf\xa7]>\x99z6\r\xdb@n\xa0\xf7\xae\x86\xe9\"\xf9\x88\x1e_\x19 \x1cf\xbc\xf6\x9c*X\x1e\xa8\xf3\x99\xc3[\xcc\xf0\xb2\x80T\xe2\xa4\xb1\xb77\xb7\"2p\xbd]\xbd\x05nj\x1aTw\x12\t\x03\x9d\xc3\x8f\x97\xd3\xde\xacYX%\xba\x00\xa3\x08y\xc9\x19\xc9\xf7\xae\xb7Yr\xf9\x99\xf2\xb2\xccq\xaa)\x11\xb0\xc68A\x93\x8a\xa7\xa8\x11\x1d\xac\x922\xfc\xbe\xb5\xaf\x1a\xbb\x1f\x95\x8fL\x0c7\x00w\xaec\xc4\x9a\x94R\xaa\xda\xdb\xe7\x19\xf9\x8ds\xc2\xf2\x9d\x8aJ\xca\xe7M\xf0\xe1L\xd6\xeeH\xce&\xc98\xce\xd5\x1c\xe6\xbd\x89\xe1T\xb2\xc9\x1d\x17\x8a\xf2O\x86\x00D\xcc\x8c@%\xbb\x8e\x7f\xcf\x15\xebws*\xd9\xf5\xc6G\x15\xdeR9\x97\x968\xd9\x89\xc2\xb1?1<R\xa4\xded\x9bFJ8\xe0\xf4\x02\xab\xcd\x17\x9b+\xeen\x07\\\x9a\x9e(\xe4\x00*\xe0\r\xb8\xce9\x14\x01aW\xcb'n\x0b\xe7\xa60\x00\xa9\xb7\xb3\x19\x17`\x07$/\x19\xa8\xd4\x13r\x81\x86Wo-\xdb5s\xca_8\xb2\x92\x00\xc1\xa0\x08\"\xb70\x05BA\xc2\x80j\xd5\x9c;\xae\x83m\xf9\x97\xaezUG\x99\x94\xb1\np\x0f\x7f\xf1\xab\xbaU\xc8welc\xa1\xc8\xe9@\x17o\xbfw\x1e08\xf9\x86N:W\x90\xfcK\xbb\xd8\xd6v\x8a\xc45\xc3\x16\x93\x9e\xa0\x1c\x7f:\xf6\r[\rk\x83\x9cg\xaa\xf6\xff\x00=+\xe7\xcf\x1f]\xc9q\xae\xc5\x92vB\x9b\x10z\x1c\xe4\x9f\xc6\x93\r\x91\xb1i\x0cr\xd9\xc6X\x01\xb8~\\T\x97\t\xb1F\xd3\xb8\x9c\x82x\xc7\xd2\xa2\xf0\xf4\xff\x00l\xd2I\x057\xa1\xcf\xfb@zU\x8b\x88\xc9\x84\xae\xcf\x95\xb9\x07\x19\x1fZ\xf3\xbe\x19;\x8aG\x02\xc0\xa3\xb2\xb7\x0c\xa7\x04\x1ak\x968\xdb\xdf\xa6*\xfe\xafh\xc93N\xab\xc1\xff\x00X=\x0f\xaf\xd0\xd2i\x16\x86\xee\xed[o\xee\xe3;\x8f\xfbG\xb0\x15\xe8).^c3\xbc\xd2\xa7u\xb11\x97\\aG\xcc9\x18\xfeUz\xf2\xe2#\x1bK\x1a\xaa`d`\xf1\x9e\xe7\xd3\xfc\xf6\xaa0\x1cF\x01$\x009\xc8\xc0\xac\xbdoP6\xf6%7|\xce0\xbe\xe7\xda\xb8\xa57'd\\#m\xcez\xc6'\xba\xd7\xf2\xb8,d\x1d\xfd\xeb\xe8-\n\xdfe\x84) \nv\xe3\x00W\x91|?\xd1^\xf2\xf7\xed2#\x18\xd4\xe4\x1cuj\xf6\x8bX\xfc\xb8\xf6\xf9\x8e08\xcb)\"\xbbb\xac_A\xb3i\xeb!*\x1f\x03\xb6\x0e\x01\xae{Q\xd2\xa7\xb3\x90\xcdm4\xdb\x07\xdeN3\xf8\x9a\xea$\x99A\x01\xa4M\xc4pB\xf5\xaa73[J\xad\x0b0~0r1\x8f\xc6\xac\x93*\xc6\xfa\t\xe3\r\x85v^\x18n\x06\xa4\xb9$(tN\x9d\x015\x914Q\xe8\xf7\xa1`*\xa9!\xfb\xa5\xb2\x01\xfa`V\xa4\x0c\xf2\x0f\xe0#\xd0v\xa0\x07\td\x952\x91`\x81\xd0\x9a\x88,\xdd]J\x90x\xc5Jv$\xbb\xd6P\xb9\xea\xa4\xf3\x9aq\x85\xe4!\xb7\xf5\xecM\x00h\xe9FV\xfb\xdf\xce\xba\x04\x1cV>\x95lPu\xe2\xb6\xd5\x08\x1c\x9a\x87\xb8\x98c\x8a\x00\xa7\xe2\x8cP!\xb8\xa7b\x97\x14\xb4\xec\x07\xcf\xbdi\xb8\xefOe(\xc4\x1c\xe4Si\x80\xc6^\xf5\x13/\xb5X\xc6i\xa5h\x02\x05-\x1bnF*\xde\xa0\xe2\xb4\xad|M\xac\xd9\x0cE{&\x07f\xe6\xa8\xb2Te;\xd4N\x9c%\xf1!\xa6\xd1\xd1\xaf\xc4\x1de@\x0eQ\xbfJ\x86\x7f\x1bjS\x02\x08\x03>\xf5\x81\xb3\x9aO/\xf3\xa8Xzc\xe6d\xf7z\xb5\xe5\xe6w\xc8@>\x86\xb3\xccd\x9c\x9eMY)\xc7\xa5\x1b=\x85i\x18(\xecKw+\x88\xf1\xf4\xa7\x05\xe3\x18\xa9\xf6\x12x\xebF\xce\x07\x14\xc0\x88\x0f\xc3\xd2\x80\x0f~\xb56\xc3\xc6zQ\xb2\x98\x11\xe2\x90\x8eq\x8e*B\xbfZa^\xf4\x80f?:a\xf7\xe6\xa5<\xf7\xa6\xed\xc8\xa6\x04\xb6\x8c\xcb*\x95\x1c\x83^\x8f\xa5\xdccNP\\\x06\x03\xa0\xaf?\xb1\xe2e%r\x05v\xb6\x97\x10\xad\xb8\x0b\xd4\xf58\xe9B)\x16\xd2p\xef\xf2\xc8\xa8\xfd\x88\xc6i.\xda\x05#\xf7\x9b\x98\xf2J\xf3\xcde\xc9\x17\x9d!x\xd4\xb7\x1d\x12\x9c\xec\x96\xf0\xb3\x18\xd46:\x03\xd2\x81\x99Z\xbc\xe1\x9bb\x8cd\xe3wV\xa8/w*D\x12 [\x1c9_\xf0<Vu\xfe\xad\x03L\x15-\xd5\xe4\xdd\xd5\xe4\xc0\xfe\x99\xae\xa2\xc6\xdf\xed\xd6+\xbe\xdf\xe6\xe0\xaf\x18\xfc\x8d\x020\r\xf3\"\x18\xa6^\x0eF\xfc\x03Y\xd2\xdd\x08\xef\x14y\xa7c\x1f\xe2\x03\x1c\xf4>\x9f\xd6\xbaK\x9bt\x13y2\x84V\xcf\xc8H<\xff\x00,\xfe\xb5\xcej\xf6r\\\x96[p\x8cS\x93\x81\xb4\xe3\xfa\xfeT\x81\x9b\x1av\xadqo2\xb4S+Jz\xabt\x7f~\x7f\xa5w\x9a?\x89M\xdc\xaf\x14\xca\xa8\x17\x8d\xa0\xee\xcdy5\xb3\xb40\x0f>\xf0s\xd5Gq\xef\x9c\x83\xf9V\xdd\xaf\xda`dx\xa6\x84@Fp\xd9\xf2\xff\x00\x1c\xafZ`\x8e\xf3\xc4\x1e\x0c\xd1\xbcQ\x1edO&\xe7\x1f%\xc4C\x0c\xbf__\xc6\xb8\xbb\xaf\tx\xa3B\x07\x085{5\x18\r\x0bm\x91G\xba\x9e\xbf\x86k\xa7\xb2\xbfxb\x13\x19\x95\xe3\x0b\xd0n~=\x8eO\xeb[\xfaf\xb1\x1d\xcd\xb7\x9cd!\x0f\xf7\xb8\xfe\xbcT\xca\x11\x92\xb3\x03\xc9g\xbb\xb7\xb7!u+ymI\xea\xb2\xc6Q\xb3\xf8\xf5\xfc\xea#}\xa6\xdb'\x9a\xb7\xb0\x90y\xe5\xc6\x7f!\x93\x9a\xf6\xc9\xa1\xb5\xbd\x8c\xa5\xc4\t27\xf0\xc8\xa1\x85d\x9f\ni+.`\xd2\xed\x01\xceq\xe4(\xc7\xbfJ\xe7xe}\xd8\xcf\x1d\xb8\xbb\xbf\xd5\xd7\xcb\xd3,\xa6h\x8f- S\xcf\xd4\xf6\x1e\xd5\x936\x875\xb4\x9b\xee\xd84\xcf\xce\x07j\xfa\x12]./-\x97hU\x1c\x95\x0b\x855\xc5\xebV\x9a|W\x9fl\xb8\xf2\xc3F\x0e\x17\x8c\xb1\xad\xa3MAZ#\xd3\xa9\xcb\xf8>_\xb2\xcf\x97o$\xe4\xf0\x07\xde\xff\x00>\xb5\xdf]\xde\x99b\x11\xac\xb9b9\x00\xf6\xf4\xfc\xeb\xcd,&i\xf5\xa8\xe4\x1c+\xb1\x1b\x83\x1d\xc0c\x809\xae\xe2\xc9Q\x18\xb1\x0c\xfc\xfc\xdb\xb3\xf9}+A\\\xbdn\x18D\xc5\xfeQ\xd3\xa7&\xb4\xa2@\xb1\x9c\x83\xd3\xbfAY\xeaM\xce\xd2\x07\xc8\x0eAQ\xde\xb4\xa2I\x1e<\xb4n\xa0q\x96=\x7fZ\x00\xa2.\x9e\x1b\xa2\x18eX\xe0\xf1\xc6+C{>\x04C\xefw\x04b\x83nL\xd1\xc9\x1e\xed\xbd\x0e{U\x93i\xe6\x9c\xa8(\x07\xf7O4\xc0\xa9$J\xc0FCp9\xe3\xbdGfLS:\x82\xc7os\xde\xa6\x9c\xbc r\xce@\xc1j\xcck\xa3\xe6nF#\xb1;\xba\xd2\x03_U\xbf\x8c\xe9\xad.\xec\xe1y\xe7\xa5xN\xb8\x92_\xde\xcc\xec\xa7,\xdcq^\x85\xe2mE\xe3\xb3\xda\xce\xad\xcf\x04\xf2\x7f\x95sv:@\xbd\xd3\xaee\x84n\x91\x18|\xbf\xd3\x9e\xf5,{\xe8r\xfa=\xd4\xda5\xee&\xdd\x18<\xabc\x8c\xfa\x1fc]\xa8\x90\xdc\xdb\xa4\xa8\xdb\xa1\x94\x10\x08<)\xf4\xae\x8fF\xf0\xed\x96\xa3\xa4*][C#\x15\x192\xa1\xe7\xf1\xa1\xfe\x1b\xad\xbe\xef\xec\xdb\xe9\xed\x0b\x9c\x94$H\x87\xf0=\xbf\x1a\xc6\xad\x1ewu\xb8\xadm\x0e.\xee\xdfs0h\xf9\xc6=\xa9\xb6\xb6\xbeY\x00DW<\x8c\x0c\xe7\xf2\xae\xb9|\x0f\xac\xc8B>\xabj#'\xb5\xb9\xce}\xb9\xad(~\x1d\xc2]\x9bP\xbe\xbc\xb8\x04\x0e\x16M\x80\xff\x00\xdf<\xe3\xf1\xa9\x8d)\xecM\x8f?\xbe\xd5!\xb2\xc4G\xe7\x94\xf4\x8999\xa9\xf4_\x07\xea\xfe$\xbc[\xcdB\x17\xb7\xb6\xcf\xca\x19q\xc7\xd0\xf6\xf7\xafT\xd3|5\xe1\xfd\x0c\x13mk\x04ru\xdeWs\xfeg&\xac\\\xeb6\xb6\xe7dR&q\x90C\x0e?\xfa\xf5\xac)(\xeb\xd4\xa2M3G\xb5\xd2\xedR\x04\x89\x10 \xfcMG{\xab\xda\xc4\xfb\x13\xcb`:\x9d\xc3\x8f\xd2\xb9\xcb\xcf\x10Ay\x96\xb6\x90\xce\xcaq\xcb\xae\xdc\xfad\xff\x00\x81\xac\x8b\xadHI\xf3\xcdr\xb1\"\x8c\x80\x8d\xc1>\x83\x8e\x7f\x01Z\x81\xd6\xbe\xa2n\xa1\x02/\xbax \xa9\x0b\xf9\xe3\xf9Tr^yh\x14A\x19\xdc1\xcc\x83\xf9W\x1f\x0e\xa6go\xb3Dey3\x8c\xbc\xa0~k\x80Mh\xcb\x9b[s+\xaa4\xa3\x90O8>\xd9\xa0\t\xf5\xbb\x94U\x88\xef\n\xf9\xe5\x01\xc8\x15n\xc5\x19\xad\xd6Q(m\xde\x87\x91\\\x9d\xc2\xcc\xde\\\x92)\xcc\xa7;\xb0\x08\x1e\xd5\xd2i\xee\xc2$\x0f&i\x88\xd6{WtV\xf9[\x9c\xf2*k\x7f7\xcc\x00(#\xebQ,\xc8\xd1\xf3\x90Gpx5%\xa4\x81\xdb\xf8\xb3\xe8M0:k\x04\n\x83\x82+DU+\x06&1\x91Z\x00\n\x91\r\xc5\x14\xfd\xb4c\x14\x08f)qKE0<s\xc5\x1a1\xb3\xb8ic\\)5\xcd\x02q^\xc1\xe2\xad9g\x86D#\xa8\xe2\xbc\x82e1N\xd1\xb7\x05N+8>\x80(8\xa5\x1e\xf4\xd0{f\x94U\x80\x10)\xa4S\xff\x00\n1\xcd\x00DW\x1fJ6\x0f\xc6\xa5\xda3F\xdcP\x04\x1bh\xc0\xf4\xa9\xb6\x9c\xd0W\xa6(\x02-\xb4\xbb1Rc\x8a1\x9a\x00\x8f\x02\x97o\x14\xfcb\x8d\xb4\x01\x1e\xc1\x8aa\x8f\xf4\xa9\xc8\xf5\xa6\x95\xa0\n\xc5i\x9by\xe6\xac\x95\xfaSq\xceq\x9a\x00\xb3hP0\xed\x8fZ\xdd\x81\x88\x8b\x8c\x8f\xf7{V\x1d\xacN\xcd\xc0\xc6z\x8a\xe9\xecm\x0b\xc4F\xc6'\xdcPRc\xec%\xf2\x83/8\"\xb35\xbb\xa1\x08!\x0eY\xbaq\x9a\x9a{k\xf8\xa51\xc5\x0b\x1c\xf4\"\xaeZh\xc6<\\]\"\xb4\x98\xe41\xcd\x038\xfb}/\xcc\x91%\xb8eb\xc78(\x01\xc7\xe7]\xa5\x981\xc0\x10BUTq\x83\xd6\xa5\x9bNYW\"1\xf4\x1cT\xab\x88\xa3)\xb4+t\xc27_\xc2\x81\x19\x1a\xc4\x1fm\xb78\x88FW8m\xa7\xaf\xd4V%\x89\x96\xe3\xf7n\x0ce\x07\x0c\x18\x9c\x8f\xc8\x9a\xe9^^\x1a7\xc1ld\x07\x15\x8by\xa7\\C1\xbc\x80\x16\x1c\xfe\xee3\xb7\x9f\\\x10sB\x19\x8f-\x8e\xa4.\x08\\\x15\xcf\x058${\x1csVm\xe0\xfd\xd8\x17\xa8\x15C\x05\xc1\xdc\xad\x8f\xf7\x97\xfa\x8a\xa7w\xad\xc8?q=\xac\x88\x1b\xfb\xc4\xf3\xfc\xbf\x95B\xfa\x8c\xb1\xc4\x88\x1d\x91$8Q\xc1#\xf54\x12t\xad*\xdb\xa6\x1dCC\x8c\xaa\xcb#;/\xa63\xc6?Zv\x97\xac\xbf\x9e\x1b+\x0c)\x93\xe6\x10X?\xb0\xc0\xfe\xa2\xb0-\xae\x93c4\xd2<\xc7\xfey\x96\x08\t\xf5'\x8c\xd5\xd8\xaf\xe2;\x8a\xc7#I\xb8g{\x12\x17\xfd\xdf\x9b\x9a\x06zv\x95\xaeA(\x11\x92\x85\xf3\xc9\xdcx\xcdn\t\xe0 \x9f1q\x8e\xd5\xe4V\xfa\xacE<\xa4\xb7\x93\xe7\x07,@8\xf5\xc2\x8d\xbcWG\xa3\xc8\xb3]\"\xa2\xcc\xa1W\x95\x95yoC\x81\x80\x07\xe7@\x1d\x95\xed\xcc+\x16\t\x04\x1f^\x95\xe6\x1e$V\x99\x9fb\x99\x01bX\x8e9\xfd+\xb8\xbe\xb8VFL.\x00\xe9\x9eO\xe5\\u\xf5\xbf\xdae\xdb+\x05'#+\xeb\xfe\x14\x0c\xc8\xd0\xec\xfc\x8b\x85wO\x9d\xc1l\x9c~\x95\xd6\xc1\xa5\xdc\xa4\x0b)b\xaa\xdc\x92:\x8f\xc2\xb2\xecp%\xd9?\xcb\xb0c+\xd0\xfb\xfbWI\x15\xe1\x98*\xc2\xd9\xda0A\xebLD\xd6v\xc8\x889f\xc0\xe2\xaf&B\xe0\x1f\x97\xd3\xd2\xa0\x88\xec\xef\xf8\n\x9c\xb6\x01 \xf1H\x07\xc4\xec\xc0\x96P9\xc5H\xd2\xbe\xdeF=\xf1\xd2\xa9\xefg!\x8f\x1c\xf3\x8a\xb8w,yS\xbb4\xc0\x8d\x91&\\0\xea;\xd66\xa1\xa5\xb0\xfd\xe0\xd8\x00=\xebx\x95\x0b\x97?\x85Vf.\xc7\x81\x8e\x98=\xa8\x03\xcf<Gjd\xd3\\\xeeo\x94\xe7\x8e\xe7\xfc\xe6\xb1|7u=\x9d\xe0\x88d\x19\x08<\x91\xb4\x8fC\xeb]\xf6\xbbfY\x01X\xd4\x82FA\xe9\xef\\\xf4zC\xc7v\x8d\x1c\x8b\xbd\x8a\x80\x18\x03\xb7\x9f\xfe\xbd ;\xcd\r7\xdb\xaf\x98\x8a\xa3\xa8\xdaz\x1fj\xdb\x11&2\x87n\x07\x19\x1cV\x06\x94\xe2?\x95[\x0f\xc8\xc0\xe1O=\xb3V\xee\xb5\x06\x88\x93\xb7\xe4#<0\x04{\x8c\xd1\xb0\xcd\x17fE\xf9\x80\r\xecx5\x99u\xa8\xf9Q\x92\\\x10\xd9\xdaq\xfdzV\\\xba\xf4\xa5\xc2\x05\x05\t;pFN=\x8f\x1f\xe7\xbdfM{\xf6\xdbr\xd0\xec\x94\x8c\x92\x91d\x15?\xed.\x7f\x9eG\xd2\x80\x19\x7f\xa8\x997\xa8\x96f?\xdd+\xf2\xfe\x99\xfc\xab\x16k\xcc\xa2\xa4\x92\x18\xd5\xba,i\xb4\x13\xee\x07\xf55\x15\xc5\xea\x10\xcb\x14\xb2.\xdc\x83\xe6\r\xdb\x8f\xf7pA\x03\xf0\xc5Re\x13B3y$\xbc\x12\xc9#1\xc7\xb9S\x91\xf9\n\x04Y\x99L\xea\xf1\xda\xad\xc0bFJ\x9c!\x1e\x83\x1c~U^\x0bYU6Kx\xdb\x89\xff\x00T\xe0\x8c}I\x15O\xc9\x8c\xed\x91\x99\x14\xf6}\xf9\xdeG\xfb8\xfe\x94\xabg/\xda<\xf9|\xc6\x89~\xeam\x7f\x9b\xfc\xfd(\x03WO\xb0K+\x9f6G\x8eG\xc89.\x07\x1e\xde\xbf\x90\xad\x99\xe0[\xd4U\x87fT\xe4\x1d\xe4\xe0zt\xac\x9b).\xee\xe7U\x8a\xd2(\xed\xc1\xe4\x84\x0f\x9f\xa05\xd7\xdbA\n\xc4\x03\xb1F\xc7\x00.?LS@sZ\xc2\x98\xd2/5]\x18\x90\xbb\x03\xe4\x1f~\xb5sLP\xf8Q\x95\xf4\xe6\xa1\xf1dhmS\x05\n\xa9\xee\xa0\x1f\xa6it\x17\xf3\"\\8\xc0\xe8(\x03\xa1s\xe5DP\xaex\xe0\xd4\x9aa%\xc6@\xc6}(v\x12@9\xc9\x03\xa6*\xc6\x9a\tp:\x8ab:\xab \x0cc\x81W\xb1\x8a\xadj\xbbc\x03\x18\xab8\xa4!)\r-\x14\x80m\x14\xb8\xa3\x1c\xd0\x06~\xbf\x1eU\x8dx\x97\x88\"\x10k3(\xe8p\x7f:\xf6\xfdy\xc6\xc6\xaf\x12\xf1d\x83\xfbu\xc0\xec\xab\xfc\xab5\xb8\xd9\x9c\x8dR\x83\x9a\xaa\x8e\x08\xe4\xd4\xcazb\xb4\x11=\x00sL\x07\x034\xec\xe7\x1f\xe3@\x0bA\xf6\xa5\x03=\x058!\xc7O\xc2\x80\x19\x8e(\xdb\xcf4\xed\xa7\xb5=\x00\x18,\xa5\xa8\x02\x1f\xeb\xc5.3\xebW\x04\xf1\xae6\xdb)\xf7,i\xc6\xf9\xb3\xf2\xdb\xc0\xbf\xf0\x12\x7f\x99\xa6\x05\x10\x8eG\nO\xe1R\x08&a\x81\x1bsV\x8d\xfd\xc7\x1b\x04I\xfe\xecK\xfdE0\xeaW\xbd\xee\\g\xfb\xb8\x1f\xca\x80\x114\xdb\xc9>\xec-\xf9T\x83G\xb8\xcf\xefZ8\xff\x00\xdf`?\x99\xaa\xb2\\O&|\xc9\xe5o\xab\x93P\x91\xdf\x8c\xd2\x03@\xe9\xb6h3.\xa5\x00>\x88w\x1f\xd0\x1ahM.3\xfe\xb2yH\xfe\xea`~\xa6\xa8c\x1f\x959G#\xbd\x00l\xd9^[Gp\xa2\x1b/\xf8\x13>O\xe9\x8a\xec \xb8\x90\xc0\xbbQ0}\x17\x15\xce\xe8\x10\xdb\xca\xe0\xb6\xdd\xd5\xd8\xb4(\xb0\xedP8\x1cqT\xb6\x19\x8e\x02\xb5\xd7\xce]\x89\xed\x9a\xd8\x8a\xc1\x1dC`\x81Yp\xb4iz\x01\x03\x8e\xfe\x95\xbe\x93\xa9L\xe4q\xde\x90\xd9Fh#N\x0e0=k.\xf2;vF\t\xb7y\xf4\xe3\xf9V\x8d\xcc\x88\xcd\x82C\x02k\x9d\xd5\xae\x92\xde7*@n\xd8\x18\xc5\x0c\x0eCV\xb8h\xae\xc872\xf0~\xe4(\xc7\x1f\xa8\xad\x9d3_\xb1\xbb\x80[L\xcd\x1c\x84`\x07\x90\xee\xfc\xbf\xfa\xf5\xc4_\xca.\xaf\x19fY$;\xbf\x86@?L\x1a\xda\xb1\xb6\xfb:+AnQ\xf1\xdec\xfd\x08\xa4\x85\xd4\xd5\x9b\xc3\xb0<\xadu\x1b*\x90wtb\xc3\xf1\x07\x8a\xe7u\x85XeY\x03\xb8pFY\x9c\x92\x7f?\xea+q\xee\xb5$`S\x126>\xeb\x16 \x9f\xd7\x1f\x85s\xba\x82\xeaw\x93\x15\x96\xc3a\xcew\x00\x7f\x99'\x14\x0c\xb0\x93\xda\x1b0\xb2\xa03\xb7>bI\xc9\xcf\xa8\xe9\xfa\ne\x93D\xf2:\x9b\x92\x0e9V\\\x90G\xa6j\xb4\x9ae\xcb$r\xb4J\xa8\xa7\x04\xa89\xcf\xd4w\xad\xfbK\x05\x9bo\xd9\x13l\x8b\xfcN\x06\x1f\xf0\xeb\xf9\xd0\x06\xae\x97k4\xb1\xc6\xe1\"X\xf8\xc39\xda?\x11\x8f\xf1\xae\x96\x041\xae\xc0\xa5\x1d\x86\x0f\x97\x85V\x1dz\xe38\xfa\xd6E\xb5\x94q\x18\xdad\x85Y~l(\xc9S\xf9u\xadc\"L6\xa2\xb1\x8f\xb6\x08\x07\xf14\x011yUIeP1\xc7\xd7\xebY\xcdng\x95\x01*\xbbN[\xde\xb4\xa2\xb73\xc1\x82\xdbs\xc7\x1c\x9f\xa6{S^\x1f\xb1\xa30\x8dG\xcb\x80\x01\xe9\xf5\xa0g%\xa9\xa5\xca\\\x91\x11+\x1ey=\xf8\xf7\xadM:q\xb4\x02\xec02O'5\x1d\xccm%\xae\xe5e.3\xb8\x8e\xff\x00\x85Q\xd3\xee\x99\x17\x12\x1d\xc474\x86u\xcb|a\x01|\x97r\xdc)\\\x1a\x90\xddL\xc8\x17)\x9c\xf4\xac\xb8\xb54\x04\x80\x00c\xdcsK\xf6\xb1\xe7\x12\n\xf3\xdf5D\x9bA\xb0\x01lq\xd2\xa7[\x9f\xc4zzV\x1a\\\xb6\xcf\x9a^{\x1fJg\xda\xce\xe3\xbf\x18\xe80y\xa0\x0e\x85\xae\x94\x80\xa0\xe3=sT.nV\x0c\x90\xddy\xaa\xd1\\\xa8\x8fw@z\xf3\xc5ejr\x19\xeec\x8e6*\xa0\xe7 \xf5\xa0\x11=\xde\xad-\xd5\xdaD\xb1\xe1\x1a>\\\x1e*[{i\x80U\x93\"N\x81X\x81\x8c\xf7\xcd$P\x18\xe2/\xb4\x16\x1c\x0e3[\x9b<\xc8\xfc\xd6U\xc8Q\x80\r\x03*[,\x85\x1a\x17;{\xee\xce1\xcdG\xa8YEuo\xf6g\x0f\xe6'\xcc\x9f1R\x0f\xb1\xe8j\xebyK\x87df\x0c2F0G\x15R\xfe\x04\xba\x87\x11+\xb2d\x15PpV\x90\x1c\x8d\xc4w\xd1I\xe5\xb4\xf1\x98\xdf J\xcb\xc1?\xdde\xed\xf8c\xfa\xd5M\xd7\x11>\xef4\x86bIx\x88\\z\x9c\x12I\xfc\xbbV\xbd\xfd\xa4\xee\x92Z\xdc\xdc\x00\xb2\x1co\x93\x1f\x87'\xfa\xf1\xf4\xac\x1dCO\x96\xc1\xd1\x9d\x10\x00y\x96>\x06\x7f\x0c\xe7\xeb@\x10\xcfs,\xf2\xe1\x8a\x12\xff\x00(b\xc6=\xdcw\xa6\xdc\xc3\x7f%\xb0Ke\xf2\xb1\xc0o1F8\xf5\xeb\xfa\xd4\x12F\x98g\x99\x00#\xe5'\x07\x04\xf6$\xff\x00Z\x83\xedw\t,b\x12\xb1F\xbd]8\xc7n[4\x08\xd1\xb1\xf0\xf6\xa3up\x93\xdc\xef\x93n?xd\xe9\xfdk\xb2\xb4\xd3\xd2(\x95$E\xcf\xf7\x99\xba\xfeu\xccE\xaf\xdb[[\x08\xa5I7\xb1\xea\xa4s\xee\x08<\xfd+J\xd7[\x06\x06V\x88\x96<\x03#\x85-\xe8pG\xe9@\x1b\x0f\r\xa41\xe5 *\x01\xfb\xbb\x8e3\xfd*\xe5\xb3\xa3>\x19@\xc7\xa1#\x15\x83o\xad\xbe\xff\x00\"T#w\x01G\xcb\x91\xf4'\x07\xf0\xadk)dv\xdb\x8f\xa8\xc6)\x80\xcdr\xcd&\xb7\xceA^\xc5\x97v+\x9f\xd2\x95a\x98\xae\xf5\xce\xef\xe0?\xd2\xbbg\xb6\x12\xf1\x86\x1cV\x05\xd6\x9a\xf1\\4\xab\xf3\x0e\xe3\x18\xa0\x0b2_\x8b`\xb9S\x93\xdf\x15\xbb\xa0\x92\xd8b>S\xcf\"\xb9\x88\xad\x1e\xeaeW$\x01\xcf5\xdch\xf6\xa1\"Q\xd7\x140f\xf4[v\x8cT\x86\x99\x1a`T\x84b\x82F\x9aB)\xd4\x94\x80LQ\x8aZ1L\x0c=zl\x923^\x17\xaf\xdd\x8b\xadr\xe5\xd4\xe4\x06\xda?\n\xf5\x0f\x17j\xebgc<\xbb\xb0\xe4\x15A\xeek\xc6\xf2]\xcb\x1eI<\x93YGV6Y\x8d\xf8\xab\n\xe3\xbdT\x00\xfd)\xe0\x90Gz\xd2\xc2.\xab\x83\xde\xa6\x8f\xe6\xaa(I\"\xafG\xc0\xcf\xa54\x80\xb0\xa0\x05\xe0Q\xcf@9\xa6\x079\xebNR\t\xe6\x98\x12/\xafz\\P\x18\x11E0# \no9\xebS*3\x9e:U\x88\xed@\xed\xcd+\x01L)'\x91N\xf2I\xf5\xad4\xb4\x1e\x95!\xb6\x0b\xc6(\xb0\xaeb\x98\x08\xa64mZ\xef\x10\xaa\xef\x17>\xf4X.g\x10\xc3\xa8\xa9#\x00\x9er*r\x9d\xb1B\xc6G \x9f\xa6i43{G\xb3$\x87V\x04\x1a\xe9\xf0\xd1E\xf3|\xbc}k\x0f\xc3\xce\x81@l\xe7>\x95\xbfv\xe9\xe4w\xa7\xb2\x19\x8b+\x18\xef\x16L\x1c\x1e\xa4\xd5\x93\xa8\xc6\x7f\x8f\x8fL\xd6\x1e\xaa\xcc\xe3\xe4R\xcd\xd8f\xa9\xc1$\xc20\x0e7\x11\x80\n\xf4\xa9\xb9F\xfd\xc6\xa5\x04\x10\x997\x0c\x81\xd7=+\x81\xd7\xfcF\xb3+G\n'^]\xdb\xfaV\xa6\xa7\xa6^]\xa1\x02F\x1e\xe0qX\xd0\xf8D+\xef\x96en{\x9c\xd1\xb8\x8c\x8d.\xdde\x93\xce\xc3Hs\xe9\xb4g\xfa\xd7S\x02\xca\xa8\x04j\x83\x1dp9\xfc\xebR\xca\xc6\xce\xdc\"$eUG>Zr\x7f\x13\x8a\xb6\xd6\xf6\xec\tHJ\xff\x00\xbc\xddh\x04\x8c\x9c\xcap7\x80=\xc04\xc2\xaf\x112\xc6\xe4\xb1\x1c\x8c\n\xb9*\x08\x86\x10}J\xd5Q\x04\xc5\xb2\xac\xaa=\xd8\xff\x00\x85!\x85\x9c?h\xb8\x12\\lbz(C\xc1\xfc\xebn\xcfN\xb5\x00\x81\x10\xf3\x06Hq\xd4\xfdES\xb2\xdfo \".\x0e3\xe5\xb8;\xbf:\xde\x8e\xfa\xde\x1f\xbd\x11\x88\x91\xd4\xa9Q\xf9\xd3\x02\x18\x98\xab\xaci\x04c\xb1\n\x9bsV\xed ie}\xd1\x05\x03\xee\xf2?\x0cU[\x8dJ\xd1\x0ca$VR\xdc|\xf9\xc1\xf6\xad\x1b[\xf8U\x03\r\xa0\x93\x80I\x19\x14 -\"\xc7\x02\x1c\xb3\x03\x8e\xbdsY\xf3\xfe\xf7\xee\x13\xcf\xf0\xd2Ip\x97\x17!\x15\x99I\xe3=\x8di%\x81e\x0c\x02\xf4\xf4\xe2\x809\x0b\xd5\xfb4\x87\x18\xc1\x1f\x85r\xf7W\xcd\x1b\x97Q\xb7\xd6\xbaO\x1cCumm\xe7Z\xf5S\x92\xb8\xea+\xcf\r\xeb_\x03\x81\xb0\x8e\x08<sKa\x9a\xdf\xf0\x91\x84;[<\xd5\xc8|G\x0e\x07\xcf\xcfrk\x99\x8e\xc4\xc9!.\xd9\xfaT\xff\x00\xd9\x91\xae\x0erj]H!\xaasgE/\x89\x91W\x08\xd9\xf6n\xf5\\x\x96P2O^\x9e\xd5\x85\xfd\x9cK\x1c\xb1\xc7ni[Nt\\\xa9\xa1U\x8fpt\xa6\x8e\x86\x0f\x12\xc8AVl\x0fc\x9a\xd4\xd2\xaeE\xc5\xd0\x95\x89\xc8\xeck\x89\x8a\x19\x95\xb9N\x87\xa8\xad;]n+)B\x85\xdd'L\x0csZ]=\x8c\xec\xd6\xe7\xa7\xc5+H\xbeX~r\rjE!\r\xb0\xe7'\xdb\x8a\xcb\xd0\xa1i\xad\xd2YAF\x90g\x15\xaf2*\xb0\xc09\xff\x00=\xe9\\\xa2Y\xadT\xe4\x82v\x91\xc5fK%\xc4N\xab\xb4\x95#h$\xe35\xad\x15\xd2p\xac21\xfaTW\xab\x0b\xc2J\xba\x90:d\xe0\xd0\x069\x91\xbc\xb2\xcd\x16\xf4\xc0\xc8\xeak\x0e\xfa\xdd\x9eR#\xb2/\x0bp|\xa1\x9c\x1f\xa7?\x95t\xf0@\xc4\xac\xa6R\xd8\xe8J\xfd\xe1\xee{\xd4\xf3D\xcc1\x19\x00\xfb\x82M\x16\x03\xc9\xf5M9\xe0\x9br\xcb\xe7/!VE-\xb7\xd0\x1e2\x05Mg\xa6Op\x06\nD\xa0\x0f\x9a%\xcf\xeaMv\xf7\xdaya\xbfa\x0c\x0e\x18\xa0\xc9\x1fL\xd5%\xb3\x8dX\xc8\xd0\xae@\xc0c\xc1\xfc\xb3B\x11\x85\x07\x87`\x03\x0c\x03\x02w|\xc3o>\xb8\x154\xdaz\x05\n\xac\x15\x87\x1c\x9d\xd8\x1fBku\x90\xe3\xe4Q\xfc\xaa\xb4\x80\xa6r\x17y\xe0nL\x03\xf8\x8a`p\xb3\xee\xb7\xd4v\xca\xc9\"\x03\xf22\x9d\xa4~\x06\xbd'\xc3\x8b\xbe\x05l\x16\x07\x90A\xe9\xf8W)}j\xd1;1\xb58\xeb\xc6\x18\x1f\xc3\xb5K\xa4x\x85m\xe4\xf2\xc01\x81\xd47Q@\x8fQ+\xf2\xafB\x08\xaa7\x912\r\xea\xa0\x83\xd4Uk\rO\xcf\x8f$\xee\x18\xeb\xde\xacMv\xa6>NGl\x8c\xd5\x01V\xda!,\xe1\x8cC\x1e\xa2\xba\xbd:\xdf\xcbA\x83\xf9\xd6&\x9f\xb1\xdc\x15\xc7\xe1]=\xb0\x01\x05H2u\x1cR\x91N\xc0\xa0\x8a\x043\x14\x98\xa7\x1aBh\x00\xc5\x14\x99\xa4&\x80>u\xf1&\xb6\xfa\xbd\xf1 \xfe\xe98QY\x08\xb8\xa6\xa2\x92}sW\xa0\x87o$sS\x18\xf2\xab\x01\x1a\xc4\xc4r0)\xdeS\x03\xd2\xac\xed8\xa7\xa2s\x9csWa\x0c\x86\x1d\xa0\x129\xa9pq\xc5XE\xf5\x03\x14\x8d\x1f\xa50 \x07\x1527\xcb\x9c\xd2\x08\t=\rY\x8e\xd0\x90(\x02 s\xd2\xa6\x8d\t5e,\x0f\xa5[\x8e\xc4\xf5\xc5;\x05\xc8\xa0\x88q\xc7\xe5W\x921\xd7h\xfaS\xa3\xb7+\xda\xa6\xf2\xf0*\xac\"-\xa0\x0e\x00\x06\x98O\xa8\x15#dS\t\x07\xb5\x00DH9\xca\x8a\x81\xf6\xe7\x98\xc1\xf7\xcdXa\xe9P8\xe0\xd2`BH\xcf\xfa\xb5\xfc\xcd&\xe5,1\x14\x7f\x88?\xe3C\n\x8cgw\\T\xb44u:C\xaa\xa8\x18\x84\x1fL\x7f\xf5\xebV\xea\xe0\x08\x88\xda\xa7\xfe\x03\\\xe6\x953.\x15\xd7#\xb5o\x91\xe6'\xca\xbd}i2\x91\x8f+\x1e[\xca\x8f?\xee\xd5\t'\x96\x197\xacv\xbf\x8a\x1a\xdf\x92\x00\x8as\x82k>D\xde\xc7\x8e\x9e\xc6\x91E\x14\xd5^f\x01\xe2\x84\xf3\xfd\xc3\xfe5?\xda\xa3p\x17\xc9\xb6a\xdf\n\x7f\xc6\xabMl\x9b\xcb2\x9c\x0fST%\x93\xec\xe7\xe4\x07n;R\xb8X\xda[\xb8_1\xad\xad\xbe\xd09\xc0a\xfdh\x92\xf9\n\x1cY\xc2\x14q\x92\xcc?\xadaE\xaam8H\xfeoS\xd0S<\xf13~\xf2V\xc0\xeb\x8a\x00\xd5:\x85\xb8\x076\x90\x8f_\x99\xb3\xfc\xe9\x16\xea\xdd\x8e\xe5\xb1\x8c\xfddj\xcbic|\"`/eA\xd7\xf1\xabpF\xc8\x98\xcf\x1e\x94\x01\xa9\x1c\xd1I\x8d\xb6\xd1\x0cs\xf7\x9b\x8f\xd6\xb5m\x10\xde\x8d\x91F3\xd0\xe0c\x02\xb9\xf8\xdd\xd5\xb0\x18u\x1f.\xdek\xb3\xd1\x1e8\xadI-\x96\xeaqLE+\xdf\n[O\x08.\xa7\xcc\x1f\xc5\xc7\x1f\xa5e]xn\xe2\x1d\x8c\x92\xc8vp0\xc0\x01\xf9\x0e+\xa6\xbc\xd5\xe2\x84\xf3\xb9\xb1\xed\xc0\xfck\x9d\xbc\xf1Q\x132\x8eP\x0e@=)\x0fQ\x80Kk$`\xc3,\x8f\x9c\x06R\t?\xe1]5\xa8\x95\xa3\r!'\x8e\x84\xf4\xac\x8b-n\xceX\x97.\x0bv\x04b\xb4\r\xc9e\xcae\x89\xf5\xa0\n\xfa\xdd\x8a^Z\xba\x109\x1d\xba\xd7\x8a\xf8\x8bG\x9bK\xbcw\x8b!I\xeb\x8e\xb5\xed\xf2\x13\"a\xf8\xf5\xe6\xb05=&\x1b\xe8\xdd\x1a1\x83\xdf\xad\x16\x11\xe1\xf0\xeaW0K\x97;\x85t\x11\\y\xb0\xab\xfa\x8a\xaf\xe2M\x10\xe9\x97Gh>Y<Tv\xd2*\xd9\x01\xbb\x90+*\x90M]\x1a\xd1\xa8\xd3\xb3/\xa4\xb9<U\rGXkw\xf2\xa3\x1b\x9a\x9dc*\x9c\xeea\xd6\xb3\xee\xe1\x12j\x04.\x18\xb1\xa8\x855\xcdfiV\xab\xe4\xf7I!\xd4\xaf%\xf9\x02\xf2k\xb2\xf0\x97\x84d\xba\x9do.U\x8f9\xc9\xebRx_\xc2^f\xc9\xee\x14\x829\x03\x15\xe9\xd66\xc9i\x08\n\xa3\xd3\x02\xba-m\x8ek\xb7\xb9~\xca\xd5a\xb7T\x00`UmI\xbe\xcc\x8e\xe5\xf2\x00\xe4\x1f\xe7V\xa2\x9d\x97\x03\xf4je\xdcks\t\x0f\xc5\x039\x89o\xc5\xccebb$\x1d\xd4r~\x94\xb1\xc3\xab\xcdh\xf9\x119\x0cq\xb9H'\xf2\xab\x89g\x1c.YJ\xb0\xcf\x188\"\xb5\xedo-\xd0\x00\x06\xd3\xdc\x1eh\x03\x9e\xb7:\xc4p\x06\x9a\xc5e`r\n\xc82?<T\xef\xae\\\xc4v]i\x17\xa3\xd2@\x10\xe3\xf2j\xeb\xe0h$\x8f\xe5(\xc0\xfaU+\xebX\xd7\x94\x00\x03\xd4c\x14\xc5s\x96}Y.H\xdfm:\x91\xd71g\xf9\x13C\xddY\xf9d\t\x02\xb9\xec\xeaW\xf9\xd6\xb5\xc0U\\\x028\xfe\x16\\\xd6|\xb2\x02\xbd\x17h\xfe\x12\xa3\xf9S\x03-\xe5\x96PAQ\x8e\xce\x9c\xff\x00Z\x89\x8c\xde^9\x913\xfd\xde\x95l\xc9j\xd2\x13\xf6d\xdd\x8e\xb1\x9d\x87\xf4\xa9!\x8e'\xc9\xb7\xb8*{\xa4\xc3?\xa8\xa4\x06d\xd6\xe9\"ac`\xdd\x86k\x96\xbf\xddorc\x9a\xd9\x83u\x0f\xfe\x7f\xc6\xbb\xf3n\xe7\x87@?\xdaS\x91Q\xdd\xe9\x8b2\xa9\x99\x03\x81\xd0\x91\xcd02\xb4[\xa6XT\x96<\x0e\xa3\xadi\xab\xc9w8\n\xcc\x98=q\xd6\xaa\xc3\xa5*?\xee\xd8\xaaz\x03\xd2\xb7\xf4\xfb1\x16\x08\xc7\xd4P\x80\xdd\xd2-BF\xa7p?\x85t\x11\xa6\x00\xe4VU\x88 \x0c\x9a\xd6N\x9di\x12IHM&i\t\xa0\x00\x9ani\t\xa4&\x95\xc0Z3M\xcd4\x9aW\x11\xf3u\xb4\x19\xc30\xc0\xab\x80zt\xa7\xaa\x00\xa0R\x95\xad\x12\x01\xaa3\xf4\xa9Tg\xfc)\x15\t5n(y\xa6\x00\x91\x16\x1c\n\x99-\x8b\x1eV\xadA\x0f\xb5hEn88\xa2\xc23\xe3\xb3\x04\xe7\x15z\x0b0:\xd5\xf4\x80`qR\x18\xc0\xe9U`+\xac\n\xbd*U\x84\xf4\x14\xf0\xb99\xa9\x91h\x11\x07\x95H\xd1`U\xcd\x9cS\x1d8\xa6\x06\\\x8b\x83\xcdWaW\xe6NzUI\x12\x90\x10\x1c\xd4.y5+\xe4v\xaa\xe4\x12h\x01\x18\xab\x1c\x10ED\xf1\xb63\x82G\xb5[\x8a\x06~T\x03N\x16\x929\xe5H\xc7qI\x8d\t\xa74h\xe3\xce\x90m\xf45\xd2Ev\x8c0\xac0+\x16==\xc0\xc9\\\x8fR+F\xd2-\xa3\x18A\xf5\x19\xa8e\xa2\xc3\r\xc3;\xb8\xf45\x0bDTg\xa5\x12;\x06\xc2\xae}\xc0\xc0\xa7o\x18\xf9\xd8\x12;v\xa4QM\xe3\xf3I\x19\x04{V}\xc46\xf1\xf0\xc0\xb3~\x95\xb0\xe42\x1c\x90\xa3\xb0\x1cU\x07PI\x0b\xcf\xbd&\x06\r\xc4I&B\x9c}\x05P\x9e\xd6\\\x00\xce6\x0e\xd5\xd1\xc9\x06O\x0b\x83\xebOM\x11\xe4\x8f\xce\xb8e\x82\x1f\xef\xc8p\x0f\xd3\xd7\xf0\xa4\x075l\xe6\"\x15T\xf3\xed[\xbau\xad\xcd\xc2\x16D\xf9\x07Vc\x85_\xa9<\nd\xb76\x16O\x8bX>\xd5 \xff\x00\x96\xb3|\xa8>\x8a9?\x89\x14\xdf\xb4\xde\xea\x0c\xa9$\xa5\xf9\xf9#\x00\x05_\xa2\x8e\x0505\xa3\x8a\xca\x01\xb9\xa4k\x86\xf4\x8f\xe5O\xfb\xe8\xf5\xfc\x05uzd \xd8\x87TT\x0c:(<~|\x9a\xc1\xb5\xd2\xa0\xb0T\x97R\x9b\x12\x1f\x98B9c\xf5\xf4\x1fZ\xd6\x83]\x85\xce\xc4\x1bUG<\xf0\x07\xf8\xd3\x11\x05\xee\x94\xd7\x9b\x84\x99\x8a/nX\xff\x00AX\xd3xj\xdc\xa9\x11\xee\x03\xa9\x19\xeb\xf5\xad\xbb\x8dj6C\xb0|\x9d\x8f\xaf\xbdd\xdcx\x80\x85\xdb\x0cM\x91\xd4\x01\x9aZ\x0fR\x94\xba\\\x91\xa2\x88[h^\x98^\x95j\xd8\xdc@\xa0\x97f\xe3\xab\x0c\x9a\x80kM\x90e\\\x9f\xee\x83\xfc\xe9\x1bZ\xf3\xc9T@\xab\xdf&\x80f\x8b\xde\xce\x13\x19\x03\xdb\x15\x17\xdaX\xe47\xde=\xf3Y2jL$\xda\x838\xef\x8a\xad-\xfd\xc8b\x1b\xa51\x13\xeb\xba:j6n\x1b\x06Lpq^M}i=\x84\xed\x1b\xe5Fx\xafO}vH\xc8F\x04\xe0s\xd8W+\xae\xbcW\xf2\xb0\x00\x11\x8eH\xf5\xa0\x19\xc8F\xee>\xe9 \xd7a\xe1}\x0eI\xe4[\x89\x93w=\x08\xac=>\xd1V\xe38\x04\xe7\x8c\xd7{c\xa9\x0bXT,x>\x80\xf4\xa1\x92\xb5:\xdbm\xb0\xa2\xa2\x80\x9bzV\x8d\xbd\xeb\x00FG\xb6;W5\x06\xa8dP\xec:\xd5\xd5\xbe\x8cs\x90\r\x05\x1b\xf7\x17^\\[\x88\xcf\x1d\xb9\xcddO\xab\xdc\xbc'b\x1flu\xa7\xc3}\x05\xca\x15g\x01\x97\xb1\xefR\xf9\xf6\xe0m*2:b\x90\xcc\x87\x17RF%\xdb)?\xc4\xa0T\xabetc\x8eXAr9\xdaN3\xf4=\xabR+\xf8\x10\x81\xd1\x07<\x8e\x95v\t\xed\x9a|F\xe0\x16\xe4\xae{\xd3H.P\xb1\xb9\x96\x052\x04\x95\xd4\x7f\xacU\x19e>\xa5z\xfeU\xa1q},\xd6\xcb5\xa3\xc58\xf4-\x8c\xfd\x0fc\xf5\xfd*\xe7\xd9\xe2iU\xd4\xec\x90p\x19\x7f\xaf\xa8\xaaz\x86\x9c\xb3\xc9\xe7\xc4\xc2\xde\xe7\xf8\xca\xfd\xc9G\xfb_\xe3\xd6\x99&Cjp\xcf(\x85\x84\x90N>\xf4S.\x0f\xe1\xea=\xc6i\x92\x0e\xbcpj\xed\xde\x9a'\xb7\xc4\xd0\x89T}\xe5a\xca\xfb\xff\x00\xf5\xc5R\x868m\xcf\x95$\xd2y]\x15\x9b\xe6+\xfe4\x86W\xf2\xe3\xe7 \x12:\x1e\xf5\x04\xad\xb3\x91\xd7\xfb\xc3\xfa\xd4\x9a\x9d\x95\xd4(%\x80\x89!<\t\x13\x91\xff\x00\xd65J\x04\xb8b\x03\x02}}\r\x00Z\x8a\xf9\x8c\x81I\xda\xff\x00\\V\xbc\x17S2\x84eR;\xe4\x7fJ\xcdK<\xfc\xc1Fj\xfc\x01\x87\xcaE;\x81\xa3\x0cv\xd2\x7f\x0f\x96\xde\xdd*\xdcV\xcd\x19\r\xc3/\xa8\xaa1\xc6O \x9f\xce\xaf\xdb\x87\xc8\xc3`\xfb\xd0\xc4l[ch\xe0U\xe5<U\x0bw |\xc0g\xda\xad+\x83Qq\x13\xee\xe2\x9aM3u!j.!KR\x13L-\xcd4\xb5+\x8a\xe3\xf3HZ\x98Z\x98Z\x95\xc2\xe7\x85\x80MH\xb1\xfa\x8ar.EO\x1ad\x8a\xdca\x0c\x03\x83\x83WV\x10(\x8e<\x0e*\xd2\xaf\xe3M\x08|\x10\xe3\x15\xa5\x0cc\x18\xaa\xd0.\x08\xad\x08\xc60)\x80\xe1\x1f\x14\x8c\xb8\x1d8\xa9\xc0\x18\xa6\xb8\x14\x08\x80\x0ejU\xe6\x9b\x8eiG\x1d\xe8\x02A\xd3\x14\xd7\xe9H_\x1di\xa5\xc1\xa6\x04\x12-Vx\xf7\x02}*\xdb\x90j\xb3\xe7\xb5 )\xc9\x1f\xb5F#\x19\xed\xf8\xd5\xb2\x03\x0e\xb8>\xf4\x82\xd5\x98\xf4#\xdcP\x03bb\x9c\x04\x07\xf1\xabp\xb4\xaep\x14\xfd\x08\xe6\xa6\xb4\xd3L\x83q\xc1\xc7a\xd6\xb7\xed-\x91\x14p\xa3\xf9\xfeu,\xa36\x0b\x0f4nu\xd8\x7f#S\x1bt\x85p\x01>\xe6\xb5\x19\x123\xf2\xd6u\xec\x8c\xaax\xa9e\xa3\x1e\xf1\x83p\x0fJ\xa5\xe7\x04\xe3\xa9\x1e\xd4\xfb\xa9\xe2\xc9\xdcW=\xf2i\x96\xe9\xf6\x81\xbc~\xee%\xfb\xce\xc7\n?\x1a\x92\x8b1G\xe6\x9c\xec\xfa\x13\xc5X]7+\xbef\t\x19\xeeO_\xa0\xefUN\xa9\x14\x1f-\xa2\xef#\xfeZ8\xe3\xf0\x1f\xe3Q\xbd\xe4\xaf\xb9\xe6}\xcc}\xe8\x11,\x97QZ\xe5m!\xcb\x7f\xcfY0O\xe0:\x0f\xd6\xb3\xeew\xdc\x1f2\xe2B\xce\x7f\xbcriC\xb4\x8d\x95\xc2\x8fqVm\xec\xbc\xc4\xf3\xaeX\xa49\xc6{\xb9\xf4Q\xdf\xf9P\x06d\x1ad\xb7\xd3yv\xf1g\x1c\xb3\x1e\x02\x8fS\xe9Zmuo\xa2\xaf\x91bD\x97}\x1e\xe4\x8e\x13\xfd\xd1\xfdj\xd5\xe5\xdf\xd9\xed<\x88\x10B\xa4d ==\xc9\xeek\x0e;sp\xe2(U\x9aY\x0e0\xbdM #\x13Mys\xb0\x17vc\xcf9f>\xf5\"[\x07\xb81\xca\xdf\xb8\x8f\xfdh\r\x80O\xa1?\xd3\xbdY\xb80h\xf6\xe5\"#\xcdpVIS\x92}U?\xab~UR\xcd\xe5\xbf\x94F\x02\xc5\n\x02\xcc\x7f\x865\xee}\xcd0:Ke\x86\xe2\x0f0\xa6\xd8\xc7\n\xbf\xde>\x95^\xf6\xde8\xd0\xc6\xb8\xf3O.\xdf\xdd\xf6\x15,\x17kof/d\xf9P\xfe\xee\xd9=\xbb\xb7\xd6\xabo\x17\nrpz\x9fj,\x06sCn\xa0\xb3\x90\xa8?Z\xa7\xe7\xc4\x1c\x94\x88\x05\xf5'\xa5^\xb8\xb73(#\xee\x12q\xf4\x1dMg\x98\xc2&H\xf9s\xc6{\xd0\x02(I\x9b \xe7\x14\xc9g#\xee\xa9l~\x15\x1b]F\xaa\x7f\\\n\xa5=\xf2*\x91\x1f&\x98\x8a\xda\x83\xb1;\xf3\x80O#<\xd6e\xd3\x85\x85\x98\x00\x00\xe2\xad\\\\\xbc\x8aB\xae\x00\xe35D\xabJ\xbb[\x04\x1a\x02\xd7*\xd8\xb6\xf9\t\x07\r\xda\xba\x0b' \x8c\xa0/\xe9X\xe9n\xc9\xf7W\x1d\x8f\x15v\xdeY\"tb:P\x16\xb1\xd3\xdb\xb22|\xdc\x1fA\xda\xb4U\x15\xa2*\xa3\xa7B+\x9f\x87P]\xd9#\x00\xf5\xad\x0b[\xf8\xdeA\x86\xc6G^\xd4\xec\x05\xd8\xa2`\x0eXg\xd7\xa6*H\xad\xa5\xdc[\xcc\xc9\x1dA\xee*\xa8\x7f1\x89\x07\x8e\x87\x15r\x05\x95\x90\x159\x00\xe3>\x94\xac\x05\xb8m\\\x1eO\xca\xdc\x8c\xd5\xcb}0\x89\x87\xceA\x1c\nu\xb2\xbb\xa9F\x1f2\x8c\x83\xef\xff\x00\xd7\xab\xd1\xe5\xa0.\xb9\xdc\x9d@\xf6\xa2\xc0YV+\x10-\x9c\xaf\r\x8e\xbfZ\x8aW\x9a\x07\x05_ \xf2\xbd\xc1\x15!\x90an\x0f#\x1b%Z\xab<\xf1D\xff\x00e\x99\xff\x00r\xe3tr\x0f\xe1\xcfC\xf4\xf5\xa6\"O\xb5\xa9O2<\xe1z\xa08d\xf7\x1e\xde\xd5R\xe2\xda+\xf8\xb7\xc2\xca\xaez0\xe1X\xfa\x1f\xee\xb7\xb5f\\=\xcd\xb5\xe1FR\x1d\x0eC\x03\xc1\x1f\xe1W`\x04\xe6\xe2\xdf\xe5$bH\xcf#\xf2\xf4\xa0ekAua;.J\xb7F\x04d0\xf4#\xbdi}\x9e\xde\xef\x06 \xb1M\xfd\xcc\xfc\xad\xf4?\xd0\xd5\x90\xb1\xdf\xc3\xb4\x9d\xb2\x01\xc1<\x91\xfe#\xf9U\x19-\xe5\x81\xf6\xb0\xc6)\x00\x08\x9a&\xda\xe0\x86\x07\x90EY\x8e0{Q\x1d\xca\xc8\xa2;\x9c\x908Y;\x8f\xf1\x14\xf6C\x01\x07;\x90\xf2\xac9\x06\x98\x12\xacm\xc0\x1c\xd6\x85\xbc*\xcb\x87Nk,J\xdb\x86\xde\xbe\xe6\xb5\xaddb\x83p\xe7\xebYN]\t/G\x18U\x00\n\x90q\xd0\xd4*\xf4\xed\xfcqSq\\\x97w\xad!l\x7f\x8dD_\x8aa\x93\x1d\r.b[%/M/\x8a\x84\xca\xa7\xaf\x1fJ\x8d\x9c\x81\x91\xcf\xd2\xa5\xc8\x96\xcb\x06J\x8c\xca\x05Ui\xc6y5\x0b\\\x0fZ\x87Q\x0b\x98\xf2\xb8NV\xad\xc62y\xac\xf8\x1fn2+B2:\xd7\xa0\x8d\x0bi\xf8\xd5\x98\xcf5Y1\xebR\x8e1L\r\x08\xbd\x85\\\x8c\xf0+6\x17\xe9W\xa3ph\x11qO\x1di\x8e\xdcR\x06\x01sQ3d\xe6\x80\x1eNi\xa5\x88\xa6o\xdari\x0c\x88\xfd\x0e\xd2=h\x00\xdf\xcf\xb5J\x13z\xe5\x0e}\xbb\xd5v\xed\x91\x83\xeb\xda\x9aK'#\xf4\xa62F\x0c\xa7\x91\xd2\xa0gZ\x91u\x1d\xbf,\xf1\x07\x1f\xde\x1c0\xff\x00\x1a\xb9l\xb6\x97\x871\xcb\xb4\xf4\xe7\xb5 )\xc3\x12\xcc\xd8R\x1b\xdb<\xd5\xf8\xe1t^\x06?\n\xbb\x0e\x87\x96\xdcv\xb7\xfbB\xb5\"\xd3B/ 6=z\xd4\xb64b\x8b\x91m\x1e\xe7\x00\x1f\xadg\x8f\x11\xc4\xb3\x18\xd8\xe3\x9f\xc2\xb6\xb5k0`o\x93#\xd0\xd7\x07:*^lUbI\xe9\xb4\x9a\x96\xf5-#\xb8\x86\xf0H\x99\x06\x96D3\x8d\xb8'5\x8f\xa7\x15\x821\xe6\xbe\x0f\xf7\x07'\xf1\xf4\xadCp\xce\x83n\x15}\x07z\x00\xc5\xbd\xb1\xb4\xb6\x94\xc8TK/\xf7A\xc2\xfe>\xb5\x91u\xe6JA\x94\xee\x0b\xf7S\x18U\xfa\x01],\xd1#r@5\x95x\x82 N\xd5 v\xa8e\x18\x82Y\x95HX\xc0\xf7\x1c\xd3\xa1\x93\r\xfb\xd3\xbf=\x16\x96i<\xd7\t\x10\xc1>\x82\xb4\xa2\xb7\x87I\x01\xeeJ\xcfzFV\x16\x1cG\xee\xde\xfe\xd4\x01=\xbcq\xdb\xa2Kt\x99f\x19\x8a\xdcpXz\x9fE\xfeue\xa6c\xfe\x911\x0c\xc0aT\x0c*\x8f@+'\xcc7\x17&I$\xdd#\x9c\xb3\x1e\xf4\xb7\x17\"WH!\xcbs\x81\x8e\xa4\xfd*\x84+\x19o.\x82(/+\x9c\x00*\xd5\xdc\xf0\xe8\xd0\x1bhJ\xbd\xdb\x0cJ\xc0\xfd\xd1\xe9\xfez\xfd:\x92\xce4X\xcd\xb4\x18\xfe\xd0\x90~\xf6N\xa6!\xfd\xd1\xef\xeb\xe9XF\"\x18\xb1,\xcd\xee{\xfb\xd4\xb0\x18\xdb\xeeg\x19%\xe4|\x0c\xff\x00@+n+0]t\xb8\xce\xd8\xd7\xf7\xb7\xb2\xfd9\xdb\xf8\x7f:\x86\xca\xd8\xd9[\x9b\xe9\x00\xf3\x9b\xe5\x84z\x1f_\xc2\xae^ \xd34Qo\x9f\xf4\xab\x9f\x9aS\xdc\x0e\xa0P\x06\x1e\xa3\xaa5\xde\xa8\x19F\"\x8c\x88\xe0\x8c\x7f\n\xf6\xfckJ\xcee\x92\xc9\xa4-\xc4\x8f\xb0s\xd4\x0e\xa6\xb1\x1e-\xa8\xf2\x15\xf9\x87\xca\xbfS\xd4\xfe_\xce\xae2\x95\x9a\xce\xd1I\xc0+\x19\xff\x00\xd0\x9f\xf9\x81LG@\xf81\xaa\x00:\x04\x1f\xcc\xd6u\xec\x0b\"\x80\x07\x02\xab\xb6\xa5\x8b\xa6\x00p\xaa[\xf19?\xe0*\xc5\xbd\xc2\\\xa2\x00y#$\x93@\xd1\x8dq\xa7\xee\x0c1\xde\xaa5\x86\xc5\xe4g\xdb\x15\xd4I\x00nq\x8a\xab$*\xae\x01\x194\x01\xce\xc9e\x85N1\x9acY`\x12\x00\xe7\x8e}+\xa2{el\x16\x1f\x87\xadG%\xa8p\xa4\x0e3\xd3\xe9H\xa3\x0cX\x86e\xc0\xcf\xadN\xbaz\x90A\xe35\xac\x96\xccc~\x07\x03\xfa\xd5\xdb{e\xf2\xc0\xc0\xc8\xa0/\xa1\x85\xfd\x96J\x06\x0b\x91\xed\xda\x9f\x0e\x96\xc1B\xf3\xd7\x83]\x14V\xe2)6\x1eP\xf4>\x95r;0\xac8\xc8\xedT\x88f-\x86\x9c\xe93\xee\x1c\x13[\xd6\xf6\xaa\xb9\x18\xe0\x8a\x91aTa\x9e)\x16\xe5R`\x87\xb1\xc51\x16W\x10\xdd#7\xddlT\x8f\"\xd9\\\xba\xae\x08\xceq\xebYw\x97L\xb1\x98\xc7\xdfE\xdc>\x99\xc5;Q\x98\x99,\xee\x87*\xe87}GZ\x00\xb0\xf7i\x1d\xe9\x81\x8f\xee%\x18\xcf\xa0<\x83Q5\xb3][\xc9i(\xfd\xfd\xb9-\x1f\xfbC\xb8\xfe\xb5\r\xc4K%\xba\xb0\x19h\x18\xc6\x7f\xdd\xea\xbf\xd6\xae,\x84\xc3\r\xe2\x1f\x9d\x08G\xfa\x8e\x87\xf2\xa4\x04\x16\xa1.\xa0[iH\xf3T~\xe5\xcf\xfe\x83\xfe\x14\x90\x96\xb7\x97\xa1\xe3\x82)\xd7\x91\x010\x96!\x88\xe5\xf9\x94\x0e\xc7\xb8\xfc\xea\xc3\x0f\xb5[\xf9\xe0~\xf9\x06$\x1e\xbf\xedP0o\x91\x96X\x8e\x01\xe4c\xb1\xab\xb1K\x15\xf4b'\xc2\xc9\xdb\x8e\xbfO\xf0\xac\xc8\x1c\x02c\x7f\xba\xde\xbd\xa8?$\x98-\xcfl\x1a`K5\xb3D\xe5X~4\xb6\xf2\x18\x89F\x05\xa3n\xa8z~\x1e\x95r\tV\xf5\x043p\xff\x00\xc1'\xaf\xd6\x98m\xa5\x85\xf6\xb7j\x99;!6J\x90 \xc4\x8b\xb9\xa3'\xf1\x1e\xc6\xae\xc6T\x0f\x95\xbf\n\xad\x03\x98\x8eF1\xdc\x1e\x86\xa7lm\xf3\x13;{\x8e\xe2\xb9e+\xea\x88l\xb0%\x03\xae(\xf3\x85Q3\x0e\xd4\xc6\x9b\x03\xadf\xea\x10\xe4^i\xbd\xea\x16\x9e\xa8\xb5\xce*\x07\xb9\xcdD\xaa\x90\xe4h5\xc7\x15\x03]\x11\xd0\xd5\x06\x9f=\xea&\x98\x9e\xf5\x93\xa9\xd8\x8ecE\xee\xc1\xc8a\x9f~\xf5]\xe6\x18\xf9[\xf3\xaa\r.{\xd4m)\x1d\xe9s\xb6.c\x88F\x18\x04U\xa8\xa5*G5\x98\x8f\x8e\xfdz\xd5\x94\x90g \xd7\xb6\x8e\xa3j\x19Cc\x9a\xb0\x1b\x1d3X\xf1\xcb\x83V\x96c\x8f\xbd\x8a\xa04\xa3p\r\\\x8eP\x06I\xacA1=\xe9\xe2\xe7\x1d\xe8\x11\xb9\xf6\x9c\xf4<R\x89+\x19n8\xeb\x9a\x9d.F@'\x14\x01\xa2_5\x0b\x1eO4\xc5\x91_\x80E;\xcbf<sL\x00I$}\t\xc7\xa7j\x99nb<I\x94'\xf8\x80\xe3\xf2\xa8$\x88\x801\xc0\xc78\xa8Bn82\xb8\xf7\x0b\x9aW\x19\xa4 Y\xf8P\xaf\xf48?\x95ji\x9a\"\x19\x03\xe0\xabU\r\"\xd7d\xc0\x99w)?t\xa6+\xb3\xb5\x11\xaa\x8c\x1f\xce\xa4\xa2\xc5\xbc\t\x1a\xe2\xa4b\x01\xe9\x9aiu\xf5\xe6\x99\xe6\x02})\x01\x1c\xf0\xa4\xa8A\xae/[\xb6{r\xedn\xa5\x0f\xf7\xd4\x02Mv\xac\xe4g\x18\xac\xdb\xe8\xd6x\xca\xb2\x8c\xd2hi\x9eqg>g\xce\xe5\x12\x8e\x0f\x05O\xe3]\x1c2\x16Q\xbb\x1fQY\xda\xcd\x9cV\xd2\x89R2\x1cu*j\x95\x9e\xb0\xb9\xf2\xe5\xdd\x19\x1e\xa3\xadM\xcagJ\xe4l\xc1\xc5Q\xb8\x81\xa7\xf9\x11X\xb1\xe9\x81\x9a\x9e\xd5\x9a\xe4\x81\x19\xdd\xeex\x00U\x99%X\x90\xa4\x07-\x8c4\x83\xf9\x0fj\x02\xe7-z\xcb\xa2\x82a\xdb-\xff\x00C'U\x87\xe9\xea\xde\xfd\xab\x9c73\xca\xec\x19\xce\xe69$\x1f\x98\x9f|\xf3]\x95\xe5\x8a\xca\xa4\x85\x04\xe2\xb9\xc94f\x9e\xe7j\xa9\x1f\x89\x15#\x1fj\x0c0\x85\x8c1\x9aN\x07\xadk\xa4\x90xr\xcb\xcf\xde\x1fQ\xb8_\xdd\x93\xce\xc5\xfe\xf0\xfe\x95V\xd2\xde\x0b8\x9e\xf2ug\xb7\x8b\x81\x96\xff\x00\\\xff\x00\xdd\x1f\xec\x8e\xf5\x91sv/\xef\x1ayw\xb4\xaerr\xdc\x0faLH\x9b\xfbEy`\xac\xeeO,F2\x7f\xad_\xd3\x0b_N\xaa\xc3j\xf5'\xd0\x0e\xb5\x99\xe5\x02\xc5U1[,\x1bN\xd2Dj\x00\x9e\xe7\xef{/\xff\x00^\x84\x06\x9c\x12\xa5\xdd\xe9\xb8l}\x9a\xd8\x00\x89\xd8\x9f\xe1\x1f\x89\xe6\xb3u\x9b\xe5\x92\xed\x97v\xe7\x03\x93\xefN\xc9\xb5\xf2,\xc1\xc7\x96\x9et\xc0wr8\x1f\x80\xc7\xebXR\x16iY\xb1\x8c\x9e\xb40/B\xa2K\xb8\x15\xf9T\x1ec\x01\xe9\xd4\xfe\x80Sb\x95\xa4\xd67\x9eLh\xee\x7f\xde \x93\xfa\x9a}\xbf\"\xea^v\x93\xe5\x8cv\x1dO\xe8?Z\xafb\xc4\xb5\xd4\x98\xe7\xcan~\xa4P2\x8c\xec\xe6V\x00\xf5J\xb7g;[[/<\x9e3\xedU\x0b\x13\xb4\x908\xe0\xd3\x1d\x9b8\x1c\xd2\xb8\x8e\xa2\xce\xf1e@\x19\x87\xca:\xd4\xef\xb5\xc8a\xd4\xf3\\\xf5\xa3\x10\x160q\xbb\xaf\xe3Z\x0bt#\x91\x89\xe0(\xe9\xfc\xaa\xae\x06\x83\xc6UG\x1c\xe2\x92D#h\x03\xb6i\x91\xdd\xab\x10\t\xcbc\x9a\xb2g\x8c<`\x90\x0b\x10)\x8a\xe1m\x16\xd7`GlU\xb8\xa1\xc1#\x1d\x05Fn\"@\xe7\xd0\xe2\x9c\xda\x84h\xaa\xf9\xea(\xb0\x13,*\x0f=\xf9\xa9\xfe\xd5\x1cqrFG\x1f\x8db\xcd\xaa\x17Pc\xfb\xddG\xb8\xaa\xb7w\x06T\xf3\x17 H>lv>\xb4\x08\xd4\xbd\xd4U$\x01X`\x8c\x83\xfa\xd6Y\xbasq\x9eH\xddY\xf2I!\x01O#\xf9U\xbbB\x1d\x94\xb78\xa5q\xd8\xbd<\xcc\xd3FO#\x05O\xd3&\xaen\xf3\xb4T?\xc5\x1b\x91\xf4\xff\x00<\xd5\x17\x8c\x91\x1b\x0eF\x0e\x7f:\xbd\x00\xdbgs\x91\x91\xbd_\x1f\xa1\xfet\x015\xac\xbed\xe6\x16\xe4M\x1e\xde}q\xc7\xea*]=\x91\x8c\x96\xc4\xe3\xcc\x1c\x03\xfd\xe1\xd2\xb3\xb2a\x9a6F\xe5{}\rKvZ\x1d@\xc9\x1e@\xc8\x91H\xec\x0f4\xc0\xbd\x03y\xd1\xcbl~\xf8\xf9\xd0{\x8e\xa3\xf2\xfeU\r\xbd\xe3[\xcc\x1cr\x07\x04\x1e\xe2\x92\xea_.\xf6;\xa8\xce7\xe1\xc6={\x8a.!A9t\x18G\x01\xd7\xd8\x1a\x00\xb1s\x1a\xf9\x82H\xb2bq\xb9O\xf4\xa9\xa3\xb7\x13\"\xb9?0\xf4\xa2\xc8\t\"6\xcd\xdf\x94'\xb1\xf4\xab6\xdf#\xed#\x1d\x8d0\x05A\x1a\xf1S\xa4\xcbt\x9eSq \xfb\x8cO_cM\x99v\x83\x81TL\x85[9\x19\x15\x85YX\x892\xc3\x16\x8d\x8a\xb0!\x87Pi\x12\xe4\xc6\xd9\x07\xf0\xf5\xa7\xb4\xa9w\x08,@u\xe3w\xf8\xfbV|\x9b\xa3b\xac0EqN\xe9\xdd\x18\xc9\x97\xa5`S\xcd\x8c\x9d\xbd\xc7\xf7MSy\xc9\xef\x9a\x8a;\x96\x89\xf29\x04`\x83\xd0\x8a[\x85\x1b\x04\xd1\x1f\xdd\x9f\xd3\xda\xa1\xaeetK\xd4C-F\xce\r@\xd2\x8fZ\x8d\xa5\xf7\xa9\xe5\xb9\x16&g\xc7z\x89\xa5\xa8Z^*\x17\x98SP\x0b\x13\xb4\xdc\xd4-7\xbdV\x92P*\xbb\xcd\x81\xd7\x9aj\x03\xb1\xce\x0e>\xb4\xf4\xe3\xa1\xaa\xfeg<b\x9e$\xafa\x1dE\xa1!^\x86\xa4K\x96\x1c\x13\x9a\xa4_\xf9R\xef\x06\x98\x8d16z7\xe1K\xe6\x1a\xcd\x0f\xe8i\xc2g\xc8\x03$\xf6\x03\x9aw\x11\xa4\xb2\x91\xd0\xd3\x85\xe2\xaf\xde\xabZf\x83\xa8_\x81\x88\xa4\xda{m\xae\x96\xdf\xe1\xdc\xd3\x00Z'\xdd\xe8[\x1f\xa7\xff\x00^\xb2u\xe0\xb4L\xaeVr\xb0\xea\x91n\xda\xe0s\xd0\x96\xc0\xfcj\xc3j\x17p>\xcf\xb3\x82;`\x12~\xb5\xd5?\xc3\xfb\xe8\x87\xee\xe0\xb6_\xa2n?\x99\xcf\xe9T\xa5\xd05\x9bH\xca\\\x16\x96\xdc\x1eP\x0c\x15\xff\x00w\x1c\xff\x00J=\xaac\xb1\x90\x97\x9a\x93\r\xd1\xf9j;\xe7\x83\xf9V\x95\x9c\xb3N\tYC?B\xab\x8f\xebQ\x8d\x0e\xd5\x98M\nJH\xfe\xe8\xc1\x07\xdcv5\xd0\xe8\xb6\xeaF\xe6\nO\xa9\\\x1a\xab\xdc,\x16\xa6\xe0C\xf3\xc7\x82?\x88\xf3\xfc\xb1K%\xdd\xd4;]It=\x95\x87\xf25\xd0\x84\x08\x9cc\x15\x99w\xb0>\xd6\x8f\x86\xe3\x8f\xf3\x83Ld\xd67E\xd1Y\x8b\xe0\xff\x00\x0b/J\xd1\x0c\n\xd6\r\xb2\"7\xee\xc9\\v\x1c\x0f\xca\xb4\xd6F\xd9\xcf\x07\xbf\x14\x05\x89\x8b\xe0\x10Mg\xcc\xcc\xc7\nI\xa9\xe4|u\xe2\xaaH\xecI\xd82})0Fe\xf4m$l\x19T\xfb0\xaeB\xe3OX\xa67\x13?\xd9\xad\x94\xfc\xcc~`}\x80\x1dO\xb5w\x17\x01B\x13*o#\xf8{\x0f\xadqz\xfa\x19\xdf\xccc \xda0\x15xP=1\xda\xa1\x94\x8b6\x9a\xdc3\xc5\xe4\xd9#Gn\xbcma\xf37\xb9?\xd2\xb4\x92B\xd8<\x8a\xf3\xab}Q\xad\xaeJ`\xa8\xcfc\x8f\xe7]m\x85\xfa\xb4`\x83\x92Gzw\x03tna\x8cg=*D\xb3Y\x01\x8caA\x19\x91\xfd\x05A\x04\x80\xedP\x87\xccoC\xd2\xae\xddH\x90A\xf6U'q\xff\x00Xs\xfaP\x07+\xae0\xba\x99c\x8ftv\xd1|\xb1\xa8\\\xfe=z\xd6lV\xa8\x1c\r\xce\xdd\xf1\x91]\x14\xf6\xea\xe0\xe0\x03\xf5\xac\xa9b1n\xdb\xd7\xa0\x00\xd4\xb1\x92i\xd6\"I\xc3>B'\xcc\xdc\xd4\xd0\xab_j\xcfs8&\x18Ar\xbc\xe3\x03\xa0\xfeB\xa3\x9aSi\xa7\xac[\x89\x9an['\xa0\xa9\x92Ymt\x91\x93\x86\x9d\xb3\xd0\xe7`\xff\x00\xeb\xff\x00*b\"\x88\xcc\xe2\xea\xe1\xc0\x0f!\xe4\x9e\xf9\xac\xf3\x0e\x1f-\xc9\xf4\x15\xaf\x04\xa1\xec\xdb\x81\x9d\xde\xb5\\F\xac\xd8\xdc\xa3'\x9cP\xd0\xc6K\x11\xfb\x1a\xc4\x07%K\x10=\xce\x07\xe8)\x96pl[\x809&#\x9f\xccU\xfb\xa3\x10,\x80\xf0\xb8^\x0f\xa0\xa8\xedq\x89@\xf9A\x8c\xf5\x1dy\xa4\x06+@w\x10\x07'\x8a\x05\xb0Q\xcf^\xf8\xad\x11\xb4|\xa8\xb9&\x98\xea\xa82\xcd\xcf\xf2\xa5`+[\xc6\x16m\xe7\x96U$\n\x8d\x87\xefY\x8fs\xd3\xda\xac.K\x1d\xa0\xe0\x8ar\xdbm\x19?\xfe\xbab#W*K\x1e\x00\xa6\xb4\xce\xf3B\xc7\xf8i\xd2\x8cp=i\"\x88\xe4\xb3\x03\x8a.\xc7bcp\xc6f\x00\x9c\x1f\xf1\xa4\x91\x98@\xab\x93\xc6@\xa7,x\x93\x9fJ\x91\xa1\x04\x0c\xf1\xde\x9d\xc4W\x00\x92\x01\xcf\x15d\x83\xe5\x81\xeb\x9ei\x862\xb8lt<T\x84\xfc\xf8\x03 P\x02\xc5\n\xc81\x8e\xd5*\xda\x94\x19Q\xc9\xe2\x9d\x12\xff\x00\x12\x1e\xf5m\\\x1eG\\t\xf5\xa0\x08\xb6\xc9\x1e\xc1\xc9\xe2\xb4\xadYZ\xd6\xe8\x1f\xee\x8f\xd0\xd4I$l\xdc\xf5\x1cU\xe8#\x8c\xc6\xe3\x8f\x99\rRB\xb9FC\x13F\xbc\xe0\xd3/\xb3\xf6ky\x14\x86\xc0(}\xf1\xd3\xf9\xd4\xf2Y\xac\x91`r\x07j\x89\xecwi\xd2 c\xf2\xb8a\xed\x91\x8a\x18\x10\xa5\xc8\x9fMq\xce\xf8\x1b8\xff\x00d\xff\x00\xf5\xea{{\xb1qa\xc1\xcb@\xdf\xf8\xe9\xff\x00\xeb\xff\x00:\xcc\xb2\x89\xe0\xbe)#\x13\x1c\xaaQ\xb3\xefY\xd6\x93\xbe\x9f\xac5\xac\xfb\x84nLO\x9e\xc0\xf1\x9aC:\xa8\xae0\xc3i\xe4{\xd6\xd6\xf14K8\xeax\x7f\xady\xecZ\xa3Z\xdc\xc9\x04\xc8\xc1\xe3b\xa7\x8e\xe2\xba=+ZY\x1f\xc9!\xbc\xa9\x06\t\xf4=\x8d;\x89\x9b\x972\x03\x019\xe4{\xd6G\x99\x93\x9a{]\x98\xe6he\x18#\x83U\xe5`\t\xe4W5us)\x93\xc3q\xe5\xbes\xf2\x9e\x0854\xce\x98\t!\xc2\x1f\xb8\xff\x00\xdd\xfa\xfbVCK\xb7\xbf5f\xdeqq\x13@\xc7\xe6\xea\xb5\x8cWC4\x86\xcf\xba\x17\xda\xdfPs\xc1\x14\x90\xdey-\x83\xf3#p\xcb\xedP\x0b\xb1\x1f\xee.\x01h\xc1\xea:\xaf\xb8\xff\x00\n\xafp\x0cXp\xc1\xe3o\xba\xeb\xd0\xff\x00\xf5\xfd\xa8T\xec\xee\x87\xca[\xbb_/\x12FwD\xdd\rQi\xf9\xeb\x9a[{\xe5Bb\x97\xe6\x89\xba\x8fOz\xafy\x19\x81\xfa\xe5\x0f*\xde\xb5~\xca\xfa\xa0\xe5\x1c\xf3\xe3<\xd4\x0f?\xbdVys\x9a\x84\xc9\x92i\xaaa\xcaO$\xe7\x15Y\xe7\xfa\xd3\x19\xcdB\xee1\xebW\x1aer\x99\x84\xf3\xcd\x1b\xc7cS\xc7\xa6]JG\x97\x0b\x1fN*t\xd0/\xd8\xe3\xec\xef\xcf\xfb5\xb7\xb4\x8e\xd74\xb1DJzu\xa5\xf3\x81\xeb\xc5\\:=\xe43.\xf8\x9cg\xb1\x15J\xee\xd2X$ \xa9\xaaSOaXC>1\x83^\x8b\xe0o\t\x9b\xb2\x97wq\xe5\x9b\x94R>\xe8\xae#\xc3\x1ag\xf6\x9e\xb0\x8a\xeb\x98\xa3\xf9\x98z\xfa\n\xfa7\xc3\xdar\xdbZ\xa1\x03\x04\x8e\xf5\x8dF\xe4\xf9QI[R\xfd\x86\x97\x05\xacJ\xab\x1a\x8c\x0fJ\xd1\x11\xaa\x8c\x00)G\x03\xa5\x07\x1e\xff\x00\x85\\a\x18\xad\x11-\xdck\xaa\xe3\x9cVu\xe6\xc0\xa4\x99!\x00vq\xb8\xd5\xd9Y\xd4dm_v\xaewT\xbb\x0c\x02\x16S\xdc\xb7\xa5P\xd1\x85\xaa\x88>\xd2\x1a8\xa4\xf3\x0fW.q\x8f`j\xc5\x9c8\x80ewg\xbe+>wi\xa6 >\xfc\x1e\xfc\xd6\x95\x934k\x89\x01\x1fJ\xa4\xac6LP\x8e\x9c\n\xadqm\xe6\xa6\n\xee\xf7\x07\x91W\x98\xa9\xf9\x81\xaa\xf2\x13\xd8\xd3bFO\xd9\xee\"$\xabn\x1e\x95a.\x8e\x02:a\xbe\x95cn\xee\xbc}i\xaeQ{`\x8e\xe6\x90\xca\xd3\xe5F\xe1&\x07\xa1\xef\xf4\xaa\x86\xf0\x93\xb5\x14\xa6}O&\x9fx2\x85\xb9>\x85O5\xcdO\xaa\xcd\x14\xa5A\x01}\x08\xda\x7f\xfa\xf4\xae;\x1b\xb2\x90\xeaU\x88\xfc\xeb.\xfe\xde\x19\xa0 d\x91\xc7\xcb\xd4~t\xd8/\x84\xe8\xae\xbd}\x87\x15a\x8a\x94%\x863\xed@\x1eg\xadZ\xad\xb5\xc1e\x127|\xe4U\xbd\x1e\xfd\x96?2TE\x8e<\x00I$\xb1\xf4\xad\xedoK3!1\xb3\x03\xeck\x9c\xb3\xd1\xe4m@\x19d+en7H\xc0\xe3>\xbf\x8fj\x846wv7\x8bki\xf6\xa7\xc3K'\x11\x0c\xf5\xf7\xa1&\xf3O\xccHn\xf5\xc9\xbe\xa8\xb7\x97\x9b\x8b\xedQ\xf2\xc7\x18pp\xbd\xabj\xca\xe4\xf1\xdb\xdc\xb0\xc5P\x1a\x8e\xaaW\x18'\xf1\xa8c\xb5I\xa5\x00\xa1\x08\xa7$\xd4\xd1\xc8\x99\x19}\xc4\xfaU\xc6\n\xb0\x15\x03\x06N:\xf6\xa0\x0eWQ\xdf-\xff\x00\x08\xad\xb8\xedPy\"\xa0\xd6\x18\xc9v \x8de\x0b\x02\x88\xc7\x1cq\xd7\xf5\xae\x96\x1bA\xf6\xa0\xecp\"\x05\xf8\x1d\xc7O\xe9X\xcf\xa3\x89\xa6i_v\x18\xe7\x07\xbf\xe3S`+[C)\xb3\xc1G\x1f78\xcf\xf8T\xf66\xcd\xf6\xa5b\xa7\x0b\x96 \x9fNkZ\x05\x8e\x0bS\x1a\xa0\xc8\xf4$\xd3#C\xb2V\xc8^6\x8f\xc6\x98\x14ZA\xf3c\xe6l\xf3\x83K\x06\xe3#\x02\x0eJ\x91\xed\xd2\xa5\xfb)^T\x8c\xfa\xd4a\xcaL\xa4\x83\xd7\xb7\x14\x99D\x0b\x1b\x96a\xd3\xd6\x8d\x88\x0e\x0fA\xde\xa5\x91XH\xca#<\xf7\xcdW\x94\xaa']\xad\xeei\x01eU\x158\x1f\xe2~\xb5\x0c\xb2)b7\x0c\x0e\xf5M\xdeV\xe7q\xc7a\x9a\xa6cw\x91As\x81\xc9\xa2\xe0h\x8d\xaex\xfc*\xc8\x03h\xe3\x81\xde\xb3\xe2\x0c\xc5\x88,\x17\xd5\x8dZ\x92\xe5J*F\xa4\x9c})\xa1\x1366\x86\x1e\xb5h\x81\xc6k?\xe6}\xa3=*\xc7\x9b\x98\xf6\x8e\x18`\x83@\x16cA$l;\xd4^N\x0e\xea\x8e\x0b\xad\xa1\x95\x88\xcfZq\xb9=\x08,\xa6\x9a\x110\x05H#\xf1\xa9\xd2d\x00\x93\x8c\xfb\n\xcc7\xcc\xb1\x9d\xbf6:\xe7\xadD\xf7\xbb\xd3tk\xcfR\t\xa2\xe0j\x8b\x98\xdb\x8c\xe4\x8e\xf5b\xd3Q_\xb5\x18\xd9\x8f\x08\xd8\xfc\xab\x0e+\x87\x9b\xe6\x08\x0b\x0fo\xebWlme\x92\xeb*J\x9d\x8d\xc19\x1d\r;\x81\xa1\xf6\xd6T8?\xad\x10\xea\x0e\xf1].\xcc\x81\x16\xec\xe7\xd0\x8a\xce\x8bN\xba\xfb\x8c\xe4\x83\xec\x0e+CN\xd1\xa5\x8eg\xfd\xe1*\xf1\xb2\xe0\x9c\xf5\x14\\\x0cy\xaee\x95\x81\x8c\x94\xe7\xaa\x8c\xd4\xda\x86\x93%\xdd\xc4Wk(\xfd\xe2\x82\xd9\xe3\x9e\xf5\xbbo\xa4*\x01\xb9U\xbe\xa2\xb5\xd2\xd1\r\xb0R\x07\xcax\xe2\x9d\x85s\x02M\x06;\xa7K\x87\xfb\xd2(\xdd\xc7q\xc1\xabQh\t\x16\x08s\xc5m$x\x84\xaf\x1f/\"\x9e\x18m\xc1\xfeT\xec\x172\xaf\xed\xbc\xcba'\x06H\xf0\xad\xee;\x1a\xc6\xb8c\xb3w~\xf5\xbf4\x9e\\\xd8$lq\x86\xcda\xde\xa0\x8eG\x8c\xe3\xd8\x83YN\x17!\xa31\xa6 \x91\x93LK\xa3\x1c\x81\xb3\x83\x9a\x8aR\x07\x19\xe6\xa9\xc8[\xb1\xa9T\xc9H\xd9\xbfu\x965\x9d:\x1e\xa2\xa8\xc3\xa8=\xbeW\x01\xe3o\xbd\x1bt?\xfd\x7fz\x8a\xd2\xeb*`s\xd7\xa5Q\x9ftr\x95\xf4\xaa\xe5*\xc6\x9c\xd1\xa4\xb1\xb4\xd6\x8c]\x17\x96C\xf7\x93\xeb\xea=\xe9\xb6\xd7\xaa\xe8m\xe7\xe5\x0fC\xe9Yq\xcf$R\t#r\x8c:\x10j\xde\xf8oz\xed\x86\xe3\xd7\xa2?\xf8\x1f\xd2\xa9E\x05\x84\xb9\x8d\xa1\x90\x8cq\xd8\xd5rs\xc8\xab\xf1\x86e6\xd7 \xab\x8f\xbaZ\xaaK\x11\x89\xf6\xb7jv\x02\x06\x06\xa2(sVz\xf6\xa4*\r0>\x81\xb2\xf0\xdd\x8d\xacH\x04\x08Xw#\xadh\x8d6\xd4r!A\x9fj\xb4\x0eS\x07\xa8\x19\x1e\xf4\x8a\xc1\xc1S\xd7\xa85\x92\xa5\x05\xd0\xab\xb32\xfbD\xb5\x9fk\x88#\xde\xbd8\xaeSX\xf0u\x85\xe8\x91\x16 \x93\x10q\xef]\xcc\xa6B\x85T\xe1\x87z\xa3rVh\xc6F$^\xf8\xe9O\x95\x08\xf3\xff\x00\x0c\xf8<\xe9\xb3\x17\x18%\xdf\x9c\x8e\x98\xafS\xb3\x8f\xcb\x85W\x1d\x05c\xc06J\x14\xb0\xc6r\x08\xef[\x0b,x\xc0p\x0f\xa3\x0e(\x8cl\xc1\x96\x0b\x8ep\t\xc5S\xbb\xbax\xa3\xca\"\x96\xf74\x93\xdd,p\x96\x99\xd5@\xf49\x1f\xca\xb9\xadGR\x96\xe1\xbc\xb8H\x11t\xc9\xc1\xfc\x8e2+@Hn\xa5\xafM\tu\xc6\x0f\xb8\xce\x07\xe3\xd2\xb0R\xef\xedR\x10$\x05\x9b\xb7\xff\x00\xaa\xa3\xbc\xb5.\xff\x00\xbc\x93vx\xc9\x04\xe6\xadi\xd6P\xab|\x83\xe6\xf5\xc6)\xa42\xcd\xad\xa3\xc6w\x1d\xb9\xfa\xd5\x97v\x0f\xf2\xc6}\xce3R\xbc%c9\xc1\xe2\xb2\xa5\x87{\x1c\x17\xce{9\x14\xc0\xd2Y\x83p\xc7\x1f\x859\xb2\x06F*\x841\xc9\x0e3\xe6c\xdd\xb3V\x8f#\x83\xf8\xd0\x16\x06\r\xc9-\x8a\x81\xce8.\x0e}EHY\x94`\xf2*\xbc\xac\x00\xeci\x01Z\xe3;N\x1b#\xd2\xb9me\x1d\xb2Q\n\xe3\xf8\x8a\x87\xff\x00\xf5WO(\xca\xfd\xecw\xac\xbb\xcbR\xeaJ\xaeOm\xa2\xa5\x94\x8eN\xc2\xfe\xf2\t\xbc\xb0\x80!<\xb1\xc9\xfd{V\xca\xddK \x18\xda\xc0\x7f\t?\xd6\xb3.\x18\xc51IV8\xb2z\xb0\xc9\x1f\x95h[B\x92)h\xdf>\xe0qE\xc0\xb6\x15\xddvl;\xdb\xb6s\x8a\xe6|T\xafig\xf6Kdn~iX7S\xe9]\x94\x16\xebk\x0f\x9eO\xcd\x8c/\x15\x8b\xa9\xda\x99Q\xda,\x99\x1b\x9d\xc7\x9c~\x14\x98\x1eq\xa7\x19\xa2\x90\xb9\x84*\xff\x00y\xdb\x03\xf9s]=\x8d\xc2\xb1\r#\x13\xd8u\x00\xfd\x05s\x9a\xa5\xa4\xb0\\\x93q4\x92\x1e\xc3q\"\xae\xe9\xb7\n\xc4\x0b\x92\x13\x8f\x94g\x1f\xa55\xa8\xb6;}>`\xf2\xa8 \x01\xeaMi\xcdrZL\x07\n\x00\xe3\x9a\xc0\xb3\xb9\xb7\xb7\xb62\x069<\x0cU\x88n\xd6T\xfd\xda\x0c\xe7<\x9c\xd03b\x12\x12\xd2Y\x00,\xccB\xd4\x06x\xa3\xe5\x83\x16\xf4\xebSm\x7f\xb2F\xb2\x0e\x08,E@\x8c\x11\x8f\x0b\x8fa\x9a\x00tR\x99\x8b(R\x83\xafJs@\x16\xdc\x96\xc9\xdc\xd9\xe0\xd3Z\x7fL/\xaez\xfeU\x15\xc4\xac\xd1 \xf9\xb0}F?\x9d *L\xcb\x9f\xa7\xaf5\x01\xbb\xf2\xce\xd5\\\xe6\xa5;\xa4'h8\x1d\xea70G\x84YC\xbf\xd75%\r\x9eg\x99\x03.\x14\x91U\xe2\xb1f}\xcc\xc5\x8f\xbd[\x8a\t\x1c\x92AT==\xe9f\x05\x06\xdf4\xa0\x1d@\xc5\x00@\xf0\xc7\x17\x01\x81c\xd7&\xab\xae\xc9\x03\xed\x18\xdb\xde\x95\xcc\x0e0\xd2\x003\xc8=M4\xcfm\x1a\x94!~\xf0 \x13\xc7\xe3E\x80F\x04\x003\xc1\xef\xebS\xf9b&Pz\x8e\xa7\xde\xaa\x1b\xb8\x1d\x8b\xbc\xa1Fy$\xd4S\xea\xa1\xb7\x08F\n\xf0\t\xef@\x1a\xb1\xe4\xc6\x08\x18 \xf5\xa9\xe3\x87\xf7\xd8#\xa0\xfc\xcdd\xa5\xf9\xf2\x88Q\xc9Q\xc9\xf5\xab\xb6\xf7\x8f6\xd6l\x07\x1f\xad\x1a\x01f[ \xd9;x\xfeT\xc8\xedYN\xdce{SE\xd4\xfei\x06<\xa9\xeaA\xc1\xcdO\x14\xc5\xb3\xbd\xf3\xdb\x9e\x08\xa6 \xfb\x00a\x9554\x16\n\xa7=\xfdi\xae\xb3o\r\x1b\xf1\xe9N\x8d\xdd2\x1c\x9fZ\x00\xb8\x96\xd0)\xe8\x01\xf5\x02\xaf\xda\xa2)$s\xf2\x9f\xe5T\")\"gp>\xb5r\xd8\xc6\xbb\xf0\xd8\xc2\x9a\xa4\x84\xc9\x10F\xbc\x9c/\xd6\xad[\xc9\x18\x91~_\xca\xab!\x8d\xf8,\rM\x17\x96\x93/'\x93\xe9LB5\xd2\x02x<\x1a\x96\xde\xe9X\x95\xce21P;D\xe4\xedq\x9e\xe2\xa9\xc8|\xb9\x03!\x19\x07\xa7J`k\xc30\xdeT\xb0\xe7\x8ao\x982FA\xfck\x9d\xbb\xd4$\xb3\xb9\xdcN\x10\xe1\xbdE$\xfa\xc2\xfd\xab\x03\xa3\x00\xc0\xfa\x83\xcd0\xb1sUf\x078#\xdc\x1a\xa1#\x1b\xab@\xeas$c\x07\xd4\x8a.5\x00c\xf9\x9f \xfbf\xa0\xb7\x91D\xbb\x95\x94\xabpW\xbd5\xa9\x0c\xa72\xee\x19\xebU\x8cY\xcf\x15\xa3s\x0b\xc33(\xe4u\x1e\xe2\xa3\x8e'\x94\xe0&O\xb0\xa2\xc22\x9a\"\xad\x91\xc74\xeb\xb8\xbc\xd8D\x83\xa8\xeb]\x0c>\x1e\xb8\xb9\xe8\xa4V\xbd\xbf\x82\xe6\xf2\x89`H\xee=\xaa\x1c\x92\x1a<\xd8\xa3\n\x07^Ez\x97\xfc+\xa2\xdc\x86\x18=\xea\xad\xc7\xc3y\x94\x12\x985\x9b\xa8\x90\xecpp]\xe1Ds\xaf\x99\x18\xe8G\xde_\xa1\xfe\x95}\xa1\x176\xfb\xe3a \x1f\xc5\x8e~\x84v5\xa1}\xe0\xbb\xfbPO\x96\xc7\x1e\xd5\x8c`\xbc\xd3\xa7\xdf\x86V\x1e\x9d\xc7\xbd8\xd4\x8c\xb6ab\xab)F\xc1\xe0\xd2\xa8\xadGHu(\xbc\xc4\xc4swS\xd0\xff\x00\x85Ph\x9e)\n:\x95a\xd75`}\x0e\xd7\x08#Y\x10\x92\xbd\x89\xed\xcf \xd3|\xf5l\x95l:`\x90=\r`6\xb1\xe6I\x94UQ&r\xa4\xf0x\xa4\x8a\xff\x00c\xabs\x95$s\xd7i\xech\x19\xd2\xc97\xfa\xb7+\xf2\xb9\xc3\x11\xda\xaaL\xfe[\xee\xcep0}\xea\x8d\xbd\xe6\xf8\x1e6$\xabr>\xb4\x92\xdc\x15V\x12\x1e\xb4XC^dS\xf36\xd29S\xebR.\xbe\xd1D<\xc3'\x1ce@?\xa5gN\xca\xe9\x81\xd0\xd6x\x8d\xd5\x8a\x9c\xed=\x0f\xa5U\x82\xc5\xdb\xdb\xbf\xed\t\x81)\x18#\xf8\xb0Aj\x80\x80\x83\r\x8f\xc3\x9a\x85\x0b\xa9\xda[ zT\x12\x96\x96`\xa0\x93\xecF\r\x00]\x8c\x06n\xa3\x1e\xc2\xb4\xa0H\xd3\x90\xa35R\xda\xd9\xc2\xa9\xdc\xc4{\x8a\xb1$\xcb\x18\xe4\x1f\xca\x98\x16\x18\xb3\x021\x81T\xa4\x8d\x19\xf9o\xd2\xa3\x92\xf3\x1c\x92\x17\xebQ$\xa2|\x98\xe4^:\xe3\x9aC$\x03\x0ev\xcb\xcf\xa0\xa9\x19\xdb\x04`\x12?\n\xa6\xee\xc0\x80\xd2\x0c\x0e\xc0d\x9a\x7f\x9aJ\x03\xb7\xe5\xf7<\xd2\x01\xd2Js\xce\x07\xe3U\xe5}\xdc\x1e\x7f\n\x97\xcf\x18\xf9q\xf9UI$,\xd8#\x8f\xad&2\tfU8\xc6F>\xb5U\x9f\xccS\xb1\x9b\x1d\xcf@)\xd7!@,\xac@\x1d\xc9\xae_R\xd6<\xb9J\x05\x91\x80\xfe\xe3p\x0f\xd6\x95\xc6^\xbd\x86,\xee\x1f;\x81\xd7\xa5M\xa7\xc3$\xf8.\x824\x1dzsYV\x93Kr\xe3c`\x11\x9c\xb1\xed[\xdedvp\xac!\x9bs\x8c\xb6O\xf2\xe2\x80'\x9bs\xfc\xaa~D\x18\x1d\xab>e\x8c\xe5F\xf0\xde\xc3\x8a\x9de\x93\x9f\x94\x91\xd0d\xf4\xfd)\xf2\x06\x91@\x1d}\xba\nL\x0e[T\xd3\x9anc\x18c\xd0\xb7\x07\xf0\xaeTi\xc2\xde\xf3\x9d\xf29?1\xe8\xbfL\x9a\xf4Ya\x0b\xbba.OV<\x0f\xa5gK\xa2\xf9\x85\xae$%H\x1f.\xda[\x03F\x04\xd7\x8b\n\xa4r\xcb\x1cq\xe3\x01\x14d\x9f\xc2\xa5\x87V[x\xf3\x11\t\xd8\xb4\x8d\x93\x9f\xa0\xe9\xf9\xd66\xab\xa4^\x89\x1f\xec\xf0\x10\x84\xf2\xecr\xcd\xf8\xd4:~\x87{%\xf4\ts\x94\x8fx\xe0\x8e?\x01O\x99\x0bS\xb6\x9f_Qp\xb0\xb9,\x11\x00\xcb\x1d\xb98\xec*\xc5\xae\xacn\x0e\"\x84\xc4\x0fR\xc3\xadV\xb3\xd2\xac\xe5\xbc\x96F\x84\xc8\xecN^L\x8c}1W\xa5\xd3\xad\xd0\x8f-\xd5}p\n\xd1q\xd8\x9f`u\xdc%`q\xe9\xb7\x15\x14\xf6N\xe9\x10\x19'\x1dI\xe4\xd5\xcbKx\x12?\x91\xf2:\xe7\x15e\xe2\x13\xaa\x98\xcb\xed^\xa4p(c0\xd7J\x00\x93s!a\xd9\x01\xe9O1[\xdb\x1c\xa4\x04\x13\xc0'\x8c\xd6\x98h\xa2f#\xe69\xc6H\xa2[h\xe7\x84\x86\xc0=\xb23\xfa\n\x96\x86bIv\xa0\xed$\xb3\x1e\x8a\x95\x8fyy{!1Y\xda\xb1\xcfW\xc7\x00{\x7f\x8du\x90\xe9v\xa1\xbae\xcf]\xc3\xadL\xea\xf1d\x14\x0e:\x00E\x00\xcf;k+\xf9PH\xce@\xef\xb7\x8ej\xab\xdbH\xc5\x83J\xe4\xff\x00\x119\xafN\xfb.\xd5b\x96\xc1I\xea\x08\xebT\xdfM\xdf\xf3\xecX\x98r\n\x8e\x94\xac\xc0\xf3\xf1f\xe5\xb3!!v\xe4\x12N1\xedR @~RN\xd1\xde\xbb+\x8d\x1e)\xe3-\x93\x18\xeaq\xd0\xfe\x15\x83s\xa48\xc8D\xc0\x1e\xbf\xd6\x93\x04W\x92\xfa4\x87hq\x8e\x07^MM\xfd\xa4-\xe1\x86E9'\x9e\x9f_\xf0\xaa/\xa53\xb0,\x1b\x8e\xde\x959\xd3\xf8P\x15\xdb\x03\x03<S\xb8X\xb6u\xceA\xceO\xa7\xa5Z\x8f\\Ip|\xaf\x9a\xb3\"\xd1\xa4\x92S\x91\xb4\x8e\xc7\xd2\xb5,\xb4!\xb7{\x1d\xde\x9d\x8d;\x88\xb1\x16\xba\x9bv\xec \x8e\xde\xb54\xba\x8c\x13G\x9f\x9d@\xeaq\xc8\xab+\xa2E\x1a\x16\x1f:\xb0\x18'\x19\xcf\xa5\\\x83I\x84\xf5\x83\xa7 \xe4\xf1U\xa8\x1c\xe3\xdc\xc9o&m\xe4.O!\xb3\xc1\xfc*\xc4\x1a\xcd\xc8\xb5\x9d\x99\x06\xe0\xa0\x1e=\xeb\xa3}\x1a\x0b\x84\xe7\n\xc0\xf0q\xd3\xda\x94\xe8\xe9\x04'\xf7a\xc3\x11\xd7\x9a\x15\xd0\x99\xc56\xbbwo>\xec#w\xe0\xf3\x8a\xd0\xb0\xf1,\xf7z\x84(b\xca\xe4}Et#B\x8aB7\xc3\x1f\xb6\x17\x91\xf8\xd5\xa8\xfc;l'IDj\x19OQ\xd6\x9d\xd8\x8e.]v\xea\x17r\x142\xee#\xe6\x04\x1f\xd2\xab\xcf\xaf\xccG\xca\xae\xbd\xces\xb4\xfe5\xd5\xdc\xf8dnv]\xcd\x93T\x7f\xe1\x1a\x8f\x8f\x91\x86=\r\x17c1\xae5\x1b\x8b\xcb[w*~a\xb4\xe0\x0e\xbfZ\xd2\x8e\xcc\xdc\xdb\xdbJ\x84\xa9PP\xfb\xfa\x7f:\xd2\x8bB\x8c[\x18\xd4gi\xc8\x15z\xde\xddc\xb5x\xf6\x95 \xe7\xe9Oq\x1c\xfc\xd6\xf2\xc6\xdf:\xe5=j\x15\x0c\xa4\x158\xc5k\xdf\xc8\xea1\xc3\x8f\xf6\x85T\xb5\x80\xdd\xce\xb1\x84\xc6O'\xda\xad\x10\xcb\x96\x96\x92j1\xa09\x05{\xf6\"\xba\xed\x1f\xc3\xca\x02\x92\x9f\x9dI\xa2\xe8\xe1c\n\xab\xb7\x1c\x83\xef]\x95\x8c\x0b\xe5)\x0b\x8fQ\xe8{\x8a\x86\xee\x16+\xdaiq\xc4\xa0\x04\x1f\x95i\xc7n\xab\xda\xa5T\xc0\xa7\xe2\x95\x86E\x14A~Lp:}*]\x83\xd0R\x9e0})h\xb0\xaeC%\xacR\x0c2\x03\xf8V\x06\xab\xe1;+\xd4lF\x03\x1fA]-\x04TN\x94d5&\x8f\x12\xd6|%q\xa6J\xd2B\xa4\xc7\x9eq\\\xfc\xbc|\xb2.\xe5\x1d\xbb\x8a\xf7\xbdF\xcd'\x89\x83(?Z\xf2\x7f\x16i\x02\xc2\xe0O\x1a\xfe\xe5\xce\x08\xf45\x9c%(;=\x8a\xdd\x1b\x10[\xb9\x05\x7f\x88\x1c\xa5^\x16\xf2<\\\x91\xbb\x18>\xf5L\xbc\xb1)#$\x80v\x929\xa8\x97Z{|\x1b\x94+\xb8\x8e{WH\x8d\x8bp\xf1\xa0\r\xeb\xcdY\x93\x12\xc6C~uJ\x0b\xf8\xae\"\x0c\x84\x1f^j\xc4s\xa31\x19\xa6\"\x06\x89\x926S\xf8\x11US\xcc\xf2\xcf\x9b\xf80\xe9\xf8\x8a\xd2\x90\xe1s\x90Gpk6\xef\xf7q\x1d\x8c\x00\xff\x00k\xa5\x0c\x0c\xc0\xb2\xb5\xf7\xcc\x8c\xb9?}\x0eA\xad\xcb{\"\x08l\x87>\xac\xb9\xaa\x1ap\x89\x9frJ\x18\x83\xc8\\\xd6\xf4O\xf2\xe0\x03@1\xca\x1c\x0c`\n\xadq\x82\x0fsS\xb3\xfa\xe2\xa9\xcb2\xe4\xe7#4\x98#\x02\xf71\xccY\xe1\xdc\x0f\xf1g\xa7\xe9T\xdbR1\x0f\x95\x02\xc28\x04\x9c\x0c\xfb\xf1\xfe5\xbbq\x10p[i?^\xf5\x81y\xa5\xcf,\x86L\x85n\xc6\xa7b\xf7\x01\xab*\x80\x1aU\x07\xd1?\xfa\xfc\xd5\xe8\xae\xbc\xdc\xe5\xc0\x03\xa795\xcfK\x1bC0F\xb7\xf3\xcf\xb8\xd9\xfa\xd4\xf0j\x16\xf1I\xb1c\x03\x1dHq\xb5\x7f\x1csM;\x85\x8d\xf5\xf9\x93*\xc4\xfa\x13N\x8eC\x109\xc1\xe3$\x9a\xcd\x86\xee\x17\xe3\xccg'\xa0P\x00\xab-w\n\xaf\x01G\xd4\x82\x7fJ\x04Gw \x946T\x15\xae?X\x85]\xf9\x08\xb9\xf4l\x13\xfaW]$\x0b2\xf9\x81\x95G\xd7\x9a\xcd\xb9\xb3\x13\xbe\xdc\x9c\x9e0\x05K\xdch\xca\xd1\x10\xb1\xde\xd1\x81\x12\x0c\r\xcez\xfe5\xac$BK9H\xf9\xe7\x0f\x92k:\xe5\x9a\xdeT\x83\xcde\x85F[o&\xad\xd9E\x15\xc3\xef\x01\x86:\x16\xc0\xcd\x00h\x0b\x84X\xc7\xccpz\x04\xeaG\xd4\xd3B\xcd9=\x02\xfa\x123Tf\xf3\xc4\xe0\x83\xbct,\xdc\xd5\x9b\x7f?r\x99\\\x9c\xf4D9\xfdE\x00Y\x10\x86`\n\xe0\x0e\x94\xf0#P\x17i\xeb\xe9R\xc3\x11g,Pq\xc67f\xa4gl`G\x8fA\xe9@\xcaS\xc6\xb8\xdc\xb1|\xdf\xde#\x91\xf4\xa2\xde\xda\x18gV\n\x0c\x9dI#\xfa\xd5\x86&<\x8c\x96s\xf9\n\x9e(\xd4cq\xf9\xca\xf3\xc5 )E\x1b\x16f\xc9^p0)\x05\xba\xb1\xc2\x80Ny\xcf9\xab\x92\xc8\xb1 \xc18\xf4\x02\xb3\xaen\xae\x8cl!U\x03\xfd\xac\x8c\xd0\x05\xa0\xa62\x13\x08\x00<\xe0g&\xad\xf9[\x90\x0c\xaeO\xbdeZ\xf9\xa9\xfe\xbe#\xb8\xf3\x90sS\xa4\xa0\xbf\xcd(\x18\xea2i\x88\xb2\xd0\x06<HH\x1d\x80\xa7\xa5\xb8\x0b\xd3vO$\x8atr1P\xb8E\\g\x95\xebRn\x98\xaf/\x19^\xc0\x0cQ`\xb8\x8b\x0c@l-\x929\xe2\xa3x\tb\xc89\x1crqR;\xaeA(2;\x83P5\xc3\x86\xe0\xe4{\n4\x00\x91\xd9\x06\x1c\x0fj\x83\xccYP\x82\xb8\xf5\xa2IRE,\xfd\x8f'\xa1\xfc\xaa\xa8,\xe44x9=\xe962I\xa0M\xb8P\t#\x91\xedY\xd7\x08P\x15PQs\xc8\xeb\x9a\xb9<F0\x0b1\x04u\xc78\xaa\xf22\xa0\xc3\xc8\x18u\x19\xa4\x06\\\x96\xca\xd2\x93\xbfh#\xe68\xef\xda\xa7Ku\x91\x14\x802\xa7\x18\xa6\xce\xfb\x86\xc0GZa\x9e\xe2)\x15\xd6\x06e\x1dp84\x86_\xfb0\xc0 \x10\xcb\xd4\x8e\xf5$\x98\x82<\xaf\xaf8\xe8j8\xae<\xdf\x99\x19\x808\xe0\xd5\xb8\xd4\xb0\xc3\x90\x01>\x82\xa9!\x12\xdb\\\x17]\xd8\x0c\x84`\x8d\xb5,s\x04\xce_`\xa6\xa4k\x1e\xe0\xbd\xcf94\xf1\xb3o\xcd\x8e\x0fBj\x90\x89#\xb8B0\x180=ja!1\xa8,H\xe4\xfd*\x99\x9e\xdcs\xb8\x05\x1f\x86*\x95\xfe\xadmj\xe1Zb\xa0\x0f\xf3\xda\x98\x8d\xc8\xe4%\x8e\x0e\x7f\n\xb3\x0c\x9f\xde\xe7\x1d\xfb\xd7+m\xe2\x1b&!\x85\xc9a\x9cc\x19\xfeU\xbde}\x04\xd9\xd8wdg8\xa0\x0b\x81\xf0\xc4\xafC\xd7\x14\xc6\x00\x9c\xed\xebP\x84\x98\x9d\xca\xc1\x87\xd3\x14\xf4\xb9\xd8\xdb\x1c\x11\xeeE0\x1e\xb1)\x1d\xb9\x15\x93<\x92\xc3r\xd1\x86\xf9X\x103Z\xc8\xe0\x9c\x02\x05V\xba\x88\xab\x16(\x1b\xbd1\x1c\xbd\xcc\xae\xef\x80p}\rn\xf8v\xc4\xe3\xccd\xc39\xe9YwJ\x82\xe9\x90)\x04\x9e;Wi\xa0A\x98\xd1\x08\xe5p\x08?\xce\x94\x9d\x91=N\xa3L\xb6\t\x1a\xf1Z\xa8\x9eT\xb9\xfe\x17\xeb\xec\x7f\xfa\xf5\x15\xa4{PU\xb2\xa1\x97i\xe8jP\xc7b\x8aD9\x18=G\x06\x9dLA@\xa2\x8a\x00)i)h\x11\x0c\xcb\x945\xc6x\x9a\xc1o,n #\x92\x84\xaf\xb1\x15\xda\xc9\xf7Msz\xa6<\xc1\x9f\\VSWV.'5eu\x1d\xccX\xc0#\x1cf\xac\xcf\xa7Cyo\xe4\xba\xf0H?\x95T\xb1\xb1\xfb\x10X\x81-\x1a\x8e3\xd4V\xb21\\\x02A\x1d\x8dl\xec\xc4b\xdb\xe8\xb2Y\xdd\xa9\x8eL!\\m\xedV/\xe3\x96\xde\x13<}Up\xc0V\xb6\xec\x80\x0f&\x98\xe8\xae\x85_\x90it\x1d\xf53\xe0\xb8-\x02\xe4\xfc\xd8\xe75^\xe5\xccHY\x14\xb4g\xa8\xf4\xa9\x1e\xdd\xa2\x98\x90F\xd3\xcdV3:9\x18V\x8c\xf0sN\xe0V\xb2b\x92\x97W|1\xe0+\x0c\x1a\xd8\x8a\xf8t9\xcdc\x08\xa3\x8em\xc16\x02rv\x9e*g\xbbX\x8e0A\xa7p6|\xe2\xcb\x95\xc9\x1e\xf5RW`NA\xaa\xd0\xde4\x9fq\x0e?\xbc9\xabhX\xe46\x0f\xd6\x93\x04E\xe6\x11\xf3`\xfbT\x12\xc9#\x12\\\xe4\x9e\x985a\xf8\x1c\xed>\xd5Rg\x93\xae\xce=\xa9\x0c\x8aK).#9PA\xec@\xac\xd9\xf4r\xaaY\xad\xd7\xfe\x00y\xadH\x18\x93\xf3\x129\xe8[5<\xbbq\xc1?\x8d&\x87s\x8a\xb8W\xb1\x97+\x0b\xa9<d\x1c\x93\xfa\xe2\xa6\xd3u\x1bm\xcd\xbelK\x9ecd\xc6?\x1ekf\xf6\xdc\\\x86V\x04\x01\\\xd5\xfe\x9a\xb0\x80\xc0\xb0\x1f\\\x93\xf8R\xbbA\xb9\xd3%\xecR\x8d\xb8S\xe8#$\xd5\x84\x86&\x0c\xcc\xa5\x8e?\x88\xf1\\^\x9d4M8W2 S\x82\x18\x8f\xe9[c\\\x82G\xf2!\x0f.>_\x94dS\xdcE\x8b\x8d9$\x04\xb4\x83\xe8\x0e\x05Q\x9a\xd6H\x18\x01 E\x1d\x15\x07&\xb4\xa2\x9fi\xcb\xb0\xf4\xd81\x9f\xc6\xab\xdd\x06\x9aL\xa4Dg\xdf\x9aM\x0c\x96\x07\x81\xc2\x99\x18;t\x00\x1f\xf3\xfaV\x89\x8e=\xbb\xb2\x15}\x07_\xf0\xac(c0\xcc\x00\x0b\xb8\xf5rz{\n\xdb\x8a\xd0\xb0Y\x18\xe7\x03\x8c\x8a\x00_0\x82v\x8d\xa0t\xf7\xa7\xb3,I\xbaI\x00\xc7\xa9\xa642>|\xb2\xa8\xbd\xdb?\xd6\xab\\Z\xb1\xca\x83\xb8\x81\xebHd\xd1\\\t$\xc8\xc0\x07\xa7\x15m\x18\"\xb1\x07\xb6Mc\xc7\x1c\xb1'\xdc\x0c\x7f\xd9\xe6\xa6F\x97%\x980\x04w4\\,Gq}\x14\x0e\x1c\xb9\xce{\xf45\x1cz\x82\\\xee\x1c`u\xe2\x89t\xf5\xba \xef*\xa7\x92s\xd2\xa5\xb6\xb7\x899]\x8cG\x19+\x93\xfc\xa8\x01\xb2,\xa5@\x88\x80Xq\x8e*\x9bi3J\xa6In[\xaev\x9cb\xb6\x9aW\x04\x87\x00\xafb\x05T\xb8\x91\x08\xf9\x95\x80\xff\x00w\x19\xa1\x80\x96\x96\xe61\x96m\xd9\xe8EiF\xe0\xfc\x80\xe7\x1d\xab\x0c\x19\t\x0c\x82@;m5\xb1n\xa0(\xca\xbe\xee\xe4\x1e)\xa12\xca\x18\xf0r\xa0~\x19\xa4\x96$pH\x00\x1fQM\x91\xc2\x0erA\xee*\xbb\\ \x1b\x818\x1e\x94\x00\x86\xc9\x14\x96.\xc4\x1e\xaa8\xa8Y\xe3\x81HT\x00\x9f^\xf4\xac\xf1\xcd\x1e\xefOV\xaaD6J\x85\xc8=\x0es\x8a\x9d\x862\xe2pT\xe5\xcf\xd3\xb5d\xbb<\xa4\xa2\x13\x91\xc7N\xa2\xb4\x1a\xd3\xccc\xc1#\xd8f\x99\xf6y\x04\xa8P~\xf3\xb1\xce\x08\xa5wq\x99\x86\xdaa  \xb9\xf4?\xd0\xd1\xf6\xab\x8btde%\x01\xc7#\xa7\xadtR\x02\xaa\x08B\x0ez\x9f\xe7Q\x04FGIU\x14\xb1\xe3\x8e\xa7\xff\x00\xd5OQ\x190\\\xc6\xc8\xc1[i\x1d\x89\xc6j\xd4z\x81\xdd\xb4\x90\xe3\x1c\x1cr)~\xc0\x8c\n\x802N\tV\xa9\x0e\x9a\xaeAx\x94\xb7Bq\x83M\x01V\xe7Pfb\xa8\xd9\x18\xeb\xd0\x8a\xa4\xf2\xdc\xe7\xfdnKzs\xbb\xf1\xeckv\x1b\x0bp@i1\xea\r_\x16h\xa3\x01\xb2\xbd\xb9\xebOQ;\x1c[I\xa8\x10\x03\x07-\x9eI<\xfe5J\xea\xda\xfa\xf9\xdb\xccW\xf2\xf3\xc6@\xc6+\xbcK]\x933c \x7f\t\x19\xa8\xe5\xb3FO\xf59\x1e\x8c\xbcS\x11\xc2\xa5\xa4P\xe3\x07q\x1f\xc2\xa9\x9f\xd4WE\xa1\xde\\\xa1d\xda\x8b\x18R@\x0b\xb7\x15\xa3&\x91\x14\xa3+\x10F\xeeB\xe0\xfe\x15n\xd6\x05\x86&C\x9e\x17\xa9\xa1 '\xb1\xb92`3\x12s\xd38\xad\t\x828\xc0r\x0f\xfb@\xd6d!C\x81\xf2g\xd7\xa6j\xeb0\\m\xcf\xf4\xaaBb\xe4\xa60y\x14\xb3K\xba,2\xf6\xebP\xbb\x96BA\xe4U9u\x00\x8aC\x11\xc1\xe9L\n\xd7(Zdl\xe7\x81\xc1=k\xb6\xd1\"\x04F\xc3\xae=y\x15\xc2]N\xaf\n\xcd\x11\xe5I\xc8\x15\xd6xcTI\xa2E,2;\xd4\xcd\xd9\x08\xf4;S\xf2\x80F\x0f\xf3\xabuR\xd5\xd6XGz\xb4\x01\x1e\xf4\x93\xba\x13\x11\x86\x1bw\xe0i\xd8\xa3\xa8\xa4\x1e\x9e\x94\xc4(\xa2\x96\x8a\x00JZJ\t\xc5\x00E3aMr\xfa\xa4\xa0J9\xe0\x1c\x9a\xde\xbc\x9c$g\x9a\xf3\x9f\x19\xebCN\xd1\xee\xa7\xdc\x04\x8c\xa64\xfa\x9a\xceL\xb4i\x89\x060\x07#\xbd\x02r0\rr\xf6\xbe'Y\x18$\x8aw\x13\x8cg5\xb2.\x12X\xf7\x1e\x05h\xc0\xd4\x13\x10s\x9f\xa58L\xa4\xf5\xac8\xee\xb6\xcaU\x9f\x0b\xdb\xde\x96k\xd1\x11\xcf\xf0\xfb\xd0\x06\xcc\xa5\x1dJ\xf5\xcds\xba\xa9\xf2\x01t\xe0\x8fz\xb2\x9a\x88\x91x\xebY\xb7\xb2\x97m\xa0\xa8\xdc;\xf3\x9a\x18!\xb1j\xf0:bR289Q\x8f\xce\x9e\xfbn#+\xca\x8e\xcc$ ~\xa6\xb9k\x89\xda\x19\xe4h\xb3\xe6\x0e\x1b\x9e\x9f\xe3Y\x87\xc4WV\xb3~\xf8\x99!'\xb8\xc8\xfc\xbf\xc2\x8b\xa6;X\xee#\xb5\x9a\xd8n3\xc9$c\xa8Q\xff\x00\xeb\xab\xb1j\xd6\xfd\x02LX\x7fxW/\xa6k0\xdd\x9c\xdb\xdc\x15?\xf3\xcd\x80\xc6=\x8dh\x8b\xa4\x91\xcf\x98\xa1\x88\xe7\xbei\x88\xe8#\xbcY\xb0\x07\x19\xff\x00g\x15&\x11\xc1\xef\xeek\x96\t\x03I\xb9%\x91\x1b\xd9\x9b\x1f\x96i$\xd4n-r2\\z\xb4\x87\xfc)h\x06\xf4\xd0/\x1f\"\x8ct\xe2\x85\x9bx\xda\xe1~\xb9\x1c\xd74\xde \xbc2\x0cD6\x0fG\xfe\x94\xb2k\xaa\x0e\x19d\x19\xe4\xfc\x9c~t\xb4\x19\xbd4E\xfe\xeb7=\xaa\x85\xc4|\x00\xe4n\xcf\x19\xaaI\xa8[\xec\xcb\xcb\xb3=\xb2EO\x1d\xed\xb9\\\x02\xd2\x11\xdf\x9a,\x05+\xbb\x1bt\x80\xaa\xcf\x18g\xfb\xdb\xf3\x92>\xbe\x95\x05\x8e\x9f\xf6p\xd8\x19C\xfci#\x10?*\x8bR\xbcW\x98\xfc\xbd:\x82\xdd)\xf6\xba\x8d\xbe\x9f\x11gB\xc5\xc6U\x01\xe0\x0fsH\rx\xad\xe3\x18\x02F\xf4\xc0\x06\xad\x0b\x0bi\xc0F\x9eB\xc7\x92\xa3\x8a\x87M\xbf\x82\xf8aK#\xe7\xee\x96\xc8\x15u\xff\x00s\xca\xa6H\xff\x00k\x19\xa6\xc7q\r\x80\x88\xa8\x865\x18\xeeNO\xe7W\xe1\\(IHf\xee;T\x16\x92\xc9(\x1b\x90 \xeaI\xa9\x9d\x90d'-\xdc\xd2\x01\xf72*\x81\xca\xe0t\x04\xff\x00J\xa4\xd2\\\xb1m\xab\x1a\xa6z\xb6(\x91\xa1\xc7\xce\xca9\xe8\r;\xc9F\xe41\x0b\xe9\xd4P\x03~\xd0\xdc+(b:\xe0qM\x8auwm\xca@\xc79\x14\xe6\xf2I\x19\x9fo\x1d\x14\xe2\x91^\xd9\x14\x85\xcbc?x\xee\xcd;\x00\x8e\xf0\xe4\x04r7v\xe6\x85W@\x021\\\x9eA\xe9T\xe4\xd4a\x8d\xfeN\x99\xe4t\xa7G\xac\xc2\xe7\x01\x87\x1d\x88\xc5+\x01|\xc17\x0c\x14\x1c\xfa\x1a\x91B\xc2\x87p8=s\xcdg\xc7\xe2;R\xec\xa7\xe5+\xd4U\xa5\xbc\xb7\xb8@\xcar\xa4f\x8b\n\xe3\xfc\xebg\x1b\\.>\x9cSX\xc6>x\xc0\xfa\x82iG\xd9\xdd\x0e\x0e\x0ej\x0f2\x08T\xeen7g\x14j=\x08n'\x00d\xc8\x14\xfb\x9a\xcc\xb9\xb9\x95r\xc8\xe7\x19\xe3\xde\xa6\x9d\xd6u-\x01\x0c*\xac\x92\x0f\xb3\x82\xear\xa4\xe1Oj\x87r\x91W\xfbFd\x98\x0c\xb0$V\xa5\x9c\xce\xe03*\x80G\xde<f\xb3c\x92\xd9\xd4I*\xb0\xcb\x11\x83\xcdk[}\x91\xa2V\xda\xbc\xf4\xc9&\x9a\xb8\x8d\x05\xdc\xf6\xe0FUA<\x11\xd8UH\xa1\x16\xad+\\H>V\xe0\xf58\xa4k\x91\x13\x14\x88\x00\xb8\xe9\x8aX!\x96\xe03>\t#\x18'\xb5\x00\\2\t\xe2g\xc6\xc5\x03\x8e+<#l*c;z\x8cU\xaf\xb3\xe1x\x966^\xa4n\xa4C\xb4\xe0\xa0en\xb9\x14\xd8\x8aq\xc3\x87\xca\x92\xdc\xf0\r_\xc3&J\xc4[\xe8h\x8a\x05R\x03as\xd0\x1e\x86\xad\xacX\xc1\\\x1fl\xd0\x81\x94\xca<\x89\x94\x01\x0fq\x8a\x96(\xd8/\xccN\x07\xa1\xab[\x01\xf9J`\xfdqL\t\x1eJ\x82Fx \xd5!\x0e,\xa8\x83' \xd5\x7f\xb5\xfe\xf3h-\x8e\xf9\x18\xa5\x91\xf6\xbe\xc0\x03c\xb7j\x8aiJ\xaev\xf1\xe9\xd4\xd5\x00\xf9\xa5D\x88\xb3!'\xb1\r\xd6\xab\xad\xef\xee\\J\x02r0\x1c\xff\x00Z\xaeu(\x83\x11\xe52\x1e\x98~?*{:OjG\xee\x9c\x13\xfezP\"\x1b\x8b\xe4\xb7V\xf32\x068?yG\xf8S\xecu4\xb8\x88\x12\xea\xc0q\x95=>\xa2\xa8^\x87\xb4\x89\xbc\xb8\xc6\xd29\xc8\xacTb\xb2y\xa8\xc9\x10'\x05D\x80\xa9\xfc(\xb8\x1d\xd8\x966\x8c\x95'#\xa8\xae{P\x97-&\xd3\x94\xc6\x0eO\"\x9fg\xaa@\xcb\xe5n;\xfd\x1b\xa1\xfa\x13Uf\xb8\xb7\x9aV\x01\n8\xe0\x83\xd4\xd0\xc0\xcd\x83Q\"\xdedf\xc3FG8\xcf\x15wJ\xd6\x85\x95\xe2\x1d\xc1C\x1e0x5\xce\xddF\xd0\\N\x99\xea\xa4\xa9\xcfZ\xcfK\x8c\xc5\x8c\xe4\x0e\xab\xfdED\x972\xb0\x1fHxw_\x8a\xe6%\x1b\xc6{\x8a\xeb\xa2\x95]A\x06\xbe[\xd1<S>\x9d*\x93#4k\xfc]\xd7\xfd\xe1\xfdk\xd7<9\xe3\xab{\xb4U3.\xefL\xd6*N\x0e\xd2\x06\xaezm!\xeb\x9a\xcb\xb5\xd6`\x99A\xdc*\xfa\\\xc4\xe3\x86\x15\xb2\x92d\xd8\x9a\x8ag\x9a\xbe\xa3\x8acN\x83\xbdP\xacHN*\t\xa7\x08\xa7&\xab\\_\xa2)\xf9\x86k\x97\xd6|Kok\x1b4\x93*\x81\xefR\xd9V-\xea\xfa\x92\xaa1,\x02\x8e\xa75\xe0\xbe6\xf1\x1bk\x1a\x97\x93\x0bf\xda\x13\x81\xfe\xd1\xf5\xab\xbe-\xf1\xc3\xea;\xadl\xd8\x88\xcf\x05\x81\xeb\\R\x0c\x9c\x9e\xb4\x92\xbb\xb83\xff\xd9"
 
 __dog__ = b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x01\x00d\x00d\x00\x00\xff\xdb\x00C\x00\x08\x06\x06\x07\x06\x05\x08\x07\x07\x07\t\t\x08\n\x0c\x14\r\x0c\x0b\x0b\x0c\x19\x12\x13\x0f\x14\x1d\x1a\x1f\x1e\x1d\x1a\x1c\x1c $.\' ",#\x1c\x1c(7),01444\x1f\'9=82<.342\xff\xdb\x00C\x01\t\t\t\x0c\x0b\x0c\x18\r\r\x182!\x1c!22222222222222222222222222222222222222222222222222\xff\xc0\x00\x11\x08\x02\x00\x02\x00\x03\x01"\x00\x02\x11\x01\x03\x11\x01\xff\xc4\x00\x1f\x00\x00\x01\x05\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x10\x00\x02\x01\x03\x03\x02\x04\x03\x05\x05\x04\x04\x00\x00\x01}\x01\x02\x03\x00\x04\x11\x05\x12!1A\x06\x13Qa\x07"q\x142\x81\x91\xa1\x08#B\xb1\xc1\x15R\xd1\xf0$3br\x82\t\n\x16\x17\x18\x19\x1a%&\'()*456789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe1\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf1\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xc4\x00\x1f\x01\x00\x03\x01\x01\x01\x01\x01\x01\x01\x01\x01\x00\x00\x00\x00\x00\x00\x01\x02\x03\x04\x05\x06\x07\x08\t\n\x0b\xff\xc4\x00\xb5\x11\x00\x02\x01\x02\x04\x04\x03\x04\x07\x05\x04\x04\x00\x01\x02w\x00\x01\x02\x03\x11\x04\x05!1\x06\x12AQ\x07aq\x13"2\x81\x08\x14B\x91\xa1\xb1\xc1\t#3R\xf0\x15br\xd1\n\x16$4\xe1%\xf1\x17\x18\x19\x1a&\'()*56789:CDEFGHIJSTUVWXYZcdefghijstuvwxyz\x82\x83\x84\x85\x86\x87\x88\x89\x8a\x92\x93\x94\x95\x96\x97\x98\x99\x9a\xa2\xa3\xa4\xa5\xa6\xa7\xa8\xa9\xaa\xb2\xb3\xb4\xb5\xb6\xb7\xb8\xb9\xba\xc2\xc3\xc4\xc5\xc6\xc7\xc8\xc9\xca\xd2\xd3\xd4\xd5\xd6\xd7\xd8\xd9\xda\xe2\xe3\xe4\xe5\xe6\xe7\xe8\xe9\xea\xf2\xf3\xf4\xf5\xf6\xf7\xf8\xf9\xfa\xff\xda\x00\x0c\x03\x01\x00\x02\x11\x03\x11\x00?\x00\xe0c\xb0\xb7(U\xa3\x05\x87\x03"\xac\x0b;v\x11\x8d\x98p?3P\xaa\x89X\xbe\t\xf5 \xff\x00:\x92Y\x19\x18\x01\x80\xdd\x85`\xeeth#Z\xc7n\x0eP\x13\xd9\xbb\xd3\xc2+\x00q\x95\'\xa1<P\xd7\x0b\xf2\xa0]\xccG#\xde\x8d\xec\x8aH\x00g\xf8ij=\x05h\xa3V\xca}6\xd24_/@I\\\x1c\x0e\xd4\xf3&\xf0\xa1\x93\x9e\xc6\xa2\xf2\xdc6O$v\xa6\x02\x88\xd3\xcae\'\x19\x1fZ\xa7%\x91\x92\x17U%\xbb&*\xf2BX\xe0\xe1S8$sOB\xb08\xc8\xcf\x18^:}h\xf4\r:\x9c\xc1\xd2\x9dg\xfb\xac9\x0b\x96\x1d*V\xd3%K\x859\xdc\xa0\xf5#\x8a\xe8e,\\\x9d\xd9\xcfc\xe9N.\x18\x83\x82G\xae8\xa7v.TcG\xa1\xbeD\xab>A<\x9ct\xab1i\xff\x00gl;\t8\xc1=+C\xed\x03\xcb\xc6\xd1\xc7ZI$\xe5\\\x03\xc9\xeb\x8a\x8b\xbe\xa6\x9c\xb1!_,\x8crv\xf4\xa63\xb6[\xa7\xcc1\xcf<U\x82\xe0\xb1\x1bAa\xd7\x15\x03\x14\xdav\xae\x0e~\xe9\xefKK\x83H\x8c1\x04\xa0*\xc7\x1ft\x9cb\xb7\xf4\xab8\xa4\xb4/<L[\x1c2c\x1f\x8ek\x1a"\xcd*\xaa\xa2\xe7?\x95l\xdd_K\r\xa8\xf9\xd6R\xac7\xec\\\x00{\x82\x07?\x8d6CI"h,\x11\xa2\x95\x03D\xaaT\x0c\xf3\xb7\xff\x00\xd7\xf8\xd6@\xd3\x1dn$\x08\xeab\x8d\x87\x1c\xf5\xef\x8a&\xd5\xdc\x05V\x8c\xb2\x1eLx\xc2\x9f\xae*\x172\xc4\xc6D\x95V&\xc7\x0b\xc0\xe7\xebQ\xa6\xe2Vc\x1d\xe6\xb0\x91\x9a\x11\x98\xf3\xc4\xa0`\x0f\xfe\xbdB\xf31\xc4\x92\x07i\x01-\xbc7\x06\xae\xfd\xb65UYWpa\x80\xcd\xea=\xe9\xa7\xec\xf2F\xae\xee\x15I\xc6\xde\xc6\x9a\xa9m,5\x15\xdc\xcci\x0eO\xc8A=\xc5iY^7\x96\xae\xd9\xc2\xf4\xcf\xf8S\x9a\x04|8\x93\xa9\xe8zT\xa9fT\xaf\xefU\x98\x9c\x90x\x0b\xf8\xd5s\xa6_!q\xee\xfc\xa8b\x0f\x8d\x92\x12N\xde\xb8\xa9b\xd4E\xdcf9\xa7uX\xc8<\x1eq\xf5\xac\xe6RD\x80:;/\x1c\xb7\xf2\xa8\xd6T\xea\x7f\x8b\xee\x901T\xb53q\xb1\xa5\xfd\xa9\x1c2\xb8\xb5r\xa9\x19\xca\x82q\x8a\xcd\xbc\xb8y\xec\xad\xd9\xa5V\x02B\x02\x85\x1f \xfa\xf5\xa2\xf64Kta\xb6B\xdc\xfe\xeb\x8f\xcf\xde\xa9\xaa\x1f/\x01@\xdc2\x0e\xee\x94O\xe1%\x8f\x86\xe1\x15$\xb7\x9eGH_\xe6S\x1e\tV\xfe\xa2\xad\xe8Z\xd4z)\x9a\xda\xeb\x17\x1au\xc9\xdc\xd1\x9c\x13\x9fc\xda\xb1g\xc2\xc5\xb2V\xfb\xc7\xd7\xbd1\x06\x10g\x0f\xb7\x90}j\x16\x89\x12\xcd]B\xee\xd6yW\xec\x81\xd6\xdfq%\x1d\x87\x1fB\x0e*\xa4\xb7[\xad\x9dbV\xc7\x05O\\Uh-\xec]\x9f\xed\r*\xc7\xe9\x17;O\xe3\xda\x9e\xd0m\xe2\x13\x95\xc0\xc7\xb8\xa5-\x18\xf5\x0b\x0b\xf3nZ;\x98\xc4\x88\xa3ro\x04`\xd6\xacw\xe2p\x01\x94\x1d\xd8\n\xa4r>\xb5\x82\x90Iw*1\xb8>Pm\xa7\'\xa7\xb5Eur\xb6\xe8\xcb\x1b7\x9c\t\x19#\x05O\xff\x00\\U\xf2\xf5\x0ef\x91\xd28s+y\xf2\x00\xea~\xe8\xe4\x01\xe9I%\xdaH\xdb\x15\xf8^\x06G5\xcd[\\_\x95\xc3+\x95+\x95-\xfe5\xa7`a3n\xb8vW?w=\xbdik\xb9JF\x9bj\x12\xb4I\x01\xf9\x94p\t\x04q\xfc\xa9&\xbe\x99\xa4\xd8\xab\x90\xa3<\xf6\xa9\xe3\x9e\'\x94\x8d\x88\xf1\xaa\x9d\xbb\xb9\xa7$~t\x83dj\xbb\xb8\xf9\xbag\xebS\xcf}\xd1iy\x90\xc7{!l\xe0\x07\xc7>\xf54\xd1\x18\xd2)\x19\x91\x03\x0e\xa1\xb7\x11\xf8U&D\x8afV\x04s\xb79\xebQ\x18.$\xb8\n\x92\xc4\xd1\x927\x07<\x81\xed\xefE\xa2;\xb3Ef\x8d\x1c\x10\\\xba\x9c\xf5\xc0?\x85:HRwYW$1\xc8\x00g\xf0\xab\x1alV\xf1\xdc,\xd3F\xac\xd9\xfb\xa7\x9e*\xe5\xcd\xd4RO\xfb\xa8\x82|\xfc\x05P3\x8f\xa5-\xba\x85\xbc\x8c\xd3\x0e\xd9\x164\x8c\xbbg\x85\x03\'\xe9Z\xd6\xfaj\xea2\xc5os \xb6M\xbb\xb6\xed\xcbq\xdb\xda\xb2\xd2\xf6HY\xe4\x8d\x929\x03|\xa4\x9c\x9ad\xfa\x95\xdc\xb7\x11\xbc\x0f\x99T\xe0\xb9\x18\xc9\xa9\xd5\xbdF\xd6\x9a\x1bz\x84vZL\xa9\x1d\xac\x106c.s\xd4\x9c\xf5&\xb9\xd8\xb5\x07\xbbg\x8d\xd4eH,3\x8e=\xa8\xcc\x8a\xf2\x1b\x92\xcc\xce\xbc\xb3t\x1c\xd5e\x91\x96e\xf2\x1d%P2NkW\x1b\xad\x0c\xd6\x9b\xb2\xcbEj%/\xb5_\x9eF*\x08`H%.\xbf\xbc9\xc8\\\xff\x00Jl\xa4\x1d\xccG\x97!\x07\x81O\x86!5\xbb\xb1l\x94\x1b\xb0:\xd4\xc54\xb5,\xbd\xf6\xa8L\r\xe5\xa8\x8d\xbbq\xdb\xd2\xa0\x13\x17\n\xaaG<s\xde\x96\x0bu\xb9\x8d\x01\x0e\x18\x9cd\x9a\xd1\xb7\xb2H\xa3dT\xce\x0eFG\xf25:"\xac\xcc\xed\xac\x88]\x94\x00;\x8a\x91V0w+\x00\xe7\xf8}\xea\xf8\t\x13\x00\xf1\x95\x1d\xd8\xf4\xff\x00\xf5\xd4!\xa1@\xad"\xee(N\t5|\xda\x0b\x97R8\xae\xe5\xcb3\xaf\x96x\xe9\xc7\xe7R\xbc\x8bt0v\x86=\x81\xaa\xb7\x1e\\\xb1m\x0c\xa3\x1c\x9c7ST\xc2\xb5\xba\x07,\xa3\x9e\x83\xbdL\x92h\x16\x8fSj(\xe1\x89CJ\xaa\xf8\x18\x1d\xf3\xedYr\xed\x90\xbf\x97\tE\xce6\xb1\'\x9fL\x9a\x9e\xda\xf7\xcc`\xb12\x8d\xbc\x00y\xebO3\xb4Q)%~\xf7\xca\x08\xe0\xff\x00\x9c\xd6q\x8bO@j\xfa\x8f\xb4\x11\xd8\xc0\xec\xccY\xdf\xe5\x00\x1cc\xd7\x8a\x86\xe1-\xe5/t!O3\x18\xf2\xd4\x8c\xfd@\xa8ZWg`\x9b\x88c\xc8\xcf\x7f\xa7\xa5\x0f\x0by\\\x06PN\x18\x1e\xb9\xf5\x06\xa9w\x0b\x197Q\xc47\xc8\x8b\x85\x03\xa69\xa8Db[u\xf9\x81Ry`:\x8a\xdb\x9e#,\n\xa2\x1c\x94\xe0\x1cr=\xbd\xc5g\xc1o$N6BJ)\xe4\x0c\xf2>\x9e\x95\xace\xa1\x94\xa0\xd3+\x02\x90LP\x920\x99\x1c\xe4T\x8a\xc0\xcaf\x89\x89\xc8\xed\xdb\xd7\xf1\xa2{y\xda\xe3\xcd\x10\xb1C\x8c\xe3\xa0\xf6\xa9\xe3\xb2x\xa5$\x81\xf3\x0e{q\xf4\xa4\xd6\x9a\x0b\x95\x95\xe3\x05\xba\x81\xb8\x9c\x90\xb8\x1cU\x7f\xb1\x90Z6r\x01;\xb0{\xfdkX[\xed#\x1b\x17wj\x82gs.\xd2\x0b\x00@ P\xa2\xfb\x87/s>\xee\xceR\xc8\xcb\x12\xa2\x81\x90\xa7\x80Nz\xf1M\xbb\x81\xd5RG\x90D\x15Aq\x8e\rj\x1b\x86\x91v\x00r\xbf\xa5C,-q"oc\x8c|\xc3\xd6\xad+\n\xdd\x8bitM\xbb,1\x05@A \x0e\xa7\xebR}\x82\xe5\xe2\x13\x1d\x9bH\xc8\x1b\x86O\xe1T\x91\x96$\x07\'\xaf>\x86\x97\xed\x85rI\xc8\xe9\xb7\xa8\xa1\xdf\xa1w\xeeL\xe5\xa1\xc1\x90\x15\xf6"\x8d\xc5\xe5\xcb\x15\xe3\xa1\x1d\xe9\x89+\xdd2\xab\x0c =3\x9a\xb8\x90G\x80\x00Q\xfe\xd68\xff\x00\xebT9\xa44\x9b!\xf9\xf2~^W\xb84\x83(\xfb\xb3\xd3\x9eEH\xca\xe9#\x02\x07\xb6;\xd4\xd1H\xa7;\xd4\x96\x1d\xc7J\xaek+\xb1Z\xecj\xb3>V_\x96LeO\xa8\xa4gm\x98y\xc3\x00p\x14\xaej\xcbB\xec\xbeo\xca\xbby\x04\x9f\xd2\xa9\xcb\xe78.N\xe6=I\xee*y\xee;\x0cY\x90\x92\xbbq\x8e\x05<\xc6\xc1\x11\x83\xf7\xa8F\xc8\xdb\x0c1\xdc\x03\x9aA4\x92\xee\x8c\x0e=3O\x99\x85\x8dU\xb1\x8d\xa2-\x90Xv\x1d\xbe\xb5\x875\xcb\xad\xebB\x18\x82:\x0f\xf0\xab\x89<\x91\x80\x8c\xf8>\x9e\xb5^{B\xd3\tW\x05\x89\xe4c\xd6\xa2W\xeaL\xbc\x8a\xf7\xb3\xcb\n\xae\xdc1q\x90\x07Z\x8a\xd7\xed\x85\xf7\xe1Y\x1b\xf0+R]\'\x98\xa3sm1\xf4\xe3\x04\x1a\xa0\xd7K$\xdb\x14\x1d\xf8\xe7st\xa27\xb1&\xc2\t\x83\x95W\\\x8e\xa3<\x8a\xb3sx\xd71\xae\xe4 \xc66\x93\xebY\x06|)#\xa9\x1c\x9e\xa4\x8f\xad$\x17a\x95\xd0L]\x8fA\xd7\x8a\xa6\xdb@\xdd\xc7\xf5\x85\xc1,Q\x9f\x04\x02r?\xc2\xab3L\xa4F\x85\xbc\xa1\xd5\xcbgv=\x8dZ3.\xd4\xfd\xda\x86\xcf;\x9b\x04\xfe=*\x07\x8dD\xcd\x96$)\xce\xcc\xf7\xa4\x91-X\xb4\xd2\xaf\x93\xe5p\x13\x19\xd9\xcf_\xadRY$\x90?\x9a\x84*\xf4N\xa0})\xe8\xc2H\x08\xd8\xe53\xc68#\xff\x00\xadN\x967FPK2c \x83\xc94r\xd8\n+\xa9]@\xea\xc6v6\xfd~q\xd7\xdb\xdc\xd5\xc7\xd7n\x0c\x90\xf9`\xa2\xb7,\x18v\xfe\xb5Z\xe6\xd9\x9c!\xf2\x81Rr\x17\x9f\xe7Ud\x84\xce\xb8\x90\x80\xeb\xc1\xdb\xd4}kND\xf7\rQ\xd0A}ku)\x89&t\x97\xae\x19z\xd4\xb2\xc6\t\xdc\xac\xc1\x87\xf1\x0eEaX\xdb\x15\xba7\x0e8\xdb\x85=3[+r\xb1\x08\xe0@\x19\xba\x92\x7f\xad\'\x1b=\r\xa3+\xadK\x90DC\xa2\x92\xf8q\xbb\xaeA\xff\x00\n[\x9bV\x8cv s\xf2\x1e\r\x10]fO\xf5\x99\xc7\'\xda\xac\xca\xe8\xce7\xb1\xd8\xdd\x0e;\xd4\xca\xfdJ\xe5\x8bF5\xcd\xa7\xc8X\x1c\x14\xe9\x9e\x7f:\x8f\xc8\xd8\xb9f\xdax8\xc7j\xd7\x966\xfd\xdceC\'~in,\x1f\xec\xceU[h\\\x85a\x9e\x87\'\x1f\x85\n\xdd\xc8t\xcc]\xfes\xb2(\xda\xa3\xbdJ\xb1\xb9R\x85\x8e1\xc7\xf8T\x85-bq0lL\xa4\x13\x11\x1f)\x1fJ\x8a[\xe7i\x99\x83\x03\xbc\x9c`}\xdf\xca\x9b\x8d\xc83]\xcd\x95\xdbp]\\\xe0\x83\xc2\xd43\\\xb4\xb7\x07|\x1c\xb6\x07\x07&\xb4e\x88\xaa,\xd2F\xe2R3\xb4\x8c\x06\x03\xa5f.\xe3 \x9d\xa3d\xdc\xd8\x07\x07\x15\xa5\x88f\x8d\xc9\x1fe\r\xb9<\x9c\xed\xda\xa7\xe6\x04\x0fOJ\xaf\r\xcf\x96~n\x15\xd7\xa6\xdc\xed\x15OP\x9d\xdbdd\x0c\x8eI\xec*\x01r\x19@\xdaY\xbd\x8dTcd\x0eZ\x9dU\xa5\xd4Q2\xede\xc1\x1bAnM_\xf3<\xb4\xd9\x14\xcf\xb8\x13\xf3 \xc5q\x0b6\xf9\x01\x19FS\xc0>\xb5z\xdfY\xb9\xb7b\x92\x11\xc7\xadK\xa7\xd8\xd25\x17S\xa1&sp\x19\xe4`\x07\\\xd5\xd8D@\x92#R{\xb7\xa5a\rY\x1dq\x92\xa4\xf7\xf7\xad\xc9\xa4\x89\x91\x04\x0e7l\x19\xed\xf3c\x9a\xceQk\xa1\xa2\x92\x15]\xb3\x95#ny>\x86\x9f\x13\xed\x95\x8bK\xc9\x18\x18\xf4\xaa\x904\x8c\x858\xe1\xb0N\xea\xd5\xb4\xd3\xa1\x92!%\xc4\x87\xd0\x14\xc7\x15\r\xe8R\x91J\xe2 \xf0\x06\xe3i=s\xcei\x0c\x07\xca\x1b\xd5@\xe0\xe4u\xab\xafib\x83\n\xec\x10\x8c\x90X\xe4~\x1d\xaa\xbc\x97\xd1\xc6\xc9\x1cC\xf7\x98\xe1\xc1\xedI=l\xc1\xbb\x1a670\xcb\x12\xa4\xc8%\xc0\xe46\t\xc7\xa6:\xd6\x06\xa1\xa7\xe1\xd9\xd2p#\xfe\x05\xda8\xc7j\xbafQ\xf31\xcb\x1c\x96n\xe2\x95#[\x80U$\x18={\xe2\xb4R\xb3\xdc\x9eX\xbd\xcc\xcb(\xfc\xeb=\x8f\x84by\xdc9\x1fJ\xd0\xb4\x8a(\x02\xb7\x1b\x81\xc1\x02\x9d-\xb3,{\x82\xab\x058\xe5\xb9\x14\xe5"\xe1v\xee\x89\x088\xe4`~t\xf7\x05d/\x9eT\x1c\x0f\x97?.GJ\x94j\x0e[%\x86\x07\x1c\x9e\xb4\x9fg\xb7\x8c"\x8b\x84\x98\xbe7\x80\x0f\x15\x7fJ\xb4\xb7\x8e\x0b\x89\xa3o6D8(c\x0cW\xf3\x15>\xefP\xe6fa\xb9.LjI\xf5Q\xde\x9a]$^_ }\xe55z\xf6\xd9\xe51[\xf9\xd6\xb04\x8c|\xb8\xc2\x8c\xe4\xfa\x91\xce*\xabxz\xe9\x00\x88\xec\x9d\xd8a\x84G\x85>\x80\x9e\xa6\x92\x8az\x0f\x99\xadH\x8d\xa1E\xc0\xdb\xb5\xc6G\xcc?Zj\xd9O\'\n\x91\x12\xb9\xc8\x0c1\xc5\x13[Kh\x8a\x8d\x13\xa2\xee\xc6[<\x9f\xad5\xb7q ,\x07\xaeq\x8a\xa5\x0b\x0b\x9e\xe3\xe1\x86\xde\x10^M\x88\x17\xe7=q\xcd%\xc4\x88Q\x1d\n\x9d\xc4\x8c\x9e\x9e\x82\xab+\xb8r\xa2DP\xd8V\x07\x9c\xf3\xd6\x96\xfe\x05\xf3\xd6\xe2\x00\xcf\x00o$`pX\x1c\x96\xff\x00\xf5SPB\xe7f\xa6\x9d\x14\t\tY\x95K\x13\x90wrji\xc4o\xc4\x0b\xb621\xc9\xaa\x10\xdf)\xdf\xb8\r\xbc\x0e\xb8?LT\xa9{\x1e\xe0\xb2\xc5&8!Kt\x1fQ\xd6\xa7\x97\xa9|\xc5\x93a\x98\xc3\x9b\x83\xc7A\xb8g\xf1\xa8\x8cP[\xc0TF\xdegR\xd98&\x9b=\xc4L\xd9@\xca\xe7\xa2\x91\x81\x9fJ\x8eK\x89\xbc\xd6\x8d\xe6A\xfe\xc9#\x18\xa3\x95\x071Z[\x92\x91\x95#\x03\xaeq\xd6\xa8\xed\x9a\\\xbcD\x9cw\xedZ"\xdd\n\x8f\xdf.\xee\x9dj\xa0\xb3\xce\xa2\xb6\xfc\xb2\x11\xf7\xd4\xf7\xf4\xabVDJ\xe4\x11\xab\xe39\xdc@\xdcy\xe0{T\xaf4\xf2\xc6"S\xc0\xec\x17\x93V\xee\xec\xbc\xa7x\xc0\xc2\xaf<\x1e\xb5O\xcba"\xeeV\x03\xbb/QL\x96E\xb2H\x98"\x83\x86=\x08\xff\x00\x1a\x9c\xa2\x11\x93\xf2\xb1\xed\x9e\x05;\xcd\x99\xa5`\x1br\x81\xc0#\xb5F\xab\xf2\x92\xc9\x90G;\xb8\xe6\x81\x10C\x12\xb1Q<\x8a\xa1\xb8\'<\x1fz\xbbkg\x1b\xc3&Q\x0b\x0e\xe78\x1e\xe2\xa9\x1b\xabF\xb9G\xb8d\x11(\xc8@0W?J\x9a{\xa0\x8c\x12<\x96a\x85\x18\xea\x0fz\x84\xdb\x1ac.%[`p\x1c\x8e\x98Q\xde\x927\xc9\x0e\x1c.}EK4\xc2FH\xd3\xe6*\xa3y\x1c\x8c\x8fz\xa4\xf2$l\xd8+\x93\xd0\xf6\xa2\xdd\xc7sv\x16\x84A\x96(Jr{\xe7\xdcP\xd2FF\xe5S\xb5\xba\xe3\xa1\xacH\xb7\xa3\xa9F\x01\x0f\xde\rJ\xd3\xcc\x1c\x82\xdbc\x1d@=G\xa5f\xfb\t\xcc\xd7\xfb^\x14\x10c\x90\x93\xc8\x1d\xa9\xcbr\x85\x1aS\x1a\x86\xef\xce8\xfaV4\x13\xc2\xc8X\x922H\n\x0e\x0f\x15"\\FIRT\x92:\xee\xc1\x1f\x855\x12\xd4\x89\x8bB\xf2n\x8c1c\x9e\r\x08\xb6\xe8\xfef%\xdd\xfe\xc9\xc6j\x1c\xece\xcan\xdf\xd1\x87ZM\xe3q,\xdb{\x03Ni\xdb@F\x92"\xbb\xf0\xed\xb7\x1c\x02\xbd\x05Y]9\x82\xab\xa2\xc9\xb5\xcf \x0c\xe2\xb2\xd6YF6N\xa0c\x93\x8ekJ\xc3T\x92\xd5F%.\xbdHn\x87\xf0\xac\xad%\xa9VL\x8bV\xd3\x04;\x9f\x0f\xe6(\xce\xd7\\\x03\xf8\xd78\xa9\x1eY\xb0\x0ez\xf7\xc1\xae\xf0\xebq]\xc4VP\xa4\xe3\x1bOoO\xadsW6\x06Yw\x0f\x90n\xc0\x08G\x1fJ\xd2\x12\xb6\x8c\xceQ\xecg\xc2\x80\xc4\xaf$k$jN3\xc0?Z,\xedm\x11\xc8\x93\xcbA.[(9\x00z\x13W.-\xaeD"\x1d\x99Bxl\xe3\x9fz\xa8\xd6\xac\x17\'h\x95x\xdaNkX\xb8\x92\xd1_P\xd8\x1bl\x17\n\xb1\xb7\xcd\x8e\x98\xfa\x8a\xafn\x92JF\x1cJ[\x18q\xd4S\xa7\xd3\xa7\xb8\xe1\\m\x07\xa1\xe35r\xd3N\xfb\x1b|\x9b\xcc\x8b\xcf\x19\xc5R\xe5\xe8JZ\x96-\xc9(\x16_\x97i\xf9\xbd\rO\xe5b-\xe0\x023\xc0\xea\x7f\n\xab\xe4\x89\x12I\x1b\xefg<S\xedf\x9dF\x1b\x95\x1cc\x1c\x91J\xc8\xa2WeL\x13\x9c\xf4\x03\x15\x9e\xf0\xc6\xee\xec\xcb\xf3\x1eG=kA\xe4\xf9\xf7\x82Kc8=\xaa92]_h\xc1\xe7\x9ei\xdc\x0c\xf3\x04\xe5\x08\x8c\x921\x92\xbf\xe1Si\xf6\x12\xe5\xa5|\xe07\x07\xbdj\xab \xfau\x1cSdp\xc0\xe0\xb6?:M\xb6;$G\xf6]\xae0\x00\xe7-\x9az\xb0*\xdbN{\xd4\x0b;#\r\xcd\x90z\xe7\xb5L&D\x88\x9e\x9d\xa8\xd4d\xb6\xb7C\x03rn\x19$\xf7\xad\xd8\x0c\x12\xa1\xdc\x06\xec\r\xa1\x07\x04\x7fJ\xe6\x91\xe3\xfe\x1d\xbd9\xdbRG&\xc5!:\xf4\xac\xe5M=KR\xb1~\xe2\xc6\xc1\x17\x8d\xccO\x1d21Q\xa5\x9d\xb2L"H\xd0F\xc0e\xb3\xf7\xaa\xbb\xcc@%Xf\x91o%\xc6\x18\x1e\x0e\x01\xa3\x91\xf7\x13i\x89\xa9XA$\xa2Hd)\x17M\xb9\xdc2=3\xda\xa8\xfd\x8a\x19W\xe6\x1eC \xfe\xef\x7fj\xd4\x86\xed\x98\x85\x95G<\xe3\xfa\xd4\xaf%\xb6\xe6;\xb9~\x0f\xa1\xa1s\xad\x18\xb9Q\xcaO\xa3\xcbq\xb9U\xdd\x98\xb6r\xe3\x1c{U\x88\xb49\xa2\x01\xf6\xa8 `\x8crEtr\x84\x10\xef\\z\x1c\xd3AS\xf3pw\x0csW\xce\xfa\x8b\x91#\x9a}2\xea\xe6\xe4I\x1cM\xd0\x0c\xd5\xb4\xd0\x1cF\xb2\xcc\x98by9\x07\x8f\xa5t(v"\xc8\x84\x83\x9aW`\xf1e\x9c\x96\x074\xbd\xa4\x96\xc3T\xe2aE\xa4\x08\xe7\x17\n8S\x95\x18\xefW[/\x1bnD#\xa1\x18\xe75q\xe5\xceK\x81\x93\xcf\\\xd5vb\xca]\x80U\xc7\x19\xa5)sn>T\x91A-\xe4{\xa0\xa2uQ\x8cy\x84\xe4\x8f\xc3\xbdX\x8a\x0b\x88dT\x8d\xfc\xc0?\x8d3\x9f\xfe\xb5L\x8boq\xe5\x85!dQ\x95\x90\x1as\xdc\xddB\xce\xe3n\xf3\xc3\xb6\xdc6G\x1d\xa9$\xacF\xc5\'\xb8\x17N#\xdd\xb2P\xc4\r\xcd\x82}\x7f\x1fj\x96\xd6\xf2($a$6\xcf2\xe4G6\t\x0e\xbe\x84\x1e3T.R=\xc6v\x8c#\xb1\xdc\xce\x0fzX\xe7\x8eTic\x12g\xdf\xbf\xbf4\xb4[".\xcb\xf0\x89\x19\xa5\x18q\xb9x\x1dq\xed\x9aD\xbfX|\x86\x8f\x19\x8c\xe4c\xa1>\xfe\xf5\x05\xbb8\x8d\xd8H\x038\xf9@\xf6\xaa\xf2J\xe0\xaa\xcbo\xdc\x90c^\xfe\xa4T(\xbb\xb1\xde\xc6\x93jF[\xb3$\xb8!?\xba\x00\xc8\xff\x00\x1a\x9a\xe1\xd2\xe2u\x96\xdeM\xc6C\xca\xe4g\xa7Z\xc7\xfb`I\xa4\x84(\x1b\x94g\'\x95\xff\x00\x1aKy|\x852nLn\xce\x14\xf5\xad#\x16\x95\xc7\xccuzt\xb0\xdbm)0\xf3X\xeden\xa3\xde\x9b\xa9\xbc\xdaa\xdbi\xe7<\xd7\x08\xc4\x9cd\xed<\x11\\\xb4Z\x9d\xd4wO\xb9\xb6\xb1=q\x9d\xa2\xaf6\xb5<\x93\xc4\x92>\xe9!]\x91\xed\x07\xe5\x1e\xa0\x8e\xb5-I\xee\x17E\x8b1,\x9b\n\xe5]\xb0\x88\xd2\x0e\x15\xbd\xbd\xabC\xca\xbc\x84\xba\xf9\xe9\xb8\x1d\xc4\x82H\xc7\xd4\xd4ZU\xc4\xd7\x9es\xcfl\xf9La\x94\xe0\xb7\x1e\x9d\x7f!U-oFn\x9e?\xdd.\xe2\xa8\x8c\xb9\xda}\xf3\xd7\xbdTSLw4\xa4\xf3\x0co\xe7]{\xf9c\x04\x0f\xce\xa8\xcfl \x7f\x95\xbc\xe8\xd8e=G\xb5U\xb8\x9f\xed\x0b\x1c\xa5T\x9d\xb8`\x99\x1d=E3\xed(\xec\xaa\xb9\x04s\xc7j\xd0\x02\xdeIc\x99p7`\xe7\xe8}+\xa0\xd6#k{\x93lb\x91\x92\x14Q\x0b\x03\xd9\x9418\xf5\xc9\xe6\xa8\xd8\x91y\xa9i\xf6\xc4\x87- \\ \x00\x92O5&\xa1y\xe6\xdc]m$HY\x99\x81\x1fw\x9c`\x0f\xc2\x86\xf5\x12]\x0c\x18\xa3x\xe6fg \xb1\xc8_O\xad^7\x10\xca\xae\xd3\x06\xf3\x1b\xa3/\x1c\xfb\xd5;\x99\xdaw\xc8?2\xe3\xefu5\n\xab4\xc4d\x83\xdb=\xe91\x96\xcd\xcb*\xec`pFA\xcf\xf5\xa8d\xba\x0e\xc0\xf06\xf7\'\x9a\x8e\xe8\xaa\x10\x8aI\xc9\xc69\xe9T\xa6\x8c\xad\xcf\x96\x14\x8c.MRAsJ6c\xb3k\x00s\x9c\xf6\xab"v\x8f \xb0f\'=\x7f\x95d\x9b\x8b\xc5\x8cB\xe4\x85 61\xfa\xd5\xcf\xb2\xc9\xf2\xb6\xe08\xceGq\xebI\xae\xa2O\xa1\xa3\xf6\xdd\xf6\xe5H;\x89\xe8z\n\x8c\xbc\xbc*2|\xfc\x80{\x9a\xacC\xed\xdc7g\xdf\xbf\xbd4\x19\x9d\xb0\xcb\x80y\xe9Bi\x8d\xa6Il\xae\xae\xfb\x90\xabu4\xf7o-\n\xedfv\xef\xd8U0\'\xfb@\\1\x06\xac\\o\x86Q\n\xa9-\xd4\xd0\xc1\x1c\xccW0\xab\xca\xec\xa0\x95\x1f&Oz\xda\xb1\xd4"{}\xce\xbb\x1e$\xc4R\xa8\xdc\xcb\xed\x8c\xf2?\x95by6\x86=\xf2J\xb1\xf1\xc6\t9\xa8\xed\x92H\xd8\x98Y\x81n\x87\x19V\x15\\\xa9+\x99\'\xa9\xd4D\x92N\x16R\xac\xc8FC1\xe4\xd3^;k2\xd1\xddF\xc2Q\xf3\x08\xf6\x90y\xf5\xcds\xed;o\xf2\xfc\x9d\x8c\x7f\xe7\x9b`\x1a\xd3\x86tE\xcd\xcd\xbb\xa602\xe3&\xb3\x97{\x16\xb5-\xda<\x8fq\xb4\xa1d# \x11\xc1\xfaT\xb2\xc4\xd1\x8d\xbcl\x7fQ\xfaf\x84\x9a\xd5\xfc\xb1\x14\x9f\xbd\\\x8d\xa7\x82\rY\xfbI\xb8\x8eK{\x9e0~\xff\x00\x19\x07\xd4\xfa\xd6|\xcbr\xb9LK\x97\xfb<\xa5b\xc3I\xea{\n\xa4%0L[~\xe7\xf4<V\xc3\xe9RN\xca\xc8\x16E>\x87\xa1\xaa\xb2i\x13o\xfb\xb8A\xd5\x8a\xf3ZsE-\xc9\xe5\x97Ab\xbf\x03\x87\x97\x9fE\xe9R\xf9\x8b4\\0\xcf\xd6\xaa\xcb\xa2\x18\xc8\xdb+\xb3\x1eO\xcb\xf7i\xf6\xdar\xbb\xe1.I8\xc8%qG<z\x16\xb9\xb6.\xac\x8b\x1a\x02H\xc7@\t\xebR\x16l\xb6\x00U\xc7#5f=2\x12\xa0\xcbw\x13\x93\xf2\x94a\x83\xf5\x04T\xa7M\x8a8\xa4\x92RLj1\x95 \xe6\xa3\x9e\xfb\x15.dg\x8b\x9bQ\x13\x17\x90\xb1\xc7U\xfeUb)\x95\xd0}\x9d\xb0:\xfa\x83YE\x04e\xd6%>[\x9c\xfb\xfd*\xcd\xbc\xecvE\x12\x11\xb7\xf5\xa6\xe2J\x96\xba\x9aA\xdd\xa0*\xcf\xd1\xb2\x01\x14\xd6I\xe6E\x88\xc7\xb8\x13\xd9?\xadKe,QH^\xe6\x10\xcc:\x001Zw\x1a\x95\x9c\xa3-\x08]\xc7\x82\x8d\x8c\x8f\xf1\xac\xdd\xd6\xc8\xab&g\xd9Y#D\xcbp>d?)\xee*\'\x8d\xd4\xb6\xc3\xdf\x8ez\x7f\x8dZ\x92Kfp\x88\xcc\x01\xe9\x9e\xb5T\xb2l$6[=3\xd2\xaa<\xdb\x83\xb1Jm\xaa\xa7\xb1\x1f\xdd\x14\xe8Q\x9d@\r\xc62\x18\xfa\xd2I\xb7z\xb7%\x8d)\xc4O\xbc\xe5I\x1c\xa8<V\xba\x906Th\xe4\n\xec3\xd4\n\x99"b\xa4}\xdc\xf1\xc8\xe4}*&\xdb\x83#\x1c\x8ct4\xe8\xe7I6\xb2?>\xe7\xa5\x1ea\xa1!\xce\xe0\x88I |\xd95#\x95t+\x1b6\x07\xcc\x08=MV(\xf2\xcd\xbd>U\xf5\xa9\xd6\x16)\xcc\xcd\x8e\xe1z\x1a9\xd2\x0b6T\x9e \x10|\xc4\xf4\xcbc\x19=\xea&`~S\xb4\xa8\xe7\x15vq\xf2\x84u;=\x87J\xcf\x9e6`|\xa1\x97\xdd\xc0\xe9T\x98\x8bV\xfbD$\x0c\xfc\xa7#\xd6\xa6F\x0c6\x9e;\xf0zT\x10\xc72\x90\xac0\xa4}qRl\r\x92\n\xaa\xf4\xa4R\x1f$_2\xe4\xf5\xed\xde\x98\xff\x00+\x91\x9e1\xf9\xd3\x1a\x16\xdd\x9d\xe0\xe3\xa7z\xa8Z\xe8\xc8\xc4\x1f\x94q\x81LW..\xe32\x96\xf9}\xe9g\x8c\x96fV\xdb\x8e\xd9\xebQ\x83*\x8f\x99In\xdd\xeadVp\x85\xf3\xb7\x1c\xd0\x03\x14\xca0w\x10\x98\xce3R\xdb\xcd\x139\xdcv{b\xaaH^$!\x88v\xe7\x85=\xbbRy\xfeS\x00\xc8Cp\x0fqI\xa4\t\x9b,\xc8\x13\xe4+\xeb\x91\xc6*\x9d\xc5\xd6\x10\x90pOQ\x9e\xb5$L\xb2\xa3e\x88\\g\x15+\xd9\xdb*\xac\xb2\x927r7\x0e\xd5\r\xad\x8a\xb3d\x0b6\xc2\t\x04\xae)\x15\x84\xc4\xab\x1c\x83\xd4c8\xa9|\xb8\xf0Z06\x0e\x84\xd4rH!M\xe0\x82\xccz\x01N\xc9\x8c\x82ku3(\x88\xec\xc9\xcf\xcb\xd75\xa5f\xfey\x11\\\x068\x19-\xbb\x9e\x07\x02\xa8\xc4\xec\xceZ@\x08\xfat\xa9\xfe\xd4\xa8\x0e\xd1\x90F9\xedS(\xbe\x81dZ\xb9\x85\xe4\x8b\xec6\xbb\xa4\xdf\xcb\xa3\x9e\xde\x8azVl\x91\'\x96 \xb7VR2\x0b\x13\x82\xbe\xc4S\x96^G\x97#\xaey\xc0\xe9V\xfe\xd6$\xf9B\xa6I\xf9\x8e\xdeZ\x93r]\x05da\xdb\xfd\xa3tJ\xd1\x13\xb0\x10H^\x95\xa1q\x13\xc8\x986\xc4+\x0f\x9b\x9e~\xa2\xb4\x9a1:\x89\x10\xa8e\x18\xc6z\x0e\xdc\x7fZ\xad#\xca\xf2,o\x18f\xea\xac\xa7\xf4\xc1\xac\xf9\x9b\x90\x9ct1\xbc\xa1\n\x10\xb1\xa9\x1b\xb0\xcd\x83\xd3\xfci\x07\x96\xce\x05\xbc\xb9\xdc1\x82\x9c\x92=\xfbV\xaa\x94+\xb4\r\xa7\xa7\xb3T\x0fg\xcb3H\x14\x9e\xd9\x1cV\xf1l\x9e]\x0c\xef\xb2\xdd#\xe6]\xb8\x8c\x12\xc4\xf5"\xa1Yf\xb5\xb8\r\x1b\xe0\xf5_l\xd6\xd90@\xe8\x8c\xe5\xdc\xa8\xe1\x93\xa7\xb8\xaci\xdc\xcdxT\xc8\n\x93\x82\x08\xe0\x9f_\xad6\xed\xb8\x8b\xcf\xad\x90L\x92:\xc9,\x7f.\x00\xe38\xeb\x9e\xf4\xed7UU\x04I\x18\x90\x1c\xe7\x9e9\xefX\xcd\xa5\x15\xcf\x97\xb9\xb7\x1e\x83\x81V\xed4y%\x7f\x971\x0cc\x1c\x92MK\x92J\xe0\xaei#\xefW\x11\xb2\xa8$\x8d\xa0\xff\x00Z\xbfkb\xa9\x1a\xa4\xa00^w\x03\x8a\x81l\xe3\x89\x03\x00\xfb\xd3\x96\xce8\xc5\x13\\C\x85\x8c\xef\xdc\xec|\xd2\x01<u\xe2\xa5\xcb\x9bb\xd5\x96\xe3\xc3\x0bi\xb3\x12mu;\xa2lg\x15VK\xa0\xa8\xa9\x18c?F\xc0\xe3\xebP\xcd~\xe0\x00\x8f\xb2>T!\xeb\xf8\x9a\x8e\x17\x99\xcb1P\xc1FM\x1c\xd6\x16\xec\x1ee.d\xdc\xa5\x9b\x01\xc69\xabV1\x89.\x0e\xd2H\'\x19#\x9czUI\x86J\xacI\x1eK|\xccON*\xfd\xbc\xa2\xc5cp\xab&\xef\xe2\r\xc0\xac\xe5&\x96\x83VLs\xd9\xa32\xb3\xc8\xc7\r\xc0\x1cf\xa6\x868|\xd0\xbb\x146\xde2rM,\x96\xf2\\\xa8x\xd3\xe7#\xd7\xadS\xbb\xb6dc\xb8\r\xcc\xbc2\xb6i\xa99+\x16\xd5\xb5/yP!a0 \x15\xc6I\xe8*o\xf4C\x00Delt\xcfOl\xd6e\xaf\x90\xf6\xe69\x1f{\x8e\x0f\xcd\xde\xa7\xb6\x88,L\xa0\x807r{T(\xb6\xec\xd8\xd4\xb4\xb94\x90Z\xb0`VH\xdc\x0c\x80\xa7<\xd4q\x92\xd0\x16\xf2\xc9\x00\xe2\xa3\xdf$\x97\xae\x88I=8\xe9V\x9bdp\x05c\x90H\xdd\x9e\xc7\xda\xb4QP\xdcN\\\xc5I\x98+\x87r\x01S\xcei\xbej\xc9>U\x89\x0c9\xcf\\\xd5\x89\x82:\x16B\xc5\x07S\x8e\x95Wi\x03t(\xc1\xc9\xc8$d\xd6\xaaq#S?P\xd2-\xa4\xba\xf2\xe4\x89\xed\xa4\x07\xee\x91\x80j4\xd1"\xb6\xb9\x1ff\x9f\xcd\r\x8e\x0f\xcb\x83]\x15\xc4\xb7\x17\x96\x11\xdb\\\xf2S\x98\xe6\xee\xa3\xf1\xa4\xb4\xb2\xb7\x86\x15y\x9cI\'wQ\x9e*9\xa5b\xb9U\xc8\xed<= \x91\xe5a\xb87\xde\xcf\xf9\xfdj\xa4\x96+\x14\x8e\x92\xc8\xc5\x83w9\xad\x89/\xa4\x85\xcb\xc7#a\xb8\xc2\xfbVd\xae\xd3\xf9\x92\xeeRW\x9d\x87\xa9\x1e\xd4\xa1\xcd\xd4\x1f/@\xb7K&\x8d\xd0Z\xecs\xcf\x98\x1c\x92\r^\xb4\xd3a\x8aU\xb9-\xb9\x9f\x86\xdc2?\x1a\xa0\x1eM\xa5\xe2V^\xfb\xbaf\xa7K\xd7\x8fj\xb8\xdc\xc4\xf65r\xf2\x12\x92F\xd4r\x0f82\xaca3\x8f\x94q\x8a\xce\xbd\x82\t\xaeJ4\xce\x89\x9c\xfc\xa3"\xa3k\xb2\xe0*&\xd6nx=\x7f\nzjp\xa2\x1f\xb4[\x99\xca\x0ct\xc7\xea9\xac\x1cd\xf65\xbcz\x8d\xfb\x04j\x8c\x03\x06o\xe1$\xe2\xa92\xb4L\xca\x08#\xd8\x00G\xe3[V\x17\x9a,\xd3\t/t\xfb\x87\x8f\x1f\xea\x92\\s\xf55a\xeem\xae#\x96\x18\xf4\xd8\xa1G?\xba\n\xa0\xba\xfdOz#\x07\xca\xee\xc8o\xa29\xdbH\x0c\xb3\x81)_/99\xebW\xe3\xb6\x9a\xe4\xc9\x14\x13B\x80\x02wJ\xdc7\xb0\xc7z\xb7o\xf6\x0bTi\r\xbb\xb3c\xa1\xc9\xcd^K\x8b7BV\x14\x1c\xe7\x04`\x9f\xce\xa2.K[\x1a(\xabZ\xe7/.\x95\xa8\'\xcf\xe4\x02\xbc\xe4\xa9\xcd^\xb3\xd3\'\xb9\xc4h\x99\xe3\x8c\x0c~\xb5\xd2\x0b\xd8\xe4\xb41H\x81\x9bo\x19\xf4\xfa\xd5_\xf8H\x8c\x10\x88aU]\xa7\x82\x9d\t\xfc\xb85rr\x96\xc4\xd9-\xcaW\xfe\x1d\x9a\xc9\x99\x91\xc3\xaf\x05W\x92zv\xaa\x1f\xd8\xf7\x0e\xe1\n4d\x91\x82x\xdb\xf8V\xc8\xf1\x1c\xf29\xdf\xcc\x89\xf7\x0e\x07\x14\xcb\x99^\xfa\x15vr\xb3!\x1c\x96\xeah\\\xcbqY\x18\xd7\x10\xcd\x00h\xf6\x86a\xc3c\xd4U\'2\x0c`\x02\xde\x80t\xfc\xeb\xa0\xb4\xb3\x9e\xe6v\x9a\xe5\x966c\x868\xe6\xaf\xcb\xa7\xe9\xcd\x0b\xc2$o4\xb7\xdf\'\xa7\xa9\xa1Tkq8\xdc\xe2[\xce~d\x81\x93\x07\xef\xe3\xa57\xcb\x9aVU\x91\x06\xdc`\xd7V\xfae\xb2\x84\x8a\x12X\x13\xc1\'\xb7z\xcf\xd4,I\x9c\x08\t@8<U\xdd\x8a\xc8\xa0\xb1[#\xa4d\xb1\xc0\xeaz\xd5;\x8f\x95\xd8&\x14\x0e\x835\xb9}\xa49\x91^9\x14\xb1L\xf4\xac\xcb\x8f\x0f\xdc<\x1b\xd9\xf2:\x90\t\x14\xe0\xfb\xb1I\x11\xdb\xf9\xb2"\xb8eA\xee\x7f\xc6\xae\x04,\x85VB\xc1z\xed\xfe\xb5Y4\xe6\x8e\x12\t%@\xe0\x8aH\xa09Gs\x8c\x03\xcb\x023U\xa3w\'TK;\xe1\x02\xb1\xc9=\x08\xa8Q\x1c9w\xec8\xfa\xd4\xc5RS\x86|2\x9e\x00\xa5`P\x91$e\x97\x1c|\xddj\x94\x81\x95%i\xba\x92A\x03\xd7\x8aX\xa4}\xbbZ,\x9a\x96m\xd1\xc4\xa1\xd1\x97<\xfdj<\xed\xfd\xd9S\xb9\xb9\x1e\xb5[\x80\xf6rA*3\x81\x9c\x8a\xac\xd2\xb9\x90\x80\x00\xc0\xcdY\xf2\xe5Q\xb5\x08=\xcf\x18\xa3\xca\x90\x80Dd\x12z\xfa\xd2\xbd\x87k\x8d\x85\xe6d\x19C\xb4q\x93\xd35\'\x98>ds\x83\xd3\x8e\xde\xb5,1m\xc1<\r\xd9*zS\xda\x18\x01\xdf\xb8\x96<R\xe6\x1a\x8b)\xdb\x84g<\xf0OSS\x01\x1b67\x1c\x0f\xbd\x81\xd6\xa61+\x0c\x85R\xbd\xf9\xebQ\xc9\xe5F\xc0\x83\x83\xdc\x0e\xf4\xee\t\x08\x17nBg\x19\xe0\xd3\x1c\xcd \xe5\x99P\xfbf\xa5[\x88\x9d\x80\xc0\xdc:\x1fj\xb4\x19v\x10\xb8\xf9y\xe4T6\xca\xb1V8J\xc6\xa0\xb1\xc3t"\x89m\x99\xbeb\xe0\x0ct\xcdN\xd2\xe4\x05\xe1Xt\xc7Jr0*~u\xf9\xb8!\xbbT\xf3\xdb@\xe5Eo,\xc6\x872n\x04g\xa5W\x89\x1d\xd9\xbe\x7f\x94\xf7\xc6?\n\xd06\xb2\x87d\x91\x08]\xbf+\xf1\xcf\xf8\xd1\xf6?(\x15\xf3\x06\x07lc4\xddK\x0b\x95\xb2\xbcq\x0e\x03d\x11\xce*I\t\xdb\x80\x17\x00\xf4\x1dj\xea\xa4\x92F\x00U\xc1\xe8x\xaa\xd2\xdb\x89\x01p\xc4?p{\xd0\xa6\x9e\xe3\xe4hlrI\x16\x14\x1c\x8fjU\x93{36C\x81\x91\xb7\xb9\xa7,\x0f\xe5\x11\x17\xceO$\x8e\xd5\x17\xd9n\xe2a\x95 \x1e\xe2\x9d\xd3b\xd4h\'xg\x00\'PG\x14\xd9\xa4A"\x02\x03z\xd3\x16\xe5\xc4\xcf\x1e\xe2\xd2.r1\xda\x9e`\x90\xc3\xe6m*\xa4\xe7\xb6*\x96\x84\xeeP\x9e\xd57\x12d 7Q\x9c\x11Wb\xd9\x95\x00nE\\\x0e\x06\xe1P\xdc[\xcd7\nwm=3\xc9\xa4\x16\x8e\xac\xbc\xb7^1CW\rQ\xae\x90\xed\xb3;N\xed\xe4uQ\x91\xcdE;\xb4\x0f\xe6\xec?7\x03\xb8Z|/%\xac`oY\x01\x18\xdc\xc3 ~\x14\tVW+\xb4\xe5N3\xdb\xda\xb3\xe5\xd7R\x99Q\x1d\xc5\xd6\xe7\xe4m\xe48\xebT\xed\xe0\x93\xedb0\\\x0eH\xda3[\x86\xca\xe3\xc9Ie\x84\xec\xec\xe4U\xdb5\x8e0\t\xc6G<\xe3 \xfbT\xb9$\x86\xa3s\x19\xb4\xad\xe3&\x02\x00a\xb8\xe7\xb5Vm;\x17\x84[\xa3\x92H\x1cd\x82?\x1a\xeb\x9aU"B\xbf>Sv*85(\x96t\xdf\nu\x1c\x81\xd3\x1e\xb5\x1c\xf6\x1b\x8a1\xa4\xd0\x99\xe4`\xb1I\xe5\x01\xbbi\x1c\x83UN\x8e\x91\xdc\xa3F\xb1\x97\xce6\x1f__j\xef\x9fW\x86\xddL\x853\xb8\x0c\xed\x19\xcfl\xd6a\x8a\x0b\x9b\x9d\xf6\xe8eW\x00\xe7\xa7?JN\xa5\xd0\xb9Q\x9a\xb6\xf71\xdb\xec<\xccG\xcc@\x18\xc7\xf8\xd58\xf4{\xb9\xd1\xad\xces#\x96L\x1cm\xff\x00\xf5\xd7Sy\x04\xb6\xf2\xa1E\x0c\xa4gk\x11\x95\xfa\xd5\xb8#\x81\xd1%\xb8`\xb2c\x82\x0fJ\x95VIh>T\xf7<\xeeO\x0e^i\xf7\x83qV\x18\xc3q\xc0>\xa6\xa7H<\xbcD\xc5X\x1f\x98\x9c\xf0k\xa9\xd4\xaeVY\xf0\x93\xc4\x10\xafq\xd7\xebYF;}\xc8P\xac\xaeA\xe4\xaf\x03\xda\xa9U\x93\x0eDd\x19@\x9c\xb0a\xdb\x18\x1f\xd6\xa5A\xbc\xb3\x0c\x93\x9c\xe4\xf4\xadO\xb0\xac\xb2\x06\x02$\xc0\x03\x8e\xa0\xd6e\xe1\x92\xda\xe5\xa2\xeaA\xe4\x83\xc1\xa6\xbd\xe5p\xb2L\xbc\xb6\x92`&\xd1\xb9\xba\x91\xe9\xe9Q\\G\x1cD`v\xce=)\xf1\xc9$\x91`#\xae\x0e\x01c\xc9\xaa\xb2\xc8\xd2<\x9bK0^8\xedJ\x9cZ\xd4r\x95\x91V\xdaTIDS\xaa\x86\x1f2o$\x83\xee\x08\xa9o5\x08\xc0&f\xda\xec{.\x06}*\x92D\xd2\xcc\xdb\xc8N\xe0\xfb\xd6\x83Y\x89\xa2r\xecCm\xf9X\xf3\x9c{v\xae\x97c&\x99\x9c\xf1Ip\xe4\xc51\xdb\xdb\x9e\x95\x03Iw\x14;\xa5p0v\x82\x14\xee\xfa\xd6\x8a[G4\xd1\x85e,W\x81\xfc\xfe\x86\xa6:d\x9co\x90.\xd1\x8c\xb3d\xe3\xda\x8eTM\x99\x85\x89\xee\t\x93\xcc\x91\x18`\x02\xa3\xe5a\xf4\xab\xf1\xc7\x1a/2\xb3\xfb\xf4\x15#[\xc6\x13\xcaK\x84\x01\x8f,3\xc8\xfaQuo\x1c\x16%\xd2E\x91\x0e0\x01\xe7\xf2\xa1\xf2\xec\x8a\x8cl\xae:\x16\xb3gM\xcc\xc4!\xce\xd1\xd7\xf3\xab\xef:\x95a\tu\x89\xba\x87\x19#\xf1\xae}\xa2h\xd1%\xf3\x00\xc0\xce\xdf\xe2\xfc\xaaS{5\xb4\x98b6\x85\xcb\x109\x14r\xf6e).\xa6\xd4\x97V\xb0*y\x11\xf3\xb7\x0c\xf8\xe5\x8f|\x8c\xd4)tw\x9d\xac\xc0\xfdk\x16+\xe1r\xef\xb0nQ\xd4\x9f\xf3\xc5"^\x19d\xf2\x91Il\xe0m\xe4\x93U\xc9\xdc\\\xfd\x8d\x87\x91\xdd\xfeeg q\xb4\xe6\x94\xcc\xe1Uv\x80}\xab<\xcb*?\x97p\xb2\xa6\x07*AR=\xf0j\xc4{\xca\x96W;}j\\JS}\x0b\t123\x10\xcaW\xf8Oj\x91w6P\xfc\xa1\x8fq\xc5Uk\xbb\x99#\x91\x03\x07\xf4f\x19#\x1e\xf5%\xbb\xb9\x83d\xeaw\x1ew\x8e\xc2\x8ftW\x97cF\xd6 \xc1Q\xc0\xcf#;z\xd2O*\xdb\xc5\xe5\x92Cg<\xd1m$\x87pQ\xb0\x05\xda7\x7f\x17\xfbU\x9d$3\xdd\xcc\x08#x?1#\x8a\x99$4\xd9e\xb5i\x15\xc6X\x828\xe7\xbf\xb5!\xbe\x93z\x82vg\x9d\xb9\xac\xf9\xe1\x9b\xcd1\xe16\x81\xc9\xdd\xc6}\xfd).m\xdc\xc7\x1bC/\x1bv\xf4\xc85\x1e\xed\xca\xe6v\xd8\xd1\x96S\x93\xb5\xc8\x03\xee\x8c\xe2\xa6\x17Nc\x06A\xf3\xa8\xfb\xd8\xeb\xf5\xacX\xfe\xd1\xc4h\xc0\xa9\xe5\x837\x02\xac\x1f3\x00G\xc8\xc7~\x86\xafE\xb3&\xf7\xe8h\x9b\xa9Q:\x06f\x1c\x02je\xbd\x17\x0b\x1e\xf5;\x87\x04v\xaa\x06\x1b\x99Sj\xc6K\x01\x85dlb\x9fkn\xd6wJ\xd7;\xd3=wG\x91Sx\xb5\xb8\xec\xeeh\xb9\x89\x86\x02lC\xf7C\x0c\xd5\x19\x92\xddfQ\xd5\x00\xcewq[r<\x13@\xa9\x1cY\xdb\xd4\x01\xfa\xd3\x13E\xb5\x9e@\x10\xf9C\x1dsY\xa9\x0e\xc73=\xbe.N\xc2\x00\xcf\xf0\x1aiIf\x04"M#(%\x8a\x8e\x98>\xb5\xd4^\xe8\xca\xa5Wp+\x18\xe5\xd4r~\xb5\x98\xd6R+\xf9\x91F\xe1\n\x9e\x80\xfe9\xab\xe7\x0eT\xcc\xe3\x0eJ;3\xfb\x8e\xc6\xa2\x94"H\x0e\xd6\xdd\xfc \xf3Z0\xa3\xb1\xc9@Q{g\x93M\x91\x03\xb6\xf9\x15x8\x03\xa1\xc5/k\xae\xa3\xf6K\xa1\x9d\x04\x82L\xe5w1\xe7\'\x8aq\xb8\n\xdb\x08\xe4\xf6\xf4\xab\xd1\x88\x95\x1dBd\x03\x90zd\xd5Cnw\xb1\x10\x90\xcd\xd0\x13\xc9\xaaU\x13%\xc5\xa7\xa1]\x9d\x99Y\xf9\x01}\xfa\xd2\xac\x80\xec$\xfd\xe28\xee*\xd46\x12%\xb1i\x90\xaex\x034\xc4\x8c\x8b\xa1\x18\x8f\x1bGq\xfa\xd5)\xa2\\X\xd9\xa7P|\xb8\xf0}\xb3Q<\x13\xeeR\xdbH\xfa\xf2*\xd4\x9ah\x8aM\xfbA\xdd\xc8\xe3\xf9\xd0\xc99\x1cG\xd4\xf2A\xce=\xa8\xe7@\xe2\xc8R\xdd\xbc\xa0\xe8\xc4\x00y^\xbc\xfe5"\x93(T\x8c\x02\xfd\xf0qW\x05\xab\x08O\xcc6\x9e\n\xf5\x00\xd1g4V\x93\x8d\xd1\x86\x03\x90@\xe8}\xe9{[\xde\xc1\xcag4\xad\x14\x85X\xe1\xf3\xc8n1\xedM\x8a\xee9\x19\x81\xdd\xbb\x18\x04v\xad\xbdM\xedn\xbfy$J\x17n|\xcc\xe0\xe7\xe9T\xad \xb4l\x8d\xacH\xed\xc75\x9c\xa4\xedv\x8a\x8c|\xc94\xfb\x88\xf6\x1f\x94\xbbg\xb8\xc1\x15\xa7s\x1cSC\x13\xab*\x9c\x90I\xebP\x95\x8ffa\x8c\x87\xcf\x03\xb6*\xbc\x90]\xab\x00\x146\xe1\xd0\xf3X\xde\xee\xe6\x9d,>T!\n*\xee\xc0\xc8\xdbT\xdd\xe5@P\xc4Y\x93\x92A\xeb[V\xf0M\xe5C$\x81c8\xc1\nq\x8a\xb1m\x05\xb924\xa8\xae\xc9\x9c\x80)\xa7\xd0Ob\x1d.\x12T\x00\x14\x96\x1d1\x83Q]\xb4\xd1\xc8]\x86>l\x15\xc6qW#\xd6-bv\x91\ne>\xef\x1d\rQ\x9bP{\xd9\x0b\xc4\x8c\xea{\xed\xc6\x7f\nj=X\x94\xb5\xb1\x1czq\x9e\xf0I\xe5\x84f\x1f{\xb6*v\xb7\x11\x8d\x91\xb8u?x>1Q\xa5\xc1\xb9\x068\xb2\n\x0es\xd8\xd3\x02\x10\xff\x00<\xc0\x9e\xd9\xe9G3\xea\xca\xb1\x1c\x96L\xf2\x0c2dd\x80\x06x\xa8gGU\x8fj\x01\xea\xd8\xcdZ\x92\x19\xed\xe5V\x00\x85\x1c\xe2\xa5x\x9d\xc8`\x18F\xfcr\xc0\xd5\xa9[[\x89\xa4f\x9b&\x9e7ep\xa5H\x1c\xf45\x02\xc1$\x13\xe4\x90@9!GZ\xb3p\xf2@\xc0\x15 \xafC\x81\xcd6\tL\xd7\x9c\xa6w\x7f\x16qW\xce\xc5di\\kR\x18D`\x11\x1e\xcd\xbb;UH\xaf|\xa8\x88\x0b\xbbx\xea\xdd+B\xfe\xd6\x03b\xa5A2\x01\x92\xc9\xd8\xd5\x18\xe03#\x19q\xecP\x01\xcf\xd2\xa2\xf1h\x12w\x11\xef\x15_\xf7xY\x14\xf6\xe9\xcf\xf3\xa9\x12\xf9\x0cA\x0c1\xb7\xcd\x9eA\xff\x00\x1a\xadq\xa6\x9f-\x8a\xb0/\x91\x8e\xdf\xa5Y\x8a\xcd\x92\x1d\xd8\xc9\xec\x00\xe9\xf8Rv\x19r\xdc\x1b\xaf\x9e\x07\n\xe3\x82\xb9\xe4S\xe4\x92h\xdd[z\x83\x18\'\x1d?\xfdu\x05\x9d\xcbZ)\xdd\x85,\xdd[\xadE-\xc9\xb9y%\x99T$`\xe0\x8eKTK}\x86\x85\x9fY\x91\xa4\xdc@B8\xcfQ\x8a{\xea\xac\xb1\xb2\x98\xf7\xc7\x81\xb4\xe7\x18\xacm\xaa\xea\t2\x00\xe4\x91\x959\xf6\xab\xb6jH\r\xb0\xa0<a\x863\xc5i.D\x89I\xb6\x12\xff\x00\xa4\xc8\xac\x84\xc4\xacyPj L\x0c\x00;\x1b?{\xaf\xe3W\x12\xe5\x92\xe1#\xb7\xb7IH\xe0\x1c\x8c\xd6\x85\xc5\xe4~XY\xed\x94\x8e\xc4\x01\xc9\xfa\xd4\xf3%\xa0X\xe7\xa4\xbbvr\xc5\xf0\xcc~\xf0\xef\xeei\xb7\x0ce\x91\x0b\x1c\xberJ\x8c\xe4\xd6\xa4V\x16\xf7\xd3\xb3\xa0m\x9d\x94\xf6\xaaw\x90\x1bs\xb1O|\x1cv\x1e\xb5\xa4&\xafb\\YXoiC\xbc\xa3\x83\xcf5z\x05\xb6\r$\x81\xc4m\x8c)+\xd4\xd5;i\x1e\xcex\xdd\xe2\x12\x00\x08#\x19\xfcj\xcd\xc02J\xdb\xe5\x12mQ\x85\x0b\xc6=j\xfd\xd0\xf5\x1e\xbao\xcc\xca\xa3\x18\xecx\xcd#\xd8K\x00M\xcd\xb4\xf4\xe7\x90?*\x88K+F&\xc9\xc18\xc1\xedN\x8a\xe0\xca\xbfg\xf3\x02\x92N\x1b=*\x1d\xd06D\xd3\x88\xa6\x117\x12\x03\xc1\xdb\xdb\xebQ}\xa0<\x9b\x1b\x03\'\x05\x86y\x1e\xf4Gd\xd1L%\xf3C\xba\x9eH9\xcdk\xc4\xa6X\xf7%\xb2\xb0\x03=1\x9avB\xbb{\x99rZA\x13\x91\x1c\x91\xbcm\xd5\x9b?)\xf4\xaa\xb3\xda\xc6\xb3\xef\x922\x17\x00\x02:\x1f\xce\xa5\xbc2\xc33(T\x0c\xc7%J\xe3\x1f\x95W\xfbIR#\x95\xb0\xa4t#\x8c\xd6\xaa(\x86\xc5\x91\x117yhv\x11\xf7\x8e9\xa1\xd6\xcfb\xca\xc9\x96\xdc\x03\x9c\xf1\xb7\xe9S>\x9fqs\x190\xc2\xe9\x08\x18\x0e\xf9Q\xef\xc9\xe0\xd5x\xd5-\xe5\xcc\x81\xddW\xd0d~5v\xb5\xae.c2h%g1E"G\x13\x1c\xe1\x07\x18\xf7"\xb6\xfc9{i\xa3\xc3p\xc8\x85\xee\x9c\xfc\xaeb\xc8\x8f\xe9\xcf5(\x0br\xca<\xa8\xa0\xdes\xbd\x93\x06\x95\xa2\xb5\x92p\x96\xf0\xc7\x18\x03\x19r@\xdd\xdc\xf0(\x93V\xb0$S\xb8{\xbdV\xf5\xe6?1\xc8\x07\x8a\xd1\x8a\xc1\x8aI\x1f\x9a\x03c;A\xe7\x8fj\xd0\xb7\x96\xe8\xed\x8b\xcb\xb4\xf9W#\xca\xc3n\x03\xb7\x14\xd9\'\xb9}\xd2\xdcYM\x1a\x16\xc8dLc\xf1\x1d+\t9=\x8d\x13I\x19S(\xb7\xdb\xe6\x1f\xbc\x03d\x8ej\x05\x99d\x93g!s\xf4\xe2\xa5\xbche\xb92$\x98\x04\xe3\xc99\xcf\xe1N\x82\xc5\x12\x05\x90\xb4\x99n[\x1d*b\xd2\xf8\x81\xc9\xbd\x11Z[\xad\xac\xb1\xa6L{\xbe\x99\xa8\xd6\xe9\xd9\x88p\xe1\t\xe3#\x91Z\x0b\xa5\xc6\xa4\xacNI\'\x8d\xc3\x8c\xff\x00J\x9a]\x15\x9a,\x92\xc8\xc3\xef2\x8c\x8anQ{\x02L\xc4f;\xd8\x0c\xa8q\xefW\x96\xd6\xd1\xa0\x81>\xd8w\x10~R\x9cS\xae\xbc9t\xc69Q\x0c\xaa\xbdv\x1e~\xb8\xa9n4;\xdcE/\xd9\x9d\x88N\xfc\x7f*\x994\t>\xa3\xa2\xb6u\x80\xab\x14q\xd4\x0cSP\xc2\xd1(\x8c\xae\xf49\xc0\xf4\xa5\xb0i\xd2a\xe6C"\x80\n\x1c\x8e*\xf5\x94V\xec\xd2\x06L\x03\xd0z\x9a\xc6M\xad\xcdV\xa8X\xda8\xd0\xb8\xce1\xf3n^3\xec*\x13$\xf3\xc8\xb8\x8c\x88\xc0\xc6\xe1\x9e\x7f\x0e\xd4\xb7/$n\xbb$*\x17\xa7\x18\xc5J&\xdbo\x88\xa5r\xe7\xef1\xe0\x0fj\x9dW@\xb92[\xa4S\xa4\xae\x06\xe2{\x9e\x82\xb4\x9a\xfe\xd5 \xda\xc2>\x1b;\x889\x1f\x95sR\\\xca\xd3\x88\xa4\x91_\x039c\xd2\xab\xdeJ\xd32\xaa\xce1\xd3\xd3\x8a\xb8\xb7\x17re\xa9\xd2A\x7fn\xb2\xb0,\xf2!#\n[\x8f\xff\x00Uj4\xafom"\xf9j\xe1\x86UU\x81%}\xab\x8c\xb2\x92\x03.\x18r\xbc\x0e:\x8a\x98_J\'\x93jgh\x18\x15-\xa6\xc5sJ\xcaKx\xdaH\x9a1\x96 \x95#\xe6\x06\xa2k@\xd7\x01\xa5\\#\x8f\x95@\xc8\xcd2\xde[\x89\xa4.Ld7e\xc1\'\xebR<\xb3\xb5\xd4\x1b$\x18L\x06\\v\xaa\xbe\xa3\xd8\x92M\x1f&FG\xce\xc5\xc8\x0b\xfdk&\xc6)|\xf9\x04]rHf<WW\x132\xfe\xfc\x0c\xcaF\xd7\'\x81\x8e\xd5\x99x\xd7\x10/\xef\x92\xd2\x15n\x81$\xcf>\xf8\xe9I7f\x82\xf73\xee\xbc\xd8B\xf9\x87pq\x92\x15\x7f\xa5-\x80\x8a\xdc5\xc4\x91\x11\xb8m\xdb\x8c\xfef\xb4ZG\xbe\xb62m@\xd1\x80\x83h\xfb\xd5\x14\x91I\xe4\x12\xc9\x16\xe0r\x02\x93\xc7\xf8\xd0\x9fB\x90\x1b#u\x12\xba\xc9\x1e6\xee\xc9<UkKG\x92"~Y0N0qV`\x8d\xe3\x8c\xa9\xb6S\x0c\x8b\x92\xcaz\x1a\xa2\xf1\xbc"6\x8d\x99W\x90}~\x94Xi\xb2h\xe3\x90\x92\xca\xa7h\xe4\xa9\xf5\xefT~\xce\xf7\x1c+0$\xf1\x83\xd2\xaf,\xd3\xec(\xeb\xb5\xb2\tbp\x08\xf6\xa9c\x99`\x0b>\xd5g\x8d\xfeUS\xc1\x14j\x80\xa1z\xd0\x88\xc4\x18\xc6\x06\x18`\xf2j\xad\x92I<\xe9o\xb1\x82\x92\n\xb6\xd3\xfa\xd6\xb5\xf4\xc8e\xf9\x91W\xcc\xf9\x80#\x9faPOst\x92,Y\x8e\x11\x1a\x02w|\xa7\x1f\x8d\\ok\x10\xed\xb9\xa3<2\xd8F\xa3\xcc\x8e@\xc3\x85^\xa3\xf1\xa94\xdbW\xbb\xbc3H\x1c/\x07\xe5n\x05f\x9dJ\x19T\x07Uw\x03\xfdfq\xfc\xab\xa2\xd3\xef\x16=7b\xc4\xd9\x90m\xc9\x15\x12Ih\x17{\x93^\xda\xd8E\x1bC\xb8\xb1>\xa7,?*\xcc\x92\xf76F\x08\xed\xd9\x00\xc8\r\x8c\x1c{\xd6\xa46v\xeb\x19\x97l\x8d(\xea\x08\xeb\xf4\xaa\xc8\xe9\'\xee\x0c*s\xf7\x8e\xeeEK\xbd\xc6\x95\xd1\xcf\xc5\xa2Cp\x85\x8bL\t\xe0\xb0\x19\xc5k-\xba\xd9X\xacqB\xc1\xca\xe3p\x1e\x95ub\x86\xc6&\xd8\xc4\xf23\x9c\x1c~5\x9f}z\xd2\x8f\xdc\x9d\xa5O\xd2\x9b\x95\xc6\xa1a\x93\xd9:\xdb\xf9\xa5\x95\x0bd\x81\xd35Z\xd9\x9e}\xd0G\x03H\xe9\xf3|\xa6\x8f\x9aE\x0b$\x84\xfa\xe4\xf0j\xdc\x0c\xc7\xf7v\xc8C(\xfb\xd4\xae;X\xb4\x96\x86\xefN\xf2\xe7\x0c\x8e\x846\x07$\x9f\xc2\xb3$\x8e\xda\xd0\xb02:`\xe4\xa6\xea\xb8\x97R\xa5\xd7\x96\xf2m\xc0\xc9 \xf2qE\xec\xdaKK\x0c\xf7\x0e\xac\xf1\xf0>S\xc8\xf7\x15H\x96\x8c{\xd14\x86"\xab\x94\x97\x90\xcd\xc1\xc7j\xd1\xb3+m\x19\x81\x906\xdf\x9c\xee\xe4~\x15\xa1\xbe\xdbS\xb7H\xac\xda& \x8c9##\xda\xa7\x8bO\x82\xd8\x95\x95\x80l`\xf3\xdb\xbf\xd6\x89\\h\xa2<\xa9P.\x1c\x05laXd\xe6\xb3\xe51[\xcc\xea\x10\x8d\xc7\xe5\x04\x9f\xca\xb7\xc4\x91\xde.\xdb}\xa0\x81\xb4\x1cV|\xbal\xf1\xde\x93\xe6\xc6K\xae\x01\xf54\x83S6xf\x9c\x88\xa3B7`\x93\xc7O\xc6\xa5\x96\x1f\'\x11$\xcf\x94\x19\x04\x9c\x10=sR\xcd\x1c\xe5\xb6\xbb\xed\x91A\x19\x1e\xbe\x95\xa9ik\x03)\xccjr\xbf0c\x9c\x9a.=Nt4\xf2m\x8d\xe4YP7\xf1v\xab\xf6\xf1\xa3\xc0\xc9)h\xe3\xce\xdc\x03\xc1\xad\t\'\xd3\x1d\x04q\xda\x82\xf9\xc1\xc8\xa9\x8c6\x8f\n\xaa\x15@H\x04H8\xa2RlV3V%\\F\x9b\x97\x8e\xbe\xd4\xe5\xd3\\\xee+#\xab\xfb\xf3\x9f\xc2\xba\x1f\xb1\xdb\x9bL\t\x17#\xa6\x00$\xd3%6\xf6\x90\x00\xc3.GQQ\xab*\xe6$\x1ak\xc6\xbf\xba`\t\xeaO5\xa1s\xa6[]\xda\xdb\x8d\xc4H\x08\xcbc\x83\xefP\xad\xc3\x0b\xa3\x14l\x1622\xc1FI\xfc\xea\xc4\xd2\xca\xa0\xaa1D\x1f1\xcf9\xa4\x9fqj\xc9\xdbG\xb4\xb6\x87\x1eim\xc79^+&\xf2;8\xd8\x86/+c\xe6\xe38\xf4\xa9.\xaf\x97\xca*\xa4\x85\xf7?\xad`\\\xea\x9eT\xa4F\x0c\x9b\x86\x14\x91\xde\xb4\x849\xb4@\xdd\x82\xe2H\xa1r\xea\x03.s\x801L\x9bR\x8ay?r\x9b\x0e0@\xa8Z/\xb5\x8d\xef2G8911\x03\xf2\xa8\x9e8\xd7s\x10\xddF\x1d\x07\r\x8f\xafz\xeb\x8d4d\xe6g\xc5p\xea7\xf2\xaa\xdd\xb7\x02[\xf0\xab\x12Z:@\x97q\xbal\x93\'\x93\xd3\xda\xae\xcb\xe1\xbd\xd0\x143\xecu9_\xa5@t\xeb\xf3\x0f\x91\xe5\x97U<\x9c`}jy\xd3\xd9\x8b\x95\x94R\xf9\xa3%F\x1b=W5\xadk\xa9<\x08\x85\x0e\\\x0f\xf5~\xa2\xa9\xc7\xa4~\xf9\x98\xa66\x90\n\xfbW_\xa7h\x16\xb1\xe9\xc2\xec\xa6v\x0eNjjI_A\xab\xad\xccl6\xa7r\xb2<_x|\xbbN1\xf5\xe2\xb35\x07\xd3\xac\'X\xa7\xfbA\x94\xf3\xb5\x90`~#\xadusF%\x81\xe4\x82\x19\x0b&vl\xe3\x9fS\\\xf5\xdd\xa5\xd5\xc5\xc0\x86Gb\xdf\xc6\x92 #>\xd5T\xe6\xbe\xd6\xc2i\xf4*\xb5\xd4\x13[yh\xa5\xfc\xc6\x07\x0c\xbcT\xb2\xc4\xeb\x11\x11[\xc8\x8a\x06\te\'\xf5\xab\x11h\x17*\xa0\x88VE-\xc0\x87\xe6\xc7\xe1\x9c\x8a\x87P\xb4\x8e\x06Q(#\xe5\xfb\xacpO\xd2\x93ka\xf2\xdc\xa5q\x05\xf5\xad\xb4q\xc7\xb9\xfc\xd1\xbc\xaer1\xed\x9e\x95\xbde`\xe7Gw\x98"O\x8eGq\xfe5J\x0b\xdbh\xad][\xedQI\x18\xc2:89\xfc\x08\xcf\xebY\xed\xad\xcd\x0cm\x9066To\xe3?\x9d_,\x9a\xd1\xdd\x11\xa4H\xf5\x0b\xbd\x8e\x98W@A\xf5\x19\xc5X\xd0Z\xed\xd4\xbd\xd4\xf3G\t!\x94$\x9f\xd3<UKk\xc4\xbe\x8bm\xc4p\xbb\xc5\xfc\x05\xb6\x96\xfaz\xd6\x9cv\xed\x04_<j\xaa\xbc\xaa\xb0\xf9\xbf1JO\x95X"\x9c\x9d\xcdk\xedV\xe6\xde\xd9\xa3f{\x98&\x00+:+\x94>\xccV\xb2-\xd9\x83lI\xa6l\x1c\x92Sn3\xedS\xee\x85\x97\xcb\x92)\x80R6\r\xc7\x83\xd8\x9e?\x95k[\xc9\x18}\xcdp\xaaB\xe1\x98\xa6\xf1\xf45\x94\xa6\xda\xb3\xd4\xd5S\xb6\xa5\x0bK\xad\xd2\x9c\xc6\x1c\x83\xc9=\xea\xc3jJ\xf2\xacWHc\x8f9\x1eN\x03~#\xbdX\x8a\xea\x10v\x08"\xb9 \xf5\xc68\xf6\xf4\xaa\xd7\r\x12\xce\x1d\xe3da\xf7YNG\xd3\xb7\x15\t\xf4/\x94\xd2\x81\xa3\x12\x03\x16\xe5\x04dy\xbd\xeaD\x9e\xea9\x82<\x85\xa3b@n\xe3\xe9Y \xcft\xd1\xb3o@\xa42\x9d\xdd\xaa\xd3E\xbd\xfc\xc8\xd5\x81S\x80r\tZV\xd4VdZ\x8d\xab\x99#Q!\x18\xc9\x04\x0eO\xe3L\xd3\xa7X&o2\x1d\xce\x9fu\x8a\xe7\xf5\xa9M\xc0\x12(\x92wn0\xd9\\`\xd5\x1b\xadE\xad\x95\x9c\x00\xdf\xdd\x05p\t\xabII\xdb\xa8l\x89\xef\xac\x96\xfbb!\x05\xf3\x9c\x13\xd6\xab\x1d\x0fP\x0b"\x92\xbbBe@\xff\x00\xeb\xd5\x0b\xadbY\xd0J\xb1\xa8e\x1ft\x0cb\x8bO\x10\xc9q\x0f\xcbvWh\xc6\xdc\xf3\x9fz\xbeI\xa5\xb1>\xe8C\xe1\xfb\xbb\x96C\x0b\xb6\xe1\x91\xf3\x0c\xe4\xfdj\xbc\xfaf\xa1m2\xfe\xe4\xab\x01\x93\x95\xae\x8bL\xf1\x1d\xd5\xa2\xc8\x92\xdcE\xe4\x9f\xbb\x83\xd3\xde\x99y\xab\x0b\x96\xcc\x922\xc4\x08!\x94d\x9f\xca\xa5\xeff\x1c\xbd\x8c\x8f.IJ\x8f-\xc3\x1cr\xa0\n\xbb&\x9f4\x1bs\xbc\x16\xc3c\x15i\xb5\xa8\xe4O/\xec\xbb\xf1\xfcQ\xf2\xd8\xfav\xaa\x97\x1a\xa7\x98\xe0\x9f5UN6\xb0\xe7\xfa\xe6\xa5\xd2l\xaedI\xf2A\x03\x17x\xc1=0j\xe5\x9c\xd6\xcb\x13|\x85\xa4a\x94a\x9e~\xb9\xaej\xeeX\xa5\xc4\xb1\xa3\x10\x18\x9cm\xc05v\xd2\xda\xe6\xe5\x07\xcc\x15H\xc6\r\'N\xddG\xab4\xce\xa5q,j\x92\xc4Lc \x94\xef\xf9qN\x8e[/\xb3\x8c\xc7\x89\x01\x1f#u#\xe9UR\t\xacm\xa4\x825w$dc\x18\x1e\xc2\xa3X\xdaRL\x83h\x03#\x00\x9f\xcc\x9aVl\xa4\x92.\xdc\xdc\x1b4\x91\xd5\x89\r\xd602\x00\xf6\xac\xb8\xae\xe5\xb8\x99<\x8d\xde[pF\r^\xc5\x88T>o\xccOV\x07\x9fj\x9eR\xea#\x8e\x10\x1d\xb1\x8cFq\x81\xf8\xd3Wj\xd6\'\xad\xca\xf6\xb1H\xf7\xeb\x14\x8eB\x01\xc1\xe7\x93\xe9\xd7\x8a\xd4\x965\x9a\x10\x8c\x02\xc6\xa3\x94\x03\x9c\xfa\xd6kD\xd0;\x19\xb7\xa3\xf6\xcb\x00~\xb5*_\x02c\x08KH\xbcdc\xaf\xad\rIt\x1e\x8f\xa9\x05\xbd\xbcw\x97@ow\x0e2\x03\x1e\x00\x1d\xaa\xd5\xee\x9b\x15\xb4\xe9\xe5\xcb\x85+\xceG\x07\xd2\xa1k\x8b\x95\x95\x9d6\x19FrvR\x0b\x99\'\x82H\xee~Vn\x87 \x8c\x1a9d\xd8h\x8a:\xc5\x84\xf7\x068\xad\x9d\x8b"\xe4\xb1\xe7\x8a\xcf\x9bJ\xd4n\xad\xd9\xa5en\x07\xcd\xbf\x96\x1e\xf9\xadh\xa51\xe4)\x85\xe2^\x03D\xc4\x95\xff\x00x\x1a\x82X\x99\xcf\x9b\x05\xc3>\xf1\x82\x01\xda\x07\xb6+U\xcd\x17bl\x9e\xa3t\xcbP\xf6\xdb\xd5Q\xa4\x8c\xe0\xeeq\xc1\xf5\x19\xae\x96\xc2\xc5\xeemD\xa2E\xde88o\xbb\\\xf0\xb7\x92\xd6\xd5\x9e\x08\xd5\xf00\xc3fy\xcf\xaf\xadEm\xabO\x1c\xa3\xcc\r\x14x\xc1\x8c\x13\x8a\x89S\xbbm\x15},wZl\xaewCw"\x80\xa7\xf8\x0e\x1b\x1fZ\x8aha\xb7\xf3D \xee\'\xef\x1eEso\xac\xb4\x89\x98\xc61\xc2\xecS\xc7\xf8\xd5\xad=\xe7\x96\x06yd,\x8f\xd01\xc75\x9c\xa2\xd2\xb0!g\x1b\xa5`\xd2\xe0\x03\xbbgcYw\xd7\x88\x91\x00\xa0\xe4\xb7Q\xd0b\xa6\x91Yn\x99\xd5\xd7\xcaPO-\xde\xa00\xdc\xcb \xde\x99BrT\x0e\x9fJ\x98\xc6\xc5y\x12\xa8\xf2aI\xa3\n\xd2\xb8\xce\xe3\xda\xac\xc3$\xf0\xc6s)\xdd \xe4\xfaTa\x12\x18\xf6\x93\x80\xb8\x1d9&\xac,+<\xc0\xb1,\xa0ps\x8c\xd4;\x94\xecB\xd3G\x12\x9f\x98\xc8\xe3\xb3\x11T.\x08t29\x1f\x9dZ\x96\xd5\x1au\x1eX$\x9c\xf5\xf4\xa5ku\x8dc\x8c\x00NHb\xdf^)\xad\x04g\xa9)\x18H\x91w\xf5\x05x$\xd6\x8cw\xf3\xcfn\xb1]7\x98\xdd\xe4\xdd\xf3-Bcb6F\xaa\x18\x12\x01>\x94\xe8\xad$\x88\x10\xcb\xf21\x0ed\x078\x15\xa2\x96\x82i\x17\xa2f\xb6\x8c\xab\xc8\x00\xc6F\\\x1d\xd4\xf8\xee\x80\x8f\x9d\xc3\x1d3\xcf>\xd5^\xe0\xfe\xe9S\x0c\xea8]\xc7\'\xf0\xaa\xb7\x111\xc2)(H\x18\xc7o\xadK\x92\x0e\x97.]]\xa08W\xf9O\xcd\x9e\xf9\xa6\xc5\xa87\x99\xb1\x1cg\xb1\xc7z\xcaxdW\xfb\xdb\xb3\xc7\xde\xebK\x1b\x18c-\xe518\xc0!\xfa\xd0\xac\xc2\xfaji\xbc\xf2y\x9bX\x80s\x9c\x81\x83M\x96[\x95\x98<s\xed\xdd\xc3\x02:\xd58\x9b\x94\r\x99\x19\x88\x0c\xe3\x9d\xbe\xd9\xab\x91\xc6\xcf\',\x17\x07\x80\xc7\x92\x05U\xac\x04\xd0^\xce\x87\x1ds\xc1\x07\xb9\xf4\xab\x02\xe1\xde@\xb2#(\xc7\x18\xa8$\x82X@\x93$\xc7\x9c}\xdc\xed\xfcj\xc4\xd2\\\xa2\xedh\xa4\\\x0f\xe3\x18>\xc6\xaaJ\xda\x12\xb5\xd4z\xdc\x95uf*\xa3<\x03W$\x97\xedP\x85%\xa3-\x9e@\xebXo<\xaf\xb0<-\x19\x18\xf9\x8d\\\xb6bPbF\x7fC\xdcV.=FP\xd4<\xf5\xbckkd.\xc1\x01\xe7\xb1\xac)\xc5\xc2~\xecu#?w9\xfaWT\xb9\x13\xbbq\xbf\xa6OqQ\x94\x88\xca\xa5\x9419PB\xf2\xbe\xf5\xb4*8\xf4"P\xb9\x8b\x06\x9e^\x04\x92B\xd9^:\x7f\x9c\xd4\xf8XS\x0cq\xe8\xb8<\xd6\xa8+\x1cJ\x86%1\x9c\x96 \xf7\xaaW!\xd8)x\x9c&\xec!o\xfe\xb559\\j+b\xc0\xd6\x1e\xc6\x08\xe2\x9a\xe0H3\xc1\x952G\xd3<\xd5\xa6\xf1=\xaa\xc6\x98\x9ePI\xfb\xa6\xdc2\xb6;\x83\xd6\xb1.l\xa3\xba\x90,\x93\x8c\x1e\x8c\xdf\xe3RA\xa5\x9bh\xd4<\x85\xd0\x0e\x1b\xefb\x94j\xb4\xb7!\xd3\xbe\xc5\x8b\xff\x00\x16y\xd2\x12\x9au\xac\x913\x00\xce\xb1\xede\xfc\x8e\x07\xe2*\xdcWk<-\x14\x13I\xe5\xb1\xf9\xd4\x801\xfa\xf3U!\xd2!YZX\xd4\x87#\'\x00\xf2=qW\x92\xd8\xc9\x1ea\x96&\xf9\xba\x02\x0e?\xc2\xb5\x9dOh\xd3Hq\xa7\xca\xb5e\xa7\x8e\xfe\xda\xcdc]\xc6>\xcc\x01\x0c\x07\xe5\xc8\xae~\xfa\xe2\xee\xdbT\x0eSqe\xca\x97\xe8*\xfc\xd7~!I\x964\xbcKt^\n\xcc\xa1r=9\x1c\x8aw\xda\xae\x88\x1eu\xbc36y\x1bC\x0c\xfd\x7f\xc2\x9c\xa1\x1bY\x92\x93n\xe6M\xee\xa2\xfelr\x9c\xef\x03$\x81\xfe\x15\r\xc6\xa94\xf0\x88U7\xab\xf0\x0b\x92CV\x8d\xef\xd9D\x8a\xa6\xd2K}\xab\xf3pv\xfe\xb5\\4H\x8a\x11\xf3\xc6x\xe7\x14{>F\x9d\x8aV\x96\x88\xc6\x87\xed\r#pY\xc7\x1b=j\xc4\xb6\xd2\xdd*$\xc8\x84\xafE\xdb\xf7O\xd6\xb5"h]\x96Gr\x1f\xf8Hl\x1f\xca\xb4U\x9ax\xff\x00y\xbc\xaf@Xd\xfdi\xfbF\xafm\x03\xd9\xae\xa7?mj\xd6\x9bF7zn\x8c\x1cU\x96\x92f\x03l\x92+\x03\x90w\x0e+rM5"\x81Z&Gbp\xc5\x89\'\x1e\xb8\xa8&\x82\xcb\xe5g]\x8c\x0f\x04\x11\x83\xecG\xff\x00^\xb3O\x99\x96\xec\x91\x98\x8f\x9d\xcc\xd2n\'\x9e\x07\xcc*\xdd\x9a\xf9\x8a\xc8d\x95\x98\xe3\x03\x1c\x1f\xad\x13G\x04vms\x1a\xbe\xd5\xe5\xb6\x91\x98\xff\x00\ne\xb6\xa1e2\x16\x8e\xeeP\xcb\xf8b\x9b\x83\xb5\xd0s!n&\x8e\x1f\xdeG\xe5\xee\r\x97\xd89\x07\xd0\xd4b\xf3\xcd\x1eaq\xc7PO\xf9\x15#l\x9dY\xa1pH##\x18?\x8f\xad%\xb5\x9a\x93"D\xf0\xf9\x8d\x96!\xce\x0f\xff\x00Z\x97%\xc7t0\xdenVpK\xabc\x04\ndW\x92\xa4\xdb\xa2\x18l\xe7.3\x8aX\xede\xc7\x18 \x9e\x08a\x8f\xccT\x82\'\x03k&\x1c\xf5\xeeE$\x92c\xb5\xc9\x99\xde\xe6H\xd1\xd9F\xeeINp=\xf3\xfe5J\xfa\x08\xd2\xe7\xf7w\xden9\xf9\xd4\x0f\xc2\xa5e;\x9d\x00fP\x06B\x9e\x95X[\xa2\x03\xb8.\xd69\xdcy\x15\xa73n\xec\x85\x14H\x96\xf0\xbb\xc7$\xbec\x900\x0b\x1e\x9f\x95Y\x82\xdbK\x8ac$\xf08I:\x88\xca\xa3c\xfe\xf9\xa8\x19\x92"\xaa\xa79\xf6\xc8\x14=\xcch\xa3\x9c\x11\xd0w\xcf\xf4\xa7\xcf7\xbb\x0eH\x9aq\xae\x8b\n\xcb\xf6H\x1d\xcb\xaf\xee\xbc\xd9yC\xea\xc3\x1bXU[k\x8b8]\xa3\xb9\xd2\xac\xae3\xc1\xd9\x98\xdc}\x19\x7f\xa8"\xa9\xac\x96\xc3\x1b\xc3\x06\'\'f\x05:8\xed\x99\xd9\xbc\xb7Q\x82w\x0f\xfe\xb5\r\xb7\xa8\x92KAn\xed\xb4\xf9%\xff\x00F\x8aXc8\xe2V\xdeA\xef\xca\xe3?\x95=#\xb5\x83o\xca\x19\xbb1\x07\x93T\xda\xday\x1d@\xdd\xb5\x86\xe5\x07\x8a\x92\x02"\x8c\xab+\x9cv\x14=\x06\xb5-Aumo\'\x99\xe4\xb7\'\x90zU\xef=\xe4\n\xe9\x0f\xee\x08\xcezUPc\xf2U\xdeh\x94\xb1\xe02t\xfeuqLRG\xe5y\xeb\xb5yVQ\xc1\xac\xa5\xa9W\xb0\x86\xe1\xe3\xcbI\x00h\x8f\xfb_\xd4t\xad\x01o\xe1\xd9\xe2\x8by\x129`\xbeZg\xcc\x1e\xfd9\x15\x98V\x03\x1f\x97\xf6\x89779Q\xc0\xfa\xd5mAc\xb4\xb4K\x99oU\x15\x9c((\xbb\x98g\xd7\x07\x81\xf4\xad\xa8\xddl\xaeeQ\xae\xae\xc6\xd6\xa1\xa0[]\xc2\x8dg|\x80/\xf0\xcb\x16\x18~\xbd+\x02\xe3G\xb9\x8a\xe3s\xcaT\x83\xc0^\x07\xd6\xadC\x7f\xa9,\xd1\xacw\x0fq\t\xc9\x04\xc6\xd8a\xf8\x8f\xe7W\xaeug\x9a\xcd\xad\xef\xadPH\xa4m\x91x\xeb\xea+[\xc1\xee\xac\xc9J]52\xe5\xb4\x8a\xf6(\xda7cp\xbf#d\x85]\xbe\x9fZ\xa4\xf6\r\x03\x96I\x83\x1f\xef`\x8e=>\xb5\xac\x9b\x11Y#Dq\xe8X\x8f\xc4\x1a\xadu\x0c\t\x0e\xe0\xab\x0c\x8a\xdb\xb0\x19\x88\xe7\xbd94\xd8$\xd1\x11\xb6\x91b8\xbc\xb6\x90\x85\xf9r\x08\xc7\xd4\xe6\xa9\xbd\xad\xc4\xca\xca%\xc1^v\xc7\x8cT\xaf\x1f\x9b\x92\xcd\x1b\xae{u\xab\xf0Ef\x92\x14,\xca\xbbpJu\xfa\xd6M\xc5l\x8b\xe5or\x95\xae\x9d\x82\xb2O\x08\x03\x83\xe6\xbf\xf8\xd6\xb3@\xb9,dM\xa3\x00\x8d\x84\x9a\xa9"\xf9`\xc7\xf6\x8c\xae~Vn\xb5b\xde\xdai\x01ct\x1d\xf1\xccx\x1cc\xb8\xac\xe5.\xc3\xb1"\xbd\xb2\xab\x07\x882\xe3\xee\x7f_\xfe\xbdRH\xee\x1e\xeacma*\xda\x0c\x03/\x94\xc7`?\x9e\x05G3]\x19\n\xc9\x0e\x1d\xb8\x1cu\xabv\x9a\x9e\xaf\xa63Ii1\x8d\x87\x1b_\xbf\xf3\xcd8\xb5\xb4\x84\xe3-\xd1N\xf2\xc2x\x9f\xf7l\xc6\x11\xca\xb0\x18\x04\x7fZ\x8e\x1b\x9b\x98\xd0l;W\xa9b\xbc\xe2\xa5\x9a{\x9b\xb9\x8c\xb73y\x05\xb3\x95N\x14\x1fP3V\xec\xf4\xb9.\xd5\x15\xa7\xb8\x9dA\xe7\xc9\x88\x96\x03\xf5\xaaQ[\x03o\xa9\x97;\x07\x93\x94,\xe4g$\x11\xc5*\xdd2\x00\x8a\xcc\x03\x7f\x0b\n\xde}\x1bG\x8e5k\x8dFt\x99\xb8\xf2\xc9\xc1_\xa8\xfe\x94\xc8\xbc%qr\xdb\xa0-q\x00\x1f\xf2\xcdr\xd8\xfa\x03R\xe9\xf3l\x8aSKvb\xdd\\\x97\xbb\x11G\x8c\x95\x01\x88\x1c\xa9\xab\xd6\xef\t\x85\xa1Y\x0b\xb2\xf0s\x80M\x13h\xf1\xd9\xcd*\x1f1%\\m\xf3\x17\xa1\xf7\xa8\xad\xec\xae\x0c\xac\xd6\xd9yy\xdcB\xe4T8\xa6\xac=I/A\x88\xa2\xc7\x92H\xdd\xbb8\xe7\xd2\xb2\xa4\x9aY\xe7]\x9b\xe4e\xea\x00<\xd6\xb3\xda\xcd4\x18\x96=\xf2!\xfe\x11\x81\x9fpi\xb6\xba[\xc7+<{\x98\x81\x9f\x9b\x8f\xe5R\xa3\x14\x82\xd2\xeeE\x10\x9e@\x85B bN\xe1\xc95\xa0\xc8\xd2\xa2\xa2\xc8S\x03\x04\xff\x00{\xda\xab\x93\xb8m\xf3\x962[\xae\xdf\xd2\x9b\x1d\xc3F\xa6 V@\x063\xd7\x15\x0e\x1d\x8a\xb1bpmbTLI!\xeb\xc6O\xd6\xa0\x91\xa4~Dd\x928\xe2\xa2YfWg2\x12[\x86\xdc\xdc~\x18\xab\xb0\xde"\x92\x83r1\x18\x19\xc9\xa9\xf6c2\xde\xdc\xbf\xdf\xdf\x80}\xb3Hl\xda\x11\x86\x1bK\x0c\xa9#$\n\xd4I\x84%\x80\xf2\x9b\'\x9d\xe9\x9cT\x92\xcf\x03e<\xb5^3\x94\x18\x14\xec+y\x19\x91\xc0\xd18\x00\x1c\xb1\xc9\xc3\x0e3\xe9S!\x00\x97\xdd\x8eq\x8c\x1e\x95t\x88FY\x1e \x08\xe4\xe4sQ\x0b\x91*\xa0.\xa7\x9d\xc0\x05\xe3>\x95vAr{x!tu\x99\xdaQ\x8c\x86\x1f)\x1f\x91\xc1\xab1\xa4h\x8a\x01i\x17<\x02\xc6\xab\xe5_r\xc6B\xae\xdd\xdc\x03Q\xc533\x04]\x83\x8f\xe2\xe4\x03\xf4\xf4\xa3q\x16D\xf6\xb07\x95\xf36y*\xdc\xd4\x82\xf6\xd1Y\xb6#!\x03\xf8\x07\x19\xf4\xe9Y\x8e\x84\xb6\xf7\x8c\x11\xf7\x8e\x0f\x14\x91\xdb\xa3J\x1a)\x82\x1c\xf23\x8c\x8faM\x05\x91v[\x83q\x10\x11[\xa36Gp\x0e?\x1a\xa6\xd2yJPnF\r\xc8<\xe2\x90\xdbD\xc0\x919f\x1f\xde\xff\x00\x11Ow\x11\xb6\xd6\xd8\xc3\x1dG$UY[a\x15Zi\x011n*9 \x81\xd6\xa5\x82TD\xcb\x87\x91}\x14\xe3\x1f\x855\x06\xe5bv\xb6\x0f\x07\x07\x81P\xca\xef\xb5J\r\xbc\xe3\x04qE\xae1\xf2[\xf9\xb1\xf2S\xe5\x19W\x1ds\xefE\xa5\xd4\xd0\xc9\x821\x9e6\xed\xe1\xabY\xad\xad\xa2\x91\x8e\xfc\xb60@\x03\xfc\x9a\xa6\x1eH\xd8"\xf1\xce~a\x90*9m\xa3\x1ai\xec\x84\x93Us\xc4aRE\xe1K\x0c\x7f*4\xdd]\xe1\x95R\xf44\xb1\x16\xcb\x00\x80\xb7\xe7\xc1\xa9\x91L\xf92\xaa\x93\xd7*\xb8\xabV\xda|\r)%\xb2\x00\xc9\'\x8cU\xc6\xa7#\xd0\x97\x14\xd6\xa5\xa7\xd7<\xa9\xbf\xd0`\x93\xca<\x18\xae\x0f\x98?^\x95J\x08\xe6\xb9\xb82I\x0e\xd5\xce@F\xc0\xab\x824\x8b%\x146\xde\xe0\xd0\xce\xb1G\xb8\xb1Ry\x0bJUd\xdd\xeeJ\x82[\x11%\xb1"P\x81p\xe3\x92\xeb\x93\xf9\xd4\xc7N\x86\xea\xcf\x88cIG\xcb\xbd\x067\x9fCU\xaeX\xa2\xb4\x83p@2px\xac\xc95d\xb7\xb7\xf9\xa0,\x99\xca\x9d\xc4s\xeb\x8a\xa8\xcd\xbd\x1b\xd0-\xd8\xb6lP8\xccI\x95\xfb\xd88&\xa1\x95\x01w\x85\x033\x8c\x10\x86Lc\xf9Q\xfd\xbfoqn\xd2\x0f\xb4y\x83\x00\x87\x85v\x8f\xc4\x1c\xd4\r#\xdd\xed\x92\x19\xa4NA*\x14g\xf3\xcej\xadn\xa1\xa9+\xc7:\xa6Pl\x03\xfe\x9a\x8f\xd2\xab\x8f0K\xf3 b;\x1ey\xa5\x8eY\xc0\x91<\xe7\xc6@!\x8ei\xcdp\xa5\xc2\xbc{\x08\xfc\x9b\xff\x00\xafR\xf9JW\xea\x02\xdcL\x1br6H\xf9\x95;\xfe\x14\xd8\xac \xdb\xf2C\x10f\xe7\x949S\xf5\xcfZy\xfbT\x0b\xbffba\x9c?\xdd"\xab\xcdv\xc8\x84\x98\x84jN~Q\xc5U\xdaZ\n\xc8\x90C\x0cH_r\xa1=[=?:X\xae \x8dw\x06\x8d\xa6\x1fw\x03\xf3\xcdUG\x17\x05\xc0\x95w\x1e\xcd\xdc\xd5\x88,\xb0\x02\xcb D\xc6\xec\x03\x8d\xc6\x93Z\x0f\x99\x13\xc7|\x93\t\x19\x01\x8c.\x04\x83!\xb2}\x87ZcL"a"\x86r\xdc\x86\xc6G\xe5Y\xb7:\x8c\x8c\xe2\xda\x1d>)Y\x8e\n\x88\xf9a\xe9\x9c\xf3W\xc2\r<\xa471\xf9\x13c;\x1d\x88\xcf\xb6M\n\x9bq\xe6H\x9eu{6c\\j\x7fg\xbb3\x08\xc0\x94\xf5\xdb\xc0\xfc\xfbS\xa0\xd5\xe2\xbe;\x19aV^\xa3\x07\x8f~\xb5\xb2\xb6Ze\xe3\xb4\xd3B\x19J\xe1\x97$\x10{\x9c\x8e\xbf\x95U\x83D\xb2[\x90\xd6\xe8H\'\xa9=\x07\xf8U\xb9SH\x94\xa7{\x90\x162!U\xda\xc9\xdfi\xe4P\xaa\x88\x99\xd9\x86<\xe4\x13\x9a\xbet\x88RB\xf2J\x11G`\xa0\xff\x00\xf5\xe8X\xec\xc9\xdd\xc3\xf3\xc1\x04\x81S\'\x1e\x8c\xd17\xd5\x19\xf9\x19\'a,W\x9c\xf3NY\x80\x07\x00\x15\x1dB\xb69\xab\x894\x1eqE\xb6@\xa0\xfc\xc4\x92EU\xb8\xb9\nJ\xc4\x8a\x0ex8\x1f\x97J\x9ep\xd0\x98M\x04\x91\x86T\x9dg\x1d~q\xb4\xff\x00\x85J\xb0[N3\x14\x93\xabc\xe6\x8c\x8c\x9c}x\xaa\xd0jw\x8do\xe40\x85\x00?\xebD 8\xf6\xe0sW\xd2\xee\xd4@VTy\xe4\x18!\xb2\x00\xcf\xa6\x0fz\xd1\xce)\\\x84P{!\xbd\x8a:\xb0\x07\x81\x82\t\xa3\xec\xd2\xc6\xc4\xb2\x908*A\xe6\xb6 \x92\xc2G)v\x93*\x91\x94\x918\xe7\xe9M\xba[H\x84\x82;\xad\xc5@d\xde\xa4\x12*T\xe3-\xca\xd8\xcc\x96i\x98\x15\x03r\x8e\xab\x8e\xb5\x1cs\\\xaa\x1d\xbc(\x18\x1f6\x00\xfc*\xe2\xe2m\xce%1\xa1\xe3r\x8d\xc3\xf9\xd3\x1e\xd6\xdc\x12\x12\x7f0\xf5\xc8\x04~b\x9aC\xd0\x8b\xed\x13\x90\x0b7#\x8c\xf5\x18\xab\x7f\xda\x0b2\x05hX\xca\x00\x00\x86\xca\xfeU\x18\x10?\x1b$F_q\xc8\xf6\xa9\xe0k}\x804n\xce:p:RR\xb6\xc0\xe2\x8a\xc5\x9a27\xc6>S\xd4\x9e\x9f\xe3K\xe7\x16!VC\xd7\xd3#\xf2\xad\tmF\xd8\xccq9V\xec\xcc\x01\x06\xa0k\x19\x19\x9dWp\xda3\xb4\xf0G\xe3I6=\n\xa43\x82\xdf0\xdb\xdc\x8c\x82\xd4\xc0\xac3\xc1%\x878<V\x8d\x9d\xaa\xc3\xe6\t\xe6A\x9c\x1c79\xfa{\xd04\xc7\x95\xdb\xc8)\x82GF\x19_\xc3\xad\t7\xb09%\xb9\x9a\x88\x15s#*(\xef\x82Fj\xd3\xacf"\xdb\x91\x8f\x19\xe0\x83Nk\x19\x1a\xe5\x95\xa3d \x90=\xea7\xb3\x99S$\xb6Gf\xe3\xf0&\x96\xabt\x17O\xa8\xe8\x9b\x10\x82\x85\t\xeaA\xe4~\x15!("\xcb\x1d\xccz\x8c\xf2*4\xb6\x91\xcb/;\xb1\xbb\x9e\xc7\xfc*ht\xd7iFX\xab\x0eO|\x8aC\xba3\xbe\xc8\xf39\x97\xe6;\xbe_\xf2j7\xb50\xba\xa8\x95\xc8\xcf\xf0\xb7#\xd8\xd6\xd3[")\xdeH\xeb\x80\x061R$z{J\xb1\x00%\x18\xe5\xb8\xebF\x8f\xa1%\xdd\x13_\xd6\x1bM\x10\xfd\x9e\xca\xee\xdd\x15\x95c\x9d\xf10\x1d\xc6z\x91\xecj\x18$\xb3\x8e\xe2/\xb2\xda=\xb4\x84\xff\x00\xcb6$\x86\xcfQ\x9c\xf1\xed\xc5A"\xd9\x92\xeb\n3\xcb\x1a\xfd\xc6\x00\xe4{\x1a\x86\xdeG\x08\xe9\xf6Y\xa1\xc6\x1b\x04cq\xfe\xb5\xbb\xab\'\x14\x99\x97$n\xda:\x8dkO\xff\x00Iw\xd4\xb5h\xda\xed\xd5J2\xc0\xec\x8c\xb8\xe8\xf8\x18\x06\xaa\xc9\xa6\xe8\xf6\xc1\x1e\x1dB\xef\xcb\x98\x0c\xba\xc2B\xe7\xbe:TV\xd3_A!\x969$\x08\xeb\x8c\x85\xe7\x07\xb1\xc5P\xbf3\xb4\xa5Z\xe6I#\xcev\x13\xc2\xd3\x9dX\xbd\xd0\xa3\x16\xba\x9a\xd0xf\xde\xee!!\xd7!\x07v\x00\'\x91\xe9\xd0\xd5\x19\xf4{\xd8\xaedH\xae\xa3\x94\xa8\xe4\xaa\x9eG\xafN\x95Sl\xac\xe0\x19Y\xe3P\x14\xa9\xe83K\x15\xa5\xdcw\x19\x8ewF^W\x9e0}+\'(>\x85\xaen\xac\xa9&\x90\xc9\xbd\xc8\x19\x1d\x87\xaf\xadT\x89\x1a\x13 *\xa1Yq\xcf\x07\xf35\xb5%\xad\xe4\x924\x93LrG\x07;\xb3U>\xc14m\xf3\xab\x12\x06\x01#\xf4\xa8\xba/\x99\x99\xed\x89<\xb5\x18,@\x00c59\xb5\x8f\xcb\xdcd\xdd\xc1\xcf\x1c\xad_:S"\tG\xc8\xc7<7 \x9f\xad6;k\x88c\x00\xb6\x0erW\xaf\x14]\x05\xcc\xc9\x11\x1c\x87\xdb\x80\x067\x0e\x87\xd0\xd3^\x0c0\xf2\xc6K\x0c\xe4\x1c\xd6\x9d\xad\xa6w\x96\xc1\x07\xa05r\x1bx\x90\x05\x91v\x80pF:Rm\x0e\xe7:\xb6\xc4\xc5"\xc9\xcb\x01\x90\x00\xefS\xc7e+\xdb\xae\xd6\'\x9e[\x1c\x0f\xf0\xae\x81-bGr]O\xa7=\x05<}\x9d$ny\x07\x1c\x8e\xb4_@\xe6g;\x1d\x9d\xceI\x0c\x17=Fz\x8fj\xb9\x1e\x97#na0\xc1\x18\r\x9eG\xd6\xb6\xa1\xf2%\xdb\x80\x15\x17\xa1lR\xf9\x90\x86\x10\x98H\x04\x9cl\x1di\\W9\xb7\xb2\xbc\x8aB\xa5\xb2\x08\x04\x81\xcd\x16\xf6R<\xff\x004\x9c\xe3\xb9\xc9\xf6\xae\x8c\xaf\x97\xb4u-\x9c\x83\xda\xa5h"\x10\t\x9e<\x1c\x03\x9c\xe7\x02\x8b\x85\xd9\xce\x1byyU\x8cds\x9295Y-\xe6\xce\nc\x9c\xe4\x8e\x95\xd4\xb8\xb7\x182gi9\x18\x19\xfc\xfd\xaa\x15xw\x11\xb4\xb4lKci\xe2\x9d\xc3\x98\xc8:{\x88\xc3g#\xa6GaU\xde\xdeU\xde\x8b\x136\t\x00\x8e2}k\xa6\xb6\x9a\x13\x88\x99HX\xc6\xe1\x9e\xf4\xc9.!WVR\xa1Y\xba\x1aW\x0b\xb3\x06o7g\xee\xd0s\xeb\xd4Q\xb2q\n\xb089\xc1\xf5\xad\x08\xee#X\xf6\x04\xde\xd8\xef\xcf\x15\x14\xeec\x0e\xa7p\x1bw.\xdfOo\xff\x00]\t_@n\xda\x94e\x98\xa2\xe3t`\x8fS\xde\xa7\xb7\xbdsf\xfb\xc0\xde\xc7\xee\xe7\x8f\xc0\xd5\x7f&\x19\xe3\x0e\xca\xc0\xb7\xdd.\x98\xcf\xe1V-\xad\xad\x9d\xcf\x98\xa4\x000v\xb6\x0f\xe5NP\xb2\xd4JI\x8f\x86\xf824a\xfeq\xd9\x8ej\xb4\x93\xc9\x03m\x0b\xb8\x93\xf7\x87o\xc2\xad}\x9d`o2(\xd5\x9c\x9e\x14\x8e\xa2\xad4\rq\n\xee\xdb\x9c\xf2\x08\xac\xec\x86\xccc7\xda\xdc##0\xce\t \xe3\xf1\x1d\xea\xec\x96BxU6\xae\xd1\xea8\xabol\x91J\x9eV\xd2O\xde\x04c4\xc9\x90\x850\xef\xd8I\xce=~\x94\xda\xea$@\xf60%\x8e\xd2\x15C\x1e\x9e\xf4Z\xc5\x02#Hc\xe9\xf2\xd3fG\x86t\x12}\xd2\xbf*\xe3\x8a\xacnJ!\x85[b\xe7%\x89\xebE\xbb\x15\xab\x14#,\xef\xf2\xe0\x93\x90\xd9\xcf\x1fJ&\x94\xf9L\xc6 \xc4\x1c\x02F?\x1a\xa8\xf7\xab\x10P\x83s\xe7\x93N\x17E\xd8\x9d\xdc\x91\x8c\x0eEU\x9e\xe2\xb7A 3\xbf\xcev\xae:\x9c\x9e=\xa9\xb2L\xedo$r\xfc\xaa\x0ex\xf4\xab\xa5L\x91\xae\xd0\x06G\xcc3\x8ag\x93\xc9I\x14yc\x8d\xc7\x9a\\\xddX[\xa1\x8a\x8a\xd1\xce\nI\x98\xd8\xf5\x07$V\x91\x82VU\x1b\xe4wc\xd0\x1eH\xf6\xab\xf6Zu\xa4\r$\x8e\x17iP\x14\x8cq\xf9\xd4\x8f\x14B?\x99\xd5\x9c0!\x88\xdb\xf8u\xabr\xbb\xb96\xb6\x84mk\xa6\xcdo\x89\xedn\xa0\x9dF\xef9\x17z\x91\xe8R\x9b.\x83\x1d\xdc\x08\xc1\xd2F^P\xb0d\xc8\xfaz\xd4\xed\xa9|\xb8\x8e7W\x1cd\x1c\xf1U\xa3\xbfx\xa3\xfd\xe0a\xce2\xad\xd4w\xad\x14\xc9\xe5\x1a\xfaD\x90\xc6\r\xba\xbcd\xf0\xca\xed\xbf\'\xdb\x1d)\xd6\xda}\xe4R\xf9\xad"0\\\x16^\x84\xfe\x1d\xea\xc0\xd4^V\xca\xca\xa0\x0e\xdby>\xfc\xd4\xb1\x1c\xe3\xcc\x90\xaa\xe79\x03\xa5L\xe6\xa4\xf5CQh\xad}o$\xd7\xbb\xe1\xe5\x1cg\xcb\xcfO\xadU\x16r$F)\xe0h\xd5\x8f\x0c\x0f\x18\xadFT\xb7\xb8e\xccn\xa7\xa3\x00?:\x9c^\xcb\x1c;|\xcd\xd1\x9e\x80q\x8a\xcf\xdd\xea\xca\xd5lsp\xdb\xf91\xb2\xe0\x0eIS\xd75J\xf3(\x02\xed.\xc7\x05\x82\xf6\xad\xe9V\xdc\xcf\x83\xf2\xb09]\xa4\xf5\xa8\x99\xed\xfc\xcf\x99X\xbepI\x157EX\xc5-4\x93\xc4\x02\x15\xf2\xf9\xf4\xc7\xe7Vn\x03\xdcbBU\x1b?\xc0\xb9\x04V\xac"\x00\xce[`A\xce>\xeb7\xe7RD\xf09\xf3b\\ 8$\n\x1d\xd0hQ\x829\xaed\x01\x94\x04\x03\x82\xd5o\xe7\x82^m\xe1\x91z\xfc\xfc\x1f\xce\xa5\xba\xbck\x1baq\xb7\xe4,0\x17\xa9\x15P\xea\x89u\x87\x01W\x8ey\xcf\xf4\xa5\x1em\xd0\x9bOCA$\xb2\x9d\x08k\x03\x16\xe1\x9d\xc9/\x7f\xa61T\xe6\xb4\x91\xe4\xdc\xa1W\xb6\x00\xfe\xb5\x04\x97\xb8 \x82|\xb28\x04sR\xc1p_\x08\x93\x15\xe3q\x06\xad\xcaOV\x1a-\x8b1\xda@\x92.\xe8\xc0=\x8b7\x14\xd3\x1crL^(\xca:\x9e9\x18?\xe1P\\O4I\xbd\xa4\x07`\xce\x07\xaf\xb5Do\xd9\xed\xbc\xc6b3\xd4`u\xfc\xa8M\xb4;\x97\x91\xe5\x9afr\xeaI<\xa9\xea*\xc4\xb9GfV\xcb\xe7\x1c\x03Y\x86Y<\x8c\xe4;7L\xf5\x154rH\xc8\x89\xbd\xb2\xdc\xee\xce*[hvD\x9b\xd6i6K\x80Gb*{\x19\x11\'\xf2\xf7\xe3\x1c|\xad\xda\xb3.\x14\xb9\xda\xe7 \x9f^\x86\xa5\xd3\xd1m\x0be\x89$s\xb8pE\\]\xf7dKm\r\xd6\x9dwms\xbc\x93\x82M4\xf9o\x95B\x08\r\x92q\xd0U)\xefT\x81\xb5\x17x\xc0\t\xd8\xe3\xbeMB\xd2\xb0\x0cc`\t?\xde\xcf\xe0i;\x89\x1a\x90X\xa3:\xc9\x1b)~\xe5y\xe0\xfa\x8a\xb9\xf68\xd44l\xbf6\x0e\xe2:V=\xb5\xf7\x94\xa5\x89>a\x1c\x91\x80?*\xb9\x1e\xad\x16\xd0YT\x1e\xec\xc7\x93E\x83Q\x8e\x8f\xbc\x8d\xacQp\xa0{z\xd1\xf6\x10\xc0\x95@\x01<\xb0\x1d\rH\x9a\xaa\x19\x1c\xed\x1b\x88\xc1>\xd5Y\xb5\xf5\x88\xb6\x15Xp*\x92v\xd0M\x96!\xb7s\x19\x8c\x84)\xb7\x1b\x88\xc9\xfcM9\xa1\x84@\x86g\n\x07\x18 \x91\xf5\x15Q5[|d\x1c\xbb\x7ft\xf7\xf44\xd9u\x08\xe6\x91Bq\xc62z\x12(\xb4\x80\xd5\xfb,9Y\xd6R\x10\x0c\x12x\xfc\xea\xa35\xb8\x90\xac\x8f\x94q\xc6z\x8cU\x0b\xadE\x96?/z\xb2c\x01A\xe0U9\x95\xe5\x82)\xf1\xb4\x9e\x11\x8bp\xd8\xed\xd6\x9a\x85\xf7\x0b\xd8\xd4K\xb8\xb7\xf1\x92\xaa}\x0e>\xb5io<\xc0\x8d\xc2m\xea\xcc:{\n\xc0\x81oLAU\xc6I\xdaP\x00z\xfb\xd3\xa4\x8e\xea\x02\xdb\xd1\xd7i\x03v\tP\xdf^\x99\xaa\xf6Z\x075\xde\x86\xdf\xdb\x0f\x94]\x14\x14\x07\r\xe8j\x19o\x8b\xc2\x8as\xb7 \xee\xc1\x1c\xd6Ii\xad\xc2\xb0\xc6\xd6\xf9\xbdj\xbb_Jv)c\xb4\x7f{\xa0\xa9POa\xd9\x9b\xf2\xdc\xc5\xf6}\xab&\xd6\x1c\xb0\xebR5\xcc&%`K\xf5$\xe7\xbfa\\\xda\xcd\x97*\xc7+ \xe5\x87\\R\xc73G \n\xed\x95\xfc\x8d5\x01;\x9b\x10\xea\x11\xf9\xe4I\x16\xd5\xdc9\xf4\x1f\x85>\xe6\xe24VPr\x80\xe0\x1d\xdc\xfeU\x91o\xa8\x1bYY\x92(\xdc\xe7\x19a\xd8\xd0\x9a\x9c\xc4\x82<\xb0O\x18\xda\xb8\xfc\xaa\x9d.\xc2R5\x0c\x8c\x19^\'8#\xb7|\xd4\x8e<\xa8\x8333\x02r\x06F\xef\xfe\xbdf\xad\xf4\xcdn\xf0\xb3\xa9\x8d\xb8#h\xe3\xf1\xc6j\x16\xbba\n\xa8\x03+\xd3\xe6&\xa5S\xe8U\xcd\xc8\xa3\x84\x81\xb9\x91\x8e:\x13\x8f\xad:K\xd8\xa1S\x1cj@+\xfcDz\xfd:W?\xf6\xc5y\t@F\x0f$\x9e3S\x99\xda\xe3,\xcf\xb1\x91v\xf0:\xd3t\x98\xae\x8dyoUcY\x01\xfd\xd9m\xa4{\xd2\x8dK\xc9GBT\x06\x19\xda+\x9b\x91\x83\xc6cv\x04\xaf\xcc1\xebOG\x1f\xf3\xd1NG\xad\n\x8a\x06\xcd;y\x84\x88\xbb\xdd\x8a\x9c\x96\x00\xe2\xb4?\xb4\xac\xe1\x0fn\xca\xa4g\xbds&pO\xde\x1bG\x15\x17\xda\x1eK\xa0\x1b\xe6b\x00$w\xab\xf6D\xdc\xe8>\xd5\x1a\xb1|\x85\x00`\xe0\xf5=\xaa\xb9\xbeY\n\xbc\xc7|\x99\xe7v\x7f\n\xcfP\x84\xb7\xce\x0e\xdes\x9cS\x93f\xeeW\x18\x04\x96?\xe1K\xd9\xa2\xae(3\xb6\xe6H\xe4X\xf1\xf3\x13\xd0~"\xb4\xa2\x9c\x0bP\xd3Lwc*\xc0t\xaer\xd2{\xab\x1c\\[\x15\x90\x16;\x96E$\x11\xee:V\xc4Z\xc4s\xda\xec\xba\xb7\x8cn\xee\x83\x18\x1e\xd4\xaaB.J\xdb\x92\xa4\xedf\x03\xed;\xb7\xa3\xa4\x81\x9b\x96o\xd3\x9a\xb8?zUX\x04\r\xfck\xd8\xfdj\x9c\x9fe\x84\x88\xd0\xca\xa8\xdc\xa8-\x92\x7f,S\xa3\x99a\x8d\x88fe\x1d\xff\x00\xc6\xa2\xa4\\^\xe5E\xf3\x1a\x13D!\x90\x05p\xc5W\x93\xff\x00\xea\xaa\x8dv\xeav\x81\xcf\xa8\xaa\x0f\xaa;1\x1b\x06:+\x8a\xaad\xb8y\xbeBT\x8eI<\x03P\x93\xecV\x88\xd3\x86\xf2S!\x12I\xb4z\xf5\xa9|\xf8\x97\x83\xb1\x9c\x1c\x86<\x83Tm\xe2\xfbSHw\x84U\xe3\xafz\x87\xec\xaf\xb9\xe2Y7\x11\xd4\xd0\xe0\xd8\xd4\x97CBi\x9em\xa3\x03\xd0\x1fJ\x8aHX\x93\x97\x1b\x07\xadV\x96;\xa8 \x8c\x0c6O\x18=\x05F\xf32\xae\x0es\x9c\xf7\xc8\xa20\x90\x9c\xd0\x97\x10G\x82\xdb\xbepxU\x1c\x1a\xa4n\x96\x17f\xc6\xdd\xa7\xb7?\x8dY\x9a_5|\xb2\xc1?\xe08\xc5R\x96\x05\xf2\x82\xa1\x00\xe3\xb1\xeb]1\x8b[\x98\xb9_a\xeb\xaa\xb7,Au=\xb1\xd3\xf24\xe4\xd4D\xd2\x85F1.2\xd9 \xe7\x1f\xca\xaaGc0\x948 c\x82v\xe7\x14\xf9\xb4\xf9\xa5u\xe1\xb2\xa3\x07\x8e*tR\xdbA\xa7"k\xadA\xd51\xe6o\xcfM\xbd*\xdd\xbe\xa8\x8c\xf1\x05B\x8c\x14\x9c7*O\xa6*\xa4Z3<ckc\x1e\xdc\x83VN\x97$1\x05\x91K`g \xe2\xa3\xdd\xe8[m\xee_\xb6\x91\xeece\x96=\xb1\x8e\x8c>\xf7\xe7U\xae\xda%\x8b\x0c\xd2\x80[\xa0\x03\x9f\xd6\xa1\xcc\x96\xea"Gx\x91\xf1\xbfn?:b\xc7\xe6y\xaf#4\xa0\x0e0\xbc\xd4\xb7a&H\x91\x19\x0eb\x95\x8eGC\xc6)\xca\xd2E \xf2\xee>`pF\xee\xb4\x90\xb2\xcb\x98\xe1\x8aP\xea\x062\t\x15\xa3m\xa0\\M0\x91P\xab\x7f\x12\xf5\x1fZ\x85+2\xae\x98\xed\xfb\x900p\\\x0f\x9f\r\xc5G\xf6\xa8\xdc,S\xb1\xd8y\xdc\xbdA\xff\x00\n}\xce\x91%\xa4\x01\xa6vb\xed\x83\xb4}\xda\xcf\x16\x92\xf4S\xb9zu\xf4\xf4\xaaJ\xfa\xb0\xf44C\r\xdb\xdc+\x15\xe4\x11\xd8v\xa6\xf9{\xa4 \xe3\xe69,N*\xb4Vn\xb1q  \x90y\xe0\x8a\xbb\x05\x8c\xd7\xcc\x11\n\x81\xef\xda\xa2P\xecR+5\xb4\xf9|\xcb\x94\xdd\xc1+\x92E"Z\x98\x9d\xd4\x02p8=1\xf8\x1a\xd2\x8e\x19\xedT\xc4\xc1\x878\xc89\x18\x15\r\xcb+H\x08s\xb0\xf3\xd3\x184ZB*\xca|\xed\xd1\xc8A]\xa39\xc6?\xfa\xd5U\xad\xa2X\xb6C\xbb`<\x82s\xf9U\xc9`ftm\xd9\x19\xc1\x1bjY`(\xd1\xab&\x1f\xdb\xa5k\x18\xdbVK}\n\x90C\x1a\xa0W\xc7*pG\x18\xa7E,Q\xbb1\x1b\xb00\t\xa9o-\xe5x\xb2\xbf3c#\xe6\xe4\xd5\x18b\xfd\xdc\x8a\xce\x17<\xed\xee*\xb9U\x89\xbblu\xc5\xc8\x0c\x1b<\xe7\'5\x01\x95\xe4R\xa3\x00c>\xbf\xa5(\xb4/&\xe31\xc8\xe4du\x14\xe9\xe3H\x8cl\x17\xa8\xc8\xe3\xadZHz\x90,\x8c\xcb\xc0<u\x1e\x95"\xcfp\xb8S\x83\xcf\x7fJ|F0\xc0d\x9c\x8c\xe3\x19\xa8\xcd\xc3\x1c\x16\x1b\x93\xd4\x8e\x05]\x93&\xed\x13\x9b\xc6*\x17\xa8\xf5=\xaaw\x9eGD*\xd9\xec*\x99\x988\x8d\xdbj\x16\x1cmP3\xefS\xc8\xd23oTwU\\\x0c\x0f\xe8*-\x15\xd0|\xcd\x8fy[\x19|\xab\x01\xf2\xe0pj/0\xecb\x06\x18\xfbS\xbc\xe8d\x84\x00YN0A\xc6EF\xe9\xf2\xe7\xee\x9e\x84t\xaa\xb6\x9a\x05\xfb\x92+\xbal\xc3rz\xe6\xae42\xdc\xba2\x0c\xf1\xc1n\xd5C\xcc\xce\x06\x068\x03\x93\xcf\xbd\x10_Z\xa4\xfb\x8e\xe9\x008\xd8\xd9\xc9\xfa\x11G+\x0b\x97e\xb6q\'\x98\xd8A\xb7\x19\x19\x01\x8f\xd6\xabI\xb7iA"7#k/85b[\xb8\xd8\x9f,\xcd\xe4\xb6@O3~*\xaaG\xbd\x95bb\xa9\x9c\x9e8\xfc\xa94\xd0\x96\xc4\xaf\x1cq\xa0\xdc\x83\xcc\x18;\xd1\xb8\xa4x\xd9YKf0\xca2\xd9\xfb\xb5)\r$\x00\x99U\x86\xfe8\xe8>\x9e\x94\x8a`\x8aM\xee\x19\xe4\x18\xe0\x01\xcd;\x89\\\xb9\xa5\xcb\x142\x14K\xd8#\x94\x82\xb9\x95\x08\x04R\xdb\xddE%\xf3\xc52" m\xa7\x03\n)\xa6;;\x87\x0f\x05\xbb\x97V\x04\x89H\xc57U}M,c\x84\xef0\x16\xcbF\xee\x19A\xf5\x04\xd3Q\x8c\xa3\xae\x84\xb7$\xcb\xb7\x1am\x8d\xac\xdb\xacY\x9d\xc9\xdd\xb9\x1c\x90(\x8a\xc9\x05\xd1\x9ek\xa6g`7\xa1c\x82\x7f\xcfsY\xb6\x11\xea,\xac\x08W@q\xc3\x84\xc0\xfaw\xab\xcb\xa3\x03\x03\xbd\xe6\xa1\x04N\x01*9\xc9\x1f\x85C\x83\x8a\xb2w);\xea\xc5\x9d\xe0\x8eE\xf2\xa3\xf3\x02\x9c`\x9e\x0f\xb6y\xacY\xa0\x9eGfdu\xc9$(\xf4\xab\x97\x02\x08\x9b\x16\xf7\x1eld\x0c\x97\xe9\xf9\xe7\xd6\x99\x1b\xb7\x92\xce\x8e\xa5\xb7m\t\x8c\x9f\xad8\xa4\x96\xa3m\xbd\x8a1\xc3!\xdb\xbd1\x8es\xc8\xe2\xa3Yd!\x8b\xab\x05^B\xb0\xc5hN\xf2o!W\x00\x9d\xbc\x13\x91P,A]\xc9\xc0\xdd\xc9\x1e\xb4+^\xe8,\xc8\xa1\x9c\x90U@\xc1\xe0\xfd)\xf2F\x14\x86\xc6\x07c\xedS\xfd\x9d"\x85\n\xab\x85\xe7#<\xfdj\xbc\x84\x81\xb5S\xa7Pz\xd5\x89\x12\x88\xa30\x07\x04\xfd=*(\xd0\x8d\xd9l\x03\xd0\xd2#\x16`\x10p\x063Ml\xb0l\x10\x08\xe4\x90x\xa6\x17\x1ae\x0b\x1b\x03\xf7[\xa8<R\xc7t\x98]\x8b\x93\xfcY\'\xa5G2\xb4\x8807\x02B\x93Mt\r\x85C\x828\xe3\xa1\xa2\xc0\xd9>r\x1860{\x91\xedJ\x8f\xb1\xf9RA\x1d\xd7\x82*\xac\x88\xeb1m\xc4\xae;\x9f\xebO]\xec<\xc6$\x0c\x8c\x0e\xb9\xa3`\xdcvdWdE\xfb\xbc\x8d\xd8\xe3\xe9\xebB\x8c\xaa\xc6\xb88\xe4\xb7zz:\x96\xf2\xdf\xef\xe7\x04\xfbU\xf8\x93(\xee\x803\x0e\xaap?\xfdt^\xe2\xb5\x8a@3)\'\xaf\xd3\xb5 y\'TH\xf3\xbce[#\x80=\x7fJ\x18\x12\xecs\x80O<\xe3\x07\xae)\xea\x9bU\xc23\x00\xc3\x9f\xff\x00]!\x9d\n\xc7-\x95\xba\tm\xfeN\xb9\xdb\xfdjo\xec\xdb{\xf2."\x88\xc4\t\xc60:\xd6\x8d\xe6\xa6\x92\x15I\x08\n\x01\\\xe3\xa5U\xb7\xbdX\xad\x8b\xc7(?6\x0fa\\\xb7v\xb1\xa5\x96\xe5\t\xb4\xeb\xd8N\xcd\xe2A\x9ca\x94\x1d\xbfL\xf2?\n\xa6\xfam\xc7\x9d\xb5\x7f\x8b\x86\xc5kO\xaa+\xa8}\xf9`ppFG\xd7\xda\x9b\x1e\xa5o\x90\xe1\xdd\xa4_\xbd\x91\xc3}1MG\xa8\xeeW\x8b\xc2\xdfduid&3\xcf\xa5n\xff\x00\xc2?\x15\xe5\x93,jcU\x19\x0cy\xc8\xaa\x83PS\xbeI\xbet\xdb\xc0\xcf\xdc\xfc+KO\xd5\x1b\xec"5\x93p \xe7\x1c\xed\x14\x9d\xc4\xceZ}.[g6\xeb\x91\x8c\x90Uz\xd5\x7f\xb0\x90w\x86bq\xd78\xc1\xae\xa2W\x89\x8b\xbbL\xed\xc9P\x13\x83\x9fZ\xcb\x9e\xd5\xc4\nm\xdb~\xdf\xbf\xb9\x86\xe1\xef\x8a5\xe8\t\xf7((\x91\xed\xda\r\x80\xb9\x18\x0e\x0fJ\xc7\x9e6c%\xa4S\xdcA1#\xf7\x81\x7f<\x9e\xa2\xb7\nLc\x1b-\xd7 \xf3\xcf\x07\xda\x9doh\xc5\x95\xbc\xad\xacs\x92G\x06\x9cemBPOr\x81\xb2\xb5\xbb\xb26\x97\xcf\xbd\xf6\xe1.Q6\xc8\xa7\xb1-\xde\xb1!\xd1.\xe1\xbf0\xfd\xa9f\x83\xf8e\xe9\xb8};\x1a\xe9\xaf\xa5\xf2\x1a0\x8a\x0fb\x00\xedM\xb6 \x8d\xd1\xc8\xc1\x94\xe4\x87\x03\x14\xd5F\xd1>\xce+R\xb4\x16\xd6\xcb\x98\xe4\x86\xe9\x9c\x0ce%\x18\xfc\xb6\xff\x00Z\xd0k{X&H\x927m\xc8\x1b>n\x0e}\x0f^\x7f\nK[\xa8\xa1w.\x0319\xcfL\x8a\xb3\xfd\xad\x06\xf4?g\xdcG<u4\x9c\xdd\xacR\x8b(\x17\x89cU\x02Uvn\x0c\xa0\x11\xf9\x8a\xb4\xfai\xb8\xb6\n\\\x13\x8e\x00`EI\xfd\xa9h\xec\xab\xe4\x91\x82@%8\xfc\xeaXn\xad\xe1}\xeb\x1a\xba\xe7$z\xd2\xbd\xb7\x1d\x99Z\xde\xcf>_\xda,\xb2\xaaq\xd2\xb5\xa2]2\x16\xc0\x88!\xec\xaa\xbd*\xcc\xba\xa41\xc0\\@F\xf1\x85\xe3\xf4\xaa\x06Xee\x0b\x13+\x91\xe9Sk\x88\xb1ks\xa7XJ\xf2\x08\x19\xf7\x1ex\xebTN\xb7=\xb5\xc3\xbc6\xc0,\x84\x80\xa7\x90\xa3\xb5\r.\xc90\xcaU\xf2v\x909\'\xf1\xa3\xed0\x0f\xbe\xb8#\x82s\x92M>[ L\xcf\x92{\xdb\x92`\x90\x14\xdeH\x18<\x1a\xb04\x0b\x88TJ\x1cg\xa1_Z\xb1\xfd\xa9\x06\x125\x83sFr\x0fB\x05]MRY\x9c\xb0\x08\xd1c\x9d\xc7\x95\xa5vQ\x9f\x06\x85yu!0\xa1\xf9FJ\xf75\xa5m\xa3M\n\xb1\x91\x99X\x8e\xa0\xff\x00\x9cU\xab\x1dI\xe3\xc1u\xc99\x04\xd5\xcb\xab\xf8\xe5\x924f\xdaH\xe7\x00\xfe\xb4\x13vs\xefc\xe4\xb9*\xe6M\xc3i\xf6\'\xe9K\x1e\x9b\xbd\xd4\x1cg<\x86\xedZ\x82\xea\x12\xcd\xb0\'\x90xf=\xcf\xb57\xce\xb2\x81|\xc3!\x91\xc6T\x0e\x9b\x7f\xc6\x9e\xa0g-\x98\x80\x99\x19~U<\x808\xaa\x17\xda\x84-p\xc8\xb8R\xbcn\xc67V\x86\xa3\xa9\xc2\xb6\xfeZ\x90\x0b\xf2\xa7\xd6\xb9\xfb\x87i\xc8T\x03q<\xd2C"\xd4\x1aW\x9c1w\xc1\xe4v\xac\xe9"nY\x89\x00\x0c\xe7\xd7\xe9W\xe4\x12`!`\x01\\\x1e;\xd5)\\\xc5\x91\xf2\xf1[BW\xd1\t\xae\xac\x9a\xddQ!.\\\xb6;w\xab\x0b\tf\\\xc4\xc51\xd1\xbf\x87\xf2\xaaQ\xcf\x8eU\x98>8\xe2\xa6M]\xfc\xb6V\\\x13\xc6G$\xd1\xca\xc6\xd9\xa5a\xa5X\xfd\xacM$\xfb\xa1\x00\r\x8cp\xca~\x9e\x95~\xfa\xc3HXJ\x0f\x95\xd9\x8e\x08\xe3\x1c\xd7.\xb3cr\r\xe3p\xce*O\xb7;\xa1E<\xe7\x1d\x0eO\x1d)\xf2\xb1\x1a\x12iZo\xcb\x10\x9b\x12I\xce\x06:\xfdj\xe46\xb6VP\xa1I\x1d\x8b!\xc9\'\x90k\x9ef\xcb\x86\xc9\xdc\x0f\x14Ip\xf9\x03id\x07\x9ez\x1fz\\\xac\x1a.j\x10\xc5l\xd1:&C\x8c\x87#\xadQWm\xff\x001\xc8c\x80\xa7\x9d\xb9\xa9\xd2rF$\r\xd3\x90\xc74\xd7\xb8D-\xe4\x80\x17<-i\x14C%\xb5\x89]\x9e=\xc1r2{g\xf2\xedOkK[s$\xab\x1aJ\xc3\x95b\xacG\xbesUc/#n\xdc\xc3\xb9\xcf\xa7j\x15\xdc\xca\x11\xd9\xbeC\xdb\xfc:\x1a\xa6\x84[\xf3U\xe0\x13\xc2\x120x\x020\x06>\x95_l\xb2\xca\x0e\xe3\xb4\x7f\x10\xfaS\xa5\x8d\xa4\x92FH\x86\xe3\xd4/\x1f\x8dF\xeeR\xde \xacVB\xf9PG\x00z\xf5\xa9\xe5\xb3\xb9JZX\x97\xec\xd2@\xa3,\xa3\x9cw\xab1\xcf<jcUFP8%FF}\xea\xba\xb4\xb7n\x10\xbeX\x8c\x10\xdd\x7fZO5\xe2\x95L\xf04\x91\xa62\x13\xe6\xfdhh\x11\xada=\xc3\xe7\xcb\x90*\x7f\x19\xfe\x02}\r_:\xc3"\xe1\xe0\xb6X\x95@\x0c\x00c\xbb\xf1\xaefK\xb6\xd9(RV9?x\xdcc\x9fqP\xc5vd;\x87$|\xc0\x1c\xfeu\x16a\xca\x99\xa1qv\xf1\xc8J\xda\xc6<\xc6\xcf\xcc\xbc\xb7\xafN\xd4\xe8\xa52#5\xc2\x9e\x14\xecRx\xac\xf9.\x9a\x7f+{\x95 \x13\xcf8\x15\x1b\\<\xbf/\x9cv\x13\xd7\xf9~5M_V\x1a-\t\xcf\xd9\xe3M\xab\xb9\\\x9c\xe4\xf4\xcf\xd2\x851\x94g\x8c\x81(\xc6F\xea\x80<\x85L8$\x03\xdf\x04\xb1\xa6\xdd,\xb0\xc6>P7|\xf9$\x01\xef\x81T\x84\xec\x89\xa6\x9c\x85\x05\x9f98\x189\xa8\xd6]\x80u.8$\x8f\xe5Ub\x96\xe3\xc9e\\\xef8\xcb\x9fOJyY\x07(7\xe1s\xc1\xfd(\rY\xa2\x1f\x05\x9a,\x8f\x93w#$\xf6\xe7\xd6\xa2\xf3K\x99\x0b\x15\x1bW\xe65N\x15\xb9s\x18h\x9d}q\xdb\xf0\xab\x07\xccPq\xc9?y\xb1\xd4\xff\x00J/f+h1\xd8;"#\x90\xa7\xae\xec\x7f\x87JFu\x00!+\xc1\xe0\xe78\xab\x01\xa5\xda\xc3\xcbW\xc0\xe3\xd4\x0e\xf5\x14V1\xcb\x92\xdb\xe3\xe4\xe4\xad\x0eHJ,iIp~|FG,9\x06\x88T9\x1bTn^@\x1cf\x9d"\xe0\xf9a7\xa9#k\x03\xceG\xf3\xa7\xc1m\xbeE\xdd\x949\xcb\x1cr~\x949!\xa8\xbe\xa4SD\x1eL\xcc\x19\x18g\x8cv\xf6\xa4\x99\x9c\xbc)\x123 <\x1e\x99\xe6\xb5\xfe\xc8\xb27\xc9\xe66\x0f\x0eW\x03\xdcR,\x11: ~$\x04\xed\xf9rO\xa6qS\xce;v*\x18^)\xa5f\x1f>}\x00"\xa5w\n\x81"#p\\\x16=\xfd\xfd\xea{\x99\x00B\xd2\x0f\x99\xc6W\x8c{f\xb3X\x05\x1b\xe4;c\x03\x00\xed\xcehZ\x03]G\xb3\xa4|\xefVn\t\xf44\xb9X\xd9H\x07\x9eH\'#\xebP<j\xdbd\xce\xe7\xfe\x12\xfcdc\xfc\xf3Ow\x03b1\x18\x00\x0c\x11\xd3\xfcj\x90\xaeh6\xa2\xd3\x8d\xb8\xfd8?Za\xbe\x92\x06\xda\xea\xac\xbf\xc4\x80\xf1\xf5\xac\xdf3f\x129\x1c\x81\xc7\xcd\xd7\x9ajG*1\\d\xf5\x07w\x1f\x8dO"*\xf66\xae\x04s\x00\xd0\xe5\x11\xc6~V\x06\xa0\x11\\\xc3\x11UP\xac>`\xc4\xe2\xab\t\xf6\xc1\xe5\xfc\xa1\xfamS\x82\xbe\xf8\xef\xf8R\x0b\xb9\x99\x0f\x99\xf3\x83\xc2\x829\xfc\xe8p\xec\t\x96\xa4\xbd\x95\xa2Ky@\xc89\xde\x07?\x8d%\x9d\xfc\x90\xc6\xde\\\x98]\xd8\xeb\xc6+*W\x8d\x8e\t\xce\x07\xdd\xceqI\x0b\x05>X\xfd\xd8\xeaw\x03\x8f\xc3\xd6\x89Sl\x15D\x8d\xe7\xd6\x84\xb1\x00\xec\xc0\xa9\xceq\xf9\xd1\x15\xdbd\xb2\xb0`\xd9#a\xeb\xf5\xac\x1d\xea\xeeU\xa4\xc2\x81\xc6?\xa0\xa9\x03\x88e\t\xf39n\x14d\x0f\xc75.\x8a+\xda#u/\'\x91\x0f\x95\xbcs\x81\x93Q\\\xdf\xca\xea\x0c\x80n\x03\x1f)\xac\x85\x9d\xfc\xd0\x06\xe1\xd8\xe6\x9e&\xf2<\xc6\xd9\x96\xce\x0ey\x18\xfci*V\x07+\x97c\xb9\xe7s61\xfd\xf3R\xb3*\x12\xcc\n\xb62Np\x0f\xd2\xb1\x9e_1\x8bn\xe4\xf4\rM\x17D\xb7\x96A`\x83\x9e\xd4\xdd \xe7/\x99[\x7f\x006\x0ex\x15\xa3\r\xad\xc0U\x9e#\x1c\xe0\xaf\xce\xb2\x85\xd8=\xb3\x90E`\xc5t@!\x97\x03?)\xe9\x8a\xb0\x97\xcd\x18\x05\x99\xba\x9c\xe4u\xa1\xc0.\x99\xaac\xb8\xb7P<\xc81\xd4 }\xdf\x96?\xc6\xad-\xd4\xc9\x1cQ\xdc\xda\xc5*\'F\x07\x91\x9f\\W?\xf6\xff\x002\xebr6I\x1e\x9c\x8a\xbborn-\xe4S.\xd6\x1f0\xcf\x1cT$\xe3\xb0YX\xdc&i\xe1\xca\xc7\xf2\x82?\x8b\x04\x1a|\xb1I!\x8eI\xf7\x8c\x0f\xbc:\xfe\xb5\x99\x04\xaeT\xaa3\x00@\x1c\x1cf\x89nZD\xda\x1d\xc8\xe9\x82pp;U+X\x96\x9d\xcb\xf2HWiy\x89\x00\xf4\xf4\xff\x00\n#\xbaX\x89\xe4\xb2g\'<\xe3\xf3\xac\xe1$\x93\x01\x1b\xa9\x88s\xf3g\xefR\x86\x8c/\x94\xcd\xb7<\x92\xddZ\xa5\xa97\xab+DZ\x96c,\x80"\x8f1O@1\xfa\nQ;BX\xb7\x0e\xc3\xee\x81\xc1"\xaa%\xcc1\xbb\x02\x15I_\x95\xb2~cM\xbcr\x19e\x08YO\xde\xdar3\xf4\xaaP\x13e\x8b\x9b\xf9\x14\x0eI-\xce:\x11Lk\xb9\x192\x8c\xd9\'\xae9?Z\xcc\xb8f\xb8\xda\x8a\xbbz\x15\xc7\x07\xf3\xa6\x1b\xc6X%F_\x99\x08\xe4\x8eO\xe5T\xa2\xba\x13v\\k\xd9\xd4\xacCr\xed<\x80I\x1f\xfe\xaaY\xaf\x1ai\xb9\x95\xc1\x1d\x06x8\xac\xbbk\xb0\x1aD\xe1wu\x0c:\x1azd\x89\x180\xca\x0c\xe1OZ\xbeUp\xe6d\xd7\x13\xbd\xcby\xacs\xd8d\x7fJ\xa5\xf6\x89\xa4"8\x9b\x1d\x9b\x9e+F\xde2\xf1;d\x12?\xbb\xcdC\xe5,N\x08U;\x9b,\xa7\xb8\xa8\xb5\x8b\xbd\xc6\x9b\xd6\xf2c\x8bv~_\x9b\xea*\xac\x97 .\x1c\xfb\xf1\xcf\xebV\xd2\xc3\x0f\xfb\xb3\x80999<\xd3%\xb5I\t\xca\xe0\xe4\x05\xf5\xa4\x9aLm\\\xae$\xca\x9c\xb3|\xc3\xa6:\x8a\x80\xa1\x90\x8d\xcd\xc6\xde@n\riC\xa6\xdc6X\xcb\xb9I;A\xe7\x1e\xde\xd4\xc6\xd3\xe6\x88\x0f1P\xbb\xf3\x85=9\xe9T\xa4KWeEm\x9b\x9c\xb3\x10\x06\x01\xce\x06}=\xea\xcc\x00I\x1e\xf6\xf9[\xd0T%\x1b\x0c\x88>\\\xfe\xb9\xefS6\xe8\xa2\x01]r2\xc0g\xdf\xa5\r\x8d%{\x8e\x03yo\x93\x803\xf7\xb85\x04\xc4\xbeYX*\xf6\x03\xa7\xe7R,\x8c/\x19HVR2Tt\xa8f\xda#\x03\xcb`s\xd7=~\x95H\x96\xc7\x19\xf0A\x04c\xda\x87+\xfe\xafq\x04\r\xc0\xb0\xe6\x91\x04J\x00b\x03\x11\x8eM9\xd5@y\x01\xf3\x1c\x0c\x10\x01\xf9G\xbd\r\xa4N\xa3\xe3\x95\x8c\x18\xf9\x9a=\xdf\xadA3\xb2J6ng\x1c6\xd3\xda\xa3ie\x89I\x8d\xdb\x91\xb8.G?\xa5\x12\xa3_88_0\x00@\x1f.isu`\xbc\x8bqJ\xdb\xd9\xe3;J\x8c\x808\xc5X\x92Q*\xa0!\x03"\xfc\xf8\xe0b\xaf\xe9\x9e\x1e\xd4\'\x8eEh\xd9\x19\x17v\xe5\xef\xc7j\xa2\xf6\x93\xc5\xe61\x8c\xae\xe5\xc0\x01x?\x95O?B\x95\x88$E2\x13\x80\x0e8\x1dA\xcd1e\x925a\x9c)o\x99@#\xa59\xe3\xb8\xccf4gp\xa0\xf4\xe0~5\x03N\xcdp\x04\xc1\x81\x07\xef\x81\x81\x9aW\xb9WI\xd8d\x93\xa6\xec\xa3\x90\xab\xc9\x038\xc50\xdd,\x92\x86\n\x80t\x0cH\xe9\xd8R"\x89\xd9\xb6\x96\x08N\x14\x11\xd7\xebR\xc7\xa7\xcd<\xc8d\x84\xaf`\x07\x1d;\xe4\xd3m\t7}\x05\x8eQ\xf6\x82\xc1\xb0\xa5~f\xdc:zb\xac\xc5\xe5F\xb8\xcf\xcb\x9f\x94\xe3$\x1a{ZD\n\xbc\x91\xaeW \x9d\xb4\xc2\x8a\xee6rG\xe1DU\xcan\xce\xe2\xe7nC.\xe5\x03\xdb&\xa6i\x8er\xcew\x0f\xb9\x81\xca\x8e\xd5\x0b*F\xff\x00tn\xc7\x19\xe7\xebI,~b\xb1\xf3x\xdb\xf7ON+U\x13)HPT\x8c\x927\xe3\xe6\xca\xf0>\x94\xe8c\xdcJ\x8c\x10EF\x19|\xb6 t\x00c\xa0\xfa\n\x92\x02X\x91\xb4\x12\xa3?J9C\x98H\xbc\xc8\x89\\\x82\xa4\xf1\xedOI\xb76\x18e\xbd3\xc1\xfe\x94\xcd\xad4\xd1\xf9m\xbb\'\'\x07\x00TRG\xbd\xd9U\x01\x00\xe7nq\x9f|\xd2\xe5\x0er\xdc\x92\xc7\x0b|\xd9$)+\x96\xe8*\x01q&\xe3\x82B1\xc8\xefQ4\xc9<\xc38\x18\x03iU\xeb\x81\xfc\xa9\xc2A\x95u\x08A\xe0\x8cv\xfe\x95-[b\xe2\xee\xb5\'\xe0D[\xa8\'8\x00\xf3\x8fj\x11\xaf<\xb7(\x8c"l\x1e2\x01\xa0f9s\xbf\x1bx!\xbd}3\xf4\xa4v(\xc7we\xce\x07\'\x9a,\x1c\xe5\x98\xaf%\x8c2H\xee\x08^0y\xcdD\xb7\x82Y\x01\xc8\xdd\xd4v\xfeUNY\xb6\xed\nO<\x9ey\xcdG\x1a\x07\x93nB\xbb\x00\xa1\x8fB}\xe8PB\x95CO\xcc\x1c\x96\xe4!\xc3)9\x18\xfc:\xd3\x07\xdc\x07\xcb\\J\xa7\x1c\xe3\x8fj\x81P\xaa\x94=2<\xc6\xdeG\xe5K4\xc5\x12%|\xb2\xa8!\x06{SH\x97&$\x9f<\x81\xb6\xe4\x01\x80\x07c\xe9SD\xa8\xc8|\xdc*\x91\xf2\x9c\xf1\x9f\xa5Ve\x93,\x13h\x1cg\xd4\xf1K\x03\xc8\n\xac\xaa\xbe\xc7\x1d*\x92%\xbb\x8cX\xc2\xca\xc7\x82\xa0\xe7\rR\xeci\x1d\x83 \xc9\xe8Oj\xabsp\x91\xc9\x85@\x84\x9c\x9e\rW7eI\xda\xad\x9e\xa1\xc7 \xfe\x15\x1a\x9a\xfb\xa6\x9b2\x88\xfetS\xf2\xe3\x1d\x08\xa8\x1fyt*3\x1fu\x07\xadU\x8e\xf31\x1f\xde)~z\x8e\x9f\x8d,\x13\x18\xf0\xc3\xcb!\xbf\x87=\x7f\xc2\x8fP\xd3\xa1a\x94|\xa5c\x001\xf4\xc3\x0f\xa59L\xb2\xbb\xeeC#\x80B\x0f,~t\xc5uK\x97R\x98l\x829\xad\xcbX\xff\x00s\xcc\xa8\tp\xc1A\xc1$\xfa\x9a\x1c\xecG"\xdc\xc1e\x1b\xdfh\xc98\xe5\xc1l{\x0fO\xca\xaaM\x13\xc5 \x8c3\x14\xea\x18\x9c\x0f\xff\x00]t0XH&1\xce#B\xcc\xcc>n\xbf\x89\xab)\xa1\xc6\xb6\xf2\xee\x03v1\x82\xc3\x9f|\xd3\xf6\x84\xf2\x1c\xc0\x93nH\xf9\x8f\xe7\x9a\x7f\x9a\xfeh\x01\xb2\xac3\x82j\xe7\xfc#\xd7h\x16C\xbc\x82\xc4\x0eA\xc8\xfc*\xe5\xa6\x84\xa6WV\xf2\xd4\x96\xda\x17\x92A\xf7\xf4\x1e\xf9\xaarH,\xd9\x8abe\xdc\xc5d*\xad\x82q\xc7\xe3R"\xf9\xca\xae\x14\x808\x07\xd7\xeb]\x84zls\xd8\xc5\x10;Ke\x19\x8c\\q\xdf9\xe6\x9d\xff\x00\x08\xdd\xa4P\xb1\x12\xee\'\xd4`7\xd3\x1d>\x95\x9b\xa8R\x89\xc8\xf9q\xab\x94.2\xc4q\xd7\x07\xebD\xeb3\xc66\xcaJ\xe0\x923\xc1\x02\xba\xb4\xd2\xd5\xd3\xcdX\xe1@\x17`Y\x0e\t\xf7\xe9U\xa1\xd1\xd1\xd44\xcd\n"\x93\xf2\xb3\xed\xcf\xe4)*\x97\xd1\x0f\x91\xf79\xc83\xe5\x86\x89N\xee\x8d\x8e\xfe\x94\xd8\xfc\xe8\x10\'\x97\xeay\xf7\xae\xa3\xfb2\xda\xd9ZC\x08U*6\xe3\xe6_\xcc\xf3V\x85\xc5\xbbB\x12h<\xce@V\xdb\x92\x05K\x95\x8a\xb1\x83lnB\xab2\x8e\x9c\xf2q\x9fJ\xb3\x17\x9ao\xf0\xeb\xdb\x1fw\x8a\xdd\xba\xf2Z\xd8[<c\xe5\xe4\x15\x15I\xccv\xa9\xbd\x1dD\x84\x1d\xaa\xc4\x9e=qB\x90\xf7\x1f-\x80\xf2V\xe6y\xc2DG\xca}\x0f\xbf\xa5e\xdcI\x06\xdd\xaa\xdb\x9cd\x03\xda\x9bw\xaa\xc91Q\x96\xf2\xc0\xc1A\xcf5\x9d\xfb\xb7\xba\xf36\x00pAQ\xce\x0f\xd2\x84\xbb\x89\x8a\xf2HN\xdf3\xe5\xce8\xe4g\xebH\xb1\xban\x11\xceJ\xf7_\x7f\xf1\xa5\x9234*\xf1/\x94\xaa6\x90\x01$\xfa\xe6\xa7\xd2\xbc\xbf8\xac\x91\xb2\xc4\x13qt8\xc0\xed\xc69\xabM\x12\xeeF\xd7Mk\x1a\xa4\xea\x03pX\x9e\xa3?\xd6\x9b$\xb6\xc4\x10r<\xc1\xb81\x1br\rNms\x14\x9e{\x07,\x0b\x06\xff\x00\xebTq\xdb\xa4\x91\xe1\x98 E\x01Wfzs\xdf\xb5RI\x11v\xd9D\xa4JX\x88\xd1#V<g\'\xf1\xabPA\xb80`v)\x00\x1cp3\xfe}j\xe4\x11\xc6\xe5\xc9\xb7wpGS\xb4\x03\xe9\x829\xa91\x18,\xceQF\xcd\xbbCu52\x9d\x8d\x14E\xb6\x80\xc7+\xc4\x81\x8e\x10\x91\xb7\x8c\x0fZ\xa5yp$\x97beYT\x06\xce0s\xde\xb44\xad\xbed\xca\'Kun6\xc8\xbb\xb8\xfc\xf2)\xb2i\xce\xb3\xed\x86X\xe4u\xe0\x80\xf9\r\xf8RS\x07\x16Eh\xab"3\xce\xf2FW\x01p\x03g\xd0\x0eA\xfciH\xc4FV\x84\xbc\x00\xe1[\x00\x92\x7f\xdd\xcfJ.\xde{\x0by\x1e[,\xb9a\xfb\xc9\x1c\x828\xe9\x8e\x95F\xd2\xe4^Z%\xabM\x1eWsrJ\x8f\xa0\xcfZ\xb6\x93#\x99\xad\r5\xbc\x9br\x95e]\xbdC\xe01\xf4\xf6\xa4\xbb\xb8\x93\xcb&v\x11\xb8\xe0.2\xdf\x87\xa5S\x8a\xe8\x082Ll\x8b\xc6e\x1c\xfe$s\xf8T\xec\xc3r@\xb6\xbek\x901\xb1\t\'\xdf\xd7\xde\xb2i\xadR4M35\xa6\r\x16\xe8\xd3?6\x0e\x0fC\xebS\xc1\xa7]]\xc9\xbe5m\x84\x95\xe4{f\xb7\xf4\xfd\x12\'e\x9aB\x15N[s\x0c\x13\xf8\x9a\xef\xb4\xbd\'O\x8a\xc9UA\xdaFAaQ\xed\x01\xabu<\xe1t\x1b\xa8t\xd6\xbaufm\x9b\xc1U\xceF:b\xae\xf8_\xc1W\xde$\x91\x81t\x8dV?4\x87\xc8\xc1\xec:Ww$\xd0Z0O\x94\xc0\xa3i\x19\xe9[\xbe\x18\x9e\x0bx%\xb8E\x01n$\xda\xbf\xee\xaf\x7f\xcc\x9a\xbaR\xbc\xb5&m\xdbC\xc6\xae<!}\x1d\xe5\xca\xc8\xa69m\x14\xa3\xa7\xa3\xf6\x19\xfa\x10A\xa8\xac\xfc\x1f\xa9]eT\x90\xbbC78\xef\xd0\xd7\xa6\xf8\xba\xf5t\xbf\x17\xc5t\xff\x00\xf1\xeb}h\x06G\x18\x9a6?\xcd[\xff\x00\x1d\xaacU\x8bQ\x89^91\xbb\xe5\xdf\xbb\xaf\xb5UI\xf2\xb6\x911WW9(\xbe\x1f^\xaa)\xfbJ\xf9d\xf1\xfe\x1cWO\xa5\xf8\x1e\xca\x0b\x9bk)\x19\x9aY2\xf3J:\xaa.2\x07\xd4\x90?\x1a\xb5\xa2\xd9\xcf\r\xcc\xb72\xc8\xbeY\xe5\x139\xe7\xd7\x15l_\x83\xac^\xcd\x11,-\xed\x922\xfc\xfd\xf7bH\xfa\xe0\n\x8am\xcaZ\x8eZ-\x0b7\xfaE\x8e\x8fbn\xac\x0b$\x01\x84w\x08\xce[\x198V\xe7\xa78\xcf\xb5P\x8fH\x82xY\x9dN>\xf3\x028\xe3\xd2\xa2\xd0\xf5\xc8\xef\xf5\x0b\x8d2r\x1e9c"@z\x15<\x7f\x9f\xa5[\xb4\x82\xe2\x19\xa4\xb5yO\x97\x03y`\xb7\xf1\x0e\xaaO\xe1UU_T(\xddhg\xcbo\xa5\xc6e\x82+t\x1b\xb8\xdc\x98\xc8\xf7\xac9\xfc)\x05\xd7\x9br\xb2\xbe\xe4\xe4m\xef\x8e\x83\xd2\xbb\x04\x8e\xc2\xc5\xdb\xcb\x89\xa7\xb8v\xe7\x8e\xe6\xa6\xba\x96\xdd\xa3)#$>b\x90y\xc7j\xc6\xe8\xb3\x80\x8e\xfb\xc3\xa2\xc1\xe2\xf2J^\xafE\xdb\x82y\xe9\xef\x8a\xc1\xb9\x92C\x96R\t=\x87Z\x97Z\xb3\x82\x0b\xd9g\x86\xe0\x0f\x9bi \xf0\xd8\xe8H\xa7\xdbi.\xf6)p[1I\xdb\xa9\'\x1dW\xd3\xf3\xaa\x8b\xb6\xe5\xea\x91\x9a\xe6K\x89I8\x0c\xa3\'\x8f\xe9P\xbc8a \x18V\xeb\xc9<\xd7O\r\xbc(\xe8\x87p\xc8\xf9\xb7\n\xbb\xfd\x9ff\xf6\x92\x891\xe6\x8c\x81\xc7\x07=\rh\xaa\x10\xd1\xc5\x87.\xa2&RUNU\x8fS\xf5\xa6\x96`@`\x01\xeaW\x1c\x8f\xcb\xa5j\xdeii\x1c\xb3I\xe6\x8f0c\x00\xf0sY\xf7Q\xa5\xbc\xca\xf2\x16\xdcP\x10\x03w\xad\x94\x932i\xa2\xa0\x8f.\xaa\x88X\x8f\xbe3\xc8\xcf\x7f\xc2\xadA\x15\xcf\x96\xe5\xe3`\xa0d\xb7Lz\x1c\xfeuP\xb6\xe9\xb7\xa0\xc9=W\x91\x93\xfe{U\xd6l\xb2\xb4\xa5\xe3F \x80\x8c\xdc0\xaa\xe6HVd\r\xe4\xb4\x8a\x1aI\x11\xdb\xd7\x07\xe9\xefP\xb4\xcf,\xf1\xc3\xb9Lk\x9c\x1c\x81\x8e\xe7\x9fz\xd46\xf0\xdcJ\xb1\xe5p\x06T\xa2m \xfa\xf5\xa1-c\xb7\x8e@\xd2.\x01\xf9\x8e\xder;\x9a\x875\xd0\xa5\x07\xd4\xa3\x14B\xe1\xdd\x96\x13\xbds\xb7?\xd6\x85\xb66\xe2C*\xe0\x91\xc3\xe79\xc7A\xf5\xad5]\xd1\x86EP\xa4\xfc\xb9\x00\xf1\xeb\xedM@\r\xbc\x9eb\xb6\xed\xdf(9!\x8df\xe6\xcd\x14{\x14\x108?1\xda\x1f\xf8O\x07\xeajx\xa2\x92u\xf2\x17,\xaa\xa5\xc39\xea{\x8f\xa5D\xf2\x9f9\xd9\x90\xa8\xc61\xe9S\xa5\xf2\x040D\x87\x85\xf9\x99\x94d\x1f\xa8\xea)\xb9\xb5\xb0r\xa1\xadl\xe2 6\x82\xe3;\xbe\x87\xbbv\xe2\xa1\x16\xbf\xe8\xbb\x0e\xd2A\xca\x93\xdf\xf1\xa5Ydh\xcc\x80\xf1\xc9\x91\x0fP3\xc6G\x15d\xdeC\x14\x1c\xa8\x91\x18e#|\x8c\x1fS\xed\xedG3\xec+$Fc0B\x1a\xe1\x8e\xe6\x1b\x94\x05\xe4/jE\xdc\x0cD`\x05M\xc0\x90\x0f\x1e\xb5BE\xba\xbb\xb9\xd8\x92\x88\xd5\x9b>{\x8e\xe7\xb7\xd2\xa70\x04UI<\xbd\xc8y!\xb2\x18\xd1}A+\x93\x98!\x04\xb3;,\x9d9\xe4`z\n\x840T\n\x8f\xb9\x86w\x03\xd3\x1e\xd4\xe4\x8eY\xa3eS\xf2\xfd~\xef\xd3\x14\xdd\xa2<\x1d\xacX\x83\x91\xbb\xa9\xf4\xfeU\xa2d\xd8\xcf\x99\x95\xeer\xd1\xb6\xd4\xc9\x1bGZ\xca\x92R\xf7*\x8c\x92\xc6\xa4\xff\x00\x0e:V\xd8\xf3\x08-\xe6|\xa4\xfc\xc4\x9cqD\x90/\x99\xb9\xa1\x01pDo\xbb<\xd4\xa7`q}\n\xc5\x18\xdcD\x19\xcf\x1dG\x1c\x83\xfaR\xcd\x19F\xfb\xca\x10\xe0\x1d\x9c\xd4\xf0\xdb\xba\xb2nel\x9c\xee\x075\xa5\x1d\xb3A;l!\x90\xfc\xcc\xad\x9eW\xf0\xe9I\xb4RL\xc2i\x1e)\x83L\x19\xd5N\x06O8\xfa\xd6\xa3\\I!\x8b\x00\xaa\xed\xdc\x00\x1d\x0f\xbdi\x8b\x0by"(\x91\xa3\xbb\xe1\x97-\x96\x06\xa7[;+[h\xe4\xb9i2\x9c\x18\xd7\x93\x8f\xafj\x87$\xc6\x95\xb7+#\xbe\xe0\xeeda\xd8\x1c\x90)\xedy>\xcc\x88\x95\x99z|\xc0d})N\xabg\x15\xac\x91\x88\xde<\xff\x00\xab*\xd8#\xeb\xebM\xd3l\xd2\xeah\xefnn<\x88\xe2m\xca\xeaWp#\x91\x90G5\x16E9\x16M\xfd\xc4@\x89\xeddA\x9c~\xf0\x10\xb9\xf4\xcfL\xd5\xbc\xdf\xdfE\x1a\xdb\x14ps\xf2}\xa1P\x8c\xfb\x12?Z]k\xc4"\xe6KX\x84\xd2\xcfo\x0e\xd7h\x9f\x017\x8e\xbc\x0e\xd5\xce\xcb;O\xa9\xdcL\x128\xd1\x89d\x8e<\x0c\x0fN)\xa4\x81s>\x86\xd5\xfd\xd5\xe5\x8c1\xc4\xd6\xf1\xf2v\x87P\x1d\x83w\r\x8c\x9a\x92/\x11[\xac(\x8f\x19i\x97\xe6h\xd8~\xb9\xe9\x8f\xc2\xb9Dk\x89o\x9aI\xe6w%\xb6\x9c\xb7j\xb8\xd6[\xa2\x90\xb3\x08\xca\xa8T\x91T\x92?\x01Z.T\xed\xd0\x8bJ\xc7V5{\x0b\xb8\xf7[\xde\xdb\x82\xe0\x81\x1c\xa0\x82H\xec\x0fJ\xa0\xba\x88Ux\xd3x\x8d\xb8r\x800?\x9dr\xb6ZV&\xd97\xcc\x8d\xc9\xdc\x0e\x0f\xe1]e\x86\x9do#\x80\x9b"\n\x01Q\x18\x03>\xbfZucN\xc9\xc7p\x8f>\xaaC\xde\xcd&H\xda;\x87E\xe8\xc8\x0f?P*\x14\xb8hg\x91,\xef$"N\x18)\xc6\x7f\x0ek\xa3\x83M\xb5kX\x9d\xf6)`\xc5\x82\xb6v\x8c\xf0j\xfb\xc7\x1f\x97\xf65\x8a3n\xc8@@\xbc\xaf\x1d\x9b\xfcEs\xeb}\xc7\xcdn\x87+\xa7\xad\xf4\xb1\xab\xc0\x93\xb8\'\x0c\xcc\xbefG\xb0=\xea\xf6\x99\xa2Y\xde\xddM\x15\xe5\xc4\xf1\xc88\xf2\xc2\xed)\xff\x00}\x0e\x95\xa1aav\xba\x84r\x1d\xff\x00g\x8e2\x98\x90\xee\xfc\x8e*\xbe\xbe&\xd3\xf1>\x9a\xf2,\xa1Ke\\\x96\xf7\xe9\xc61\xedU\xa3\xd2\xe2\xe6}\x10\xddO\xc3:5\xbd\xb3Oh\xf722\xff\x00\xac\xdd2\x9e;q\x8ek\x11!\xd1\x86\xe0\x16\xef(\xd9\x128^\xfe\xc3\xb5g\x1b\xbb\xb9TO$\xa7\xe78t+\x8f\xc4zS\xd9\x95 \x90\x05m\xd8\xc6\xec\xf4\xa1\xa7{"\xa3{j,\xd6\x828\x93\xc9\xb8|;\x11\x82p\xa6\x9bb\xb2\x88\xe4\xdew\x078>\x84\x0e\xc0\xd4pL\x02;2\xb6\xd09\xeeA\xa7\xc7pKe\x1fp>\x9cb\xad&\x90\xde\xa2\xcc\xec\x80*($\xe4\x02@\x07\x15]n\xf7\xc2\x02FU\xd8\x9d\xd8n\xbfZ\x9a;\x85\x91\xd3*\x8e\x1b \x06\xf5\xf5\xa6\x1b`\xd2\x81\x08?+p\xd8\xc9\xa6\xb9\x9e\xe8\x8d\x16\xc5src\xcb\xb3\x15\xe9\x95\x19\x19\xfc\xe8\x17\xb9i\x0b\x07X\x91w`\x8e\xb4\xc9"R\xdf1;G\x07#9?\xe1Ko\r\xb6$b\xa4\xc9\x80\xa5A\xe3\xda\xa5\xc5\xbd\nR\xb1\x14\xba\x98\x9e0\xc4\xe1I\xc6\t\xc5l\xa5\xee\x9d%\x9ed\xb4_\xb5\xec\xda\'\x00\x8e\x9d\x07\x18\x19\xfa\xd7+u\xf6f\x9f-(\x89#l\xaa\xb0\xe0\xfeU\x14w\xff\x00\xe9j\xe2W\xf2\xd0\x15UQ\x93\xbb\xb7z\xb8\xd3M\xd8R\x9e\x9a\x9d\xae\x9f\x0c\xd3\xc4#\xb9\x84\xcb\x0b)\x1f7\xccT\xf65\x15\xe7\x82\'\x8d\xfc\xe8\xee!\x00\x03\x91\x13o ~\x1f\xe3Tl<H\xcb\x1f\x92t\xe6p\xcaI\x7f0\xa8\x07<\x1c\x0e\xb5n]U\xeem\xa1V\xde\x1c\x0e\x00B\xa3\xaf\x00\xf6\xa1\xde*\xd7\'w\xb1\x0c\x9e\x1c\xd4!\xb57!VT\n\x14\x93\x19!\x88\xfc*\x0bYob\x99\xe6a\x12\xcb\x1f\x11\xa9\r\x91\xf4\x03\x83Zvz\xcd\xe9\xcc\xcc\xcc\xa5\x0f?\xbc\xeb\xd8\x8c\x7f\x85Io\xad"\x97yb\x0f\xb5\xb7\x0e6\xe4\x8e9\xf7\xaaS\xf7v\x0eK\xb3N\xde{\xd9lwK\x1a\xb0\xe8\xa4\x82\x00\xfa\xe3\xa5tB\xea\xee0%`UJt?\xce\xb9\x94\xf1N,\xe6S\x17-!a\xbd6\xa6v\xfbf\xb64\xdf\x15\xda\\\xc5\x1b\xdf\x18\xc0f\nUA\xda\xbf\xd6\xb9\xa6\x9d\x8bHls\\H\xc63\x03\x90\xf9\'q\xcdu\x93\x0f\xb2\xf8\x7fO\xf2\xfeC\xe5\xef\x03\xd3$\x9a\xa9\x15\xcd\xbb\x81\xe5^Y\x90\xc4\x91\x86#\xf0\x1cV\xb7\x89\xac%_\x0fZ\xa8d\xf3aA\x9c~\xbf\x85\\"\xd2m\x92\xda\xba2\xbcAf\x9a\xd6\x85\xe5\\L-\xe6LK\x0c\xd8\xce\xc6\x07<\x7f/\xa1\xaf*\xbe\xf1U\xbd\x9c\x8d\x00\x9eF\xba\'\x05"\x03\x19\x1e\x9ct\xafB\x9a\xe6\x03\xa5\xac\xda\xac\x8f\x15\xa2\'\x96\xca\x0e\x0c\x87\xda\xb8\x01\xe1\xa1\xaf\xf8\x8d\x86\x83\xa7\xc7\x1c\xb2\xaf\x02G\xc9\x0b\xc7<\xf4\xad\xe1\x055\xef\x19JN/@O\x14k\x93\xdbH\xeb{\x15\xaaF\xb9(#\x0c\xe4}}k\xb8\xf0\xae\xaf\xa6\xddx}\xc4\xf7\xben\xa3v|\xdb\x80F\xd3\x9cm\x00}\x05p\x1a|rYN\xf9X\xdd\xe1s\xf27!\x888\xe6\xb7\xafu]\x1e\xfa\xd8\xcb\xe4\x0b;\x94\x1f)\x8c\xe3\x07\xd7\xff\x00\xadW(t\x8a%K\xb8\xcb+\xfb=3\xe2L1\xc0KF1\x1b\x83\xdc\x9e?\x95w\xfe(\xd6\xe1\xb0\xd7\xd0\x80\xaf\x04\xd1\x05b\x06p\xd9!s\xf5\xaf$\xd2\x11\x1b\xc6\x96:\x85\xf1H-Q\xc3\x16\xeb\xe6`\xe7?\xca\xbd;\xc7\xf7:kx\x12v\xb0[|9\xe1\xd7\x05\xd8\xfa\xe7\xfa\xd4\xa8sK\x95\xf5*R\xb2L\xb9!\x8a\xea\xd5^\xdaP\xae\x17\xf8[\x80{\xfe\xb5\x91%\xa2}\xa9mn]\xe5v<\xbez\x11\xcdq\xbe\x12\xf1\x15\xbd\x9d\xacq4\xdf\xbc\x1c|\xe4\xfa\xe75\xdc\x9db;\xb6B\xbbI\x03\xe5n0O\xd6\xb9\xaaRpz\x9aBj[\x197^\x1bk\xab\xff\x000F\xbeH\x1cl\x1d\xba~u\xa6\xf2[Z\xdb\xc5e$J\x14\r\xa3p\xebS\x86\xbe\x96f\xfb3*FX\x01\x8e{rj6\xd0\xefL\xd14\xae\x97 e\xb3\xb3\x02\xb3\xd4\xb6\xee`_\xbcAX%\xbb\x8c\xbeKDy\xfdO\xe8k\x9f\xb7\x96i.d1\xc8\xd2*\xaedW\xf9\x08\x1e\xdd\x8dv\xfa\x96\x84e\xccl\xaf\xb4\xae\xe0\x8c\xbd?\x1a\xc6\xbf\xd0\x96\xc7\x17\x0eBF\xcb\xc1Q\x8c\xfe\x1d\xa9\xeb\xd0\x0c\x0b\xc5\x91\xda)\x83\x90\xc4u d\x0fq\xd2\xb3\r\xa5\xcc\xe7\xee\x90\xec\xd9V<\x0fz\xe8\xee,w\xef\t\x0c\x93/\xf7\xbab\x95\xa2\xb8tH!\x8aNq\xf2\xba\xe7i\xf5\x1d\xc5\\]\x90\x99\x9f\xa6\xe8\x8f\x1b1b\x01\xe8\x1d[\xf5\xab\x82\xda\x16\xb6`\xd2\xbf\x9a\x9c\xe3w_\xf0\xad\xb4\xf0\xbd\xf8\x9b\xcbYr\x8c\x0br0:qQ\\x]\xa5\x81\xd1\xa4X\xe6\r\x82\xcd\xdc~\x14\xf9\xb5\xb8\xacs\x13\x94\x8e\x19\x9c\x16\x8aE!W\xb8n\xfdzVl\x97E\x82\xc20[nH\xc6r\x7f\xcf\xa5u\xf7~\x11\x85\xc4p\x8b\x9f\xde\xb1\x19\x1c\x91\\\xf6\xab\xa2Kez\xdb\xb6\xc9\x1a\x9f\x95\xa29o\xa1\x1e\xb5JI\x8e\xdd\x0c\xd9\xaf\'\x80!*Cc\x95#\x18\xaa\xe7R\xba*c]\xab\x9ew\x05\xe4\x9e\xe35e\xf4\xf9\xc6\xd7\xdf\xf2\xb9\xe51\xd3\xe8=i&\xd3\'a\xbdN\xf5^\x8e;\xfbUsDM2\xa0\x96I%&@\xc5{\x92z\x1a\xb4>E\xc6~a\xfd\xd6\xf9\xb1\xf4\xad\x9d3F7\xf0\xaaG\x1c\x9f(\x0e\xcf\x8eGl\x1c\x0f\xadL4\x95\xd2\xa7\xfbDQ\xae\xd6pX0\xc8\x07\xbf\xe7\x9a\x9eh\xb0\xb3\xd9\x18\xa2\xda\xea_\xdeF\x93E\x18\xe4\x907dw\xe3\xafL\xd5\xf3g\x05\xdcBhnL\x8a\x83(\xbbs\xd7\xbeA\xc5l\xc7\xa90\x05c\x851\xbb\x90[\x81\xf4\xff\x00\xeb\xd6\xc3[\xc1\x12G/\xcb4r&G\x96\x9d\x0f\xb9\xcd\'Qv\x1a\x83[\x9cCD\x82`\x8e\xd9N\xa5\x82\xf4\xfc\xfd)\x152\xaf*\x95\n\x0e\xd6u\x18\xdc3\xe9]\x12i\xf6\xf3\xde\x07\x91LQ\x1c\xed\xda\xb8\xc9\xeey\xa8\xaeM\xad\xac\x93G\x18B\x8e\xf8P8\xedR\xa5r\x9b\xe8V\xb2\x8e\xde)\x8b\xc6\x0e\xd6R\x06\xdf\xe1\xef\x83V\r\xa4w\xa1N\xe0\x17\xb0U\x00\x8e\xbd\xbb\xf3U\xee\xaf\xed\xb6\xc1n\xf0\xf9l\xa3;\xa2=~\xb5^\xd6l\t\x00\x94\t\x18\xe1\x0e}9\xc6i\xa4\xf7%\x98%\xa4Hb\x95NXp\x1c\x0cd\xfe5b\x0b\xc5\xb8}\xb2G\x1cA\xfe\xec\x8e\x84\x8c\xff\x00\xc0q\xff\x00\xd6\xad\x88`\xb0\xb9\x85\xc1\x88>[\x01\x96O\x94v\xef\xd6\x88\xf4G\x81\x9e\xddoO\x92\x87\xe5]\xbb\x88\xfc\xb1]4\xd2\xbe\xac\xcaR}\x0c\xeb\xadR\xda\x1bW\xfd\xd0\x9aEb\x1aF\xc0\xce{\xed\x07\x8c~F\xb3WZiH\xdc\xcc\x838!q\xc8\xa9f\xd2-5\t[a\x96\x19\x02\x90\xc0}\xd2Gr\xbdG\xe7V\xad\xf4\xfd.;Cn`h\xe7\xcf\xc93\xe5\x84\x9d\xb0\xbc\x8csZN\x83\xb79*\xae\xbc\xa5\x8bmJ[\x95X\xe1(\x8c\xa9\x8d\xa7\x03>\xf5:\xd9]\\\x07yeF\xef\x8c\x86-\xed\x9a\xd7\xf0\xe6\x83wi)y\x92\xd1\xc4\x88\x00\xf3b\x0c\xcb\xf49\xae\xa1t;l\x02\xf1\xc7\x92pU\x16\xb9$\xe0\xb66W{\x9e]-\xb5\xcd\xb6\x03\xc4W\r\xb8#\xe0\x9a\xb0\xc2\xf2\xed\x16Ir\n0\xe7\xb0\xe3\xa6+\xbe\xd4t\xe9@\xf2\xfc\xa5\xf2\xc9\n\xa1\xc8\x03\xf2\xaaWzR\xdbAo\x0c\xb7\x10\xc5+\x92\xd89\xfe\x95.Zh\x86\xac\x8eIm\x1eD,L\x9f?\x19\xeb\xfaV]\xcc\xcb\x0b\xe5\xc3\x10\xbf(\n0A\xae\xd9t\x1dFx\x0c\x96\xd6\xd3<Y$K\x1a\xef\x1e\xfc\x8c\xd75ykp%h\x96\x06\xf3\x00 \x82\x87#\xf3\xa4\xb9\x93\xf7\x91M\xdffc\xc3z\xb8b\xccZP>@\x063Wb\xbf\x95\xd7,J\xb7\x19\xc2\xf2?\n[_\x0b\xdf^\xcc\x0c\xc8c\xcf\xcd\xb4\x0c\x16_\xf1\xae\x8a\x0f\x0b"\x86Y>i\xfe\xe8\x8c\xe4pz\x11W\'\x1e\x82\x8b}Lx5\x03u\x1e\xcf-IC\x81 \x07\xad]G\xbb\xdb\x108\xdf\x8f\x93=\x0f\xb7\xd6\xb5b\x8e\xc7N\x9e8\xfe\xc8\x88\xdb\xb0\xc4\xe7\x04\xf7\xe6\x8dJx\x99\xd9-\xe3M\x8c2\xa1\xb9^;\x8e\xe0\xd4\xdd67q\xf6W\x12\x19\x10]&W\x87o\x9c\xae1\xdb\xde\xb6l/\xa1\x85\xf7G\x18-.C\x16pv\xe3\xd8\xfbW#\x05\xe8\x102H70l\x02\xbc\x91\xec{\x91W\xef\xf58\x9d"U@\xf2\x1cd\xe3\x95\xc0\xeb\xecj\x9a\xd2\xd6#[\xdc\xed.5\x850[\x98\xccxYFB\xaf\xe9\\\xae\xab\xa8\xca\xd2J\x03\xe1\x8f!q\x83\x9e\xf8\xf6\xae{\xfbBh\x88Vva\x9c\xb7\xccp\x0faD\xb7\xa1\xd9d,\x0e\xef\x98\x9c\xe7\x06\x92\x85\x98\xef\xa0\x99\x02BH\'+\x93\x91\x92>\x95\x11\x8dc\x04\xeea\xd0\x05\x1dy\xebL\x8ap\xf2\x01\xf7\x97$\xa8<R\xcb<a\xb0c\x19\'\xee\xed\xc0\x06\xb4h\x13\'\x89\xa20I\xb9\x08\x950C\xe3 \xfdj\xab\xdc\x00\xbe[\xfc\xa4\x9c\xb1\xfe^\xe2\x9e\xd3\x96\x8dF\xed\xb8\x0c\xc5FH\xfa\xd4\x13:\xba\x10\xc9\xbcm\xc8=\x1b\xdf\xbf?J~\xa2b\xda"\xa5\xd4j\n0l\x82H\xe3>\xf8\xab\xc9.\xe3\xe54\x8afc\xb4\x81\xc7\xff\x00\xae\xa9Am\x0c\x84,\x93<x\xc19R\x0f\xd3\x1f\xe3S<O<\xef"\xc8\\D>a \x0c\xca=A\x00qU\xcc\xb63\xb3+\xa2\xa6@.p\xdfy\x89\xe9\xf8\xd4\xd3"@\xa1\xe5u99\xda\x0e\x01\x1d\xbaUP\xf0\xca"\'he\xf9\xca8\xed\xe9\xd3\x93Z\x10\xd9\xac\xb0\xcb4\xc2I<\xb202>_o\xfe\xbdK)\x19\xf7\x16\x96\x976\xee\x8f\x1b\xbb\x9f\xba\xc8\xc3\x03\xea8?\xa8\xaavV\xab\x89^H\xad\xd1S\xa4\x99 \xfe\xa6\xb4\xee#\x90\xe7\xca\xb6Tn\xab\x8e\xa4Ti\xf6\x87\xf2\xa3!\x86\xfe\x0ces\x9f\xc2\xa7b\xd4z\x92\x04\x8d\x146~U9\x047A\xfdj\xd3\xcc\x971\xc6\xcc2\xf1\xe3x\x0b\xd5}i-<9wt\xf3\x7f\xa4\x18U\x1b\x07z`t\xe8\x14\xe2\xb4\xec\xec\xe0\xd2\xe56\xf7N\xdepB\xbf:\xed\xfa\x91\x9f\\\x8e3Q\'\xe7\xa9W\xecf\\i\xd3\x89Js\x80\xd9r\x0fL\xff\x00J\x96\xfbK\x95\x12)\xb9\xdcxU\'\x96__Z\xd2y$1\xa32\xe2B<\xb3\xb8g\xe5\xed\x91\xd2\x9fg\x1d\x91\xb5ifH\xc3\xc2\xbb\x95\xa4\xc6[=\xd7\xd3\xd2\x857a4\x8er\xe7\x10D\xae\x8a\xc1P\x10pz{\xd56\xd4\x9f1\x9d\xd9\x8c\x0f\xb88\xc0\xfe\xb5\xaa\xd6\xff\x00i[\x96yK+9\x08\x83\xa0\x07\xa74\xff\x00\n\xe8/%\xd35\xcd\xc24Lr\x10\xaeH\xf4\xc1\xa6\xdaH\x97sw\xc3\xd0\xc1w\x86\x8e&P\xc4n\xf3[p#\xd4c\x15\xd3k\x9e&\xbd\xbf\xd6\xa4\xb4\x01#\xb2\xb2@g\x94\x11\x83\xc7Bz\x9c\xfbS\xed$\xd3\xf4\xdbw\x8c\x00\x0chC.:\x8flw\xaf<\xd6\xa1\xd4c\xbf\x97\xec\xf0Is\r\xc1S\x84\xcb\x10\t\xe0\x1cS\xa0\xd4\xae\x99\x15/\xb9\xdc\xe9\x9au\xd7\x8eY\xa4\\-\xac\x07\x11\xa3}\xc1[^\x10\xd0\x1fF\xf1\xde\xa7\x0b\xb7\x16\xf6\x8b\xb4\xf6p\xc79\x1fJ\x87\xc3\xfe%\xd2\xfc#\xe5h\xcfm8\x90\x98\xda\xf6\xed\xf0\xaa%u\xce\xd5S\xf3\x10\x06\x06@\xc0\xe3\xd6\xba\xeb\xf8\xd3\xed\x1a\x8e\xad\x0c\x8a\nX\x18wg\x82O\xcc?/\xeb])\xa6d\xd3G\x9eh>\x06\xbc\xd4\xb4\x0b\x8b\xd3"\xc6K\xb8\x85\x18\x7f\xac\x19\xfb\xc4\xfb\xf3\x8a\xf1\x9f\x10\xfd\xb7N\xd6\xe7\xb3\x91\xb0\xca\xc4\x10F3^\xeb\xe3\x8f\x1d\x0f\x0b\xfc<\xb4\x16\x0f\x1a\xdf\xc9\x1aF\x88\xa3;\x07\xf7\xbf\xcf\xadyn\xa1w\x7f\xe3M\x1a[\xa9\xf4\xdbH~\xc9\x1a\xbb]FpGP\x01$d\xb1\xc6H\xcfz\xa4\xd6\xc4\xb4\xf78\xd85\xae^\xda\xe5\xf3\x11\xfb\xa5\x89\xf9O\xb7\xa5K\xa7x\x92\xf2\xd8\xcbm\xf6\x97{w\x05r\xc7?\xce\xa8\xb7\x87oe\xcb\xabG\xb4\x9e\xe7\x9aG\xd2.,\xa3,YO\x1dGj\xd3\x95\xa7r.:k\xcd\xb3\x86\r\xde\xbb\x1f\rk\x92Yj\x10I$\x8ec=W<b\xbc\xe6w>`\xe7<\xd7Ea.aW\xce0;\x8a\x1aS\xba`\x9b\x8e\xa8\xfa?L\xd4m\xee-RH"\xff\x00X28\xc0\xa6\\\xcf\x7f\x14\x8c\x9b\\`\xe3 \xf0;\xe6\xb9\x0f\x02\xea\r\xa9\xe8\xea\xe6^P\x94(\x9e\x82\xba+\xab\xa9\x8b\x08\xdd$\x8a/nY\xb3\xe9^L\xae\x9bGr\xd5\\\xd0\x82\xee\xe2Kdr\xcc9\xef\x8ei\xd7\x17\xa8\xc8\xcaQ\x18\xae8p\x0ek\x1a\xda\xf6\xef\xcbam\x11\xf2\xd5\xf6\x01\xb7?\x8d<[\xbd\xd5\xd2\x99\x93yB\t\xc7\x1f\x89\xa5p\xb14\x92\xach\xaa\xaa"\xc9\x07n85f)\xa4\x13\xac\xb3[\xc6W\x00+(\xe6\xab\xcd\x7fl\xd24/\x10UN\xb2~\x1c\xe2\xab\xc3%\xbcWYwb\xa1s\x8c\xf0\x07aE\xd0\xcd]CP[(\x97\x113\xb6x\xdb\xdczV}\xf5\xbc\xb7\xf6\xc8K<ru$\x800=*W\xbb{\x99c\x10\xc2\xbd\xb1\x9ev\xd4\xc1\xe6I\x82\xc81\xcf\x0cO\xde\xa4\x04V\xfau\xab<n\\<\xe80Xw\xa8\xae\xb4m>W\x0f\x1a\xa8\x98\x1c\x92{\x9c\xd2\xea\th\x92\xc6\xf2\xc8b\x90\x9d\xd9S\x8ejCm\x02\x85?iug\x19\r\xbb\x90:\xd3\xbd\x98\x18w\xfa4)n\xf3\x86\x8c\xcb\x1eI\x8a1\xbb\x82\x7f\x86\xb1\x8d\xac1\xbb\xc7\x1c\xfed$\x06\xfd\xda\x12W=\xff\x00\xc4Wjl\xad\xe5r\xcdp_\x7fF#\x04~U\x9by\xe1\xed<\xb1\x97\xccs+\x82B\x93\xfe\x15W@\x9b9\xc8uD\xb4W\x86\xdd\x98\x16\xce\xd7\x1cn\x1d\x0fQU\xc4\xeb%\xbf\xce\xdb\x81^I>\x99\xef\x8ej-OL\xba\x12\xc8\x90\xc0\xbbx\x11\x92\xc4\xe5\xb3\xd4z\xf1\xd6\xa6\xb9\xd2\xa6\x8aD[\x942\xb1B?vB\xaa\x0fa\x8e3Ud\xd0\xeffR\xb2\x9e\xd4H\xcb(\x95I\x07\x0e\xa7\xbf\xb8\xefNk\xf7\x90\x0c\xbb\xb1#\x07\xe6\x0b\xb4\x8e\xa2\x99m\xa5\xdd<\xa5\xda\x12\x13\xaa\xab\x1c`c\xf9\xfe5z\xcb\xc3\x7fh\x99^Y\x9bh\xf9\x9c\x1e\xa4P\xf9G\xcc>\x0b\x98L?\xc4\xc0\xe7k;t\x15R\xf6f\x9e\xd9b\xf2\xe1L\x92IU\x1c\x8a\xea\xb5=>\xc6\xc41X\xdagt%v\x11\xc8\xe9\xcdr\xcbeu$\xa68\x91\xe4\x87\x85\xcf\\\x11I-t\x0b\xae\xa6IW\x8f\x06B\x8e3\x92\xca\xdc\xe3\xdcT\xb6\xea\x1d\xe4\x026h\x9f\xe6\x01\x93#\xd3#\xb8\xe6\xb6a\xf0\xcc\xa25i\\\xf2\xdf2\xf0x\xf6\xae\x82\x0f\x0c[XF\xb2\xcc\xf8\x80\x8c\x9e\xd8\xcd_5\x88v{\x18\xba\x17\x86f\x82\xce9\xeec<\x02]z\x8c\xe6\xb4\x10[\xbc\x93 *\xb8 \x9e\x80\x8a\xb1\x05\xfc\x9fe\x91%\xb9\n\t\xc2\x8c\xf2\xc2\x9b\x06\x97\x13\xc3<\xecK;\x0c(\r\xd2\xb3S}\x05dc\xcf\xa4G-\xd0\x9e-Em\xcb\xf0\x0b\xc6J\x12=\xf8\xa6%\x9a\xc3\x1b-\xe7\xd8\xe4Wo\x90[\x83&O\xe28\xadH\xbc>ew\x17\x13\xbb\xc4\xfc\x05=\x10\xfbV\xe6\x9f\xa2Z\xe9q\xaa\xc7h\xa8O\x02H\xfefc\xee{\x1f\xca\xba~\xb0\xf9yH\xf6i;\x8c\x82\xca\xf5!\x82G\x88F\xaa0\xa1\xba\xb7\xe4\x0e+b\xde\x08\xd2\xdd\xa5\xb79#\x92wn\xfc*\x8f\xf6,p\\\xc7s&\xa0\xd9\x1f0\x8c\x9e\x01\xf5\xeb\xd6\xac\xcb\xab[F\xa5\x03\xa9\x9c\x03\x91\xd8\xfe\x15\xcb&\x91\xa2\xbb\x1b}\x0cW1\x98\xd9\tG\x18c\x8e\xa6\xa8\x18\xad-\xe2\xf2dPpw.X\xf3\xc5XmJ\'h\xa2$\x00\xe79\xdd\x83Y\xb7\xaf\x11&\'\xcb\xf3\xb9y\xe4\x0e\xf9\xa4\x9d\xc7b\x8a\x18/L;\xad\x9a&g\xe1\xa0%x\x1e\xbe\xb5\xd0\xc1w)\x84\xc6\xf2\xc8\xd1\xa9\xc7\xccs\xfc\xea\xa5\x9c0\xd9\xfc\xe2\x07\x94\x9eWi\xc8Q\xf4\xab/2+<V\xc4\xb2\xbe:\x8e\x0e{S\xe6\x95\xadp\xb2 \xbc_*\xceY\x90\xafL\x81\x9c~4\x91Gl\xd6a\xe7\x94M)Q\xe5\xa8<\x06>\x95\x05\xe4\xccYl\xa1S\xe6d+y\x83?)\xeb\xfeMW\xd5<\xbf,\xddZ\xbb\x0b\x8bv\x08Fp?\x9d$\x98_C\x99\xd44\xab\x99\xed..\xf6\xe6\x08\xdc\x82I\xe8\xd9\xed\xeb\\\xcc\xba\xac\xd0\xc2\x91\x96\n\xd1\xe7\x18\xe0}+\xd1n\x8c+\xa4\x86\x94+\xcd\x80[kt>\x87\x1dk\x1d4+{\xcb\x8b{\xbf%\xb6\'2\xa8\x19\xff\x00"\xb4\x8b]D\xeesKs\x14\xb71\xf9\x92\x04@\x8aJ\xaa`\x93\xdf\xbf5ed\x92\xe5\xcaCn\xa0.vI\xc8$\x8eNO=\xab\xac\xb9\xf0\xb6\x8b-\xd41=\xceC\xab<![ \x1fNx\xab\x1a\\6v\xd15\xb4\x9bVA\x95p\xd9UF\x1dp\xdd0~\xb5\xa3\x9a\xe5\xd0\x9bjp\xf2[J\xb6\xe5\xc5\xba\xb3\x03\x82\xc8\xe7\'\xf0\xc63Py\xeb\x8d\x9e[+\x1e\xa7\xd0\xfat\xae\xe1\xf6\xa4\xb2\x82\xbf\xba\xe9\xbb=\xfdG\xb5`\xdfh\xb2\x19\x1d\xe2\xc0U\xcb\x10G\xf2>\xb5J]\xc4b\xc7\xb4\xbc`\x90\x87nI?t\x1a\x98\xb3HB\xf4\x01\xb2\xdcdg\xeb\xda\xb6t\xbd2\xdd\xec.$\xb9\xd8\xe5X*)`H\xf5\xeb\xde\x88a\x8a\x19\xe7\x81YN_;\x98\xed\xf2\xc7\xb9#\x9aM\xdfb\xd3\xeeV\x87G\x9e\xe8\x85\x8a9\x00\xcf\xde\x1c\x9f|z\x8a\x0f\x87\xef\xa0\x92Y\x12!$\x91\x7f\tJ\xee\xd1\xad\xac\xecc\x86\x0cJ\xc0\x03\x1e\xd3\x81\x9f\xebL\xc1\x94Iz%\x90n\x1f0>\xab\xd4c\xade\xce\xef\xa0\xda\xb9\xe7\xefa}\x0b+\xb2\x16\xdcO\'\x80\x0f\xa53\xc9\xbcyLj\x1fq\x05\x08\xdd]-\xc6\xaf#\xed$\x85\x89\xbe\xe1\x00\x10[\xb8"\xa9\xdb^\xbc\xabq+\xc4v\x0c\x8d\xcb\xdb\xfc(Sh\x12Fm\xb5\x9a\x19<\xab\x88\xe3\xdd\xb4u8 \xe7\xfc\xf4\xad\xfd6\x14\xb6\x0e\x01\xf3Y\xdb\x1bJ\xe3>\xfc\xd65\x94O\x91v\xc5\xfc\xb6\x07b\x92\x07>\x995\xd1\xcb\x1c\x91D$\x82h\xd6Q\x19,\x8er[\x07\xa0\xf7\xa1\xbe\x83$\x82\xc2\x18\xde?2\xdc\xbc\xacA\xc1\xc0\xe0\x9e\x9d{V\xb5\xc5\x95\x82\\C0\x862\xea\xbc\xb9\xe3\x9fJ\xe6\xda{\x91qk\x1b\x91*\x80\x0e\xd8\x81\x18$\xf7\xcdi4W\xd7R4p\x82\xe5@\xe1y \xfaVm\xb1\x11k1y\xb7\xabwu\x1c\tm\x02\xed\xc1\xc9\xdb\x9e\x8f\xc7q\xd3\xf1\xac\x8dKL\x96y"\x9d\x0f\x99\x10"I\x12?\x99H>\x84\xf3\x81\xebW\xae[S\xb5\x88\xda\xc9\x11i\x9f*rya\x8e\x9e\xf5\xcfC}>\x99y\x06\xf4\xc4D|\xa7\x04\xe0\x03\xf7I#\x81T\xae\x06\xbd\xee\x9a\x8b\xa7K\x99\xa0\xd8@\xf9\x18\xe4;c\x8f\x9b<g\xf1\xa8\x92\xda\xea-)\x9aD\x11*\xa2\xed\x93\x1f.\xd3\xd4\x7f\x81\xaa\x13\xea\x17+,\xca\xd2\x84\x85\xdbrB\x9fxw8\x04`\x8f\xc3\xe9Q\xbd\xf3K\x1d\xa8\xf3\x18\x13\xb6<\x1f\x98\xb7\xbe\xda\xa4\xb4\x02\xfd\x9cv\xabq\x1c,e\x8e\xcdIa\xba\x12\xbb\x9b\xfd\x92A\xe0\xf5\xf7\xf5\xae\x82\xd0\xc5\x14\xb6\xa8b@\xf2p\xc4`|\x83\xa7\xff\x00\xaa\xb0\xb5)\x9b\xec\xa2+\xa1",k\xf20\x18\x19\x1d\xf2;\xd58\xde\xe2G\x89\xa4FP\x17(s\xd7\xe8s\xc9\xa9m\xb5\xa0\xaez\x1d\xac:S\x16D\x94\x17\x0b\x95#\x04\x1f\xc7\xd6\xb7\xfc%\x0cpk\x97\n\n\x16x\x01\x18\xf6o\xfe\xbdy-\x95\xed\xdd\xbd\xef\x08\x8f\xb7\x826\x9ey\xf5\xe9\xc5wz\x0c\xd3\xe9\xda\xcd\xbd\xec\xf2\xf5a\x1b)\\|\x8d\xd7\xeas\xfc\xaa\xe9\xfb\x93W&j\xf1;mg\xc2\x1a>\xbbz\x97w\x96\xc0\xdc \x03\xcc\x19\x1b\xb1\xc8\x04w\xc7\xbdG\xab\xdaEi\xa1M\xa6\xdb\x1d\xbeb\xe0\xb3s\x8c\xf7\xab>\'\xf1\x1d\xb7\x864G\xd4\xae\x11\xa5\x1b\x968\xa3B3#\xb7A\x93\xc0\xfa\xd7\x95k\x1f\x19\x15/!\xb4\xd4\xb4v\xb6\x0c\x01fY\x83\xfc\xa4\xf5\xe2\xbb\xd4z\x9c\xbc\xce\xc4~&\x8fN\xb1\xb6\x865\xf2\xe6\xb9\x88m%\x940e\xf45\xcc\xf8\x8b[\x8fR\xb2\x8a\xc6\xcf\xcc\x8a\x04\xe5\x90\xf4\xcd7S\xd5moQ\xda\xda2\x12S\xe6\x02\xc7\x91\\\xfb?n\xe6\xba\x1d5\x15r\x14\x9b\x1f\x12\x08\x94\xa8\xe4\x1aI\xa3I!e~\x84w\xab\x11FXd\xf5\xf4\xa8.\x88Ta\x9e\x00\xe4\xd4s\xdc|\xb68+\xe0\x82\xf5\xd21\x85\x07\x03\xde\xae\x8b\x99\x96\xdcD\x8b\x8e:\xe6\xaa\x05\xfbF\xa4Gb\xc6\xb4\xe5O(\xa8\x03\x86\xe2\xa5n\x06\x8f\x85\xfc]{\xe1y\xd6H\x95d\xb7\x90\xfe\xf26\x1f\xa8>\xb5\xec\xb6\x1e#\xb0\xd7\xc5\xbd\xdd\xac\xd2\x02\xc0\x07^\x9e_=\rx\x05\xdcf;p1\x8f\x9b#\x15\xd1x3Q6:\xbd\xbb<\xb8\x82V\xda\xeaO\x1e\xd5\x85z\t\xa7%\xb9\xad*\x96vg\xb9\xea\x125\xba\xc8\xb0\xed*\xc71\xe3\x81\xf8\xd5(\xb5\x1b\x9b{\x01p\xb03\xc9*\x95\x11\x8c\x13\xc7\x19\xfaT\ty\x1d\xd3H\xb9\x1b\xe2\x19U\xec}sNm\xd11\xbaIN\xd7\x8fj\x85\xe7\x07\xd8W\x9dk\xb3\xab\xa1J\xe8^\xdd\x08\xd5\xed\xa6\x8aPA;\x97\xae}+B\xd7D\x9a\xd6u\x99\x89\xc3\x13\xf20<\x7f\x8dT>+[i\xde+\x8d\x8aQL\x8an\xa5\xdb\x95\xcf\\RZ\xfcE\xd2g\x91\x1aGUwlo\x03!y\xc5h\xe9\xca\xd7\xb17W\xdc\xe8\x1eao\x142\x08\xc2\x978\xc8\xea\xa7\xfc*\x95\xde\xa0\x93\xacq\xc4\x06\xe2\xe4\xb1\x1d\x8d_\xbb\xb4\x92\xea\x14\x9a\x1b\x95\xd9 \xe0\xf5\xe2\xa1\xb4\xd1l\xa3g\x8f\xe6\x03\x18\xdb\x93\x86\xf55\x96\xa5\x1c>\xa9\xa8Owx\xb6\xd8r\x15\xb7e{\xf3]6\x9bn\xf7\x8e\xd1J\x18\x12\x00\xef\xf2\x8f\xaf\x1e\xd5\xbdk\xa4i\xf1N\xdeT\x01\\\xf1\xbb\x19\xc5[\x11\xad\x9c\xbb\x9d\x06\xecpz\xd1\xca\x173Z\xc8\xad\xbb+\x12\x19x\x00\xf3\x9ez\xd3\r\x92+C\xe6HI\\c\x9f\xe7Z7S\xa4\x91\x8d\xd9\x12\x01\x9e*\xbaZ\xb3C\xb1\x9c\xb6\xe2r\xf9\xe4S\x03*\xeex\x04\xb1$\x8a\x81y\r\x92>\\\xf7\xac\x99\xa7]\xef\xb7\r \x1b\x17\x8f\xbc\xbfZ\xb5\xabh\xb2y~e\xba\xb8+\xf7\xe4c\xf7\xbd\xb8\xfe\xb4\xdb\x1f\x0eL\xa9\x9b\x96a\x19 \x86\xf4>\x87\xda\x8dn\x17C,.Vh\xe3\xd8\x9bOlq\xc7\xff\x00Z\xaeZK\xba\xe1\x90\xdb\x85\xeaQ\xdb\xb8\x1d\x7f\n\xd9\x8a+X\xa3\x8e\x05\xb7%\x81# q\x9aH\xf4H\xaeQ\xe5q\xc1\xc2\x05n\xc2\x9d\xf5\x11\xcc,R\xc9\xa8J"\x05\xd120OL\xf6\x1f\x8d%\xa4W\xd6\x84y\xaa\xea\x8d\xc3g\xb1\'\xa8\xae\xae\xdf\xc3\xdfe\x9c$2\xec\x19\x18\x04\xe4c>\xb5!\xb1{K\xef\x95\x84\xdb\xdb\x858\xc0\x1f\xfe\xacU]\x81\xce\xdb\x99\xe5\x96H\x9dv.p\tM\xccq\xdf\xe9U\xee\xa7\xb9\x13\x1bA\x9b\x98C\xe7\x080\x07\xa6s\xef\xef\xda\xba\xcb\x94\x10y\x9b\x93hn8\xeb\x8e\xfc\xd5{_&y\x1bdi\x95]\xa3#\xa8\xa9c1\xb4\xdf\x0f\xd9\xc3\x1en\x19\xa5\x94a\xf0\x7f\x87\xe9W.\xaf\x9e6\xf3\x12\xd9\x00\x1c\x028\xc8\xf7\xaej\xc3^{\xdb\xd6\x11\x83\xe5\x1c\x82rF\x07\xa6kR\xf7P\x9c\xc6\xa5l\x96}\xbfx\x97Q\x81\xfe}\xa9%\xd0e\xa3-\xcc\x93+\xbcX\x8c\xe0\x1cu\xcfjD\x96\xf6\xdaY9`\x85\xc3g\x04\xf7\xe9M\xb4\xbca"\xbc\x83iC\xb5T\xafo\xafz\xd1}R\x15\xb7x\xeecPX\xfc\xbcg"\x8bj#\x0bU\xbd\x96K\x84U\x93\x97n\x84\xf0=\xc7\xa5K\r\x88\x8a\xee\x07\xbc`W\xab1\xe0\x10}\xfbU\xf3k\x05\xdd\xc7\x99"\r\xa0\x05\x03<q\xe9RI\x1d\xb3\xdb\xdc\x89\xa3\xf3\x19\x066\x1e\xe3\xd3\x9e\xb44\x81\x197\xb2\xda\t\xdf\xf7Q8@L`\x8d\xc7\x8e\x9fZ\xcc\xb8\xd4e\x82\x01\xe5\xc0_\x8c\x83\xb3i\x1e\xc35\xa35\xad\xb5\xeb\xa7\x97e\x86\x80ya\xf7\xf1\xb8\x8e0\x07\xa5b\xde\xc4\x902G\x0b\xa9\xb78\x12y\x87s\xee\x1c\x95\x1f\xe1V\x90\xc9\xa2\xd5f\x8a\x00\xc9\x1b#H\xe4\x02_,}G\xa5t\x16\xb7\x17\x1fbP\x15 i\x01`\xcc\xc3i\xf4\xceEQ\x8au\xb9\xf2U\xa5\x8d\x95\xc0h\xe2a\x91\x11=y\xf5\x15\xb0\xd7Q[\xac\xb2\xab\x97ueU\xdc>l\x0e\xb8\xf4\xa1\x88\xce\xbcy\x18\xc2\xd717\x98\xa5~PB\xe4\xf6<\xf2G\xe5T\xf5\xa6\xff\x00\x89|\x8c\xcc\xf1J\x18\x82\xe5\x86H\xe9\xfaz\xd4:\xbe\xac\x97\x9a\x8a\x9c,n\xb1\x96\xf3Yp\x0f\xf9\xfa\xd595\xa8\xef\xec\xcd\xb4\x8b\x8f+\n\x1cpG\xd6\xa9\t\x9b\x9aL\xd6\xf7J\x96\xd3\x08\xd8"\x8d\xc0\x9c\xf4\xf4\xfa\xd6\xad\xfe\xdbx\xd6[\\B\xca}\x06=\xc1\xcdp\xf1j\x17v\xd7?\xe8\xef\x88\x8b\x05\x98\x00\x01\x04w\xc8\xeb\xf8\xd6\xd4\x97\xcfx\xcb\x84eP\xbbX\xbb\xee\x19\xed\xd2\xa5\xee;\x11^[E#J\xc2$\x92&|\x16\x85\xbf\xd4\xb1\xef\x8c\xe0\xaf\xaf\x19\xf4\xaej?\xb5\rf\xe1\x1eC#\x82\x15\xd8|\xca\x07m\xbf\xe1\xcdt\xc1\x85\xf4\xdeLS v\x91T\xae:\x81\xeb\x8ek\x1d-\xdbO\x9a\xf6[\x88c3\x07#\x11\xe4\x9eORq\xe8G5q}\xc4\xd0!\x96K-\xa8$(A\x01\x90g\x8fr\x07\xaf\xd2\xa2G\xb9\x89\x84r\xcc#\x89\xbfw#\x84\x07\x82q\xdf8\xe2\xbaO\x0c\xa4\x17\x91?\x99g\n\xb1;I\x19\xcbzg=k\x13\\\xb3\xb9\x8a\xf2\xe3\xec\xd6\xf1\xbb#\x84\xc6\n`g\x929\x19\xab\xbe\xad\x122\xee[\xd8u;\xa5\x9e\xdf\xf7\x1c:\x04\xe4\xb1U\xc6\xe00{c\x93\xc5W\xb5/us%\xe0A,\xd71\x8c\xab\x16\xda\x81{\x8c\x8f_Jl\xec-\xef\x1a\xc8\\\x96F\x1b\xcb<\xac\xeb\x8e2\x84z\x1e\x05[YLw\x9fk\xf2"\x110*IV\x19\x19\x19\xe7\xaf^\xde\xdd\xa9\xf4\x02\xc5\xd5\xe9\x9b\xcbI\xa0B\x15<\xb4a\xb8\xe3\xd7\x1e\xf5qn\xae.R7\xb6X\xc1\x88\x04\x91dl)\xfa\xe3\xbf\xbd]\xd4\xfe\xc5\x1c\t\x1cp\xa9\x86T]\xb8nY\xcf\x00\x81\xd7\xafz\xc2\x81\xa6\xb2\x94\xe2q\xb9Sl\x9b\x80\x0c@<\xf0q\xb8\xfeU\x95\xaeU\xf4/Y\xda\xcd3\x88\xcd\xa6\xd7\x91Hvq\x90\xb8\xeb\x9f\xcf\x8e\x86\x9d\x06\xa5oi\xa7\xdc \xb5\x08\xb1\x8d\xc5Q\t_O\x9c`\xed\'\xadg\xd9L\xd2\xddG\x1c\xcf<\x91)f&&\xda\xcc:\x0c\x80x\x15\x03\xc7\xfd\x97o{sm\x1b,r\x12\x8c\xb2I\x92H\xfeG\xd0\xd3\x94u\xb0&Z\x86vU\x80\xa4v\xef\x05\xaa\xac\xad\x18P@c\xeb\xe8EO\xab^\xdb^\xf9fEhnL\x8a\x02\x94$2\x81\x9e=\xfe\xa2\xa5D\x92]14\xe8#T`\x81\xb67\x19\x07\x9eOO\xc6\xb2/\xbc\xf8\xf5\x05\x178.r\xa7g\xcc0=0:\nQCgU\xa5\xc1l4\xd77\x104\xf2\xf9\x98\x05N\x19GN\xa3\x91\xf5\xa4\xbb\xbc\xb8\xb2\x94\xd8y_g\x84\x01\xb2\xe3v\xed\xe3<\x8c\x0e\xe3\xbdU\xd3\xef\xec\xae-Wbp2\x0bm#,{\x7f\xfa\xeb>\xe6[\xb9f\x8e+q\x1e\xcd\x85Hrv\xa6OQ\xfdin\x16\xb1\xb3t\xa2\xeae\x92b\xa2r\xc0\xc4\xec\xe3\x03\x8e\x00\xe7\xff\x00\xad\\\xc4\xd3I\x1e\xa6\xf6z\x9d\xbci\x949\x9d\x1b9]\xdf1\x03\x8e{\x0c\xfe\xb5cM\xb8\xb9\x89\xee\xf7\xdc\xee\x7f,\xa8\xc99T\xf5\x03\xbe{V~\xbb\x04\xf7\x02\xde\xf9\xb6\x12\x1co\x91\xb2\xec\xaax\xc1\x19\x04}3T\xa3\xad\x98\xaf\xd4\xbel\xb4\xd9!\x96\xe3\xcd{{X\xdb\xca\x80\xcb\xb9\xdd\xc8\xee\xc1\xb2\x00\xed\x8cUM\x0bG\xdd\xaa\xbd\xd4~\\\xef\xb1\x94\xc4\xb1o\x00\xe4`\x8c\r\xa0\xff\x00\x8d\\\x9d%:S.\xa7\xa9\t\xc4 0\x8c\x90\xa20\x0f\x18\x1d\x8f\xe2sY\xd6SKi\xa9\xf9\xfaU\xe8\x93\xcc\x01\xdd\xbc\xcc\xc3\xb8\xf6|\x1f\x94\xf6\xe2\x9d\xdd\x98Y6u\x1a\x85\xdb}\x8eB\xfa]\xceV\x12<\xc5\x91\x18n=\xc8,?\xc2\xb0t\x89nQa\xf9F\xe1\x8d\xed \x0c\x13\x9c\x10}\xbb\xf5\xa8\xe6\xf12\xcft\xb6\x97\xd1\xfd\x9am\xd9\x01\tul\xfa\x109\x02\xa5\x8a\xfa\xde\xe6\xe8\xc2]UKf9\x18\x1c\x909\x1c\xe3\xb7\xa54\xac\xb6\x05\xa9\xb9;C\n%\xb5\xb5\xbaOw08`B\xe3\xa1\x19#\xf9T\xf2Iu3\x98\xe6TM\x8a\x15\x86\xec\x91\xfa\x7f\x8ds6\x8a\xd1kkx\x1b\xf7(|\xd2\xac\n\xf3\x8cn<t\xf6\xad\x1dZ\xe1\xbc\xb6\xbb\xb7\x90\x02\x83{\x8e\xb9\xa9q\x0b\x9d\xfe\xbb\xa6Zx\xcf\xe1\xcc\x96w\xaa\x1aXp\xd1\xe1\x8a\xe6T\x1f/>\xe0\xe3\xf1\xaf\x9ff\xf0\xd5\xd5\xdc\xd3\xdc\\\xc2\xb0-\xba\x85>k\xe3*\x0fA\xef^\xcd\xe1/\x16\x8d:g\x8a\xe6O6\xdauVU$r{\x90=zU-o\xc3\xdau\xf7\x8e\xb5\x9bk\xbbX\xda-^\xca+\xfbY\x0b\x10\xe8\xe8v\xc8\xa9\x8e\xe4e\xbd8\xae\xb8T\xbc|\xd1\x87\xb3\xf7\xb9_S\xca\xe3\x92\x1bxL\x93H\x12$\x1dZ\xb0\xa7\xd4\xeen\xf5\xab{kF\xd9\x13\xba\x80\xa0rr{\xd6\xb7\x89|!w\xa7\xeal\x8e\xaf%\x94V\xebsq2.\xe3\x02\x1e\xcd\xf8\xe3\xf3\x15W\xc2:{\xdej\x13j\xd2\xa8X\xe2\xf9c\x00`\x16#\x18\x1fAZ\xba\xad\xc6\xe4J\x97$\xf9N\x8f\xc9`X)\xc0\xce3\xebY\x1a\xec\xe9g\xa7\xb0\x1c\xbb\x0c\n\xda\x99\xb61\xf3\x99T\x7f\t\x1c\x0cW%\xab\xac\xba\x9d\xe6\xc4\xcf\xd9\xd3\xa9\xf5\xa8\x8c\xba\x84\x91\x95\xa4[\x16\x95\xa6`p+j\xd2\xd9n\xae\xc0\xdb\x90\xbd\xe9\xb1\xdb\x04\x8f\xcb\x88\x0c\n\xd9\xd3\xad\xd2\x05R\x06\t\xe4\xd6\xf4\x95\xd9\x9c\xb49\xedr\x03\x1a\x80\x07\x01\xb0qQ\xc4H\xb1,0\x1d\x06A\x1c\x13Z\xfa\xfcJ\xcd:\x11\xc1!\x85d\xceDzgpqW%fJ=\x93\xc1\xb3i\xda\xce\x83\x16\xa8/\x1a;\x91\xf2\xce\xb8\xce\xc6\x1dA\xf6\xef\x9a\xc9\xd7|y-\xad\xd0\xba\xd3b\x87\xcb\\\xc3\x11)\xd4\xfa\x81^S\xa3k7zW\x9c\x90J\xc2\x19\x80\x12 \xfe,t\xa9%\xd6\xd2KU\x88\xc4K!%I\xeck\x8a4\x14[gC\xabt\\\xd4\xae\xee\xb5MD^j\x04\xb7A\x8c\xf6\xf4\x15\xabs&\x9e\xd6(-"\xc0\xc0\x18=s\xdc\xd72u\x1f=\x14\xb8\x01\x87\x19\x1d\xea\xfe\x9e\x0b+n\xe4\x1eEl\xa2f\xe4z\x1f\xc3\xcf\x11\xcf\x03K\xa5\\>\xf8\x19KD\xceI\xd8}3\xe9^\x81\x0e\xa9\xe7\xcd\x08F_\x93\xe5\xe9\xc1?\xcf\x9a\xf2\x1f\t\xc5"\xf8\xa2\xc9#%VG\xc3\x1fn\xf5\xec2ZY\xc3<i\x00\xdd$\x80\x803\x9c\xe3\xb9\xaf?\x11\x14\xa7\xa1\xd5I\xde:\x97\xee/%E&\x14*2y$u\xf7\xac\xd9\xf5\x89\xf7*J\xdb\tS\xdb\'\xea\x0fj\xbdoo \x90+\x05(\xa4\x07\xdaq\x8e\xe0\xe2\x8b\xcd"\xd3P2\x82\x842(b\xc7\x8c}?\xc2\xb0we\xe8W\xb0\xbd\xfe\xd4\x95\xd4\xc2B\xc5\x81\xe6\x03\xc1\xab7W\x10\xf9\x8dn\xa0\x89v\xe5\n\x8e3\xe9\xd7\xfaT\xb6\x10\xc5k+Y"\x95\x12\x0c\x87\x03\x00\x9cz\xd5s\xa4\xf2\xae\xbf\xeb\x10\xfc\xc7\xb9\xfch\xd6\xc1\xa0Y\xc2n%x\x9ef]\xa0\x03\x18n3Wf\xbc\x8a\t\xfc\xb9#\xdd\xb4d\x82+"K\x19\xa2\xbcic\x97=>^\xe4w53\xbcr0\rp\xc1\xc8\xe0\x15?\x89\xc8\x06\x98\x13\xc9\xad\xc3\x83"0\x01\x9b \x95\xf4\x19\xaaVZ\xd4\xf7j\xaf\x8erX\x8e\xc5j\x97\xf6\\\xa6q\x88\x1a\xe2\xdc\x12\xc1\xe3 s\xe8wc#\xe9\x9a\x85\xed%\x82{\x8b\x7f3\xc8\x91\x17;\x9b\x92\x019\xe8;S\xb3\r\r\x8f\xedI\xe4\xd4\xfc\xb8\xd7pP\x0f#\x92+FWa(n\x01\xdapG`;\xd7<\x97ikikq\x14\x91-\xd0\'g\x7f3\x9e\x8c\x07<\xd5k\xedv(\xe5\x9aIT\xa3\xc8\x01e\xdd\xd0v*?\x9d\x1c\xac\x0e\x92X\x85\xcc-\xbaQ\xbc\x13\x90\xdd\xbd)\xbf\xd9\x11[\xa1\xc4\x84\x14\x19c\xdcb\xb8\xf9\xf5\x82\x1d\xe5.\xc3\xe5\xc0\xf9\x8f=\x86q\xfc\xebR=tM\xa6\xcd)\x96?:F\np\xdf.O\xf7G|S\xe5h\x06\x9f\x0f\xc7m\x1a\x9by#U#\x86\xdd\x86\xe7\xfc\xf7\xa6\xcd\xa7\x05\xb5*\xf2\x12\xab\xf3\x07Q\xfa\xd6\x95\xce\x9c\xb2\\F\xfb\x9blD\x0f\x97\x8c\xff\x00\x8d<\xd9+M\xb0\xca\xcaH;\x0f]\xa3\xd2\xa5X\x0eb\x0b\x91ao)-\xb8\x978\x98\xa9\'\x18\xe9\xf5\xa8F\xadlo>\xd6\x8c\xd2\x88z\x8d\xd8\n\x0f^;\x9f\xa7J\xb7&\x94\xedq4r:\xb2\xc6C\x06\xce\xc3\x83\xea9\xfeX\xac\xed_N)\x07\x96\xf1\x88\x88\x1b\x94\xb9\\0\xfc*\xba\x94\xb664\xbdu\xe6\x804j\x8c\x84\x90\x19\xb9%\xb3Vm\xae\xe4\xba\x9eB\x12\'\xd9\xf2\xb9-\xb4\xe6\xa8\xe9\xd0[A\xa7Eq,@\xa0P\xa1\x83\xf5\xc7\xf7GrM\x16:\xcc\t3\xdb<o\x89[z\xff\x00\x11<z\xff\x00:k\xb8\x9a1/\xae\xcd\x86\xa1>\xe0c^7F\xae\xc7\xd7\x90s\xd3\xda\xb2\x96kI\xee\xa2\x91\xee\x1673\x07\x11\xb7E^\xe7\x19\xe0\xe3\xd6\xb6u9Zk\x89\xaed\x85|\x96R\xa1G\x19\'\xa75\x93\xaa\x08.\x96;\xf0\x88\x88v\x86\x8ff@#\xd3\xaes\xf5\xadl+\xb3n\xc6\xe0\xb3E\xb6)n\x11\x89Tr\xbf!\x00\xf0A\xeakq"\xb5\xd3]\xc7\xcd+\xc8\t2H\xa4\x15>\x83\xaeG\xe3\\\xb5\x96\xdf&K\xb7wI\xc6H\xd9\x90\x02{{\xfbV\xc5\xee\xabiu\xa2\xc9\x14s4\x8f\x1e\x19]\x88\xce\xe1\xdb \xfe\x86\xa2\xda\x8d\x94\xf5\x19aq+\xb2\xaa\x88\xf0\xd1\xb2\xf0H=\x7f\x1a\xa1\xa7\xbcBIf\xf3s\x06q0,\x0e@\xe8x\xe4\xd5MB\xff\x00q\x8f\xcc\x0c\xc4\xe0gh\x18\xe3\xd0u\xaa\xd0\xbaD\x042\xa6c2\x02\xa0\x1d\xac\xc7\xd3>\x9e\xd5v\xd0,l\x1b\x9bo7\xca\x8e\x0c\xda\x92A\x11\xbf\x07#\xb9\xef\xd7\xa5Z\xb0\xbdK=6\xe1\x0cJ\xfeXX\xe3\xdc\xc3\xe6\xc1\xe3\xa7C\xcd`+F\xd20\x01\xe1p\xc4\xaf\x95\x923\x9e\x98\xa9V`Z\x06b\xde{\xb0bA\xdc\xb8\x07\xa3\x03\xdf\xebC\x8ab\xb1\xa71\xbd\xb5\x8c\\\xdb\xa2\t\xd9\xf2\xc8\xa0q\xf8\xf756\x9b\xab\x0b\x8b\xf9d\xb9B\xd8Q\xc1\\\xa88\xe4\x9c\x9eH\xaa7Whcp\x8f\xd4\x1c\x05\x00}x\xec*\x93J\x97(<\xb8#V\xdb\x9d\xc7\x03\xf0\xcf\xad+h;\x1b\xd0\xea&\xda\xf2g\xd3\xddc\x8c(fG\x00\x16#\xb8\'<\xd4S\xcd\xe6\xcb2\xb0-p\xdf0b\xdbv\xe7\xd7\xa8\xac\xd5\x9cE\x1f\x9f\xe6\xa0i\x07\x96\xb1\x00\x07\x1e\xe7\xd75\x18x\xeem\xd4\\\x04,>We\xf9\xb77\xb8\'\xfa~5iu&\xdd\x0b\xf3\x89-\xad\xbc\x8b|\xc7\x199\xde\xeb\x83\x9e\xf9\xe9\x9a\xcfx\xe3d7Is:\x01\xf2r\x9b\x82\x9c\xf5_c\xefQ\xbbK9\x91D\xd29\x8b\x08RO\xbb\x9e\xd8\xcfJ\x89\x8c\xd7\x13\xa8\xc7\x94\x10\x85f\xdb\xf2\xe4u\x1e\xf5IlOsbK\xc4{\xc8\xc4s\x99\xc4q\x8c\x86UC\x8f^\x00={\xd5\xbd2S=\xd5\xcc\x8e\x91\xb0t\xcb\xb3\x1d\xd8\xcf\x1c\xe3\xe9\xf8V\x15\xeb\xdb\xfd\x9a8\xd4\x17\xe7hr\x9c\xa8\xff\x00=\r1<\xc5\xf3D\x13\xcc\x0b\r\xa5\x80`\xc0z\x10)8h\x17\xd4\xba \x96\xda\xe5\xe3\xb7%\x04\xb9q\x97\x01\xb09\xe7\xf2\xf7\xaaq\xca\xd0\xc8\xaf41\x08\xa5}\xdbe|\xabq\xd7\x928\xa8\xe7\xd4!R\x9bQ@#\x0cO\xde_o\xf6\xb8\xf5\xaa\x92\xb7\x9a\n\x02\xdb\x0erQ\x86\xe1\x9e\xc7\x1d\xa9\xb8\x82\x95\xce\xa7F\xba*\x8fy=\xd3\x12\x87b\xc7\x19,\xad\xc7\xdc\x039<Vm\xf4W\xd2J\xd7\x82\xdd\x9b\xcb;\x8a\x87U\xceA\xe9\x9e\xb8\x1d\xaa\xb5\xb4\x91\xa4\x11\xc3\x0c\xc2&,G\xcd\xc0\x07\x1cr:\xe6\x9c\xf3J\xb7\x10\xc7ut\xc3\xca\x00aX6\xc6\x1d\x06\x08\xe9S\xd4\x0bV\xb7A\x9a\x0b\x7f6k_9C\x97\x98`\xbf\xb2\x83\x85\xcf\xbf5\xa3\x04\xd6v\xf3\\\xc0$\x81\x9d\xe4\xc1\x8e\xe4\x85v\x1e\xc78_\xcb\x15\x91\xfd\xa1%\xd4\xff\x00\xbdO\x94\x00\xa1v\xe7>\xbfL\xd4n#\x8e\xe6HU\xd2\x07\x88\x9f4+\x16R{\x8c`/\x7fJ\x97\xb9Vd\xf3\xce\x90\xcf$\x82\xf1\x98\xb0\xe7i\x07\nzd\x8c\x8cTi#\xdc\x82\x86\xdf\xcf\x90\x82?t\xa0#\xa0\xef\xcfA\xebP\x92\x12\xf2\\\xca\xae0\x00o/\x0c=\x8e21\xf4\xa7[K4M\x1c\x98BA8\x8fa+\x8fqD\x99qVE\x81m\xa7C\x01\x92[(\x16H\xc0\xf2\xd2\x08\x94\x0e{\xb1\x03\x93\xd7\x1d\xbd\xaa\x17\x8fO\x9a\xd3\xcf1\x08&\x19\r9e\x8f=\xc0;[\x93\xf5\xa5\xd5![W~b\xf3\x0e\x1flqmV_^\xa7\x06\xa8O<\x1er4\xcf\xc7$\x0cn\'\x81\x80G@8\xf5\xabH\xcd\x96,lUc\x9bPx\xe4\x942|\xae\xb8\n\xb8\xf4\xcf~EYa,\x16\x90K\x02\xc4\xb2?\xce\x0ey\x1d\xb1\x9e\xf5B\x01"!\x9c\xdc\x82\x00!Q\xb9\xca\x9e\xb8\xcfN\x9f\xfe\xaa\x9a%i\xdf\xe5\xe5\x97\xe7\x00gj\x9e\xa4\xf1\xd6\x86\x81\x1a\xb6P\xce\\\x9f\xb4\xb1\\\xaa\x98\xd4\xfc\xd2\x06\xce\x0f\xcc8_\xca\xbb\r;C\xb1\xbd\x85\xecu\t\x8d\xac\x81B\x85\x1bAd99\xf9\xb3\x93\xfc\xab\x85[\xbf.\xe8\xbc\x84\xc8\xecI\x01\xd7\x0c\x0fa\xea+sQ\xd4\xa4e\x85\x8f\x97\x08h\xd9&\x95XHd-\xc9\x07\x807\x0e\x07\x1d*f\x9b\xd1\x10\xee[\xb3\xd3lt\xaf\xb5\xc7\x04\xded\x91G\xbe\xd6_/(\xc4w!rNy\x19\x1c\n\xea\xbe \xe8\x92^i\xfa-\xcc7oh\xd6\xf1:\xcc\xd0D\xcf1\x8c\xa8,\x10\x0eOB1\xfe\xd6{W\x9ei!\xa5\xd4l\xad\xd7\xcdwy\x02\xacjX1^\xc0\xfd:\xf1^\xfb\xa9ip\xea6\x91\xc2\xef$O\x11\r\x0c\xb16\x1a6\x03\x19\x1f\x99\xe0\xf0kjj\xc9\x99\xc9\xd9\xa3\xe7K\xd8\x83|=\xd7/&{\xbb\x08d\x92+k8gB^\xf0\x86\x18\x1b\x88\x19\x00/ \x0e\xddj\xce\x9f\xe1\xcb\xbd\'\xc3v0\xbcEfh\xf7\xc8=\x18\xf3\x8f\xd6\xbb=S\xc26\xd6z\xbcw\xba\xa5\xed\xf6\xad4,Z\x13u.\xe5C\xdb\x0b\xd0V\x8c\xb0\xbe\xa3\xa7\xa0\xb9\x85\x86\x14\x95\x08\xbc\xb5)\xcd=\x11Z\xbdO%\xb8\xb3\x96YJ\xb9#i\xc1\x15\x0c\xd6\x11C\x01\xc2\x83\x9e\xc2\xbb\xdb\xaf\x0e\xb4\x1f6\xd3\xb5\x86\xee}j\x11\xa1\x8c\x03\xb3>\xc4V|\xed\x0e\xc9\x9epm\xa5W\xc9\x1ct \x8e\xb5v\xdca@\xae\xc2\xff\x00B+\te\x8f\x18\x1e\x95\xc9\xccV)vt9\xf4\xae\xaa\x1551\xa9\x133]_4\x80\x07;Fq\\\xf6\xa7&\xd8\x1601\xebZ\xb7\x97a\xf5M\x99\xe3\x18\xacmLn\x98\x8er\xbd\x8dt\xd4ww2\x8a\xd0\xa4\x80\x02\xa7\xb5:g\xd8\x86\x12\xab\xb0\x9d\xd9\xc7?\x9d\x11\x8fn{b\xa6\x96\r\xca\xbcpx5\x9a\xd4l\xa1\xb4n\x1bO\x1e\xf5\xd4\xe9\x16\xf9\xb6V\xday\x15\xcb\x10S \xf0A\xae\xdf\xc1\xb3E\x7f\t\xb5\x91\xb1,g\x81\xdd\x96\xa2z+\x97\x1dY\xd5x:\xc0\xb6\xa6n\xcb\x150\x8f\x97\xeak\xd1\xadg\x98\xab\x8b\x94P\xb2\xfc\xf1\xba\x8c`\x8fn\xc6\xb3t\x8d,i\xd1$\xa5\x08@\xfbN\x01\xc9\xc8\xe0\xd5\xedE\xcci\x18e`\x8c>i\x17\xa2\x0e\xc7\x1d\xab\xcd\x9c\xb9\xa4\xd9\xd9\x15ebqp\x93_\xa1\xf3<\x86T\xcc\xa0\xff\x00\x10\x1c\x0ei\xd7Z\x9f\x92\xac\x9bI$(\xc8\xe7#\xfc\xf7\xac\xab[\x83<\x12\xca\xf2\xee{f]\x87o.\t\xef\xef\xcf\xe9P\x9b\xec\xc6\x1ed\x0cr\xca6\xf0\x17\x1d*,3B\xebV\x0f\x1a$d3(9\xc8\xe4\x9a\xadk\xaf\\\xbbL\n\xb1`21\xd3\x15\x8e%v\x11\x85\x8fb\xf5\xe0\xe7\xafzF\xbf0\xb9\x95\xb3\xb4`8\x07\xef\x83O\x95\xdc.\xacu"\xeaf\x16\xd3<\xb1\xbck\x93\xf2\xc7\x8e\xa3\xd7\xa8\xe2\xa2\xba\x9e;[\x84hDA\x98\x85q;\x95\x1bO\xae2x\xcde\xc7\xae\xa40\x04]\xaaW,\xb9#\xe6\xe3\xee\x9fQX\xc7YI\x9e@\x832\xbe\t\x00d\xc6G\xbf\xa6=h\xb0\x8e\xc2_\x11Ae?\x96\xd8+\x8c\x85-\x9d\xbfC\xdcW+\xae]\x8dB\xef\xed\x16\xd1<RHN\xd0\xed\x8e\x07\xbfaXW\x97)#\x0f%\xc38\x19\xdb\xc1\xc1\xf4\x04\xf5\xab:<\xea\x0c\xb2\xc8\xd8!K\xef\xdaw\x1e\xc5Gn\xf5qV\x0e\x97$0B\xd3\xbf\xdb\xa6\xfd\xea\xb2\xb7\x99\t\xe3\x83\xd7=\x87\xd2\xadK\xaa5\x942\xda\x86\xf2a\x93\x0c\xdd\x0e\xfes\xb9\x89\x07\x19\xe9\xc5d5\xec\xf6\xb6\x92\x08\xa3\x8fd\xac6\x9d\xa3\x81\xdf\x9fN\x9c`\xf3\xe9P\xff\x00h\xcf\x1d\x93F&>I!\xb8L\xaf>\xa0\xfeUk\xc8M_r\xd5\xc6\xa4K\xcb\x14\r+@\xdbJ\xa4\x98#nx\nO\x7f\xca\xa4]H"$r@6\xac\x9f!\x8b\x18Q\x8e\x99\xe9\xe9\x93\xcdfZH\xed?\x90K$R\x10\n\xa9\xe3\x1dz}*\xdd\xd9\x85I\x86\xdd\xe6\xf2\xc2\x90\x87\xb08\xe7\x8c\x10\x7f\x0c\x1a\xd1\x12\xf4=*MA\x8c&g\xdcv\xf0\xbcpO\xa1\xf4\xaa\xb3j\x0bw\xe5<2\xe1\xf2U\xc18\xc6GL\xf6\xaa\xfav\xa1,\xb0][\x1f)\xb0\xc4\xb2`\x0f\x97\xb1\xcfj\xe5/\xe6\xfb%\xeeVY\x19\x86\x18\x12=\x0fLta\xef\\\xca\x1a\x9a\\\xd8\xd5/\x9b\xecw\x17\n\xd2#\xc46M\x18\xf9\x89^\xc7\x1fZ\xadi\xa9\t-Z+\xe4YY@\xca\xabs\xeck\'S\xbf\xfblP\xa4q\xa9a\x927(\x19\xcfS\xc1\xaa\xb6\xd24q\xedUt9*\xc7\xef\x13\xed\x8c\xd5r\xe84\xba\x1a\xd6z\xff\x00\x97h\xf6\xd2\x1c\xf9,T&\xd0\xd9\x1d\xb3\x9f\xe9YV\xf7\x8c\x84\xfe\xf3\xe6`p\xa7\x8d\xa2\xa9\xdcH\xf1O\x10\xdd\xb9\x94\x8d\xdega\xef\xcfjd\xce\x9b\x8b\xc5"\xb2\xa9%\xb7\xf3\xd3\xfcj\xd4l\r\x9a\x12H&\xb6\xe6M\xa22N\x15\xb8n=\x08\xe9T\xe6\x9a3!\xca\x15\xf3\x07\x01T`\x8fo\xc6\x80\xe5\xed\xc5\xccq\xa4n1\x86f\xc7\xca{\x0e\xe6\x9a\x8ce)\x02\xc4\xab \xcb1\\n\xcf\xe3V&Y\x82\xfea\x0bE$\xa5\xc0_\x9bh f\x9d}q\x1cq\xbe\xd2\xc1G-\xbb\x92\xc3\xeb\x93\xfa\x1a\xa3jZ\x1b\x86g_2)x\xc0\\\x9e?\xcfZ\x92\xee\xdcH\x81\xa1\xdb$\x19\r\xf3\xbf\xdd\xcf\xaf\xf9\x14\x9a\x08\xb2#<\xcce\x95\x18\xcc%9!\xa4##\x1crj\x18\xee%\x8dY\xfeS\xbcaQ\xb9\x1b\xbd\xf1\xd6\x9b;1\x12|\xeb\xbe0[\xcb\xceT\x8e\xf8=\xe8\xb2S"\x170e\x18\x03\xb4aW\x15I+\t\xb7wb\xf8s\xcf\x9d\x18\x86\\\xe0*\xe3o<\x9e\x00\xc0\xfaS%\x85\xa2\x01\x92]\xe5\xc8\x0e\xb9\xe5~\xbe\xd8\xa8\xfc\xc6\x82\x04w88\xc0\x84r\xa3\'\xaesQ\xc3\x0bB\x8c\x18\xc6\x8c\xef\x80\xe0\xe4\xe0s\xd4qCZh8\xbdu\x15f1\xdd\xba\x9c\xc8\xa3\x012;\xfb\x11\xd6\x96fdH\x86\xe0dv%y$\x93\x9e\x7f*\xabr\xc2\x06D6\xed\x93\'\xef\x0b1\xdcs\xd3\x8fJ\xb9,H\x08\x12Kn\xfeR\x02U\xc1\x1bKv\xe9\xc9\xf6\xa1.\xa1~\x84Q\xdc\xf9\xf21\xde8\xf9\x01\'%Or\xb5pJ^\r\xa6m\xd0#\x8f\xdd\x8ew\x13\xea\x01\xe8+*2\x99P\xc5\x9f\x07$\x15\x03\x8e\xdc\n\x94#>DQ\xb3\xba\x11\x92T(\xcf\xa7\x14\xdaD&_\x9esl\xcf)\n\xcb&\xd0\xaeI\xc8\xc7?t\x83\x8ax\xbb\xb9\x96\xd8\x9d\xd8f%\xca\xb1"3\xeaH\x1ct\xe0\x0e*\x0b\x99\x9a\x08aS\x16\xf2\xe7\x95\xe4\x9cz\xe3\xad2;\x834\x0c\xb1[\xb4%yY:\xb3\x8e\x98\x1d\xbf\x1a\x16\xc0\xf7\x1bis"^}\x94\x03!|\xed\t\x9f\x90c\'\xad\\\xb9\xf3!W\x8e/)\x83\xa8\x04\xb1$\x10}\x7f\xc2\xa2\xb3\x92\xe2\x16FFU\nv\xba\x16\x04\xa8\xc7|px\xa1\xee\x0bA4\x8du\x8f-\x8e\xe8\xd5>l\x1a\xb5\xa9\x0c\x82\xfd^\x18Lr\xc2\x1aLp\xa8\xa0yy\xfe"\x00\xfd*#-\xb5\xb4i\xe4\xac\x82O\xbc\x18\xa0P\x0f\xe7\x9a\x17\xcck\x84!Uc\xdb\xc2\x13\x8c\x0fs\x9a\xacJ\xdc\xb4"RX&xl\xf1\xe9\x81I\xdb`M\xeeX\x89\x8c\xd25\xdc\xad"\x08\xe3;aQ\xb7\x92z\xe7\x14\xc9\x16L\xc6\xd7 \xb4\x81B\xc6\xccy \x9e\xb5\xaa\xd0\x08\xad\x91\x8cq\x9d\xd821o\x94z\x0e\xc7\xf0\xaa\x17"\x0704[s\xbc\x90\xeb\xd4\x81\xc608\xa8\xeah\xc8\x17s\xce\xea\xb2\x02##$1\x03n{U\x8b\x8dE\xd6O\x94\x92Qv\xf4\xc0\xc6zc\xf3\xa6\xc02\x1f\xc8\\\xa1\x19V\xeei\xe0<\x97\x81#L\xb4\xa4\x11\xb7\x1b\xbd\xf8\xa5\xa5\xd8\xd6\xc8\x9d\xa6\x95W\xccX\xd4\xef\xc3\xee\xc1\xc8\xe3\xa7\x15N{\x89Z\r\xca\xe5dC\x9f\x91H\xdd\x9fq\xde\x92\xe8\xc7o$\xa8\xb2\xbaH\x99YU\x87 zu\xefH\xbb$\xd3\xd2U\x90\x8d\xad\xb4\r\xa7 \x9fn\x80\xd2I\x16\xe5a$\xbe\xb8{p<\xe9\x0b\x06\xdd\xb47a\xc5:\xd9\xb0\xc66 \x12\xbbdD*\xc0\x0c\xe7\xf0\xf45\x0c\xc2;G\x1el- \n\x02\x1f\xbb\x8fs\xebOYb\x16\xc0Gen$Q\xbb\xcc\xdc\xec\xed\xfa\xe3\xf4\xa6\xdfC>e\xd4\xb2\xf3%\x94\xb22\x86%\x81DP\x98T=\xcf_N=*[[\xb2\x92\x0f(0\\nU\xdd\xc0\xf6\x18\xaa\xafq\xbe\xc9n#\x82<\x0c),\x9f1\xfa\xe7\xa56\x0b\xe9\x17n\xe8m\x19\xd8r\xe2\x04\'\x1f\x96s\xf5\xa9\xbbh\xafv\xe6\xb8\xbaa\x18\x8fd2\t\x06w0\xc1C\xea\x07\x7f\xaf\xd3\x9a\x8f\xed\xd3J\x89\x14\xf2\xc8\xf0!\xf9T\x9cF\xbf\xe7\xf5\xac\xc9/\xd6Y\xbc\xbb\x9bh\x98\x13\xb4\xed%\x1b\x07\xdc\x1f\xe9V\x1a\x14\xb7\xdd*\xcb\xe6\xda\xaa\x91\x16z\xef\xcfO\xaf\xbf\xa5U\xaf\xb97KckN\xb9\x97L\xd7\xad\xa7\xf9\x83B\xe0\xab\x01\xc1^\xb9\xcd}\x17iz\xb7zl\x17{HYP7\x1e\xf5\xf3\x8f\x864\t|S\xe2[\x0b8\tX\xa1_2\xe6R\xd9*\x9c\x13\xdb\xaex\x02\xbd\x7f_\xf1\x13X\xed\xd3\xb4\xe8\xf1n\x9f\xbb\'\x1d\xc7`j\xd3\xb4L\xa6\xae\xeckk\x96K82\xae\x19x\xe4~u\x8do}-\xad\xe4V\xc7j\x89\x1b$\x81\x93\x81\xdb\xf2\xac\xe95\x1b\xcbK\x88&P\x9eS\x9f\xde\xaepN}\xaa{\xdb\xa8\xeeo-\xbe\xc8\x86Ie\xca\x80\xbdC\xe7\xe5\xfc1\xc5F\xfa\x86\xabC\xa9\xd7lE\xc5\x8cf\x18\xcb9\xc6\x02\x8e\xd5\xe3\x9a\xaf\x8buk\x1b\x96\x89-b\x80!#\xf7\xb1\xe5\x8f\xe7^\xf9\x02\xb2[F\xb2\xe3xQ\xba\xb8o\x1d\xc1\xa6\xc7j\xb7\x17\xb1\xc7"\x868Ld\x93\xfe\x15\xbcc\x1emU\xcc\x9bv\xd1\x9eM?\x8b\xf5\xbb\x84*\xf7(\x14\xff\x00v5\x15\xc9\xeb\x17\x97\xf2~\xf0\x12\xdd\xebr\xe7\xc9y\xd9\xa2\x01#c\x95_A\xe9QyH\xc9\x8c\x83\xeb]\xb1\x85;hcyu8(\x9ef\xbe\x12I\x9e{\x9a\xd0\xd5m\xce\xf1&\x01\xc8\xadk\xcd1\x16A$c\x1f6x\xa6\xea\xb1\'\x96W\x18*\xa0q\xebY\xcdY\x15\x1dNe\x01\xdc21\xe8j\xcc\x84y\x07\x07\x0c\x0fjaBC\x11\xd0u\xa6\x96o)\xbe^=k(\xb2\xda)]\xed\xdc\n\xf7\x1c\xd5\x8d*\xe6\xe3N\xbd\x86\xf2\xdd\xb6\xcb\x13n_\x7fcT\xb9\x92\\u\x19\xad\x10\xbeXM\x9cc\xd0S\xdfql{\xae\x8f\xf1\x02\xc3\xc4v\x11DV;{\xac\xaf\x9d\x01\'\xb7u\xf5\xad]B\xfa\x16\x8c\xae8\x8c\x00A\xea=?:\xf9\xff\x00J\x96[\rV+\xa4_\x99NN\xe1\xda\xbdpj1\\\xc1i"\\.\xf22\xeb\xd7\x808\'5\xc1Z\x92\x84\xb4\xd8\xec\xa5.h\xeb\xb9<\x125\x94w\r\x03\t\x11$\x13\xec\\\xe5s\xf2\x95\xe8\x7f_j\x82K\xaf\xb4\xc6.\xa4!"*\x01\x10\xa1%\xbd:\xfe\xbcUIP\xd9\x85\n\xad\'\x983"\xbb\xf4\xc9\xed\x8e\xd9\xe3\xadY\xd3\x16\x19\xd4\xc8\xd1\xbcq\xb3\xec\xda\x18\xf0q\xc9\x04\xe7\x8a\x86\x92\xd6\xe6\x88\xbfm\x14-q\x08yR`P\xee8\xda\x18\xfap0*-f\x17[\x0f5\x95\x95\x15\xf8)\xd0\x0e\x98\xf7\x15\x1a,V\x8d"!\x191\xe1\x12@9\xcf\xf1\x03\xebU\xa7\xb8\x9e[x\xe0\xf3L\x91\xb6U\x90\x9d\xdb~\x9e\x94\x93\x0eR\x81\x8b\xed\x91+e\x00$\x84\x0cs\xc7nO\x14\x8f4p\xc6\x8c\x91\x91$\xc8\x01\xf9O$\xf7\xfc\xaa\xc4SEj\xf3\xc1*2\xdb:d\xae0\xccx\xeax\xfc\xea\xa2\xcf(\xbaI\x93;\x12=\xbeZ\x9c\xa9\xc7\x07\x91\xc18\xad-tB\xf3#:a\x8dL\x8c\xb3\x97^\x99@\x07N\x87\x9e*h!\xbb\xb6\xb6vh\xc8,\x08\xf2\xdbvG\x1e\xbd8\xe6\xb7\xad5[\x06\xb3!%]\xd2.\x0cA:zd\x91\x91\xeb\x91Q\x9b\xd9e\x9d\x92\\\xc7\x1a\xeeea\x83\xce1\xe9\xc8\xc7\xadG0\xedc\x9d\x84\\]\xc8\x07\xca\xe4\x8e\\t\n>\xa7&\xa4\x9aX&Ke\x9afe\x84\x1d\xc4|\xe0\x0e\xc4c\xa5^\xb7\xbe\x8e\xd6\xfc\xad\xc2\x83\x11O*;\x820\xa0u\xe7\xfd\xac\x9a\x95!\xd3\xa6yY\x19>\xd6[b\x8d\x80\x07\x1bz\x93\xcfC\xc9\xab]\x04\xccP#3I\xe5o\xd8@\x0b\x91\x82\x0f\xf5\xaa\xb2\xa4\x93B\xd0,\xaeU\\\xb1\xdaH\xc9=k\xaa\xb7\xb1Hm\x0e\xa35\xb4\x8d\xb6M\xc3\xcb@C\x90:\xe7\xa61\xff\x00\xeb\xaaw\xdal\x97\xf7\xe9\xa8ZYC\xf6ipX\x06\xc0\xc0\xc6I$\x81\x9e\xb9\xc6\x07\xd6\xa92J\xa2\xf6\xfa\xcaF,\xd1\xb4<\xe0\x90A\x07\xd0\xd4/1\x90\xa3\x90\xbc\x02I\xc6j)L\x92\xec\x8d\x161\x1a}\xe5+\xd0\x7f\x8dWVh]\xa0\x86I@l\x96\xf9\xba\n\x1cK\xb9$\xf2\xac\r\x19Y\x03q\x90z\x80;\xd3\xed\xee\tY\n\x01"\xe7\n\x18\xfd\xe3\x8e\xb5Nx\xc3\xca`\xde  \x83\xf3\xf1\xf3T\xf6q7\x9c\xe5\xe5\x91\x827\xcc\xccr\x14z\x81M\xab\n\xf7\x0b\x91=\xc4\xaa\x9f\xea\xc9L\x10\x07\x18=\xcd \xb6\x82\x13\x85o0\xe3\n\xab\xf2\x95=\xf2OSS\\\xdcCx\x91\xa4p\xed\x10\xc7\x85\xdc\xf9\'\x9f\xbd\x8cTR\xc2\xcbi4\x8c\x1b`P\xc5\xcfB\x0f\xb5;\x87\x99\x1f\xdb\x8aH\xb0\xc9\xb2Lgf\xe1\xcf^\x84\x82*\x01\xe5%\xce\xe8C\xc7\x9c\xed\xd9\x90\x18\xf7>\xb4\xb1\xbcp\x80@wb\xc0\x120\xa4\x03\xd0\xe7\xad:[x\x93a\x8a\xe48\x03iG#p\xf5\xc6:\x8f\xd6\x8b]\n\xf6.H\x1c\xce\xb0;F\xc9\xb4\x01\xb7\xe5\xc6}\xf8\xe7\xf1\xaa\xa4yW^\\\x05e\x080]\x86\x04\x83\xbeFNqU\xcc\xa7\xec\xbc\xe4\xe1\xb2\x14\x1f\x99G\xbe{T\xc73\x98%\x94\xe1\x8b\x04 0\x07>\xcb\x9e~\xb8\xa3\xa8-\x89&\x84C4w\x18\xdb\x11\x00:\x83\xccY\xecs\xfeMEpE\xb2*\xe4\xff\x00y\x18\x96\xe1s\xc6\x01\x1c\xd3\x9a\xed\xedo\xe3\xd8fD\xf3DlC\x11\x9c~\xb4\xeb\x95!\xe4*\xc5\xc8|\x02I<\x9f\xad4\xf5\xb3\x15\xb4\xba\x12\xe5\x9a\xe2\xcfqu\xc4g9\xe0\x01\xf8\x8e\x9fJ\x82\xe4M\r\x9c.\x8a\x85\x07S\x9d\xc1\x8e?\x01P4J\xd1a\x8c\x8f2\x8c+F;\xfa\xd6\x82\x19L\xc6\xd1\xb6\x98\xa4E\xeb\xf3\x0c\xf4\x19\x1d\x05+\xa6\xc1lS8\xb9\x92\x17\x91\xfeg\xc9\x18\xf6\xe9W\xe7\xb6\x89e-"3\xb7\xf0\x85;T\x91\xd3=\xc8\xa1\xa2\x8a\x05X\xc4dG\x1b\xedl\xb0e\x18<q\xee{\n\x9e\xed\x04r\xb8\xba\x90n\x95\x8b\xb7\x96rX\x01\xc2\xf0~_\xf3\xc5R\xd8\x97\xb9J\xe6\x11\x1f\x9a\xe1\x90\xec\xc4\x8eQH\x07\xd0\xf3Tb\xbc\x97rI\xe7\xb2;?\xfa\xb4\x07p\x1fP3\xfezS\xe3\x90"\xc5\xbe\tUd\x0cJ\xb2\xee\xdd\xce0O\xae)a\xb2\x8a\x15o<\x99-\\m\x89\xf7m\xe4\xf4\xcf\x04\xf1\xdcQ\x14\xde\x80\xe5a\xb2\x18\xae\xefb&V\x94\xaa\x81\x92N\xd0s\xd7=I\xfa\xd6\x8bD\x92L\x81\xa4i\x08\x18T\xe3\x03\x15-\xcc>|Q\xab\x8d\xccbX\xc7\x96\x9c\xe7\x1c\x8e\x00\xc1\xc0\x1d\xaa\x91Y\xd8N-\xe2h\xa3\x81v\x99v\xb3\x00}3E\xba\x0e\xfdI\xa4\x94\x96\x9e7\x99\xb1\x1c[\x81\x07\xb6z\x01\x8e\x06j\xad\xbc\xe5-s":G3\x8f\xba\xbb\x89>\xf5e"E\x9c\xc9+\xc6\xee\xf1\xf9N\x81p\x0fL\x13\xefH \x90\xae\x18E\n\x8f\x94\x87$n\xf6\x1f\xe4\xe6\x8b\xe8\r+\x90"\t\' \t\x04K\xf7\x81\xc9$\xfb\xfa\n\xb3:B\xe3\xc8\x93*\xc4\x00\x01\x1c\x8fJh\xd3&\x99\x8a\x8f\x95\xbb\xb9\xe3\xf2\xed\x8az\xc0\xd2\xdd\xa6\x18FA\xeaG\xdf\xcf^{\n\x87\xb9Ka\xd7/;\xd9-\xb8\x8b\xcdt\x05\xd8 \'`\xed\xfeMS\x82\t`\xb4Y\x9a@\x85X\x90\xa5Ac\xf9\xf6\xcdhk6\xc2!\x1f\x94\xf2\xa91\xe6GE$0\xe9\x82G\xf5\xa8\xa0\xb2\x8e\xf1>\xd6\xd1yrF\x84\x12\xa9\xf2\x9cz\x8c\xe38\xee*\x9a\xecM\xc9\xdd\x9eX\xa1L\x82\x81H\xf9\x17.s\xea\x7f\xc8\xaa\xb2^\x99\x18\xb2JUQ\x02\x068\x05{g"\xac\x84I\x1a\x11\xbd\x9a\xe6\\\x1d\x8b\x8cg\xb0\xf5\xcd@t\xe9-\xd8\x0b\xac\xdb\x926\xfd\x9d\xbf\xd6g=X\xf5U\xfa\xe0\x9e\xd56\xd4\xab\xb4\x88|\x8f\x99RI\xdb\x94\xc2\x02\xa5\x86:\x9c\x9e\x95,"\xd5m\xb2\xd3\xb1\x91\x98yJG\x07\x1d\xd8\xf6\x1e\xd5\r\xaa\xabO \x90\xacB7\xc1$\xe7v\x07N\xfc\x7f\x9c\xd4&8\x96\xea9\x9d\xe4\x11\xe5\x8cb2\x00\xf5\xe3\xd7\x93U\xb2!\xea\xcdk\x8bkEt\x9d\x0f\xda\'+\x87\x8eB\xae\xa9\xc7a\x8f\xe7\x9a\xac\xb6\xb0yg~\xe8\x8a\xfc\xc0*\x8c\xfe\xa7\x8a\xa7q4\x92\xc0\x91\xaa\xba3u\xe8\xbb\x86y\xed\xed\xdc\x9a\xb0!\xfbK\xab\xc7\x0c\x81Dx\x1b\x8f\xde#\xa9\xac\xa6\xee\x8aP\xe85-U#1\x99p$\xce\xd6\xc19c\xd7?\x85W\x9a\xda\x18.$t\x8b\xee\xa8\xcf\xcd\xc0\x1d\xcfN*k\xabw\x86H@\x12\xf9*\xbb\xcb\x9c\xed_\xc3\x14\xd9/\xcf\x9f\n\x98\xe0\xb9V\xf9[1\xb0\xfa\x1e\xa3\xf5\xa5\x05u\xa85fb]2\xb9V\x00\x92\xc0\x90H\xed\xd3\x81RAu5\xac\r\x18\x9d\xb6\x9f\x9bi\x19+\xd8v\xef]\x88\xd2\xf4\x8dV\xca\t\xe3\x9f\xc8\x9e0Q"(\x08l\x0e\x99\xaa\xf0\xf8jU\xb6I\xda\xc9\xd9U\x8eT\xa1n\x87\xbe?\xc6\xb6\x8d\xf4H\x99D\xf4\xdf\x81V\x83\xfb\x03Q\xbdt\xfd\xe4\x97\x02\x10\xd8\xec\xa38\x1f\x8bWa\xe3\r)\xee\xf4\xa2\xb0\xe5U\x186Uy\x07\xfc+\x9e\xf8Itc\xd3o,\x1a\x05\x85\x04\xbele@\x00\x920F?\x01^\x8f,I4m\x1c\x83*z\x8a\xd1$\xd1\x8c\x9bR<J\xe2y\xadgh.\x17\xe6\x1c\x92[#\xeb]\xef\x83\xbc;-\xab\xae\xab|\xc7\xcdt\xc4\x11\x9f\xe0\x07\xb9\xf7"\xa7\xba\xf0.\x9fq\xae[\xdem"\x05%\xa5\x8b<9\xec>\x99\xae\xa2@6\x10\x0e\x0fA\xf5\xedR\xa1gq\xb9\xddX\x86\xf6UXYK\x84,\xa7\x06\xbcs\xc4\x9a\x8d\xce\xbb\xaaM\xa7Y.\xe8\xe2?3\xfa\x9f\xadt^1\xf1+\xd9h\xefjI7\xd2\xcaQ\x7f\xd9\x1e\xa3\xf0\xae\x16\xc3W\x86\xc21\xfb\xcc\x96\xc9\x91\xcfW5\\\xfc\xaa\xfdD\xa3r\x9d\xcf\x84\xee\x8e[c\x02\xa3\x9e8\xe9\x9e\xb5\x85qiqi+)\x07\x8a\xf4\xfd\x1b\xc4\xb1\xdcZ\xcd\x1c\xf2\x02L\x98T\xc7\xf0\xd6\x0e\xa7{e\x10\xb8\x06\xdcH\xe3\x84 \xfd\rO5\xdd\xd1~G\r\x0cr\xbc\x8f3\xfd\xc4\x19\x00\xfa\xd6]\xfb\xef\xc8l\xe0\x9c\xe4WE\xa9\xcf\x13\x82\xb6\xeb\xb6<\xf0}Ea\\\xc2vaG\xccMi\xcd\xa5\x89\xb1\x8f"\x15\x01?\xbd\xce}*\x14W\xd8\xfc\x02\x08*3V\xee\x02\xc6\xfeS\x1c\xb9\xe7\x1e\x95r\x08\x92E\xc6\x05$\xf5\x06\xb40-\xacv\xbe\xe6`Oa\x9eO\xe1RL\xfe[\x90\xa0\xef\x1f\xcb\xd6\xaf\xea\x16\xc6\xdc\x86\x8dB\x86\xfb\xc4\x0eMW\x11\xb4\x80/9 \x8c\xfbV\x8brZ%\xb5\\*LU\xbeq\xf3\xf6\xcf\xbdvZU\xe3\xfd\x87\xfdYd\x80d\xe1\x86H\'\xa75\xc6\xc3+\xac&6b@\xc0\x00\x9a\xdf\xd2R\xe2\xe1d\x8e\xd1\xa4YY\x06\x15I\xf9\xb9\xc1\'\xf3\xcdD\xe3x\xea\\%g\xa1\xd5Coysd.\x06\xf5\x88\xe4\xe5w\x15^x\x03\xae?N\xb5h\x8b\x96\x90F\xc8\xe8\xe4\x02\xc4\xae\x1b\xdf\xf0\xab\xde\x05\xbb\xb9X\xe2I\xeen\x96\xc6\xdeG)\'\x96\x0f\xda\x068,\xc7\x903\x9e\x9e\xc2\xba\x8f\x10[\xc1\xa8\xe8\xebu&\xc5\x89s$o\xb4+\x10G\xd3\xbf\xbds\xfb5-MUW\x17k\x1c=\xc4\xb2\x88\x82\x98\xf6\xaa\x9f\x91\x9cc\xf1\xfci\xcb\x1c\x93\xb3\xb8vNT\x07\x03\xbe\xde\x7f\n\xb3{\xa9\x8b;hmZ\xdaV\x88\x05\xdd\x11F\x8d\xca\xf6\xceG\xf2\x1c\xd4\xd0k\xf3-\xbe\xdf\xb3\xb1i\x17a@\xb9\x00\x0c\x0c\xf3\x81S\xecU\xf4f\x9e\xd7MQNx\x95&\x93\xcc\xd9\xbd\x86\xd1\x18^\x14z\x83\xfd+6[8\xe0\xba\x0fl\xe2\x16\x1f1\x87\xacs\x1c`\xe7\x1d\x0f\xeb\xf5\xad\x1b\xcdR;\x81\xfb\xcbx7! \x98\xe2\xf2\xc7\xa0\\\x92A\xfc\rOig\x04o\x01e\x9e$\x9b!B\xb6N\x00\xceNz\xf3I\xc7\x91\xd9\xb1s_S\x1a\x0e\x10+Z\x18.1\xce\xec\x90\xeb\x9cu f\xb5$Y^L\x15U\x11m\x90\xac`\x00\x00\xea\x06}\xbd+f\xe3J[\xb8\xd1\x0e$V\x1dT\xe7\xa7CU\xa4\xb5\x92$\xb8t\x93\x88\xfef\x00\x80\x02\x82\x14\x92;\xd4\xb5\xd4|\xd7\xd0\xc9\x16\xe2(\xee\xf6E\x1e\xf9\xf7\x13\xe68\x1f6z\x8c\xff\x00.*\xa6\x96\xaen\x96\x7f,\x84\xce\xd6\x8fi\n\xfc\x11\x9f\xcb\x8e+n\xe2\x1b\x98\xf4\xf3$\x88\x0f\x94U\xd8\xc4>d\x04g\x1c\xfd\xef\xe7U4\xfb\x8b)$\xf3\xa3<O \xdab\xc2\x91\xea}\xe8\xf5\x1f\xa1j\xcei\x8b\x08Z)V\x14\x1b\x82q\xb4\x8eq\xd2\x89-\x12[\xc7\xb6b\xcb\t*\xde[\x959\xf5=28\xf5\xa9l\xed\xf7\xcb(g\x9b\xcbB\xb1\xb6\xe6+\xbb\x9c\x93\xc7>\x95\xa1\xa7M\x0bAs\xb1\x84f\x14\xc9\xe0\xe7gnM\x0fMI<\xfbN\xb4V\x87\xcc\x97\xcc\x0e\xe7\xfdX;z\xfb\xf6\xaa\xd2\xac\x91JU\x02\x82\x1b\x96Q\x97P;\x06\xf5\xab\ron\xa9\x0c\xcfr\xa5\xd8|\xcb\x18r\x1cw\x07\xd0g\xbf\x19\xabv\xd1\x86\xd5Wz.#M\xfeP\xce\x05n\xe5bR\xb9E\x16/5\x92C#\xb0_\x9d\xf8\xc9\xfaf\x92\xde$\x80\xcc\xf6\xb2\xcaSw\x0c\xaaC`\xfa\x8eF*[\x87\x03\xcc\xda\xc0&\xe2x\xecz\x9f\xa55\xd6\xe2\xd8eJ\x81(\xca\xb1\xdd\x85>\xa4c\xb5K\x96\xa3KA\xd0\xaecD\x91Y$c\xf3\xecm\xbbT\xf4\x1e\xc4\xfaS\xf7\xb2LQ\xc6!\xdaF\xf6\x04\x86\xf6<U{h\x9dQ\x8c\xf2,\x8b\x19\x05H\x1f\xc5\x9e\xdf\xe3Zv\x96\xd3\xce\xae-\xf0\xe4\x8d\xec\xb2:\xf2;\xe3\x9aRi\xbd\x01\x14-\x17}\x90\x93b\xecS\x8d\xaa>\xe0\xed\xc9\xe9U\xa4\x8e\xda[\xb7h\xfc\xc5\x1f(\xe4\x82I\xc7Q\xdb\x19\xab\xb2Y\xdeX\xc9-\xb3\xd9<6\xee7\x99\x1d\x8e\x14u\xcf\xa6*\xbd\xab\xc1=\xdd\xbb\x0cL1\xcb\xae={\xd4\xdd\xdbA\xbb\x0c\x9bM\xb8\xba\xb5i\x16W\x12#\x03\xb1\t\xfc)d\xb4C\x1c-$8(\xa3x\xdeIw\xee\xd9#\x83[p92\xccC\xba)nr\xdf{\x14\xdb\x9bu7\x04\x19\x1fc\x8d\xc1\x15\x17\x00\xfeU*w~bZ\x18\xa05\xd0;]y!v98\r\xea\x0f\xaf\xb5O\x05\xda\xa1\xc4\xee\x0exU \x9f3\x1cv\xebQ\xdcZ\x85\x8d\xfc\x99v\xfc\xb9ef\xf9\x8b\x7f\xbay\xe9RX\\#\x91\x1b\xc3\xb8\xc4\x9c\x96 \xf5\xe3\x03\x1d\xfd\xabG\x1b\xad\x05\xcdgr\xbc\xdaPG\x17\x16\x97\x02H\x9f%\x83q\x80;\xf3\xde\x9c!U\xb5fu*\xce\xd8\xf3Y\xb3\x8f|w\xab\xb7V\xd7V\xb1\x95A\x14\xb06\x0f\xcav\x14\xf5\xdd\x91\xfc\xb3T\xc5\xd4iq\x1a:J\xe7\xcc\xc9i\x06\x06\x7f\xfa\xddqIE\xab6J\xb5\xd9j\xeeI-\xe4\x8a0\x8c\xcb4a\xc4\x84\x81\xc1\xf5\xe2\xab\x9f6\t\x929\x84q\x9cq\xb1A\xe4\xfbV\xbd\xcd\xa2\xb5\xd7\x9a S >PW,#<u\xc0\xc9\x06\xab\xc3\xe7O\xfb\x89"\xdb,g\x08\xa1\x88R\x83\xb0c\x83\xfc\xebW\xa8\x93\xb1\x97\x0c\xabqq-\xb5\xdd\xbc{S?4\x7f#\x00F:\x0e\x0f\x1d\xaaA\xa6=\x9blyG\x97.v\xcd\x9c\xe0\x01\xc6\xd1\xdb\xeb\xd6\xb6%\xd3\xed\xb0.@$\xa3\x0eK}\xd3\xdc\x91\xfe5_R\x80<\xe1\xa4iHUPI\\\x83\x9ex<c\xf0\xac\xee\xd2)+\xbdHf\xbc\x9a(<\x81.\xe9R \xcc20\xdf\x8fsQ\x97c\x01\x91\x18E\xbc\xee\x01O\x19\xee1V\xde\x08w\xb1h\x08I\x04`"G\xc8\'<\x82x\xedTC;\x89<\xceac\x95\x1c\x06\x07\xa68\xa9rw-+\x11\xacRM2\xab:*\xb1\xdc\x19q\x92s\xdc\x7f\xf5\xea\xf5\xbd\xa2\xcd?\x9a\xc27\x9d$\xc2\xa3r\x07\xb69\xcdg;4l\xa4\xb4cp\xc2\x80\xdb\xb6\xfe"\xae]2\xe9\xd8D\x92\x19#\xda%\x12\x8e\xacO\xb6\x018\xab\x8b&H\xd1\xd4t\xd9\xda\xdfw\x99\x17\x9b\x0f,\x8a\xdd\xbd\x94v\xac\xe8\x9d\x9d\x9eV\x8c\xecP0\xcazv\xfdj\xf5\x8e\xa38\xb0}\xb1\xaa\x19#;\x98*3\xb7\xe3\xdb\xe9\xd6\xa2\x91\x929\x06#\x92\x08\xe5\x7f\x99\\c t\xad\xe6\xb4\xba3\x8b\xd6\xccm\xd4\x9b\x1a"\x8e\xa5\xb1\xf2\xa8<`\x8e\x9e\xf5^;\x93ez\xce-wJS\x84\r\xb0(=I#\x91\xf4\x1c\x9a\xbf\xa9\xa8\xb7\xb7I\x18d\x12DE0\x19\xbf\xd9\x15O\xf7Ft\x9e\xe1\xd6M\xaa\x19\xdbc\x03\x1f\x1c\x0e\xa3\xf3\xefI\xb5\xa0\xedt[\x81m\xbe\xd8\x97RO2o%\xb0\x9f\'nG<\x91W\xf5\x1d5bI/\x17\x1e^\xe0\xa0\x05 \xb6G\x18\xff\x00\xf5\xd5\x05\x95.\xed\xe2qk#\xc5\xceT\xca\xaa\xab\x8e\x98\x04d\xe7\xbfZn\xa92\xdc\xdc\x1bq\x14`&\xd61\xf5\x1f\x9e:\xd3\xe5\x83~b\xbc\x91\xa3c\xe1K-B\xf4\xc7}x-\x90[\t\xa7\x84\xce#,wp\x18\x9e\xad\xc7AT\xbcC\xa1\xd9[#\xdc\xc6\xf7(\x80l\x8a6}\xae\x10g\xe6\xc9\x04\x01\xd7\x9cf\x8bK\xcb\x8d"\xe2y\xd8Z\xa3\x8cyD!s\x1f\x7fnMB\xba\xac\x93[\xe2U\x90+opJ\xee-\xf9\xf4\x14IC`\\\xcfR\x1b{;{{\x1bGW\xca2\x91\xf3\x10I^\xa3\x9a\x9a),\xe1\xb4d\xb6\x8c(=\x15\x14\x9f\xc3\xe9D\xc6\xc6{5X\x03\x83\x0ce\x87\x98\xb9\x0cO`\x00\xe2\xacG\xa5I(\x19\xdf/\xee\xb7\xbb\xf0\x821\x8e\x9f1\xf6\xae)\xd1\x96\xfb\x9b\xa9#>kQ}\xe5\xa3f%Q\x8c\x90?\x0f\xf1\xa8\xe0\xd1\xadl^\xd7\xcd\xba\xfbJ\x01\x92\x08\xdb\xb4\x9c\xe7<\xf3\xd6\xb6-t\xbb\xb9-\xcd\xdc\xab" \x18\x1b\x86\x15W\xb1\'\xd6\xa3\x9a\xd0\xf9\xf1\\0\x00\x06\xe4\x1e\xde\x87\x15\x9b\x94\xa2\xb9X\xf4z\x8b\xfd\x97\x00Y\x12\xdd@f9M\xa7\xe5\x07\x1f\xca\xa5\xb4\xb8\xb9\xd2\xc8H\xe4x\xb7\x80\xa5\x01\x01O\xb9\xf6\xadM>71B\xf7-\x16\xd92R@F\xe7#\xfd\x9fn\x95q\xf4g\x91d!<\xc0\xce\x02\xc8F\x0f \xe4z\xf5\xe9Q\x1ek\xdd2\x9bV*\xf8~\xe6{\x0b\xfb\x8b\xc5x\xa1\x93\x03(r\x01=p\xa3\xa0\xfa\n\xf6\x9d#R\x8bU\xd3b\xbb\x88\x8f\x98a\x809\xda\xdd\xc5x\xc6\x99\xa5\x1b(\xe7\x9e\xe6A).R \xe7\x016\xa6N}\xc7\xeb]g\x87uv\xd1\xee\xd8\xb6\xe6\xb2x\xd7r\xaf?\xf01]\xb4\xea\xd9\xdaG=H]]\x1e\x91\\O\x8e<mm\xe1\xdf"\x0co\x99\xe6S\xd7\x8c)\x04\xd7P\xf2[\xea\x16\x81\xe0\xbe)\x1c\x8a\x08x\x98d\xaf\xb7\xa5y\x17\x89\xf4\xebtK\x83\x1c2O,-\x98\xe6\x98\xee\xe3\xd75\xbb}\x0c\x12\xeef:\xdd\xf8\x8bU\xba\xbd\x9dx+\xfb\x92\xc7\x00/\x1c\x9fN*\x8d\xdf\x87\xa5\x99\x88\xb6\x1b\xd0\x92U\x90dq\xcfZ\xd7\xf0\xf7\x91\xa9ho;\x83\xe7B\xe1\\c\x03\x8e\x9c{\xd6\xecs\\H\xde\\\x8d\xf2\x18B\xa8\x1cl\x04\xfd\xd0;f\xb9]MZ\x91\xd3\xc8\xed\xa1\xe7)g{i*\xb8\x07 \xee\x03\xd4S\x16+\x89\x1c#\x02_\xf9\xd7~\xd6\xa6x\xbe\xd0c\x01\x16M\x8c\x15{\xe7\x0b\xcf\xa7#\x8a\x88\xe9\x0b\x1d\xd8\xc2\xa9r2\x0e2\t\xfc)s\xa0\xe5g\x08ldc\x8d\xa4\x8c\xe2\xa0\x92\xd5\x11\xca\xb1\x01\x87\x18=\xab\xd5\xb4{\x0bK\xeb\xd7\xb0\xb9\x89\x02\xc4\x8b9u8iAb1\xf4\x18\xe7\x1e\xa2\xb0<Y\xe1\x18\xa4\xbf\xb8\x9e\xcah\xa3\xb7\x88/\xee\x87\x18>\x83\xf2\xad\xd3\xd2\xe6o\xb1\xe5\x17:i\x92\xf8\xb8\xebR\xc1l\xf1\xe4\x11]4VKj\xce\xd2\'\x99"\xf0A\xe0\x0fA\x9e\xe7\xd8S\xdfGiX\x95\x19\xc1\xe4\x0e\xe6\xa2R\xd7A\xa4r\xf7p\xf9\xb02\x91\xf45\x8e\xcd\xb1\x96\x16\xe4\xafL\xf6\xae\xb6\xf6\xcd\xa3\xe0\x8c\x01\\\xdd\xfd\xa1.\xb2\x01\xf7y\xe2\xb7\xa7+\xa2$\xacB\x00\xcfNk\xb8\xf0\x04^v\xa37\xcb\xf7`$\xf0z~\x15\xc3\x06 \x82+\xbf\xf8s\x95\xd72\xad \xf9y\xf2\xce8\xaa\xa8\xfd\xd6(\xab\xb4v\x0f`\xd6ZjF\x8b&\xd0\xe1!S\x92\x00\xc1\xab\x1a\x8d\xdd\xbcZ,\xc9%\xd4\x9bXmEu\xda\xe4c\x9c\x83\xd3\xfc+\xa0\xd4\xe4MCO\x8a!\'\x97p\x92+lnw\x003\x8e\xbe\x95\xe7\x9e\'\x84\\\xea\xb0[\x04\xf9\x06\x1ao\x98\x92\xcc{\x0f\xc3\xf9W2\x92\xdd3NW}H-u\x08\xedt\xbbsp\xd1\xbblX\xbc\xc9\x18\x96\x0b\xdb\x03<\xf5\xc7\xd2\xb4\x86\xb7gm2\xc4\xf6\xaf!#ce\x02c\xb8\xc6;z\xe3\x1e\xf5\xcc\x7f`\xa4\xd7\x0c\x16\xe4\xabGq\xb1\x0b\x91\xb5@Q\xc8\xfc\x7f\x95_\xd64}\xda\x9d\xbd\xef\xda\x84I\xb5\x96B\xeaO\xcd\x91\xc8\xed\x80?\x95[\xe6\x8e\x83\xbaf\xd0\xd5\xa0\x87Z\x16\xb3\xa2\xa6\xe9\x15c\x18\x04\xa9=x\x1d\xbd\xea\xe5\xd4p\x93\x8c\xaca\x15\xe4\x0b\x8coS\xd0\x0c\x9e\t5\x1d\x96\x9fiv\xcd\xa8\\E$\x922\x13\x02\x85\xc3c=On\xbc\xe0\xf6\xabz\x9f\xfaM\xd9D1f(\x898p~^\x08\xc7\x1f\xa5r\xceNOSU\xa1B\xd7Rq\x0f\x93;\xc672\xc4\x93I\xc2\xa7S\x96\xc7~x\xf5\xa3V\xbc\xb7\xfb,\x80J\x86I\x13\xef&A#8\xca\xf4\xeb\xd3\x15\x15\xbc\re*|\x90O\xbd\xc9\x90\x1c\xbf\x1d\x9bo\xaf\xa5g\xcf\xa7NukD\xb7\x99@\x8c\xb4\xa8\xb2\xb9\xc2\x1e\xa0\x01\xeb\xd0\xf08\xa0j\xc7I\xa7\xed\xb8\xd3\xce\xe4V\x91A\x06\x06\x1c\xc6W\x8c\x10y\xc9\x19$\xf4\xac\xddO\xc3\x8a\x977-\x02$-\x1b#\xc4\xca\x07\x1b\x87|u\x00\xe6\xa6\x8aY\xeduxn.me\x0e\xdb\x9aY\x8f\xce\nz\xe4r\x06}EK7\x89-&\xbe\x04M\xfb\x94\x02)IRN;7\xd34\xb5\x04K\xb5$\x9a\x14\x9d\xd4\xcc#\x1ef\xc5\xf9O\xa1\xc9\xf75r}\x96\x972\xf9\xa5w\x88\xd9P\x95\xf9J\xf5\x00\xfa\xf4\xack\x8dA\xc4\xeb\x14R\xc3v\xcaUc\xec\x1czg\xa61R\xdd\xde\x99c\x8f\xed\x16\xab\x80\xd8\x106\x0bE\x8fRx\xe4\x9e\xbfJrvBKS\x96\xb6\xb4\xb3KH\x86\xd9Y[.\xceGV\xf7\xc7=\xba\xd5\xdb\x83\x14\x01o"\x89RV\x8bb\xb2\x8c\x97\x1d\xc0\x1f\xd4\xd41\xbbYB\xdeI\x0f\x18B\xd2*e]\x01\xe8F\xee\xde\xd5R6MN\x19#\x17\x0571 \xab\xfc\xee\x7f\xba02\x07c[[\xb9\x9d\xfb\x0f\xb8y<\xeb\x97[E\x13\xb6\x04j\xed\xb8\x93\x8e2\xbe\x94\xd9\xe2\xba\x08\xa9"\xc6\xf2:\xfc\xd8\n\x18z\x90Fq\xe9\x8a\xaa\xbakXyb\xd83<\xc0\xe0\xa9$\xb9\x07\xee\x95\xcf\x03\xeb[v\xec\xb7Z<\x92\xc6\x0e\xed\xf8%@\xc8#\x8cv\xe2\xb3\x94\x92\xf45Z\x9c\xf4\x97W>tn\x01\x89\xa1\xccnDk\xb3gnq\xd7\xf0\xad{&\x8b\xc8\x13@\xa1\x93v|\xc5\x18#\xe8;\x8a\xcd\xbdI\x9e\xcb\xcbeU\xd8\xff\x003\xaa|\xe3\x9e\x9cg\x9e\xc38\xa6}\x91\x96\xf27T\x06,\x0f17c\x8e\xe3\x18\x1c\xfd+K-\x08gC5\xd4W\x16\xcfow\xfe\x97\x1c\xc8R5\r\xe50=r\x00\x07?J\xe5\xed4\xb3\xa6K;\x17\xccl~H\xdb\xe5 \x9e\xf5\xb2lE\xde\x98XF\xb1I\x04\xbf&\xc1\xc9\xee\t=O\x154\xaaoYD\x88c\xda\xbb\\\xed;\xdb\xdfwaUt\x97)6oQl\xa0\x8b\xc9\x0c\x0c\xa8\xec\x9b\xdc\x86\xe8~\x86\x90F\x86_\xb3\x89N\xf1\xf7\x17\x00\xf5\xeaq\xde\xa5\x8e\xdc\xc2\xae\xce\xf1\xab\xa1\xca\xb6FXzsDR-\xc3\x89C\xca\xd1\xac\xc5ILa\xb8\xe8*#\xbb\x1b\xd8\xc6\xb8\x9b\xc8\x9fi\xb8S\xb7\xe6V\x006\x0eq\xc0\xcf\x1f\x9d>\xf5\x12\xe3e\x9d\xbc\xac\xca\xd2\x00w\xed\x04\xfd03\xd7\xd6\x96\xee\t$\xbeY\xa1\xb4Q\x14\x07y\xdc3\x93\xff\x00\xd6\xab\x90\xcbf\xa3}\xe6\xd43s\x89\x13\xa9\xfe\x9e\xd5\xa5\xec\xaeM\xae\x06\t\xb4\xf8L$\xaa\xbeB\xa0\xdap3\xfa\x1e=jYt\xf4\xb8\x8f\xcf\x86\x11!\x8c\xf9\x8f\t\x18l\xff\x00{$\xff\x00*e\xb9\x068\xfc\xf0\xadl\x0eDm)\xf9A\xe9\xf3z\xfbV\xe6\x93<Kq\x10\x8c\t\t;\x99Y\xf3\xbb\xdb?\xe4\xd6z\xa2\xb414\xfbk\x9b\xdd\xb3O#\xf9{\x8cr0Q\xca\x9e\xe4\xf3\x86S\xc7AZ\rn\x82\xe0Z\xf9\xa2\xe5\xad\xa3\xcbJ\x10a\x87b\x07s\x8a\xe8.\xad#\x93P\x91\xec\x8ck\t\x00K\x1b\x1077\xaf\xf8\xd6LP\x95\x92\xea\xe1\x07\xcf\x01\xd9\xe5z\xfa\x8e:SR&\xc2\xa5\xba=\x8c\xd9]\xb1HOE\xfb\xa3\xa05\x93\xa8K\x14\x96\x92*\xee\x95\x14\x81+\xb66\xa9\xed\xd3\xe9\xc0\x15\xa25!a\xbeP\x04\x88[\x84f\x1f6GJ\x88F\xbfa\x92h\xa2A\x14\x83s\xc606\x1e\xc7\xdcT_\xb9V(;\xb4\xb6\x08c\x99"v\x88\r\x81\xb0wt\xcf\xe5\xfc\xeb"h\xfc\xc5{b\xc23\xb3\xe5wl\xe4\x03\xea+f\xea\x056q\xbb\xab\x9br\xa4\xb3\xa3}\xd3\xd7\xf1\xfck.(\x11n$%\x8e\xe6@v\xb7\xcd\x91\xd7\x8fn(I=JE\x1f\xb3\x12\x81\xd0n\x9dW<G\xc0\xc7_`*\xdaH\xc7li\x18$u~\x98=r}x\xedQ\xdcI<Lo\x08\xd9\x0c\xacQW\x188\x1d\xcf\xff\x00Z\xab,\xb2\xc1<\x80\xb30\xfe\xf3&\xec\x1cv\x07\xaf\xd7\xb5]\xad\xa0\xba\x13Z\xc9\x1a\\L\x8c\xe18\x19\x81P\xa99=Cv\xfe\xb5<\xb7\xc1#\x90\x1bX@~R[\x90p\xbc\xf4\xc0\x07&\x9dgf\x8df\xb7\\3\x99~\xe8n\xb8\x1c\x93\x9a\xb1mlO\x9a\xacw#Hv1\x18\n\x9dx\xfcjUf\xaf\xca\x1e\xce\xfb\x93L\xad<\x82\x18\xe4\x8a[\x89\n\x13\xbb!T\x900G@\x07\xe5Z:\x86\x94\x96\xd2[ys#\x8b\x8d\xb1\x1d\xeb\xb4.;\xe39\xeb\x9e\xa6\xb3n\xb4\xc8\xd5\x04\xeb\xe7\x98\x8a\x00\xe0\xb8\xc7\x1c\x102\t\xfdj$\xb6\xb8\xb2\xb6\x02\x06\xf3-\xf0\x18\xa9m\xec\xbd\xc0\xcfc\xf9SR\xe6H\x1a\xb3.1S7\x95+ \n\xc5`W\x19\xf3\x0fs\x8fL\xf7\xf6\xa9.a\xb9\xfe\xd6\x95\xed\xee\x144\x81v\xae\xcc\xa9\xf9q\xd0\x8cV}\xb5\xd4k;\xde<RM+p\xc6\x14\r\xe5\x8fA\x92\x07\xbe3\x8c\xd6\xf5\xc6\xbb\x01\xb2K\xa8\xedn!\xb7\x89B\xc93\x01\xdf\xa0\'\xa0\xfa\n\xa8>W`\x92\xbe\xa6A\x8e6\xb4\xb9IC,\xe2P\xce\xaaF\xd0:d\x9cq\xf4\x1e\x95M-\x02I\x15\xa2\xc8\xab8\xcc\x8c\xf2(\xd9\xb4\x9c\x0f\xaf\xd2\xba\x02\xb3]h\xd3\xddGoo\xb9\xe6T\xdf!\'\ny\xca\x8e\x01<\xf7\x15\x95\x05\xcd\xda\xdf\xfd\x92R0\x0e\xef8\xb9\xda\x0e3\xb7\xff\x00\xd7\xebT\xdabWE\x9f\xb3\xc3kl\x16DY<\x86\x0e\xac27\xb18\xc61\xef\x9a\xd1\xdbnu\x17\xb9f"!"\x04\x89\x01m\xed\x8cr\xbd\xfaUh\xa0\xb8[\xc5H%C#\xb6\x03`\x9d\x839$v\xe7\xa6kf\x0b\x85\xb5\xb3\x96\x18\x95RF\x94.p>\\\x1e\x83\x9e\x9f\x90\xac\xdc\xec\xefr\xadr\xe5\xee\xa7oz\xb1\xe9\xc2\xeaE\x99\xce\x15&\x1b\x1c\x0e\xa4\xf3\xcfAQG\xa3\xc1\x7f\x0cb\xd0NbR3<\x92\xe3~8;{\x91\xd7\x9e\xfd\xaaO\xb6\xda\xc9\x14\x91\xc9\x1a\xb2\xb2\xedvp\x0e\xe6>\xb5\x98\xda\xa9\xb6W\x0b!\xf2\x17\xe5\x05\xa4\xe9\x8e\xdc\xf4\x1e\xf9\xe2\xb3\x94\xdc\x81D\xe9M\x846v\xc6(v\xa4\x19.\xc0\x7f\x11\xfa\xfdk\x9e\x93[\x9a\xe2\xdaK=*8\xa5\xbcW\xdaK\xbe\xc5F\xc7\xf7\xb1W,\xef\xce\xaf\x0bD\xb3\x14\x89\x94\x86l\x03\xbc\xf6\x0b\xe8=\xff\x00\xfduN\xd7G\xd3\xd1\x9d^\x08\x9aV\xdb\xef\x91\x9e;\xf0\x7fZ\x95d\xee\xc7f\x10\xdb\\H\xa2\x0b\xb7\xbb\xce\x02\x94\x8f\xcb\xf2\xd8\xe7$\x1c6\xe3\xf5\'\x9e+^U\xb8B\xab\x0f\x95h\xa4\x1d\x87v\xf7\xe3\xd4\x1e3\xf8\x9a\xc4\x92\xd1\xad\xaf|\xe3q.\xec\xe1\x03>U1\xe8\x07\xa51\xef\xe5\x0e\xb1\x93\x99K\xb6\xed\xfc\xfa`b\x95\xc7c~d6\xb6\xd0\xb5\xb5\xdb\x80\x1f\xf8\xb3\x82\xdd\xf1\x8e\x00\xf4\x03\x14\xdb\x8b\xd7x\x0cw\x10>\xc6\xc9.\x14\xb6~\xb5\x87\xfd\xa9\xe6Y\xdc:\xbb\x11o!9c\xf7A;H\x03\xf0\xab\x96:\x84\xb2\xb1h\xdf\xfd`!Fy\x00\x0e\xe7\xb58\xd4h\x99SM\x12xF\xde\xd9%\xbeR\x06\xd6p\xc4\x00B\x91\xedZW\x8do\x06\xa0\xd0I\x84Ir\x11\xc78\xc7 \x9a\x88\xeaJ\xd7Q"\xc6Ue\x8c\x91\x81\xc0 \xe0\x9f\xcf\xb5bj\xf6\xd2-\xdc\xd0\xc4\xcf\xbaI\xf8T?\xc3\x8c\x83\x9e\xdd\xe9Jwm\xb1\xa8\xdbC~H#\x16\xa6\x19d\xd9\x1c\x8a\x10\xb6x\x07<\x11\xf8\xd4\xd6r5\xad\xa41\xdcJ^\xe2\x16\xf2\x8eG\x0c9\x1b\x87\xb1\xae\x1d\x97X\xb8\xfbM\x94\xd3\xcc\xb1\xe0*4\x11\x16 \x7f\xb7\xb7\x9c\x9f\xc4T\xd3G\xad\t-\xa0\x13_^\xec*\x13\xcc\xb5\x11F\x84u\x00\x91\x91\xf8\xf1Ug\xdc:\x9d5\xac\xd1\xd8\xf8\x92\xd2\xee\xe6\xf5\xe3W\x89\xe3\x91\x91\x141\xee\xa0\x0c\x1c\xe4\xe4b\xb0|M5\xc4\xb7&xY\x95&\x1b\x99\x10r\x99\xecX\xf1\x91\xdf\x1d*;\x89\xa6\xd4\xaet\xb8u+(\x12\x18\xa7\xc3\x02Ir\xd8!I\xe3\x18$t\x1d\xf1]\x0f\x8bt\x98%\xf0\xfc\x17\xf6\xeab\x965\t(B@\x07\xe88\xcdi\x07\xa7)\x13Z\xdc\xe2!\x8c\xb0\x8dOD\x1c}}y\xef\xef]6\x97\x02\x8b|\x84\xcb\x9ezv\xaej\xdd^5Ugf\xe7\xef7Z\xedl\xca\xae\x93\x0c\x8a\xbb\x81\x1bI\xee\x1b&\xa9\xbeX\\\x94\xaf+\x15\xb5\x0f\n\x1dN\xd1\xe4\xb7EY\xd1rS?{\xde\xbc\xb7Q\xb0ku\x92\']\xa5[h\x1e\x95\xed0=\xf4\r\xe7\x17\x8a@\xf1\xe1\x83!\x04\xa1\x04q\xb7\xaf>\xd5\xca\xf8\x9f\xc2\x97W\x01\xb5\xbb+b\xf0\xc8\x81\x9e?4\x17<}\xect\xeb\xda\xaa\x8c\xaf\xb95\x15\x8f%h\x0cEw\xf4<Wg\xe0\xcb\xa8\xec\xf5"\x15$\xdc\xc9\x83\x822=\xff\x00*\xc3\xfb#\xcbr\x19\xb1\x1c*\xdbCI\x85\xe7\xbfZ\xeb\xfc\x0f\xa6C{\xab\xc9l&,\x15w3d|\xe3\xa7\x18&\xb7\xa8\xd3\x83&\n\xcc\xe8F\xb4\x8dp\x10(.\x9f6\xe6\x93\x01:\xe3&\xa2\xbd\xb4\x0c\x97R\xac\xd1\xcf!A!.\xa0\x82{\x81\xd8v\xad\x0b\xbd\x06\x04I\x922!\x92\x106c\x8f0g8>\xa0\xe7\xf9\xd6\r\xcaK\xa5$wRZ$\x99\xccjm[#-\xc8\x051\x92A\xfa\xd7\x12J\xfa\x1d\r\xdd\x17\xc5\xad\xa5\x95\x94R\xcfp\x05\xc4\x93.\xf6\xdd\xcc\x87#\xd7\xf0\xfaU\xcdj\xccK\x7f\x15\xb5\xbb\x19\xae$\xe5\xe3P\x1b\n\x1b\x9cg\xa0\xc8\xeaz\xd5g\xcd\xd5\xdc\xf6\xf6\xd0\t\xe4P\xb7\x06K\x88\x8a\xfc\xe0\x7f\x168\x00v\x03\x9ex\xef[\x16\xe6\xd6\xd2\xcaxn\x1c,\xac<\xe9&n\x0b\xb7P\x17\x93\xd3\xa0\x157\x19\x1cn\x03E\x03!\x12\x02c\xe0`\x878\xeb\x8f\xc0q\xe9Y\xba\x9d\xd8\x9e\xe7\xca0K\xe6D\x8d\xbb\xcbR\xc5\x8e>\xe9\'\xbf\x06\xa4\xb7\xfbe\xdd\xc0Y\xd0,\xe21 E;s\xddK{\xfd2)-\xee \xbbk\x8b\x9b\xe9M\xbc\xaf\x89<\xb6]\xa5\xca\x9c\x11\x9e\xdczu\xefJ\xc32\xec\xa7\x98\xc0\x8f)C\xf2\x95vc\xf7C\x1e>\xa3\x03\xb5\x177p\xc9\xadZ\xdc\xcf\x02\xfe\xef\x18\xc1\'h\xe9\x9c\x0e\xb4\xe8\x94\r\x12k\xfbx\xfc\xcc\xdc\x95;\xd4o@\x0f`GA\xe9U\xa2\x82\xf7\xecF\xed\xd6\x06\x8d\x8f\x94\x8a\xf2m+\xc9*I\x1f\x9f\xb54\xb5\xb0i\xb9\xd0]j0\x9dF\xc6Hn\x9f\xcc\x04\xc6Si\x01\x868\xe7\x8a\x9a;\xbd6\x1b\xcb\xb5k\x95Xn\x18y\x88\xd3\x06e\x00\xe3\xa8<\x8f\xa5a\xea\n\'\x0b|\xd8\x86\xe5"V\x11\x86\xf9K\x8e\xa4\x1e\xfd\xbaR\xad\xcc\xf2h\xf7W\n\xf1\x95h\xfeeU\xc7\xcf\xe9\xc78\xcd\t\x01>\xb8\xbaT\xb1Z\\\xa3\xb6S\x11\\4hs\x8f\xe19 \x0c\x8fJ\xcd\xd4\xa3\xbe\xb6\xb0t\x9fl\x91\xb6\xd1\x08lI*\xc7\xcf!\x87\x1c\x9c\xf6\xcdU\xd5\xb5Y5\x1b!\x1c\x892\xcb\n(\xb9\xca`\x16\x1cg\xdb\x8a\xb5a\xaa\xc3q4\x16w\x92#\xcf\x1ca v]\xc7\x00|\xaaGL\xf3\xcfZ\xbe\x82\xf3\x1c\xd2B\x90\xb0\x93\xec\xc6\xe24(\xea\xc80T\xf7Q\xc1\xcf\xe7U\xad\xf4\xef*\xf2]BC\x1amE\xdac\xe0\x93\xfd\xecc\x8a\xc2\xce\x9bwnc\xbf\xd9\xa7\xde\xc7\xff\x00,\xae\xbe]\xe7\xd9\xba\x11\xf8\xd7Wd\x97\xb7Zu\x9c\x97Q\xa4\xcb"\x892\x87\x80q\xc0\xc0\xf6\xc5\\\xf4FkVW\xbc\xb1\x12hI\xa8G\xb0\xdc#\x17fq\xcb\x02q\xd3\xaeqU\xb4\xd6\xfb;\xcabY%\x86@\xae\x19\xe3\xd9\x9c\x1fL\x0e\x7f\n\xd6*\xb1Z\xc8\'I"bv\x00\x08\x03\x03\xdb\xb1\xa6[\x95\xb4\xbe\x96)\xd4\x9bp\x9b\xe02\x00\xcd\xbb\xd3\x83X7r\xd6\x86F\xbbs\x0c\x97fkiW\xcat\r#\x14\xca\xb1\x1d\xf8\xff\x00\xebSu%\x96k\x08_\xca\xf2]W\xcc&$\xfb\xcaz\x1e\xa7\x156\xb9\xa6D\xf6f(\xcc~|\xb2+nW\xda\x13=P\xd6h\xd5>\xc3\x1f\xd9n\x1aeO\xf5yp\xc3\xfe\x03\xbb\xa1\x1d\xf3\x9a\xd5wB\xf2-\xdbI\x11X\xe5H\x83DT\x03\x19\x93\x18|}\xef\x7f\xa7z\xb9\x1a\x8b\xb9\xa4/#"\xf9\\\xa0\xeb\xbb\xb6{b\xb2 \x98Cr\xd6v\xac\xbb\n\xee\x0c\xdf7\x1dx\x03\xa9\xa6}\xb7\xc9\xbc\x9a\xe6 \xe7\x0c\x81\xa2l\x85\x9b\xd79\xe4\x1f\xa56\xfa\x82\xb6\xc6\xdd\xc5\xba\xa5\x84Q\xb3\xc4\xee\xa4\xf9\x8e\xaaO9\xe3\x83\xc5gAreX\xad\xa4\x91\xa2\xf2\xd7\x80\x13qnx8\xcf\xbdUK\xb7\x91\xe5fh\xe2\xb6\x97\xe5\x05e,\x10\xfac\xb8\xa8\xdaX\xa2\x9e9\x19\xa50\xb0*\x16>\x19\x8e:\x1ct\x06\x9cWV&Iuw\xe5,\xa9\x1fI\x1bkJ\x0fo`x\xabV\x9fgM9\xa4\x92s$\xd2\x82\x15\x88\r\x90\xa38\xdczq\xf9V\x7f\xdac\xb7x\xc9\xb4A\x1a\x92\xbbI\x04`\x8e~\x94\xc9&\x83\xcd\x80A\xb1\x91N\xc2T\x92@\xeb\x8c\x1e\x07=\xaa\x9c\x85c\xa4\x8aH\xe0\xb4\xb6\xb5\xb4\x9a\x15f\x05\x99\x82\xe5T\xe7\'\x1cs\x8c\xfbS \xb8\xb8\xb1\x91^u"\x06%F\xd9\x00|\xf78\xf4\xfc*\x1bx\xc9\x93\x11\xc0\x8e\xa4\x83\xe8@\xef\x9cc\xbf\xb5Z\x82\x7f5\x96;\xdbG\x85S*p\xbf3\xfa\x0c\x7fZ\x8ek\x8e\xd67\xed6\xc0\x86@\x9eY\x10\x9f\xde&\x08#\xdc\x9c\xf4\xf7\xaa\nm\xed\xad\xdc\xc0\xb8f\x84\xb0\x90\x1eI>\xb8\xeaM$\x02I.~\xcfu$\x82\x06\xc2E\xb9\x06\x02\x8eNrF\x7f*\x92kh\xfc\xdf:\x16\x84\x06\x060\x178\x18\xf6=?\n\xcd\xe84r\xf1*M~\xc03\x89\t\x05C.\xdc\xfa\xf5\xed\xd6\xb5\xda\x7f\xb0d\x8d\xa9\x1f;@\x19\x04\xf7\x06\xa0\xb0\x8a3\xad\\3\xed\xdcSl\xa1\x0fNz\x81\xd8U\xfdF\xd1\xbe\xcc\x02&a\x03\x9f\x9b\xaf\xb8\xf7\xa8\x98\xdb\xb9\x04\x84\xdd[\xb4H\xc9\xe42\xed\xd9\xdb\x07\xa9\x1e\x95\x91\x15\xb3B\xf0\xc5\x1f\xcc\xd1\xa0\x8eF-\x8c\x8a\xd7\xb6\x85c\xb5\x9c\xc9*aW\xef/p}=\xe9t\xff\x00*H\x0b1\xf9\xb7\xe0\xed\x1dB\xf4\xcdRzj\x06\x16\xa0\x1d\xaeC>\xf5\x8d\x11V5\xdc\x08\x04\x1fO\xc4\x93NH\xfe\xd3\x08\x88\x15\x1b\x97~\xe1\x80r3\xcez\xe3\xb5Or")s9\x8d\x9cF\x14\xaa\x97\xe3$\xf7\x1e\x86\xa4\x92(\x9e\xdbc@\xe8\xc8\x9b\x15c\\|\xa7\x1cb\xa9\xca\xfa\x8dheZH-\x10\x06,\xe8\xc3vx\xc2\xfdO\xf8U\xd3z\x8d\x13\x98CH\xecB\x97R6g\x921\xeb\xfd*1a0\x92%\x0c\xa57\xed~6\xa8\xf6\xc7\xe3\xfaR\xea\xf6D\xdc[4l\x91\xc7\x10\xce\x15x,~\xf7\xb1\xe9\x8e)\xab\\\x1b.M<\xae\xb6\xcb\xf6\xb8\xda#\xb7\xe5\xd9\xcf=\xb2p\rV\xb6[h\xee\x92A}\xb2X\xdc\x87Fa\xb9\x88\xf4\xf4\xc7s\xcdF\xf1\xdc\xc0\xaa\xa7\xca\xf2\xd2&\x00\xba\x8c\xe4\x9eH\xf4 We\xa7[h\xfaI\x86\x04\xb3\x82\xf2\xe1bVgYX\x99\x06=r;\xfe\x15\xb58&e)3\x1d4y\xa4\xb7\x99\xa7\x0c\xb1\x02\x0b0F\xcb)\xef\x93\xc9\xfc\xaa\xae\xb3c\x1a\\y\xb1\t\x16\xd9F\xd5\x91xL\xf6<\xf5\xfc\xab\xb3\x9b^\x8dnbe\xb5\x8e\xdeQ\xd2\x12\xd8\x1c\xfa\xfe\x15Q\x9a\xc1\xfeCio\xe69\xdc\x16\x18\x95\x9b>\xe4\xf4\xcd\x0e0o}JR\x928[[\xdb\xe9\x14[-\xcc\x92[!\xc1@\x9bT\xb7nG\x07\x8e\xfdj\xdd\x9f\x97=\xec6\xce\xc9\xb89}\x9b\xbe\xe0\xdb\xdb\x9c\x9a\xec\xadZ\xc80\x8fRTVg\xc2\xec\x1bpzr\xdd\xff\x00J\xc1\xf1\x1e\x97aaym\xa8[,\x7fdYBK\x99T\xf9\x99\xe0c\xf8\xba\x9e\xd4\xa5I\xee\x98\xd5E\xb5\x8a\xb0j"+\x851\xc6\xb2*\xb9\x8c\xca\x0f\n}y\xed\xc5Mx\x16\xe1\xcf\x933\xa9H\xd7,\x1b \xb6y4\xc5\xd3\xa3E\x06<,W`\x83\xb3\x1bC\xf4\x1dz`\xd3!\x82i\xf4\xe9Y\xe0\t4/\xb0\x90HV\\\x8f\x98\x1e\xf9\xed\xdb\x8a\xe7qoSK\xa2\xc5\xe0t\xb5\x926,\xa7xR\x00\xe6C\x8e\x00\xf6\xe7\xadc\xdfG\'\x9d\x00\xfb7\x97\x8c\xb6\xc0w\xaf\xb7^\xbf\xca\xae\xb6\x9d \xb7\x96\xe6\xe2i\'H\xa4\xf2\xcf$\xf9\x83\x8e\xfdO$\x8f\xc3\xa5.\xe3\x12\xac\x90\xa4D\x15l\xa1\xea\xbfR8\xfc\xe9l\xb4\x0bk\xa9=\x85\xef\xf6}\xbbM\xb6=\xae\x81\xb8nz\xf3\xc7\xb5j\rB8Fxu\xe1\x92N\x80g\x9c\xfd+\x9c\x82m\xdaH1\x02\xc0\\\x05\x00\x9cd\x11\xcf5vw\x92\xde\xd2\x1bd\xc4\x910\xf9\x95\x8eB.:g\xaf\xa0\xa8hv\xb9f\xea\xe8\x11\xb4\xee\xf2\xe7\xdc\xc8\xe7\'fOZ\xa7\xe4\xcbu<RI\x90\xd1\x8c\x94\x04\xf2pkR\xc2\xde\xdeVX\xa5e,@(\xdc\xe0\x0f\xa1\x1dkZ-"H\xd5\xd8\x9d\xc4H\x0e02W\xb8\xc0\xa4\x96\x83\xbaFV\x9f`\xaa\xe9\xbd\x11\xad\xa5\x8c\xc4G\xeb\x9f\xaek_L\xb0\xfb<\xaa\x8c\xa5\x01\x0c\x87\x03\xb6;\x9a\xbam\xa0\x8a\x12\x0eT\x17\xf30\xc2\x88\xae\xfc\xaf\xde\xb9\x07\xe6=[\xa8\xa3\xd0W\xb91\xd2\xe3\x1fgP\xdc(|/|\x8c\x1a\xcf6B\xf2Y\xdc\xe4I\xe5\x99!bpq\x8e\x84V\x8e\xabt-\xec\x92U\xe7\xe6\xdaN9\\\xf1\xc1\xfcE>\tcY%\xda\xa4\xf4Q\xb4\xfc\xa0\x8e1\xfa\xd5"u!\xb0\xb7E\x81\xe5\x08\x16r>p:\x1c\x0eI\xcd[\xb9x\x0cm$\x92\x95m\x80\x82O\x07\x9c\xff\x00N\xb5\x00\x9f\x85PT\x18\xc9F\'\x9d\xd8\xeb\x8fQ\xd2\x93Yp\xb6\x16^X}\xf1\xce\xa1\x8a\x81\x8d\xbdp}\xbaP\x06f\xa7h\xba\xa5\x9c\x91\xa7\xda\x92s\xfb\xd8\'X\x1c\xa8pr;t\xc8\x19\xab:N\xad\x1e\xb1\xa4\xdd\xdbL\xa6;\x92\xc1n``AG\x1c\x13\xc8\xce\x0f\\\xe3\xa6*\xf8\xbe\t{\x0cxf\r&\xecrp;\xe3\x1f^\xbe\xd5\x99\xe2v\x82\t\xa1\xbf\xb4\x06;\xa8\xc6\xd68\xc9\x95I\xe1O\xa9\x18\xc8\x1d\xaa\xa0\xf5\xd0RZjs\x1a\xbd\x99\xd3\xefY:\xa1?)\xf5\xab:\x16\xbe\xb6BK;\x88\xc9I\x9c\x15o\xee\x9e\xff\x00\x9du\x1a\x1d\xae\x9f\xe2\xbbc\x0c\xa4\xbcl\xac\x11\x98\xfc\xc1\x879\xcf\xe7\x9a\xf3\xddN\xd3\xfb:\xfaKp\x08\xf2$`3\xf5\xae\x985(\xd9\xec\xcce\xa3\xd0\xef&\xbb\xb6\xfe\xcf`L\x92y?(\x8e }x\x05\xba~\x15\x9e\xf2j\x16Z\x18}:\x08\xbe\xd2\x99\x80\xc3,\x85\xd1w\x9f\x94\x91\xd0\x8eEW\x81\xad$\xb0\r\x0c1\x96\x92\x11\x91\xb0p\xc0\xf3\x9f\\\xf5\xa7Ksi\x05\xac3\xd8\xc6\x15\xc4\x80H\xb1\x80<\xc1\x91\x95 v\xcdb\xbd\xd9X\xd5\xaeds\xfe&\xd3\xec\x85\xe9\xb6\x88F\xbeO\xca\xea\xa7#p\xef\x91\xd8\xf3\xd2\xad|;\x8d-\xb5Y\xe7gP\x89\x19\x0c\xc7\x803]Y\xd2c\xb9\xf0\xf9\xbc\xb9\xf2\x05\xcc\xee\xe6Ve\xdb\xb8\xe7\xa0\xecq\xd3\xf0\xae\x17Lk[[\xeb\xc8%\xb81\xc6yf\x8d\xce\xec\x0e\xc3\xadtJW\x832K\xde=\x0bX[\x89b\x86K}B\xde1\x0bdy\xe3\xef\x7f\xb3\xb8\x1e+\x81\xd5\xafu-Rm>\xd7\xec\xd0\xce\xf3HHvc\x95b\x0f-\xb4e@\x1c\xe3\xda\xb4-u9\xe4\x96g\x91\x1eH\x84[\x16va\x13\x12\x7f\xd9\xc9\xdc0}\xaa\x19\x8f\x9b-\xc0\x8ex\x92\xe61\xf6x\x9d\x08\x06L\x91\xce\x0fP3\x8cq\xde\xb9\xe2\xed\xa1\xabGi\xa2[.\x99,Zn\xc50\xb5\xa1\x8f\xcc\x8f\xa3\xb2\x9f\x99\xb1\xef\x9f\xc8UX\xac-\xeft\x89\x99\xd4\xc5l\xa7\xcb\x08\x17\xee\xba\x8cy\x83\xfb\xdd?\n\xc2y\xb5\x9d1\xac\x83\x05\x8d\xed\xd1\xbc\xb9\x95N$\xf5\xce3\x8c\xfa\x0c\xd5\xab\x0f\x10\xdfN!x\xe1\x1eY\x0c\x9ep\xc9B\xcc\x0eF\xd1\x92}8\xc7\\\xd6j\xf7*\xdd\x8b\xf7Z\x9d\xb4\xfa4H\xc5\x96H\x9b\xe5\x90\x0c2\x90:\x021\xbb<\x0f\xe9\\\xcf\x97t\xdatw.\xea\xc5\xf7\xb3nm\xd8l\xe7\r\xdb\x91\xd0s\xd2\xa8%\xdc\xb3\\\x0f>\x16\xf2\xe0\x94\xca\x17vJ\x9e\xdfL~f\x95\xef\x92\x04\x869\x8e\xebW\x1b\xc7\xcd\x82\x0ez\xe7\xbf\xa6M^\xda\x02Z\x93A}\xe5\xda\x1bG\x00\x99\x01t\x93$d\x9c\x0e}~\x95n\xc4\xba\xda,&RY\x9d\xc4\x87\xee\xee_B=z\xd6$\x97q\xefFHg\x12+d?\xde\xc9\x07\x8ey\xfe\x95c\xce\x9dd\x87\xf7\x81E\xcb>A?u\xfd\xbe\xbd*\x9a\xea\x84\x8b:\xbd\xf1\xbeC\x0b\xfd\xc8P\x08\xdf\x18\xda=I\xa6\xda\xea\x11K\xa6\xdb\x84\\\xb4G\x13\x10\x0eK\x0e\x87==+*KI.\xddS\xcdH\x9dX\x96^[\xa7\xaf\xa6j\x14\x9d\xd6)-U1!rr\t\x19\xe3\xff\x00\xadT\xfa\\\x12\xd3CkP\x9bz\xcb$\xd3?\x99q\x18r\xdb\x0e\xcc\xa9\xe8q\xd3\x9cu\xacI\xff\x00\xd2\xac\x8b!\xd8\x03n$\x1e\x87\x18\x04\x11\xc85r\t\xa7\x9a7\x88\xb3+D\x85}\x87N}\xea\xbd\xaa\x16I\xb7\x88\xdba\xc6\xec\x00:w\xc5R%\xa3oU\xb0E\x95\xc4\xee\xa8\x8f\x88@q\x8cg\x8e\xe3\x8e\xe75\xb1\xa0=\xbd\xad\xa4\x9a\\S\xaa\xdb[\xc7\x91*\xb8P\x0f\xfb<\xf3\xf8\x1c\xd6Zj\x0f?\x8a\x18\x9b\x88\xcf\xd9\xd46\xd9\x1c\x1c\x13\xc1\'\xdf\x1f\xce\xaf\xc5\xae\xda\xc71\x82"\x92H\x1c\x97a\x80\x11O\xf2\xf4\xe0qY6\xdd\x90\x8b\x17~\\\xderH\xa7%F\xdd\xee0Pt#$\x93\xcfs\xcdf\xdf\xdc\x08d\x8aI\xe4d\xdc\xc0m*7\x0c{q\xf9\x9e*\xbd\xee\xadm\xa8k\x12\x88n\xad6\xdb\xfc\xab\x1c\x0e\xa4\xcb\xed\xd7\x9ct\xce\x05`\xdc\xcdu%\xc35\xc2\xbc\xc6F\xc1\xf3\x8f\xca\x80t\n3\x9f\xce\x97*N\xe0\x9d\xf4&\xd4u\xe9\x9a\xe5\xa0\xbf\xb6[h\xf7\xf9\xabr$l\x03\xdb\xa0\xe3\xadK$\x90\xdcZ\xac\x97;\x961\x80\xc1\xc1\n\xc3\xd7\x9eFi\xb04\x92Z\x87t27\xddtX\xf3\xb0g\x80s\xd6\x9f\x1cV\x11L\xd1\xc5h\x90\xb4\xd1\x8c\xa0a\x92\x079\x03\xa6sG:*\xcffa\xdeY\x8d:t\xbb\xd3\x9c\xae\x0e]7\x06\xdb\xf4\xf4\xc5Kha\x91\xd1\x9dg\xba\x94\xa1\x94\xaa ;\xe4>\xbc\xe4\x00:\xe4\xe3\xebZ\x17?8\xdd\x1ew\x10w\x02\x800\xe3\'\x8e\x06*\x0b`\xd1\x18\xdeK\x92\xcc\xe9\xc2\xbb\x90B\xf5\xc6\x07\x03\xe8*\xef\xa5\xde\xe4\xb5\xae\x83,m\xeen&\x96Y\x9e7u_\xbd\xb7\x020}\x14\x7fZb\xc1=\xb4\xb7\x16\xec\x8d\x9e\xbev;u\x0c=\xfbsZ\x16\xe6KV/\x12\x83\'.C\x1f\x95\x94\xf6\xcf\\}jg\xb1$\xb4\xb0\xaa\xec\x80\x8c\x89\x10\xfc\xeay\xe0\x9e\xbdzS\xe6\xdc,dE\x12\x1bxT\x90!y0\xc1\x89o\xc0\xf3\xd7\xf4\xad\x0f\xb25\x9a\x9b[]\x8e\x14\x1d\x8c\x0eNz\x90q\xd3\x15$vj#\x16r[\x14-)(\xf8\xe4\xfb\xf1\xc7\xf3\xad\xfd#Myu\x14\x7f/\r\x19\xdd\xf3)\xc1\xe3\x19\xfc\xab)M\xb6TU\x8a\xec\x97\xac!b\xbf"\xa1\xf3\x19p\t\xcfF\xaav\xa4\xcb\x028\x12\xbb\xc4\xe5\xd6V\x1f1=\xf0\x01\xcdu\x1eO\x97\xe63\xa3\x91\x19h\xc7\'i\xef\x9a\xcf6\xea\xba\xa7\x97!\xda\xed\x19\xda\xd1\x0c`v\x18\xf5\xf7\xa5p3\x12\xf6s\x1a\xca\x97\x04q\xbfk\xae1\xea\x7f\x1au\xc4\x92J\x91>\xfc3\x00\x7fvxS\xe9\xff\x00\xd7\xa2I^"\xc0D\x03d\x14\\\xf2\xd8\xeb\x9c\xf1\x9a\x86YD\x008\xcf\xefNZ//iO\xaf\x1c\xfdi\xee\x1ee\xad>\xf2#q$R"D\xea\x80\x06\xe7\xe6=\xea\xd6\xe4x\x89o5\x0cd\x8c\xf1\xcf\xeak"M\x90\xc0\xf34\x92y\xeb\x83\x14j\x03o\xcf\xf0\x9a\xb9l\xaf5\xa3[\xa6brT\xb3p\n\x1f\x7fzMu\x04\xc7En\xd77N\xae\xa6$U\xca\xb2rI\xa6Z\xb9\xb4I$I\xb2\x99e\x91Yrs\xea).\x9eH\xa7\x8d-\x88w\xc33*\x93\xc9\x1e\xbe\xd5\x04\xb7\xf2\xdb\xc6\xf6w\x00\x19\x9e3 \x93\xa1S\xe9\xec)\xda\xe8W\t\x9a9l\xde2\x033\xae2p9\xf7\xa7[\xdb\xcb\x17\xd9\x16g,%\x8f\xcc(\xbc\x00\x07\x18\x1e\xb5\x99mv`\x9a\x196\xad\xc6\xd6?\xc3\xbb\'\xd7mi\xd9\xde\xcfq\xa8\xcf%\xc9\x1eU\xbc{!\x18\xda\nu\xfet\xe4\xfd\xd0Z;\x1a\r\x1cf\x13\xf6}\xbc\x9d\xec\x08\xe0\x1f@jS5\xad\xd4"\xdaB\x9edcr\xb1\x03\x8c\xf6\xf4\xebU\xa4\x99\xdb\xcb\x90\x90\x03\x1c\x9d\xbf\xca\xa6\xd3\xee\x91\xed\xd9\xe4\x90\x18#\x97r\xae\xdcs\xd8\xfb\xd4\xc6Vcj\xe8\xabc\x1bY]\x1f\xb5F$\x8bvW\xcd9\x01\x80\xe7\x81\xd6\xb6\xf4\xb7Ty^i\x169nd.\xc0 \xd8\x00<q\x8eO\x7fj\xe7\xafn&\xbb\xbd\x0f\xca\xca\xe7f\xed\xe4c\xdf\x1d8\x1e\xb5\xd1i\xadi\x0cr\x15\xc5\xc9\x8c\x11\x10q\x9d\xc7\xf1\xe83\xde\xb6\x8c\xf9P\xa4\xaeY\xbb\xb3\xb8\xbf\x94\xb4\xad\xb9\x02\x906\xc6\x07\x00\xfa\x9c\x9f\xadbjI<\x13\xc3\xba\t\xe0\x8eVb\xef\t\xfb\xa4z\x9f|\xd6\xee\x9f2\xddE5\xe1;\n\xf2\x13\x7fO\\\xfbR\xeaWR\xcd\r\xbd\xa4\n\xc5ep7\xe3\x19\xc7\xa0\xff\x00\x1a\x99IKp\x8akc\x06?\x11_\xf9j\x91\xcf\xba2\xbf$d\x0f\x9c\xe7\xae8\'\xa1\xa9_QK\xc8\x924@\x0b\xe5\xa5\x00`\x10=I\xe9\xcfZ\x9bR\xb5\xb7`\xd6\xc6\x132\xc6\xfbw(\xc1\xc9\xc8\xc8\xf4\xc6:\x8a4\x9d$4\xb1\xdc`\xc9o\x01\xda\x04\x8c\x19\xdd\xb1\xdd\xbb\xfe<\x8ajmu\x06\xaf\xd0\xa3k\xa7y\xb2\xcb\x89X\\:\x93\x1f\x19\x8c\x10F>^\x988\xfdk\xa1\xd3\xb4\xdb\x8b\x89&\x90\xacq\x87@\x8f\x96\xc0\x07\x1d@\xea\x08\xad\x15\xd3\x16$y\xe5\x85\x15A\x08\xdb\x067\xa9\xf6\xedV -n\xa7\xcah\xe4ll\xdc\xfcq\xd8\x9c{T\xb9\x02G\x1fsn\xd0\xe8\xf2$M,\xd2\xbc\xbe_\xefpB\xe0\xf2v\xf7\xf5\xa8\xact\xf9\xcaI%\xc4\xa5\x90\x80\x00Q\xd3\xd0\xfb\x1c\xf6\xad\xadj\xd4E#yGp,\t\x19<\x13\xe8=h-j\xae\xb1\xa8f\x05\xc0\xdb\x9c\xe4c\xbf\xf8\xd4\xab\x15w\xb1\x83}\xa62\xa9\x82\x17\x06IF\xdc\x86\x03\x1cg\x8f\xc2\x9b\xa2X\xdc4.\xf2\xc6\xf3,H\xa8\xaa\xd8$c\xa9\x1e\xbd\xabzF\x86;\xa5\x13\xec\x8e\x05b\xcd+\x82\xfeZ(\xce\x06;\x9ey\xab~\x1f\xbf[\x9d$K\x1cB \xe5\x88Y\x07$\x13\xeb\xf9P\xd2l.\xd22\xed\xe4h.\x00\xbb\xb5\x04\x86\xe0\x0c\x06)\xdb\x18\xae\x82\x14h\xa5n@F\xe4n\x1cH\x08\xe3\xf1\x15B\xf2Xe\x9a\x05f\xdc\xe0\xb6\xd6Q\x93\xd0\x1f\xe7L\xbe\x96g\x826`\xa6E\x1f\xc3\xd1\x07\xa0\x1d\xcf\xbdD\x86\x8b\xb7l\xef\x10\x0f\x85\n\xa5\xc8\x1f\xd6\xb0\xfc\xc2\xc8\xd6\xdbrW\xe6\x05s\x95\xef\x93\xf8U\xe5giY\xa4a\xb2T\x11\xae\xe3\xd8\xd3\xe3\xb60\xda-\xcf\xca\x1d\x1f\xca\x90\x05\xe4d\x80?\xc9\xa8\x8e\xe0\xc8uf\x9am\t\xc8?\xea_w#\xaf\xa1?J\xafep\xedl\xb0\xbbp\xb8\n\x00\xee{\x9f\xf3\xde\xa9x\x8eY-\xf4\xe3\xb9\x82\\\xc8J2\xae{7\xa0\xeaOaV,4\xe9\xae\x16\x08\xa6;dl\xba)\xe7h\x039o\xc7\x8a\xb6\x87\xd0\xd8(\x04\xc9*\xa9T\x19\xca\xe3\x83\xc5S\x9bR\xf2\xe60\xb30\xda\xbf1<\x82G<g\xf2\xad&\xb6\x7f\xb2\x04\x0f\xe5\xb9v\xdc[#\'\x19?\x86\x07\x15\xc7>\x98\x9a\x9d\xe4a\'$\xc4\xc5\x8c[\xc8\xca\x8e={\xe6\x80E\xe8\xbcA\x14r\t\xb7\x8f8\xa9\x08\x88p\x13\x8es\xefS\xcfp\x9a\x8d\xacm;\x9d\xac7(C\x8d\xc41\x1cz\xd5q\xe1\'Yc\x96\x05@\x167\t\x1a\xb8\x03w=3\xf5\xebN\x1a;\xc7\x0e\x9f\xf6\x940\x88\xd4\xa3\x15;\x953\xceN\t\xff\x00&\x84\x9a\xd4W\xe8U\xb7\xbe\xbc\xf0\xd5\xf1\xd4#(t\xef1Y\xf6716y\xf9z\xfc\xdd\xea\xcf\x8c\xc4w\xb7\xab\xa8Y\x80\xd6\xd7\x11\x87\x0e\x9d\x0f\xf8S\x1a[}B\xf6\x08X\x15\xb7\x17!\xaePr\xa3\x03\x03#\xdc\xe0\xd2x\x9e\xce\xe5\xb5\xc6\xb7\xd3\x91<\xb9@Y\x15_\x01\x89\x1d@=8\xe4\x91[E\xbd\x89qW24\xaf:I\xe1\x85Z@\xbefI\x1d\x01\x035r\xe3O1\xcf5\xb2[\xcbt!\xc3\xbcj\xf8\xdd\xdc\x12~\x83?\x8ddi\xbedZ\xb1\xb6\x927a\x0f\x0c\x81\xb9\xe4\xe3q\xeeGz\xef4\xd5\xb6\x86\xd0\xc8\xd0\xb7\x9aY\xa2\xf3Q\x89i@$\x12\x7f\x01Jo^b\xb6\xd0\xb3\xe1\x8d:\xda\xefL\x9e\xd7PQ\x96\x8c<1\xbc\xbf"7\xb0\xce3^tb\x9bI\xd7d2n\xcaeJ\x91\x90Fz\x7f*\xf4=\x08\xad\xbc\xf7\x16\x9f\xbd\x0f\x04l\xd1\xa8\x8c\x12\xe0\xff\x00w#\x8e3\\f\xb3o/\xf6\xbb\xdf\xdc\xc74\x02g \xc2\xec7(\xe9\x8c\x0e\x98\x00g\xde\xb4\x83\\\xba\x99\xd9\xf3hn\xbd\xdc\x12\xd9\x99\xa5\x93\xec\xcd\xc6\xdc\x02\x03v\xe4t?\x85d\xc3{\x1aO,\xcfo\x14\xb2\xb4\x88\xf1\xc9\x18\x03\x04q\xc09#\xb5#\\\x96\x89\x1a\x1d\xd2D\xd8\xce\xdey\xc68\xac\xe9\xe7\x90\x89%\x8f\xcb\x19\xc3\x05\xe39\x07\xf9\xd6\n\xd76\xb6\x86\xb4\x1a\x95\xcc\x97\xec&\x95\xcb\xc5j\xe7\xc9\x97\xd4\xfa/\xf3\xac\xa1\xa9\xdc[\xdeO\xb2O*\xd5\xce\xe5\x05>@q\xf5\xf5\xa8c\xb9\x91\xaf\'\xbaT7\x05\x10\x89"\x07\x96\xc8\xc6A\xea~\x95\x1e\x9a\x18\xf9my\x81\x1c\xa7~JeW\x1d\x87?\x85\x0b\xad\xc6\xd5\x86\xdc\xdd\xcf\te\x7f\xf4r\xc4\xb9\x96&\xdd\x9c\x8f\xf0\xa8n"[\xbd*\xd4c\x12\x84\xf9\x81\x1dFx\xa9\xef\x83L$r\xa9\xb1\x17\xee\x05\xdb\xdf\xb7\xad2)\x92Tf\xb8B\x8a\x914x\xda1\xbb\xa8"\xaa\xfb4\x08>\xd3q)\xb6\xb4\x8e\xdf\x8d\xfc\xc8\xc3\x01x\xcb\x15\xc7\x1e\xd5} \xf2\xac\xa3Y7\xa2$\x8c\x00\x90\xee*F\x0f\'\x8fZ\xadd\x8a-\xa3.\xf1\xefl\x00\x07\x00\x7f\xbd\xeb\xda\xa5.E\xbc\x90\xc8DC\xcd\xdd\xc2\x8c\x13\x8ct\xcf\xb5>er9G]\x88\x9a\xf6\xdc\xc2\x18\xc1p\xe6U\xf2\x80\xf9\xce>\x9d3\x9a\xa15\xa3\xbd\xd4f#\x82Ie9\xe8>\xb4\xebT\x8e&\x9d`\x9d\xe4\x90\xb1P\xe5\xc8TR\x0e@\x03\xa7&\xaeOe\x1c\x11lf.T\xedNO\x18\xe9U\x17wa=\x11W\xed\x17J\xb2[\xb1F\xb8m\xad\xf2\x8e\x1cd\xf1\xcfn\x94\xcbK\t\xe3\xb8\x91\xeev\xe5\x1f\x1bU\xb8\'\xfa\xd4\xe2\xe2O\xb6\x89\xa3\x9aH\xef#9\x8ee\xc7\xde\xec9\xe3\x14\xf8n\xe7\xd4T\xcf*\x87\x96<\xef\xe3\x1f1=\xea\xae\xd3\x15\x939You]:\xf2\xe6\xe1\x92Uy\xd8\x02\xd1\x9e\x07\xd0\x0eH\xfa\xd3R\xff\x00T\xd3\xa7\x96u\xb4\x92h\xe5\x05&|\x0ewps\x8ekV\xe6$\xbb\x85\x16\xe5\x9e9\x84\xb9\x1c`\x10:\x8e;U\x8bK4\xf2]!\x12,\x84\xee\xc8bW\xe9\x8f\xf1\xa5*\x9c\xbd\x08Pl]/W\x86m\x1dl\xedQ\x11G\x07\xf7j\xae\xa4\x1e\xb9\x1dj8\xaf%[\xd1\x1cqc\x03\x89\x0f\x00/~i\x90\xdaF\x9a\xaa\xc9\x1a\x14V\'#\xb2\x9e\x87#\xbf>\xf5j\xf6\xcd!\xc33\x05\x0c\xe3 \x0e\xa3\xd2\xb0rW\xd0\xd61c/.\x84\x93E\x0c;\x95GB\xa7\x82\x0fv\xf5\xa9-\xa3Y\xae\xa4\x92I\x10\xbd\xb8\xe0p\xca@\xec*\xbcH\x92[\xbb\x92\xe8\xc0\x10\x03/9\xed\xd2\x92\'0Z\x96xZ;\x878X\xc0\xc3q\xd5\xbd\x80\xfeup\x85\xd8\xa4]kK{\x81"\xc4\xad\xb9\x8eenF\xd0z(\xf7\xee\x7f\x01U\xd3K\x16\xf1\x99\xa2/&A\x04>r@\xf7\xedVa\xba\x8a\xca\xd9\xa1\x01\x7f{\x10\x95ZF\xc1w\xf4?\xe3V4\x8dIf\x8cy\x8b3\xc5\x19\xde\xa0/\x7fL\x9arM\xbd\x1e\x80\xb4F\x8e\x94\xa9,\xf1E"$g\xcb\x03\xef\x03\xda\xb4/m\xa5\xbbtx\x9c\x06\x89\x06\xd4`\x08|\x1f\xf3\x8a\x87I\xb4H\xed\xa2\xba\xb8\xbf\x88\xbd\xc9*-\x8a\x861\xafU\x07\x8e\xb5\xb8\xa2\xdd\xa3D\xdd\x08\x98\x9f\x95c\xe7n=x\xe2\x93\xa54\'%\xb9\xcfZB\xd2\xcf#\xb2\x94\xb9\x89\xb82\xe7\x8e\xfd;\xe6\xb4\xb4\xfdF\x04\xbd\x8aUx\x9d\xc0!\xd13\xb8\x1af\xb1\xa2\xde^\xc8\xf3ZD\x8e\xf9\n9\xda8\x1f\x81?\xca\xb2\xb4h\xb5\x0b\xedRKt\x81\xe1\xbaE\xf3rW\x86\x1e\xc3\xf8\x87\xb0\xa4\xe9Mt\x1a\x94_S\xa8d\x82\xe0\xcd:HLm\xf3l\xcf\nz\x1e\x0f~\x95Be\x8f\xca\x99\x8b\x8f1F\x13$\x83\xed\xf4\xac\xb9u9gkB\xd1\x80Rc\x0c\xae\x0e\x018\xc9\xc0\x1c\xd5\xf9%\t#\xef\x85\x96FA\xb1\x9b,\x02\xfd\r\x1c\xadn\x80\x86K\x88\x0c)3\x85Y6\x85\x93w<\x8foZ\xc8\x9de\x82\xd5\xee\xe4\x91\x90\x12~M\xbc\x81\xe9\xedQj7M%\xc2$8\x11\xaf\xde%\xb9\xcf\\})\xf7\x971\xdf \xf2d(Xa\x97?(\x02\xa7\xa9v\xb0\xdf*;\x8b-\xdb\x8a4\xc3(\x8b\xd8\x8fJ\x8fIf\x10\x07\x8e\x16\x12\xb6\xe4s\xfd\xd6\x1c\x8e\xb4\xa2u\xb5\xd3\x96\xea\xd1G\xee\xbeR6\xf1\xef\xf8\xd5\xc6a\x1c\xb6w\x01\x91\x04\xee<\xc0\xa39`:\x1f\xccS\xbd\xc8j\xc2Cr-\xa2\xbb\x99X\xcf*(b\xc4\xed\xc9\xc7@\x07N\xb5E\xcf\xda\xed\xda\xe2w"F\xb68\n\xb9\xe4\x1e*\xe5\xcd\x87\x9e\xac\t\xfb,\xa1\xf6\xa3/\n\xddk5\xb5\x05\xd3\x1e\x18n"Fh!\xc2B\xccJ\xc9\x93\xf7\x9b\x1d\xabE\xaa l3\xad\xb5\x99k\x90\x89,\xb8\xda\xd1\x80\x08\xfc\xba\x0f\xa5,\xd6\xf7\x10B$.\x0c-\xf7d\r\xbb9=\x0f\x7f\xce\xab\xa2>\xa0\xc6\xdef\n\xdf{\xa6\x01>\x9e\xd4\xf2g\xb7\xb6\xb83\xb0{R\xbb\x1f<\x12G\xdd<t?JV+\xd0\xd2\xb7\x99\xee\xd9J\x1d\xc9\x14Y\x91H\xe0\xb0\xec\x7fJ\x9d`\x95\xac\\\xb6\x15&\x1b\xd0\x04\xc6\x08\xe4\x1c\x7f\x8dU\xd1\xa2\x8e\xc1f\x99\xdc\x12\xeb\xbdP\x92\x00\xc7\xf1f\xaf\xe9W\xb2\xdc\xdf\x0b{\xc9V`\xdc\x89Ks\x83\xd7\x1e\xb56]\x03\xa1\x8e\xb2\x03\xe5\xbb\\\xc6\xa6Nd.\xb9\xf9F3\x9e\xc0\xd6\x9cM\x1c\xdel(&0$\x9b\x8cQ\r\xe1\x97\xb9<\xff\x002\x05f\xf8\x86\xc0[]$(\xa1\xa3F\xda\x9b\x8e\x02c\xbe=}\x8dJ\xd3\xc8\x9ff\x8d\x1c\xae\xf8\xcaH\x01\xfb\xc0\x7f\xf5\xe8\xf3\x19\xd3\xa6\xa4\xb3\xdd\x98a\xb3H\xed\x81\xc2\x06a\xb4\x1fp\xa0\x8f\xa0\xab\xfa\xad\xd35\xcc1\xa1\x87\x11cp\x19m\xdf\x89\xae2\rS\xfb2\x16&V2g\x0e\xc3\x8c\xfb\x0e\xc2\xb4-a2Z\x19\xa5r\xac\xf2e\xbdpG\x00{\x9aL\x16\xe6\xec\xcd5\xc3H\x16h\x9dT\x87\xfd\xdb\x103\xd7\xa69\xfc\xea\xf5\xbcQ\xc3\x16\x0b\x0c\xc5\xc8(N\t=I\xef\xff\x00\xea\xac\xfb8>\xc8\xc9\xd3p\x1bB\x9c\xe1Gl\xfb\xd5\xfb?.H\x14\xb3\x91";nH\xce\xdf\xcf=jor\x8dYn\x8b\xc6\xa0I\xbbr\x0f1\x1f\x9f\xc2\xb2\xda\xe9"\x96x\x96\x03\xc0\x07\x8f\xba8\xaa\xf2\x97\x85\x9d6\xc8\xaf\xc3\x00q\x93\x9fz.\xe2i\xa2c\x11$:\xa8-\xd5G<\xe0\xd4\xb6\xee4\x90\x93\xdcn\xbcw\x8f\x0c\x8c\x83(\xdf\xc3\x8f\x7f\xadS\xb6\x8e\xe4NLH\x0f\x96\x9b\x98\xe39$\xf5\x1f\xadM\xa8\xf96V\xaf\xa8"\xb7\x97\'$c$`b\x9eI6-qf\xed\xbc\xa1#i#8\xe7\x14\xd5\xec/1\xcd\x0c7\xb0#\\~\xf6(@\x95\xc7M\xc3\xb0 u\xcf\xa7N(\xb9\xd5\x93\xec\xe1\xed\xd0\xbb\xb9b>o\xb9\x83\xd3\xdb\xf5\xa5\xb7\x9040yN\x1f\xceQ\x14\xa7\x18\x07w\x19\x1f\x8f\xf2\xac\x96\x8d\xa3\xba\x93L\x88\xa8T$\xaa\xf5\xe4pI=\xc9\xaa%jg\t\xae\xe3O\xb5\xfd\xa4\xcfy$\xdb\x03\x8f\xba\xa8s\x90\x00\xe9]U\xb5\x9a\xdci\xce\x16\xe0;\xe0>\xed\xd9\xc5e=\xa3\\_\xc5e\x14+\x0cA\x1dd*y\x07\x1e\x98\xf5\xfeu\xa9d\xd6\xf3h\xf2Z\x9d\xbb\xa3@\xac\x18\xf2\xccx\x03\x8e\xc3\x8c\xf3R\xd3(\x9a\xda\xd4\xddD\xe6g)\x1c\x7f,,yh\xc8\xeaA\xf5&\x9b-\xfd\xb4\x9a\x149\x85\xe5\xb8\x0e7\xaa\x9e\\r:\xf7\xce+FY\xa0\xb6\xd1"\xfb\xbc\x11\x0e\xd5N\t\x1f\xca\xb9\xa0\xb6\xed\xaa\xb4\x13<\xbeVK/\x94\xdb[\'\xa2\xee\xfe\x1c\x0e\xb4Eu\x01\xdaP\x89\x8c\x17\xf7;\xcc\xa9q#J\xa5\xb3\xe5\x80\xbdr\x07\xccN@\xf6\xe8+\xa4\xd3\xca\x16\x8eg\x0c&\x98\xfd\xd2>\xea\x8e1\\\x14\xb1\xc9\x17\x88#A1\x8e2\xeb\xc1\xcb\x02\x0f#>\xb8\xae\xba\xd7U\x89Z\x18Ha\x14LUe(C\x0f|\x1f\xe7U-\xc1\xad\r\xbdF\xe2+xcE\xc0\x95\xd8\x0e\x06\xee\t\xcfO\xa0\xac\x1dQ<\x9dF\x1b\x9f\xb3\x08g\xc0W\x99SvW9\xe9\x9fn\xbc\xf5\xa9\xed\xef7\\O+\xba\xe6\'\xc26z\x82\x00\xfeu\x93\xe3\x1b\xc9C\x01m\x89\x19\x08>K9\x05\x97\xb9\xebR\xb7\xb0X|\xda\xcd\xa4z\xba\xc7+H\xb0ydA\x95\xff\x00\x96\x87\xaf\xff\x00\xaa\xa5\xbe\xbe\xb2\x92\xce3sq\x1aBT\xc8d\xe5\x81p1\xb0g\xa18\xfeu\xcc\xde\xeasGqetDd\xb2\xe4I\t\xe4\x93\xd4~\x14j\xad=\xcd\x9c\xb6\xd2n\xc4\xce\x01\xf9@\x08N:z\xf1\xfc\xea\xed\xb0\x919\x9f\xecFK\xb4\x8eH\x96y\x92\xe3nF\xe4C\xd01\xe8\x07|V\x86\x85\xa9\xc5&\xae\xd7W\xd0\x19\xc2[~\xe5v\xed%\x8b}\xe6\'\x8e\xd8\x04\xfe\x15\x89yw\x08\x94X"\x94\xf2\xe3\xf2\xa4\x11\x82\xd98\xef\xf5\xfe\xb5F\xf0\xde\xdaM\xf6(\xf6\x19\x965BQ\xc3\x05\x18\x07\x07\x19?\xd6\x9aWBo[\x17\xaf\xe6\x13\xeaW:\x9b\xe6\xdaY?y\xe6+\xf2=\x06{\x8c\x0f\xc6\xb7\xf4\xeb\xdb\x9b\x0b;h\xa6\xb7\x86+c\xfb\xcf1I\xc4\x8cFy=\x8f&\xb9-EcKu@\xd7\x12;\xaa\xc4$u\xc2\x82OE\x04\x0c\xf1\xc7\xadh\x8b\xddBK\x13j\xd3\x88\x95\x18#*\x80I\xcfPX\xf4\xe2\x9b\xd5"\x8d\xe5\x92\xebV\xb7\x86[vd\xd4\'\x98\x88\xd69\x0ev\xe3\x80Nzb\xb9\x95\xbb\x9eh\x1aI\x98\xb4\xf6\xb32\xbc[B\x90\xb9\xe4\x93\x9f\x98\x9e\xff\x00\xce\xac\xd8\xeaS\xe8\xfa\xb5\xc9\x88\xb42\xc2\xa7dd\x1d\xbc{\x9e\t\xc75\x8bet.\xb5+\xab\x8f\x90<q\x8c!\x18\xde\xdfO\xa5\x0bD\xc0\xb7\r\xdc\xd0\xa3G\x1d\xb3\xa4r\xfc\xe8H\xea\x0f\xa6q\xc6jY\x10\x1bt\x13G\x0b\\$fM\xb9\xdc\xd9\xfe\xf0\xc7\x1c{\xe2\xa85\xe1:|\r9\x7f)\xf0\x17\x198\xe7\x9a\xaf\x1c\xd2\xae\xf9\x1eQ\xb2D9,y\x00qB\x8e\xa9\x94\xd9\xadb\x96\xd2_I\x1f\xdafF\x92\x10\xff\x00\xbbQ\xf39\xf5\'\x91\xf8Q\x15\xdc>L\x9az\xc5$\xad\x13|\xbed\xbdFy\xc6\x00\xef\xff\x00\xeb\xac\xa7\x9d!\x8a+\xa4\xcb+/\x97\xb5H\x059\xe0\xe3\xd3\x1d\xea0\xef\x88\x8c\x0e\x1eX\x1b{9\xee\xa7\x9e\xbf\xd2\x93\x8bAt\xcdY\xb5(\x0b\xc8\x86\xd2\r\xd0\x10e\xc9\x90\x82?\xbc\xa4\x9e\x7f\xfa\xf4\xcbi\xedf\x87\xe7\x81\x11\xcb\x12P;d\x81\xceFN?J\xa95\xf7\xda\x11\xf6\x051y\x8a\x0e\xdf\xbf\xdb=\xb8\xe6\xa9\x19\x1d\xdc"\xa9\x0e\xb9\x00)\xf9\x8f?\xd6\x94U\xd0\xaf\xa9\xaa\x9fc{\xd4\x0c\xb2\xa7\xce\xcc\x88\xcd\xb8\x0e\x9e\xdc\x93S\\\xd8\xf9\x1e\\\x91\xdc\xefYyb\xe8A\xcf\xa0\x1f\xd6\xa0\x82D\x84:\xa8\x8dn\x97%\x8b`\x85\xc8\xc6\t\xe9\xd7\xd3\xa5T7\x13Io \x962$\x00\xf9k\xb8\x90\x0f\xb1\xa7\xad\xee\x1eE\xe6e\x8e\xd4\xc1ej \x19%\x94\x1d\xc07l\xfa\xf3Oi\xdd\xe3g\x9aeb\x80o\xca\xed\xeaz\x8fLzUt\xb8\x8a\xd9\x967\xc2>\xd1\x80\x17\xa3\x1ep\xde\xb9\xa8\xe6&{9\xee|\xa9U\x8e\xd5D^2s\x9c\x01\xdf\xa7Z\xd21\xd52$\xf4\xb5\x89\x99\x95\x04\xcc\x9ec,\x8d\x8d\xed\xf7\x87\xbf5\r\xb7\x9eg\xccn\xa2,\x1c\xfc\xc0e\xbd\xea\x06\xbdR\x88\xa8\x0cS3\x1f2\tI\xc0\xe3\x8c~?\xce\xafIx\xebi\xe5\xe2# M\x84\x95\xe7\x1f\xfdj\xb7k\x99\xa6UK\x02\xb6n\xdeV\xe921\x8e\x00\xfc\xea\xacw\x92\xda\xbby\x96\xce\x19\x1b\x1b\x97<T\xd7\x17\x04\xca\x92Bq\x1f\x00\x02\xdc\xb1\xf5\xc5T}BO>S\xb2D#\xa4\\\x92}\xeb\x06\x9d\xdd\xce\x84\xd6\x84\xb1\xea7\x0b;O1*\xcf\x9d\xbb\xc7Q\xf4\x1d\xea\xdd\xe0k\x98`\x90!\xde\xcd\x92\xc5\xba~\x14\xcby\x83\xdc\x05\x95\x14\xba\xf0\xa0\x91\x81\xf5\xab\x12]@2\n\xb9E|\x9cc>\xfd*.1\xd6\xed\x03\x80\xf2\xaaD?\xd69$\x9cc\xd0\x7f\x85Gp\xe5gk\xd2\x1b\x12\x8d\xa1\x0f8^\xc0z}}j\xc9\x9a(4Yd\x84\x02\xd7\x8ccFR7m\xea\xc3\x1f\x90\xaa\xb1\xc5y8H\xd5\x1aX\xf1\x90\x8a9\xfaf\xb4\xd6(\xca\xf7lb\xdd\xbd\xcd\xde0\x03\xaaaN2~\x87\xfcj\x9d\x94\x8d>\xa74m$\xa00%T\xf6\x1fZ\x9a\xd2?\xb3\xeaR\xb4\x81d\x0c\xa4<y\xce\xc3\xd8\x93VDH/\xa3\xba\x87\xe7\x89\xc1\r\x85\xc0\xeb\xd3\x9cR\xbd\xb4\x047M\x9a\xe0\xdc\x1b8\xa4\n\x88\xa4\x97\xe8\xdcw\x1d\xebM.\x7f\xb3\xed\xad\xc3\xce\xe6gW\x97<\xf1\xcfB}\xe9!\xb2\x97\xf7\xb7)\x12\xa4\xa1\x0b\xf9a\xb2YA\xed\x8f\xe5Z\x92\xe8\xff\x00j\x82)\x99\xc3\x1c*"\x14\x00\xaa\xe3\'\xf0\xaa\xd6OA6\x92\xd4\xa1y\xab\xdf3G5\xb8*\x92\xc63\xbf\x90I\xe3<\xd6|\xb7\xf7\xd6\xfa\xb40\x8b\xc7\x8ex\x93td\x1c`\x1e\xab\x9e\xdcWZ\x9e\x1ck\x8b[e\x8at\x11+\x96c\x82Y\x94q\x80)\xd7\xde\x0e\xb3\xbc\xbd\x8eu\xbc\x9c\xcf.\x19\x83 \n\x8a=\xba\xe7\xa5h\xa1U\xa37(\xa3\xcfc\xbf\x95\xae~\xd2\x96\xf3\x19d82\x1f\x9bkt=\xebp_\xdfJ\x92Z\xfd\xa2Ye\xf2\xf7\x91+\x92\xa4};U\xf9|;-\x9d\x8d\xdc\x11\xb1i\xa4\xb8\x05N:\'s\xc7\xaf\xd7\xb5Ie\xe1\xb0.\xe3\x9e\xe2I\xa0\x0c\x1a5b\x80\xe7\x1d22:\x8a\x89S\x9b\xd1\x1a)\xabjs\xcb$\xc6\xe1M\xc3\xa3" $)\xeb\xeb\xc5S\x16\xa6\xd6\xe5\xe4FgdfA\x82N\t\xee\x07\xe2+\xd0"\xf0\xa6\x9bor\xd3\xcd\x17\x99\x12\xc5\xcf\xcf\xb43\x1f\xa7\'\x02\xa4\xb0\xb7\xb5\x8c\xc3,0\xc7\xf23|\xcd\x1ev\xf6\x1c\x91\xd4\xe2\x9a\xa3+j\xc9s]\x0e2\xdd/!\xd3\x9a\xd0\xa9\xc4\x8e\x01\r\xceX`\x9e\xbf\xca\xbad\xd3e>\x1d\x89X\xa2\xdc)iC\xae0\x06q\x9a\xcfk\xc8\xd6\xf7\xcc\xbc]\xbb\xe7u@\x84\x9d\xd9<\xee\'\xe8:WL\xf70\x94tb\xa9o\xe4\xa8T\'?)<\xfd)\xb8E\r\xc9\xd8\xc9\xb5\x98\x93\x0c\x17\nR\xd6A\xb9e(\t^y\xeb\xd2\xb2nm\xcc\xd7W\x90\xb82\xa2\x10w\x84\xcb(\x1f\xd0\xf1[\xa9%\x84j\xd0\xac\xa0*\xe3\xc9\x8c\x82\xcc\xc7>\xbdq\x8aX wk\x84b#\x9aRdo\x97\x19\xcf\\\x9e\xf5<\xbebL\xe3\xa0f\x82X\xe5\x99@R\xe5S=p:\xb7\x1d\xab^o%R{s\xb2M\xe0nq\xceA\xe9\xc5R\xbc\xb3M>GV\x8cN\x0f1\xc8[\n\x99\xe0}M%\x9d\xc4s\xc6\xb1@\x19\xd5\x81Y%s\xf7\xb1\xf7\xb1\xfe4\xac\xca-\xdd:.\x15"*\xae\x81J\x9e\x8d\x8a\x8e\xda+\x83{\x03\xa4N\xd2\x820\x14m\xf2\xd4Tv\xd3\x96C#\xa24\x85~M\xa7%y\xe3\xafj\xd6I\xa3\xb4\xb2\x8f\xcdf\x91\xa4?9\xf7\xf6\xedKa\x96\xee\xed \xb81\x1d\xc0\xb1%\xc8=\x8e1\xc9\xaaRik4\x0c\xd0\xa0\xde\xb2\x85$\x1cm\xe3\xfc\x9aj\xeaqy,\xdb\xfep@%Gc\xfc\xa9\xd2\xce\xff\x00\xda\x0c\xac\xec\x10*8+\xdc\xf3D%\xab\xb8\x9a\xecgk:5\xb4w0f|\x08\x0e\xe7\xc9\x1bI>\xdcU\xeb{\xe8\xee\xef\x0cr(\x8e\xd9\x19@f\x04\xe4\x8ez\x7f\x85>K(\xb5\x07b\xf1\x86\x95\x80`\xcez\x1fJ\xb8\xd61\xc343\x1bQ&a\x08\xc0\x9d\xbc\xed\x1c\xfb\xd3\xb2a{\x1a\xd6\xd2\xc3$\xe8\xa0\x91\xb9\xceP\x8cmQ\xc0\xa9\xbe\xd0\x80\xb0\xd8>f\xe5r\x0ey\xe3\xafL\xd6H\x8d\xa0\xbc\x1eP%JmT\x1d\x863\xf5\xa9w\xc8\xe5?w\xf3!\xc8\x04\x1e?\n\x97\x04\xf6a\xcch\xdc"\x9b\x94i\x1bx~$`:\x1e\xc2\xad[\xe2;x\xac\xd8\xae7\x1ez~\x1fJ\x8e\t\xa2\xba*zL@\x18\'\x80?.\xb5$\x8a\xd1\xbc\x86]\xbb\x18d\x12@\xe0VmX\xad\xc8\xae\xa0\xf3 k4;Q\xb2\xc8W\xa8\x1d\xfe\x95\x08\xb3\x96\xc2Q\r\x9d\xa4\x8dfb%\xa5\xdf\x96\xc9>\x87\xff\x00\xaf\x9a\xb5\x0c\xab$\xf1\r\xcd\x92\x0fQ\xd0U\xf8d\x0b4\xb6\xf22\x8c.\xf4n\xe0\x1fZI\x81\x8f\xa5Y\xa5\xb6\x94&q\x1a\x8c6_\xa9l6G\x1d\x8ek\'l\x0fy=\xccR1\x93x%\x81\xdcw7\xde\x03\xfc\xf6\xabS\xdc3\xc3%\xa4\x0c\x04F@\xc3\r\xb4\x0c\x1eG\xa8\xc9\xabp\xd9%\xb4n\xa8\xa8\x85\xd4\x15\xdb\xd469\x1f\xfdz\xa6\xc1"\x88Y\xed\xd8o\x94,\xd7R\x83\x1c\xa4\xe4\xa0\x1c\x1av\x95g\xb6\xf9\x12a3)B\xff\x00!\xdb\x93\x9e\x84\xfaT\x17\x80\xcd\x0cv\xfb\xdf\xcc\x88n-\x80\x14(\xec\x0f\xe5Tl\x9e\xe1nH\xf3\x0f\xcc\x15\x15\xba\x823\xdb4\xde\xc0\x8d\xfdV\xe5\xbe\xcdm\x1c(\xc5\x0c\xac\x1c\xff\x00\xb5\xef\xe9It-\xad\xf4\xd9-$es*\x97\'\x8029\xe4\xfa\xd6F\xab|\xf6\xd7P\xc1#\xb3"\xb6J\xaf\xdd\xfa\x9fSUu\rd\xa5\xbc\xed\x14^ls|\xa08\xe0\x1cc \xfa\x0e\xb4%\xd0Dk;\\\xea\xeb:\xc7\x14f4\xccLP\xb7#\x00\x0c\x03\xf9\xf6\xad5\xba\xdfp\xc5\x8e^L\xc8F\xd0\x00=\xb1\x8a\xe6\xed]\x8ch\xf8\x92;\x89Sd\x83<\x8c\x9e\xa3\x8e\xfcU\xab\x9b\xd6\xb3\xbb\x8aG\x8bj<~C\x12rT\x8e\x9f\x9f\xad\x12\xdc/\xa1\xa9\x03\x04\xb6\xbav}\x81\x0b\xef\x19\xf7\x04\x1a\xc8]b}CP3\x92\x88"\xca\xa2\x95?6N>l\xf6\xc6+B\xdaH\xa5\xb5\xba\xd8\xc4\xb3\xab?\xc9\xce\xe1\x8f\xcc\xf4\xae^k\xd1g\xa9\\\xc38Q\x14\xb1\x8f(\x81\xb7\xe68\xcf\xe3R\x95\xc7s^\xe1\xe4{\xa8&H\xd4\x05\'\xe5\x0b\x80T\x1f\xbc*)e\xb8\xd5\x9b\xec\xf32\xc2\xaa\xdb\xdaA\x9c\xf1\xd8\x13\xd6\xa4X\xee\xd6(\r\x9c\xad1U\x0c\xb1(\xdc2{{q\xd6\xa0m>\xf0\xbd\xbc1F\xc6\x06-,\xc1\\\x1f\'\xb9`Fx\xed\x8a\xde1\xd8\x8edW\xb0\xb6\xb6\xb8\x82\x1b{r\xd1\xdc\xc8\xee\xdb[\x92\xc0t\x00\xfb\x8a\xe8\xb4\x9d,\x8dU\xa4\x9e\xdc\x9c\xe2\xe3v\xdc\xf3\x9d\xbbI\xf5\x15DX\x8b\xab\xbf\xb4\xd8\x18A\xf3\x17s\x920\x8c\x07a\xdb<~\xb5\xbfq~\xdf\xd9V\xb2;J\x92\x89\xf6\x0f0\xe1Q\xbas\x8e0\x7f\xc2\x93\xd1X705\xbc\xea\x13\x1f\xdc\xacRB\xa5\x1d\x8b\x8d\xc1\x81\xeb\x8f^1X\xd6\x1fk\x8eS=\xc4\xb1<AK\x0f\x9b\x90A\xe8}MI\xa8\xe9\x171\xf8\x9a\xedW\xce\x0e\xf9\xf9\x97\xee\xe0\x8c\x8f\xf7\x8f\xbdg\xebp>\x8dw\x1b\x14\x98\xc3<@)\x03\x87n\xe7\xadW#z!s\xd9\x97#\xf3^\xd1\xe6\xfd\xe6\xc1\x96\xc4\x9d\x01\'\x93\x8fz\x82u\x91\xafb\x92!6%\x01W\xcdP\xaarz\x83\xdcc\xbdI\xe6\xdcG\x03\xdb^m\x89\x06\xc2\x99erF3\xd7\'\x9f\xe5Zv\xf6\xa0\xd9K\xba\xdeF\x93\xcb\n\x06\xd2~|}\xc5\xfc9?\x95%\x16\xae\x8b\xe7\xbd\x9a1\xef\xadSM\x9aH<\xc2\xf1\xc6\xc1\xb8}\xa0\x92?\x96h\xd2\xde8e7\x17$I\x1c\xb8!K\x13\xb7\x9f\xff\x00_\x15%\xe9\'I\xb7\x8eV\xda\xc1\xc2a\xc6\x1b\xd8\x11\xdb\xa5Emr\x04\x1e]\xc2*\xc7\x01\xc8\xc1\xc9##8\xf5\xfaV\x91D6>\x0f0Z\x99a\x8f\xf7\t/*\xc3\x9cg\x00g\xdf5\x97}#5\xdf\x91\x14>Tpg!\x1b\x80\xfdrOs\xf5\xab\xf7\xb2\x83r\xca\x91G\x14/\x1bJ\xa9\x92\x08\xe3#\x8e\xc2\x9d\xa5\xe8\xd2\\D\n\xc8\x92!M\xe5U\xd7q=q\xcf52\xd1\rj\xcaVq=\xd4\xdeb\xc0\xc5\xe5\xf9\xb7\x93\x85\x18\xea\xc4\xf6\x1e\xf54\x13\xdb\xc5qq1\x95\x882|\xbbT\x82\xf8\xeb\xf8f\xa5\xb8i>\xc6\xf0\xdbE$p\xe4o!\xba\x9c\xf7\xed\x8fj\xcedb\xf13\x15\x94\xab\xf9j\xa4g\'\xdc\x1a\x95\xb3l\xa7\xa36-\x842\xc3%\xc3\xc6\xd1\xb4\x83\x18Q\xd7\xa9\x1c\x9c\xfaSV\xf8\xaaB%\x8e6i\x8eL\x84\x16\xf2\xcfc\xed\xd3\xf1\xa6H\xde]\xa2\x97M\xb29\xdc\x07%P{f\xa6\x86\xcaK\x9b\xa5\x13a\xa3X\x9bb\x8f\x97,Fri%\xad\xc1\xbb\rk[\xb87L\xf1\xb7+\x92\xcb\xf3\x0f\\\xe7\xd6\xb2u]D4\x16\xf6\xf6\xf8\xdc\xce\x19\xb2Hf#\xb7\xe1[\x1eD\xe9e\xb5dUt#i#\x9ct\xc1\xf6\xaas\xda1\xb4\x8cD\x14\xcf\x13\xb6\x14ry\x1f{\xf3\xad"\xad\xabDI\xf4\x12\xcax\xf5\x07\x91\xa5U\x17\n\tx\xdeB\x9bq\xfd\xd3\x8c\x13\xd2\xae\x86L\xc2\xa8\xec\x14\x02X\xban`\x0f\xaf\x1cg\xd6\x97N\xd3\x01\xf3\xe7\xbb\x99\x9dV>\x14\x9d\xbek\xe0g\x00\xf6\x1d8\x15_\xcb\x11K,\xcc\xc5-\xd8\xf2\xddC\x03\xfa\x8a\x9ee}F\x93J\xe5[\xa0a\x8e\x12eW#wA\x8d\xb8=\xbd\xaa\xcd\xbd\xcf\x99\t+\x13H\x8b\xf7\xdc&\x06=\xf3\xcf\xe7UY\xe2\x10\x03\x87s\xbcs\xb7\xa0\xf7\xadH.\x16-&x\xcb~\xf1\x86\x06\xc6\'\x83\xc8\xc8\xf5\xa9\x94S\xb1I\xd8\x85Y\x19\xb7\xa8E\n\xe0\xec\x1d\xfd;f\xac\xad\x9c\xd2\xc2!\xc8y\xe5\x1b\x18*\xe0\xa9\'\x8ezW?\r\xf4\x8d>\x1aWX\xb3\xf3\x8cs\'\xf4\xae\x8e\xddV8\xe5\xd5\x8c\xca^\xdd6C\x01|\xe2S\xc0\x01Ga\x9e\xb4\xd46D\xb9ir-B\x08\xed\xd9\xa0\x12|\x96\xa3\xcb\x0c\x8f\x83\xc7\xde\'\x8cu\xefQ]jMmm\x040,\x88#l\xf9\xea\xd8\xdc?\xa5d\xecr\xcf\x18\x94\xb7\xcb\x87b6\x8c\x93\xd0\x93\xc9\xab\xf6\xb6\xaf=\xabi\xb2s\x83\xbe\t:\x81\xea\xb9\xa2n\xee\xe0\xb4E\x86\x96\t/\x1eC+K4\x80\xab\x18\xd7\xee\xe7\xabp+V\x0b\x06\x83L\x86\xe07\x9b\x139p\xd8\xfb\xd8\xedY:v\x97qb\xefsx\xec\xa0\x02\x91D\x0f-\x93\xce\xefn\xde\xf5\xdb\xfd\x93\xcb\xd3\xa1,\x11\xad\xe6\xd8\xeb\x18\x1fs\xdb\xebP\xd2*\xf71t\xc2\x91\xeb\x86\x16\x99\x9d\xe7\x87|H\xc7\x8d\x99\xef]D\xa5&\xbb[5$\xc6\xcc\xbeX\x8dA\\\x8eqX\x17:p\xb6\x9c\xce\xe5\xa2\x90\'\x96\x8cy\xf9s\xd0f\xb4\xf4\x8b\xb5\xba\xbc\x81\xc9\n\x8a\xdeDk\xb4`c\x92G\xd7\xa5E\xec=\xc7\xea\xba\xdb\xe8\xd1\x92\x18E\xe7\xeeTD\\\x9cd\x03\xf9\x9a\xcd\xb1\xd6\xcc\xda\xbd\xe6\xe3\xca\xfe\xedy\x07\x07\x19\xab\xde"\xb5IC\xc8#\x18\x8b\x008\x19*3\xdb\xd3\xa9\xaecJ\xb3\xfbn\xb3\x1d\x840\xb2BY\x9d\x86y`O\x07\x03\xdb\x15\xacj\xc9"9\x13:\xd1u\r\xac\xc9{$\xcca\x11\xf4a\x93)oAY\xd7fXn\xec\xfc\xa7s\x01o,\xb3s\xbb\x8c\xe7\xfaU\xdb\xcbW\x9e\xf4\r\xe3\xfb\x88\x88>A\xdb9<v\xabr[F\x96b,\x861\x80Q\xc7s\xdc\x0f\xc7\xbd/h\xdb\xb8\xec\x92,\x1bek;l\xc8\xc4G\x1e\xd2X\x9e\x0f\xad2\xd7N\x96\xda\x08\x9aW\xd8\x03cin\x069\xce?\xad6;\xabvD\xf3\xb7y y\x83\x03!\x98z\x9f\xad1o\xa4\xb8t\x81\x95v\x96\\63\x9c\xff\x00\xfa\xaa\xf9\xef\xb96\xb1\x81>\x8env3\x90wN\x01n\x9b=?\xfa\xf5\x7f[\x89\xbc\xdf&\xd4\xc7\xe6\xcc\xc5W-\xfc\x0b\xc0?\xa5jY\x18\xe7\xdb2\xb0\x96Ef*\x80n\x03\x00\x9c\x93Y\x17\xd6~u\xfbm\x91\x99\xa3O)Tp\x06q\x9f\xebD\xb7@d@\xdb\x19\x91\'I.\x19\xb0\xa4\xf2F=\x05hN\xc6\xd4\xdc\xcd#\x99|\xa1\xfb\xd2\xc3\x00\x82zg\xf9`Tv\xc8\x8b\xe7\\F\x00r\xc5\x10\xf0\x01=\xcf\xa9\xc5fk:\x90\xb6\xf2\x12yI\x8c\xca\x1eHPg\x8cq\x9f^{T\xab\xdc\x05\xd4\xee\xbc\xfd)6I\xb3`$\x1c\xe3+\xc0#\x04\x8f\xa7\xbdd\xd8\xf9\xa8\x86E\x8eH\xed\xb0K\tUc]\xd9\xec8\xe3\xdb\x9a5\t%I#/m\x1e\x18\xb1\x00\x8c\xb4g\xb8\xf4\x07\xa7L\xfdj\x91\x8c\x96b\xea\xee\x92c,\xceN\x18v\x15\xaa\x04h5\xc9\x95\xfc\xc5\xc9D\'n\x17\x03\x1e\xb5=\xcc\xc9u\x1e\xe6`\x19H\xe1NMf\xdeJ\x9f2\x1c\x8e\x02\x80\xc7\xbf\xbd\\\xb6\xb4\x98"-\xc2\x00\xb3\xb2\xa8\x94\x1c\xe4\xe3\xa7\xb5g5a\xa6@\xf7m\x15\xbc\x88\xb1\x92\xbb\xb7\x0c\xb7\xbfZ\xe8.\x9d\xce\x9e\x92\xa2\xe1\xd5U6\xee\xe4\x8e\xa3\x9a\xcd\x83K\x99-\xe7Y 2\x8d\xfb\x82 \xdcv\x8eN\x0f\xb1\xad=.\xd5\xaeDs\xc6\xce\x96\xf2&ry\xdaA\xe8OL\xe6\xb2\xb9K\xcc\xb1\xa4J\x0c\xd1\xa4\xaa#\x92E\x0c\xe5\xb3\x85\xf5\xae\xbdLo\x11\x00\x81\xe5\x00\xa4\xe3\x96\\py\xe9\\\xbc\x7f\xe8\xb7\x02Q\x01\xf3&*\x9eh\x00\xe0`\xe4\x81\xf5\xe2\xba\xd8`\x12\xe9\xa9<\xa8\tb\x17r\xf7\xc6:\x9a\x1b\xb0Z\xe7=2\xc8\xae\xb7^h\x8aD\nLk\xd4\x83\xd3\x9fS[\xda|\xfe}\xa21Y\x1eV\xe1\xd4\xff\x00\t\x1d\xfe\x95\x8d\xac\xcd\x1c\xb3\xac)\x1bF@\xe8?N}\xfbU\xbd\x1ey\xa1\x9c\xdb\xdc\xed\x12\x906es\x93\xe9\x9e\xf4\xef\xd4-\xa0\xfb\xf9\x90\xdd"\x11\x89d\xf9\xfe\xf1\x04`\xe3\xb5N\xf7\xc6\xe2\xd65t;D\x84Hq\x82=\xc7\xb5fMs4\xd7\x92\xc3p\x8agT\xdaHP\x08\x00\xd2h\xbb.\xeen\xcb1\x0f\x8e\x8ax\xc0\xe7\x81\xeaio\xab\x19\xa0\xe0\xb5\xd5\xab\xdb\xb0\xc0\x93pR\xd88\xc7\xf3\xf6\xa5\xbb\xbc\x91.\x8c\xee\x04{\xd7f\\\xed\xe0\x1c\xb1\x1e\xb5:\xc0\xb0(iU\x14\xc4w\xa9\xdd\xc69\xeb\xef\x83\xd2\xb1\x99b\xd5\xae\xe5Y\xd3y\x1d\x15\xbf\x84\x8e\xdf\\c\xf3\xa9j\xc0\x9a\xeaX\xb0\x86\x15\x82\xe6\xe1\xd5\xe4Y\x0b\x14R\xbc\x05\'\xad8\\,q\x08\x96]\xd3)\x0418\xca\x93\xd3\xebC\xdd\x8f\xec\xe90\xe3\xcd.\x15A=\xb3\x8c\xfe\x14\xcdN\x08n\xe3"x\x8a\xb8 #\xc6\xf8m\xd4%}\xc2\xe4w\xb2\x87\x95f\x91\x87\x97\xb4*)\xee\xdd+\x9c\xbc\xb8\x9e;[\x07C\xe5\x919\x19S\xc6\xdc\xf2\x07\xbdK%\xdc\xd1FR@\xbb\xe0,bv\xe8\xc7\xa0\xcf\xb8\xebN\x92&\x92K\x01\x03\xc7p\x11\xf1\xb0\x0e\x01\xces\x9e\xfe\xb5\xa4Wq7b7\x9f\xed\xb7\x90\xdd\x02\xa1\t(\xc8\x00,\xd8\xe3\xf9U\x1c\xfd\xa5\x84q\x94/\x10\xda\xdb\x87\xca\x0fO\xa7z\xce\xb8\x9ah.\xe7u#\x10H[\xcc\xe4`n\xc1\xfa\xd5\xb8\xdb}\xe4RL\x93F\xb2E\xbblx\x0c\xed\x91\xd4\x1e\x80\xe7\x82G\xe1T\x97Qh=\xa4i.C\xfc\xa1bm\xbb|\xdd\xb9\x03\xf8\xbd\xebJ0\xb3)W1\x81\x82\xd2y\xa7\x9cv\xeb\xd6\x96[\xa3s\x18k$\x8e6\xb6\xca\xddy*\t\x19\xe8\x0b\x1eI\xacIdq"\x11)m\xea\xccH<\xc4\xbd\xf2z\xf3\x9aN7\x0b\xdc\xd4\x83P\xba%\xa1\x8fd\xad\xe5\x92\x02\xf3\x8c\x0f\xba\x00\xf55=\xd6\x9d\xa7\xc9\xa25\xd6\xa3\x8f8\xa2*\xa4l\x0b+\x81\x93\x93\xd0\x0e\x9c\n\xe7\xe3\xbdKU\x96\xd6\xd9KI0\xc7\x9c\x17\x0e\xca}=\x07\xb55\xbc\xd7\xb1\xb4\x8d\xd4\xf9Qo`I\xeaA\xfetrkpe\xc6\xbd7\x11\xacYHlac\xba(\xce\xdf3\x81\x80q\xc9\xfa\x9a[K\xe3\xbd\'Y\xcf\x95+\x90|\xc2\x06\x10\x0eF{z\xf3\xe8+\x0e\xe5g\x9e$\x8e,\x13s*3!l\x01\xc7\x04\xf7\xc098\xab\xb6\xa7}\xf2\xc8c\xf3`\x84\xe6\x12W\xe5\xeb\x82q\xd3\xb6kf\xac\x8c\xd3\xd4\xebm\xa2\x8e;\xd7q\xb6h\\\t\x8b*\x02H\xc7\x04\xf7\xc7\xd2\xb4a\x89uS%\x9b\x1f-\x03eC/\x18\x1f\xcc\x83\xcdf[\xea\x04J\xf1\xc5#\xb4\xbbI\x0c\x0evg\xb6}}\x05j\xd8_\x1d\xd0\xc4\x8e\x99\xd9\xf3.>\xb9 \x9e\xf5\x93K\xa1}\x0c\x1dv\xcbf\xabi{<\xa0\xea6\xaa\xa9 \x03;\xd4t?^\xff\x00J\xe6\xee\xa6\x9bU\xb1\x93Kw*\xe9r\xec\xae\xc0\x90\xcb\x8e\x83\xb7\xbdw\xda\xbd\xbb\\\\G d\xdc\xa0\xa2\xbe0\xc4\x15\xe4g\xbdsM\xa0\xdc\xc1<\xd7\xd1]\x96\x80\xbcl\xa0|\xa4\x8c|\xca\x17\xebZS\x84\x99\x13i\x18\xf1\xd8\xc5\x14\x0f5\xb4\xe6\xe0\xda\xc8#E(\x082c\x93\xd7$\x0f\xf0\xad\x1d\x1fZ\xbc\xb1\xba\xfbE\x94nM\x9b\xe0\xc9\x82\x04\x8cNX\xa8\xef\x9e\x9fAV[K\xb0\x83R]\xed4P\xacx\x85\xe3\x9bn\x18\xf5\xc8\xc7\'\xafZ\xca\xba\xbd\xb5Y\x9e=*e\xb7C\x9d\x8d\xb7\x7f\xccp\x0b~\\V\xe95\xa9\x95\xfb\x9dm\xd6\xb1a\xacYM}u\x01X\xe7m\xdb\x84?q\xbf\xfdb\xb0 \xb7\xd3t\xbb\xb9\r\xc4"\xdeY\x02n\x13+|\xe7$\xf1\x9e\xcc?\xa5\x1atW\xf7\x1a5\xa4rM\xe6Y\xc5pY\x8ca\x80\x89\xb3\x9c\xb6{\x1a\xe8WK\xb7\xb9r.%,#\x8f\xf7n\xcb\xb9\xf1\x9e\x0eiJ\xa2e(\xd8\xce\xd64\x01sy\r\xe0\xb3\xfb4%\x07\xc8\xbb\xb6\x81\xd4\x02I\xce}\xb1T.\xec\xe5\xb2\x86\xe26b\x8a\xa8\xa7\x01y(zV\xe6\xa9\xa8\xea\x8d\xa8\xdaG9\xf3Lk\x98\xe5#\x00\x90~\xf1\xf4\x18\xabW\xd6\xfej\xc7)\x19\x16\xf9i\xb6|\xdb\x93\xa9\xe9\xefY\xd4q\xb6\x85\xc2\xe9\xea`Yi\x8bwg(\xf2\xf0\x88C\x08\\\xe3\x90y\x03\xf0\xc1\xa5\xbc\xd3\xa6\x93S\xb8\x82\x1bo\x9a\x10\xcc\x92\xb2\x823\x81\xc7\xb7s\xf8V\xf7\x87!\x8a\xe2\'\x8d\xe5\x0e\xe5\x7fvG\\d\xf1\xf9\n\xb7(\x16\xda\x95\xd0\x91<\xa8\x88$3\x0c\x99\x18\x8e\xbf\x9e\x06+\x9d;\xe8\xcd\x19\xc3N\x928B\x0c\xb3E\x1cD\x1d\xa7\x90\xc4\xff\x00\x9e)\xfao\xfa^\xaa\xc4F<\xc4\x18\xdc\xa7\x80:c\xeb\xfe5\xb9>\x8f\xfb\xcbK0w\xab\xa0g\xe0\x86\x07\x1c\x7fS\xf8\xd5$\xd3\xcd\xb5\xf0\xb6Y\x14\x95\xda\x1fk\x0ey\r\x9c\xfe\x00SS\xec\r]\x19\xf2\xc3,\xa5c1\x13\xe6JW%\xb1\x83\x8e\xa7\xf1\xad\x19t\xd9\xac-\'\x9c\x98\xcb2\xacaq\x9c\x12y?\x95:Uyu\x05\x8ef\x06\x19%|\x109\xdb\x9e\xb5\xaf$I=\x90\xb70\xb4J\xe5U\x9d\xb8v\x1d0=\x06*\xdc\xeeJ\x8d\x8eV8.V(\xfc\xf2Y\x98\xedB\x83\xeb\xc5Y{`\xc9\x129\xc2)\xde\x06=\xb8\xfc+\xad\x96\xd1\xad\x9ai\xa2 \xb0\x8bz\xa7r{\xe3\xde\xb9\xf9b\rd\x1cE\xe7\x15\xf9[2|\xca\x0fl\x01\xd0T\xb9s\r+\x1clV\xb2$\x96\xf1\xa4[\xfe\xd09\x1dp;\x1a\xe8\xe3\xd1n\x05\xb4\x86GX[\x7f\xceK\x02q\x8e\x07\xbf\xf4\xad\x9b\r1nnl\x9e\x02!D\x8c\xb1e\x03*\xa5z\x1c\xfb\xd4\xb6M$\xdb\x89ue\x84\xfe\xf8\x1e\x0b\x0eq\xd7\x8c\x1fJ\xa4\xee\x81\xbdN>\x1d9\x12y\xc8\x98\x17\x90\x18\xf0\x01\xf9X\x9e\x83\xdf\xdf\xde\x9bqg-\xa4&\xde7\xe2\xd0\x17\x95\x97\xf8\xa4n\xa3\xf0\x1cV\x87\xef\xafn\x19\xa0\xf2\x9c\x86m\x81\x93#\'\xb0\xfaz\xf6\xabwZ2\xe9\x9a]\xaa\xbc\xccod\xe5\x94r\xa4\xb7L\x06\xef\x8fN\x95WV\xb1-;\xdc\xe4\xad\x89\xbd\x0e\xa62\x9c\x16\xf9\xb0I>\x82\xb5\xec-\xc4\x86$(b\xda\xc0\x80\xa7\xe6\x18\xef\x93\xc7_\xad\\\xb5\x89-\xe6\xb6\xd9\x01B\xcd\xb2BO\x08\x83\xa9,{\x9f\xc2\xba;\x1d\x0e\xd2Kg\x9a\x0b\xb5{\x87}\xc8\x19\xc3o`}}\x05F\xc5\xbdJ\xd0D\x97\xb1\xc8$\x0b)\x98mx\xb9\r\x12\x8e\x85x\xe0\x9fc[\xf7\xaf\x14:\\P\xdb\xc0<\xe2\xaa\x91\xfc\xf9\x03\xeb\xf4\xa8>\xcft\x1aI\xe2\x9c}\xa5%\x00\x86P\x00\x18\xef\xedS^^oK)\xe2\x8902\xacy<\x9e\x0f\xb5E\xc2\xc6P\xd3uI\xa1\x91\x9aHndg!w&\xcd\xa0t\x0b\x83\x8f\xce\xacilm\xbe\xcc\x92\xc3#J\xa0\xbe\xd8\xd7vO\xd4V\x83\xcc\xd1\xcb:@\xa3\t\xf7w\x1e\xa7\xbf5\x0cK.\x9em\xdfp\x88I\x88\xe4-\xd8\x1f\xe5Y\xb7q\x94\xee\xefM\xf5\xac\xfeh6\xf6\x88\xaa$\x92Te.I\xed\xe8?SW\xb4x#\x8a7\xb9\x8a\x17\x06X\xf0\xac\xe3n\xe1\xec:\xfe4\x92X\x8b\xb9\x15\xfeW\xb2\x8a@\xed\xe6\x13\x87>\xa3\xd8\x0ei\xba\x85\xe3\xa6\xa4mb\x89\xa5D\x1f1\xc7@G=\xe9\xb7\xa5\x80u\xdd\xc0\x8d\xd8&\x19\xdd\xb0\x87\x19#\xdf\xf0\xe9\x8a\x92\xe9VX\xca;\x1f\xb8\x02\xe3\x8eGST\xec\x1d\xa7\xb6%z\x94uA\'PqQ@a\xb4\xbag*K\x12\x01Rr\x17\xd4\xfe4\xae;\x16\xaf\xdev\xd0\x93\xc9\x862\xe8\x83\x1c\xf1\xbb\xeb\xf4\xa9\x96\xdaO&9\x14G\xb62$c\x9es\x8e\xde\xb5\x9b<\xe2kW\x89\xddc/\x9c+\xb1\xc2\x1c\xf1\xedR\x8b\x88\xe5\xb5xQ\x8bF6+\xed\'\x83\x9c\x9ei\xa2K\x9au\xc1\xd3\xb4\xe7s\x12\x0f\xdc\x92J\x9c|\xecz\x9fST.dt\xb6I\xa1\x99D\xca\x01\xc1\x07\x04\xe0\xff\x00>i\x85^\xe1A{ic\xb2\x85N\xe7@B\xbbv\xe7\xa5P7nQ\xee#\x8b\xcdkrIC\xd1\x8e09\xe7\xa7\xadj\xa3"[W*\xd9\xc4\xf2i\xb0\xfd\xa6e\x12H\xbf,i\'1\xaes\xdb\x92}MW\xd5\xa4\x82\x0f"0\x88\xd2\xc7\xfc+\xd4\xb1\xe9\x91\xdc\x81\xfc\xea;\x14\x9a\x19\xe6\x99!\x0e\xecz\xa8%\x86y\xc0\xc7l\xfd\x05M|\xd3XA\x05\xd2\x88\xc4\x87\x87;C\x92Oa\x9e~\xb8\xaaVN\xc0\x8c\x1dBy\x96\x18\xec\xb6\x86d\xc0|/\n[\x92\t\xee\xdc\xfa\xf1M\xb8\xcf\x93)H\xf6G\x16\x15\x1f\x1f\xc4+z7\x95\xed&\xb9\xb8\xf2cE\x1b\xa0o*4%q\x93\xb4cq\xc7\xe1\xcdU\tu-\xb3B\xd1.\xc5\x01\xc0\xb8L\x94$\xf4\x19\xe8j\xd3]\x04\xd3([\xc5.\xa5s\x19KY$e\x85c@\x13pf\xeb\xc8\xff\x00\x1a\xe9\xec\xacM\x9d\xb2I=\xca\xf9\xb1\xc8\x0bE\x19\x12\x10@\xfb\xa4\xf4\x1fN\xb5\x8b$n4\xe8\xe3,\xe1B\xe0\x858\xc7\xa7\xd4\xd5\xb8#\x9a+U\xf2ef\xf3\x80\x0c\xc8\xf9<pA>\xa3\xb9\xac\xe7w\xa9it7]`\xb6\xb6y\xad\xa4\xf3br6\x83\xcbn?\xc3\x8e\xf4\xb6\xf6`Ed\xe3+\x1a\xc8\x1b\xec\xe0\xf5#\xefg\xdb\xda\x8f\x0f\xc39y<\xc7\x90\xc5\x1b\xb2\xab\x81\x80\x0f\xd7\xd4\xd5\xab\x9b\x93)\x91\x11\x03\xb9\x98$~X\xc3\x0c\xf0s\xf8f\xb2\x93[!\xa4h\\@\xe5\xecn\x9e\xde<,\x8c\xa1\x11\xb00z\x7f*\x8fX\xb9{\r"kl\xac\x87\xca\x04!8\xeaz\xfdG\xf4\xaa\xd7\x8fs\xfd\xa3m\x0cd\x14\x817\xb4\xcaN\x14t\x1b\x96\xa2\xd7\xe4f\x069\xe5\x94N\xc7\x08\xce\xa0\x83\x9f\x7fjV\xbb\x18\xf8\x98\xdd\xa42\xcb\xbf\x0cF\xd7^\x8c\xa3\x9csWn\x96\xe0\xc1\x04\xb0I\x99\x0f(s\x80\xc4\x1c\xf4\xfe\x95\x95\xa6\xdao\xb6\x10\x94\r5\xb9\xc0]\xe4e\x8fS\x9fO\xf0\xadf\xbd)s\x99#-\x0c\x800d\x1c\x06\x1dO\xff\x00^\xb4J\xc8L\xa8\xd2\xbaHd\x08\xbelq\x90\xc4\x903\x91R\xe8pZX\xe9\xf2JT\xc8\xcb \x03f@\xc9\x18\xfcI=\xaa;\xab\xf6\x9e\xdey a F\x12:\xb2\x000:\x0f\xff\x00]Y\xd3\xaf\x02\xc0\xd7r\xe5\x8bH\x1a \xe9\xf2\xc7\xf4\x03\xa9\xa5k\x00\xeb\x88\x9dt\xd9\xa69%\xc9~p0q\xfe\x15KF\x99\x85\x93K$K\xbe\\\x8c\xa8\xed\xeaM\\\xd6\xef7y\x96\xecF\xe9"\xf9\x1c\x9c\x0c\x9e\xc2\xab[\xed\x1e\x1d\x84F\xc0O\x00\xf9\xf9\xfb\xd9\xeb\x8f\xa5C\xd8e+v\xdbr\xc9"`D\xe5P\x81\xd4\x9ei\xd7\xb2\xcd#\t\x03!^W\x91\xcej8VY\xae\xdfp\xc1*yn\xa3\xdczT\x93\xb6\xf6\x19uT#\r\xbb\xb15:\xdc\n\x97\xccZ)`\x9e\xd8K\x90\x1c\x98{\x01\xd7\x8f\xfe\xbdc\xe9\xf7\xb1YIp\x8c\xb3*\x8c\x14\x0e\xb8w\xcfj\xd3\xd4$\x91!\xf2RG\xde\xd1\x1c\xe0\xf5\x03\xde\xb3\x15`\xbb\x97\xc8p\xc8\xec\xcawo\xef\x8e+jo\xa1,\x8eYH\x9c\x17\xb5+3\xcc\xc4(Q\x88\xdb\xb1#\xbf u\xaaou$q\xc7s5\xc17R0i\x9dPnlw\x07\xb8\xaa\xd3\\\\\xc9{%\xace\x8d\xcb6e~\x81\xb6\x9e\x07\xd3\x8a\x8a\xe5\xe4\xbeQ\x1c\x01\xc0\r\x86\'\x01NO\xf2\xcfoZ\xd5G[\x12\xd9j\xc0\xb3,\xe6\xd3x\xfbSz\xf6\x1dO\x1e\xe74\xeb\xef1cS\x91\xe7\xb81\xcb\xb4\xe40\x1d\x06*\xdc\x01mgo2V\x8e+x\x95HA\xfe\xb1\x89\xc9\\\xfdz\xd5I\xd5\xee\xae|\xd4\x872;d\xee\xf9{\xf1\xf8R\x92\xd7B\xa0e\x97\x96BH\x0f\x19PT\xa8\';G|\nkO"\x98\x16(Y\x9e\x08X\xb2\x15;\x99\x98\x9c\x1f\xafJ\xbd,o\x03}\xa6\x19\x8a\xc8\xbf\xbbx\xfa\xe3\xd0\xfb\x03\x8a\xbdl\xf2jz\x81[{\x7f-\xc2\xf9s\x01\xfc \x8c\x82=\x8d\r\xe8K\xdc\xa9\x14ook\xf6\xe3\x04\xb7O\xb0B\xb0\x91\x83\xcf,q\xc9\xee\x07\xe3W-\x16\xe2\xefOiv\xa6ae!X\x94\xda3\xd3\x8eN;\xfa\xd4\x9a\x86\x96nd\x82\xc8]\x83u\x04;T3`HA\xe7\xaf\x7fOZ\xbd\xa6Y\\\xa4\x8aY\x02\x96\x8c;*\xff\x00\x16\x0f zR\x94\x90\xe2\xbb\x91\xd8i1\xde\xc2\xd3\x83\xe4\xdc\x07\x91\x9d\x86p\xc0\x0c\x82G\xadi-\xed\xc0\x992\xaa\x970\xed8\xfe\xf0\xc7oPkf\xce$\x82;\xe0\xc8\xc8\xf0\xae\xf5#\x19!\xbat\xea0j;{\x06\x91\xe7\x17Q\xee\x89\x81\xf2\x9b\x1f0\x19\xe9\xf8T^\xe3-Z\xcb\x15\xf4ir\x83\x7f\x94\x03\xc8\x9e\xe3\xd3\xda\xb3\xa7\x85.3\xc9\x8e\x12\xf9S\x8eW\xdb\xff\x00\xad[\xfaE\xa2XYN\xab\xfb\xc8\xa49x\xc9\xf9\x81\xeei\x8bi\x14\x96\xf2\xc2N\xf3\xd1x\xc9Zt\xea\xb8;\x8aQ\xb9\x8b\xaah\x82\xe6\x18\xa2\xfbC\x08\xe4u\x8f\x06<\xe4\x1f\xeb\x8aj\xfc;\xd2\xcc\xf1\xdd\xd9\xb4\x91\x082#\x89\x9bpc\x8e\xa7\xe9]\r\xe4\x19\xb0\r\x1a\x91\'\xdc\x0f\x9e\xc3\xaej8\xdeh\x00I\xe3A\xe5|\xe1\xa3\xe4\xfd1Z\xbcE\xd6\xc4*D\x1a\\RXGsk4i\xe5\xb8\xdb\'\x19\xde\x08\xf4\xecA\xac\xf9\xb4\x9b\xed.)"\x8f|\xd1\x14S\xc8\xe5\x00l\xe3\xe9\x8a\xd1\x89\x9e+\xc3\xe5\x86x\x9b\xe6~y\xcfa\xfdqZ\xf2\xe6F\x80\x05\xdc<\xa2\x8c\x07\x04\x10x\'\xf35\xce\xe4ic\x9f\x89\x0b\xddy\x91\x91$O\x1ep\xe7\xee\x0fz\xd1\x96\xda2\xb3\x18"e\x91\x95Q\x8a7\\s\xcfj\x9e\xe3N\x85\xe2i\x11\x8cS0\xfd\xe3!\xfec\xebYQ\xc9<bi\x80\r$\x8d\xb7`?0?\x87\xe7R\xe4\xd8\xd2\x1bo\x1czU\xf5\x9d\xd9a\x1aO;\x07\xf9@#\x8cg5~\xe4\t\xae\xcc\xa0g\'\x10\xb3}\xdf\xff\x00_z\xce\xb9H\xe6\xd2\xe0\x12\xc6e\x85\xcbb<}\xdfB=\xf8\xebN\xd1\x8c\x97v\xd1\xb3\xca\xe2=\xa1\x9b=\x01\xe9\x8f\xafZ/\xd4v!\x92[\xa85{\x9b\xc7\x8d^\x15@\xa8b\x1c\x01\xc89=\xeb\x1a\xe6\xf2F\xb5G\xfb2\x86y2\xc7\xb6\x0fE\xc7Rq\xeb].\xaa\xb1\xc5er\x96\xe1\x91\xcc{6!\xe1\x86z\xfe\x15\x85b\x93K\xa6\xd9\x88\xd2\'"\x1d\xcc\xe7\x92@8\x19\x1f\xd6\x84\xfd\xe0\xb7\xbaKi\x04\x93\xc9\x05\xee\xd9\x03\xc6\xe4!8\xf9s\xd7\x8a\xd1\x8ekB\xd76\xf3FLA\xf7$\x84\xf0z\x9e\xbf\x95U\xb7E\xcb<L\xd2FF\x02\x00NA\xe3+\xfa\xd3\xe0kU\x99\xb4\xc9\xa6D\x88\xa9-\x92\x0e\xd2\x06v\xfe5D\x976Iob\x93\xa3\x03\xbdIR9\xc2\xff\x00\xf5\xeb\x90\x96S\x0b\xba\xa2\x063\x16\x04\xe3\x91\x9e\xe7\xd2\xba;\xf1,\x96q\xceI%[\xcbm\x87\x021\xd0q\xd3\x18\xaa7~\x1fm.\xe2\xdax\x9eId\xc9y\x01\xe7\xb7qV\x9a\x16\xa3|\xb94\xcdB\x0b3$\xb9e\x00\xa4`\x1d\xa3\xb1>\x95\x0bZ[4\xb7\x0cL\x8b\xb7\x05A\'\x96_OSZ7\x88\xb9\x9fiW\xb9\x94\x9d\xd2g\x8d\xdd\xb1\xd7\x9aP\xb1\xde[\xb2\xa8\xf2.!L2\xbes\x9fl\xf5\xa3\x9b\xa2\x0bu0\xee\x92E\xbe\xb5x\x1d\xd0H\x169\x15\xb2\xb9\xc9\xe4\xfa\x0e}kmt\xc14\xf7\xa4\xb0\xc2v?\xcf?\xd3\xbdCs\x96\xb0\x861\x89\x9e6V`\x1b?1\xe9\xf5\x02\xae\xc9=\xe2\xc0>\xcf**l\xf9\xe3\n\x01f\xees\xd7\xf5\xa8v\xea\xca\xe8U\xd3\xedm\xacf\x9eY7Iq \xc2\xa9_\x94\x81\xfcC?\xd6\x9d\x05\x93\xc6\xef4\x96\xd0\x82\xbf2\xcb\x8eq\xf5\xf4\xad(\xae\x9e\xd9?r\x82Hv\xfe\xf7ss\xee;c\xebH&\x91\x82\xa4m\xe6\x89\x97\x84\xdb\xd0\xd1f\xb6\x0b\x92\xa47\xd3\xc15\xda\xce\x11\xd4~\xf5v\xe3w\x1d\x7f\x11PZ\xa4p\xd8I\xe5\xab\x10\x06J\xc8\xdb\x88\x1d\xc0\xab\x9al0\xee\x85f\x9c\xe3,P\'@{\x03\xebIu,&\x16\x87\xce%\xc7\x0e\xc0`\x03\xedC\x04U%E\xfd\xac\x19\x01]w\x16\xeaH\xeb\x8a\x9a\xf0\xc7|\xbb\xa6\x8cH\xab*\x92\x87\xbe\x0f\x7fj[h\xde\x03\r\xc8\x0b,`\x03\x86\\\x9c\xf6\x1e\xc6\xaa\xce\xa6[K\xd7I\n\x07\x94m$d\xe4rp*:\x8c\x9a\xea\xe0E\x82\xae>g9\xc2\xe0/\x18\xc7\xe1QG(\x96i\xe6`\x11\x02\xee8\xfb\xce\xdd\x07\xff\x00Z\xab\x95\x92\xeaxT\x17\x7f3##\x80\x13\xd4\xfb\x9a|\x92\x94\x92[\x18d\n\xd1D\x0b2\x7f\x0f\x1cg\xde\x9d\xc0\x89/f\xbb\xd5v\xacm\x02[\xa1\xd8\xd8\x19\x1cc\x9e\xd9$\xd6dp\xdcZ\t\x01\x99\xa2*|\xcf\x9d\xc12/\xa9\x1c\x90=\xea\xce\x98\x17F\x86]J\xed\xa4\x95\xa5]\xab\xf2\xfd\xd0{\x9a\xc6\x9a\xe6\xfa\xe2K\x89\xde\x07X\xe4|\x93\x81\xf3\x0fBOq\x9a\xb8\xb6KF\xae\x9bso$\xb1\x89\xac\xe2\xba\x9d\xe4\xf97\xca\xec\x17\xd3\n\x08\x07\xeaj\xd4\x9a\xe3[\xdeMida\x16\xf1\xee\x06H\xa3\x1f3\x7f\x11\xf6\xf4\xeb\\\x9a\\\x1d\x16\x19\xa5.\xbfn\x9f>N\xe6\xc6\xc4\xee\xdfS\xd0T\xf0\xc0\xd6\xfaH\xbeWR\x81\xb6\x90\x99\xea\xdd\xa9\xf3HJ(\xbb\xaa\xeb77Q}\x9eI\x99\x84d`\x16\'\x1e\xdf\xfe\xaa\x8b\xed2_G\x1aHX\x7f\x08\xdb\xc37\xb1\xf4\x1csY\xe9\xb1\xaf$c\xb9\xc3\x8d\xc3,\x07\n3\xf8sM\xb7/%\xd4PB\x00\xfbA]\xc5\x0ex\xf64\xd5\xf7\x1bKcj\xc6\xc2\xda\x06\xb9\x89\xcb\\]B\xc8\xfeYm\xaa\xcaz\x83\xea;U\x86\xd4\xb4\xb9Y`k?\xb2H\xac\xc49\x9b \x8c\xfa\x93\xfaf\xb2?\xd0\x9a\xff\x00S\x9aH\x9b\xcc\x07\x124O\x96 0\xf55&\xa1\x13j\xb1@mbh\x87\x95\xbb2I\xb5\x97\xd0\xf1\xc1$UE\xea&\x95\x86I\x1a\xb3\xbd\xb5\x9a@\xca9RI<g$\x12I#\xf4\xa9-\xae\xae%\x92]\xca\xb8D\x02,ca\xdb\xcezrk1\xb4\xbb\xfb+v\x95\x12A\xb8\x02\xde_\xcf\xb8\n\xdc\xb4\xb3\xb8m.+\xf4ec\x8c\xaa\x85\xdb\x80z\x82=\xa8\x9bhj\xc3,\xf4\xab\xab\xcb\x16\x920\xb2ol\xc9\x13\xaf,3\xc9\x1f\xe4Tw\x894\x1eL^atI64dg\x00\xf1\x85\xc7\xa7\xe9W\xa1\xd5\xde\xd1dI\xd8l\xc3\x08\xfc\xb1\x8ej\xad\x84\xeb{\xa9!R\x8a\xaa<\xdd\x8f\xeb\xd3\x9c\xe3\x9aM\xe8\t;\x96ln\xa5\xb6\xb8\x9e\x02\xe6\r\xe00F\x04n\xf5#\xd3\x8fJ\xb1\x0bFVY\x94\x92\xcd\xb4\x16\x1c\x12s\xd8v\xac\xef\x12\xdb\x11\xa9D\xa0?\x9aQUT\x82\x00\xcf\xa3z}j\xad\xbd\xd4\xda|)\xbb\x1b<\x92U\xf6\x95,\xd9\xc1\xe4\xf55\x9b(\xebd\xb8\x10\xc3\xb4\xca\x81\xa4\xf9db\xa4\xef\xf4\x1d\x7f\n\xa1y{\x02\xdeFYL\x9e[\x16\x8aw\xfe\x11\x8f\xbb\xcf\xf3\xaeeu{\xab\xc7H\xc4\xa3\xcb\x91\xf6\xe7\x1c(\xeb\x81\xfe4\xaf\xa9\t\x92\x1b;Vgr\xcc\x02\x13\xb9s\xedE\xb5\x15\x8d!~\xed\xac\\D\xe6P\xcd \xf3T?,=\x01\x1d\xbd\xa9//n\xde\xec\xc4\xef\xf3\x03\x8cq\xf2\x8fOJ\xc7\xb1y\x12\xe7\xcd\xd8d\x11\xa03*\xf0T\x03\xd7\'\xaf\xf3\xa9\x95\xa13Ne\x97{\xc1.J7`~\xe9?\xec\xe0\x8a\xd2\xdd\x05tt&\x16\x83M\xf38gt \xb2\xbe\x14\xe7\xf9\x9a\xb0\x97\xd6\xf6\xfaT\x16\xd2\xa6\xd5\x00\x9c>K)\xedQH\x11\xda\x05\x05R>$f\x04\x0e\x9d\x17\xaf\x02\xab\xddH\xa98\xb8\x96M\xa3\xd3\x19\xcf\xf8T\xad@\x8fS\xd5m\xae/bFuv\xb7\x88\x14c\xc8\xe7\xdb\xf0\xab\xf7Z\x84qi\xd6\xd2E\xf3\r\x81J\x15\xc0\x07\xbdc\x1bH\xa4\xbd\xc9*\x01\t)8\xe3\x03\xb7\xaf\xff\x00\xae\xa0\xd5\xad\xa1\x96\xe9\xe4\x82FP\xc7\x80\xad\x85\x148+\x85\xcd}:\xe2\xe9L\xd2\xb8%J\xfc\x99\xf7\xebU%\x9e;\x89\xe33\xe1\x17\x1b\x99\xb3\xd3\x1cb\xaa\xc5\xaaM"\xe1\xe2U0\x12\x85y\xc9\xf7\xaa\xd2\\[\xf9\xaa\xb1\xc6e\x0c\xdb\xc9\xdd\xc2\x9e\xf9\xac\xf9_5\xd9W\xd2\xc3\xaf%\x91\x98,V\xc8\xaap\x0b\x92q\xb7=\xff\x00\xc2\xa9\xdc\x1bM?VGe\x9aL6UQ\xb1\x93\xdb5\xb0\xb7\xb2\xdaM\x1c\xdeZ\xb8\x8d~n\x84\x0c\xf2\t\xe3\x8a\xc8\x98\x9dB\xee+\x86\x10\xa4\xae\xc7\n@\x1c\x1e\x8dZ\xc1\xdd\\\x87}\x87jW6\xb77&\xea\x07\x01\xdd\t\x91I\xe8\xdd\x08?\xe1N\xb7)5\xe2M\xb1\xca\xa2)UE\xce\xe7\xc7@*\xf2\xe8\xf6\xb6K$wx\x8f\xcf\x01T\x03\xb8\xa9\xecI\xf55B\x15X\xdf\xc9\x8ey>\xd8\x18\xa8\x93\xfey\x01\xd0\x80?\x9do\xe6BzX\xd1{k\x8b\x88\xdek\x86Ao\x10\xf9\x96D\xd8P\xfd:\x93Q\\\xdd\xdaI\r\xb8\x86\t#!J\x90\xdf3?#\xd3\xa6}*\x11sq\x05\x9c\xc9,\x92\x1d\x87n\xeev\xc9\xcf,{\xb1\xaa\xb6\x93\xc3,\xa95\x9d\xdf\x95p2\x0cN\t\x1f\xfe\xaa\x86\xba\x94\x9fCN\xdf@\x8dnZ\xe1\xe2i"\xb8p\x0ch@9\xf7\xf4\xfeu-\x94P\xda\xf8\x9e[icx!g1\xc4\x13\x85\xdd\x8e\x9f\x863\x9a\xdd\xd1Y\xee\xdf\xca\xb8\x8d\xa1xp\xec[\xe6\x04\x9e:w\x1fJ\xd0\xba\xd3Vi!H\x16\x16T\x9by\r\xd5Nz\x8a\xc6N\xc5$r\xb7\x9a\x0c\xd3\xde\xb0Y\x12I\xa2M\xea\\n!\x80\xca\x9f\xe7]f\x97\x01\x99\x1dn#As\xb1Y\xbeP\x06\xecs\x83N{E\x8a\x7f\xb4JJy\x80\xe4\xf6\xce9\xfc)\xa9\x180\xcdr\xb9b\x8a\t\x00\xe4\x1cq\x9a\x8b\xdc\xab\x1a\xb1Z[yH~Y#\x90\x18\x8f\xd3\xa8\xe7\xf0\xa6\xcd\x02\xabM\xe5\xb08|\x949\x03\x9fJ~\x9eR\x18\x1c1F\x8d\xb0\x18\x83\xceq\xd4zU$Ce\x9d\xfb\xca\xb3\xee]\xe79#\xdf\xd2\x8b\x88\xd5\x86%x\xdc\xae7\x93\xbb\x93\xfc\xea\xbd\x9f\xee\xd6e2|\xf8\xce3\xd7\xe9Q\xc7\xa8\xad\x95\xb1t\x89\xa4\x9d\x8e\x15G }j\x15\xfbE\xd2\x92\xdb"\x8aF\xf9P.\xe2\x07S\x82jF<\xdc\x18\x9d\x91\xca\x94\xfb\xe7\x90\t\xcdT\xbb\xbc\x8c\xc8\x14,\x82Y\x06\xd4\x0c8oq\xf4\xa9\xe6\xb6\xb5\xb7\x9c\xc8\xdf\xbd\x1b\x01\x0cz\xfd\x056\xc5bYf\x9eP\xae<\xa20\xc7\xee\x95\xe7\xa7\xa5\t\x81\x1b\x18\xa6k]\xaa\xd0\xb8\'!\xbdGS\xcdOp6\xb80\xb7\x959l\xe7v\x01\xc7\xadU\xbb\x8d\xa70\xba\x9d\xceCw\xc1\xdd\xd7\xf9\nl\x0f5\xccj\xb2\xa9\x0c\x0f\x19\x00t\xech\xd5\x81\xb7\x14\xdf\xba\x13\xe1Y\t\xdb\xb4\xf7\xf69\xacT\xc9\xd5e$\xb2\xa8]\xce\xa3\xa0\xff\x00=\xaa\xe3/\x9bo2\x86h\xc3\x8c\x02?\x85\xb1\xc5W\x8a\xdf3\x03!\xcc\x8a\xa2)6\x8cy\x83\xa6A\xa5`\x1f5\xbe\xf9"\xc9\xf9\x01\xde\x14\xf4>\xbf\xcf\xf4\xa8!\x85\xede{8\xd0,O p\x0fA\x95\xf9\x89\xfe\x95u\x8c[\x80+\xf3\xc4\xc1\x90\x8e03\xfa\xd5\xa9\xed\x85\xd3\xe9\xf7(\xf8\x1b\xc7\x9a\xaa\xbdF3M\t\xb2\xb3\xc3\x14\xb2\xa2\xaan(\xbf{8\xce\x06+\x16\xc2\xcagwe\x91\x966\xb6\xda\xeb\x18\xe5FxQ\xee}\xeb\xa1\xfb4\xa2\xe24F \xb1\xdb\xbdF\xed\x9dwq\xeb\x8a\x99"\x82&\x92\xd6\x06h\xce\xef\xa9\'\xb6}\xb1M-n\x17\xd2\xc58"\x16\xbaq\x85\x00\x0c\xa1Bdt\xe3\xb7\xe1U\xf5\x8d=.\xec~\xce\xd2\xa4\x0e\xa8$\xdd\x8c`\x8e\xf8\x1dzU\xc9\xd1\xda\xe3e\xb6\x165l\xb1\x7f\xe2>\x80V5\xe9\xbc\x96x\xe1Q\xe6N\x14\x9c\x0e\n\x83\xce\rZZ\x8a\xe2\x0b[\x84\x80\xc0\x1f|\x9bD\xe4\\\x00\xca\xcc\xbe\x87\xb6j\x17y\xafg\xb5\x96P\xf1\x99\xb2\x1c7n\xfc\x9fJ\xd0\xb6\xb4\xb8\x9a\xd4\x04\xc6\xe6\xcfV\xe7\x8e\xd9\xaaicq<{\x8b\xb6\x04\x80m\'\x04\x0e\x9f\xce\x9b\xe5\xee-Jp\xcfnt\x8b]\x8a\xe4\xdcJ\xcd\xbc\x82\xaa\xa7=\x07\xaf\xa5Aw\'\x9b3\xdf\xabm\x91\x10\xa3\x058\n=@\xe75ph\xb2\x05\x81\xcd\xd4q\xc3\x14e#\x0f\x90\xb9<\x92;\x7f*\xca\x17>M\xd1\xb7X\xb7$yP\xd9\x05\x9c\xfe\x1cPU\xael\xc3\x05\xbc\x91\xdb,\x03\xe5(\n\xa9\xcf=9\xa9Z\x17\x94H\x99o\x91\xf20v\x83\xf8\xd6Dz\xc3Bb\x82\xe60\xd7\x11\xae\x06\xcc\xf2\x9e\xe7\x8cb\xad_j`\x0f1\x15\xb0y\xf9[\xa0\xf5\xac\xa4\xca\xb1<\x92H&0\x11\x19\x04\xec\xdcW8\xfa\x12?Z\xb1k2A V@\xdeX\x0b\x1a\xee\xe3q\xe35\x9c\x97\x8cg\x8eX\xd8\xa2\x91\x93\x19\xe3>\xf8\xa9\xb4\xb9q\xa8J\xd7\x0e\xa1U\x818\xfe\x15\xab\xbe\x82\xb1\xa9%\xecv\x93-\xba\x15|.C\x03\xc9n\xdcT\x17\x9e\\W\xa1\x99\x97\xb31\'\x92\xc7\xb0\x1d\xeb#W\xb8\x9c_-\xdbF\xd2B\xccUW\x18\xca\x8e\x87\xdf4\x97\xb3\x87\x0b1l\xbb0\x1c\x0e\x14z\x03\xf4\xa9z\x01\xbb$\xc5.#\xf3\xe4\x11\xc6\x8f\x8d\x84\xf0Kt\xc5:\xedP2i\xeb(\xde#f`;g\xa9?\x87j\xa5:\xbc\x96\xb0\xcd7\xcf\na\x9b\xe5\xc9~3\xc0\xa2\xd3\nN\xa4\tg\xb9B\xa1\xc8\xe5GlzR@^\xb1_-c]\xa5\x8ad\x16\xcf%\xbb\x0fj\x86\xd6\x04\xfe\xd0\xbbY\xb7r\xd9p\xa3\x80O\xa9\xeex\xfc*K\x1b\x95\x94\x98\xceB\x7f\xac\x92D8,{\xf3\xd8q\xd6\x99\x05\xfd\x9cs4\x087u1E\x0f\xf1\x1fR\xde\x83\xb9\xa6\x84f\xea\x96\xedyo\x1b&\xe3\x127\xcb\x8e\x14\xb7A\xeek\x9e\xfb%\xcd\xe6\xa2\xb6WP\xb2\xdbF\x0b\xbc\xca[b"\x8c\x90;\x12\x7f\xadu\x0ft\xd3$\xec\xc0!\x87\xfd^\xd6==\xb3\xd0\x12y\xf5\xaa>K\x88\x1c\xdc\xee\x9a\xee\xe0}\xec\x1f\x95Ga\xe83\xcdk\x17\xd0L\xe3\xbcEoq\xa8\xb2\xca\xc1cs\x90#?\xc2\x83\xee\x8f\xca\xb44x.\x93E\xb9\xb5\xb8\x8c>cY"\xdc{n\xe2\xae\xdbX\xfd\xad\xae\xa0\xba{}\xf10}\xbb~P=\xfb\xfe\x15xi3j\xab4\xb0\x15\x82X\x9dT[\xc2\xa4\x96\x1c\x12G\xb6?\x9dt{?v\xddH\xe7\xd4\xc7\xb8\xb5\x8bN1#sr\xf6\x92M7 l\xdcp\xa3\x9e\x9cV\x7f\x87\xa4Kk\xf8\x95n\xe3h\xa2}\xcc\xfd\x90\xf4\x03=\xf95\xd6x\x8bLyf2\x98\x11\x1e\xe7\xcb\x8eGg\x00F\x80g\x04\x9e\x95\xcf\xc9\xa1\xf9:\\mi4\x8f\x07\x9a\x1b\xceD\xc2\x1c\x1e\x8a\xc7\x00\x9c\xe4\xe6\xb2Q|\xa5_Rm>\']R\xf2Y\xdb\x11\xe1\x86\xe9\x139\x00\xfe\xb5fS-\xdc\xf1\xc9aa/\xd9\xcaanenO|\x03\xd3\xf2\xfc\xea\xe5\xbd\xad\x8d\xdc\xe8\x18n\xce\xe22\xdf! \x7f\x10\xef\xf8U{\xb4\xb8\xf2\x10\x14"A*\x1d\xb1\x1c.\xdf_a\xedP\xd1I\x94\xa3\xfbm\xbby\xf2K\x98\xf9-\x08rXz\xf3\xeb\xda\x85\xd4\xe5\x11\x81\x0c\x8c\x96\xcc\xf9*\xfc:{s\xda\xa5\x8ek\x89ds\x0c\x05\xd7v\xd5v\x1c.y5\x1c\xb8\x8eeY\xe3\x12Z\x827\x129\x15<\xcf\xa8\xedbK\xe3\x1cp\xe0H\x15\xb2\x0eXr\t\xee=\xab6{\x87\x8aq.\xd6\xdc\x84\xa8\x91\xbf\x9f\xd6\xa5\xbe\x88\xa4\xae\xf1d\x15e\x01Y\xb3\x95=:\xd4W2}\x9b\xf7k1O0\x162y{\x8f\xb8\x15pWZ\x93{2[[\xd9\xf59X\xad\xdc\x85\xc3r\x00\xce\x00\xeb\xc0\xa6\xdd\xb3\xdf\xe9\xed\x18\x908\x8d\x8aF\xacF\x02\xe7?\x8f\x1f\xce\xb0\xad/.\xa7\x9e\xd6\x1d>\x11l\xc1\xb6\xc72\x83\xf3s\xcb\x1f\xadi[\xcf<\xd3\x9b}HE\x84\x90\xa7\x9d\x14Cr\x8e\x00 t\xedN\xd1\xb8\x94\x9c\x8a\xb1^\x8b&f\x86\xdfw\xcc\x17\x0f\xf7zt?\xfdj\x96\x15\xb9T2\xca<\xb8\xc4D\xa1\x8cp\x99\xe9\x93\xd7\xaf\x18\xab\xb6\xba\x07\x9e\xac\xcbwn\xd1\x99\x0f\xc8\xed\xb2D`x8\xc7z\x96\xff\x00Krm\xa0\x8dZb\xac\x1aB\x01\xd8\xcc=q\xd2\x9f(s\x95l\xe6\xfb=\x8a\xb1\x8a\xe0\xdcL\xe3y(T\x05>\xe4r>\x9e\xfc\xd4\xa0\xc4e2:9\xb8\x91\n:.0\n\xf1\xc9\xfab\xa3kYSY\xbci\xc4\x88\xcf\x8d\xb1\xa3\x1f\x95\x7f\xd9\xc1\xae\xb6\xcbE\xfb[\xce\xf6\xce\xca\xccWn\xfc\x1c6\x06A\xf5\xc8\x14\xa4\xec\xd5\x84\x96\x9a\x986\xda\xa3\xb4\xf61\xacJ\xa2`wn-\x81\x8e\xa0\x9c\xf3Z\x93\xbf\xd9\xef3\xb0\x98\xdaP\x19\x86\n\xa1\xff\x00&\xb3\xa7\xb2\xb8hnb$\xc7$\x12\xb3o\x1f{\x19\xe4\x01\xe9[\x05m\xa2\x8a\xdc\x03!\x9ba\x95\xb7\x01\xf7[\xe9I&\x9d\x8a\xba\xb1\x89:\x88X\xc9\x15\xcf\xca\x9b\x94\x95\xee\x9e\xb9\xa8\x9a\xc3\xed\x97\x10\x9bK\x80\x15\xd3$\x91\xc2\x9fz\x89\xe7\xc5\xc1\x82\xe3\xe4\x85I\xdc\x01\x07+\x9e1\xda\xaf\xe8\xd0\x0f\xb2\xdcC\x00g\x94F%\x1c\xf2\xc1Oj\xd2N\xd7\x12Fm\xd3\x18\xde6\x99[\xcdB\x11\x82\x9e\xe3\xd0\xf7\xa7#\xba\xcaV\x08\x80#\xe6b\xc7\xa9\xef\xc5M$\x9b\xb5$\x12\xb6\xe8\xe4\x1b\xe2|`c\x1d\xfe\x95A\xa5v\xbb\x12\x18\xf1\x10\xe7#\x82Go\xcc\xd4\xc6)\xab\x89\xbe\x85\xbb\x94\x96\xe7E\x9d-\x90\x9ew\xcb\x82\x06H\xe8=\xfa\x8a\xa6\xd1]\xda\xc3\x1b\xca\xcb\x96\\o?\xc3\x8eq\xf5\xad}:H\x99\x95J9\x95\xbe\xee\xde\xa1\x8fLT:\x9e\x97;\xc5i\x14\x92\xfe\xe4\xc8K\x16\x1f\xc5\x82\t\xcf\xbdJ\xbav\xe8^\x84\x86\xc0^B&\x96\xf26\xdc>b\x8c\x7f\x01\x9a\x81\xb4\xcbh/\xe0\xb8\x9dduN\x1b\x03\x97\xcf\xadK\x16\x99y\xa4y\xb3\x98\xc1%6\x85\xf3\x06\x18z\xe2\xb5\x13\xcf\x90D\xc2\xd3*P\x91\xe6?L\xfa\xfd+w+#+\\\xb95\x9d\xa5\xce\x9e\xd1\x05S\x14H\x19\x95A\xe4\x9e\xd4\xc4\xf0\xcaG6\xfb4\xf3-\xc4@\x98\xa4\x88n\xc1\xea\x14\xf7\xfc\xea\x89}GC\x94;\xc6\xf3X\xbe\x0b\xed\x04r+\xb0\xd1\x1b\xcf\xb0\x8e[;\xa8\x92\xd2C\x96\x8aS\xb8\xc7\xc7N\xc7\xaf\xa95\x94\xe4\xeeR\xd8\xc0\xd3\xaf\xe2\x8al\xbf\x9d\xb1\x08\x03r\xedh\xc6y\x0c\x0f\xf0\xfd+\xa9\xb2H\x0c\xb7\x17O\xb7k\x02Upr\xbe\xfc\xfa\xd6.\xa5a.\xa0\xf2D\xb7\x90\xac\x8f(S&\xc06\xaa\xfaV~\xa7\x0e\xadnZ\x14\xbb\x17\x10\x08\xb1\x95?1\xf6&\xa6Q\xb8\xd3F\xdc\xcf\x0b\x17\x84\xb8hG\xdd~\x8b\x93\xdb\xe9Y\xee\xd3\x1b;\x88,\x1dB\xcf\x94C\xd4\x93\xdf\xf0\xac]\x16i\x9e\xd6\xee\xd9\xa4\x96H\x0c\xb9\x85{\xb1\xc7\xdd>\xd5\xbbigu\x10\t."\x99Wv\xf2\xa4\x84\xf6\x02\xa3\x92\xda\x95\xcd\xd0\xbb\x1c\xcb\xa5\xda\xc5\x03J\x1d\xc0\nT\xff\x00\x17\xbd:[\x89\xaeol\xe3\x8e\x16\x8e9_\x0eX\xe3\x8ey\xfci\xf6zB\x1b\x95\xd4\xaen\x04\xacI\xe7`\x00c\xd2\xa4\xb8\xf2n%\x17!\xd8ym\x80\x13\xf8\x7f\xfdU,\x11\x00\x9b\xec\xd2\xcd\x12\xa9\xfd\xdb\x1d\xa79\xe0\x8a\x92\xd6\xf5\xa5\xb4`2D/\x92q\xcf\xb8\x02\x9e\xc1\xd5\xf3(\x04\x91\xb5\xcfB\xf9\xe4\x1cz\xd4\xabc\xe5\xdeF\xaaG\x97.X\xa8\xf4\xf4\xfa\xd4o\xa8\xc8\xe2\x89\x8e\xe9\x94\xfe\xeeA\xc2\xb0\xc9Q\x9f\xf1\xa8\xae\xcb-\xe9\x11\'\x98\xaf\x1e\xd2\x1b\xa2\x93\xceO\xebW\xed\x10bvbS\x0ec\x00\xf6\x00\xf1Y\x96\xcaSW\x91\xa5F\xd9 \nq\xd7 \x9e?\x95\x08E\xa4D\xba\x81Lr*\x96q\xb5\xc0\xce\xcd\xb9\xcdK\x08I\n\xcb\x0b(V\xcb\x17\xdb\xd7\x07\xa7\xf5\xabv1:i\xcf\xb1@1\xcaX\x800p?\xadAc\x0e\xd1(u\x11\xabK\xe6&?\x91\xfdj\xd0\x89o\x10\xda\xb3\xbc\xac\x8b\x1c\x8c\x08\x188\xddP1Qq\x1c\xac<\xb0\xcb\x9e\x7f"\x07\xb6qZwV\xab\xa8Z}\x9aU;p6\xb7\xb8\xe9QC\xba{53D\xb1\xbc@\xa7\'\xbe9\xff\x00\x1a\x1a\x1a0d\x90.\xaa"\x1b\x99\x11\x0e\xf2x\xea\x7f\xfdU\xb5k\xe6G4q\x19\x04p\xb2\x83\x9c\x8er:U\rI<\x9f-\x9c\x05g\xd8\x0e;\x8e\xff\x00\xa50H\xb0~\xf9\x86\xd8\xf6\x15\x8c\x16\xcf\x03\xbf\xe7DU\xd8=\x8bKq4\xf7\xc8\xb6\xe4,hH/\x9e\xa4\xf5?\xe7\xd6\xad\xc5\x13\xdb\xdcH\xea\x15\x8c\x87\x1b\x88\xe8\x05Q\xd3\xdc\xbd\x9e\xf8\xe4Y\x93\x1b\xf2\xb8\xeb\xe9W\x16\xf6as\x86B\xb0\x04\x1f\x99\xa6+\x14\xe7\x96@\xf16\xed\xb8\xc2\xb7l7^*+\xa66\x92\x1b\xc4\xc4\x82y2\xc4\x0eq\x8e\x94\xb3\x18\xe4\xd4\x1d\xf7\x8d\xa8\xd9\xc7l\xf0sU\xb5\x0b\xa9\x04o\x12\xca6\x12\x14\x1d\xb8\x07\xbf\x14\xe3\x17{\r\xb4_\xd3\xe0x\xaeL}w\xe0\xe4\xf29\xe7\xf3\xa9\xee\xac\x8aj0<@\xec\x07t\xa3\xb1\x1e\x95Z\t\xee\x928\x80\x8c\x05P\x06\xfe\x84\xd4\xad}"\x99\xf2\xbb\x93\x1f>9\xc5\x1c\x82\xbd\xcf\xff\xd9'
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/tinyvit/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/tinyvit/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/tinyvit/tinyvit.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/tinyvit/tinyvit.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,11 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import math
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format, initializers
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     add_with_layer_scale_and_drop_block,
     batchnorm_with_activation,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
     inverted_residual_block,
@@ -13,128 +14,140 @@
     mhsa_with_multi_head_position,
     window_attention,
     MultiHeadPositionalEmbedding,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
-LAYER_NORM_EPSILON = 1e-6
+LAYER_NORM_EPSILON = 1e-5
 
 PRETRAINED_DICT = {
     "tiny_vit_11m": {"imagenet": {224: "6cc52dda567fd70f706d57c69dfd81d8"}, "imagenet21k-ft1k": {224: "f84673169e5c7a4ec526866da24cd5f4"}},
     "tiny_vit_21m": {
         "imagenet": {224: "5809a53bc3abe0785475c3f2d501a039"},
         "imagenet21k-ft1k": {224: "08d16dd06ddd85c2e4d2d15143c24607", 384: "fe6d364e99fa5a7f255ad3f3270bc962", 512: "ba5822042f0cb09bd8189290164b6ec3"},
     },
     "tiny_vit_5m": {"imagenet": {224: "a9c53f53e6da6a9b2edf6e773e5e402b"}, "imagenet21k-ft1k": {224: "cd10dbebf0645769dcda3224a0f330c4"}},
 }
 
 
 def tiny_vit_block(inputs, window_size=7, num_heads=4, mlp_ratio=4, layer_scale=0, drop_rate=0, activation="gelu", name=""):
-    input_channel = inputs.shape[-1]
+    input_channel = inputs.shape[-1 if image_data_format() == "channels_last" else 1]
 
     """ attention """
-    nn = layer_norm(inputs, epsilon=LAYER_NORM_EPSILON, name=name + "attn_")
+    nn = inputs if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(inputs)  # channels_first -> channels_last
+    attention_block = lambda xx, **kwargs: mhsa_with_multi_head_position(layer_norm(xx, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "attn_"), **kwargs)
     nn = window_attention(
-        nn, window_size, num_heads=num_heads, attention_block=mhsa_with_multi_head_position, use_bn=False, qkv_bias=True, out_bias=True, name=name + "attn_"
+        nn, window_size, num_heads=num_heads, attention_block=attention_block, use_bn=False, qkv_bias=True, out_bias=True, name=name + "attn_"
     )
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_last, channels_first
     attn_out = add_with_layer_scale_and_drop_block(inputs, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "attn_")
 
-    pre_mlp = depthwise_conv2d_no_bias(attn_out, kernel_size=3, strides=1, padding="SAME", name=name + "pre_mlp_")
+    pre_mlp = depthwise_conv2d_no_bias(attn_out, kernel_size=3, strides=1, padding="same", name=name + "pre_mlp_")
     pre_mlp = batchnorm_with_activation(pre_mlp, activation=None, name=name + "pre_mlp_")
 
     """ MLP """
-    nn = layer_norm(pre_mlp, epsilon=LAYER_NORM_EPSILON, name=name + "mlp_")
+    nn = pre_mlp if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(pre_mlp)  # channels_first -> channels_last
+    nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "mlp_")
     nn = mlp_block(nn, input_channel * mlp_ratio, activation=activation, name=name + "mlp_")
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(nn)  # channels_last, channels_first
     nn = add_with_layer_scale_and_drop_block(pre_mlp, nn, layer_scale=layer_scale, drop_rate=drop_rate, name=name + "mlp_")
     return nn
 
 
 def TinyViT(
     num_blocks=[2, 2, 6, 2],
     out_channels=[64, 128, 160, 320],
     block_types=["conv", "transform", "transform", "transform"],
     num_heads=[2, 4, 5, 10],
     # window_sizes=[7, 7, 14, 7],
     window_ratios=[8, 4, 1, 1],  # For `input_shape=(224, 224, 3)` will be window_sizes=[7, 7, 14, 7], for `(384, 384, 3)` will be `[12, 12, 24, 12]`.
     mlp_ratio=4,
+    strides=[1, 2, 2, 2],
     input_shape=(224, 224, 3),
     num_classes=1000,
     activation="gelu",
     drop_connect_rate=0,
     dropout=0,
     layer_scale=0,
     classifier_activation="softmax",
     pretrained=None,
     model_name="tiny_vit",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
 
     """ Stem """
-    nn = conv2d_no_bias(inputs, out_channels[0] // 2, kernel_size=3, strides=2, padding="SAME", name="stem_1_")
+    nn = conv2d_no_bias(inputs, out_channels[0] // 2, kernel_size=3, strides=2, padding="same", name="stem_1_")
     nn = batchnorm_with_activation(nn, activation=activation, name="stem_1_")
-    nn = conv2d_no_bias(nn, out_channels[0], kernel_size=3, strides=2, padding="SAME", name="stem_2_")
+    nn = conv2d_no_bias(nn, out_channels[0], kernel_size=3, strides=2, padding="same", name="stem_2_")
     nn = batchnorm_with_activation(nn, activation=None, name="stem_2_")
 
     inverted_residual_block_kwargs = {
         "stride": 1,
         "expand": 4,
         "shortcut": True,
         "is_torch_mode": True,
         "use_last_bn_zero_gamma": True,
         "activation": activation,
     }
 
     """ stacks """
     total_blocks = sum(num_blocks)
     global_block_id = 0
-    for stack_id, (num_block, out_channel, block_type) in enumerate(zip(num_blocks, out_channels, block_types)):
+    for stack_id, (num_block, out_channel, stride, block_type) in enumerate(zip(num_blocks, out_channels, strides, block_types)):
         stack_name = "stack{}_".format(stack_id + 1)
         if stack_id > 0:
             name = stack_name + "downsample_"
-            expand = out_channel / nn.shape[-1]
-            nn = inverted_residual_block(nn, out_channel, stride=2, expand=expand, shortcut=False, is_torch_mode=True, activation=activation, name=name)
+            expand = out_channel / nn.shape[-1 if image_data_format() == "channels_last" else 1]
+            nn = inverted_residual_block(nn, out_channel, stride=stride, expand=expand, shortcut=False, is_torch_mode=True, activation=activation, name=name)
 
         is_conv_block = True if block_type[0].lower() == "c" else False
         num_head = num_heads[stack_id] if isinstance(num_heads, (list, tuple)) else num_heads
         # window_size = window_sizes[stack_id] if isinstance(window_sizes, (list, tuple)) else window_sizes
         window_ratio = window_ratios[stack_id] if isinstance(window_ratios, (list, tuple)) else window_ratios
-        window_size = [int(tf.math.ceil(nn.shape[1] / window_ratio)), int(tf.math.ceil(nn.shape[2] / window_ratio))]
+        height, width = nn.shape[1:-1] if image_data_format() == "channels_last" else nn.shape[2:]
+        window_size = [int(math.ceil(height / window_ratio)), int(math.ceil(width / window_ratio))]
         # print(f">>>> {window_size = }")
         for block_id in range(num_block):
             name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
             if is_conv_block:
                 nn = inverted_residual_block(nn, out_channel, **inverted_residual_block_kwargs, drop_rate=block_drop_rate, name=name)
                 nn = activation_by_name(nn, activation=activation, name=name + "output_")
             else:
                 nn = tiny_vit_block(nn, window_size, num_head, mlp_ratio, layer_scale, block_drop_rate, activation=activation, name=name)
             global_block_id += 1
 
     if num_classes > 0:
-        nn = keras.layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
         nn = layer_norm(nn, epsilon=LAYER_NORM_EPSILON, name="pre_output_")
         if dropout > 0:
-            nn = keras.layers.Dropout(dropout, name="head_drop")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
 
-    model = tf.keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "tinyvit", pretrained, MultiHeadPositionalEmbedding)
     return model
 
 
+@register_model
 def TinyViT_5M(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
     return TinyViT(**locals(), model_name=kwargs.pop("model_name", "tiny_vit_5m"), **kwargs)
 
 
+@register_model
 def TinyViT_11M(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
     out_channels = [64, 128, 256, 448]
     num_heads = [2, 4, 8, 14]
     return TinyViT(**locals(), model_name=kwargs.pop("model_name", "tiny_vit_11m"), **kwargs)
 
 
+@register_model
 def TinyViT_21M(input_shape=(224, 224, 3), num_classes=1000, activation="gelu", classifier_activation="softmax", pretrained="imagenet21k-ft1k", **kwargs):
     out_channels = [96, 192, 384, 576]
     num_heads = [3, 6, 12, 18]
     return TinyViT(**locals(), model_name=kwargs.pop("model_name", "tiny_vit_21m"), **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/uniformer/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/uniformer/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/uniformer/uniformer.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/uniformer/uniformer.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,18 +1,23 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
+    add_with_layer_scale_and_drop_block,
+    batchnorm_with_activation,
     ChannelAffine,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
     drop_block,
+    layer_norm,
     MixupToken,
     mlp_block,
+    scaled_dot_product_attention,
     output_block,
     add_pre_post_process,
 )
 from keras_cv_attention_models.download_and_load import reload_model_weights
 
 BATCH_NORM_DECAY = 0.9
 BATCH_NORM_EPSILON = 1e-5
@@ -26,90 +31,87 @@
     "uniformer_small_plus_32": {"imagenet": {224: "7796cce29b5ea6572330547ba7eb5e0d"}, "token_label": {224: "b1d32f5e5714b66d76ef2fecce636dfb"}},
     "uniformer_small_plus_64": {"imagenet": {224: "7d10381f4527496adb2d39c4a665c808"}, "token_label": {224: "15d6af207a0f09957a5534ae1ad540ed"}},
     "uniformer_large_64": {"token_label": {224: "b1020b4e8029209a326e8fe7183d7d28", 384: "809ba104d43e905d5b24a8ec6ee02bdd"}},
 }
 
 
 def multi_head_self_attention(
-    inputs, num_heads=4, key_dim=0, out_shape=None, out_weight=True, qkv_bias=False, out_bias=False, attn_dropout=0, output_dropout=0, name=None
+    inputs, num_heads=4, key_dim=0, out_shape=None, pos_emb=None, out_weight=True, qkv_bias=False, out_bias=False, attn_dropout=0, output_dropout=0, name=None
 ):
-    _, hh, ww, cc = inputs.shape
-    key_dim = key_dim if key_dim > 0 else cc // num_heads
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
-    out_shape = cc if out_shape is None or not out_weight else out_shape
+    input_channels = inputs.shape[-1]
+    blocks = inputs.shape[1:-1]
+    key_dim = key_dim if key_dim > 0 else input_channels // num_heads
+    out_shape = input_channels if out_shape is None or not out_weight else out_shape
     qk_out = num_heads * key_dim
     vv_dim = out_shape // num_heads
 
-    qkv = keras.layers.Dense(qk_out * 2 + out_shape, use_bias=qkv_bias, name=name and name + "qkv")(inputs)
-    qkv = tf.reshape(qkv, [-1, qkv.shape[1] * qkv.shape[2], qkv.shape[-1]])
-    query, key, value = tf.split(qkv, [qk_out, qk_out, out_shape], axis=-1)
-    query = tf.transpose(tf.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  #  [batch, num_heads, hh * ww, key_dim]
-    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
-    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, vv_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, vv_dim]
-
-    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale  # [batch, num_heads, hh * ww, hh * ww]
-    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attention_scores)
-    attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + "attn_drop")(attention_scores) if attn_dropout > 0 else attention_scores
-
-    # value = [batch, num_heads, hh * ww, vv_dim], attention_output = [batch, num_heads, hh * ww, vv_dim]
-    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])
-    attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
-    attention_output = tf.reshape(attention_output, [-1, inputs.shape[1], inputs.shape[2], num_heads * vv_dim])
-    # print(f">>>> {attention_output.shape = }, {attention_scores.shape = }")
-
-    if out_weight:
-        # [batch, hh, ww, num_heads * vv_dim] * [num_heads * vv_dim, out] --> [batch, hh, ww, out]
-        attention_output = keras.layers.Dense(out_shape, use_bias=out_bias, name=name and name + "output")(attention_output)
-    attention_output = keras.layers.Dropout(output_dropout, name=name and name + "out_drop")(attention_output) if output_dropout > 0 else attention_output
+    qkv = functional.reshape(inputs, [-1, int(np.prod(blocks)), inputs.shape[-1]]) if len(inputs.shape) > 3 else inputs
+    qkv = layers.Dense(qk_out * 2 + out_shape, use_bias=qkv_bias, name=name and name + "qkv")(qkv)
+    query, key, value = functional.split(qkv, [qk_out, qk_out, out_shape], axis=-1)
+    query = functional.transpose(functional.reshape(query, [-1, query.shape[1], num_heads, key_dim]), [0, 2, 1, 3])  #  [batch, num_heads, hh * ww, key_dim]
+    key = functional.transpose(functional.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch, num_heads, key_dim, hh * ww]
+    value = functional.transpose(functional.reshape(value, [-1, value.shape[1], num_heads, vv_dim]), [0, 2, 1, 3])  # [batch, num_heads, hh * ww, vv_dim]
+    # qkv = functional.reshape(qkv, [-1, np.prod(qkv.shape[1:-1]), 3, num_heads, key_dim])
+    # query, key, value = functional.transpose(qkv, [2, 0, 3, 1, 4])
+    # key = functional.transpose(key, [0, 1, 3, 2])
+
+    output_shape = [*blocks, out_shape]
+    attention_output = scaled_dot_product_attention(query, key, value, output_shape, pos_emb, out_weight, out_bias=out_bias, dropout=attn_dropout, name=name)
+    attention_output = layers.Dropout(output_dropout, name=name and name + "out_drop")(attention_output) if output_dropout > 0 else attention_output
     return attention_output
 
 
-def block(inputs, out_channel, num_heads=0, qkv_bias=True, mlp_ratio=4, mlp_drop_rate=0, attn_drop_rate=0, drop_rate=0, gamma=-1, activation="gelu", name=""):
-    is_conv = False if num_heads > 0 else True  # decide by if num_heads > 0
-    input_channel = inputs.shape[-1]  # Same with out_channel
-    pos_emb = depthwise_conv2d_no_bias(inputs, kernel_size=3, padding="SAME", use_bias=True, name=name + "pos_emb_")
-    pos_out = keras.layers.Add()([inputs, pos_emb])
+def attn_block(
+    inputs, out_channel, num_heads=0, qkv_bias=True, mlp_ratio=4, mlp_drop_rate=0, attn_drop_rate=0, drop_rate=0, gamma=-1, activation="gelu", name=""
+):
+    pos_emb = depthwise_conv2d_no_bias(inputs, kernel_size=3, padding="same", use_bias=True, name=name + "pos_emb_")
+    pos_out = layers.Add()([inputs, pos_emb])
 
     # print(f">>>> {is_conv = }, {num_heads = }")
-    if is_conv:
-        attn = keras.layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=name + "attn_bn")(pos_out)
-        attn = conv2d_no_bias(attn, out_channel, 1, use_bias=True, name=name + "attn_1_")
-        attn = depthwise_conv2d_no_bias(attn, kernel_size=5, padding="SAME", use_bias=True, name=name + "attn_")
-        attn = conv2d_no_bias(attn, out_channel, 1, use_bias=True, name=name + "attn_2_")
-    else:
-        attn = keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, name=name + "attn_ln")(pos_out)
-        # attn = multi_head_self_attention(
-        #     attn, num_heads, qkv_bias=qkv_bias, out_bias=True, attn_dropout=attn_drop_rate, output_dropout=mlp_drop_rate, name=name + "attn_mhsa_"
-        # )
-        attn = multi_head_self_attention(attn, num_heads, qkv_bias=qkv_bias, out_bias=True, attn_dropout=attn_drop_rate, name=name + "attn_mhsa_")
-    attn = ChannelAffine(use_bias=False, weight_init_value=gamma, name=name + "1_gamma")(attn) if gamma >= 0 else attn
-    attn = drop_block(attn, drop_rate=drop_rate, name=name + "attn_")
-    attn_out = keras.layers.Add(name=name + "attn_out")([pos_out, attn])
-
-    if is_conv:
-        mlp = keras.layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=name + "mlp_bn")(attn_out)
-    else:
-        mlp = keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, name=name + "mlp_ln")(attn_out)
-    mlp = mlp_block(mlp, int(out_channel * mlp_ratio), drop_rate=mlp_drop_rate, use_conv=is_conv, activation=activation, name=name + "mlp_")
-    mlp = ChannelAffine(use_bias=False, weight_init_value=gamma, name=name + "2_gamma")(mlp) if gamma >= 0 else mlp
-    mlp = drop_block(mlp, drop_rate=drop_rate, name=name + "mlp_")
-    return keras.layers.Add(name=name + "output")([attn_out, mlp])
+    pre_attn = pos_out if backend.image_data_format() == "channels_last" else layers.Permute((2, 3, 1), name=name + "permute_pre")(pos_out)
+    height, width = pre_attn.shape[1:-1]
+    pre_attn = functional.reshape(pre_attn, [-1, height * width, pre_attn.shape[-1]])  # Using 3D for attention inputs
+
+    attn = layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "attn_ln")(pre_attn)
+    attn = multi_head_self_attention(attn, num_heads, qkv_bias=qkv_bias, out_bias=True, attn_dropout=attn_drop_rate, name=name + "attn_mhsa_")
+    attn_out = add_with_layer_scale_and_drop_block(pre_attn, attn, layer_scale=gamma, drop_rate=drop_rate, axis=-1, name=name + "1_")
+
+    mlp = layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, axis=-1, name=name + "mlp_ln")(attn_out)
+    mlp = mlp_block(mlp, int(out_channel * mlp_ratio), drop_rate=mlp_drop_rate, use_conv=False, activation=activation, name=name + "mlp_")
+    out = add_with_layer_scale_and_drop_block(attn_out, mlp, layer_scale=gamma, drop_rate=drop_rate, axis=-1, name=name + "2_")
+    out = functional.reshape(out, [-1, height, width, out.shape[-1]])  # Revert 3D to 4D
+    return out if backend.image_data_format() == "channels_last" else layers.Permute((3, 1, 2), name=name + "permute_post_output")(out)
+
+
+def conv_block(inputs, out_channel, mlp_ratio=4, mlp_drop_rate=0, drop_rate=0, gamma=-1, activation="gelu", name=""):
+    pos_emb = depthwise_conv2d_no_bias(inputs, kernel_size=3, padding="same", use_bias=True, name=name + "pos_emb_")
+    pos_out = layers.Add()([inputs, pos_emb])
+
+    attn = layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=name + "attn_bn")(pos_out)
+    attn = conv2d_no_bias(attn, out_channel, 1, use_bias=True, name=name + "attn_1_")
+    attn = depthwise_conv2d_no_bias(attn, kernel_size=5, padding="same", use_bias=True, name=name + "attn_")
+    attn = conv2d_no_bias(attn, out_channel, 1, use_bias=True, name=name + "attn_2_")
+    attn_out = add_with_layer_scale_and_drop_block(pos_out, attn, layer_scale=gamma, drop_rate=drop_rate, name=name + "1_")
+
+    mlp = layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=name + "mlp_bn")(attn_out)
+    mlp = mlp_block(mlp, int(out_channel * mlp_ratio), drop_rate=mlp_drop_rate, use_conv=True, activation=activation, name=name + "mlp_")
+    return add_with_layer_scale_and_drop_block(attn_out, mlp, layer_scale=gamma, drop_rate=drop_rate, name=name + "2_")
 
 
 def stem(inputs, stem_width, use_conv_stem=False, drop_rate=0, activation="gelu", name="stem_"):
     if use_conv_stem:
         nn = conv2d_no_bias(inputs, stem_width // 2, kernel_size=3, strides=2, padding="same", use_bias=True, name=name + "1_")
-        nn = keras.layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=name + "1_bn")(nn)
+        nn = layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=name + "1_bn")(nn)
         nn = activation_by_name(nn, activation, name=name)
         nn = conv2d_no_bias(nn, stem_width, kernel_size=3, strides=2, padding="same", use_bias=True, name=name + "2_")
-        nn = keras.layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=name + "2_bn")(nn)
+        nn = layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=name + "2_bn")(nn)
     else:
         nn = conv2d_no_bias(inputs, stem_width, 4, strides=4, padding="valid", use_bias=True, name=name)
-        nn = keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, name=name + "ln")(nn)
-    nn = keras.layers.Dropout(drop_rate) if drop_rate > 0 else nn
+        nn = layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, name=name + "ln")(nn)
+    nn = layers.Dropout(drop_rate) if drop_rate > 0 else nn
     return nn
 
 
 def Uniformer(
     num_blocks=[3, 4, 8, 3],
     out_channels=[64, 128, 320, 512],
     head_dimension=64,
@@ -129,15 +131,18 @@
     drop_connect_rate=0,
     dropout=0,
     classifier_activation="softmax",
     pretrained=None,
     model_name="uniformer",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+    inputs = layers.Input(input_shape)
 
     """ stem """
     stem_width = stem_width if stem_width > 0 else out_channels[0]
     nn = stem(inputs, stem_width, use_conv_stem, drop_rate=mlp_drop_rate, activation=activation, name="stem_")  # It's using mlp_drop_rate for stem
 
     if mix_token and token_label_top:
         scale = 8  # downsample 3 times
@@ -146,102 +151,116 @@
         nn = mixup_token.do_mixup_token(nn, bbox * scale)
 
     """ stage [1, 2, 3, 4] """
     total_blocks = sum(num_blocks)
     global_block_id = 0
     for stack_id, (num_block, out_channel, block_type) in enumerate(zip(num_blocks, out_channels, block_types)):
         stack_name = "stack{}_".format(stack_id + 1)
-        is_conv_block = True if block_type[0].lower() == "c" else False
-        num_heads = 0 if is_conv_block else out_channel // head_dimension
         if stack_id > 0:
             if use_conv_stem:
                 nn = conv2d_no_bias(nn, out_channel, kernel_size=3, strides=2, padding="same", use_bias=True, name=stack_name + "downsample_")
-                nn = keras.layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=stack_name + "downsample_bn")(nn)
+                nn = layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name=stack_name + "downsample_bn")(nn)
             else:
                 nn = conv2d_no_bias(nn, out_channel, kernel_size=2, strides=2, use_bias=True, name=stack_name + "downsample_")
-                nn = keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, name=stack_name + "downsample_ln")(nn)
+                nn = layers.LayerNormalization(epsilon=LAYER_NORM_EPSILON, name=stack_name + "downsample_ln")(nn)
 
+        is_conv_block = True if block_type[0].lower() == "c" else False
+        num_heads = 0 if is_conv_block else out_channel // head_dimension
         for block_id in range(num_block):
             block_name = stack_name + "block{}_".format(block_id + 1)
             block_drop_rate = drop_connect_rate * global_block_id / total_blocks
-            nn = block(nn, out_channel, num_heads, qkv_bias, mlp_ratio, mlp_drop_rate, attn_drop_rate, block_drop_rate, layer_scale, activation, block_name)
+            if is_conv_block:
+                nn = conv_block(nn, out_channel, mlp_ratio, mlp_drop_rate, block_drop_rate, layer_scale, activation, block_name)
+            else:
+                nn = attn_block(
+                    nn, out_channel, num_heads, qkv_bias, mlp_ratio, mlp_drop_rate, attn_drop_rate, block_drop_rate, layer_scale, activation, block_name
+                )
             global_block_id += 1
-    nn = keras.layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name="post_bn")(nn)
+    nn = layers.BatchNormalization(momentum=BATCH_NORM_DECAY, epsilon=BATCH_NORM_EPSILON, name="post_bn")(nn)
 
     """ output """
     if token_label_top and num_classes > 0:
         # Training with label token
         nn_cls = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=None)  # Don't use softmax here
-        nn_aux = keras.layers.Dense(num_classes, name="aux_head")(nn)
+        nn_aux = layers.Dense(num_classes, name="aux_head")(nn)
 
         if mix_token:
             nn_aux = mixup_token.do_mixup_token(nn_aux, bbox)
-            nn_aux = keras.layers.Reshape((-1, nn_aux.shape[-1]), dtype="float32", name="aux")(nn_aux)
+            nn_aux = layers.Reshape((-1, nn_aux.shape[-1]), dtype="float32", name="aux")(nn_aux)
 
             left, top, right, bottom = bbox
             lam = 1 - ((right - left) * (bottom - top) / (nn_aux.shape[1] * nn_aux.shape[2]))
-            lam_repeat = tf.expand_dims(tf.repeat(lam, tf.shape(nn_cls)[0], axis=0), 1)
-            nn_cls = keras.layers.Concatenate(axis=-1, dtype="float32", name="class")([nn_cls, lam_repeat])
+            lam_repeat = functional.expand_dims(functional.repeat(lam, functional.shape(nn_cls)[0], axis=0), 1)
+            nn_cls = layers.Concatenate(axis=-1, dtype="float32", name="class")([nn_cls, lam_repeat])
         else:
-            nn_aux = keras.layers.Reshape((-1, nn_aux.shape[-1]), dtype="float32", name="aux")(nn_aux)
+            nn_aux = layers.Reshape((-1, nn_aux.shape[-1]), dtype="float32", name="aux")(nn_aux)
         out = [nn_cls, nn_aux]
     else:
         out = output_block(nn, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)
 
-    model = keras.models.Model(inputs, out, name=model_name)
+    model = models.Model(inputs, out, name=model_name)
     post_process = token_label_imagenet_decode_predictions if token_label_top else None
     add_pre_post_process(model, rescale_mode="torch", post_process=post_process)
     pretrained = "token_label" if pretrained is not None and "token" in pretrained.lower() else pretrained
     reload_model_weights(model, PRETRAINED_DICT, "uniformer", pretrained)
     return model
 
 
 def token_label_imagenet_decode_predictions(preds, top=5, classifier_activation="softmax", do_decode=True):
-    preds = preds[0] + 0.5 * tf.reduce_max(preds[1], axis=1)
-    preds = getattr(keras.activations, classifier_activation)(preds) if classifier_activation is not None else preds
-    return tf.keras.applications.imagenet_utils.decode_predictions(preds.numpy(), top=top) if do_decode else preds
+    from keras_cv_attention_models.imagenet.eval_func import decode_predictions
+
+    preds = preds[0] + 0.5 * functional.reduce_max(preds[1], axis=1)
+    preds = activation_by_name(preds, classifier_activation) if classifier_activation is not None else preds
+    return decode_predictions(preds.numpy(), top=top) if do_decode else preds
 
 
+@register_model
 def UniformerSmall32(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", token_label_top=False, pretrained="token_label", **kwargs):
     num_blocks = [3, 4, 8, 3]
     head_dimension = 32
     return Uniformer(**locals(), model_name="uniformer_small_32", **kwargs)
 
 
+@register_model
 def UniformerSmall64(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", token_label_top=False, pretrained="imagenet", **kwargs):
     num_blocks = [3, 4, 8, 3]
     head_dimension = 64
     return Uniformer(**locals(), model_name="uniformer_small_64", **kwargs)
 
 
+@register_model
 def UniformerSmallPlus32(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", token_label_top=False, pretrained="imagenet", **kwargs):
     num_blocks = [3, 5, 9, 3]
     head_dimension = 32
     use_conv_stem = True
     return Uniformer(**locals(), model_name="uniformer_small_plus_32", **kwargs)
 
 
+@register_model
 def UniformerSmallPlus64(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", token_label_top=False, pretrained="imagenet", **kwargs):
     num_blocks = [3, 5, 9, 3]
     head_dimension = 64
     use_conv_stem = True
     return Uniformer(**locals(), model_name="uniformer_small_plus_64", **kwargs)
 
 
+@register_model
 def UniformerBase32(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", token_label_top=False, pretrained="token_label", **kwargs):
     num_blocks = [5, 8, 20, 7]
     head_dimension = 32
     return Uniformer(**locals(), model_name="uniformer_base_32", **kwargs)
 
 
+@register_model
 def UniformerBase64(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", token_label_top=False, pretrained="imagenet", **kwargs):
     num_blocks = [5, 8, 20, 7]
     head_dimension = 64
     return Uniformer(**locals(), model_name="uniformer_base_64", **kwargs)
 
 
+@register_model
 def UniformerLarge64(input_shape=(224, 224, 3), num_classes=1000, classifier_activation="softmax", token_label_top=False, pretrained="token_label", **kwargs):
     num_blocks = [5, 10, 24, 7]
     out_channels = [128, 192, 448, 640]
     head_dimension = 64
     layer_scale = 1e-6
     return Uniformer(**locals(), model_name="uniformer_large_64", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/visualizing/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from keras_cv_attention_models.visualizing.visualize_filters import visualize_filters
 from keras_cv_attention_models.visualizing.gradcam_heatmap import make_gradcam_heatmap, make_and_apply_gradcam_heatmap
 from keras_cv_attention_models.visualizing.attention_score_maps import plot_attention_score_maps
-from keras_cv_attention_models.visualizing.plot_func import get_plot_cols_rows, stack_and_plot_images, tensorboard_parallel_coordinates_plot
+from keras_cv_attention_models.plot_func import tensorboard_parallel_coordinates_plot
 
 
 visualize_filters.__doc__ = """
 Copied and modified from: https://keras.io/examples/vision/visualizing_what_convnets_learn/
 Displaying the visual patterns that convnet filters respond to.
 Will create input images that maximize the activation of specific filters in a target layer.
 Such images represent a visualization of the pattern that the filter responds to.
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/attention_score_maps.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/visualizing/attention_score_maps.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import numpy as np
 import tensorflow as tf
-from keras_cv_attention_models.visualizing.plot_func import get_plot_cols_rows, stack_and_plot_images
+from keras_cv_attention_models.plot_func import get_plot_cols_rows, stack_and_plot_images
 
 
 def matmul_prod(aa):
     vv = aa[0]
     for ii in aa[1:]:
         vv = np.matmul(vv, ii)
     return vv
@@ -37,107 +37,116 @@
 
 
 def down_sample_matrix_axis_0(dd, target, method="avg"):
     if dd.shape[0] == target:
         return dd
     rate = int(np.sqrt(dd.shape[0] // target))
     hh = ww = int(np.sqrt(dd.shape[0]))
-    dd = dd.reshape(1, hh, ww, -1)
+    dd = dd[: hh * ww].reshape(1, hh, ww, -1)
     if rate == 0:  # Upsample
         hh = ww = int(np.sqrt(target))
         dd = tf.image.resize(dd, [hh, ww]).numpy()
     elif "avg" in method.lower():
-        dd = tf.nn.avg_pool(dd, rate, rate, "VALID").numpy()
+        dd = tf.nn.avg_pool(dd, rate, rate, "valid").numpy()
     elif "swin" in method.lower():
         dd = dd.reshape(1, hh // 2, 2, ww // 2, 2, -1).transpose(0, 1, 3, 4, 2, 5)
     else:
-        dd = tf.nn.max_pool(dd, rate, rate, "VALID").numpy()
+        dd = tf.nn.max_pool(dd, rate, rate, "valid").numpy()
     dd = dd.reshape(-1, dd.shape[-1])
     return dd
 
 
 def plot_attention_score_maps(model, image, rescale_mode="auto", attn_type="auto", rows=-1, base_size=3):
     import matplotlib.pyplot as plt
+    from keras_cv_attention_models.common_layers import PreprocessInput
 
     if rescale_mode.lower() == "auto":
         rescale_mode = getattr(model, "rescale_mode", "torch")
         print(">>>> rescale_mode:", rescale_mode)
 
     if isinstance(model, tf.keras.models.Model):
-        imm_inputs = tf.keras.applications.imagenet_utils.preprocess_input(image, mode=rescale_mode)
-        imm_inputs = tf.expand_dims(tf.image.resize(imm_inputs, model.input_shape[1:3]), 0)
+        imm_inputs = PreprocessInput(input_shape=model.input_shape[1:3], rescale_mode=rescale_mode)(image)
         try:
             pred = model(imm_inputs).numpy()
             if model.layers[-1].activation.__name__ != "softmax":
                 pred = tf.nn.softmax(pred).numpy()  # If classifier activation is not softmax
             print(">>>> Prediction:", tf.keras.applications.imagenet_utils.decode_predictions(pred)[0])
         except:
             pass
-        bb = tf.keras.models.Model(model.inputs[0], [ii.output for ii in model.layers if ii.name.endswith("attention_scores")])
+        outputs = [ii.output for ii in model.layers if ii.name.endswith("attention_scores") and "channel_attn_attention_scores" not in ii.name]
+        bb = tf.keras.models.Model(model.inputs[0], outputs)
         attn_scores = bb(imm_inputs)
         layer_name_title = "\nLayer name: {} --> {}".format(bb.output_names[-1], bb.output_names[0])
     else:
         attn_scores = model
         layer_name_title = ""
         assert attn_type != "auto"
+    print(">>>> attn_scores:", {name: out.shape.as_list() for name, out in zip(bb.output_names, attn_scores)})
 
     attn_type = attn_type.lower()
     check_type_is = lambda tt: (tt in model.name.lower()) if attn_type == "auto" else (attn_type.startswith(tt))
     if check_type_is("beit"):
         # beit attn_score [batch, num_heads, cls_token + hh * ww, cls_token + hh * ww]
         print(">>>> Attention type: beit")
         mask = [np.array(ii)[0].mean(0) + np.eye(ii.shape[-1]) for ii in attn_scores][::-1]
         mask = [(ii / ii.sum()) for ii in mask]
         cum_mask = [matmul_prod(mask[: ii + 1])[0] for ii in range(len(mask))]
         mask = [ii[0] for ii in mask]
-    elif check_type_is("levit"):
-        # levit attn_score [batch, num_heads, q_blocks, k_blocks]
-        print(">>>> Attention type: levit")
-        mask = [np.array(ii)[0].mean(0) for ii in attn_scores][::-1]
-        cum_mask = [matmul_prod(mask[: ii + 1]).mean(0) for ii in range(len(mask))]
-        mask = [ii.mean(0) for ii in mask]
-    elif check_type_is("bot"):
+    elif check_type_is("mobilevit_v2"):
+        # mobilevit_v2 attn_score [batch, patch_size * patch_size, patch_hh * patch_ww, 1]
+        print(">>>> Attention type: mobilevit_v2")
+        mask = [np.array(ii)[0].mean((0)) for ii in attn_scores if len(ii.shape) == 4][::-1]
+        cum_mask = [down_sample_matrix_axis_0(mask[ii], mask[-1].shape[0]) for ii in range(len(mask) - 1)] + [mask[-1]]
+        cum_mask = [np.prod(np.stack(cum_mask[: ii + 1]), axis=0).mean(-1) for ii in range(len(cum_mask))]
+        mask = [ii.mean(-1) for ii in mask]
+    elif check_type_is("bot") or check_type_is("cmt") or check_type_is("mobilevit"):
         # bot attn_score [batch, num_heads, hh * ww, hh * ww]
-        print(">>>> Attention type: bot")
+        print(">>>> Attention type: bot / cmt / mobilevit")
         mask = [np.array(ii)[0].mean((0)) for ii in attn_scores if len(ii.shape) == 4][::-1]
         mask = [clip_max_value_matrix(ii) for ii in mask]  # Or it will be too dark.
         cum_mask = [mask[0]] + [down_sample_matrix_axis_0(mask[ii], mask[ii - 1].shape[1], "avg") for ii in range(1, len(mask))]
         cum_mask = [matmul_prod(cum_mask[: ii + 1]).mean(0) for ii in range(len(cum_mask))]
         mask = [ii.mean(0) for ii in mask]
-    elif check_type_is("coatnet") or check_type_is("cmt") or check_type_is("uniformer") or check_type_is("swin"):
-        # bot attn_score [batch, num_heads, hh * ww, hh * ww]
-        print(">>>> Attention type: coatnet / cmt / uniformer / swin")
+    elif check_type_is("coatnet") or check_type_is("uniformer") or check_type_is("swin") or check_type_is("davit"):
+        # coatnet attn_score [batch, num_heads, hh * ww, hh * ww]
+        print(">>>> Attention type: coatnet / uniformer / swin / davit")
         mask = [np.array(ii)[0].mean((0)) for ii in attn_scores if len(ii.shape) == 4][::-1]
         downsample_method = "swin" if check_type_is("swin") else "max"
         cum_mask = [mask[0]] + [down_sample_matrix_axis_0(mask[ii], mask[ii - 1].shape[1], downsample_method) for ii in range(1, len(mask))]
         cum_mask = [matmul_prod(cum_mask[: ii + 1]).mean(0) for ii in range(len(cum_mask))]
         mask = [ii.mean(0) for ii in mask]
     elif check_type_is("coat"):
         # coat attn_score [batch, num_heads, cls_token + hh * ww, key_dim]
         print(">>>> Attention type: coat")
         mask = [np.array(ii)[0].mean((0))[1:] for ii in attn_scores if len(ii.shape) == 4][::-1]
         mask = [ii.max(-1, keepdims=True) for ii in mask]
         target_shape = np.min([ii.shape[0] for ii in mask])
         cum_mask = [down_sample_matrix_axis_0(mask[ii], target_shape, "max") for ii in range(len(mask))]
         cum_mask = [ii[:, 0] for ii in cum_mask]
         mask = [ii[:, 0] for ii in mask]
+    elif check_type_is("levit"):
+        # levit attn_score [batch, num_heads, q_blocks, k_blocks]
+        print(">>>> Attention type: levit")
+        mask = [np.array(ii)[0].mean(0) for ii in attn_scores][::-1]
+        cum_mask = [matmul_prod(mask[: ii + 1]).mean(0) for ii in range(len(mask))]
+        mask = [ii.mean(0) for ii in mask]
     elif check_type_is("halo"):
         # halo attn_score [batch, num_heads, hh, ww, query_block * query_block, kv_kernel * kv_kernel]
         print(">>>> Attention type: halo")
         from einops import rearrange
         from keras_cv_attention_models.attention_layers import CompatibleExtractPatches
 
         mask = [np.array(ii)[0].mean(0) for ii in attn_scores if len(ii.shape) == 6][::-1]
 
         qqs = [int(np.sqrt(ii.shape[2])) for ii in mask]  # query_kernel
         vvs = [int(np.sqrt(ii.shape[3])) for ii in mask]  # kv_kernel
         hhs = [(jj - ii) // 2 for ii, jj in zip(qqs, vvs)]  # halo_size
         tt = [rearrange(ii, "hh ww (hb wb) cc -> (hh hb) (ww wb) cc", hb=qq, wb=qq) for ii, qq in zip(mask, qqs)]
         tt = [tf.expand_dims(tf.pad(ii, [[hh, hh], [hh, hh], [0, 0]]), 0) for ii, hh in zip(tt, hhs)]
-        tt = [CompatibleExtractPatches(vv, qq, padding="VALID", compressed=False)(ii).numpy()[0] for ii, vv, qq in zip(tt, vvs, qqs)]
+        tt = [CompatibleExtractPatches(vv, qq, padding="valid", compressed=False)(ii).numpy()[0] for ii, vv, qq in zip(tt, vvs, qqs)]
         tt = [rearrange(ii, "hh ww hb wb cc -> hh ww (hb wb) cc").mean((0, 1)) for ii in tt]
         # tt = [tf.reduce_max(rearrange(ii, "hh ww hb wb cc -> hh ww (hb wb) cc"), axis=(0, 1)).numpy() for ii in tt]
         cum_mask = [matmul_prod(tt[: ii + 1]).mean(0) for ii in range(len(tt))]
         mask = [ii.mean((0, 1, 2)) for ii in mask]
     else:
         print(">>>> Attention type: cot / volo / unknown")
         # cot attn_score [batch, 1, 1, filters, randix]
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/gradcam_heatmap.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/visualizing/gradcam_heatmap.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/visualizing/visualize_filters.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/visualizing/visualize_filters.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import numpy as np
 import tensorflow as tf
 from tqdm import tqdm
-from keras_cv_attention_models.visualizing.plot_func import get_plot_cols_rows, stack_and_plot_images
+from keras_cv_attention_models.plot_func import get_plot_cols_rows, stack_and_plot_images
 
 """ visualize_filters """
 
 
 def __gradient_ascent_step__(feature_extractor, image_var, filter_index, optimizer):
     with tf.GradientTape() as tape:
         tape.watch(image_var)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/volo/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/volo/__init__.py`

 * *Files identical despite different names*

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/volo/volo.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/volo/volo.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,12 @@
-import tensorflow as tf
-from tensorflow import keras
-from tensorflow.keras import backend as K
+import math
+import numpy as np
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, models, functional, image_data_format, initializers, register_keras_serializable
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.download_and_load import reload_model_weights
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     batchnorm_with_activation,
     conv2d_no_bias,
     fold_by_conv2d_transpose,
     CompatibleExtractPatches,
@@ -20,105 +22,114 @@
     "volo_d4": {"imagenet": {224: "b45c6518b5e7624b0f6a61f18a5a7bae", 448: "c3e48df2a555032608d48841d2f4a551"}},
     "volo_d5": {"imagenet": {224: "19c98591fb2a97c2a51d9723c2ff6e1d", 448: "6f9858b667cfef77339901c3121c85a1", 512: "f2aa0cb8e265cabee840a6b83858d086"}},
 }
 
 
 def outlook_attention(inputs, embed_dim, num_heads=8, kernel_size=3, padding=1, strides=2, attn_dropout=0, output_dropout=0, name=""):
     _, height, width, channel = inputs.shape
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(embed_dim // num_heads, "float32")))
-    hh, ww = int(tf.math.ceil(height / strides)), int(tf.math.ceil(width / strides))
+    # qk_scale = float(1.0 / functional.sqrt(functional.cast(embed_dim // num_heads, "float32")))
+    qk_scale = 1.0 / (float(embed_dim // num_heads) ** 0.5)
+    hh, ww = int(math.ceil(height / strides)), int(math.ceil(width / strides))
 
-    vv = keras.layers.Dense(embed_dim, use_bias=False, name=name + "v")(inputs)
+    vv = layers.Dense(embed_dim, use_bias=False, name=name + "v")(inputs)
 
     """ attention """
     # [1, 14, 14, 192]
-    pool_padding = "VALID" if height % strides == 0 and width % strides == 0 else "SAME"
-    attn = keras.layers.AveragePooling2D(pool_size=strides, strides=strides, padding=pool_padding)(inputs)
+    pool_padding = "valid" if height % strides == 0 and width % strides == 0 else "same"
+    attn = inputs if image_data_format() == "channels_last" else layers.Permute([3, 1, 2])(inputs)  # channels_last -> channels_first
+    attn = layers.AvgPool2D(pool_size=strides, strides=strides, padding=pool_padding)(attn)
+    attn = attn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(attn)  # channels_first -> channels_last
     # [1, 14, 14, 486]
-    attn = keras.layers.Dense(kernel_size**4 * num_heads, name=name + "attn")(attn) / qk_scale
+    attn = layers.Dense(kernel_size**4 * num_heads, name=name + "attn")(attn) * qk_scale
     # [1, 14, 14, 6, 9, 9]
-    attn = tf.reshape(attn, (-1, hh, ww, num_heads, kernel_size * kernel_size, kernel_size * kernel_size))
-    # attention_weights = tf.nn.softmax(attn, axis=-1)
-    attention_weights = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
+    attn = functional.reshape(attn, (-1, hh, ww, num_heads, kernel_size * kernel_size, kernel_size * kernel_size))
+    # attention_weights = functional.softmax(attn, axis=-1)
+    attention_weights = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
     if attn_dropout > 0:
-        attention_weights = keras.layers.Dropout(attn_dropout)(attention_weights)
+        attention_weights = layers.Dropout(attn_dropout)(attention_weights)
 
     """ unfold """
     # [1, 14, 14, 1728] if compressed else [1, 14, 14, 3, 3, 192]
-    # patches = tf.image.extract_patches(pad_vv, patch_kernel, patch_strides, [1, 1, 1, 1], padding="VALID")
-    patches = CompatibleExtractPatches(kernel_size, strides, padding="SAME", compressed=False, name=name)(vv)
+    if backend.is_torch_backend:
+        patches = functional.extract_patches(vv, sizes=kernel_size, strides=strides, padding="same")
+    else:
+        # patches = functional.extract_patches(pad_vv, patch_kernel, patch_strides, [1, 1, 1, 1], padding="valid")
+        patches = CompatibleExtractPatches(kernel_size, strides, padding="same", compressed=False, name=name)(vv)
 
     """ matmul """
     # mm = einops.rearrange(patches, 'D H W (k h p) -> D H W h k p', h=num_head, k=kernel_size * kernel_size)
-    # mm = tf.matmul(attn, mm)
+    # mm = functional.matmul(attn, mm)
     # mm = einops.rearrange(mm, 'D H W h (kh kw) p -> D H W kh kw (h p)', h=num_head, kh=kernel_size, kw=kernel_size)
     # [1, 14, 14, 9, 6, 32], the last 2 dimenions are channel 6 * 32 == 192
-    mm = tf.reshape(patches, [-1, hh, ww, kernel_size * kernel_size, num_heads, embed_dim // num_heads])
+    mm = functional.reshape(patches, [-1, hh, ww, kernel_size * kernel_size, num_heads, embed_dim // num_heads])
     # [1, 14, 14, 6, 9, 32], meet the dimenion of attn for matmul
-    mm = tf.transpose(mm, [0, 1, 2, 4, 3, 5])
+    mm = functional.transpose(mm, [0, 1, 2, 4, 3, 5])
     # [1, 14, 14, 6, 9, 32], The last two dimensions [9, 9] @ [9, 32] --> [9, 32]
-    mm = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_weights, mm])
+    # mm = layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_weights, mm])
+    mm = attention_weights @ mm
     # [1, 14, 14, 9, 6, 32], transpose back
-    mm = tf.transpose(mm, [0, 1, 2, 4, 3, 5])
+    mm = functional.transpose(mm, [0, 1, 2, 4, 3, 5])
     # [1, 14, 14, 3, 3, 192], split kernel_dimension: 9 --> [3, 3], merge channel_dimmension: [6, 32] --> 192
-    mm = tf.reshape(mm, [-1, hh, ww, kernel_size, kernel_size, embed_dim])
+    mm = functional.reshape(mm, [-1, hh, ww, kernel_size, kernel_size, embed_dim])
 
     """ fold """
     # [1, 28, 28, 192]
-    output = fold_by_conv2d_transpose(mm, inputs.shape[1:], kernel_size, strides, padding="SAME", compressed=False, name=name)
+    output = fold_by_conv2d_transpose(mm, inputs.shape[1:], kernel_size, strides, padding="same", compressed=False, name=name)
 
     # output = UnfoldMatmulFold((height, width, embed_dim), kernel_size, padding, strides)([vv, attention_weights])
-    output = keras.layers.Dense(embed_dim, use_bias=True, name=name + "out")(output)
+    output = layers.Dense(embed_dim, use_bias=True, name=name + "out")(output)
 
     if output_dropout > 0:
-        output = keras.layers.Dropout(output_dropout)(output)
+        output = layers.Dropout(output_dropout)(output)
 
     return output
 
 
 def outlook_attention_simple(inputs, embed_dim, num_heads=6, kernel_size=3, attn_dropout=0, name=""):
     """Simple version not using unfold and fold"""
     key_dim = embed_dim // num_heads
-    qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
+    # qk_scale = float(1.0 / tf.math.sqrt(tf.cast(key_dim, "float32")))
+    qk_scale = 1.0 / (float(embed_dim // num_heads) ** 0.5)
 
     height, width = inputs.shape[1], inputs.shape[2]
-    hh, ww = int(tf.math.ceil(height / kernel_size)), int(tf.math.ceil(width / kernel_size))  # 14, 14
+    hh, ww = int(math.ceil(height / kernel_size)), int(math.ceil(width / kernel_size))  # 14, 14
     padded = hh * kernel_size - height
     if padded != 0:
-        inputs = keras.layers.ZeroPadding2D(((0, padded), (0, padded)))(inputs)
+        inputs = layers.ZeroPadding2D(((0, padded), (0, padded)))(inputs)
 
-    vv = keras.layers.Dense(embed_dim, use_bias=False, name=name + "v")(inputs)
+    vv = layers.Dense(embed_dim, use_bias=False, name=name + "v")(inputs)
     # vv = einops.rearrange(vv, "D (h hk) (w wk) (H p) -> D h w H (hk wk) p", hk=kernel_size, wk=kernel_size, H=num_heads, p=key_dim)
-    vv = tf.reshape(vv, (-1, hh, kernel_size, ww, kernel_size, num_heads, key_dim))  # [1, 14, 2, 14, 2, 6, 32]
-    vv = tf.transpose(vv, [0, 1, 3, 5, 2, 4, 6])
-    vv = tf.reshape(vv, [-1, hh, ww, num_heads, kernel_size * kernel_size, key_dim])  # [1, 14, 14, 6, 4, 32]
-
-    # attn = keras.layers.AveragePooling2D(pool_size=3, strides=2, padding='SAME')(inputs)
-    attn = keras.layers.AveragePooling2D(pool_size=kernel_size, strides=kernel_size)(inputs)
-    attn = keras.layers.Dense(kernel_size**4 * num_heads, use_bias=True, name=name + "attn")(attn) / qk_scale
-    attn = tf.reshape(attn, [-1, hh, ww, num_heads, kernel_size * kernel_size, kernel_size * kernel_size])  # [1, 14, 14, 6, 4, 4]
-    # attn = tf.nn.softmax(attn, axis=-1)
-    attn = keras.layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
+    vv = functional.reshape(vv, (-1, hh, kernel_size, ww, kernel_size, num_heads, key_dim))  # [1, 14, 2, 14, 2, 6, 32]
+    vv = functional.transpose(vv, [0, 1, 3, 5, 2, 4, 6])
+    vv = functional.reshape(vv, [-1, hh, ww, num_heads, kernel_size * kernel_size, key_dim])  # [1, 14, 14, 6, 4, 32]
+
+    # attn = layers.AvgPool2D(pool_size=3, strides=2, padding='same')(inputs)
+    attn = layers.AvgPool2D(pool_size=kernel_size, strides=kernel_size)(inputs)
+    attn = layers.Dense(kernel_size**4 * num_heads, use_bias=True, name=name + "attn")(attn) * qk_scale
+    attn = functional.reshape(attn, [-1, hh, ww, num_heads, kernel_size * kernel_size, kernel_size * kernel_size])  # [1, 14, 14, 6, 4, 4]
+    # attn = functional.softmax(attn, axis=-1)
+    attn = layers.Softmax(axis=-1, name=name and name + "attention_scores")(attn)
     if attn_dropout > 0:
-        attn = keras.layers.Dropout(attn_dropout)(attn)
+        attn = layers.Dropout(attn_dropout)(attn)
 
-    out = tf.matmul(attn, vv)  # [1, 14, 14, 6, 4, 32]
+    # out = tf.matmul(attn, vv)  # [1, 14, 14, 6, 4, 32]
+    out = attn @ vv
     # out = einops.rearrange(out, "D h w H (hk wk) p -> D (h hk) (w wk) (H p)", hk=kernel_size, wk=kernel_size)  # [1, 28, 28, 192]
-    out = tf.reshape(out, [-1, hh, ww, num_heads, kernel_size, kernel_size, key_dim])  # [1, 14, 14, 6, 2, 2, 32]
-    out = tf.transpose(out, [0, 1, 4, 2, 5, 3, 6])  # [1, 14, 2, 14, 2, 6, 32]
-    out = tf.reshape(out, [-1, inputs.shape[1], inputs.shape[2], embed_dim])  # [1, 28, 28, 192]
+    out = functional.reshape(out, [-1, hh, ww, num_heads, kernel_size, kernel_size, key_dim])  # [1, 14, 14, 6, 2, 2, 32]
+    out = functional.transpose(out, [0, 1, 4, 2, 5, 3, 6])  # [1, 14, 2, 14, 2, 6, 32]
+    out = functional.reshape(out, [-1, inputs.shape[1], inputs.shape[2], embed_dim])  # [1, 28, 28, 192]
     if padded != 0:
         out = out[:, :-padded, :-padded, :]
-    out = keras.layers.Dense(embed_dim, use_bias=True, name=name + "out")(out)
+    out = layers.Dense(embed_dim, use_bias=True, name=name + "out")(out)
 
     return out
 
 
-@tf.keras.utils.register_keras_serializable(package="volo")
-class BiasLayer(keras.layers.Layer):
+@register_keras_serializable(package="volo")
+class BiasLayer(layers.Layer):
     def __init__(self, axis=-1, initializer="zeros", **kwargs):
         super(BiasLayer, self).__init__(**kwargs)
         self.axis, self.initializer = axis, initializer
 
     def build(self, input_shape):
         if self.axis == -1 or self.axis == len(input_shape) - 1:
             bb_shape = (input_shape[-1],)
@@ -134,187 +145,209 @@
         return inputs + self.bb
 
     def get_config(self):
         config = super(BiasLayer, self).get_config()
         config.update({"axis": self.axis})  # Not saving initializer in config
         return config
 
+    def get_weights_channels_last(self):
+        # channel_first -> channel_last
+        weights = self.get_weights()
+        if backend.image_data_format() != "channels_last" and self.axis == 1:
+            weights = [np.squeeze(ii) for ii in weights]
+        return weights
+
+    def set_weights_channels_last(self, weights):
+        # channel_last -> channel_first
+        if backend.image_data_format() != "channels_last" and self.axis == 1:
+            weights = [np.reshape(ii, self.bb.shape) for ii in weights]
+        return self.set_weights(weights)
+
 
 def attention_mlp_block(inputs, embed_dim, num_heads=1, mlp_ratio=3, attention_type=None, drop_rate=0, mlp_activation="gelu", dropout=0, name=""):
     # print(f">>>> {drop_rate = }")
     nn_0 = inputs[:, :1] if attention_type == "class" else inputs
-    nn_1 = keras.layers.LayerNormalization(epsilon=BATCH_NORM_EPSILON, name=name + "LN")(inputs)
+    nn_1 = layers.LayerNormalization(epsilon=BATCH_NORM_EPSILON, axis=-1, name=name + "LN")(inputs)
+    key_dim = embed_dim // num_heads
 
     if attention_type == "outlook":
         nn_1 = outlook_attention(nn_1, embed_dim, num_heads=num_heads, name=name + "attn_")
     elif attention_type == "outlook_simple":
         nn_1 = outlook_attention_simple(nn_1, embed_dim, num_heads=num_heads, name=name + "attn_")
     elif attention_type == "class":
         # nn_1 = class_attention(nn_1, embed_dim, num_heads=num_heads, name=name + "attn_")
-        nn_1 = keras.layers.MultiHeadAttention(
-            num_heads=num_heads, key_dim=embed_dim // num_heads, output_shape=embed_dim, use_bias=False, name=name + "attn_mhsa"
-        )(nn_1[:, :1, :], nn_1)
+        query = nn_1[:, :1, :]
+        nn_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, output_shape=embed_dim, use_bias=False, name=name + "attn_mhsa")(query, nn_1)
         nn_1 = BiasLayer(name=name + "attn_bias")(nn_1)  # bias for output dense
     elif attention_type == "mhsa":
-        # nn_1 = multi_head_self_attention(nn_1, num_heads=num_heads, key_dim=embed_dim // num_heads, out_shape=embed_dim, out_weight=True, out_bias=True, name=name + "attn_")
-        nn_1 = keras.layers.MultiHeadAttention(
-            num_heads=num_heads, key_dim=embed_dim // num_heads, output_shape=embed_dim, use_bias=False, name=name + "attn_mhsa"
-        )(nn_1, nn_1)
+        # nn_1 = multi_head_self_attention(nn_1, num_heads=num_heads, key_dim=key_dim, out_shape=embed_dim, out_weight=True, out_bias=True, name=name + "attn_")
+        nn_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, output_shape=embed_dim, use_bias=False, name=name + "attn_mhsa")(nn_1, nn_1)
         nn_1 = BiasLayer(name=name + "attn_bias")(nn_1)  # bias for output dense
 
     if drop_rate > 0:
-        nn_1 = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1, 1), name=name + "drop_1")(nn_1)
-    nn_1 = keras.layers.Add()([nn_0, nn_1])
+        nn_1 = layers.Dropout(drop_rate, noise_shape=(None, 1, 1, 1), name=name + "drop_1")(nn_1)
+    nn_1 = layers.Add()([nn_0, nn_1])
 
     """ MLP """
-    nn_2 = keras.layers.LayerNormalization(epsilon=BATCH_NORM_EPSILON, name=name + "mlp_LN")(nn_1)
-    nn_2 = keras.layers.Dense(embed_dim * mlp_ratio, name=name + "mlp_dense_1")(nn_2)
+    nn_2 = layers.LayerNormalization(epsilon=BATCH_NORM_EPSILON, axis=-1, name=name + "mlp_LN")(nn_1)
+    nn_2 = layers.Dense(embed_dim * mlp_ratio, name=name + "mlp_dense_1")(nn_2)
     # gelu with approximate=False using `erf` leads to GPU memory leak...
-    # nn_2 = keras.layers.Activation("gelu", name=name + "mlp_" + mlp_activation)(nn_2)
-    # approximate = True if tf.keras.mixed_precision.global_policy().compute_dtype == "float16" else False
-    # nn_2 = tf.nn.gelu(nn_2, approximate=approximate)
+    # nn_2 = layers.Activation("gelu", name=name + "mlp_" + mlp_activation)(nn_2)
     nn_2 = activation_by_name(nn_2, mlp_activation, name=name + mlp_activation)
-    nn_2 = keras.layers.Dense(embed_dim, name=name + "mlp_dense_2")(nn_2)
+    nn_2 = layers.Dense(embed_dim, name=name + "mlp_dense_2")(nn_2)
     if dropout > 0:
-        nn_2 = keras.layers.Dropout(dropout)(nn_2)
+        nn_2 = layers.Dropout(dropout)(nn_2)
 
     if drop_rate > 0:
-        nn_2 = keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1, 1), name=name + "drop_2")(nn_2)
-    out = keras.layers.Add(name=name + "output")([nn_1, nn_2])
+        nn_2 = layers.Dropout(drop_rate, noise_shape=(None, 1, 1, 1), name=name + "drop_2")(nn_2)
+    out = layers.Add(name=name + "output")([nn_1, nn_2])
 
     if attention_type == "class":
-        out = tf.concat([out, inputs[:, 1:]], axis=1)
+        out = functional.concat([out, inputs[:, 1:]], axis=1)
     return out
 
 
-@tf.keras.utils.register_keras_serializable(package="volo")
-class PositionalEmbedding(keras.layers.Layer):
+@register_keras_serializable(package="volo")
+class PositionalEmbedding(layers.Layer):
     def __init__(self, input_height=-1, **kwargs):
-        super(PositionalEmbedding, self).__init__(**kwargs)
-        self.pp_init = tf.initializers.TruncatedNormal(stddev=0.2)
+        super().__init__(**kwargs)
+        self.pp_init = initializers.TruncatedNormal(stddev=0.2)
         self.input_height = input_height
 
     def build(self, input_shape):
-        self.pp = self.add_weight(name="positional_embedding", shape=(1, *input_shape[1:]), initializer=self.pp_init, trainable=True)
-        super(PositionalEmbedding, self).build(input_shape)
+        self.positional_embedding = self.add_weight(name="positional_embedding", shape=(1, *input_shape[1:]), initializer=self.pp_init, trainable=True)
+        super().build(input_shape)
 
         if len(input_shape) == 3:
             # height and width in input_shape reshaped as one, like [None, 16 * 16 + 1, 32]
             self.is_fused_height_width = True
-            self.height = self.input_height if self.input_height > 0 else int(tf.math.sqrt(float(input_shape[1])))
+            self.height = self.input_height if self.input_height > 0 else int(float(input_shape[1]) ** 0.5)
             self.width = input_shape[1] // self.height
         else:
             self.is_fused_height_width = False
             self.height, self.width = input_shape[1:3]
 
     def call(self, inputs, **kwargs):
-        return inputs + self.pp
+        return inputs + self.positional_embedding
 
     def get_config(self):
         base_config = super().get_config()
         base_config.update({"input_height": self.input_height})
         return base_config
 
-    def load_resized_weights(self, source_layer, method="nearest"):
+    def load_resized_weights(self, source_layer, method="bilinear"):
         # For input 224 --> [1, 14, 14, 384], convert to 384 --> [1, 24, 24, 384]
         if isinstance(source_layer, dict):
-            source_pp = source_layer["positional_embedding:0"]  # weights
+            source_pp = list(source_layer.values())[0]  # weights
         else:
-            source_pp = source_layer.pp  # layer
+            source_pp = source_layer.positional_embedding  # layer
 
-        source_pp = tf.cast(source_pp, self.pp.dtype)
+        source_pp = np.array(source_pp).astype("float32")
         if self.is_fused_height_width:
-            hh = ww = int(tf.math.sqrt(float(source_pp.shape[1])))  # assume source weights are all square shape
-            ss = tf.reshape(source_pp[:, -hh * ww :], [1, hh, ww, -1])  # If has cls_token
-            tt = tf.image.resize(ss, [self.height, self.width], method=method)
-            tt = tf.reshape(tt, [1, self.height * self.width, -1])
-
-            tt = tf.concat([source_pp[:, : -hh * ww], tt], axis=1)  # If has cls_token
+            hh = ww = int(float(source_pp.shape[1]) ** 0.5)  # assume source weights are all square shape
+            ss = source_pp[:, -hh * ww :]  # If has cls_token
+            ss = ss.reshape([1, hh, ww, -1])
+
+            tt = backend.numpy_image_resize(ss, target_shape=[self.height, self.width], method=method)
+            tt = np.reshape(tt, [1, self.height * self.width, -1])
+            tt = np.concatenate([source_pp[:, : -hh * ww], tt], axis=1)  # If has cls_token
         else:
-            tt = tf.image.resize(source_pp, [self.height, self.width], method=method)
-        self.pp.assign(tt)
+            tt = backend.numpy_image_resize(source_pp, target_shape=[self.height, self.width], method=method)
+        # functional.assign(self.positional_embedding, tt)  # For TF it's `parameter.assign(data)`, for Torch `parameter.data = torch.tensor(data)`
+        self.set_weights([tt])
 
     def show_pos_emb(self, rows=16, base_size=1):
         import matplotlib.pyplot as plt
 
-        ss = self.pp[0]
-        cols = int(tf.math.ceil(ss.shape[-1] / rows))
+        ss = self.positional_embedding[0]
+        cols = int(math.ceil(ss.shape[-1] / rows))
         fig, axes = plt.subplots(rows, cols, figsize=(base_size * cols, base_size * rows))
         for id, ax in enumerate(axes.flatten()):
             ax.imshow(ss[:, :, id])
             ax.set_axis_off()
         fig.tight_layout()
         return fig
 
 
-@tf.keras.utils.register_keras_serializable(package="volo")
-class ClassToken(keras.layers.Layer):
-    def __init__(self, **kwargs):
-        super(ClassToken, self).__init__(**kwargs)
-        self.token_init = tf.initializers.TruncatedNormal(stddev=0.2)
+@register_keras_serializable(package="volo")
+class ClassToken(layers.Layer):
+    def __init__(self, num_tokens=1, **kwargs):
+        super().__init__(**kwargs)
+        self.num_tokens = num_tokens
+        self.token_init = initializers.TruncatedNormal(stddev=0.2)
 
     def build(self, input_shape):
-        self.class_tokens = self.add_weight(name="tokens", shape=(1, 1, input_shape[-1]), initializer=self.token_init, trainable=True)
-        super(ClassToken, self).build(input_shape)
+        self.class_tokens = self.add_weight(name="tokens", shape=(1, self.num_tokens, input_shape[-1]), initializer=self.token_init, trainable=True)
+        super().build(input_shape)
 
     def call(self, inputs, **kwargs):
-        class_tokens = tf.tile(self.class_tokens, [tf.shape(inputs)[0], 1, 1])
-        return tf.concat([class_tokens, inputs], axis=1)
+        if backend.is_torch_backend:
+            class_tokens = self.class_tokens.expand(inputs.shape[0], -1, -1)
+        else:
+            class_tokens = functional.repeat(self.class_tokens, functional.shape(inputs)[0], axis=0)
+        return functional.concat([class_tokens, inputs], axis=1)
 
     def compute_output_shape(self, input_shape):
-        return (input_shape[0], input_shape[1] + 1, input_shape[2])
+        return (input_shape[0], None if input_shape[1] is None else (input_shape[1] + self.num_tokens), input_shape[2])
+
+    def get_config(self):
+        base_config = super().get_config()
+        base_config.update({"num_tokens": self.num_tokens})
+        return base_config
 
 
-@tf.keras.utils.register_keras_serializable(package="volo")
-class MixupToken(keras.layers.Layer):
+@register_keras_serializable(package="volo")
+class MixupToken(layers.Layer):
     def __init__(self, scale=2, beta=1.0, **kwargs):
-        super(MixupToken, self).__init__(**kwargs)
+        super().__init__(**kwargs)
         self.scale, self.beta = scale, beta
 
     def call(self, inputs, training=None, **kwargs):
-        height, width = tf.shape(inputs)[1], tf.shape(inputs)[2]
+        height, width = functional.shape(inputs)[1], functional.shape(inputs)[2]
         # tf.print("training:", training)
         def _call_train():
-            return tf.stack(self.rand_bbox(height, width))
+            return functional.stack(self.rand_bbox(height, width))
 
         def _call_test():
-            return tf.cast(tf.stack([0, 0, 0, 0]), "int32")  # No mixup area for test
+            return functional.cast(functional.stack([0, 0, 0, 0]), "int32")  # No mixup area for test
 
-        return K.in_train_phase(_call_train, _call_test, training=training)
+        return backend.in_train_phase(_call_train, _call_test, training=training)
 
     def sample_beta_distribution(self):
+        import tensorflow as tf
+
         gamma_1_sample = tf.random.gamma(shape=[], alpha=self.beta)
         gamma_2_sample = tf.random.gamma(shape=[], alpha=self.beta)
         return gamma_1_sample / (gamma_1_sample + gamma_2_sample)
 
     def rand_bbox(self, height, width):
-        random_lam = tf.cast(self.sample_beta_distribution(), self.compute_dtype)
-        cut_rate = tf.sqrt(1.0 - random_lam)
+        import tensorflow as tf
+
+        random_lam = functional.cast(self.sample_beta_distribution(), self.compute_dtype)
+        cut_rate = functional.sqrt(1.0 - random_lam)
         s_height, s_width = height // self.scale, width // self.scale
 
-        right_pos = tf.random.uniform(shape=[], minval=0, maxval=s_width, dtype=tf.int32)
-        bottom_pos = tf.random.uniform(shape=[], minval=0, maxval=s_height, dtype=tf.int32)
-        left_pos = right_pos - tf.cast(tf.cast(s_width, cut_rate.dtype) * cut_rate, "int32") // 2
-        top_pos = bottom_pos - tf.cast(tf.cast(s_height, cut_rate.dtype) * cut_rate, "int32") // 2
-        left_pos, top_pos = tf.maximum(left_pos, 0), tf.maximum(top_pos, 0)
+        right_pos = tf.random.uniform(shape=[], minval=0, maxval=s_width, dtype="int32")
+        bottom_pos = tf.random.uniform(shape=[], minval=0, maxval=s_height, dtype="int32")
+        left_pos = right_pos - functional.cast(functional.cast(s_width, cut_rate.dtype) * cut_rate, "int32") // 2
+        top_pos = bottom_pos - functional.cast(functional.cast(s_height, cut_rate.dtype) * cut_rate, "int32") // 2
+        left_pos, top_pos = functional.maximum(left_pos, 0), functional.maximum(top_pos, 0)
 
         return left_pos, top_pos, right_pos, bottom_pos
 
     def do_mixup_token(self, inputs, bbox):
         left, top, right, bottom = bbox
-        # if tf.equal(right, 0) or tf.equal(bottom, 0):
-        #     return inputs
         sub_ww = inputs[:, :, left:right]
-        mix_sub = tf.concat([sub_ww[:, :top], sub_ww[::-1, top:bottom], sub_ww[:, bottom:]], axis=1)
-        output = tf.concat([inputs[:, :, :left], mix_sub, inputs[:, :, right:]], axis=2)
+        mix_sub = functional.concat([sub_ww[:, :top], sub_ww[::-1, top:bottom], sub_ww[:, bottom:]], axis=1)
+        output = functional.concat([inputs[:, :, :left], mix_sub, inputs[:, :, right:]], axis=2)
         output.set_shape(inputs.shape)
         return output
 
     def get_config(self):
-        config = super(MixupToken, self).get_config()
+        config = super().get_config()
         config.update({"scale": self.scale, "beta": self.beta})
         return config
 
 
 def patch_stem(inputs, hidden_dim=64, stem_width=384, patch_size=8, strides=2, activation="relu", name=""):
     nn = conv2d_no_bias(inputs, hidden_dim, 7, strides=strides, padding="same", name=name + "1_")
     nn = batchnorm_with_activation(nn, activation=activation, name=name + "1_")
@@ -344,18 +377,19 @@
     mean_classifier_top=False,
     token_label_top=False,
     first_attn_type="outlook",
     pretrained="imagenet",
     model_name="VOLO",
     kwargs=None,
 ):
-    inputs = keras.layers.Input(input_shape)
+    inputs = layers.Input(input_shape)
 
     """ forward_embeddings """
     nn = patch_stem(inputs, hidden_dim=stem_hidden_dim, stem_width=embed_dims[0], patch_size=patch_size, strides=2, name="stem_")
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)  # channels_first -> channels_last
 
     if mix_token:
         scale = 2
         mixup_token = MixupToken(scale=scale)
         bbox = mixup_token(nn)
         nn = mixup_token.do_mixup_token(nn, bbox * scale)
 
@@ -371,115 +405,120 @@
     for ii in range(num_block):
         name = "outlook_block{}_".format(ii)
         block_drop_rate = drop_connect_rate * global_block_id / total_blocks
         nn = attention_mlp_block(nn, embed_dim, num_head, mlp_ratio, first_attn_type, block_drop_rate, mlp_activation, name=name)
         global_block_id += 1
 
     # downsample
-    nn = keras.layers.Conv2D(embed_dim * 2, kernel_size=2, strides=2, name="downsample_conv")(nn)
+    nn = layers.Conv2D(embed_dim * 2, kernel_size=2, strides=2, name="downsample_conv")(nn)
     # PositionalEmbedding
     nn = PositionalEmbedding(name="positional_embedding")(nn)
 
     # MHSA attentions
     num_block, embed_dim, num_head, mlp_ratio = num_blocks[1], embed_dims[1], num_heads[1], mlp_ratios[1]
     for ii in range(num_block):
         name = "MHSA_block{}_".format(ii)
         block_drop_rate = drop_connect_rate * global_block_id / total_blocks
         nn = attention_mlp_block(nn, embed_dim, num_head, mlp_ratio, "mhsa", block_drop_rate, mlp_activation, name=name)
         global_block_id += 1
 
     if num_classes == 0:
-        model = tf.keras.models.Model(inputs, nn, name=model_name)
+        model = models.Model(inputs, nn, name=model_name)
         reload_model_weights(model, PRETRAINED_DICT, "volo", pretrained, PositionalEmbedding)
         return model
 
     _, height, width, channel = nn.shape
-    nn = tf.reshape(nn, (-1, height * width, channel))
+    nn = functional.reshape(nn, (-1, height * width, channel))
 
     """ forward_cls """
     nn = ClassToken(name="class_token")(nn)
 
     embed_dim, num_head, mlp_ratio = embed_dims[-1], num_heads[-1], mlp_ratios[-1]
     for id in range(classfiers):
         name = "classfiers{}_".format(id)
         nn = attention_mlp_block(nn, embed_dim, num_head, mlp_ratio, "class", mlp_activation=mlp_activation, name=name)
-    nn = keras.layers.LayerNormalization(epsilon=BATCH_NORM_EPSILON, name="pre_out_LN")(nn)
+    nn = layers.LayerNormalization(epsilon=BATCH_NORM_EPSILON, axis=-1, name="pre_out_LN")(nn)
 
     if token_label_top:
         # Training with label token
-        nn_cls = keras.layers.Dense(num_classes, dtype="float32", name="token_head")(nn[:, 0])
-        nn_aux = keras.layers.Dense(num_classes, dtype="float32", name="aux_head")(nn[:, 1:])
+        nn_cls = layers.Dense(num_classes, dtype="float32", name="token_head")(nn[:, 0])
+        nn_aux = layers.Dense(num_classes, dtype="float32", name="aux_head")(nn[:, 1:])
 
         if mix_token:
-            nn_aux = tf.reshape(nn_aux, (-1, height, width, num_classes))
+            nn_aux = functional.reshape(nn_aux, (-1, height, width, num_classes))
             nn_aux = mixup_token.do_mixup_token(nn_aux, bbox)
-            nn_aux = keras.layers.Reshape((height * width, num_classes), dtype="float32", name="aux")(nn_aux)
+            nn_aux = layers.Reshape((height * width, num_classes), dtype="float32", name="aux")(nn_aux)
 
             left, top, right, bottom = bbox
             lam = 1 - ((right - left) * (bottom - top) / nn_aux.shape[1])
-            lam_repeat = tf.expand_dims(tf.repeat(lam, tf.shape(inputs)[0], axis=0), 1)
-            nn_cls = keras.layers.Concatenate(axis=-1, dtype="float32", name="class")([nn_cls, lam_repeat])
+            lam_repeat = functional.expand_dims(functional.repeat(lam, functional.shape(inputs)[0], axis=0), 1)
+            nn_cls = layers.Concatenate(axis=-1, dtype="float32", name="class")([nn_cls, lam_repeat])
 
-        # nn_lam = keras.layers.Lambda(lambda ii: tf.cast(tf.stack(ii), tf.float32))([left_pos, top_pos, right_pos, bottom_pos])
+        # nn_lam = layers.Lambda(lambda ii: functional.cast(functional.stack(ii), "float32"))([left_pos, top_pos, right_pos, bottom_pos])
         nn = [nn_cls, nn_aux]
     elif mean_classifier_top:
         # Return mean of all tokens
-        nn = keras.layers.GlobalAveragePooling1D(name="avg_pool")(nn)
-        nn = keras.layers.Dense(num_classes, dtype="float32", name="token_head")(nn)
+        nn = layers.GlobalAveragePooling1D(name="avg_pool")(nn)
+        nn = layers.Dense(num_classes, dtype="float32", name="token_head")(nn)
     elif token_classifier_top:
         # Return dense classifier using only first token
-        nn = keras.layers.Dense(num_classes, dtype="float32", name="token_head")(nn[:, 0])
+        nn = layers.Dense(num_classes, dtype="float32", name="token_head")(nn[:, 0])
     else:
         # Return token dense for evaluation
-        nn_cls = keras.layers.Dense(num_classes, dtype="float32", name="token_head")(nn[:, 0])
-        nn_aux = keras.layers.Dense(num_classes, dtype="float32", name="aux_head")(nn[:, 1:])
-        nn = keras.layers.Add()([nn_cls, tf.reduce_max(nn_aux, 1) * 0.5])
+        nn_cls = layers.Dense(num_classes, dtype="float32", name="token_head")(nn[:, 0])
+        nn_aux = layers.Dense(num_classes, dtype="float32", name="aux_head")(nn[:, 1:])
+        nn = layers.Add()([nn_cls, functional.reduce_max(nn_aux, 1) * 0.5])
 
-    model = tf.keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     add_pre_post_process(model, rescale_mode="torch")
     reload_model_weights(model, PRETRAINED_DICT, "volo", pretrained, PositionalEmbedding)
     return model
 
 
+@register_model
 def VOLO_d1(input_shape=(224, 224, 3), num_classes=1000, pretrained="imagenet", **kwargs):
     num_blocks = [4, 14]
     embed_dims = [192, 384]
     num_heads = [6, 12]
     mlp_ratios = [3, 3]
     stem_hidden_dim = 64
     return VOLO(**locals(), model_name="volo_d1", **kwargs)
 
 
+@register_model
 def VOLO_d2(input_shape=(224, 224, 3), num_classes=1000, pretrained="imagenet", **kwargs):
     num_blocks = [6, 18]
     embed_dims = [256, 512]
     num_heads = [8, 16]
     mlp_ratios = [3, 3]
     stem_hidden_dim = 64
     return VOLO(**locals(), model_name="volo_d2", **kwargs)
 
 
+@register_model
 def VOLO_d3(input_shape=(224, 224, 3), num_classes=1000, pretrained="imagenet", **kwargs):
     num_blocks = [8, 28]
     embed_dims = [256, 512]
     num_heads = [8, 16]
     mlp_ratios = [3, 3]
     stem_hidden_dim = 64
     return VOLO(**locals(), model_name="volo_d3", **kwargs)
 
 
+@register_model
 def VOLO_d4(input_shape=(224, 224, 3), num_classes=1000, pretrained="imagenet", **kwargs):
     num_blocks = [8, 28]
     embed_dims = [384, 768]
     num_heads = [12, 16]
     mlp_ratios = [3, 3]
     stem_hidden_dim = 64
     return VOLO(**locals(), model_name="volo_d4", **kwargs)
 
 
+@register_model
 def VOLO_d5(input_shape=(224, 224, 3), num_classes=1000, pretrained="imagenet", **kwargs):
     num_blocks = [12, 36]
     embed_dims = [384, 768]
     num_heads = [12, 16]
     mlp_ratios = [4, 4]
     stem_hidden_dim = 128
     return VOLO(**locals(), model_name="volo_d5", **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolor/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolor/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,23 +16,26 @@
 Args:
   backbone: backbone model, could be any model with pyramid stage structure.
       Default None for CSPDarknet with csp_depthes={csp_depthes}, csp_channels={csp_channels}.
 """
 
 __tail_doc__ = """  features_pick: specific `layer names` or `pyramid feature indexes` from backbone model.
         Default `[-3, -2, -1]` means using the last 3 pyramid feature output from backbone.
+  regression_len: bbox output len, typical value is 4, for yolov8 reg_max=16 -> regression_len = 16 * 4 == 64.
   anchors_mode: one of ["efficientdet", "anchor_free", "yolor"], controls which anchor to use.
       - efficientdet anchors default settings: use_object_scores=False, num_anchors=9, anchor_scale=4,
           aspect_ratios=[1, 2, 0.5], num_scales=3, grid_zero_start=False.
       - anchor_free default settings: use_object_scores=True, num_anchors=1, anchor_scale=1,
           aspect_ratios=[1], num_scales=1, grid_zero_start=True.
       - yolor default settings: use_object_scores=True, num_anchors=3.
+      - yolov8 default settings: use_object_scores=False, num_anchors=1, anchor_scale=1,
+          aspect_ratios=[1], num_scales=1, grid_zero_start=False.
       Default "yolor".
   num_anchors: number of anchors for a single grid point, should be same with dataset used value.
-      Default "auto" means: anchors_mode=="anchor_free" -> 1, anchors_mode=="yolor" -> 3, else 9.
+      Default "auto" means: anchors_mode=="anchor_free" / "yolov8" -> 1, anchors_mode=="yolor" -> 3, else 9.
   use_object_scores: bollean value if model header output includes `object_scores`.
       Default "auto" means: True if anchors_mode=="anchor_free" or anchors_mode=="yolor", else False.
   input_shape: input shape if backbone is None, else will use input_shape from backbone.
   num_classes: total output classes. Set `0` to disable `classifier` output. Default 80 for COCO.
   activation: activation used in whole model, default `swish`. Default "swish".
   classifier_activation: The activation function to use for classifier output if `num_classes > 0`.
       Set `classifier_activation=None` to return the logits of the "top" layer. Default `sigmoid`.
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolor/yolor.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov8/yolov8.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,384 +1,439 @@
-import tensorflow as tf
-from tensorflow import keras
+import math
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, models, initializers, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
-    BiasLayer,
-    ChannelAffine,
     activation_by_name,
     batchnorm_with_activation,
     conv2d_no_bias,
-    depthwise_conv2d_no_bias,
     add_pre_post_process,
+    ZeroInitGain,
 )
 from keras_cv_attention_models import model_surgery
 from keras_cv_attention_models.download_and_load import reload_model_weights
 from keras_cv_attention_models.coco import eval_func, anchors_func
 
 PRETRAINED_DICT = {
-    "yolor_csp": {"coco": "ed0aa82a07c4e65e9cd3d2e6ad2d0548"},
-    "yolor_csp_x": {"coco": "615125ce1cd1c855f8045bf079456598"},
-    "yolor_p6": {"coco": "059c6d0dd8ca869f843081b13f88f7f4"},
-    "yolor_w6": {"coco": "a3dc1e70c5064aebfd8b52609e6ee704"},
-    "yolor_e6": {"coco": "556263cf6aeea5b628c1814cd126eb21"},
-    "yolor_d6": {"coco": "a55469feef931a07b419c3e1be639725"},
+    "yolov8_l": {"coco": "db0fcde5d2811b33b7f5f0f400d76911"},
+    "yolov8_m": {"coco": "cb8c25148bb17485776ade4cf80cc6f6"},
+    "yolov8_n": {"coco": "4cb83c7e452cdcd440b75546df0b211e"},
+    "yolov8_s": {"coco": "4e1ac133e2a8831845172d8491c2747a"},
+    "yolov8_x": {"coco": "2be28e650bf299aeea7ee26ab765a23e"},
+    "yolov8_x6": {"coco": "f51ed830ccf5efae7dc56f2ce5e20890"},
+    "yolov8_l_cls": {"imagenet": "071f41125034dd15401f6c6925fc1e6f"},
+    "yolov8_m_cls": {"imagenet": "35ef50aa07ff232afa08f321447e354d"},
+    "yolov8_n_cls": {"imagenet": "b1cfac787589689c0f2abde6893ec140"},
+    "yolov8_s_cls": {"imagenet": "2caa57e8cf67b39921c35f89cea5061c"},
+    "yolov8_x_cls": {"imagenet": "2d4b8b996c24f5fde903678ee8b7cf20"},
 }
 
 
-""" CSPDarknet backbone """
-BATCH_NORM_EPSILON = 1e-4
+""" Yolov8Backbone """
+BATCH_NORM_EPSILON = 1e-3
 BATCH_NORM_MOMENTUM = 0.97
 
 
-def conv_dw_pw_block(inputs, filters, kernel_size=1, strides=1, use_depthwise_conv=False, activation="swish", name=""):
-    nn = inputs
-    if use_depthwise_conv:
-        nn = depthwise_conv2d_no_bias(nn, kernel_size, strides, padding="SAME", name=name)
-        nn = batchnorm_with_activation(nn, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name + "dw_")
-        kernel_size, strides = 1, 1
-    nn = conv2d_no_bias(nn, filters, kernel_size, strides, padding="SAME", name=name)
-    nn = batchnorm_with_activation(nn, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name)
-    return nn
-
-
-def csp_block(inputs, expansion=0.5, use_shortcut=True, activation="swish", name=""):
-    input_channels = inputs.shape[-1]
-    nn = conv_dw_pw_block(inputs, int(input_channels * expansion), activation=activation, name=name + "1_")
-    nn = conv_dw_pw_block(nn, input_channels, kernel_size=3, strides=1, activation=activation, name=name + "2_")
-    if use_shortcut:
-        nn = keras.layers.Add()([inputs, nn])
-    return nn
-
-
-def csp_stack(
-    inputs, depth, out_channels=-1, expansion=0.5, use_shortcut=True, use_pre=False, use_post=True, use_shortcut_bn=True, activation="swish", name=""
+def conv_bn(inputs, output_channel, kernel_size=1, strides=1, use_bias=False, activation="swish", name=""):
+    # print(f">>>> {inputs.shape = }, {output_channel = }, {kernel_size = }, {strides = }")
+    nn = conv2d_no_bias(inputs, output_channel, kernel_size, strides, use_bias=use_bias, padding="same", name=name)
+    return batchnorm_with_activation(nn, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name)
+
+
+def reparam_conv_bn(inputs, output_channel, kernel_size=3, strides=1, use_bias=False, use_identity=True, activation="swish", name=""):
+    branch_3x3 = conv2d_no_bias(inputs, output_channel, 3, strides, use_bias=False, padding="same", name=name + "REPARAM_k3_")
+    branch_3x3 = batchnorm_with_activation(branch_3x3, activation=None, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name + "REPARAM_k3_")
+
+    branch_1x1 = conv2d_no_bias(inputs, output_channel, 1, strides, use_bias=use_bias, padding="valid", name=name + "REPARAM_k1_")
+    # branch_1x1 = batchnorm_with_activation(branch_1x1, activation=None, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name + "REPARAM_k1_")
+
+    # out = (branch_3x3 + branch_1x1 + inputs) if use_identity else (branch_3x3 + branch_1x1)
+    out = layers.Add(name=name + "REPARAM_out")([branch_3x3, branch_1x1, inputs] if use_identity else [branch_3x3, branch_1x1])
+    return batchnorm_with_activation(out, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name)
+
+
+def csp_with_2_conv(
+    inputs,
+    channels=-1,
+    depth=2,
+    shortcut=True,
+    expansion=0.5,
+    parallel_mode=True,
+    use_bias=False,
+    use_alpha=False,
+    use_reparam_conv=False,
+    activation="swish",
+    name="",
 ):
-    out_channels = inputs.shape[-1] if out_channels == -1 else out_channels
-    hidden_channels = int(out_channels * expansion)
-    if use_pre:
-        inputs = conv_dw_pw_block(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "pre_")
-    if use_shortcut_bn:
-        short = conv_dw_pw_block(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "short_")
-    else:
-        short = conv2d_no_bias(inputs, hidden_channels, 1, name=name + "short_")
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    channels = channels if channels > 0 else inputs.shape[channel_axis]
+    hidden_channels = int(channels * expansion)
+
+    # short = conv_bn(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "short_")  # For YOLO_NAS
+    # deep = conv_bn(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "deep_")  # For YOLO_NAS
+    pre = conv_bn(inputs, hidden_channels * 2, kernel_size=1, activation=activation, name=name + "pre_")
+    if parallel_mode:
+        short, deep = functional.split(pre, 2, axis=channel_axis)
+    else:  # parallel_mode=False for YOLOV8_X6 `path_aggregation_fpn` C2 module, deep branch first
+        deep, short = functional.split(pre, 2, axis=channel_axis)
 
-    deep = inputs if use_pre else conv_dw_pw_block(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "deep_pre_")
+    out = [short, deep]
     for id in range(depth):
-        block_name = name + "block{}_".format(id + 1)
-        deep = csp_block(deep, 1, use_shortcut=use_shortcut, activation=activation, name=block_name)
-    if use_post:
-        deep = conv_dw_pw_block(deep, hidden_channels, kernel_size=1, activation=activation, name=name + "deep_post_")
-
-    out = tf.concat([deep, short], axis=-1)
-    if not use_shortcut_bn:
-        out = batchnorm_with_activation(out, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name + "concat_")
+        cur_short = ZeroInitGain(name=name + "short_{}_alpha".format(id))(out[-1]) if use_alpha else out[-1]
+        cur_name = name + "pre_{}_".format(id)
+        if use_reparam_conv:
+            deep = reparam_conv_bn(deep, hidden_channels, kernel_size=3, use_bias=use_bias, activation=activation, name=cur_name + "1_")
+            deep = reparam_conv_bn(deep, hidden_channels, kernel_size=3, use_bias=use_bias, activation=activation, name=cur_name + "2_")
+        else:
+            deep = conv_bn(deep, hidden_channels, kernel_size=3, use_bias=use_bias, activation=activation, name=cur_name + "1_")
+            deep = conv_bn(deep, hidden_channels, kernel_size=3, use_bias=use_bias, activation=activation, name=cur_name + "2_")
 
-    out = conv_dw_pw_block(out, out_channels, kernel_size=1, activation=activation, name=name + "output_")
+        deep = (cur_short + deep) if shortcut else deep
+        out.append(deep)
+    # parallel_mode=False for YOLOV8_X6 `path_aggregation_fpn` C2 module, only concat `short` and the last `deep` one.
+    out = functional.concat(out, axis=channel_axis) if parallel_mode else functional.concat([deep, short], axis=channel_axis)
+    # out = functional.concat([*out[1:], out[0]], axis=channel_axis) if parallel_mode else functional.concat([deep, short], axis=channel_axis)  # For YOLO_NAS
+    out = conv_bn(out, channels, kernel_size=1, activation=activation, name=name + "output_")
     return out
 
 
-def res_spatial_pyramid_pooling(inputs, depth, expansion=0.5, pool_sizes=(5, 9, 13), use_shortcut_bn=True, activation="swish", name=""):
-    input_channels = inputs.shape[-1]
-    hidden_channels = int(input_channels * expansion)
-    if use_shortcut_bn:
-        short = conv_dw_pw_block(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "short_")
-    else:
-        short = conv2d_no_bias(inputs, hidden_channels, 1, name=name + "short_")
+def spatial_pyramid_pooling_fast(inputs, pool_size=5, activation="swish", name=""):
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    input_channels = inputs.shape[channel_axis]
+    hidden_channels = int(input_channels // 2)
+
+    nn = conv_bn(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "pre_")
+    pool_1 = layers.MaxPool2D(pool_size=pool_size, strides=1, padding="same")(nn)
+    pool_2 = layers.MaxPool2D(pool_size=pool_size, strides=1, padding="same")(pool_1)
+    pool_3 = layers.MaxPool2D(pool_size=pool_size, strides=1, padding="same")(pool_2)
 
-    deep = conv_dw_pw_block(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "pre_1_")
-    deep = conv_dw_pw_block(deep, hidden_channels, kernel_size=3, activation=activation, name=name + "pre_2_")
-    deep = conv_dw_pw_block(deep, hidden_channels, kernel_size=1, activation=activation, name=name + "pre_3_")
-    pp = [keras.layers.MaxPooling2D(pool_size=ii, strides=1, padding="SAME")(deep) for ii in pool_sizes]
-    deep = tf.concat([deep, *pp][::-1], axis=-1)  # yolor_csp.cfg, SSP concat layers=-1,-3,-5,-6
-    for id in range(depth - 1):  # First one is `pre`
-        deep = conv_dw_pw_block(deep, hidden_channels, kernel_size=1, activation=activation, name=name + "post_{}_".format(id * 2 + 1))
-        deep = conv_dw_pw_block(deep, hidden_channels, kernel_size=3, activation=activation, name=name + "post_{}_".format(id * 2 + 2))
-
-    out = tf.concat([deep, short], axis=-1)
-    if not use_shortcut_bn:
-        out = batchnorm_with_activation(out, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name + "concat_")
-    out = conv_dw_pw_block(out, hidden_channels, kernel_size=1, activation=activation, name=name + "output_")
+    out = functional.concat([nn, pool_1, pool_2, pool_3], axis=channel_axis)
+    out = conv_bn(out, input_channels, kernel_size=1, activation=activation, name=name + "output_")
     return out
 
 
-def focus_stem(inputs, filters, kernel_size=3, strides=1, padding="valid", activation="swish", name=""):
-    if padding.lower() == "same":  # Handling odd input_shape
-        inputs = tf.pad(inputs, [[0, 0], [0, 1], [0, 1], [0, 0]])
-        patch_top_left = inputs[:, :-1:2, :-1:2]
-        patch_top_right = inputs[:, :-1:2, 1::2]
-        patch_bottom_left = inputs[:, 1::2, :-1:2]
-        patch_bottom_right = inputs[:, 1::2, 1::2]
-    else:
-        patch_top_left = inputs[:, ::2, ::2]
-        patch_top_right = inputs[:, ::2, 1::2]
-        patch_bottom_left = inputs[:, 1::2, ::2]
-        patch_bottom_right = inputs[:, 1::2, 1::2]
-    nn = tf.concat([patch_top_left, patch_bottom_left, patch_top_right, patch_bottom_right], axis=-1)
-    nn = conv_dw_pw_block(nn, filters, kernel_size=kernel_size, strides=strides, activation=activation, name=name)
-    return nn
-
-
-def csp_conv_downsample(inputs, filters, strides=2, activation="swish", name=""):
-    # DownC: https://github.com/WongKinYiu/yolor/blob/paper/models/common.py#L584
-    max_down = keras.layers.MaxPooling2D(strides, strides=strides, padding="SAME")(inputs)
-    max_down = conv_dw_pw_block(max_down, filters // 2, activation=activation, name=name + "max_down_")
-    conv_down = conv_dw_pw_block(inputs, inputs.shape[-1], activation=activation, name=name + "conv_down_1_")
-    conv_down = conv_dw_pw_block(conv_down, filters // 2, kernel_size=3, strides=strides, activation=activation, name=name + "conv_down_2_")
-    return tf.concat([conv_down, max_down], axis=-1)
-
-
-def CSPDarknet(
-    depthes=[2, 8, 8, 4],
-    channels=[128, 256, 512, 1024],
-    stem_width=-1,  # -1 means using channels[0] // 2
-    use_focus_stem=False,
-    ssp_depth=2,
+def YOLOV8Backbone(
+    channels=[32, 64, 128, 256],
+    depthes=[1, 2, 2, 1],
     out_features=[-3, -2, -1],
-    use_csp_downsample=False,
-    use_shortcut_bn=True,
-    use_pre=False,
-    use_post=True,
-    input_shape=(512, 512, 3),
+    csp_expansions=0.5,
+    csp_parallel_mode=True,
+    use_alpha=False,
+    use_bias=False,
+    use_reparam_conv=False,
+    input_shape=(640, 640, 3),
     activation="swish",
-    model_name="",
+    num_classes=0,  # > 0 value for classification model
+    dropout=0,  # for classification model
+    classifier_activation="softmax",  # for classification model
+    pretrained=None,  # for classification model
+    model_name="yolov8_backbone",
+    kwargs=None,  # Not using, recieving parameter
 ):
-    # base_channels = int(width_mul * 64)
-    inputs = keras.layers.Input(input_shape)
+    inputs = layers.Input(backend.align_input_shape_by_image_data_format(input_shape))
+    is_classification_model = num_classes > 0
+
+    global BATCH_NORM_EPSILON
+    global BATCH_NORM_MOMENTUM
+    if is_classification_model:
+        BATCH_NORM_EPSILON = 1e-5
+        BATCH_NORM_MOMENTUM = 0.9
+    else:
+        BATCH_NORM_EPSILON = 1e-3
+        BATCH_NORM_MOMENTUM = 0.97
 
     """ Stem """
-    stem_width = stem_width if stem_width > 0 else channels[0] // 2
-    if use_focus_stem:
-        nn = focus_stem(inputs, stem_width, activation=activation, name="stem_")
+    # stem_width = stem_width if stem_width > 0 else channels[0]
+    stem_width = channels[0]
+    if use_reparam_conv:
+        nn = reparam_conv_bn(inputs, stem_width // 2, kernel_size=3, strides=2, use_bias=use_bias, use_identity=False, activation=activation, name="stem_1_")
+        nn = reparam_conv_bn(nn, stem_width, kernel_size=3, strides=2, use_bias=use_bias, use_identity=False, activation=activation, name="stem_2_")
     else:
-        nn = conv_dw_pw_block(inputs, 32, kernel_size=3, strides=1, activation=activation, name="stem_1_")  # Fixed as 32
-        nn = conv_dw_pw_block(nn, stem_width, kernel_size=3, strides=2, activation=activation, name="stem_2_")
-        nn = csp_block(nn, expansion=0.5, activation=activation, name="stem_3_")
+        nn = conv_bn(inputs, stem_width // 2, kernel_size=3, strides=2, use_bias=use_bias, activation=activation, name="stem_1_")
+        nn = conv_bn(nn, stem_width, kernel_size=3, strides=2, use_bias=use_bias, activation=activation, name="stem_2_")
+
+    """ blocks """
+    block_kwargs = dict(use_bias=use_bias, use_alpha=use_alpha, use_reparam_conv=use_reparam_conv, activation=activation)
     features = [nn]
+    for stack_id, (channel, depth) in enumerate(zip(channels, depthes)):
+        stack_name = "stack{}_".format(stack_id + 1)
+        cur_name = stack_name + "downsample_"
+        if stack_id >= 1:
+            if use_reparam_conv:
+                nn = reparam_conv_bn(nn, channel, kernel_size=3, strides=2, use_bias=use_bias, use_identity=False, activation=activation, name=cur_name)
+            else:
+                nn = conv_bn(nn, channel, kernel_size=3, strides=2, use_bias=use_bias, activation=activation, name=cur_name)
+        csp_expansion = csp_expansions[stack_id] if isinstance(csp_expansions, (list, tuple)) else csp_expansions
+        parallel_mode = csp_parallel_mode[stack_id] if isinstance(csp_parallel_mode, (list, tuple)) else csp_parallel_mode
+        nn = csp_with_2_conv(nn, depth=depth, expansion=csp_expansion, parallel_mode=parallel_mode, **block_kwargs, name=stack_name + "c2f_")
 
-    """ dark blocks """
-    # depthes = [max(round(depth_mul * ii), 1) for ii in [2, 8, 8, 4]]  # YOLOR_CSP depth
-    # channels = [base_channels * 2, base_channels * 4, base_channels * 8, base_channels * 16]
-    # use_spps = [False] * (len(depthes) - 1) + [True]
-    for id, (channel, depth) in enumerate(zip(channels, depthes)):
-        stack_name = "stack{}_".format(id + 1)
-        if use_csp_downsample:
-            nn = csp_conv_downsample(nn, channel, activation=activation, name=stack_name)
-        else:
-            nn = conv_dw_pw_block(nn, channel, 3, strides=2, activation=activation, name=stack_name + "downsample_")
-        nn = csp_stack(nn, depth, use_pre=use_pre, use_post=use_post, use_shortcut_bn=use_shortcut_bn, activation=activation, name=stack_name)
-        if id == len(depthes) - 1:
-            # ssp_depth = max(round(depth_mul * 2), 1)
-            nn = res_spatial_pyramid_pooling(nn, ssp_depth, use_shortcut_bn=use_shortcut_bn, activation=activation, name=stack_name + "spp_")
+        if not is_classification_model and stack_id == len(depthes) - 1:
+            nn = spatial_pyramid_pooling_fast(nn, pool_size=5, activation=activation, name=stack_name + "spp_fast_")
         features.append(nn)
 
-    nn = [features[ii] for ii in out_features]
-    model = keras.models.Model(inputs, nn, name=model_name)
+    if is_classification_model:
+        nn = conv_bn(nn, 1280, kernel_size=1, strides=1, activation=activation, name="pre_")
+        nn = layers.GlobalAveragePooling2D(name="avg_pool")(nn)
+        if dropout > 0:
+            nn = layers.Dropout(dropout, name="head_drop")(nn)
+        outputs = layers.Dense(num_classes, dtype="float32", activation=classifier_activation, name="predictions")(nn)
+    else:
+        outputs = [features[ii] for ii in out_features]
+    model = models.Model(inputs, outputs, name=model_name)
+
+    if is_classification_model:
+        add_pre_post_process(model, rescale_mode="raw01")
+        reload_model_weights(model, PRETRAINED_DICT, "yolov8", pretrained)
     return model
 
 
 """ path aggregation fpn """
 
 
-def upsample_merge(inputs, csp_depth, use_shortcut_bn=True, activation="swish", name=""):
-    # print(f">>>> upsample_merge inputs: {[ii.shape for ii in inputs] = }")
-    upsample = conv_dw_pw_block(inputs[-1], inputs[0].shape[-1], activation=activation, name=name + "up_")
-
-    # inputs[0] = keras.layers.UpSampling2D(size=(2, 2), interpolation="nearest", name=name + "up")(fpn_out)
-    inputs[-1] = tf.image.resize(upsample, tf.shape(inputs[0])[1:-1], method="nearest")
-    nn = tf.concat(inputs, axis=-1)
-    use_shortcut, use_pre, use_post = False, True, False
-    nn = csp_stack(nn, csp_depth, nn.shape[-1] // 2, 1.0, use_shortcut, use_pre, use_post, use_shortcut_bn, activation=activation, name=name)
-    return nn
-
-
-def downsample_merge(inputs, csp_depth, use_csp_downsample=False, use_shortcut_bn=True, activation="swish", name=""):
-    # print(f">>>> downsample_merge inputs: {[ii.shape for ii in inputs] = }")
-    if use_csp_downsample:
-        inputs[0] = csp_conv_downsample(inputs[0], inputs[-1].shape[-1], activation=activation, name=name)
-    else:
-        inputs[0] = conv_dw_pw_block(inputs[0], inputs[-1].shape[-1], 3, 2, activation=activation, name=name + "down_")
-    nn = tf.concat(inputs, axis=-1)
-    use_shortcut, use_pre, use_post = False, True, False
-    nn = csp_stack(nn, csp_depth, nn.shape[-1] // 2, 1.0, use_shortcut, use_pre, use_post, use_shortcut_bn, activation=activation, name=name)
-    return nn
-
-
-def path_aggregation_fpn(features, fpn_depth=2, use_csp_downsample=False, use_shortcut_bn=True, activation="swish", name=""):
-    # p5 ---------------------> out2
-    #        [up -> concat]   [down -> concat]
-    # p4 -> p4p5 ------------> out1
-    #        [up -> concat]   [down -> concat]
-    # p3 -> p3p4p5 -------------> out0
+def path_aggregation_fpn(features, depth=3, parallel_mode=True, use_reparam_conv=False, activation="swish", name=""):
+    # yolov8
+    # 9: p5 1024 ---+----------------------+-> 21: out2 1024
+    #               v [up 1024 -> concat]  ^ [down 512 -> concat]
+    # 6: p4 512 --> 12: p4p5 512 --------> 18: out1 512
+    #               v [up 512 -> concat]   ^ [down 256 -> concat]
+    # 4: p3 256 --> 15: p3p4p5 256 --------+--> 15: out0 128
     # features: [p3, p4, p5]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
     upsamples = [features[-1]]
     p_name = "p{}_".format(len(features) + 2)
     # upsamples: [p5], features[:-1][::-1]: [p4, p3] -> [p5, p4p5, p3p4p5]
-    for id, ii in enumerate(features[:-1][::-1]):
+    for id, feature in enumerate(features[:-1][::-1]):
         cur_p_name = "p{}".format(len(features) + 1 - id)
-        nn = conv_dw_pw_block(ii, ii.shape[-1] // 2, kernel_size=1, activation=activation, name=name + cur_p_name + "_down_")
         p_name = cur_p_name + p_name
-        nn = upsample_merge([nn, upsamples[-1]], fpn_depth, use_shortcut_bn, activation=activation, name=name + p_name)
+        size = functional.shape(feature)[1:-1] if image_data_format() == "channels_last" else functional.shape(feature)[2:]
+        nn = functional.resize(upsamples[-1], size, method="nearest")
+        nn = functional.concat([nn, feature], axis=channel_axis)
+
+        out_channel = feature.shape[channel_axis]
+        nn = csp_with_2_conv(
+            nn, out_channel, depth, shortcut=False, parallel_mode=parallel_mode, use_reparam_conv=use_reparam_conv, activation=activation, name=name + p_name
+        )
         upsamples.append(nn)
 
     downsamples = [upsamples[-1]]
     # downsamples: [p3p4p5], upsamples[:-1][::-1]: [p4p5, p5] -> [p3p4p5, p3p4p5 + p4p5, p3p4p5 + p4p5 + p5]
     for id, ii in enumerate(upsamples[:-1][::-1]):
         cur_name = name + "c3n{}_".format(id + 3)
-        nn = downsample_merge([downsamples[-1], ii], fpn_depth, use_csp_downsample, use_shortcut_bn, activation=activation, name=cur_name)
+        nn = conv_bn(downsamples[-1], downsamples[-1].shape[channel_axis], kernel_size=3, strides=2, activation=activation, name=cur_name + "down_")
+        nn = functional.concat([nn, ii], axis=channel_axis)
+
+        out_channel = ii.shape[channel_axis]
+        nn = csp_with_2_conv(
+            nn, out_channel, depth=depth, shortcut=False, parallel_mode=parallel_mode, use_reparam_conv=False, activation=activation, name=cur_name
+        )
         downsamples.append(nn)
     return downsamples
 
 
-""" YOLORHead """
-
-
-def yolor_head_single(inputs, filters, num_classes=80, num_anchors=3, use_object_scores=True, activation="swish", name=""):
-    initializer = tf.initializers.truncated_normal(stddev=0.2)
-
-    nn = conv_dw_pw_block(inputs, filters, 3, activation=activation, name=name + "1_")
-    nn = BiasLayer(initializer=initializer, name=name + "shift_channel")(nn)
-
-    ouput_classes = num_classes + (5 if use_object_scores else 4)  # num_anchors = 3, num_anchors * (80 + 5) = 255
-    nn = keras.layers.Conv2D(ouput_classes * num_anchors, kernel_size=1, name=name + "2_conv")(nn)
-    control_channels_layer = ChannelAffine(use_bias=False, name=name + "control_channel")
-    control_channels_layer.ww_init = initializer
-    nn = control_channels_layer(nn)
-    # return nn
-    return keras.layers.Reshape([-1, ouput_classes], name=name + "output_reshape")(nn)
-
+def yolov8_head(
+    inputs,
+    num_classes=80,
+    regression_len=64,
+    num_anchors=1,
+    depth=2,
+    hidden_channels=-1,
+    use_object_scores=False,
+    activation="swish",
+    classifier_activation="sigmoid",
+    name="",
+):
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
 
-def yolor_head(inputs, num_classes=80, num_anchors=1, use_object_scores=True, activation="swish", classifier_activation="sigmoid", name=""):
     outputs = []
-    for id, input in enumerate(inputs):
+    if hidden_channels == -1:
+        reg_channel = max(64, regression_len, inputs[0].shape[channel_axis] // 4)
+        cls_channel = max(num_classes, inputs[0].shape[channel_axis])
+        reg_channels, cls_channels = [reg_channel] * len(inputs), [cls_channel] * len(inputs)
+    elif isinstance(hidden_channels, (list, tuple)):
+        reg_channels, cls_channels = hidden_channels, hidden_channels
+
+    for id, (feature, reg_channel, cls_channel) in enumerate(zip(inputs, reg_channels, cls_channels)):
         cur_name = name + "{}_".format(id + 1)
-        filters = int(input.shape[-1] * 2)
-        out = yolor_head_single(input, filters, num_classes, num_anchors, use_object_scores, activation=activation, name=cur_name)
+
+        reg_nn = feature
+        for id in range(depth):
+            reg_nn = conv_bn(reg_nn, reg_channel, 3, activation=activation, name=cur_name + "reg_{}_".format(id + 1))
+        reg_out = conv2d_no_bias(reg_nn, regression_len * num_anchors, 1, use_bias=True, bias_initializer="ones", name=cur_name + "reg_3_")
+
+        strides = 2 ** (id + 3)
+        bias_init = initializers.constant(math.log(5 / num_classes / (640 / strides) ** 2))
+        cls_nn = feature
+        for id in range(depth):
+            cls_nn = conv_bn(cls_nn, cls_channel, 3, activation=activation, name=cur_name + "cls_{}_".format(id + 1))
+        cls_out = conv2d_no_bias(cls_nn, num_classes * num_anchors, 1, use_bias=True, bias_initializer=bias_init, name=cur_name + "cls_3_")
+        if classifier_activation is not None:
+            cls_out = activation_by_name(cls_out, classifier_activation, name=cur_name + "classifier_")
+
+        # obj_preds, not using for yolov8
+        if use_object_scores:
+            bias_init = initializers.constant(-math.log((1 - 0.01) / 0.01))
+            obj_out = conv2d_no_bias(reg_nn, 1 * num_anchors, kernel_size=1, use_bias=True, bias_initializer=bias_init, name=cur_name + "object_")
+            obj_out = activation_by_name(obj_out, classifier_activation, name=cur_name + "object_out_")
+            out = functional.concat([reg_out, cls_out, obj_out], axis=channel_axis)
+        else:
+            out = functional.concat([reg_out, cls_out], axis=channel_axis)
+        out = out if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(out)
+        out = layers.Reshape([-1, out.shape[-1]], name=cur_name + "output_reshape")(out)
         outputs.append(out)
-    # return outputs
-    outputs = tf.concat(outputs, axis=1)
-    return activation_by_name(outputs, classifier_activation, name="classifier_")
+    outputs = functional.concat(outputs, axis=1)
+    return outputs
 
 
-""" YOLOR models """
+""" YOLOV8 models """
 
 
-def YOLOR(
+def YOLOV8(
     backbone=None,
-    csp_depthes=[2, 8, 8, 4],
-    csp_channels=[128, 256, 512, 1024],
-    stem_width=-1,  # -1 means using csp_channels[0] // 2
-    use_focus_stem=False,
-    ssp_depth=2,
-    csp_use_pre=False,
-    csp_use_post=True,
-    use_csp_downsample=False,
-    use_shortcut_bn=True,
-    fpn_depth=2,
-    features_pick=[-3, -2, -1],
-    anchors_mode="yolor",
-    num_anchors="auto",  # "auto" means: anchors_mode=="anchor_free" -> 1, anchors_mode=="yolor" -> 3, else 9
+    csp_channels=[32, 64, 128, 256],  # [YOLOV8Backbone parameters]
+    csp_depthes=[1, 2, 2, 1],
+    features_pick=[-3, -2, -1],  # [Detector parameters]
+    regression_len=64,  # bbox output len, typical value is 4, for yolov8 reg_max=16 -> regression_len = 16 * 4 == 64
+    use_reparam_conv=False,  # Use reparam_conv_bn instead of conv_bn block in all csp_blocks.
+    paf_parallel_mode=True,  # paf_parallel_mode=False for YOLOV8_X6 `path_aggregation_fpn` module, only concat `short` and the last `deep` one.
+    anchors_mode="yolov8",
+    num_anchors="auto",  # "auto" means: anchors_mode=="anchor_free" / "yolov8" -> 1, anchors_mode=="yolor" -> 3, else 9
     use_object_scores="auto",  # "auto" means: True if anchors_mode=="anchor_free" or anchors_mode=="yolor", else False
     input_shape=(640, 640, 3),
     num_classes=80,
     activation="swish",
     classifier_activation="sigmoid",
     freeze_backbone=False,
     pretrained=None,
-    model_name="yolor",
+    model_name="yolov8",
     pyramid_levels_min=3,  # Init anchors for model prediction.
     anchor_scale="auto",  # Init anchors for model prediction. "auto" means 1 if (anchors_mode=="anchor_free" or anchors_mode=="yolor"), else 4
     rescale_mode="raw01",  # For decode predictions, raw01 means input value in range [0, 1].
     kwargs=None,  # Not using, recieving parameter
 ):
     if backbone is None:
-        # Save line width...
-        csp_kwargs = {"use_pre": csp_use_pre, "use_post": csp_use_post, "input_shape": input_shape, "activation": activation, "model_name": "darknet"}
-        backbone = CSPDarknet(
-            csp_depthes, csp_channels, stem_width, use_focus_stem, ssp_depth, features_pick, use_csp_downsample, use_shortcut_bn, **csp_kwargs
+        backbone = YOLOV8Backbone(
+            csp_channels, csp_depthes, features_pick, use_reparam_conv=use_reparam_conv, input_shape=input_shape, activation=activation, model_name="backbone"
         )
         features = backbone.outputs
     else:
         if isinstance(features_pick[0], str):
             features = [backbone.get_layer(layer_name) for layer_name in features_pick]
         else:
             features = model_surgery.get_pyramide_feature_layers(backbone)
             features = [features[id] for id in features_pick]
-        print(">>>> features:", {ii.name: ii.output_shape for ii in features})
-        features = [ii.output for ii in features]
+        feature_names, features = model_surgery.align_pyramide_feature_output_by_image_data_format(features)
+        print(">>>> features:", {ii: jj.shape for ii, jj in zip(feature_names, features)})
 
     backbone.trainable = False if freeze_backbone else True
     use_object_scores, num_anchors, anchor_scale = anchors_func.get_anchors_mode_parameters(anchors_mode, use_object_scores, num_anchors, anchor_scale)
     inputs = backbone.inputs[0]
 
-    fpn_features = path_aggregation_fpn(features, fpn_depth, use_csp_downsample, use_shortcut_bn, activation=activation, name="pafpn_")
-    outputs = yolor_head(fpn_features, num_classes, num_anchors, use_object_scores, activation, classifier_activation, name="head_")
-    outputs = keras.layers.Activation("linear", dtype="float32", name="outputs_fp32")(outputs)
-    model = keras.models.Model(inputs, outputs, name=model_name)
-    reload_model_weights(model, PRETRAINED_DICT, "yolor", pretrained)
+    fpn_features = path_aggregation_fpn(features, csp_depthes[-1], paf_parallel_mode, use_reparam_conv=use_reparam_conv, activation=activation, name="pafpn_")
+
+    header_kwargs = {"use_object_scores": use_object_scores, "activation": activation, "classifier_activation": classifier_activation}
+    outputs = yolov8_head(fpn_features, num_classes, regression_len, num_anchors, **header_kwargs, name="head_")
+    outputs = layers.Activation("linear", dtype="float32", name="outputs_fp32")(outputs)
+
+    model = models.Model(inputs, outputs, name=model_name)
+    reload_model_weights(model, PRETRAINED_DICT, "yolov8", pretrained)
 
     pyramid_levels = [pyramid_levels_min, pyramid_levels_min + len(features_pick) - 1]  # -> [3, 5]
-    post_process = eval_func.DecodePredictions(backbone.input_shape[1:], pyramid_levels, anchors_mode, use_object_scores, anchor_scale)
+    post_process = eval_func.DecodePredictions(
+        backbone.input_shape[1:], pyramid_levels, anchors_mode, use_object_scores, anchor_scale, regression_len=regression_len
+    )
     add_pre_post_process(model, rescale_mode=rescale_mode, post_process=post_process)
+    model.switch_to_deploy = lambda: switch_to_deploy(model)
     return model
 
 
-def YOLOR_CSP(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
-    csp_depthes = [2, 8, 8, 4]
-    csp_channels = [128, 256, 512, 1024]
-    fpn_depth = 2
-    ssp_depth = 2
-    return YOLOR(**locals(), model_name=kwargs.pop("model_name", "yolor_csp"), **kwargs)
-
-
-def YOLOR_CSPX(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
-    csp_depthes = [3, 10, 10, 5]
-    csp_channels = [160, 320, 640, 1280]
-    fpn_depth = 3
-    ssp_depth = 3
-    return YOLOR(**locals(), model_name=kwargs.pop("model_name", "yolor_csp_x"), **kwargs)
-
-
-def YOLOR_P6(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
-    csp_depthes = [3, 7, 7, 3, 3]
-    csp_channels = [128, 256, 384, 512, 640]
-    features_pick = [-4, -3, -2, -1]
-    fpn_depth = 3
-    ssp_depth = 2
-    use_focus_stem = True
-    csp_use_post = False
-    return YOLOR(**locals(), model_name=kwargs.pop("model_name", "yolor_p6"), **kwargs)
+def switch_to_deploy(model):
+    from keras_cv_attention_models.model_surgery.model_surgery import fuse_reparam_blocks, convert_to_fused_conv_bn_model
 
+    new_model = convert_to_fused_conv_bn_model(fuse_reparam_blocks(convert_to_fused_conv_bn_model(model)))
 
-def YOLOR_W6(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
-    csp_depthes = [3, 7, 7, 3, 3]
-    csp_channels = [128, 256, 512, 768, 1024]
-    features_pick = [-4, -3, -2, -1]
-    fpn_depth = 3
-    ssp_depth = 2
-    use_focus_stem = True
-    csp_use_post = False
-    return YOLOR(**locals(), model_name=kwargs.pop("model_name", "yolor_w6"), **kwargs)
+    # Has to create a new one, or will `ValueError: Unable to create group (name already exists)` when saving
+    post_process = eval_func.DecodePredictions(
+        input_shape=model.decode_predictions.__input_shape__,
+        pyramid_levels=model.decode_predictions.pyramid_levels,
+        anchors_mode=model.decode_predictions.anchors_mode,
+        use_object_scores=model.decode_predictions.use_object_scores,
+        anchor_scale=model.decode_predictions.anchor_scale,
+        regression_len=model.decode_predictions.regression_len,
+    )
+    add_pre_post_process(new_model, rescale_mode=model.preprocess_input.rescale_mode, post_process=post_process)
+    return new_model
 
 
-def YOLOR_E6(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
-    csp_depthes = [3, 7, 7, 3, 3]
-    csp_channels = [160, 320, 640, 960, 1280]
-    features_pick = [-4, -3, -2, -1]
-    fpn_depth = 3
-    ssp_depth = 2
-    use_focus_stem = True
-    csp_use_post = False
-    use_csp_downsample = True
-    use_shortcut_bn = False
-    return YOLOR(**locals(), model_name=kwargs.pop("model_name", "yolor_e6"), **kwargs)
+@register_model
+def YOLOV8_N(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
+    return YOLOV8(**locals(), model_name=kwargs.pop("model_name", "yolov8_n"), **kwargs)
+
+
+@register_model
+def YOLOV8_S(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
+    csp_channels = [64, 128, 256, 512]
+    return YOLOV8(**locals(), model_name=kwargs.pop("model_name", "yolov8_s"), **kwargs)
+
+
+@register_model
+def YOLOV8_M(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
+    csp_channels = [96, 192, 384, 576]
+    csp_depthes = [2, 4, 4, 2]
+    return YOLOV8(**locals(), model_name=kwargs.pop("model_name", "yolov8_m"), **kwargs)
+
+
+@register_model
+def YOLOV8_L(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
+    csp_channels = [128, 256, 512, 512]
+    csp_depthes = [3, 6, 6, 3]
+    return YOLOV8(**locals(), model_name=kwargs.pop("model_name", "yolov8_l"), **kwargs)
 
 
-def YOLOR_D6(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
-    csp_depthes = [3, 15, 15, 7, 7]
-    csp_channels = [160, 320, 640, 960, 1280]
+@register_model
+def YOLOV8_X(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
+    csp_channels = [160, 320, 640, 640]
+    csp_depthes = [3, 6, 6, 3]
+    return YOLOV8(**locals(), model_name=kwargs.pop("model_name", "yolov8_x"), **kwargs)
+
+
+@register_model
+def YOLOV8_X6(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
+    csp_channels = [160, 320, 640, 640, 640]
+    csp_depthes = [3, 6, 6, 3, 3]
     features_pick = [-4, -3, -2, -1]
-    fpn_depth = 3
-    ssp_depth = 2
-    use_focus_stem = True
-    csp_use_post = False
-    use_csp_downsample = True
-    use_shortcut_bn = False
-    return YOLOR(**locals(), model_name=kwargs.pop("model_name", "yolor_d6"), **kwargs)
+    paf_parallel_mode = False  # C2
+    return YOLOV8(**locals(), model_name=kwargs.pop("model_name", "yolov8_x6"), **kwargs)
+
+
+""" Classification models """
+
+
+@register_model
+def YOLOV8_N_CLS(input_shape=(640, 640, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    return YOLOV8Backbone(**locals(), model_name=kwargs.pop("model_name", "yolov8_n_cls"), **kwargs)
+
+
+@register_model
+def YOLOV8_S_CLS(input_shape=(640, 640, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    channels = [64, 128, 256, 512]
+    return YOLOV8Backbone(**locals(), model_name=kwargs.pop("model_name", "yolov8_s_cls"), **kwargs)
+
+
+@register_model
+def YOLOV8_M_CLS(input_shape=(640, 640, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    channels = [96, 192, 384, 768]
+    depthes = [2, 4, 4, 2]
+    return YOLOV8Backbone(**locals(), model_name=kwargs.pop("model_name", "yolov8_m_cls"), **kwargs)
+
+
+@register_model
+def YOLOV8_L_CLS(input_shape=(640, 640, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    channels = [128, 256, 512, 1024]
+    depthes = [3, 6, 6, 3]
+    return YOLOV8Backbone(**locals(), model_name=kwargs.pop("model_name", "yolov8_l_cls"), **kwargs)
+
+
+@register_model
+def YOLOV8_X_CLS(input_shape=(640, 640, 3), num_classes=1000, activation="swish", classifier_activation="softmax", pretrained="imagenet", **kwargs):
+    channels = [160, 320, 640, 1280]
+    depthes = [3, 6, 6, 3]
+    return YOLOV8Backbone(**locals(), model_name=kwargs.pop("model_name", "yolov8_x_cls"), **kwargs)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolov7/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov7/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,23 +17,26 @@
 Args:
   backbone: backbone model, could be any model with pyramid stage structure.
       Default None for YOLOV7Backbone.
 """
 
 __tail_doc__ = """  features_pick: specific `layer names` or `pyramid feature indexes` from backbone model.
         Default `[-3, -2, -1]` means using the last 3 pyramid feature output from backbone.
+  regression_len: bbox output len, typical value is 4, for yolov8 reg_max=16 -> regression_len = 16 * 4 == 64.
   anchors_mode: one of ["efficientdet", "anchor_free", "yolor"], controls which anchor to use.
       - efficientdet anchors default settings: use_object_scores=False, num_anchors=9, anchor_scale=4,
           aspect_ratios=[1, 2, 0.5], num_scales=3, grid_zero_start=False.
       - anchor_free default settings: use_object_scores=True, num_anchors=1, anchor_scale=1,
           aspect_ratios=[1], num_scales=1, grid_zero_start=True.
       - yolor default settings: use_object_scores=True, num_anchors=3.
+      - yolov8 default settings: use_object_scores=False, num_anchors=1, anchor_scale=1,
+          aspect_ratios=[1], num_scales=1, grid_zero_start=False.
       Default "yolor".
   num_anchors: number of anchors for a single grid point, should be same with dataset used value.
-      Default "auto" means: anchors_mode=="anchor_free" -> 1, anchors_mode=="yolor" -> 3, else 9.
+      Default "auto" means: anchors_mode=="anchor_free" / "yolov8" -> 1, anchors_mode=="yolor" -> 3, else 9.
   use_object_scores: bollean value if model header output includes `object_scores`.
       Default "auto" means: True if anchors_mode=="anchor_free" or anchors_mode=="yolor", else False.
   input_shape: input shape if backbone is None, else will use input_shape from backbone.
   num_classes: total output classes. Set `0` to disable `classifier` output. Default 80 for COCO.
   activation: activation used in whole model, default `swish`. Default "swish".
   classifier_activation: The activation function to use for classifier output if `num_classes > 0`.
       Set `classifier_activation=None` to return the logits of the "top" layer. Default `sigmoid`.
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolov7/yolov7.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolov7/yolov7.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,10 @@
-import tensorflow as tf
-from tensorflow import keras
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, models, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     batchnorm_with_activation,
     conv2d_no_bias,
     add_pre_post_process,
 )
 from keras_cv_attention_models import model_surgery
@@ -23,97 +24,103 @@
 
 """ Yolov7Backbone """
 BATCH_NORM_EPSILON = 1e-3
 BATCH_NORM_MOMENTUM = 0.97
 
 
 def conv_bn(inputs, output_channel, kernel_size=1, strides=1, activation="swish", name=""):
-    nn = conv2d_no_bias(inputs, output_channel, kernel_size, strides, padding="SAME", name=name)
+    # print(f">>>> {inputs.shape = }, {output_channel = }, {kernel_size = }, {strides = }")
+    nn = conv2d_no_bias(inputs, output_channel, kernel_size, strides, padding="same", name=name)
     return batchnorm_with_activation(nn, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name)
 
 
 def __concat_stack__(inputs, filters, concats=[-1, -3, -5, -6], depth=6, mid_ratio=1.0, out_channels=-1, activation="swish", name=""):
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
     first = conv_bn(inputs, filters, kernel_size=1, strides=1, activation=activation, name=name + "1_")
     second = conv_bn(inputs, filters, kernel_size=1, strides=1, activation=activation, name=name + "2_")
 
     gathered = [first, second]
     mid_filters = int(mid_ratio * filters)
     for id in range(depth - 2):
         nn = conv_bn(gathered[-1], mid_filters, kernel_size=3, strides=1, activation=activation, name=name + "{}_".format(id + 3))
         gathered.append(nn)
-    nn = tf.concat([gathered[ii] for ii in concats], axis=-1)
-    out_channels = out_channels if out_channels > 0 else nn.shape[-1]
+    nn = functional.concat([gathered[ii] for ii in concats], axis=channel_axis)
+    out_channels = out_channels if out_channels > 0 else nn.shape[channel_axis]
     nn = conv_bn(nn, out_channels, kernel_size=1, strides=1, activation=activation, name=name + "out_")
     return nn
 
 
 def concat_stack(inputs, filters, concats=None, depth=6, mid_ratio=1.0, out_channels=-1, use_additional_stack=False, activation="swish", name=""):
     concats = concats if concats is not None else [-(ii + 1) for ii in range(depth)]  # [-1, -2, -3, -4, -5, -6] if None and depth=6
     nn = __concat_stack__(inputs, filters, concats, depth=depth, mid_ratio=mid_ratio, out_channels=out_channels, activation=activation, name=name)
     if use_additional_stack:
         cur_name = name + "another_"
         parallel = __concat_stack__(inputs, filters, concats, depth=depth, mid_ratio=mid_ratio, out_channels=out_channels, activation=activation, name=cur_name)
-        nn = keras.layers.Add()([nn, parallel])
+        nn = layers.Add()([nn, parallel])
     return nn
 
 
 def csp_downsample(inputs, ratio=0.5, activation="swish", name=""):
-    input_channel = inputs.shape[-1]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    input_channel = inputs.shape[channel_axis]
     hidden_ratio, out_ratio = ratio if isinstance(ratio, (list, tuple)) else (ratio, ratio)
     hidden_channel, out_channel = int(input_channel * hidden_ratio), int(input_channel * out_ratio)
-    pool_branch = keras.layers.MaxPool2D(pool_size=2, strides=2, padding="SAME", name=name + "pool")(inputs)
+    pool_branch = layers.MaxPool2D(pool_size=2, strides=2, padding="same", name=name + "pool")(inputs)
     if out_channel == 0:
         nn = pool_branch  # Maxpool only
     else:
         pool_branch = conv_bn(pool_branch, out_channel, kernel_size=1, strides=1, activation=activation, name=name + "pool_")
         conv_branch = conv_bn(inputs, hidden_channel, kernel_size=1, strides=1, activation=activation, name=name + "conv_1_")
         conv_branch = conv_bn(conv_branch, out_channel, kernel_size=3, strides=2, activation=activation, name=name + "conv_2_")
 
-        nn = tf.concat([conv_branch, pool_branch], axis=-1)
+        nn = functional.concat([conv_branch, pool_branch], axis=channel_axis)
     return nn
 
 
 # Almost same with yolor, just supporting YOLOV7_Tiny with depth=1
 def res_spatial_pyramid_pooling(inputs, depth=2, expansion=0.5, pool_sizes=(5, 9, 13), activation="swish", name=""):
-    input_channels = inputs.shape[-1]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    input_channels = inputs.shape[channel_axis]
     hidden_channels = int(input_channels * expansion)
     short = conv_bn(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "short_")
 
     deep = conv_bn(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "pre_1_")
     if depth > 1:  # depth = 1 for yolov7_tiny
         deep = conv_bn(deep, hidden_channels, kernel_size=3, activation=activation, name=name + "pre_2_")
         deep = conv_bn(deep, hidden_channels, kernel_size=1, activation=activation, name=name + "pre_3_")
-    pp = [keras.layers.MaxPooling2D(pool_size=ii, strides=1, padding="SAME")(deep) for ii in pool_sizes]
-    deep = tf.concat([deep, *pp], axis=-1)  # yolov7 SPPCSPC concat, different from yolor
+    pp = [layers.MaxPool2D(pool_size=ii, strides=1, padding="same")(deep) for ii in pool_sizes]
+    deep = functional.concat([deep, *pp], axis=channel_axis)  # yolov7 SPPCSPC concat, different from yolor
     for id in range(depth - 1):  # First one is `pre`
         deep = conv_bn(deep, hidden_channels, kernel_size=1, activation=activation, name=name + "post_{}_".format(id * 2 + 1))
         deep = conv_bn(deep, hidden_channels, kernel_size=3, activation=activation, name=name + "post_{}_".format(id * 2 + 2))
 
     if depth == 1:  # For yolov7_tiny
         deep = conv_bn(deep, hidden_channels, kernel_size=1, activation=activation, name=name + "post_1_")
 
-    out = tf.concat([deep, short], axis=-1)
+    out = functional.concat([deep, short], axis=channel_axis)
     out = conv_bn(out, hidden_channels, kernel_size=1, activation=activation, name=name + "output_")
     return out
 
 
 # Same with yolor
 def focus_stem(inputs, filters, kernel_size=3, strides=1, padding="valid", activation="swish", name=""):
+    is_channels_last = image_data_format() == "channels_last"
+    channel_axis = -1 if is_channels_last else 1
     if padding.lower() == "same":  # Handling odd input_shape
-        inputs = tf.pad(inputs, [[0, 0], [0, 1], [0, 1], [0, 0]])
-        patch_top_left = inputs[:, :-1:2, :-1:2]
-        patch_top_right = inputs[:, :-1:2, 1::2]
-        patch_bottom_left = inputs[:, 1::2, :-1:2]
-        patch_bottom_right = inputs[:, 1::2, 1::2]
+        inputs = functional.pad(inputs, [[0, 0], [0, 1], [0, 1], [0, 0]] if is_channels_last else [[0, 0], [0, 0], [0, 1], [0, 1]])
+        patch_top_left = inputs[:, :-1:2, :-1:2] if is_channels_last else inputs[:, :, :-1:2, :-1:2]
+        patch_top_right = inputs[:, :-1:2, 1::2] if is_channels_last else inputs[:, :, :-1:2, 1::2]
+        patch_bottom_left = inputs[:, 1::2, :-1:2] if image_data_format() == "channels_last" else inputs[:, :, 1::2, :-1:2]
+        patch_bottom_right = inputs[:, 1::2, 1::2] if image_data_format() == "channels_last" else inputs[:, :, 1::2, 1::2]
     else:
-        patch_top_left = inputs[:, ::2, ::2]
-        patch_top_right = inputs[:, ::2, 1::2]
-        patch_bottom_left = inputs[:, 1::2, ::2]
-        patch_bottom_right = inputs[:, 1::2, 1::2]
-    nn = tf.concat([patch_top_left, patch_bottom_left, patch_top_right, patch_bottom_right], axis=-1)
+        patch_top_left = inputs[:, ::2, ::2] if is_channels_last else inputs[:, :, ::2, ::2]
+        patch_top_right = inputs[:, ::2, 1::2] if is_channels_last else inputs[:, :, ::2, 1::2]
+        patch_bottom_left = inputs[:, 1::2, ::2] if is_channels_last else inputs[:, :, 1::2, ::2]
+        patch_bottom_right = inputs[:, 1::2, 1::2] if is_channels_last else inputs[:, :, 1::2, 1::2]
+    nn = functional.concat([patch_top_left, patch_bottom_left, patch_top_right, patch_bottom_right], axis=channel_axis)
     nn = conv_bn(nn, filters, kernel_size=kernel_size, strides=strides, activation=activation, name=name)
     return nn
 
 
 def YOLOV7Backbone(
     channels=[64, 128, 256, 256],
     stack_concats=[-1, -3, -5, -6],
@@ -125,15 +132,16 @@
     csp_downsample_ratios=[0, 0.5, 0.5, 0.5],
     out_features=[-3, -2, -1],
     spp_depth=2,
     input_shape=(512, 512, 3),
     activation="swish",
     model_name="yolov7_backbone",
 ):
-    inputs = keras.layers.Input(input_shape)
+    inputs = layers.Input(backend.align_input_shape_by_image_data_format(input_shape))
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
 
     """ Stem """
     stem_width = stem_width if stem_width > 0 else channels[0]
     if stem_type == "focus":
         nn = focus_stem(inputs, stem_width, activation=activation, name="stem_")
     elif stem_type == "conv1":
         nn = conv_bn(inputs, stem_width, kernel_size=3, strides=2, activation=activation, name="stem_")
@@ -153,58 +161,61 @@
     """ blocks """
     features = [nn]
     for id, (channel, csp_downsample_ratio) in enumerate(zip(channels, csp_downsample_ratios)):
         stack_name = "stack{}_".format(id + 1)
         if isinstance(csp_downsample_ratio, (list, tuple)) or 0 < csp_downsample_ratio <= 1:
             nn = csp_downsample(nn, ratio=csp_downsample_ratio, activation=activation, name=stack_name + "downsample_")
         else:
-            # nn = conv_bn(nn, nn.shape[-1] * 2, kernel_size=3, strides=2, activation=activation, name=stack_name + "downsample_")
-            ds_channels = nn.shape[-1] * 2 if csp_downsample_ratio <= 0 else csp_downsample_ratio
+            # nn = conv_bn(nn, nn.shape[channel_axis] * 2, kernel_size=3, strides=2, activation=activation, name=stack_name + "downsample_")
+            ds_channels = nn.shape[channel_axis] * 2 if csp_downsample_ratio <= 0 else csp_downsample_ratio
             nn = conv_bn(nn, ds_channels, kernel_size=3, strides=2, activation=activation, name=stack_name + "downsample_")
         out_channels = -1 if stack_out_ratio == 1 else int(channel * len(stack_concats) * stack_out_ratio)
         nn = concat_stack(nn, channel, **common_kwargs, out_channels=out_channels, name=stack_name)
 
         if id == len(channels) - 1:
             # add SPPCSPC block if it's the last stack
             nn = res_spatial_pyramid_pooling(nn, depth=spp_depth, activation=activation, name=stack_name + "spp_")
         features.append(nn)
 
     nn = [features[ii] for ii in out_features]
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     return model
 
 
 """ path aggregation fpn, using `concat_stack` instead of `csp_stack` from yolor """
 
 
 def upsample_merge(inputs, hidden_channels, mid_ratio=0.5, concats=None, depth=6, use_additional_stack=False, activation="swish", name=""):
     # print(f">>>> upsample_merge inputs: {[ii.shape for ii in inputs] = }")
-    upsample = conv_bn(inputs[-1], inputs[0].shape[-1], activation=activation, name=name + "up_")
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    upsample = conv_bn(inputs[-1], inputs[0].shape[channel_axis], activation=activation, name=name + "up_")
 
-    # inputs[0] = keras.layers.UpSampling2D(size=(2, 2), interpolation="nearest", name=name + "up")(fpn_out)
-    inputs[-1] = tf.image.resize(upsample, tf.shape(inputs[0])[1:-1], method="nearest")
-    nn = tf.concat(inputs, axis=-1)
-    out_channels = nn.shape[-1] // 2
-    hidden_channels = hidden_channels if hidden_channels > 0 else nn.shape[-1] // 2
+    # inputs[0] = layers.UpSampling2D(size=(2, 2), interpolation="nearest", name=name + "up")(fpn_out)
+    size = functional.shape(inputs[0])[1:-1] if image_data_format() == "channels_last" else functional.shape(inputs[0])[2:]
+    inputs[-1] = functional.resize(upsample, size, method="nearest")
+    nn = functional.concat(inputs, axis=channel_axis)
+    out_channels = nn.shape[channel_axis] // 2
+    hidden_channels = hidden_channels if hidden_channels > 0 else nn.shape[channel_axis] // 2
     nn = concat_stack(nn, hidden_channels, concats, depth, mid_ratio, out_channels, use_additional_stack, activation=activation, name=name)
     return nn
 
 
 def downsample_merge(
     inputs, hidden_channels, mid_ratio=0.5, concats=None, depth=6, csp_downsample_ratio=1, use_additional_stack=False, activation="swish", name=""
 ):
     # print(f">>>> downsample_merge inputs: {[ii.shape for ii in inputs] = }")
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
     if isinstance(csp_downsample_ratio, (list, tuple)) or csp_downsample_ratio > 0:
         inputs[0] = csp_downsample(inputs[0], ratio=csp_downsample_ratio, activation=activation, name=name)
     else:
-        inputs[0] = conv_bn(inputs[0], inputs[-1].shape[-1], kernel_size=3, strides=2, activation=activation, name=name)
+        inputs[0] = conv_bn(inputs[0], inputs[-1].shape[channel_axis], kernel_size=3, strides=2, activation=activation, name=name)
 
-    nn = tf.concat(inputs, axis=-1)
-    out_channels = nn.shape[-1] // 2
-    hidden_channels = hidden_channels if hidden_channels > 0 else nn.shape[-1] // 2
+    nn = functional.concat(inputs, axis=channel_axis)
+    out_channels = nn.shape[channel_axis] // 2
+    hidden_channels = hidden_channels if hidden_channels > 0 else nn.shape[channel_axis] // 2
     nn = concat_stack(nn, hidden_channels, concats, depth, mid_ratio, out_channels, use_additional_stack, activation=activation, name=name)
     return nn
 
 
 def path_aggregation_fpn(
     features, hidden, mid_ratio=0.5, channel_ratio=0.25, concats=None, depth=6, csp_downsample_ratio=1, use_additional_stack=False, activation="swish", name=""
 ):
@@ -213,21 +224,22 @@
     #               v [up 256 -> concat]  ^ [down 512 -> concat]    #                [up 384 -> concat]  [down 512 -> concat]
     # 37: p4 1024 -> 63: p4p5 256 -------> 88: out1 256             # 37: p4 768 --- 59: p4p5 384 ------- 103: out 384
     #               v [up 128 -> concat]  ^ [down 256 -> concat]    #                [up 256 -> concat]  [down 384 -> concat]
     # 24: p3 512 --> 75: p3p4p5 128 ------+--> 75: out0 128         # 28: p3 512 --- 71: p3p4p5 256 -- 93: out 256
     #                                                               #                [up 128 -> concat]  [down 256 -> concat]
     #                                                               # 19: p2 256 --- 83: p2p3p4p5 128 ------> 83: out 128
     # features: [p3, p4, p5]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
     hidden_channels = hidden.copy() if isinstance(hidden, list) else hidden
     upsamples = [features[-1]]
     p_name = "p{}_".format(len(features) + 2)
     # upsamples: [p5], features[:-1][::-1]: [p4, p3] -> [p5, p4p5, p3p4p5]
     for id, ii in enumerate(features[:-1][::-1]):
         cur_p_name = "p{}".format(len(features) + 1 - id)
-        nn = conv_bn(ii, (ii.shape[-1] * channel_ratio), kernel_size=1, activation=activation, name=name + cur_p_name + "_down_")
+        nn = conv_bn(ii, int(ii.shape[channel_axis] * channel_ratio), kernel_size=1, activation=activation, name=name + cur_p_name + "_down_")
         hidden_channel = hidden_channels.pop(0) if isinstance(hidden_channels, list) else hidden_channels
         p_name = cur_p_name + p_name
         nn = upsample_merge([nn, upsamples[-1]], hidden_channel, mid_ratio, concats, depth, use_additional_stack, activation=activation, name=name + p_name)
         upsamples.append(nn)
 
     downsamples = [upsamples[-1]]
     # downsamples: [p3p4p5], upsamples[:-1][::-1]: [p4p5, p5] -> [p3p4p5, p3p4p5 + p4p5, p3p4p5 + p4p5 + p5]
@@ -241,41 +253,55 @@
         downsamples.append(nn)
     return downsamples
 
 
 """ YOLOV7Head, using Reparam Conv block """
 
 
-def yolov7_head_single(inputs, filters, use_reparam_conv_head=True, num_classes=80, num_anchors=3, use_object_scores=True, activation="swish", name=""):
+def yolov7_head_single(
+    inputs, filters, use_reparam_conv_head=True, num_classes=80, regression_len=4, num_anchors=3, use_object_scores=True, activation="swish", name=""
+):
     if use_reparam_conv_head:
         # OREPA_3x3_RepConv
         rep_conv_3 = conv_bn(inputs, filters, 3, activation=None, name=name + "3x3_")
         rep_conv_1 = conv_bn(inputs, filters, 1, activation=None, name=name + "1x1_")
-        nn = keras.layers.Add()([rep_conv_3, rep_conv_1])
+        nn = layers.Add()([rep_conv_3, rep_conv_1])
         nn = activation_by_name(nn, activation=activation, name=name)
     else:
         nn = conv_bn(inputs, filters, 3, activation=activation, name=name + "1_")
 
-    ouput_classes = num_classes + (5 if use_object_scores else 4)  # num_anchors = 3, num_anchors * (80 + 5) = 255
-    nn = keras.layers.Conv2D(ouput_classes * num_anchors, kernel_size=1, name=name + "2_conv")(nn)
+    ouput_classes = num_classes + regression_len + (1 if use_object_scores else 0)  # num_anchors = 3, num_anchors * (80 + 5) = 255
+    nn = layers.Conv2D(ouput_classes * num_anchors, kernel_size=1, name=name + "2_conv")(nn)
+    nn = nn if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(nn)
     # return nn
-    return keras.layers.Reshape([-1, ouput_classes], name=name + "output_reshape")(nn)
+    return layers.Reshape([-1, ouput_classes], name=name + "output_reshape")(nn)
 
 
 def yolov7_head(
-    inputs, use_reparam_conv_head=True, num_classes=80, num_anchors=3, use_object_scores=True, activation="swish", classifier_activation="sigmoid", name=""
+    inputs,
+    use_reparam_conv_head=True,
+    num_classes=80,
+    regression_len=4,
+    num_anchors=3,
+    use_object_scores=True,
+    activation="swish",
+    classifier_activation="sigmoid",
+    name="",
 ):
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
     outputs = []
     for id, input in enumerate(inputs):
         cur_name = name + "{}_".format(id + 1)
-        filters = int(input.shape[-1] * 2)
-        out = yolov7_head_single(input, filters, use_reparam_conv_head, num_classes, num_anchors, use_object_scores, activation=activation, name=cur_name)
+        filters = int(input.shape[channel_axis] * 2)
+        out = yolov7_head_single(
+            input, filters, use_reparam_conv_head, num_classes, regression_len, num_anchors, use_object_scores, activation=activation, name=cur_name
+        )
         outputs.append(out)
     # return outputs
-    outputs = tf.concat(outputs, axis=1)
+    outputs = functional.concat(outputs, axis=1)
     return activation_by_name(outputs, classifier_activation, name="classifier_")
 
 
 """ YOLOV7 models, almost same with yolor """
 
 
 def YOLOV7(
@@ -293,65 +319,75 @@
     fpn_channel_ratio=0.25,
     fpn_stack_concats=None,
     fpn_stack_depth=-1,  # -1 for using same with stack_depth
     fpn_mid_ratio=0.5,
     fpn_csp_downsample_ratio=1,
     use_reparam_conv_head=True,
     features_pick=[-3, -2, -1],  # [Detector parameters]
+    regression_len=4,  # bbox output len, typical value is 4, for yolov8 reg_max=16 -> regression_len = 16 * 4 == 64
     anchors_mode="yolor",
-    num_anchors="auto",  # "auto" means: anchors_mode=="anchor_free" -> 1, anchors_mode=="yolor" -> 3, else 9
+    num_anchors="auto",  # "auto" means: anchors_mode=="anchor_free" / "yolov8" -> 1, anchors_mode=="yolor" -> 3, else 9
     use_object_scores="auto",  # "auto" means: True if anchors_mode=="anchor_free" or anchors_mode=="yolor", else False
     input_shape=(640, 640, 3),
     num_classes=80,
     activation="swish",
     classifier_activation="sigmoid",
     freeze_backbone=False,
     pretrained=None,
     model_name="yolov7",
     pyramid_levels_min=3,  # Init anchors for model prediction.
     anchor_scale="auto",  # Init anchors for model prediction. "auto" means 1 if (anchors_mode=="anchor_free" or anchors_mode=="yolor"), else 4
     rescale_mode="raw01",  # For decode predictions, raw01 means input value in range [0, 1].
     kwargs=None,  # Not using, recieving parameter
 ):
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+
     if backbone is None:
         # Save line width...
         csp_kwargs = {"out_features": features_pick, "spp_depth": spp_depth, "input_shape": input_shape, "activation": activation, "model_name": "backbone"}
         backbone = YOLOV7Backbone(
             csp_channels, stack_concats, stack_depth, stack_out_ratio, use_additional_stack, stem_width, stem_type, csp_downsample_ratios, **csp_kwargs
         )
         features = backbone.outputs
     else:
         if isinstance(features_pick[0], str):
             features = [backbone.get_layer(layer_name) for layer_name in features_pick]
         else:
             features = model_surgery.get_pyramide_feature_layers(backbone)
             features = [features[id] for id in features_pick]
-        print(">>>> features:", {ii.name: ii.output_shape for ii in features})
-        features = [ii.output for ii in features]
+        feature_names, features = model_surgery.align_pyramide_feature_output_by_image_data_format(features)
+        print(">>>> features:", {ii: jj.shape for ii, jj in zip(feature_names, features)})
 
     backbone.trainable = False if freeze_backbone else True
     use_object_scores, num_anchors, anchor_scale = anchors_func.get_anchors_mode_parameters(anchors_mode, use_object_scores, num_anchors, anchor_scale)
     inputs = backbone.inputs[0]
 
     # Save line width...
     fpn_stack_depth = fpn_stack_depth if fpn_stack_depth > 0 else stack_depth
     fpn_kwargs = {"csp_downsample_ratio": fpn_csp_downsample_ratio, "use_additional_stack": use_additional_stack, "activation": activation, "name": "pafpn_"}
     fpn_features = path_aggregation_fpn(features, fpn_hidden_channels, fpn_mid_ratio, fpn_channel_ratio, fpn_stack_concats, fpn_stack_depth, **fpn_kwargs)
 
-    outputs = yolov7_head(fpn_features, use_reparam_conv_head, num_classes, num_anchors, use_object_scores, activation, classifier_activation, name="head_")
-    outputs = keras.layers.Activation("linear", dtype="float32", name="outputs_fp32")(outputs)
-    model = keras.models.Model(inputs, outputs, name=model_name)
+    outputs = yolov7_head(
+        fpn_features, use_reparam_conv_head, num_classes, regression_len, num_anchors, use_object_scores, activation, classifier_activation, name="head_"
+    )
+    outputs = layers.Activation("linear", dtype="float32", name="outputs_fp32")(outputs)
+    model = models.Model(inputs, outputs, name=model_name)
     reload_model_weights(model, PRETRAINED_DICT, "yolov7", pretrained)
 
     pyramid_levels = [pyramid_levels_min, pyramid_levels_min + len(features_pick) - 1]  # -> [3, 5]
-    post_process = eval_func.DecodePredictions(backbone.input_shape[1:], pyramid_levels, anchors_mode, use_object_scores, anchor_scale)
+    post_process = eval_func.DecodePredictions(
+        backbone.input_shape[1:], pyramid_levels, anchors_mode, use_object_scores, anchor_scale, regression_len=regression_len
+    )
     add_pre_post_process(model, rescale_mode=rescale_mode, post_process=post_process)
     return model
 
 
+@register_model
 def YOLOV7_Tiny(
     input_shape=(416, 416, 3),
     freeze_backbone=False,
     num_classes=80,
     backbone=None,
     activation="leaky_relu/0.1",
     classifier_activation="sigmoid",
@@ -373,29 +409,32 @@
     fpn_mid_ratio = 1.0
     fpn_channel_ratio = 0.5
     fpn_csp_downsample_ratio = [0, 0]  # [0, 0] means using conv_bn downsmaple
     use_reparam_conv_head = False
     return YOLOV7(**locals(), model_name=kwargs.pop("model_name", "yolov7_tiny"), **kwargs)
 
 
+@register_model
 def YOLOV7_CSP(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
     return YOLOV7(**locals(), model_name=kwargs.pop("model_name", "yolov7_csp"), **kwargs)
 
 
+@register_model
 def YOLOV7_X(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
     stack_concats = [-1, -3, -5, -7, -8]
     stack_depth = 8
     stem_width = 80
 
     fpn_stack_concats = [-1, -3, -5, -7, -8]
     fpn_mid_ratio = 1.0
     use_reparam_conv_head = False
     return YOLOV7(**locals(), model_name=kwargs.pop("model_name", "yolov7_x"), **kwargs)
 
 
+@register_model
 def YOLOV7_W6(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
     csp_channels = kwargs.pop("csp_channels", [64, 128, 256, 384, 512])
     features_pick = kwargs.pop("features_pick", [-4, -3, -2, -1])
     stem_type = kwargs.pop("stem_type", "focus")
     csp_downsample_ratios = kwargs.pop("csp_downsample_ratios", [128, 256, 512, 768, 1024])  # > 1 value means using conv_bn instead of csp_downsample
     stack_out_ratio = kwargs.pop("stack_out_ratio", 0.5)
 
@@ -404,30 +443,33 @@
     fpn_csp_downsample_ratio = kwargs.pop("fpn_csp_downsample_ratio", 0)
     use_reparam_conv_head = kwargs.pop("use_reparam_conv_head", False)
 
     kwargs.pop("kwargs", None)  # From other YOLOV7_*6 models
     return YOLOV7(**locals(), model_name=kwargs.pop("model_name", "yolov7_w6"), **kwargs)
 
 
+@register_model
 def YOLOV7_E6(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
-    stack_concats = [-1, -3, -5, -7, -8]
-    stack_depth = 8
-    stem_width = 80
-    csp_downsample_ratios = [1, 1, 1, [1, 480 / 640], [1, 640 / 960]]  # different from YOLOV7_W6
+    stack_concats = kwargs.pop("stack_concats", [-1, -3, -5, -7, -8])
+    stack_depth = kwargs.pop("stack_depth", 8)
+    stem_width = kwargs.pop("stem_width", 80)
+    csp_downsample_ratios = kwargs.pop("csp_downsample_ratios", [1, 1, 1, [1, 480 / 640], [1, 640 / 960]])  # different from YOLOV7_W6
 
-    fpn_mid_ratio = 0.5
-    fpn_csp_downsample_ratio = [1, [1, 240 / 320], [1, 320 / 480]]  # different from YOLOV7_W6
+    fpn_mid_ratio = kwargs.pop("fpn_mid_ratio", 0.5)
+    fpn_csp_downsample_ratio = kwargs.pop("fpn_csp_downsample_ratio", [1, [1, 240 / 320], [1, 320 / 480]])  # different from YOLOV7_W6
 
     kwargs.pop("kwargs", None)  # From YOLOV7_E6E / YOLOV7_D6
     return YOLOV7_W6(**locals(), model_name=kwargs.pop("model_name", "yolov7_e6"), **kwargs)
 
 
+@register_model
 def YOLOV7_D6(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
     stack_concats = [-1, -3, -5, -7, -9, -10]
     stack_depth = 10
     stem_width = 96
     return YOLOV7_E6(**locals(), model_name=kwargs.pop("model_name", "yolov7_d6"), **kwargs)
 
 
+@register_model
 def YOLOV7_E6E(input_shape=(1280, 1280, 3), freeze_backbone=False, num_classes=80, backbone=None, classifier_activation="sigmoid", pretrained="coco", **kwargs):
     use_additional_stack = True
     return YOLOV7_E6(**locals(), model_name=kwargs.pop("model_name", "yolov7_e6e"), **kwargs)
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolox/__init__.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolox/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -16,23 +16,26 @@
 Args:
   backbone: backbone model, could be any model with pyramid stage structure.
       Default None for CSPDarknet with depth_mul={depth_mul}, width_mul={width_mul}.
 """
 
 __tail_doc__ = """  features_pick: specific `layer names` or `pyramid feature indexes` from backbone model.
         Default `[-3, -2, -1]` means using the last 3 pyramid feature output from backbone.
+  regression_len: bbox output len, typical value is 4, for yolov8 reg_max=16 -> regression_len = 16 * 4 == 64.
   anchors_mode: one of ["efficientdet", "anchor_free", "yolor"], controls which anchor to use.
       - efficientdet anchors default settings: use_object_scores=False, num_anchors=9, anchor_scale=4,
           aspect_ratios=[1, 2, 0.5], num_scales=3, grid_zero_start=False.
       - anchor_free default settings: use_object_scores=True, num_anchors=1, anchor_scale=1,
           aspect_ratios=[1], num_scales=1, grid_zero_start=True.
       - yolor default settings: use_object_scores=True, num_anchors=3.
+      - yolov8 default settings: use_object_scores=False, num_anchors=1, anchor_scale=1,
+          aspect_ratios=[1], num_scales=1, grid_zero_start=False.
       Default "anchor_free".
   num_anchors: number of anchors for a single grid point, should be same with dataset used value.
-      Default "auto" means: anchors_mode=="anchor_free" -> 1, anchors_mode=="yolor" -> 3, else 9.
+      Default "auto" means: anchors_mode=="anchor_free" / "yolov8" -> 1, anchors_mode=="yolor" -> 3, else 9.
   use_object_scores: bollean value if model header output includes `object_scores`.
       Default "auto" means: True if anchors_mode=="anchor_free" or anchors_mode=="yolor", else False.
   num_classes: total output classes. Set `0` to disable `classifier` output. Default 80 for COCO.
   input_shape: input shape if backbone is None, else will use input_shape from backbone.
   activation: activation used in whole model, default `swish`. Default "swish".
   freeze_backbone: set `True` for `backbone.trainable = False`. Default `False`.
   pretrained: one of `None` (random initialization) or 'coco' (pre-training on COCO).
```

### Comparing `keras-cv-attention-models-1.3.9/keras_cv_attention_models/yolox/yolox.py` & `keras-cv-attention-models-1.4.1/keras_cv_attention_models/yolox/yolox.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,11 @@
-import tensorflow as tf
-from tensorflow import keras
+import math
+from keras_cv_attention_models import backend
+from keras_cv_attention_models.backend import layers, functional, models, initializers, image_data_format
+from keras_cv_attention_models.models import register_model
 from keras_cv_attention_models.attention_layers import (
     activation_by_name,
     batchnorm_with_activation,
     conv2d_no_bias,
     depthwise_conv2d_no_bias,
     add_pre_post_process,
 )
@@ -25,75 +27,79 @@
 BATCH_NORM_EPSILON = 1e-3
 BATCH_NORM_MOMENTUM = 0.97
 
 
 def conv_dw_pw_block(inputs, filters, kernel_size=1, strides=1, use_depthwise_conv=False, activation="swish", name=""):
     nn = inputs
     if use_depthwise_conv:
-        nn = depthwise_conv2d_no_bias(nn, kernel_size, strides, padding="SAME", name=name)
+        nn = depthwise_conv2d_no_bias(nn, kernel_size, strides, padding="same", name=name)
         nn = batchnorm_with_activation(nn, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name + "dw_")
         kernel_size, strides = 1, 1
-    nn = conv2d_no_bias(nn, filters, kernel_size, strides, padding="SAME", name=name)
+    nn = conv2d_no_bias(nn, filters, kernel_size, strides, padding="same", name=name)
     nn = batchnorm_with_activation(nn, activation=activation, epsilon=BATCH_NORM_EPSILON, momentum=BATCH_NORM_MOMENTUM, name=name)
     return nn
 
 
 def csp_block(inputs, expansion=0.5, use_shortcut=True, use_depthwise_conv=False, activation="swish", name=""):
-    input_channels = inputs.shape[-1]
+    input_channels = inputs.shape[-1 if image_data_format() == "channels_last" else 1]
     nn = conv_dw_pw_block(inputs, int(input_channels * expansion), activation=activation, name=name + "1_")
     nn = conv_dw_pw_block(nn, input_channels, kernel_size=3, strides=1, use_depthwise_conv=use_depthwise_conv, activation=activation, name=name + "2_")
     if use_shortcut:
-        nn = keras.layers.Add()([inputs, nn])
+        nn = layers.Add()([inputs, nn])
     return nn
 
 
 def csp_stack(inputs, depth, out_channels=-1, expansion=0.5, use_shortcut=True, use_depthwise_conv=False, activation="swish", name=""):
-    out_channels = inputs.shape[-1] if out_channels == -1 else out_channels
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    out_channels = inputs.shape[channel_axis] if out_channels == -1 else out_channels
     hidden_channels = int(out_channels * expansion)
     short = conv_dw_pw_block(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "short_")
 
     deep = conv_dw_pw_block(inputs, hidden_channels, kernel_size=1, activation=activation, name=name + "deep_")
     for id in range(depth):
         block_name = name + "block{}_".format(id + 1)
         deep = csp_block(deep, 1, use_shortcut=use_shortcut, use_depthwise_conv=use_depthwise_conv, activation=activation, name=block_name)
 
-    out = tf.concat([deep, short], axis=-1)
+    out = functional.concat([deep, short], axis=channel_axis)
     out = conv_dw_pw_block(out, out_channels, kernel_size=1, activation=activation, name=name + "output_")
     return out
 
 
 def spatial_pyramid_pooling(inputs, pool_sizes=(5, 9, 13), activation="swish", name=""):
-    input_channels = inputs.shape[-1]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    input_channels = inputs.shape[channel_axis]
     nn = conv_dw_pw_block(inputs, input_channels // 2, kernel_size=1, activation=activation, name=name + "1_")
-    pp = [keras.layers.MaxPooling2D(pool_size=ii, strides=1, padding="SAME")(nn) for ii in pool_sizes]
-    nn = tf.concat([nn, *pp], axis=-1)
+    pp = [layers.MaxPool2D(pool_size=ii, strides=1, padding="same")(nn) for ii in pool_sizes]
+    nn = functional.concat([nn, *pp], axis=channel_axis)
     nn = conv_dw_pw_block(nn, input_channels, kernel_size=1, activation=activation, name=name + "2_")
     return nn
 
 
 def focus_stem(inputs, filters, kernel_size=3, strides=1, padding="valid", activation="swish", name=""):
+    is_channels_last = image_data_format() == "channels_last"
+    channel_axis = -1 if is_channels_last else 1
     if padding.lower() == "same":  # Handling odd input_shape
-        inputs = tf.pad(inputs, [[0, 0], [0, 1], [0, 1], [0, 0]])
-        patch_top_left = inputs[:, :-1:2, :-1:2]
-        patch_top_right = inputs[:, :-1:2, 1::2]
-        patch_bottom_left = inputs[:, 1::2, :-1:2]
-        patch_bottom_right = inputs[:, 1::2, 1::2]
+        inputs = functional.pad(inputs, [[0, 0], [0, 1], [0, 1], [0, 0]] if is_channels_last else [[0, 0], [0, 0], [0, 1], [0, 1]])
+        patch_top_left = inputs[:, :-1:2, :-1:2] if is_channels_last else inputs[:, :, :-1:2, :-1:2]
+        patch_top_right = inputs[:, :-1:2, 1::2] if is_channels_last else inputs[:, :, :-1:2, 1::2]
+        patch_bottom_left = inputs[:, 1::2, :-1:2] if image_data_format() == "channels_last" else inputs[:, :, 1::2, :-1:2]
+        patch_bottom_right = inputs[:, 1::2, 1::2] if image_data_format() == "channels_last" else inputs[:, :, 1::2, 1::2]
     else:
-        patch_top_left = inputs[:, ::2, ::2]
-        patch_top_right = inputs[:, ::2, 1::2]
-        patch_bottom_left = inputs[:, 1::2, ::2]
-        patch_bottom_right = inputs[:, 1::2, 1::2]
-    nn = tf.concat([patch_top_left, patch_bottom_left, patch_top_right, patch_bottom_right], axis=-1)
+        patch_top_left = inputs[:, ::2, ::2] if is_channels_last else inputs[:, :, ::2, ::2]
+        patch_top_right = inputs[:, ::2, 1::2] if is_channels_last else inputs[:, :, ::2, 1::2]
+        patch_bottom_left = inputs[:, 1::2, ::2] if is_channels_last else inputs[:, :, 1::2, ::2]
+        patch_bottom_right = inputs[:, 1::2, 1::2] if is_channels_last else inputs[:, :, 1::2, 1::2]
+    nn = functional.concat([patch_top_left, patch_bottom_left, patch_top_right, patch_bottom_right], axis=channel_axis)
     nn = conv_dw_pw_block(nn, filters, kernel_size=kernel_size, strides=strides, activation=activation, name=name)
     return nn
 
 
 def CSPDarknet(width_mul=1, depth_mul=1, out_features=[-3, -2, -1], use_depthwise_conv=False, input_shape=(512, 512, 3), activation="swish", model_name=""):
     base_channels, base_depth = int(width_mul * 64), max(round(depth_mul * 3), 1)
-    inputs = keras.layers.Input(input_shape)
+    inputs = layers.Input(backend.align_input_shape_by_image_data_format(input_shape))
 
     """ Stem """
     nn = focus_stem(inputs, base_channels, activation=activation, name="stem_")
     features = [nn]
 
     """ dark blocks """
     depthes = [base_depth, base_depth * 3, base_depth * 3, base_depth]
@@ -106,47 +112,50 @@
         if use_spp:
             nn = spatial_pyramid_pooling(nn, activation=activation, name=stack_name + "spp_")
         # nn = SPPBottleneck(base_channels * 16, base_channels * 16, activation=act)
         nn = csp_stack(nn, depth, use_shortcut=use_shortcut, use_depthwise_conv=use_depthwise_conv, activation=activation, name=stack_name)
         features.append(nn)
 
     nn = [features[ii] for ii in out_features]
-    model = keras.models.Model(inputs, nn, name=model_name)
+    model = models.Model(inputs, nn, name=model_name)
     return model
 
 
 """ path aggregation fpn """
 
 
 def upsample_merge(inputs, csp_depth, use_depthwise_conv=False, activation="swish", name=""):
     # print(f">>>> upsample_merge inputs: {[ii.shape for ii in inputs] = }")
-    target_channel = inputs[-1].shape[-1]
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    target_channel = inputs[-1].shape[channel_axis]
     fpn_out = conv_dw_pw_block(inputs[0], target_channel, activation=activation, name=name + "fpn_")
 
-    # inputs[0] = keras.layers.UpSampling2D(size=(2, 2), interpolation="nearest", name=name + "up")(fpn_out)
-    inputs[0] = tf.image.resize(fpn_out, tf.shape(inputs[-1])[1:-1], method="nearest")
-    nn = tf.concat(inputs, axis=-1)
+    # inputs[0] = layers.UpSampling2D(size=(2, 2), interpolation="nearest", name=name + "up")(fpn_out)
+    size = functional.shape(inputs[-1])[1:-1] if image_data_format() == "channels_last" else functional.shape(inputs[-1])[2:]
+    inputs[0] = functional.resize(fpn_out, size, method="nearest")
+    nn = functional.concat(inputs, axis=channel_axis)
     nn = csp_stack(nn, csp_depth, target_channel, 0.5, False, use_depthwise_conv, activation=activation, name=name)
     return fpn_out, nn
 
 
 def downsample_merge(inputs, csp_depth, use_depthwise_conv=False, activation="swish", name=""):
     # print(f">>>> downsample_merge inputs: {[ii.shape for ii in inputs] = }")
-    inputs[0] = conv_dw_pw_block(inputs[0], inputs[-1].shape[-1], 3, 2, use_depthwise_conv, activation=activation, name=name + "down_")
-    nn = tf.concat(inputs, axis=-1)
-    nn = csp_stack(nn, csp_depth, nn.shape[-1], 0.5, False, use_depthwise_conv, activation=activation, name=name)
+    channel_axis = -1 if image_data_format() == "channels_last" else 1
+    inputs[0] = conv_dw_pw_block(inputs[0], inputs[-1].shape[channel_axis], 3, 2, use_depthwise_conv, activation=activation, name=name + "down_")
+    nn = functional.concat(inputs, axis=channel_axis)
+    nn = csp_stack(nn, csp_depth, nn.shape[channel_axis], 0.5, False, use_depthwise_conv, activation=activation, name=name)
     return nn
 
 
 def path_aggregation_fpn(features, depth_mul=1, use_depthwise_conv=False, activation="swish", name=""):
-    # p5 > fpn_out0 > pan_out0
-    #                               
-    # p4 > f_out0 > fpn_out1 > pan_out1
-    #                               
-    # p3 > pan_out2 
+    # p5 -> fpn_out0 -------------+-> pan_out0
+    #       v                     ^
+    # p4 -> f_out0 -> fpn_out1 ---+-> pan_out1
+    #                 v           ^
+    # p3 -----------> pan_out2 ---+-> pan_out2
     csp_depth = max(round(depth_mul * 3), 1)
     p3, p4, p5 = features  # p3: [64, 64, 256], p4: [32, 32, 512], p5: [16, 16, 1024]
     # fpn_out0: [16, 16, 512], f_out0: [32, 32, 512]
     fpn_out0, f_out0 = upsample_merge([p5, p4], csp_depth, use_depthwise_conv=use_depthwise_conv, activation=activation, name=name + "c3p4_")
     # fpn_out1: [32, 32, 256], pan_out2: [64, 64, 256]
     fpn_out1, pan_out2 = upsample_merge([f_out0, p3], csp_depth, use_depthwise_conv=use_depthwise_conv, activation=activation, name=name + "c3p3_")
     # pan_out1: [32, 32, 512]
@@ -155,124 +164,149 @@
     pan_out0 = downsample_merge([pan_out1, fpn_out0], csp_depth, use_depthwise_conv=use_depthwise_conv, activation=activation, name=name + "c3n4_")
     return [pan_out2, pan_out1, pan_out0]
 
 
 """ YOLOXHead """
 
 
-def yolox_head_single(inputs, out_channels, num_classes=80, num_anchors=1, use_depthwise_conv=False, use_object_scores=True, activation="swish", name=""):
-    bias_init = tf.constant_initializer(-tf.math.log((1 - 0.01) / 0.01).numpy())
+def yolox_head_single(
+    inputs, out_channels, num_classes=80, regression_len=4, num_anchors=1, use_depthwise_conv=False, use_object_scores=True, activation="swish", name=""
+):
+    bias_init = initializers.constant(-math.log((1 - 0.01) / 0.01))
 
     # stem
     stem = conv_dw_pw_block(inputs, out_channels, activation=activation, name=name + "stem_")
 
     # cls_convs, cls_preds
     cls_nn = conv_dw_pw_block(stem, out_channels, kernel_size=3, use_depthwise_conv=use_depthwise_conv, activation=activation, name=name + "cls_1_")
     cls_nn = conv_dw_pw_block(cls_nn, out_channels, kernel_size=3, use_depthwise_conv=use_depthwise_conv, activation=activation, name=name + "cls_2_")
-    cls_out = keras.layers.Conv2D(num_classes * num_anchors, kernel_size=1, bias_initializer=bias_init, name=name + "class_out")(cls_nn)
+    cls_out = layers.Conv2D(num_classes * num_anchors, kernel_size=1, bias_initializer=bias_init, name=name + "class_out")(cls_nn)
     cls_out = activation_by_name(cls_out, "sigmoid", name=name + "class_out_")
-    cls_out = keras.layers.Reshape([-1, num_classes], name=name + "class_out_reshape")(cls_out)
+    cls_out = cls_out if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(cls_out)
+    cls_out = layers.Reshape([-1, num_classes], name=name + "class_out_reshape")(cls_out)
 
     # reg_convs, reg_preds
     reg_nn = conv_dw_pw_block(stem, out_channels, kernel_size=3, use_depthwise_conv=use_depthwise_conv, activation=activation, name=name + "reg_1_")
     reg_nn = conv_dw_pw_block(reg_nn, out_channels, kernel_size=3, use_depthwise_conv=use_depthwise_conv, activation=activation, name=name + "reg_2_")
-    reg_out = keras.layers.Conv2D(4 * num_anchors, kernel_size=1, name=name + "regression_out")(reg_nn)
-    reg_out = keras.layers.Reshape([-1, 4], name=name + "regression_out_reshape")(reg_out)
+    reg_out = layers.Conv2D(regression_len * num_anchors, kernel_size=1, name=name + "regression_out")(reg_nn)
+    reg_out = reg_out if image_data_format() == "channels_last" else layers.Permute([2, 3, 1])(reg_out)
+    reg_out = layers.Reshape([-1, regression_len], name=name + "regression_out_reshape")(reg_out)
 
     # obj_preds
     if use_object_scores:
-        obj_out = keras.layers.Conv2D(1 * num_anchors, kernel_size=1, bias_initializer=bias_init, name=name + "object_out")(reg_nn)
+        obj_out = layers.Conv2D(1 * num_anchors, kernel_size=1, bias_initializer=bias_init, name=name + "object_out")(reg_nn)
         obj_out = activation_by_name(obj_out, "sigmoid", name=name + "object_out_")
-        obj_out = keras.layers.Reshape([-1, 1], name=name + "object_out_reshape")(obj_out)
-        return tf.concat([reg_out, cls_out, obj_out], axis=-1)
+        obj_out = obj_out if image_data_format() == "channels_last" or num_anchors == 1 else layers.Permute([2, 3, 1])(obj_out)
+        obj_out = layers.Reshape([-1, 1], name=name + "object_out_reshape")(obj_out)
+        return functional.concat([reg_out, cls_out, obj_out], axis=-1)
     else:
-        return tf.concat([reg_out, cls_out], axis=-1)
+        return functional.concat([reg_out, cls_out], axis=-1)
 
 
-def yolox_head(inputs, width_mul=1.0, num_classes=80, num_anchors=1, use_depthwise_conv=False, use_object_scores=True, activation="swish", name=""):
+def yolox_head(
+    inputs, width_mul=1.0, num_classes=80, regression_len=4, num_anchors=1, use_depthwise_conv=False, use_object_scores=True, activation="swish", name=""
+):
     out_channel = int(256 * width_mul)
     outputs = []
     for id, input in enumerate(inputs):
         cur_name = name + "{}_".format(id + 1)
-        out = yolox_head_single(input, out_channel, num_classes, num_anchors, use_depthwise_conv, use_object_scores, activation=activation, name=cur_name)
+        out = yolox_head_single(
+            input, out_channel, num_classes, regression_len, num_anchors, use_depthwise_conv, use_object_scores, activation=activation, name=cur_name
+        )
         outputs.append(out)
-    # outputs = tf.concat([keras.layers.Reshape([-1, ii.shape[-1]])(ii) for ii in outputs], axis=1)
-    outputs = tf.concat(outputs, axis=1)
+    # outputs = functional.concat([layers.Reshape([-1, ii.shape[-1]])(ii) for ii in outputs], axis=1)
+    outputs = functional.concat(outputs, axis=1)
     return outputs
 
 
 """ YOLOX models """
 
 
 def YOLOX(
     backbone=None,
     features_pick=[-3, -2, -1],
     depth_mul=1,
     width_mul=-1,  # -1 means: `min([ii.shape[-1] for ii in features]) / 256` for custom backbones.
     use_depthwise_conv=False,
+    regression_len=4,  # bbox output len, typical value is 4, for yolov8 reg_max=16 -> regression_len = 16 * 4 == 64
     anchors_mode="anchor_free",
     num_anchors="auto",  # "auto" means: anchors_mode=="anchor_free" -> 1, anchors_mode=="yolor" -> 3, else 9
     use_object_scores="auto",  # "auto" means: True if anchors_mode=="anchor_free" or anchors_mode=="yolor", else False
     input_shape=(640, 640, 3),
     num_classes=80,
     activation="swish",
     freeze_backbone=False,
     pretrained=None,
     model_name="yolox",
     pyramid_levels_min=3,  # Init anchors for model prediction.
     anchor_scale="auto",  # Init anchors for model prediction. "auto" means 1 if (anchors_mode=="anchor_free" or anchors_mode=="yolor"), else 4
     rescale_mode="raw",  # For decode predictions, raw means input value in range [0, 255].
     kwargs=None,  # Not using, recieving parameter
 ):
+    # Regard input_shape as force using original shape if len(input_shape) == 4,
+    # else assume channel dimension is the one with min value in input_shape, and put it first or last regarding image_data_format
+    input_shape = backend.align_input_shape_by_image_data_format(input_shape)
+
     if backbone is None:
         width_mul = width_mul if width_mul > 0 else 1
         backbone = CSPDarknet(width_mul, depth_mul, features_pick, use_depthwise_conv, input_shape, activation=activation, model_name="darknet")
         features = backbone.outputs
     else:
         if isinstance(features_pick[0], str):
             features = [backbone.get_layer(layer_name) for layer_name in features_pick]
         else:
             features = model_surgery.get_pyramide_feature_layers(backbone)
             features = [features[id] for id in features_pick]
-        print(">>>> features:", {ii.name: ii.output_shape for ii in features})
-        features = [ii.output for ii in features]
+
+        feature_names, features = model_surgery.align_pyramide_feature_output_by_image_data_format(features)
+        print(">>>> features:", {ii: jj.shape for ii, jj in zip(feature_names, features)})
         width_mul = width_mul if width_mul > 0 else min([ii.shape[-1] for ii in features]) / 256
         print(">>>> width_mul:", width_mul)
 
     backbone.trainable = False if freeze_backbone else True
     use_object_scores, num_anchors, anchor_scale = anchors_func.get_anchors_mode_parameters(anchors_mode, use_object_scores, num_anchors, anchor_scale)
     inputs = backbone.inputs[0]
 
     fpn_features = path_aggregation_fpn(features, depth_mul=depth_mul, use_depthwise_conv=use_depthwise_conv, activation=activation, name="pafpn_")
-    outputs = yolox_head(fpn_features, width_mul, num_classes, num_anchors, use_depthwise_conv, use_object_scores, activation=activation, name="head_")
-    outputs = keras.layers.Activation("linear", dtype="float32", name="outputs_fp32")(outputs)
-    model = keras.models.Model(inputs, outputs, name=model_name)
+    outputs = yolox_head(
+        fpn_features, width_mul, num_classes, regression_len, num_anchors, use_depthwise_conv, use_object_scores, activation=activation, name="head_"
+    )
+    outputs = layers.Activation("linear", dtype="float32", name="outputs_fp32")(outputs)
+    model = models.Model(inputs, outputs, name=model_name)
     reload_model_weights(model, PRETRAINED_DICT, "yolox", pretrained)
 
     pyramid_levels = [pyramid_levels_min, pyramid_levels_min + len(features_pick) - 1]  # -> [3, 5]
-    post_process = eval_func.DecodePredictions(backbone.input_shape[1:], pyramid_levels, anchors_mode, use_object_scores, anchor_scale)
+    post_process = eval_func.DecodePredictions(
+        backbone.input_shape[1:], pyramid_levels, anchors_mode, use_object_scores, anchor_scale, regression_len=regression_len
+    )
     add_pre_post_process(model, rescale_mode=rescale_mode, post_process=post_process)
     return model
 
 
+@register_model
 def YOLOXNano(input_shape=(416, 416, 3), freeze_backbone=False, num_classes=80, backbone=None, activation="swish", pretrained="coco", **kwargs):
     return YOLOX(**locals(), depth_mul=0.33, width_mul=0.25, use_depthwise_conv=True, model_name=kwargs.pop("model_name", "yolox_nano"), **kwargs)
 
 
+@register_model
 def YOLOXTiny(input_shape=(416, 416, 3), freeze_backbone=False, num_classes=80, backbone=None, activation="swish", pretrained="coco", **kwargs):
     return YOLOX(**locals(), depth_mul=0.33, width_mul=0.375, model_name=kwargs.pop("model_name", "yolox_tiny"), **kwargs)
 
 
+@register_model
 def YOLOXS(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, activation="swish", pretrained="coco", **kwargs):
     return YOLOX(**locals(), depth_mul=0.33, width_mul=0.5, model_name=kwargs.pop("model_name", "yolox_s"), **kwargs)
 
 
+@register_model
 def YOLOXM(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, activation="swish", pretrained="coco", **kwargs):
     return YOLOX(**locals(), depth_mul=0.67, width_mul=0.75, model_name=kwargs.pop("model_name", "yolox_m"), **kwargs)
 
 
+@register_model
 def YOLOXL(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, activation="swish", pretrained="coco", **kwargs):
     return YOLOX(**locals(), depth_mul=1.0, width_mul=1.0, model_name=kwargs.pop("model_name", "yolox_l"), **kwargs)
 
 
+@register_model
 def YOLOXX(input_shape=(640, 640, 3), freeze_backbone=False, num_classes=80, backbone=None, activation="swish", pretrained="coco", **kwargs):
     return YOLOX(**locals(), depth_mul=1.33, width_mul=1.25, model_name=kwargs.pop("model_name", "yolox_x"), **kwargs)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `keras-cv-attention-models-1.3.9/setup.py` & `keras-cv-attention-models-1.4.1/setup.py`

 * *Files 10% similar despite different names*

```diff
@@ -5,25 +5,28 @@
 from os import path
 
 here = path.abspath(path.dirname(__file__))
 
 # Get the long description from the README file
 with open(path.join(here, "README.md"), encoding="utf-8") as f:
     long_description = f.read()
+long_description = long_description.replace(
+    "](keras_cv_attention_models", "](https://github.com/leondgarse/keras_cv_attention_models/tree/main/keras_cv_attention_models"
+)
 
 exec(open("keras_cv_attention_models/version.py").read())
 setup(
     name="keras-cv-attention-models",
     version=__version__,
     description="Tensorflow keras computer vision attention models. Alias kecam. https://github.com/leondgarse/keras_cv_attention_models",
     long_description=long_description,
     long_description_content_type="text/markdown",
     url="https://github.com/leondgarse/keras_cv_attention_models",
     author="Leondgarse",
-    author_email="leondgarse@google.com",
+    author_email="leondgarse@gmail.com",
     classifiers=[
         # How mature is this project? Common values are
         #   3 - Alpha
         #   4 - Beta
         #   5 - Production/Stable
         "Development Status :: 3 - Alpha",
         "Intended Audience :: Developers",
@@ -36,18 +39,22 @@
         "Topic :: Scientific/Engineering :: Artificial Intelligence",
         "Topic :: Software Development",
         "Topic :: Software Development :: Libraries",
         "Topic :: Software Development :: Libraries :: Python Modules",
     ],
     # Note that this is a string of words separated by whitespace, not a list.
     keywords="tensorflow keras cv attention pretrained models kecam",
-    packages=find_packages(exclude=["tests"]),
+    packages=find_packages(exclude=["tests"]) + ["keras_cv_attention_models.pytorch_backend"],
     include_package_data=True,
     install_requires=[
-        "tensorflow-macos;platform_system=='Darwin'",
+        "pillow",
+        "tqdm",
+        "ftfy",  # required for language models
+        "regex",  # required for language models
+        # "tensorflow-macos;platform_system=='Darwin'",  # [???]
         "tensorflow;platform_system!='Darwin'",
-        "tensorflow-addons;platform_machine!='aarch64' and platform_machine!='aarch32'",
-        "tensorflow-datasets"
+        # "tensorflow-addons;platform_machine!='aarch64' and platform_machine!='aarch32'",  # [deprecated]
+        "tensorflow-datasets;platform_machine!='aarch64' and platform_machine!='aarch32'",  # >4.7.0 needs dm-tree, failed on arm, just skip
     ],
     python_requires=">=3.6",
     license="Apache 2.0",
 )
```

