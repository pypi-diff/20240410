# Comparing `tmp/lamini-2.1.4a8-118-py3-none-any.whl.zip` & `tmp/lamini-2.1.5a2-119-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,43 @@
-Zip file size: 44675 bytes, number of entries: 40
--rw-r--r--  2.0 unx     1362 b- defN 24-Mar-26 19:23 lamini/__init__.py
--rw-r--r--  2.0 unx     2200 b- defN 24-Mar-26 19:23 lamini/api/classifier.py
--rw-r--r--  2.0 unx     1129 b- defN 24-Mar-26 19:23 lamini/api/embedding.py
--rw-r--r--  2.0 unx    12272 b- defN 24-Mar-26 19:23 lamini/api/lamini.py
--rw-r--r--  2.0 unx     2369 b- defN 24-Mar-26 19:23 lamini/api/lamini_config.py
--rw-r--r--  2.0 unx     2518 b- defN 24-Mar-26 19:23 lamini/api/precise_trainer.py
--rw-r--r--  2.0 unx     6208 b- defN 24-Mar-26 19:23 lamini/api/rest_requests.py
--rw-r--r--  2.0 unx     6732 b- defN 24-Mar-26 19:23 lamini/api/streaming_completion.py
--rw-r--r--  2.0 unx     1547 b- defN 24-Mar-26 19:23 lamini/api/synchronize.py
--rw-r--r--  2.0 unx     6629 b- defN 24-Mar-26 19:23 lamini/api/train.py
--rw-r--r--  2.0 unx     4051 b- defN 24-Mar-26 19:23 lamini/api/utils/async_inference_queue.py
--rw-r--r--  2.0 unx     5376 b- defN 24-Mar-26 19:23 lamini/api/utils/async_inference_queue_3_10.py
--rw-r--r--  2.0 unx     1559 b- defN 24-Mar-26 19:23 lamini/api/utils/base_async_inference_queue.py
--rw-r--r--  2.0 unx     1343 b- defN 24-Mar-26 19:23 lamini/api/utils/completion.py
--rw-r--r--  2.0 unx     2865 b- defN 24-Mar-26 19:23 lamini/api/utils/process_batch.py
--rw-r--r--  2.0 unx     5959 b- defN 24-Mar-26 19:23 lamini/api/utils/reservations.py
--rw-r--r--  2.0 unx      413 b- defN 24-Mar-26 19:23 lamini/api/utils/shutdown.py
--rw-r--r--  2.0 unx     1393 b- defN 24-Mar-26 19:23 lamini/api/utils/upload_client.py
--rw-r--r--  2.0 unx    23827 b- defN 24-Mar-26 19:23 lamini/classify/llama_classifier.py
--rw-r--r--  2.0 unx      896 b- defN 24-Mar-26 19:23 lamini/error/error.py
--rw-r--r--  2.0 unx     1273 b- defN 24-Mar-26 19:23 lamini/generation/base_generation_queue.py
--rw-r--r--  2.0 unx      594 b- defN 24-Mar-26 19:23 lamini/generation/base_node_object.py
--rw-r--r--  2.0 unx      797 b- defN 24-Mar-26 19:23 lamini/generation/base_prompt_object.py
--rw-r--r--  2.0 unx     1418 b- defN 24-Mar-26 19:23 lamini/generation/embedding_node.py
--rw-r--r--  2.0 unx     3815 b- defN 24-Mar-26 19:23 lamini/generation/generation_node.py
--rw-r--r--  2.0 unx     3807 b- defN 24-Mar-26 19:23 lamini/generation/generation_pipeline.py
--rw-r--r--  2.0 unx     6017 b- defN 24-Mar-26 19:23 lamini/generation/generation_queue_3_10.py
--rw-r--r--  2.0 unx      965 b- defN 24-Mar-26 19:23 lamini/generation/modify_node.py
--rw-r--r--  2.0 unx     3448 b- defN 24-Mar-26 19:23 lamini/generation/process_generation_batch.py
--rw-r--r--  2.0 unx     1390 b- defN 24-Mar-26 19:23 lamini/generation/split_response_node.py
--rw-r--r--  2.0 unx     8451 b- defN 24-Mar-26 19:23 lamini/runners/base_runner.py
--rw-r--r--  2.0 unx      720 b- defN 24-Mar-26 19:23 lamini/runners/basic_model_runner.py
--rw-r--r--  2.0 unx     3062 b- defN 24-Mar-26 19:23 lamini/runners/llama_v2_runner.py
--rw-r--r--  2.0 unx     2625 b- defN 24-Mar-26 19:23 lamini/runners/mistral_runner.py
--rw-r--r--  2.0 unx       21 b- defN 24-Mar-26 19:23 llama/__init__.py
--rw-r--r--  2.0 unx    11340 b- defN 24-Mar-26 19:23 lamini-2.1.4a8.dist-info/LICENSE
--rw-r--r--  2.0 unx     1484 b- defN 24-Mar-26 19:23 lamini-2.1.4a8.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Mar-26 19:23 lamini-2.1.4a8.dist-info/WHEEL
--rw-r--r--  2.0 unx       13 b- defN 24-Mar-26 19:23 lamini-2.1.4a8.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3507 b- defN 24-Mar-26 19:23 lamini-2.1.4a8.dist-info/RECORD
-40 files, 145487 bytes uncompressed, 39033 bytes compressed:  73.2%
+Zip file size: 45886 bytes, number of entries: 41
+-rw-r--r--  2.0 unx     1412 b- defN 24-Apr-09 20:32 lamini/__init__.py
+-rw-r--r--  2.0 unx     2200 b- defN 24-Apr-09 20:25 lamini/api/classifier.py
+-rw-r--r--  2.0 unx     1129 b- defN 24-Apr-09 20:25 lamini/api/embedding.py
+-rw-r--r--  2.0 unx    12631 b- defN 24-Apr-09 20:25 lamini/api/lamini.py
+-rw-r--r--  2.0 unx     2369 b- defN 24-Apr-09 20:25 lamini/api/lamini_config.py
+-rw-r--r--  2.0 unx     2518 b- defN 24-Apr-09 20:25 lamini/api/precise_trainer.py
+-rw-r--r--  2.0 unx     6208 b- defN 24-Apr-09 20:25 lamini/api/rest_requests.py
+-rw-r--r--  2.0 unx     6732 b- defN 24-Apr-09 20:25 lamini/api/streaming_completion.py
+-rw-r--r--  2.0 unx     1547 b- defN 24-Apr-09 20:25 lamini/api/synchronize.py
+-rw-r--r--  2.0 unx     6617 b- defN 24-Apr-09 20:25 lamini/api/train.py
+-rw-r--r--  2.0 unx     4605 b- defN 24-Apr-09 20:25 lamini/api/utils/async_inference_queue.py
+-rw-r--r--  2.0 unx     5930 b- defN 24-Apr-09 20:25 lamini/api/utils/async_inference_queue_3_10.py
+-rw-r--r--  2.0 unx     1494 b- defN 24-Apr-09 20:25 lamini/api/utils/base_async_inference_queue.py
+-rw-r--r--  2.0 unx     1343 b- defN 24-Apr-09 20:25 lamini/api/utils/completion.py
+-rw-r--r--  2.0 unx     2865 b- defN 24-Apr-09 20:25 lamini/api/utils/process_batch.py
+-rw-r--r--  2.0 unx     5959 b- defN 24-Apr-09 20:25 lamini/api/utils/reservations.py
+-rw-r--r--  2.0 unx      413 b- defN 24-Apr-09 20:25 lamini/api/utils/shutdown.py
+-rw-r--r--  2.0 unx     1393 b- defN 24-Apr-09 20:25 lamini/api/utils/upload_client.py
+-rw-r--r--  2.0 unx    23827 b- defN 24-Apr-09 20:25 lamini/classify/llama_classifier.py
+-rw-r--r--  2.0 unx      896 b- defN 24-Apr-09 20:25 lamini/error/error.py
+-rw-r--r--  2.0 unx     1273 b- defN 24-Apr-09 20:25 lamini/generation/base_generation_queue.py
+-rw-r--r--  2.0 unx      594 b- defN 24-Apr-09 20:25 lamini/generation/base_node_object.py
+-rw-r--r--  2.0 unx      797 b- defN 24-Apr-09 20:25 lamini/generation/base_prompt_object.py
+-rw-r--r--  2.0 unx     1418 b- defN 24-Apr-09 20:25 lamini/generation/embedding_node.py
+-rw-r--r--  2.0 unx     5101 b- defN 24-Apr-09 20:25 lamini/generation/generation_node.py
+-rw-r--r--  2.0 unx     3868 b- defN 24-Apr-09 20:25 lamini/generation/generation_pipeline.py
+-rw-r--r--  2.0 unx     6615 b- defN 24-Apr-09 20:25 lamini/generation/generation_queue_3_10.py
+-rw-r--r--  2.0 unx      965 b- defN 24-Apr-09 20:25 lamini/generation/modify_node.py
+-rw-r--r--  2.0 unx     3448 b- defN 24-Apr-09 20:25 lamini/generation/process_generation_batch.py
+-rw-r--r--  2.0 unx     1390 b- defN 24-Apr-09 20:25 lamini/generation/split_response_node.py
+-rw-r--r--  2.0 unx      557 b- defN 24-Apr-09 20:25 lamini/generation/token_optimizer.py
+-rw-r--r--  2.0 unx     9354 b- defN 24-Apr-09 20:25 lamini/runners/base_runner.py
+-rw-r--r--  2.0 unx      720 b- defN 24-Apr-09 20:25 lamini/runners/basic_model_runner.py
+-rw-r--r--  2.0 unx     3062 b- defN 24-Apr-09 20:25 lamini/runners/llama_v2_runner.py
+-rw-r--r--  2.0 unx     2625 b- defN 24-Apr-09 20:25 lamini/runners/mistral_runner.py
+-rw-r--r--  2.0 unx       21 b- defN 24-Apr-09 20:25 llama/__init__.py
+-rw-r--r--  2.0 unx    11340 b- defN 24-Apr-09 20:32 lamini-2.1.5a2.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1507 b- defN 24-Apr-09 20:32 lamini-2.1.5a2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-09 20:32 lamini-2.1.5a2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       13 b- defN 24-Apr-09 20:32 lamini-2.1.5a2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3599 b- defN 24-Apr-09 20:32 lamini-2.1.5a2.dist-info/RECORD
+41 files, 150447 bytes uncompressed, 40096 bytes compressed:  73.3%
```

## zipnote {}

```diff
@@ -84,14 +84,17 @@
 
 Filename: lamini/generation/process_generation_batch.py
 Comment: 
 
 Filename: lamini/generation/split_response_node.py
 Comment: 
 
+Filename: lamini/generation/token_optimizer.py
+Comment: 
+
 Filename: lamini/runners/base_runner.py
 Comment: 
 
 Filename: lamini/runners/basic_model_runner.py
 Comment: 
 
 Filename: lamini/runners/llama_v2_runner.py
@@ -99,23 +102,23 @@
 
 Filename: lamini/runners/mistral_runner.py
 Comment: 
 
 Filename: llama/__init__.py
 Comment: 
 
-Filename: lamini-2.1.4a8.dist-info/LICENSE
+Filename: lamini-2.1.5a2.dist-info/LICENSE
 Comment: 
 
-Filename: lamini-2.1.4a8.dist-info/METADATA
+Filename: lamini-2.1.5a2.dist-info/METADATA
 Comment: 
 
-Filename: lamini-2.1.4a8.dist-info/WHEEL
+Filename: lamini-2.1.5a2.dist-info/WHEEL
 Comment: 
 
-Filename: lamini-2.1.4a8.dist-info/top_level.txt
+Filename: lamini-2.1.5a2.dist-info/top_level.txt
 Comment: 
 
-Filename: lamini-2.1.4a8.dist-info/RECORD
+Filename: lamini-2.1.5a2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## lamini/__init__.py

```diff
@@ -2,14 +2,15 @@
 
 # isort: off
 from lamini.error import error
 
 from lamini.runners.llama_v2_runner import LlamaV2Runner
 from lamini.runners.basic_model_runner import BasicModelRunner
 from lamini.runners.mistral_runner import MistralRunner
+from lamini.evaluators.benchmark import Benchmark
 from lamini.api.lamini import Lamini
 from lamini.classify.llama_classifier import LaminiClassifier, BinaryLaminiClassifier
 from lamini.api.classifier import Classifier
 from lamini.api.embedding import Embedding
 from lamini.generation.generation_node import GenerationNode
 from lamini.generation.generation_pipeline import GenerationPipeline
 from lamini.generation.base_prompt_object import PromptObject
```

## lamini/api/lamini.py

```diff
@@ -8,19 +8,16 @@
 import jsonlines
 import pandas as pd
 from lamini.api.lamini_config import get_config
 from lamini.api.synchronize import sync
 from lamini.api.train import Train
 from lamini.api.utils.async_inference_queue import AsyncInferenceQueue
 from lamini.api.utils.completion import Completion
-from lamini.api.utils.upload_client import (
-    get_dataset_name,
-    upload_to_blob,
-    upload_to_local,
-)
+from lamini.api.utils.upload_client import get_dataset_name, upload_to_blob
+from lamini.generation.token_optimizer import TokenOptimizer
 
 logger = logging.getLogger(__name__)
 
 
 class Lamini:
     def __init__(
         self,
@@ -120,15 +117,19 @@
             return result
 
         assert isinstance(prompt, list)
         if metadata is not None:
             assert isinstance(metadata, list)
             assert len(metadata) == len(prompt)
         results = await self.async_inference_queue.submit(
-            req_data, self.local_cache_file, callback, metadata
+            req_data,
+            self.local_cache_file,
+            callback,
+            metadata,
+            token_optimizer=TokenOptimizer(model_name or self.model_name),
         )
 
         if output_type is None:
             results = [single_result["output"] for single_result in results]
 
         return results
 
@@ -173,26 +174,26 @@
         except Exception as e:
             print(f"Error uploading data pairs: {e}")
             raise e
 
         return dataset_id
 
     def upload_file(
-        self, file_path, input_key: str = "input", output_key: str = "output"
+        self, file_path: str, input_key: str = "input", output_key: str = "output"
     ):
         items = self._upload_file_impl(file_path, input_key, output_key)
         try:
             dataset_id = self.upload_data(items)
             return dataset_id
         except Exception as e:
             print(f"Error reading data file: {e}")
             raise e
 
     def _upload_file_impl(
-        self, file_path, input_key: str = "input", output_key: str = "output"
+        self, file_path: str, input_key: str = "input", output_key: str = "output"
     ):
         if os.path.getsize(file_path) > 1e10:
             raise Exception("File size is too large, please upload file less than 10GB")
 
         # Convert file records to appropriate format before uploading file
         items = []
         if file_path.endswith(".jsonl") or file_path.endswith(".jsonlines"):
@@ -241,19 +242,26 @@
         if dataset_id:
             output = self.trainer.get_upload_base_path()
             self.upload_base_path = output["upload_base_path"]
             output = self.trainer.get_existing_dataset(
                 dataset_id, self.upload_base_path, is_public
             )
             self.upload_file_path = output["dataset_location"]
+            data = None
 
-        if dataset_id is None and data is not None:
+        elif not dataset_id and data:
             dataset_id = self.upload_data(data, is_public)
             data = None
 
+        else:
+            raise ValueError(
+                "Data pairs can not be empty. Either data or dataset_id must be provided."
+            )
+
+        # TODO: remove passing `data` completely.
         job = self.trainer.train(
             data,
             self.model_name,
             self.upload_file_path,
             finetune_args,
             enable_peft,
             peft_args,
```

## lamini/api/train.py

```diff
@@ -1,15 +1,14 @@
 import logging
 from typing import Optional
 
 import lamini
 from lamini.api.lamini_config import get_config, get_configured_key, get_configured_url
 from lamini.api.rest_requests import make_web_request
 from lamini.api.utils.upload_client import SerializableGenerator
-import json
 
 logger = logging.getLogger(__name__)
 
 
 class Train:
     def __init__(
         self,
```

## lamini/api/utils/async_inference_queue.py

```diff
@@ -1,31 +1,47 @@
 import asyncio
 import logging
+from typing import Optional
 
 import aiohttp
 import lamini
 from lamini.api.utils.base_async_inference_queue import BaseAsyncInferenceQueue
 from lamini.api.utils.process_batch import process_batch
 from lamini.api.utils.reservations import create_reservation_api
+from lamini.generation.token_optimizer import TokenOptimizer
 
 logger = logging.getLogger(__name__)
 
 
 class AsyncInferenceQueue(BaseAsyncInferenceQueue):
-    async def submit(self, request, local_cache_file, callback=None, metadata=None):
+    async def submit(
+        self,
+        request,
+        local_cache_file,
+        callback=None,
+        metadata=None,
+        token_optimizer: Optional[TokenOptimizer] = None,
+    ):
         # Break the request into batches
         results = []
         exceptions = []
         local_cache = None
         if local_cache_file:
             local_cache = self.read_local_cache(local_cache_file)
         loop = asyncio.get_running_loop()
         self.reservation_api = create_reservation_api(
             self.api_key, self.api_url, self.config
         )
+        if token_optimizer is not None and "max_new_tokens" in request:
+            request["max_tokens"] = (
+                token_optimizer.calculate_heuristic_max_tokens_from_prompt(
+                    request["prompt"], request["max_new_tokens"]
+                )
+            )
+            logger.debug(f"Adjusted max_tokens to: {request['max_tokens']}")
         self.reservation_api.initialize_reservation(
             len(request["prompt"]),
             request["model_name"],
             self.get_batch_size(),
             request["max_tokens"],
         )
         self.reservation_api.pause_for_reservation_start()
```

## lamini/api/utils/async_inference_queue_3_10.py

```diff
@@ -1,30 +1,46 @@
 import asyncio
 import functools
 import logging
+from typing import Optional
 
 import aiohttp
 from lamini.api.utils.base_async_inference_queue import BaseAsyncInferenceQueue
 from lamini.api.utils.process_batch import process_batch
 from lamini.api.utils.reservations import create_reservation_api
+from lamini.generation.token_optimizer import TokenOptimizer
 
 logger = logging.getLogger(__name__)
 
 
 class AsyncInferenceQueue(BaseAsyncInferenceQueue):
-    async def submit(self, request, local_cache_file, callback=None, metadata=None):
+    async def submit(
+        self,
+        request,
+        local_cache_file,
+        callback=None,
+        metadata=None,
+        token_optimizer: Optional[TokenOptimizer] = None,
+    ):
         # Break the request into batches
         results = {}
         exceptions = []
         local_cache = None
         if local_cache_file:
             local_cache = self.read_local_cache(local_cache_file)
         self.reservation_api = create_reservation_api(
             self.api_key, self.api_url, self.config
         )
+        if token_optimizer is not None and "max_new_tokens" in request:
+            request["max_tokens"] = (
+                token_optimizer.calculate_heuristic_max_tokens_from_prompt(
+                    request["prompt"], request["max_new_tokens"]
+                )
+            )
+            logger.debug(f"Adjusted max_tokens to: {request['max_tokens']}")
         self.reservation_api.initialize_reservation(
             len(request["prompt"]),
             request["model_name"],
             self.get_batch_size(),
             request["max_tokens"],
         )
         self.reservation_api.pause_for_reservation_start()
```

## lamini/api/utils/base_async_inference_queue.py

```diff
@@ -1,14 +1,13 @@
 import json
 import logging
 import os
 
 import lamini
 from lamini.api.lamini_config import get_config, get_configured_key, get_configured_url
-from lamini.api.utils.reservations import create_reservation_api
 
 logger = logging.getLogger(__name__)
 
 
 class BaseAsyncInferenceQueue:
     def __init__(self, api_key, api_url, config):
         self.config = get_config(config)
```

## lamini/generation/generation_node.py

```diff
@@ -1,43 +1,47 @@
 import logging
 import sys
-from typing import AsyncIterator, Iterator, Optional, Union
+from typing import AsyncIterator, Generator, Iterator, Optional, Union
 
 from lamini.api.lamini_config import get_config
 from lamini.generation.base_node_object import BaseGenerationNode
 from lamini.generation.base_prompt_object import PromptObject
+from lamini.generation.token_optimizer import TokenOptimizer
 
 logger = logging.getLogger(__name__)
 
 
 class GenerationNode(BaseGenerationNode):
     def __init__(
         self,
         model_name: str,
         max_tokens: Optional[int] = None,
+        max_new_tokens: Optional[int] = None,
         api_key: Optional[str] = None,
         api_url: Optional[str] = None,
         config: dict = {},
     ):
         self.config = get_config(config)
         self.model_name = model_name
+        self.token_optimizer = TokenOptimizer(model_name)
         if sys.version_info >= (3, 10):
             logger.info("Using 3.10 InferenceQueue Interface")
             from lamini.generation.generation_queue_3_10 import (
                 get_global_inference_queue as get_global_inference_queue_3_10,
             )
 
             self.async_inference_queue = get_global_inference_queue_3_10(
                 api_key, api_url, config=config
             )
         else:
             raise Exception("Must use Python 3.10 or greater for this feature")
 
         self.model_config = self.config.get("model_config", None)
         self.max_tokens = max_tokens
+        self.max_new_tokens = max_new_tokens
 
     def __call__(self, prompt, *args, **kwargs):
         prompt = self.transform_prompt(prompt)
         results = self.generate(prompt, *args, **kwargs)
         results = self.process_results(results)
         return results
 
@@ -50,18 +54,18 @@
         max_new_tokens: Optional[int] = None,
     ):
         assert isinstance(prompt, Iterator) or isinstance(prompt, AsyncIterator)
         req_data = self.make_llm_req_map(
             prompt=prompt,
             model_name=model_name or self.model_name,
             output_type=output_type,
-            max_tokens=max_tokens,
-            max_new_tokens=max_new_tokens,
+            max_tokens=max_tokens or self.max_tokens,
+            max_new_tokens=max_new_tokens or self.max_new_tokens,
         )
-        return self.async_inference_queue.submit(req_data)
+        return self.async_inference_queue.submit(req_data, self.token_optimizer)
 
     def make_llm_req_map(
         self,
         model_name,
         prompt,
         output_type,
         max_tokens,
@@ -82,29 +86,50 @@
     async def transform_prompt(
         self, prompt: Union[Iterator[PromptObject], AsyncIterator[PromptObject]]
     ):
         if isinstance(prompt, Iterator):
             for a in prompt:
                 if hasattr(self, "preprocess"):
                     mod_a = self.preprocess(a)
+                    if isinstance(mod_a, Generator):
+                        for a in mod_a:
+                            if a is not None:
+                                assert isinstance(a, PromptObject)
+                                yield a
+                        continue
                     if mod_a is not None:
                         a = mod_a
+                assert a is None or isinstance(a, PromptObject)
                 yield a
         elif isinstance(prompt, AsyncIterator):
             async for a in prompt:
                 if hasattr(self, "preprocess"):
                     mod_a = self.preprocess(a)
+                    if isinstance(mod_a, Generator):
+                        for a in mod_a:
+                            if a is not None:
+                                assert isinstance(a, PromptObject)
+                                yield a
+                        continue
                     if mod_a is not None:
                         a = mod_a
+                assert a is None or isinstance(a, PromptObject)
                 yield a
         else:
             raise Exception("Invalid prompt type")
 
     async def process_results(self, results: AsyncIterator[PromptObject]):
         async for a in results:
             if a is None or a.response is None:
                 continue
             if hasattr(self, "postprocess"):
                 mod_a = self.postprocess(a)
+                if isinstance(mod_a, Generator):
+                    for a in mod_a:
+                        if a is not None:
+                            assert isinstance(a, PromptObject)
+                            yield a
+                    continue
                 if mod_a is not None:
                     a = mod_a
+            assert a is None or isinstance(a, PromptObject)
             yield a
```

## lamini/generation/generation_pipeline.py

```diff
@@ -2,14 +2,15 @@
 import logging
 import sys
 from typing import AsyncIterator, Iterator, Optional
 
 import lamini
 from lamini.api.utils.reservations import create_reservation_api
 from lamini.generation.base_node_object import BaseGenerationNode
+from lamini.generation.token_optimizer import TokenOptimizer
 
 logger = logging.getLogger(__name__)
 
 
 class GenerationPipeline:
     def __init__(
         self,
```

## lamini/generation/generation_queue_3_10.py

```diff
@@ -1,14 +1,15 @@
 import asyncio
 import functools
 import logging
-from typing import AsyncIterator, Iterator, TypeVar, Union
+from typing import AsyncIterator, Iterator, Optional, TypeVar, Union
 
 from lamini.generation.base_generation_queue import BaseGenerationQueue
 from lamini.generation.process_generation_batch import process_generation_batch
+from lamini.generation.token_optimizer import TokenOptimizer
 
 T = TypeVar("T")
 
 logger = logging.getLogger(__name__)
 
 global_inference_queue = None
 
@@ -38,20 +39,22 @@
         self.appendlist.append(item)
 
 
 class GenerationQueue(BaseGenerationQueue):
     async def submit(
         self,
         request,
+        token_optimizer: Optional[TokenOptimizer] = None,
     ):
         batches = self.form_batches(
             request,
             self.client,
             self.api_key,
             self.api_prefix,
+            token_optimizer,
         )
         batches = AppendableAsyncGenerator(batches)
         wrapped = return_args_and_exceptions(process_generation_batch)
         async_iterator = map_unordered(wrapped, batches, limit=self.get_max_workers())
 
         async for result in async_iterator:
             if isinstance(result[1], Exception):
@@ -78,19 +81,27 @@
 
     async def form_batches(
         self,
         request,
         client,
         key,
         api_prefix,
+        token_optimizer: Optional[TokenOptimizer],
     ):
         batch_size = self.get_batch_size()
         async for prompt in next_n(request["prompt"], batch_size):
             batch = request.copy()
             batch["prompt"] = prompt
+            if token_optimizer is not None and "max_new_tokens" in batch:
+                batch["max_tokens"] = (
+                    token_optimizer.calculate_heuristic_max_tokens_from_prompt(
+                        [p.prompt for p in batch["prompt"]], batch["max_new_tokens"]
+                    )
+                )
+                self.reservation_api.max_tokens = batch["max_tokens"]
             yield {
                 "api_prefix": api_prefix,
                 "key": key,
                 "batch": batch,
                 "client": client,
             }
```

## lamini/runners/base_runner.py

```diff
@@ -228,14 +228,45 @@
             data = self.data
         elif len(self.data) > limit:
             data = self.data[:limit]
         else:
             data = self.data
 
         if self.lamini_api.upload_file_path:
+            response = self.lamini_api.train(
+                is_public=is_public,
+                **kwargs,
+            )
+        else:
+            response = self.lamini_api.train(
+                data,
+                is_public=is_public,
+                **kwargs,
+            )
+        return response
+
+    def train_and_wait(
+        self,
+        limit=500,
+        is_public=False,
+        **kwargs,
+    ):
+        """
+        Train the LLM on added data. This function blocks until training is complete.
+        """
+        if len(self.data) < 2 and not self.lamini_api.upload_file_path:
+            raise Exception("Submit at least 2 data pairs to train to allow validation")
+        if limit is None:
+            data = self.data
+        elif len(self.data) > limit:
+            data = self.data[:limit]
+        else:
+            data = self.data
+
+        if self.lamini_api.upload_file_path:
             final_status = self.lamini_api.train_and_wait(
                 is_public=is_public,
                 **kwargs,
             )
         else:
             final_status = self.lamini_api.train_and_wait(
                 data,
```

## Comparing `lamini-2.1.4a8.dist-info/LICENSE` & `lamini-2.1.5a2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `lamini-2.1.4a8.dist-info/METADATA` & `lamini-2.1.5a2.dist-info/METADATA`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lamini
-Version: 2.1.4a8
+Version: 2.1.5a2
 Summary: Build on large language models faster
 Author-email: PowerML <info@powerml.co>
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
@@ -14,14 +14,15 @@
 Requires-Dist: tqdm
 Requires-Dist: numpy
 Requires-Dist: jsonlines
 Requires-Dist: pandas
 Requires-Dist: azure-storage-blob
 Requires-Dist: scikit-learn
 Requires-Dist: aiohttp
+Requires-Dist: lm-eval
 
 # Lamini
 
 Create your own Generative AI.
 
 This python package is a client + SDK compatible with the Lamini API, documented here: [https://lamini-ai.github.io/](https://lamini-ai.github.io/). For this reason, we recommend that most users stay up-to-date with the most recent stable version of our package available via `pip install --upgrade --force-reinstall lamini`.
```

## Comparing `lamini-2.1.4a8.dist-info/RECORD` & `lamini-2.1.5a2.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,40 +1,41 @@
-lamini/__init__.py,sha256=CpSMXKPw2c0Gh3fqzZqWi5phd9z7rtPWuvMQMT2m6Sg,1362
+lamini/__init__.py,sha256=8_cAQc2ifOfYpLkzj8tJ5L5yC14qN9BFYk9QkLSb6DE,1412
 lamini/api/classifier.py,sha256=Xv8EwtyLD4n5M5N7tW33wmKFBhXI5OV6uMT7DYqxBX4,2200
 lamini/api/embedding.py,sha256=jbfbNmS7jyBYO_Nylp3kw7L-Zb0GrZa1owoRAtgFmNY,1129
-lamini/api/lamini.py,sha256=GBAFEQQDqkyX1rXuz4Ln36SeYQClK1ntodRK9zsOuzc,12272
+lamini/api/lamini.py,sha256=_GPyCZeqT1qvwj2qvhF-jKEMcIvlFowNkX9gZa_6q7E,12631
 lamini/api/lamini_config.py,sha256=LcPqdlaMAdoahFNlH1ig1NYfMK_z0nQMneafhcZWuu4,2369
 lamini/api/precise_trainer.py,sha256=cO02f9asuFQumcTKxYNXzWNVutBZSgTKKb6BlZfC0yQ,2518
 lamini/api/rest_requests.py,sha256=i1nIcAOrtE4zvs0le4mPz37NUq1_Hrty4f5zl0D446w,6208
 lamini/api/streaming_completion.py,sha256=ASGTPSUrNeACEdRUnCJ5ax6JcUSuAZPzeo8X4QSjIsY,6732
 lamini/api/synchronize.py,sha256=Uo6-lQToa-42zVdQ4Qlefvf0Mi3i2He_W1S9yciMMNk,1547
-lamini/api/train.py,sha256=zF_3Oxs6TXdTALTM2f0h26v6LnEfoaT_yb1HQUWUygc,6629
-lamini/api/utils/async_inference_queue.py,sha256=idRn416zuQVP1DYTOLER5Qjo0Q8D-ghdtCG9XR50N9w,4051
-lamini/api/utils/async_inference_queue_3_10.py,sha256=uAhOqPHgZYfXF9eGEoQWYICJRFiJrdU_Kv_djLbIDPE,5376
-lamini/api/utils/base_async_inference_queue.py,sha256=x13AkwLCfHBRIXmSjw6Sh0zGxwbDV7et7FPllpW0KoA,1559
+lamini/api/train.py,sha256=WOtuMceFrvE-aAmYwif3xliuOxkDdzwEYKXG0AIWlXQ,6617
+lamini/api/utils/async_inference_queue.py,sha256=M-7RMyMIFcMS47Z2l9-8W05SafuJnnYiQvLUTgWWfdw,4605
+lamini/api/utils/async_inference_queue_3_10.py,sha256=Itmn1E9bfotiVmnO-k1dznAP87sY-EKRUC-zuZkkLtQ,5930
+lamini/api/utils/base_async_inference_queue.py,sha256=4JvyjGSQfxcSAvV5hQ2-VqbN_1diqLW_nJzNUsEXXnU,1494
 lamini/api/utils/completion.py,sha256=37vn81fvpGR7HMuGQy1Cwz2ozIj0aZQTcxHzXl-cVpI,1343
 lamini/api/utils/process_batch.py,sha256=nNWu80NSeYig_MoO1GQs_JOiJMQgpts0F5Y-xQMDmG8,2865
 lamini/api/utils/reservations.py,sha256=yXnGPM1eQXgzRdQ1rjtoHMSxn4DhDPetxJfPl5221_4,5959
 lamini/api/utils/shutdown.py,sha256=oznFdh3JsE3CDH4Fx30c8_pVGBlIF0iPlFCFpoTE4iU,413
 lamini/api/utils/upload_client.py,sha256=n4VpSgg5n17NUr6PL7eT_aQESPz7GyrJb3x9jG-o1Kw,1393
 lamini/classify/llama_classifier.py,sha256=SrSCAf5tTiDyhRVzt9kn4lFqEC_A7Caq0lSrOB2SU08,23827
 lamini/error/error.py,sha256=C4SbfG3obNYGH8onkUkhUc61XRTwejCBlsG1QBNaEQI,896
 lamini/generation/base_generation_queue.py,sha256=D_gav4v76N9Oo2r0dWYNuho2A8nKdr0Q-0xMjZMmyPk,1273
 lamini/generation/base_node_object.py,sha256=Q_t8eqCnf5HnLOHQTMpFaIq-_I2_YhXUhi64FeRF5-M,594
 lamini/generation/base_prompt_object.py,sha256=nC5VTLoiNzYoQquA2nPeqAvWY_zeSJlDtQnb5iuutSk,797
 lamini/generation/embedding_node.py,sha256=Iq2PlnI6yMTpwxG67w07029VDp_aJjKNbHhAgN4utnk,1418
-lamini/generation/generation_node.py,sha256=xcs1evFVcM2ujCirG3HdFsPxk3WCJxK0POtjiNV1GsM,3815
-lamini/generation/generation_pipeline.py,sha256=321qwa5nuElWPuejxlX2ZedLe6aBJgGCrFezIY29rDU,3807
-lamini/generation/generation_queue_3_10.py,sha256=cwlQIMgcSqVIwqtBFREYrbbVVK9koySnnrsmDyrq7Cg,6017
+lamini/generation/generation_node.py,sha256=62X4Bm_qcdPd0J4Z6AyYWDOMNeiQgkT8EpIs77BZaWQ,5101
+lamini/generation/generation_pipeline.py,sha256=jbBYbVV4w8Tmrdzo84c-0GpSfM9a_ZIqJ6yqiXyWvks,3868
+lamini/generation/generation_queue_3_10.py,sha256=sm33jtLqN7HpIhWvJYX6v9pNPq3IXLWlYv068dOFK-k,6615
 lamini/generation/modify_node.py,sha256=OKH1dmJmptteccspAxS3sgyP4NvFyix3HTnzaGyb7DI,965
 lamini/generation/process_generation_batch.py,sha256=siVQuzJMX0x5xesyzUucUzX6nJ-Y8jFgs3csfbUoP6I,3448
 lamini/generation/split_response_node.py,sha256=sZJaPj_4mdf-m-c4CAQ22O6nzA53fsHKzI5P7Irw7Do,1390
-lamini/runners/base_runner.py,sha256=hzw7HJMm8B7cfhClN87rIBdnXOE4E2lr9hki0Kq5qkg,8451
+lamini/generation/token_optimizer.py,sha256=LI7-APb8XM_Jg1WNL2DuQV4NcoY3y1FL-B3KlDPNe0E,557
+lamini/runners/base_runner.py,sha256=VYPlMdx2sR9BLXrO_jsyp-p0_3IQUXwPkVta8g6xFPE,9354
 lamini/runners/basic_model_runner.py,sha256=7e4xTotKGdmC0PH-ynAlOtXxHDvhH8HInqWCggG5DOA,720
 lamini/runners/llama_v2_runner.py,sha256=T2OjiJTC0goNpZebD1iKOKkuL5aAjfdrTvxss-d5ezM,3062
 lamini/runners/mistral_runner.py,sha256=uHS2ptK01JMGnvSi4zH6bcWquuZYjc3eIFgzWnFeg1Q,2625
 llama/__init__.py,sha256=E77xncFEWyRrg-MsjkiLrCkNJBIYIxr111hLd7WpgjY,21
-lamini-2.1.4a8.dist-info/LICENSE,sha256=-alRIf0b5B1SavU0njHUTAanPUn6GHxH9a2Q_ACz1HM,11340
-lamini-2.1.4a8.dist-info/METADATA,sha256=8rEMX6VobRMoJQull3AuAYIdZNqhouNR473LSO6kiRE,1484
-lamini-2.1.4a8.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-lamini-2.1.4a8.dist-info/top_level.txt,sha256=5h0-n2aeXxQUxmNPPJ0AnsKIBAZV_qWqU0JIpA6Q2zo,13
-lamini-2.1.4a8.dist-info/RECORD,,
+lamini-2.1.5a2.dist-info/LICENSE,sha256=-alRIf0b5B1SavU0njHUTAanPUn6GHxH9a2Q_ACz1HM,11340
+lamini-2.1.5a2.dist-info/METADATA,sha256=BbIkzpMauz9gZjrv1N6l7ckJBhV2Zu3p2we9lH4aaxs,1507
+lamini-2.1.5a2.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+lamini-2.1.5a2.dist-info/top_level.txt,sha256=5h0-n2aeXxQUxmNPPJ0AnsKIBAZV_qWqU0JIpA6Q2zo,13
+lamini-2.1.5a2.dist-info/RECORD,,
```

