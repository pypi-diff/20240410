# Comparing `tmp/nucliadb_dataset-2.9.0.post267-py3-none-any.whl.zip` & `tmp/nucliadb_dataset-3.0.0.post414-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,17 +1,18 @@
-Zip file size: 15815 bytes, number of entries: 15
--rw-r--r--  2.0 unx     1638 b- defN 23-May-15 13:40 nucliadb_dataset/__init__.py
--rw-r--r--  2.0 unx     3040 b- defN 23-May-15 13:40 nucliadb_dataset/api.py
--rw-r--r--  2.0 unx    16318 b- defN 23-May-15 13:40 nucliadb_dataset/dataset.py
--rw-r--r--  2.0 unx     2837 b- defN 23-May-15 13:40 nucliadb_dataset/export.py
--rw-r--r--  2.0 unx     2842 b- defN 23-May-15 13:40 nucliadb_dataset/mapping.py
--rw-r--r--  2.0 unx     3969 b- defN 23-May-15 13:40 nucliadb_dataset/nuclia.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-15 13:40 nucliadb_dataset/py.typed
--rw-r--r--  2.0 unx     3454 b- defN 23-May-15 13:40 nucliadb_dataset/run.py
--rw-r--r--  2.0 unx     2325 b- defN 23-May-15 13:40 nucliadb_dataset/settings.py
--rw-r--r--  2.0 unx     2410 b- defN 23-May-15 13:40 nucliadb_dataset/streamer.py
--rw-r--r--  2.0 unx     1314 b- defN 23-May-15 13:42 nucliadb_dataset-2.9.0.post267.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-15 13:42 nucliadb_dataset-2.9.0.post267.dist-info/WHEEL
--rw-r--r--  2.0 unx       61 b- defN 23-May-15 13:42 nucliadb_dataset-2.9.0.post267.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       17 b- defN 23-May-15 13:42 nucliadb_dataset-2.9.0.post267.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1307 b- defN 23-May-15 13:42 nucliadb_dataset-2.9.0.post267.dist-info/RECORD
-15 files, 41624 bytes uncompressed, 13625 bytes compressed:  67.3%
+Zip file size: 17001 bytes, number of entries: 16
+-rw-r--r--  2.0 unx     1306 b- defN 24-Apr-10 13:42 nucliadb_dataset/__init__.py
+-rw-r--r--  2.0 unx     3040 b- defN 24-Apr-10 13:42 nucliadb_dataset/api.py
+-rw-r--r--  2.0 unx    11484 b- defN 24-Apr-10 13:42 nucliadb_dataset/dataset.py
+-rw-r--r--  2.0 unx     2780 b- defN 24-Apr-10 13:42 nucliadb_dataset/export.py
+-rw-r--r--  2.0 unx     5637 b- defN 24-Apr-10 13:42 nucliadb_dataset/mapping.py
+-rw-r--r--  2.0 unx     3969 b- defN 24-Apr-10 13:42 nucliadb_dataset/nuclia.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 13:42 nucliadb_dataset/py.typed
+-rw-r--r--  2.0 unx     3015 b- defN 24-Apr-10 13:42 nucliadb_dataset/run.py
+-rw-r--r--  2.0 unx     2364 b- defN 24-Apr-10 13:42 nucliadb_dataset/settings.py
+-rw-r--r--  2.0 unx     2591 b- defN 24-Apr-10 13:42 nucliadb_dataset/streamer.py
+-rw-r--r--  2.0 unx     5719 b- defN 24-Apr-10 13:42 nucliadb_dataset/tasks.py
+-rw-r--r--  2.0 unx     1246 b- defN 24-Apr-10 13:44 nucliadb_dataset-3.0.0.post414.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-10 13:44 nucliadb_dataset-3.0.0.post414.dist-info/WHEEL
+-rw-r--r--  2.0 unx       60 b- defN 24-Apr-10 13:44 nucliadb_dataset-3.0.0.post414.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       17 b- defN 24-Apr-10 13:44 nucliadb_dataset-3.0.0.post414.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1389 b- defN 24-Apr-10 13:44 nucliadb_dataset-3.0.0.post414.dist-info/RECORD
+16 files, 44709 bytes uncompressed, 14685 bytes compressed:  67.2%
```

## zipnote {}

```diff
@@ -24,23 +24,26 @@
 
 Filename: nucliadb_dataset/settings.py
 Comment: 
 
 Filename: nucliadb_dataset/streamer.py
 Comment: 
 
-Filename: nucliadb_dataset-2.9.0.post267.dist-info/METADATA
+Filename: nucliadb_dataset/tasks.py
 Comment: 
 
-Filename: nucliadb_dataset-2.9.0.post267.dist-info/WHEEL
+Filename: nucliadb_dataset-3.0.0.post414.dist-info/METADATA
 Comment: 
 
-Filename: nucliadb_dataset-2.9.0.post267.dist-info/entry_points.txt
+Filename: nucliadb_dataset-3.0.0.post414.dist-info/WHEEL
 Comment: 
 
-Filename: nucliadb_dataset-2.9.0.post267.dist-info/top_level.txt
+Filename: nucliadb_dataset-3.0.0.post414.dist-info/entry_points.txt
 Comment: 
 
-Filename: nucliadb_dataset-2.9.0.post267.dist-info/RECORD
+Filename: nucliadb_dataset-3.0.0.post414.dist-info/top_level.txt
+Comment: 
+
+Filename: nucliadb_dataset-3.0.0.post414.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nucliadb_dataset/__init__.py

```diff
@@ -16,42 +16,28 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 from enum import Enum
 from typing import Dict
 
-from nucliadb_dataset.dataset import (
-    NucliaCloudDataset,
-    NucliaDBDataset,
-    Task,
-    download_all_partitions,
-)
+from nucliadb_dataset.dataset import NucliaDBDataset, Task, download_all_partitions
 from nucliadb_dataset.nuclia import NucliaDriver
 
 NUCLIA_GLOBAL: Dict[str, NucliaDriver] = {}
 
 CLIENT_ID = "CLIENT"
 
 
-class DatasetType(str, Enum):
-    FIELD_CLASSIFICATION = "FIELD_CLASSIFICATION"
-    PARAGRAPH_CLASSIFICATION = "PARAGRAPH_CLASSIFICATION"
-    SENTENCE_CLASSIFICATION = "SENTENCE_CLASSIFICATION"
-    TOKEN_CLASSIFICATION = "TOKEN_CLASSIFICATION"
-
-
 class ExportType(str, Enum):
     DATASETS = "DATASETS"
     FILESYSTEM = "FILESYSTEM"
 
 
 __all__ = (
     "NucliaDBDataset",
-    "NucliaCloudDataset",
     "Task",
     "download_all_partitions",
     "NUCLIA_GLOBAL",
     "CLIENT_ID",
-    "DatasetType",
     "ExportType",
 )
```

## nucliadb_dataset/dataset.py

```diff
@@ -13,73 +13,46 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
-import base64
-import json
 import os
-import re
-from enum import Enum
-from typing import (
-    TYPE_CHECKING,
-    Any,
-    Callable,
-    Dict,
-    Iterator,
-    List,
-    Optional,
-    Tuple,
-    Union,
-)
+from dataclasses import dataclass, field
+from typing import Any, Callable, Dict, Iterator, List, Optional, Tuple
 
-import boto3
 import pyarrow as pa  # type: ignore
-from google.auth.credentials import AnonymousCredentials  # type: ignore
-from google.cloud import storage  # type: ignore
-from google.oauth2 import service_account  # type: ignore
-from nucliadb_protos.dataset_pb2 import (
-    FieldClassificationBatch,
-    ParagraphClassificationBatch,
-    SentenceClassificationBatch,
-    TaskType,
-    TokenClassificationBatch,
-    TrainSet,
-)
+from nucliadb_protos.dataset_pb2 import TaskType, TrainSet
 
-from nucliadb_dataset.mapping import (
-    batch_to_text_classification_arrow,
-    batch_to_text_classification_normalized_arrow,
-    batch_to_token_classification_arrow,
-    bytes_to_batch,
-)
 from nucliadb_dataset.streamer import Streamer, StreamerAlreadyRunning
+from nucliadb_dataset.tasks import (
+    ACTUAL_PARTITION,
+    TASK_DEFINITIONS,
+    TASK_DEFINITIONS_REVERSE,
+    Task,
+)
 from nucliadb_models.entities import KnowledgeBoxEntities
 from nucliadb_models.labels import KnowledgeBoxLabels
-from nucliadb_sdk.client import NucliaDBClient
-from nucliadb_sdk.knowledgebox import KnowledgeBox
-from nucliadb_sdk.utils import get_kb
+from nucliadb_models.resource import KnowledgeBoxObj
+from nucliadb_models.search import (
+    KnowledgeboxSearchResults,
+    SearchOptions,
+    SearchRequest,
+)
+from nucliadb_models.trainset import TrainSetPartitions
+from nucliadb_sdk.v2.sdk import NucliaDB, Region
 
 CHUNK_SIZE = 5 * 1024 * 1024
 
-if TYPE_CHECKING:  # pragma: no cover
-    TaskValue = TaskType.V
-else:
-    TaskValue = int
-
-ACTUAL_PARTITION = "actual_partition"
-
 
-class Task(str, Enum):
-    PARAGRAPH_CLASSIFICATION = "PARAGRAPH_CLASSIFICATION"
-    FIELD_CLASSIFICATION = "FIELD_CLASSIFICATION"
-    SENTENCE_CLASSIFICATION = "SENTENCE_CLASSIFICATION"
-    TOKEN_CLASSIFICATION = "TOKEN_CLASSIFICATION"
+@dataclass
+class LabelSetCount:
+    count: int
+    labels: Dict[str, int] = field(default_factory=dict)
 
 
 class NucliaDataset(object):
     labels: Optional[KnowledgeBoxLabels]
     entities: Optional[KnowledgeBoxEntities]
 
     def __new__(cls, *args, **kwargs):
@@ -132,185 +105,166 @@
     ):
         raise NotImplementedError()
 
 
 class NucliaDBDataset(NucliaDataset):
     def __init__(
         self,
-        client: NucliaDBClient,
+        sdk: NucliaDB,
+        kbid: str,
         task: Optional[Task] = None,
         labels: Optional[List[str]] = None,
         trainset: Optional[TrainSet] = None,
         base_path: Optional[str] = None,
+        search_sdk: Optional[NucliaDB] = None,
+        reader_sdk: Optional[NucliaDB] = None,
     ):
         super().__init__(base_path)
 
         if labels is None:
             labels = []
 
+        task_definition = None
         if trainset is None and task is not None:
-            if Task.PARAGRAPH_CLASSIFICATION == task:
-                trainset = TrainSet(type=TaskType.PARAGRAPH_CLASSIFICATION)
-                trainset.filter.labels.extend(labels)
-            elif Task.FIELD_CLASSIFICATION == task:
-                trainset = TrainSet(type=TaskType.FIELD_CLASSIFICATION)
-                trainset.filter.labels.extend(labels)
-            elif Task.SENTENCE_CLASSIFICATION == task:
-                trainset = TrainSet(type=TaskType.SENTENCE_CLASSIFICATION)
-                trainset.filter.labels.extend(labels)
-
-            elif Task.TOKEN_CLASSIFICATION == task:
-                trainset = TrainSet(type=TaskType.TOKEN_CLASSIFICATION)
-                trainset.filter.labels.extend(labels)
-
-            else:
+            task_definition = TASK_DEFINITIONS.get(task)
+            if task_definition is None:
                 raise KeyError("Not a valid task")
+            trainset = TrainSet(type=task_definition.proto)
+            if task_definition.labels:
+                trainset.filter.labels.extend(labels)
+        elif trainset is not None:
+            task_definition = TASK_DEFINITIONS_REVERSE.get(trainset.type)
         elif trainset is None and task is None:
             raise AttributeError("Trainset or task needs to be defined")
 
-        if trainset is None:
+        if trainset is None or task_definition is None:
             raise AttributeError("Trainset could not be defined")
 
+        self.kbid = kbid
         self.trainset = trainset
-        self.client = client
-        self.knowledgebox = KnowledgeBox(self.client)
-        self.streamer = Streamer(self.trainset, self.client)
+        self.task_definition = task_definition
+        self.train_sdk = sdk
+        if search_sdk is None:
+            self.search_sdk = sdk
+        else:
+            self.search_sdk = search_sdk
 
+        if reader_sdk is None:
+            self.reader_sdk = sdk
+        else:
+            self.reader_sdk = reader_sdk
+
+        self.streamer = Streamer(
+            self.trainset,
+            reader_headers=self.train_sdk.headers,
+            base_url=self.train_sdk.base_url,
+            kbid=kbid,
+        )
+
+        self._set_schema(self.task_definition.schema)
+        self._set_mappings(self.task_definition.mapping)
         if self.trainset.type == TaskType.PARAGRAPH_CLASSIFICATION:
-            self._configure_paragraph_classification()
+            self._check_labels("PARAGRAPHS")
 
-        if self.trainset.type == TaskType.FIELD_CLASSIFICATION:
-            self._configure_field_classification()
+        elif self.trainset.type == TaskType.FIELD_CLASSIFICATION:
+            self._check_labels("RESOURCES")
 
-        if self.trainset.type == TaskType.TOKEN_CLASSIFICATION:
-            self._configure_token_classification()
+        elif self.trainset.type == TaskType.TOKEN_CLASSIFICATION:
+            self._check_entities()
 
-        if self.trainset.type == TaskType.SENTENCE_CLASSIFICATION:
-            self._configure_sentence_classification()
+    def _computed_labels(self) -> Dict[str, LabelSetCount]:
+        search_result: KnowledgeboxSearchResults = self.search_sdk.search(
+            kbid=self.kbid,
+            content=SearchRequest(
+                features=[SearchOptions.DOCUMENT], faceted=["/l"], page_size=0
+            ),
+        )
+
+        response: Dict[str, LabelSetCount] = {}
+        if search_result.fulltext is None or search_result.fulltext.facets is None:
+            return response
+
+        label_facets = {}
+        facet_prefix = "/l/"
+        if "/l" in search_result.fulltext.facets:
+            label_facets = search_result.fulltext.facets.get("/l", {})
+        elif "/classification.labels" in search_result.fulltext.facets:
+            facet_prefix = "/classification.labels/"
+            label_facets = search_result.fulltext.facets.get(
+                "/classification.labels", {}
+            )
 
-    def _configure_sentence_classification(self):
-        self.labels = self.client.get_labels()
-        labelset = self.trainset.filter.labels[0]
-        if labelset not in self.labels.labelsets:
-            raise Exception("Labelset is not valid")
-        self._set_mappings(
-            [
-                bytes_to_batch(SentenceClassificationBatch),
-                batch_to_text_classification_normalized_arrow,
-            ]
-        )
-        self._set_schema(
-            pa.schema(
-                [
-                    pa.field("text", pa.string()),
-                    pa.field("labels", pa.list_(pa.string())),
-                ]
+        for labelset, count in label_facets.items():
+            real_labelset = labelset[len(facet_prefix) :]  # removing /l/
+            response[real_labelset] = LabelSetCount(count=count)
+
+        for labelset, labelset_obj in response.items():
+            base_label = f"{facet_prefix}{labelset}"
+            fsearch_result: KnowledgeboxSearchResults = self.search_sdk.search(
+                kbid=self.kbid,
+                content=SearchRequest(
+                    features=[SearchOptions.DOCUMENT], faceted=[base_label], page_size=0
+                ),  # type: ignore
             )
-        )
+            if (
+                fsearch_result.fulltext is None
+                or fsearch_result.fulltext.facets is None
+            ):
+                raise Exception("Search error")
+
+            for label, count in fsearch_result.fulltext.facets.get(
+                base_label, {}
+            ).items():
+                labelset_obj.labels[label.replace(base_label + "/", "")] = count
+        return response
 
-    def _configure_field_classification(self):
+    def _check_labels(self, type: str = "PARAGRAPHS"):
         if len(self.trainset.filter.labels) != 1:
             raise Exception("Needs to have only one labelset filter to train")
-        self.labels = self.client.get_labels()
+
+        labels: KnowledgeBoxLabels = self.reader_sdk.get_labelsets(kbid=self.kbid)
         labelset = self.trainset.filter.labels[0]
-        computed_labelset = False
 
-        if labelset not in self.labels.labelsets:
-            if labelset in self.knowledgebox.get_uploaded_labels():
-                computed_labelset = True
-            else:
-                raise Exception("Labelset is not valid")
-
-        if (
-            computed_labelset is False
-            and "RESOURCES" not in self.labels.labelsets[labelset].kind
-        ):
-            raise Exception("Labelset not defined for Field Classification")
-
-        self._set_mappings(
-            [
-                bytes_to_batch(FieldClassificationBatch),
-                batch_to_text_classification_arrow,
-            ]
-        )
-        self._set_schema(
-            pa.schema(
-                [
-                    pa.field("text", pa.string()),
-                    pa.field("labels", pa.list_(pa.string())),
-                ]
-            )
+        if labelset not in labels.labelsets:
+            computed_labels = self._computed_labels()
+            if type != "RESOURCES" or labelset not in computed_labels:
+                raise Exception(
+                    f"Labelset is not valid {labelset} not in {labels.labelsets}"
+                )
+
+        elif type not in labels.labelsets[labelset].kind:
+            raise Exception(f"Labelset not defined for {type} classification")
+
+    def _check_entities(self) -> None:
+        entities: KnowledgeBoxEntities = self.reader_sdk.get_entitygroups(
+            kbid=self.kbid
         )
-
-    def _configure_token_classification(self):
-        self.entities = self.client.get_entities()
         for family_group in self.trainset.filter.labels:
-            if family_group not in self.entities.groups:
+            if family_group not in entities.groups:
                 raise Exception("Family group is not valid")
 
-        schema = pa.schema(
-            [
-                pa.field("text", pa.list_(pa.string())),
-                pa.field("labels", pa.list_(pa.string())),
-            ]
-        )
-        self._set_mappings(
-            [
-                bytes_to_batch(TokenClassificationBatch),
-                batch_to_token_classification_arrow(schema),
-            ]
-        )
-        self._set_schema(schema)
-
-    def _configure_paragraph_classification(self):
-        if len(self.trainset.filter.labels) != 1:
-            raise Exception("Needs to have only one labelset filter to train")
-        self.labels = self.client.get_labels()
-        labelset = self.trainset.filter.labels[0]
-
-        if labelset not in self.labels.labelsets:
-            raise Exception("Labelset is not valid")
-
-        if "PARAGRAPHS" not in self.labels.labelsets[labelset].kind:
-            raise Exception("Labelset not defined for Paragraphs Classification")
-
-        self._set_mappings(
-            [
-                bytes_to_batch(ParagraphClassificationBatch),
-                batch_to_text_classification_arrow,
-            ]
-        )
-        self._set_schema(
-            pa.schema(
-                [
-                    pa.field("text", pa.string()),
-                    pa.field("labels", pa.list_(pa.string())),
-                ]
-            )
-        )
-
     def _map(self, batch: Any):
         for func in self.mappings:
-            batch = func(batch)
+            batch = func(batch, self.schema)
         return batch
 
     def _set_mappings(self, funcs: List[Callable[[Any, Any], Tuple[Any, Any]]]):
         self.mappings = funcs
 
     def _set_schema(self, schema: pa.Schema):
         self.schema = schema
 
-    def get_partitions(self):
+    def get_partitions(self) -> List[str]:
         """
         Get expected number of partitions from a live NucliaDB
         """
-        partitions = self.client.train_session.get(f"/trainset").json()
-        if len(partitions["partitions"]) == 0:
+        partitions: TrainSetPartitions = self.train_sdk.trainset(kbid=self.kbid)
+        if len(partitions.partitions) == 0:
             raise KeyError("There is no partitions")
-        return partitions["partitions"]
+        return partitions.partitions
 
     def read_partition(
         self,
         partition_id: str,
         filename: Optional[str] = None,
         force: bool = False,
         path: Optional[str] = None,
@@ -346,138 +300,31 @@
                     writer.write_batch(batch)
         print("-" * 10)
         self.streamer.finalize()
         os.rename(filename_tmp, filename)
         return filename
 
 
-class S3DatasetsClient:
-    def __init__(self, settings):
-        self.client = boto3.client(
-            "s3",
-            aws_access_key_id=settings["client_id"],
-            aws_secret_access_key=settings["client_secret"],
-            use_ssl=settings["ssl"],
-            verify=settings["verify_ssl"],
-            endpoint_url=settings["endpoint"],
-            region_name=settings["region_name"],
-        )
-
-    def list_files(self, bucket_name: str, path: str) -> List[str]:
-        objects = self.client.list_objects(Bucket=bucket_name, Prefix=path)
-        files_list = [obj["Key"] for obj in objects.get("Contents", [])]
-        return files_list
-
-    def download(self, bucket_name: str, filename: str, file_obj) -> None:
-        obj = self.client.get_object(Bucket=bucket_name, Key=filename)
-        data = obj["Body"]
-        file_obj.write(data.read())
-
-
-class GCSDatasetsClient:
-    def __init__(self, settings):
-        if settings["base64_creds"] is not None:
-            account_credentials = json.loads(base64.b64decode(settings["base64_creds"]))
-            credentials = service_account.Credentials.from_service_account_info(
-                account_credentials,
-                scopes=["https://www.googleapis.com/auth/devstorage.read_write"],
-            )
-        else:
-            credentials = AnonymousCredentials()
-        self.client = storage.Client(
-            project=settings["project"],
-            credentials=credentials,
-            client_options={"api_endpoint": settings["endpoint_url"]},
-        )
-
-    def list_files(self, bucket_name: str, path: str) -> List[str]:
-        bucket = self.client.bucket(bucket_name)
-        arrow_files = [blob.name for blob in bucket.list_blobs(prefix=path)]
-        return arrow_files
-
-    def download(self, bucket_name: str, filename: str, file_obj) -> None:
-        bucket = self.client.bucket(bucket_name)
-        blob = bucket.get_blob(filename)
-
-        if blob is None:
-            raise ValueError(f"File {filename} not found on {bucket_name}")
-
-        blob.download_to_file(file_obj)
-
-
-class NucliaCloudDataset(NucliaDataset):
-    client: Union[GCSDatasetsClient, S3DatasetsClient]
-
-    def __init__(self, base_path: str, remote_storage: Dict[str, Any]):
-        super().__init__(base_path)
-        self.bucket = remote_storage["bucket"]
-        self.key = remote_storage["key"]
-        if remote_storage["storage_type"] == "gcs":
-            self.client = GCSDatasetsClient(remote_storage["settings"])
-        elif remote_storage["storage_type"] == "s3":
-            self.client = S3DatasetsClient(remote_storage["settings"])
-
-    def get_partitions(self):
-        """
-        Count all *.arrow files on the bucket
-        """
-        arrow_files = self.client.list_files(self.bucket, self.key)
-        partitions = []
-        for file in arrow_files:
-            match = re.match(r".*\/([^\/]+).arrow", file)
-            if match:
-                partitions.append(match.groups()[0])
-        return partitions
-
-    def read_partition(
-        self,
-        partition_id: str,
-        filename: Optional[str] = None,
-        force: bool = False,
-        path: Optional[str] = None,
-    ):
-        """
-        Download an pregenerated arrow partition from a bucket and store it locally
-        """
-        if filename is None:
-            filename = partition_id
-
-        if path is not None:
-            filename = f"{path}/{filename}.arrow"
-        else:
-            filename = f"{self.base_path}/{filename}.arrow"
-
-        if os.path.exists(filename) and force is False:
-            return filename
-
-        filename_tmp = f"{filename}.tmp"
-        print(f"Downloading partition {partition_id} from {self.bucket}/{self.key}")
-        with open(filename_tmp, "wb") as downloaded_file:
-            self.client.download(
-                self.bucket, f"{self.key}/{partition_id}.arrow", downloaded_file
-            )
-            downloaded_file.flush()
-
-        print("-" * 10)
-        os.rename(filename_tmp, filename)
-        return filename
-
-
 def download_all_partitions(
     task: str,  # type: ignore
     slug: Optional[str] = None,
+    kbid: Optional[str] = None,
     nucliadb_base_url: Optional[str] = "http://localhost:8080",
     path: Optional[str] = None,
-    knowledgebox: Optional[KnowledgeBox] = None,
+    sdk: Optional[NucliaDB] = None,
     labels: Optional[List[str]] = None,
 ):
-    if knowledgebox is None and slug is not None:
-        knowledgebox = get_kb(slug, nucliadb_base_url)
+    if sdk is None:
+        sdk = NucliaDB(region=Region.ON_PREM, url=nucliadb_base_url)
+
+    if kbid is None and slug is not None:
+        kb: KnowledgeBoxObj = sdk.get_knowledge_box_by_slug(slug=slug)
+        kbid = kb.uuid
 
-    if knowledgebox is None:
-        raise KeyError("KnowlwedgeBox not found")
+    if kbid is None:
+        raise KeyError("Not a valid KB")
 
     task_obj = Task(task)
     fse = NucliaDBDataset(
-        client=knowledgebox.client, task=task_obj, labels=labels, base_path=path
+        sdk=sdk, task=task_obj, labels=labels, base_path=path, kbid=kbid
     )
     return fse.read_all_partitions(path=path)
```

## nucliadb_dataset/export.py

```diff
@@ -18,40 +18,40 @@
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 
 import requests
 from nucliadb_protos.dataset_pb2 import TaskType, TrainSet
 
 from nucliadb_dataset.dataset import NucliaDBDataset
-from nucliadb_sdk.client import NucliaDBClient
+from nucliadb_sdk.v2.sdk import NucliaDB
 
 
 class NucliaDatasetsExport:
     def __init__(
         self,
-        client: NucliaDBClient,
+        sdk: NucliaDB,
+        kbid: str,
         datasets_url: str,
         trainset: TrainSet,
         cache_path: str,
         apikey: str,
     ):
         self.datasets_url = datasets_url
         self.trainset = trainset
-        self.client = client
+        self.sdk = sdk
         self.nucliadb_dataset = NucliaDBDataset(
-            trainset=trainset, client=self.client, base_path=cache_path
+            trainset=trainset, kbid=kbid, sdk=sdk, base_path=cache_path
         )
-        self.datasets_url = datasets_url
         self.apikey = apikey
 
     def export(self):
         dataset_def = {
             "type": TaskType.Name(self.trainset.type),
             "filter": {"labels": list(self.trainset.filter.labels)},
-            "name": str(self.client.reader_session.base_url),
+            "name": str(self.sdk.base_url),
         }
         response = requests.post(
             f"{self.datasets_url}/datasets",
             json=dataset_def,
             headers={"x-stf-nuakey": f"Bearer {self.apikey}"},
         )
 
@@ -67,18 +67,19 @@
                     headers={"x-stf-nuakey": f"Bearer {self.apikey}"},
                 )
 
 
 class FileSystemExport:
     def __init__(
         self,
-        client: NucliaDBClient,
+        sdk: NucliaDB,
+        kbid: str,
         trainset: TrainSet,
         store_path: str,
     ):
-        self.client = client
+        self.sdk = sdk
         self.nucliadb_dataset = NucliaDBDataset(
-            trainset=trainset, client=self.client, base_path=store_path
+            trainset=trainset, kbid=kbid, sdk=sdk, base_path=store_path
         )
 
     def export(self):
         self.nucliadb_dataset.read_all_partitions()
```

## nucliadb_dataset/mapping.py

```diff
@@ -19,48 +19,56 @@
 
 # Mapping to transform a text/labels onto tokens and multilabelbinarizer
 from typing import Any, TypeVar
 
 import pyarrow as pa  # type: ignore
 from nucliadb_protos.dataset_pb2 import (
     FieldClassificationBatch,
+    ImageClassificationBatch,
     ParagraphClassificationBatch,
+    ParagraphStreamingBatch,
+    QuestionAnswerStreamingBatch,
     SentenceClassificationBatch,
     TokenClassificationBatch,
 )
 
 BatchType = TypeVar("BatchType", ParagraphClassificationBatch, FieldClassificationBatch)
 
 
 def bytes_to_batch(klass: Any):
-    def func(batch: bytes) -> Any:
+    def func(batch: bytes, *args, **kwargs) -> Any:
         pb = klass()
         pb.ParseFromString(batch)
         return pb
 
     return func
 
 
-def batch_to_text_classification_arrow(batch: BatchType):
-    X = []
-    Y = []
-    for data in batch.data:
-        if data.text:
-            X.append(data.text)
-            Y.append([f"{label.labelset}/{label.label}" for label in data.labels])
-    if len(X):
-        pa_data = [pa.array(X), pa.array(Y)]
-        output_batch = pa.record_batch(pa_data, names=["text", "labels"])
-    else:
-        output_batch = None
-    return output_batch
+def batch_to_text_classification_arrow():
+    def func(batch: BatchType, schema: pa.schema):
+        TEXT = []
+        LABELS = []
+        for data in batch.data:
+            if data.text:
+                TEXT.append(data.text)
+                LABELS.append(
+                    [f"{label.labelset}/{label.label}" for label in data.labels]
+                )
+        if len(TEXT):
+            pa_data = [pa.array(TEXT), pa.array(LABELS)]
+            output_batch = pa.record_batch(pa_data, schema=schema)
+        else:
+            output_batch = None
+        return output_batch
+
+    return func
 
 
-def batch_to_token_classification_arrow(schema: pa.schema):
-    def func(batch: TokenClassificationBatch):
+def batch_to_token_classification_arrow():
+    def func(batch: TokenClassificationBatch, schema: pa.schema):
         X = []
         Y = []
         for data in batch.data:
             X.append([x for x in data.token])
             Y.append([x for x in data.label])
         if len(X):
             pa_data = [pa.array(X), pa.array(Y)]
@@ -68,21 +76,100 @@
         else:
             output_batch = None
         return output_batch
 
     return func
 
 
-def batch_to_text_classification_normalized_arrow(batch: SentenceClassificationBatch):
-    X = []
-    Y = []
-    for data in batch.data:
-        batch_labels = [f"{label.labelset}/{label.label}" for label in data.labels]
-        for sentence in data.text:
-            X.append(sentence)
-            Y.append(batch_labels)
-    if len(X):
-        pa_data = [pa.array(X), pa.array(Y)]
-        output_batch = pa.record_batch(pa_data, names=["text", "labels"])
-    else:
-        output_batch = None
-    return output_batch
+def batch_to_text_classification_normalized_arrow():
+    def func(batch: SentenceClassificationBatch, schema: pa.schema):
+        TEXT = []
+        LABELS = []
+        for data in batch.data:
+            batch_labels = [f"{label.labelset}/{label.label}" for label in data.labels]
+            for sentence in data.text:
+                TEXT.append(sentence)
+                LABELS.append(batch_labels)
+        if len(TEXT):
+            pa_data = [pa.array(TEXT), pa.array(LABELS)]
+            output_batch = pa.record_batch(pa_data, schema=schema)
+        else:
+            output_batch = None
+        return output_batch
+
+    return func
+
+
+def batch_to_image_classification_arrow():
+    def func(batch: ImageClassificationBatch, schema: pa.schema):
+        IMAGE = []
+        SELECTION = []
+        for data in batch.data:
+            IMAGE.append(data.page_uri)
+            SELECTION.append(data.selections)
+
+        if len(IMAGE):
+            pa_data = [pa.array(IMAGE), pa.array(SELECTION)]
+            output_batch = pa.record_batch(pa_data, schema=schema)
+        else:
+            output_batch = None
+        return output_batch
+
+    return func
+
+
+def batch_to_paragraph_streaming_arrow():
+    def func(batch: ParagraphStreamingBatch, schema: pa.schema):
+        PARARGAPH_ID = []
+        TEXT = []
+        for data in batch.data:
+            PARARGAPH_ID.append(data.id)
+            TEXT.append(data.text)
+
+        if len(PARARGAPH_ID):
+            pa_data = [pa.array(PARARGAPH_ID), pa.array(TEXT)]
+            output_batch = pa.record_batch(pa_data, schema=schema)
+        else:
+            output_batch = None
+        return output_batch
+
+    return func
+
+
+def batch_to_question_answer_streaming_arrow():
+    def func(batch: QuestionAnswerStreamingBatch, schema: pa.schema):
+        QUESTION = []
+        ANSWER = []
+        QUESTION_PARAGRAPHS = []
+        ANSWER_PARAGRAPHS = []
+        QUESTION_LANGUAGE = []
+        ANSWER_LANGUAGE = []
+        CANCELLED_BY_USER = []
+        for data in batch.data:
+            QUESTION.append(data.question.text)
+            ANSWER.append(data.answer.text)
+            QUESTION_PARAGRAPHS.append(
+                [paragraph for paragraph in data.question.paragraphs]
+            )
+            ANSWER_PARAGRAPHS.append(
+                [paragraph for paragraph in data.answer.paragraphs]
+            )
+            QUESTION_LANGUAGE.append(data.question.language)
+            ANSWER_LANGUAGE.append(data.answer.language)
+            CANCELLED_BY_USER.append(data.cancelled_by_user)
+
+        if len(QUESTION):
+            pa_data = [
+                pa.array(QUESTION),
+                pa.array(ANSWER),
+                pa.array(QUESTION_PARAGRAPHS),
+                pa.array(ANSWER_PARAGRAPHS),
+                pa.array(QUESTION_LANGUAGE),
+                pa.array(ANSWER_LANGUAGE),
+                pa.array(CANCELLED_BY_USER),
+            ]
+            output_batch = pa.record_batch(pa_data, schema=schema)
+        else:
+            output_batch = None
+        return output_batch
+
+    return func
```

## nucliadb_dataset/run.py

```diff
@@ -14,77 +14,70 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from pathlib import Path
+from urllib.parse import urlparse
 
 import pydantic_argparse
-from nucliadb_protos.dataset_pb2 import TaskType, TrainSet
+from nucliadb_protos.dataset_pb2 import TrainSet
 
-from nucliadb_dataset import DatasetType, ExportType
+from nucliadb_dataset import ExportType
+from nucliadb_dataset.dataset import TASK_DEFINITIONS
 from nucliadb_dataset.export import FileSystemExport, NucliaDatasetsExport
-from nucliadb_sdk.client import NucliaDBClient
-
-DATASET_TYPE_MAPPING = {
-    DatasetType.FIELD_CLASSIFICATION: TaskType.FIELD_CLASSIFICATION,
-    DatasetType.PARAGRAPH_CLASSIFICATION: TaskType.PARAGRAPH_CLASSIFICATION,
-    DatasetType.SENTENCE_CLASSIFICATION: TaskType.SENTENCE_CLASSIFICATION,
-    DatasetType.TOKEN_CLASSIFICATION: TaskType.TOKEN_CLASSIFICATION,
-}
+from nucliadb_sdk.v2.sdk import NucliaDB
 
 
 def run():
     from nucliadb_dataset.settings import RunningSettings
 
     parser = pydantic_argparse.ArgumentParser(
         model=RunningSettings,
         prog="NucliaDB Datasets",
         description="Generate Arrow files from NucliaDB KBs",
     )
     nucliadb_args = parser.parse_typed_args()
-    errors = []
 
     trainset = TrainSet()
-    trainset.type = DATASET_TYPE_MAPPING[nucliadb_args.type]
+    definition = TASK_DEFINITIONS[nucliadb_args.type]
+    trainset.type = definition.proto
     trainset.batch_size = nucliadb_args.batch_size
-    if nucliadb_args.type in (
-        DatasetType.FIELD_CLASSIFICATION,
-        DatasetType.PARAGRAPH_CLASSIFICATION,
-        DatasetType.SENTENCE_CLASSIFICATION,
-    ):
+    if definition.labels:
         if nucliadb_args.labelset is not None:
             trainset.filter.labels.append(nucliadb_args.labelset)
-    elif nucliadb_args.type in (DatasetType.TOKEN_CLASSIFICATION):
-        if nucliadb_args.families is not None:
-            trainset.filter.labels.extend(nucliadb_args.families)
+
+    nuclia_url_parts = urlparse(nucliadb_args.url)
+    url = f"{nuclia_url_parts.scheme}://{nuclia_url_parts.netloc}/api"
 
     Path(nucliadb_args.download_path).mkdir(parents=True, exist_ok=True)
     if nucliadb_args.export == ExportType.DATASETS:
         if nucliadb_args.apikey is None:
-            errors.append("API key required to push to Nuclia Dataset™")
-        client = NucliaDBClient(
-            environment=nucliadb_args.environment,
-            url=nucliadb_args.url,
+            raise Exception("API key required to push to Nuclia Dataset™")
+        sdk = NucliaDB(
+            region=nucliadb_args.environment,
+            url=url,
             api_key=nucliadb_args.service_token,
         )
         fse = NucliaDatasetsExport(
-            client=client,
+            sdk=sdk,
+            kbid=nucliadb_args.kbid,
             datasets_url=nucliadb_args.datasets_url,
             trainset=trainset,
             cache_path=nucliadb_args.download_path,
             apikey=nucliadb_args.apikey,
         )
         fse.export()
     elif nucliadb_args.export == ExportType.FILESYSTEM:
-        client = NucliaDBClient(
-            environment=nucliadb_args.environment,
-            url=nucliadb_args.url,
+        sdk = NucliaDB(
+            region=nucliadb_args.environment,
+            url=url,
             api_key=nucliadb_args.service_token,
         )
         fse = FileSystemExport(
-            client=client,
+            sdk=sdk,
+            kbid=nucliadb_args.kbid,
             trainset=trainset,
             store_path=nucliadb_args.download_path,
         )
         fse.export()
```

## nucliadb_dataset/settings.py

```diff
@@ -15,21 +15,22 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
 from pathlib import Path
-from typing import List, Optional
+from typing import Optional
 
 import pydantic
 from pydantic import BaseSettings
 
-from nucliadb_dataset import DatasetType, ExportType
-from nucliadb_sdk.client import Environment
+from nucliadb_dataset import ExportType
+from nucliadb_dataset.tasks import Task
+from nucliadb_sdk.v2.sdk import Region
 
 
 class Settings(BaseSettings):
     train_grpc_address: str = "train.nucliadb.svc:8080"
 
 
 settings = Settings()
@@ -39,30 +40,32 @@
     export: ExportType = pydantic.Field(
         ExportType.FILESYSTEM, description="Destination of export"
     )
     download_path: str = pydantic.Field(
         f"{Path.home()}/.nuclia/download", description="Download path"
     )
     url: str = pydantic.Field(description="KnowledgeBox URL")
-    type: DatasetType = pydantic.Field(description="Dataset Type")
+    type: Task = pydantic.Field(description="Dataset Type")
     labelset: Optional[str] = pydantic.Field(
-        description="For classification which labelset"
+        description="For classification which labelset or families"
     )
-    families: Optional[List[str]] = pydantic.Field(description="List of family group")
 
-    datasets_url: Optional[str] = pydantic.Field(
-        description="Base url for the Nuclia datasets component (including /api/v1)™"
+    datasets_url: str = pydantic.Field(
+        "https://europe-1.nuclia.cloud",
+        description="Base url for the Nuclia datasets component (excluding /api/v1)™",  # noqa
     )
 
     apikey: Optional[str] = pydantic.Field(
         description="API key to upload to Nuclia Datasets™"
     )
 
-    environment: Environment = pydantic.Field(
-        Environment.OSS, description="CLOUD or OSS"
+    environment: Region = pydantic.Field(
+        Region.ON_PREM, description="region or on-prem"
     )
 
     service_token: Optional[str] = pydantic.Field(
         description="Service account key to access Nuclia Cloud"
     )
 
     batch_size: int = pydantic.Field(64, description="Batch streaming size")
+
+    kbid: str = pydantic.Field(description="Knowledge Box UUID")
```

## nucliadb_dataset/streamer.py

```diff
@@ -13,63 +13,70 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
-from typing import Optional
+from typing import Dict, Optional
 
 import requests
 from nucliadb_protos.dataset_pb2 import TrainSet
 from urllib3.exceptions import ProtocolError
 
-from nucliadb_sdk.client import NucliaDBClient
-
 SIZE_BYTES = 4
 
 
 class StreamerAlreadyRunning(Exception):
     pass
 
 
 class Streamer:
     resp: Optional[requests.Response]
-    client: NucliaDBClient
 
-    def __init__(self, trainset: TrainSet, client: NucliaDBClient):
-        self.client = client
-        self.base_url = str(self.client.train_session.base_url).strip("/")
+    def __init__(
+        self,
+        trainset: TrainSet,
+        reader_headers: Dict[str, str],
+        base_url: str,
+        kbid: str,
+    ):
+        self.reader_headers = reader_headers
+        self.base_url = base_url
         self.trainset = trainset
+        self.kbid = kbid
         self.resp = None
 
     @property
     def initialized(self):
         return self.resp is not None
 
     def initialize(self, partition_id: str):
-        self.resp = self.client.stream_session.post(
-            f"{self.base_url}/trainset/{partition_id}",
+        self.stream_session = requests.Session()
+        self.stream_session.headers.update(self.reader_headers)
+        self.resp = self.stream_session.post(
+            f"{self.base_url}/v1/kb/{self.kbid}/trainset/{partition_id}",
             data=self.trainset.SerializeToString(),
             stream=True,
         )
 
     def finalize(self):
-        self.resp.close()
+        if self.resp is not None:
+            self.resp.close()
         self.resp = None
 
     def __iter__(self):
         return self
 
     def read(self) -> Optional[bytes]:
         if self.resp is None:
             raise Exception("Not initialized")
         try:
             header = self.resp.raw.read(4, decode_content=True)
-            payload_size = int.from_bytes(header, byteorder="big", signed=False)
+            payload_size = int.from_bytes(header, byteorder="big", signed=False)  # noqa
             data = self.resp.raw.read(payload_size)
         except ProtocolError:
             data = None
         return data
 
     def __next__(self) -> Optional[bytes]:
         payload = self.read()
```

