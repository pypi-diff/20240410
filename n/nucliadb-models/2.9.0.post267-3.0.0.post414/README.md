# Comparing `tmp/nucliadb_models-2.9.0.post267-py3-none-any.whl.zip` & `tmp/nucliadb_models-3.0.0.post414-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,29 +1,33 @@
-Zip file size: 32980 bytes, number of entries: 27
--rw-r--r--  2.0 unx     1273 b- defN 23-May-15 13:40 nucliadb_models/__init__.py
--rw-r--r--  2.0 unx     2314 b- defN 23-May-15 13:40 nucliadb_models/cluster.py
--rw-r--r--  2.0 unx     6240 b- defN 23-May-15 13:40 nucliadb_models/common.py
--rw-r--r--  2.0 unx     3982 b- defN 23-May-15 13:40 nucliadb_models/conversation.py
--rw-r--r--  2.0 unx     1682 b- defN 23-May-15 13:40 nucliadb_models/datetime.py
--rw-r--r--  2.0 unx     3023 b- defN 23-May-15 13:40 nucliadb_models/entities.py
--rw-r--r--  2.0 unx     8090 b- defN 23-May-15 13:40 nucliadb_models/extracted.py
--rw-r--r--  2.0 unx     2191 b- defN 23-May-15 13:40 nucliadb_models/file.py
--rw-r--r--  2.0 unx     1735 b- defN 23-May-15 13:40 nucliadb_models/keywordset.py
--rw-r--r--  2.0 unx     1468 b- defN 23-May-15 13:40 nucliadb_models/labels.py
--rw-r--r--  2.0 unx     3147 b- defN 23-May-15 13:40 nucliadb_models/layout.py
--rw-r--r--  2.0 unx     2177 b- defN 23-May-15 13:40 nucliadb_models/link.py
--rw-r--r--  2.0 unx    11144 b- defN 23-May-15 13:40 nucliadb_models/metadata.py
--rw-r--r--  2.0 unx      998 b- defN 23-May-15 13:40 nucliadb_models/processing.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-15 13:40 nucliadb_models/py.typed
--rw-r--r--  2.0 unx     7361 b- defN 23-May-15 13:40 nucliadb_models/resource.py
--rw-r--r--  2.0 unx    12454 b- defN 23-May-15 13:40 nucliadb_models/search.py
--rw-r--r--  2.0 unx     1904 b- defN 23-May-15 13:40 nucliadb_models/synonyms.py
--rw-r--r--  2.0 unx     2373 b- defN 23-May-15 13:40 nucliadb_models/text.py
--rw-r--r--  2.0 unx     1960 b- defN 23-May-15 13:40 nucliadb_models/utils.py
--rw-r--r--  2.0 unx     2553 b- defN 23-May-15 13:40 nucliadb_models/vectors.py
--rw-r--r--  2.0 unx     4372 b- defN 23-May-15 13:40 nucliadb_models/writer.py
--rw-r--r--  2.0 unx      415 b- defN 23-May-15 13:42 nucliadb_models-2.9.0.post267.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-15 13:42 nucliadb_models-2.9.0.post267.dist-info/WHEEL
--rw-r--r--  2.0 unx       16 b- defN 23-May-15 13:42 nucliadb_models-2.9.0.post267.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 23-May-15 13:41 nucliadb_models-2.9.0.post267.dist-info/zip-safe
--rw-rw-r--  2.0 unx     2288 b- defN 23-May-15 13:42 nucliadb_models-2.9.0.post267.dist-info/RECORD
-27 files, 85253 bytes uncompressed, 29280 bytes compressed:  65.7%
+Zip file size: 46250 bytes, number of entries: 31
+-rw-r--r--  2.0 unx     1379 b- defN 24-Apr-10 13:42 nucliadb_models/__init__.py
+-rw-r--r--  2.0 unx     7296 b- defN 24-Apr-10 13:42 nucliadb_models/common.py
+-rw-r--r--  2.0 unx     1786 b- defN 24-Apr-10 13:42 nucliadb_models/configuration.py
+-rw-r--r--  2.0 unx     4242 b- defN 24-Apr-10 13:42 nucliadb_models/conversation.py
+-rw-r--r--  2.0 unx     1682 b- defN 24-Apr-10 13:42 nucliadb_models/datetime.py
+-rw-r--r--  2.0 unx     3952 b- defN 24-Apr-10 13:42 nucliadb_models/entities.py
+-rw-r--r--  2.0 unx     1255 b- defN 24-Apr-10 13:42 nucliadb_models/export_import.py
+-rw-r--r--  2.0 unx     8873 b- defN 24-Apr-10 13:42 nucliadb_models/extracted.py
+-rw-r--r--  2.0 unx     2191 b- defN 24-Apr-10 13:42 nucliadb_models/file.py
+-rw-r--r--  2.0 unx     1735 b- defN 24-Apr-10 13:42 nucliadb_models/keywordset.py
+-rw-r--r--  2.0 unx     3508 b- defN 24-Apr-10 13:42 nucliadb_models/labels.py
+-rw-r--r--  2.0 unx     3147 b- defN 24-Apr-10 13:42 nucliadb_models/layout.py
+-rw-r--r--  2.0 unx     2581 b- defN 24-Apr-10 13:42 nucliadb_models/link.py
+-rw-r--r--  2.0 unx    12517 b- defN 24-Apr-10 13:42 nucliadb_models/metadata.py
+-rw-r--r--  2.0 unx     4355 b- defN 24-Apr-10 13:42 nucliadb_models/notifications.py
+-rw-r--r--  2.0 unx      998 b- defN 24-Apr-10 13:42 nucliadb_models/processing.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 13:42 nucliadb_models/py.typed
+-rw-r--r--  2.0 unx     9610 b- defN 24-Apr-10 13:42 nucliadb_models/resource.py
+-rw-r--r--  2.0 unx    44060 b- defN 24-Apr-10 13:42 nucliadb_models/search.py
+-rw-r--r--  2.0 unx     1402 b- defN 24-Apr-10 13:42 nucliadb_models/security.py
+-rw-r--r--  2.0 unx     1904 b- defN 24-Apr-10 13:42 nucliadb_models/synonyms.py
+-rw-r--r--  2.0 unx     2785 b- defN 24-Apr-10 13:42 nucliadb_models/text.py
+-rw-r--r--  2.0 unx      933 b- defN 24-Apr-10 13:42 nucliadb_models/trainset.py
+-rw-r--r--  2.0 unx     2181 b- defN 24-Apr-10 13:42 nucliadb_models/utils.py
+-rw-r--r--  2.0 unx     2367 b- defN 24-Apr-10 13:42 nucliadb_models/vectors.py
+-rw-r--r--  2.0 unx     7766 b- defN 24-Apr-10 13:42 nucliadb_models/writer.py
+-rw-r--r--  2.0 unx      657 b- defN 24-Apr-10 13:44 nucliadb_models-3.0.0.post414.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-10 13:44 nucliadb_models-3.0.0.post414.dist-info/WHEEL
+-rw-r--r--  2.0 unx       16 b- defN 24-Apr-10 13:44 nucliadb_models-3.0.0.post414.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 24-Apr-10 13:43 nucliadb_models-3.0.0.post414.dist-info/zip-safe
+-rw-rw-r--  2.0 unx     2639 b- defN 24-Apr-10 13:44 nucliadb_models-3.0.0.post414.dist-info/RECORD
+31 files, 137910 bytes uncompressed, 41998 bytes compressed:  69.5%
```

## zipnote {}

```diff
@@ -1,25 +1,28 @@
 Filename: nucliadb_models/__init__.py
 Comment: 
 
-Filename: nucliadb_models/cluster.py
+Filename: nucliadb_models/common.py
 Comment: 
 
-Filename: nucliadb_models/common.py
+Filename: nucliadb_models/configuration.py
 Comment: 
 
 Filename: nucliadb_models/conversation.py
 Comment: 
 
 Filename: nucliadb_models/datetime.py
 Comment: 
 
 Filename: nucliadb_models/entities.py
 Comment: 
 
+Filename: nucliadb_models/export_import.py
+Comment: 
+
 Filename: nucliadb_models/extracted.py
 Comment: 
 
 Filename: nucliadb_models/file.py
 Comment: 
 
 Filename: nucliadb_models/keywordset.py
@@ -33,50 +36,59 @@
 
 Filename: nucliadb_models/link.py
 Comment: 
 
 Filename: nucliadb_models/metadata.py
 Comment: 
 
+Filename: nucliadb_models/notifications.py
+Comment: 
+
 Filename: nucliadb_models/processing.py
 Comment: 
 
 Filename: nucliadb_models/py.typed
 Comment: 
 
 Filename: nucliadb_models/resource.py
 Comment: 
 
 Filename: nucliadb_models/search.py
 Comment: 
 
+Filename: nucliadb_models/security.py
+Comment: 
+
 Filename: nucliadb_models/synonyms.py
 Comment: 
 
 Filename: nucliadb_models/text.py
 Comment: 
 
+Filename: nucliadb_models/trainset.py
+Comment: 
+
 Filename: nucliadb_models/utils.py
 Comment: 
 
 Filename: nucliadb_models/vectors.py
 Comment: 
 
 Filename: nucliadb_models/writer.py
 Comment: 
 
-Filename: nucliadb_models-2.9.0.post267.dist-info/METADATA
+Filename: nucliadb_models-3.0.0.post414.dist-info/METADATA
 Comment: 
 
-Filename: nucliadb_models-2.9.0.post267.dist-info/WHEEL
+Filename: nucliadb_models-3.0.0.post414.dist-info/WHEEL
 Comment: 
 
-Filename: nucliadb_models-2.9.0.post267.dist-info/top_level.txt
+Filename: nucliadb_models-3.0.0.post414.dist-info/top_level.txt
 Comment: 
 
-Filename: nucliadb_models-2.9.0.post267.dist-info/zip-safe
+Filename: nucliadb_models-3.0.0.post414.dist-info/zip-safe
 Comment: 
 
-Filename: nucliadb_models-2.9.0.post267.dist-info/RECORD
+Filename: nucliadb_models-3.0.0.post414.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nucliadb_models/__init__.py

```diff
@@ -18,18 +18,21 @@
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import logging
 
 from .common import *  # noqa
 from .conversation import *  # noqa
 from .datetime import *  # noqa
+from .export_import import *  # noqa
 from .extracted import *  # noqa
 from .file import *  # noqa
 from .keywordset import *  # noqa
 from .layout import *  # noqa
 from .link import *  # noqa
 from .metadata import *  # noqa
+from .notifications import *  # noqa
 from .processing import *  # noqa
+from .security import *  # noqa
 from .text import *  # noqa
 from .writer import *  # noqa
 
 logger = logging.getLogger("nucliadb_models")
```

## nucliadb_models/common.py

```diff
@@ -17,15 +17,15 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import base64
 import hashlib
 import re
 from enum import Enum
-from typing import Dict, List, Optional
+from typing import Any, Dict, List, Optional
 
 from pydantic import BaseModel, Field, root_validator
 
 from nucliadb_protos import resources_pb2
 
 FIELD_TYPE_CHAR_MAP = {
     "c": "conversation",
@@ -42,14 +42,37 @@
     r"/?kbs/(?P<kbid>[^/]+)/r/(?P<rid>[^/]+)/(?P<download_type>[fe])/(?P<field_type>\w)/(?P<field_id>[^/]+)/?(?P<key>.*)?"  # noqa
 )
 DOWNLOAD_TYPE_MAP = {"f": "field", "e": "extracted"}
 DOWNLOAD_URI = (
     "/kb/{kbid}/resource/{rid}/{field_type}/{field_id}/download/{download_type}/{key}"
 )
 
+_NOT_SET = object()
+
+
+class ParamDefault(BaseModel):
+    default: Any
+    title: str
+    description: str
+    gt: Optional[float] = None
+    max_items: Optional[int] = None
+
+    def to_pydantic_field(self, default=_NOT_SET) -> Field:  # type: ignore
+        """
+        :param default: to be able to override default value - as some params
+        are reused but they will have different default values depending on the endpoint.
+        """
+        return Field(
+            default=self.default if default is _NOT_SET else default,
+            title=self.title,
+            description=self.description,
+            gt=self.gt,
+            max_items=self.max_items,
+        )
+
 
 class FieldID(BaseModel):
     class FieldType(Enum):
         FILE = "file"
         LINK = "link"
         DATETIME = "datetime"
         KEYWORDSET = "keywordset"
@@ -62,17 +85,17 @@
     field: str
 
 
 class File(BaseModel):
     filename: Optional[str]
     content_type: str = "application/octet-stream"
     payload: Optional[str] = Field(description="Base64 encoded file content")
-    md5: Optional[str]
+    md5: Optional[str] = None
     # These are to be used for external files
-    uri: Optional[str]
+    uri: Optional[str] = None
     extra_headers: Dict[str, str] = {}
 
     @root_validator(pre=False)
     def _check_internal_file_fields(cls, values):
         if values.get("uri"):
             # Externally hosted file
             return values
@@ -201,21 +224,38 @@
     key: Optional[str]
 
 
 class Shards(BaseModel):
     shards: Optional[List[str]]
 
 
-FIELD_TYPES_MAP: Dict[int, FieldTypeName] = {
-    resources_pb2.FieldType.FILE: FieldTypeName.FILE,
+FIELD_TYPES_MAP: Dict[resources_pb2.FieldType.ValueType, FieldTypeName] = {
     resources_pb2.FieldType.LINK: FieldTypeName.LINK,
+    resources_pb2.FieldType.FILE: FieldTypeName.FILE,
     resources_pb2.FieldType.DATETIME: FieldTypeName.DATETIME,
     resources_pb2.FieldType.KEYWORDSET: FieldTypeName.KEYWORDSET,
     resources_pb2.FieldType.TEXT: FieldTypeName.TEXT,
     resources_pb2.FieldType.LAYOUT: FieldTypeName.LAYOUT,
     resources_pb2.FieldType.GENERIC: FieldTypeName.GENERIC,
     resources_pb2.FieldType.CONVERSATION: FieldTypeName.CONVERSATION,
 }
 
-FIELD_TYPES_MAP_REVERSE: Dict[str, int] = {
+FIELD_TYPES_MAP_REVERSE: Dict[str, resources_pb2.FieldType.ValueType] = {
     y.value: x for x, y in FIELD_TYPES_MAP.items()  # type: ignore
 }
+
+
+class Question(BaseModel):
+    text: str
+    language: Optional[str] = None
+    ids_paragraphs: List[str]
+
+
+class Answer(BaseModel):
+    text: str
+    language: Optional[str] = None
+    ids_paragraphs: List[str]
+
+
+class QuestionAnswer(BaseModel):
+    question: Question
+    answers: List[Answer]
```

## nucliadb_models/conversation.py

```diff
@@ -18,15 +18,15 @@
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from datetime import datetime
 from enum import Enum
 from typing import TYPE_CHECKING, List, Optional, Type, TypeVar
 
 from google.protobuf.json_format import MessageToDict
-from pydantic import BaseModel
+from pydantic import BaseModel, Field
 
 from nucliadb_models import CloudLink, FileB64
 from nucliadb_protos import resources_pb2
 
 _T = TypeVar("_T")
 
 
@@ -40,31 +40,39 @@
 
 
 class MessageFormat(Enum):  # type: ignore
     PLAIN = "PLAIN"
     HTML = "HTML"
     RST = "RST"
     MARKDOWN = "MARKDOWN"
+    KEEP_MARKDOWN = "KEEP_MARKDOWN"
 
 
 # Visualization classes (Those used on reader endpoints)
 
 
 class MessageContent(BaseModel):
     text: Optional[str]
     format: Optional[MessageFormat]
     attachments: Optional[List[CloudLink]]
 
 
+class MessageType(Enum):
+    UNSET = "UNSET"
+    QUESTION = "QUESTION"
+    ANSWER = "ANSWER"
+
+
 class Message(BaseModel):
     timestamp: Optional[datetime] = None
     who: Optional[str] = None
     to: Optional[List[str]] = []
     content: MessageContent
     ident: Optional[str]
+    type_: Optional[MessageType] = Field(None, alias="type")
 
 
 class Conversation(BaseModel):
     """
     This is the real conversation object that will be used when visualizing
     a conversation in the field level.
     """
@@ -116,14 +124,15 @@
 
 class InputMessage(BaseModel):
     timestamp: Optional[datetime] = None
     who: Optional[str] = None
     to: List[str] = []
     content: InputMessageContent
     ident: str
+    type_: Optional[MessageType] = Field(None, alias="type")
 
 
 class InputConversationField(BaseModel):
     messages: List[InputMessage] = []
 
 
 # Processing classes (Those used to sent to push endpoints)
```

## nucliadb_models/entities.py

```diff
@@ -39,57 +39,80 @@
             message,
             preserving_proto_field_name=True,
             including_default_value_fields=True,
         )
         return cls(**entity)
 
 
-class EntitiesGroup(BaseModel):
-    entities: Dict[str, Entity] = {}
-    title: Optional[str] = None
-    color: Optional[str] = None
-
+class EntitiesGroupSummary(BaseModel):
+    title: Optional[str] = Field(
+        default=None, description="Title of the entities group"
+    )
+    color: Optional[str] = Field(
+        default=None,
+        description="Color of the entities group. This is for display purposes only.",
+    )
     custom: bool = Field(
         default=False, description="Denotes if it has been created by the user"
     )
 
+    entities: Dict[str, Entity] = Field(
+        default={},
+        title="[Deprecated] Entities in the group",
+        description="This field is deprecated and will be removed in future versions. It will always be empty. Use the /api/v1/kb/{kbid}/entitiesgroup/{group} endpoint to get the entities of a group.",  # noqa: E501
+    )
+
     @classmethod
     def from_message(
         cls: Type[_T],
-        message: knowledgebox_pb2.EntitiesGroup,
+        message: knowledgebox_pb2.EntitiesGroupSummary,
     ) -> _T:
-        entities_group = MessageToDict(
+        entitiesgroup = MessageToDict(
             message,
             preserving_proto_field_name=True,
             including_default_value_fields=True,
         )
-        entities_group["entities"] = {}
+        return cls(**entitiesgroup)
 
-        for name, entity in message.entities.items():
-            if not entity.deleted:
-                entities_group["entities"][name] = Entity.from_message(entity)
 
-        return cls(**entities_group)
+class EntitiesGroup(BaseModel):
+    title: Optional[str] = Field(
+        default=None, description="Title of the entities group"
+    )
+    color: Optional[str] = Field(
+        default=None,
+        description="Color of the entities group. This is for display purposes only.",
+    )
+    custom: bool = Field(
+        default=False, description="Denotes if it has been created by the user"
+    )
+    entities: Dict[str, Entity] = {}
 
     @classmethod
-    def from_summary_message(
+    def from_message(
         cls: Type[_T],
-        message: knowledgebox_pb2.EntitiesGroupSummary,
+        message: knowledgebox_pb2.EntitiesGroup,
     ) -> _T:
-        entitiesgroup = MessageToDict(
+        entities_group = MessageToDict(
             message,
             preserving_proto_field_name=True,
             including_default_value_fields=True,
         )
-        return cls(**entitiesgroup)
+        entities_group["entities"] = {}
+
+        for name, entity in message.entities.items():
+            if not entity.deleted:
+                entities_group["entities"][name] = Entity.from_message(entity)
+
+        return cls(**entities_group)
 
 
 class KnowledgeBoxEntities(BaseModel):
     uuid: str
-    groups: Dict[str, EntitiesGroup] = {}
+    groups: Dict[str, EntitiesGroupSummary] = {}
 
 
 class CreateEntitiesGroupPayload(BaseModel):
     group: str
     entities: Dict[str, Entity] = {}
     title: Optional[str] = None
     color: Optional[str] = None
```

## nucliadb_models/extracted.py

```diff
@@ -21,24 +21,31 @@
 from typing import Any, Dict, List, Optional, Type, TypeVar
 
 from google.protobuf.json_format import MessageToDict
 from pydantic import BaseModel
 
 from nucliadb_protos import resources_pb2
 
-from .common import Classification, CloudFile, CloudLink, FieldID, Paragraph
+from .common import (
+    Classification,
+    CloudFile,
+    CloudLink,
+    FieldID,
+    Paragraph,
+    QuestionAnswer,
+)
 from .metadata import Relation, convert_pb_relation_to_api
 
 _T = TypeVar("_T")
 
 
 class ExtractedText(BaseModel):
     text: Optional[str]
-    split_text: Optional[Dict[str, str]]
-    deleted_splits: Optional[List[str]]
+    split_text: Optional[Dict[str, str]] = None
+    deleted_splits: Optional[List[str]] = None
 
     @classmethod
     def from_message(cls: Type[_T], message: resources_pb2.ExtractedText) -> _T:
         return cls(
             **MessageToDict(
                 message,
                 preserving_proto_field_name=True,
@@ -238,17 +245,37 @@
 
 
 class PagePositions(BaseModel):
     start: Optional[int]
     end: Optional[int]
 
 
+class PageStructurePage(BaseModel):
+    width: int
+    height: int
+
+
+class PageStructureToken(BaseModel):
+    x: float
+    y: float
+    width: float
+    height: float
+    text: str
+    line: float
+
+
+class PageStructure(BaseModel):
+    page: PageStructurePage
+    tokens: List[PageStructureToken]
+
+
 class FilePages(BaseModel):
     pages: Optional[List[CloudLink]]
     positions: Optional[List[PagePositions]]
+    structures: Optional[List[PageStructure]]
 
 
 class FileExtractedData(BaseModel):
     language: Optional[str]
     md5: Optional[str]
     metadata: Optional[Dict[str, str]]
     nested: Optional[Dict[str, str]]
@@ -267,14 +294,28 @@
         return cls(
             **MessageToDict(
                 message,
                 preserving_proto_field_name=True,
                 including_default_value_fields=True,
             )
         )
+
+
+class QuestionAnswers(BaseModel):
+    question_answer: List[QuestionAnswer]
+
+    @classmethod
+    def from_message(cls: Type[_T], message: resources_pb2.QuestionAnswers) -> _T:
+        return cls(
+            **MessageToDict(
+                message,
+                preserving_proto_field_name=True,
+                including_default_value_fields=True,
+            )
+        )
 
 
 def convert_fieldmetadata_pb_to_dict(
     message: resources_pb2.FieldMetadata,
 ) -> Dict[str, Any]:
     value = MessageToDict(
         message,
```

## nucliadb_models/labels.py

```diff
@@ -19,19 +19,81 @@
 #
 
 from enum import Enum
 from typing import Dict, List, Optional
 
 from pydantic import BaseModel
 
+BASE_LABELS: dict[str, list[str]] = {
+    "t": [],  # doc tags
+    "l": [],  # doc labels
+    "n": [],  # type of element: i (Icon). s (Processing Status)
+    "e": [],  # entities e/type/entityid
+    "s": [],  # languages p (Principal) s (ALL)
+    "u": [],  # contributors s (Source) o (Origin)
+    "f": [],  # field keyword field (field/keyword)
+    "fg": [],  # field keyword (keywords) flat
+    "m": [],  # origin metadata in the form of (key/value). Max key/value size is 255
+    "p": [],  # origin metadata in the form of (key/value). Max key/value size is 255
+    "k": [],  # kind of text paragraph to be stored
+}
+
+
+LABEL_QUERY_ALIASES = {
+    # aliases to make querying labels easier
+    "icon": "n/i",
+    "metadata.status": "n/s",
+    "metadata.language": "s/p",
+    "metadata.languages": "s/s",
+    "origin.tags": "t",
+    "origin.metadata": "m",
+    "origin.path": "p",
+    "classification.labels": "l",
+    "entities": "e",
+    "field": "f",
+    "field-values": "fg",
+}
+
+LABEL_QUERY_ALIASES_REVERSED = {v: k for k, v in LABEL_QUERY_ALIASES.items()}
+
+
+def translate_alias_to_system_label(label: str) -> str:
+    parts = label.split("/")
+    if parts[1] in LABEL_QUERY_ALIASES:
+        parts = [""] + [LABEL_QUERY_ALIASES[parts[1]]] + parts[2:]
+        return "/".join(parts)
+    else:
+        return label
+
+
+def translate_system_to_alias_label(label: str) -> str:
+    parts = label.split("/")
+    if parts[1] in LABEL_QUERY_ALIASES_REVERSED:
+        parts = [""] + [LABEL_QUERY_ALIASES_REVERSED[parts[1]]] + parts[2:]
+        return "/".join(parts)
+    elif "/".join(parts[1:3]) in LABEL_QUERY_ALIASES_REVERSED:
+        parts = [""] + [LABEL_QUERY_ALIASES_REVERSED["/".join(parts[1:3])]] + parts[3:]
+        return "/".join(parts)
+    else:
+        return label
+
+
+def flatten_resource_labels(tags_dict: dict[str, list[str]]) -> list[str]:
+    flat_tags = []
+    for key, values in tags_dict.items():
+        for value in values:
+            flat_tags.append(f"/{key}/{value}")
+    return flat_tags
+
 
 class LabelSetKind(str, Enum):
     RESOURCES = "RESOURCES"
     PARAGRAPHS = "PARAGRAPHS"
     SENTENCES = "SENTENCES"
+    SELECTIONS = "SELECTIONS"
 
 
 class Label(BaseModel):
     title: str
     related: Optional[str] = None
     text: Optional[str] = None
     uri: Optional[str] = None
```

## nucliadb_models/link.py

```diff
@@ -17,15 +17,15 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from datetime import datetime
 from typing import Dict, Optional, Type, TypeVar
 
 from google.protobuf.json_format import MessageToDict
-from pydantic import BaseModel
+from pydantic import BaseModel, Field
 
 from nucliadb_protos import resources_pb2
 
 _T = TypeVar("_T")
 
 
 # Shared classes
@@ -38,14 +38,16 @@
 class FieldLink(BaseModel):
     added: Optional[datetime]
     headers: Optional[Dict[str, str]]
     cookies: Optional[Dict[str, str]]
     uri: Optional[str]
     language: Optional[str]
     localstorage: Optional[Dict[str, str]]
+    css_selector: Optional[str]
+    xpath: Optional[str]
 
     @classmethod
     def from_message(cls: Type[_T], message: resources_pb2.FieldLink) -> _T:
         return cls(
             **MessageToDict(
                 message,
                 preserving_proto_field_name=True,
@@ -59,17 +61,29 @@
 
 class LinkField(BaseModel):
     headers: Optional[Dict[str, str]] = {}
     cookies: Optional[Dict[str, str]] = {}
     uri: str
     language: Optional[str] = None
     localstorage: Optional[Dict[str, str]] = {}
+    css_selector: Optional[str] = None
+    xpath: Optional[str] = None
 
 
 # Processing classes (Those used to sent to push endpoints)
 
 
 class LinkUpload(BaseModel):
     link: str
     headers: Dict[str, str] = {}
     cookies: Dict[str, str] = {}
     localstorage: Dict[str, str] = {}
+    css_selector: Optional[str] = Field(
+        None,
+        title="Css selector",
+        description="Css selector to parse the link",
+    )
+    xpath: Optional[str] = Field(
+        None,
+        title="Xpath",
+        description="Xpath to parse the link",
+    )
```

## nucliadb_models/metadata.py

```diff
@@ -18,21 +18,21 @@
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from datetime import datetime
 from enum import Enum
 from typing import Any, Dict, List, Optional, Type, TypeVar
 
 from google.protobuf.json_format import MessageToDict
-from pydantic import BaseModel
+from pydantic import BaseModel, Field
 from pydantic.class_validators import root_validator
 
 from nucliadb_models.common import FIELD_TYPES_MAP
 from nucliadb_protos import resources_pb2, utils_pb2
 
-from .common import Classification, FieldID, UserClassification
+from .common import Classification, FieldID, QuestionAnswer, UserClassification
 
 _T = TypeVar("_T")
 
 
 class EntityRelation(BaseModel):
     entity: str
     entity_type: str
@@ -230,15 +230,15 @@
     def from_message(cls: Type[_T], message: resources_pb2.ComputedMetadata) -> _T:
         values: Dict[str, List[FieldClassification]] = {"field_classifications": []}
         for fc in message.field_classifications:
             values["field_classifications"].append(
                 FieldClassification(
                     field=FieldID(
                         field=fc.field.field,
-                        field_type=FIELD_TYPES_MAP[fc.field.field_type],
+                        field_type=FIELD_TYPES_MAP[fc.field.field_type],  # type: ignore
                     ),
                     classifications=[
                         Classification(label=c.label, labelset=c.labelset)
                         for c in fc.classifications
                     ],
                 )
             )
@@ -271,31 +271,61 @@
 
 
 class ParagraphAnnotation(BaseModel):
     classifications: List[UserClassification] = []
     key: str
 
 
+class QuestionAnswerAnnotation(BaseModel):
+    question_answer: QuestionAnswer
+    cancelled_by_user: bool = False
+
+
+class VisualSelection(BaseModel):
+    label: str
+    top: float
+    left: float
+    right: float
+    bottom: float
+    token_ids: List[int]
+
+
+class PageSelections(BaseModel):
+    page: int
+    visual: List[VisualSelection]
+
+
 class UserFieldMetadata(BaseModel):
     """
     Field-level metadata set by the user via the rest api
     """
 
     token: List[TokenSplit] = []
     paragraphs: List[ParagraphAnnotation] = []
+    selections: List[PageSelections] = []
+    question_answers: List[QuestionAnswerAnnotation] = []
     field: FieldID
 
     @classmethod
     def from_message(cls: Type[_T], message: resources_pb2.UserFieldMetadata) -> _T:
         value = MessageToDict(
             message,
             preserving_proto_field_name=True,
             including_default_value_fields=True,
             use_integers_for_enums=True,
         )
+        value["selections"] = [
+            MessageToDict(
+                selections,
+                preserving_proto_field_name=True,
+                including_default_value_fields=True,
+                use_integers_for_enums=True,
+            )
+            for selections in message.page_selections
+        ]
         value["field"]["field_type"] = FIELD_TYPES_MAP[value["field"]["field_type"]]
         return cls(**value)
 
 
 class Basic(BaseModel):
     icon: Optional[str]
     title: Optional[str]
@@ -320,14 +350,15 @@
     modified: Optional[datetime] = None
     metadata: Dict[str, str] = {}
     tags: List[str] = []
     collaborators: List[str] = []
     # old field was "colaborators"
     filename: Optional[str] = None
     related: List[str] = []
+    path: Optional[str] = None
 
 
 class Origin(InputOrigin):
     class Source(Enum):
         WEB = "WEB"
         DESKTOP = "DESKTOP"
         API = "API"
@@ -344,9 +375,27 @@
         )
         # old field was "colaborators" and we want to keep pb field name
         # to avoid migration
         data["collaborators"] = data.pop("colaborators", [])
         return cls(**data)
 
 
+class Extra(BaseModel):
+    metadata: Dict[Any, Any] = Field(
+        ...,
+        title="Metadata",
+        description="Arbitrary JSON metadata provided by the user that is not meant to be searchable, but can be serialized on results.",  # noqa
+    )
+
+    @classmethod
+    def from_message(cls: Type[_T], message: resources_pb2.Extra) -> _T:
+        return cls(
+            **MessageToDict(
+                message,
+                preserving_proto_field_name=True,
+                including_default_value_fields=False,
+            )
+        )
+
+
 class Relations(BaseModel):
     relations: Optional[List[Relation]]
```

## nucliadb_models/resource.py

```diff
@@ -17,45 +17,49 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
 import string
 from datetime import datetime
 from enum import Enum
-from typing import Dict, List, Optional, Type, TypeVar, Union
+from typing import Any, Dict, List, Optional, Type, TypeVar, Union
 
 from google.protobuf.json_format import MessageToDict
 from nucliadb_protos.knowledgebox_pb2 import KnowledgeBoxConfig as PBKnowledgeBoxConfig
-from pydantic import BaseModel, validator
+from nucliadb_protos.utils_pb2 import ReleaseChannel as PBReleaseChannel
+from pydantic import BaseModel, Field, validator
 
 from nucliadb_models.conversation import FieldConversation
 from nucliadb_models.datetime import FieldDatetime
 from nucliadb_models.extracted import (
     ExtractedText,
     FieldComputedMetadata,
     FileExtractedData,
     LargeComputedMetadata,
     LinkExtractedData,
+    QuestionAnswers,
     VectorObject,
 )
 from nucliadb_models.file import FieldFile
 from nucliadb_models.keywordset import FieldKeywordset
 from nucliadb_models.layout import FieldLayout
 from nucliadb_models.link import FieldLink
 from nucliadb_models.metadata import (
     ComputedMetadata,
+    Extra,
     Metadata,
     Origin,
     Relation,
     UserFieldMetadata,
     UserMetadata,
 )
+from nucliadb_models.security import ResourceSecurity
 from nucliadb_models.text import FieldText
 from nucliadb_models.utils import SlugString
-from nucliadb_models.vectors import UserVectorSet, VectorSimilarity
+from nucliadb_models.vectors import SemanticModelMetadata, VectorSimilarity
 
 _T = TypeVar("_T")
 
 
 class NucliaDBRoles(str, Enum):
     MANAGER = "MANAGER"
     READER = "READER"
@@ -72,25 +76,63 @@
     TEXT = "text"
     METADATA = "metadata"
     SHORTENED_METADATA = "shortened_metadata"
     LARGE_METADATA = "large_metadata"
     VECTOR = "vectors"
     LINK = "link"
     FILE = "file"
-    USERVECTORS = "uservectors"
+    QA = "question_answers"
+
+
+class ReleaseChannel(str, Enum):
+    STABLE = "STABLE"
+    EXPERIMENTAL = "EXPERIMENTAL"
+
+    def to_pb(self) -> PBReleaseChannel.ValueType:
+        return RELEASE_CHANNEL_ENUM_TO_PB[self.value]
+
+    @classmethod
+    def from_message(cls, message: PBReleaseChannel.ValueType):
+        return cls(RELEASE_CHANNEL_PB_TO_ENUM[message])
+
+
+RELEASE_CHANNEL_ENUM_TO_PB = {
+    ReleaseChannel.STABLE.value: PBReleaseChannel.STABLE,
+    ReleaseChannel.EXPERIMENTAL.value: PBReleaseChannel.EXPERIMENTAL,
+}
+RELEASE_CHANNEL_PB_TO_ENUM = {v: k for k, v in RELEASE_CHANNEL_ENUM_TO_PB.items()}
 
 
 class KnowledgeBoxConfig(BaseModel):
-    slug: Optional[SlugString] = None
-    title: Optional[str] = None
-    description: Optional[str] = None
-    enabled_filters: List[str] = []
-    enabled_insights: List[str] = []
-    disable_vectors: bool = False
-    similarity: Optional[VectorSimilarity]
+    slug: Optional[SlugString] = Field(
+        default=None, title="Slug", description="Slug for the Knowledge Box."
+    )
+    title: Optional[str] = Field(
+        default=None, title="Title", description="Title for the Knowledge Box."
+    )
+    description: Optional[str] = Field(
+        default=None,
+        title="Description",
+        description="Description for the Knowledge Box.",
+    )
+    release_channel: Optional[ReleaseChannel] = Field(
+        default=None,
+        title="Release Channel",
+        description="Release channel for the Knowledge Box.",
+    )
+    learning_configuration: Optional[Dict[str, Any]] = Field(
+        default=None,
+        title="Learning Configuration",
+        description="Learning configuration for the Knowledge Box. If provided, NucliaDB will set the learning configuration for the Knowledge Box.",  # noqa: E501
+    )
+
+    similarity: Optional[VectorSimilarity] = Field(
+        default=None,
+        description="This field is deprecated. Use 'learning_configuration' instead.",
+    )
 
     @validator("slug")
     def id_check(cls, v: str) -> str:
         for char in v:
             if char in string.ascii_uppercase:
                 raise ValueError("No uppercase ID")
             if char in "&@ /\\ ":
@@ -113,44 +155,49 @@
 
 
 class KnowledgeBoxObjID(BaseModel):
     uuid: str
 
 
 class KnowledgeBoxObj(BaseModel):
+    """
+    The API representation of a Knowledge Box object.
+    """
+
     slug: Optional[SlugString] = None
     uuid: str
     config: Optional[KnowledgeBoxConfig] = None
+    model: Optional[SemanticModelMetadata] = None
 
 
 class KnowledgeBoxList(BaseModel):
     kbs: List[KnowledgeBoxObjSummary] = []
 
 
 # Resources
 
 
 class ExtractedData(BaseModel):
-    text: Optional[ExtractedText]
-    metadata: Optional[FieldComputedMetadata]
-    large_metadata: Optional[LargeComputedMetadata]
-    vectors: Optional[VectorObject]
-    uservectors: Optional[UserVectorSet]
+    text: Optional[ExtractedText] = None
+    metadata: Optional[FieldComputedMetadata] = None
+    large_metadata: Optional[LargeComputedMetadata] = None
+    vectors: Optional[VectorObject] = None
+    question_answers: Optional[QuestionAnswers] = None
 
 
 class TextFieldExtractedData(ExtractedData):
     pass
 
 
 class FileFieldExtractedData(ExtractedData):
-    file: Optional[FileExtractedData]
+    file: Optional[FileExtractedData] = None
 
 
 class LinkFieldExtractedData(ExtractedData):
-    link: Optional[LinkExtractedData]
+    link: Optional[LinkExtractedData] = None
 
 
 class LayoutFieldExtractedData(ExtractedData):
     pass
 
 
 class ConversationFieldExtractedData(ExtractedData):
@@ -179,106 +226,112 @@
 
 
 class Error(BaseModel):
     body: str
     code: int
 
 
-class FieldData(BaseModel):
-    ...
+class FieldData(BaseModel): ...
 
 
 class TextFieldData(BaseModel):
-    value: Optional[FieldText]
-    extracted: Optional[TextFieldExtractedData]
-    error: Optional[Error]
+    value: Optional[FieldText] = None
+    extracted: Optional[TextFieldExtractedData] = None
+    error: Optional[Error] = None
 
 
 class FileFieldData(BaseModel):
-    value: Optional[FieldFile]
-    extracted: Optional[FileFieldExtractedData]
-    error: Optional[Error]
+    value: Optional[FieldFile] = None
+    extracted: Optional[FileFieldExtractedData] = None
+    error: Optional[Error] = None
 
 
 class LinkFieldData(BaseModel):
-    value: Optional[FieldLink]
-    extracted: Optional[LinkFieldExtractedData]
-    error: Optional[Error]
+    value: Optional[FieldLink] = None
+    extracted: Optional[LinkFieldExtractedData] = None
+    error: Optional[Error] = None
 
 
 class LayoutFieldData(BaseModel):
-    value: Optional[FieldLayout]
-    extracted: Optional[LayoutFieldExtractedData]
-    error: Optional[Error]
+    value: Optional[FieldLayout] = None
+    extracted: Optional[LayoutFieldExtractedData] = None
+    error: Optional[Error] = None
 
 
 class ConversationFieldData(BaseModel):
-    value: Optional[FieldConversation]
-    extracted: Optional[ConversationFieldExtractedData]
-    error: Optional[Error]
+    value: Optional[FieldConversation] = None
+    extracted: Optional[ConversationFieldExtractedData] = None
+    error: Optional[Error] = None
 
 
 class KeywordsetFieldData(BaseModel):
-    value: Optional[FieldKeywordset]
-    extracted: Optional[KeywordsetFieldExtractedData]
-    error: Optional[Error]
+    value: Optional[FieldKeywordset] = None
+    extracted: Optional[KeywordsetFieldExtractedData] = None
+    error: Optional[Error] = None
 
 
 class DatetimeFieldData(BaseModel):
-    value: Optional[FieldDatetime]
-    extracted: Optional[DatetimeFieldExtractedData]
-    error: Optional[Error]
+    value: Optional[FieldDatetime] = None
+    extracted: Optional[DatetimeFieldExtractedData] = None
+    error: Optional[Error] = None
 
 
 class GenericFieldData(BaseModel):
-    value: Optional[str]
-    extracted: Optional[TextFieldExtractedData]
-    error: Optional[Error]
+    value: Optional[str] = None
+    extracted: Optional[TextFieldExtractedData] = None
+    error: Optional[Error] = None
 
 
 class ResourceData(BaseModel):
-    texts: Optional[Dict[str, TextFieldData]]
-    files: Optional[Dict[str, FileFieldData]]
-    links: Optional[Dict[str, LinkFieldData]]
-    layouts: Optional[Dict[str, LayoutFieldData]]
-    conversations: Optional[Dict[str, ConversationFieldData]]
-    keywordsets: Optional[Dict[str, KeywordsetFieldData]]
-    datetimes: Optional[Dict[str, DatetimeFieldData]]
-    generics: Optional[Dict[str, GenericFieldData]]
+    texts: Optional[Dict[str, TextFieldData]] = None
+    files: Optional[Dict[str, FileFieldData]] = None
+    links: Optional[Dict[str, LinkFieldData]] = None
+    layouts: Optional[Dict[str, LayoutFieldData]] = None
+    conversations: Optional[Dict[str, ConversationFieldData]] = None
+    keywordsets: Optional[Dict[str, KeywordsetFieldData]] = None
+    datetimes: Optional[Dict[str, DatetimeFieldData]] = None
+    generics: Optional[Dict[str, GenericFieldData]] = None
 
 
 class QueueType(str, Enum):  # type: ignore
     PRIVATE = "private"
     SHARED = "shared"
 
 
 class Resource(BaseModel):
     id: str
 
     # This first block of attributes correspond to Basic fields
-    slug: Optional[str]
-    title: Optional[str]
-    summary: Optional[str]
-    icon: Optional[str]
-    layout: Optional[str]
-    thumbnail: Optional[str]
-    metadata: Optional[Metadata]
-    usermetadata: Optional[UserMetadata]
-    fieldmetadata: Optional[List[UserFieldMetadata]]
-    computedmetadata: Optional[ComputedMetadata]
-    created: Optional[datetime]
-    modified: Optional[datetime]
-    last_seqid: Optional[int]
-    last_account_seq: Optional[int]
-    queue: Optional[QueueType]
-
-    origin: Optional[Origin]
-    relations: Optional[List[Relation]]
-
-    data: Optional[ResourceData]
+    slug: Optional[str] = None
+    title: Optional[str] = None
+    summary: Optional[str] = None
+    icon: Optional[str] = None
+    layout: Optional[str] = None
+    thumbnail: Optional[str] = None
+    metadata: Optional[Metadata] = None
+    usermetadata: Optional[UserMetadata] = None
+    fieldmetadata: Optional[List[UserFieldMetadata]] = None
+    computedmetadata: Optional[ComputedMetadata] = None
+    created: Optional[datetime] = None
+    modified: Optional[datetime] = None
+    last_seqid: Optional[int] = None
+    last_account_seq: Optional[int] = None
+    queue: Optional[QueueType] = None
+
+    origin: Optional[Origin] = None
+    extra: Optional[Extra] = None
+    relations: Optional[List[Relation]] = None
+
+    data: Optional[ResourceData] = None
+
+    security: Optional[ResourceSecurity] = Field(
+        default=None,
+        title="Security",
+        description="Resource security metadata",
+    )
 
 
 class ResourcePagination(BaseModel):
     page: int
     size: int
     last: bool
```

## nucliadb_models/search.py

```diff
@@ -16,58 +16,70 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from dataclasses import dataclass
 from datetime import datetime
 from enum import Enum
-from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union
+from typing import Any, Dict, List, Literal, Optional, Type, TypeVar, Union
 
 from google.protobuf.json_format import MessageToDict
 from nucliadb_protos.audit_pb2 import ClientType
 from nucliadb_protos.nodereader_pb2 import DocumentScored, OrderBy
 from nucliadb_protos.nodereader_pb2 import ParagraphResult as PBParagraphResult
 from nucliadb_protos.utils_pb2 import RelationNode
 from nucliadb_protos.writer_pb2 import ShardObject as PBShardObject
 from nucliadb_protos.writer_pb2 import Shards as PBShards
-from pydantic import BaseModel, Field, validator
+from pydantic import BaseModel, Field, root_validator, validator
+from typing_extensions import Annotated
 
-from nucliadb_models.common import FieldTypeName
+from nucliadb_models.common import FieldTypeName, ParamDefault
 from nucliadb_models.metadata import RelationType, ResourceProcessingStatus
 from nucliadb_models.resource import ExtractedDataTypeName, Resource
-from nucliadb_models.vectors import VectorSimilarity
+from nucliadb_models.security import RequestSecurity
+from nucliadb_models.vectors import SemanticModelMetadata, VectorSimilarity
 
 _T = TypeVar("_T")
 
 
+class ModelParamDefaults:
+    applied_autofilters = ParamDefault(
+        default=[],
+        title="Autofilters",
+        description="List of filters automatically applied to the search query",
+    )
+
+
 class ResourceProperties(str, Enum):
     BASIC = "basic"
     ORIGIN = "origin"
+    EXTRA = "extra"
     RELATIONS = "relations"
     VALUES = "values"
     EXTRACTED = "extracted"
     ERRORS = "errors"
+    SECURITY = "security"
 
 
 class SearchOptions(str, Enum):
     PARAGRAPH = "paragraph"
     DOCUMENT = "document"
     RELATIONS = "relations"
     VECTOR = "vector"
 
 
 class ChatOptions(str, Enum):
+    VECTORS = "vectors"
     PARAGRAPHS = "paragraphs"
     RELATIONS = "relations"
 
 
 class SuggestOptions(str, Enum):
     PARAGRAPH = "paragraph"
     ENTITIES = "entities"
-    INTENT = "intent"
 
 
 class NucliaDBClientType(str, Enum):
     API = "api"
     WIDGET = "widget"
     WEB = "web"
     DASHBOARD = "dashboard"
@@ -79,23 +91,32 @@
 
 
 class Sort(int, Enum):
     DESC = 0
     ASC = 1
 
 
+class JsonBaseModel(BaseModel):
+    def __str__(self):
+        try:
+            return self.json()
+        except Exception:
+            # fallback to BaseModel implementation
+            return super().__str__()
+
+
 class Facet(BaseModel):
     facetresults: Dict[str, int]
 
 
 FacetsResult = Dict[str, Any]
 
 
 class TextPosition(BaseModel):
-    page_number: Optional[int]
+    page_number: Optional[int] = None
     index: int
     start: int
     end: int
     start_seconds: Optional[List[int]] = None
     end_seconds: Optional[List[int]] = None
 
 
@@ -110,53 +131,67 @@
 
 
 class Sentences(BaseModel):
     results: List[Sentence] = []
     facets: FacetsResult
     page_number: int = 0
     page_size: int = 20
+    min_score: float = Field(
+        title="Minimum score",
+        description="Minimum similarity score used to filter vector index search. Results with a lower score have been ignored.",  # noqa
+    )
 
 
 class Paragraph(BaseModel):
     score: float
     rid: str
     field_type: str
     field: str
     text: str
     labels: List[str] = []
     start_seconds: Optional[List[int]] = None
     end_seconds: Optional[List[int]] = None
     position: Optional[TextPosition] = None
+    fuzzy_result: bool = False
 
 
 class Paragraphs(BaseModel):
     results: List[Paragraph] = []
     facets: Optional[FacetsResult] = None
     query: Optional[str] = None
     total: int = 0
     page_number: int = 0
     page_size: int = 20
     next_page: bool = False
+    min_score: float = Field(
+        title="Minimum score",
+        description="Minimum bm25 score used to filter bm25 index search. Results with a lower score have been ignored.",  # noqa
+    )
 
 
 class ResourceResult(BaseModel):
     score: Union[float, int]
     rid: str
     field_type: str
     field: str
+    labels: Optional[list[str]] = None
 
 
 class Resources(BaseModel):
     results: List[ResourceResult]
     facets: Optional[FacetsResult] = None
     query: Optional[str] = None
     total: int = 0
     page_number: int = 0
     page_size: int = 20
     next_page: bool = False
+    min_score: float = Field(
+        title="Minimum score",
+        description="Minimum bm25 score used to filter bm25 index search. Results with a lower score have been ignored.",  # noqa
+    )
 
 
 class RelationDirection(str, Enum):
     IN = "in"
     OUT = "out"
 
 
@@ -190,55 +225,96 @@
 # TODO: uncomment and implement (next iteration)
 # class RelationPath(BaseModel):
 #     origin: str
 #     destination: str
 #     path: List[DirectionalRelation]
 
 
+class SentenceSearch(BaseModel):
+    data: List[float] = []
+    time: float
+
+
+class Ner(BaseModel):
+    text: str
+    ner: str
+    start: int
+    end: int
+
+
+class TokenSearch(BaseModel):
+    tokens: List[Ner] = []
+    time: float
+
+
+class QueryInfo(BaseModel):
+    language: Optional[str] = None
+    stop_words: List[str] = []
+    semantic_threshold: Optional[float] = None
+    visual_llm: bool
+    max_context: int
+    entities: TokenSearch
+    sentence: SentenceSearch
+    query: str
+
+
 class Relations(BaseModel):
     entities: Dict[str, EntitySubgraph]
     # TODO: implement in the next iteration of knowledge graph search
     # graph: List[RelationPath]
 
 
+class RelatedEntity(BaseModel, frozen=True):
+    family: str
+    value: str
+
+
 class RelatedEntities(BaseModel):
     total: int = 0
-    entities: List[str] = []
+    entities: List[RelatedEntity] = []
+
 
+class ResourceSearchResults(JsonBaseModel):
+    """Search on resource results"""
 
-class ResourceSearchResults(BaseModel):
     sentences: Optional[Sentences] = None
     paragraphs: Optional[Paragraphs] = None
     relations: Optional[Relations] = None
-    nodes: Optional[List[Tuple[str, str, str]]]
-    shards: Optional[List[str]]
+    nodes: Optional[List[Dict[str, str]]] = None
+    shards: Optional[List[str]] = None
 
 
-class KnowledgeboxSearchResults(BaseModel):
+class KnowledgeboxSearchResults(JsonBaseModel):
+    """Search on knowledgebox results"""
+
     resources: Dict[str, Resource] = {}
     sentences: Optional[Sentences] = None
     paragraphs: Optional[Paragraphs] = None
     fulltext: Optional[Resources] = None
     relations: Optional[Relations] = None
-    nodes: Optional[List[Tuple[str, str, str]]]
-    shards: Optional[List[str]]
+    nodes: Optional[List[Dict[str, str]]] = None
+    shards: Optional[List[str]] = None
+    autofilters: List[str] = ModelParamDefaults.applied_autofilters.to_pydantic_field()
+
 
+class KnowledgeboxSuggestResults(JsonBaseModel):
+    """Suggest on resource results"""
 
-class KnowledgeboxSuggestResults(BaseModel):
     paragraphs: Optional[Paragraphs] = None
     entities: Optional[RelatedEntities] = None
-    shards: Optional[List[str]]
+    shards: Optional[List[str]] = None
 
 
 class KnowledgeboxCounters(BaseModel):
     resources: int
     paragraphs: int
     fields: int
     sentences: int
-    shards: Optional[List[str]]
+    shards: Optional[List[str]] = None
+    index_size: float = Field(default=0.0, title="Index size (bytes)")
 
 
 class SortField(str, Enum):
     SCORE = "score"
     CREATED = "created"
     MODIFIED = "modified"
     TITLE = "title"
@@ -274,29 +350,32 @@
     fields: int
     sentences: int
 
 
 class DocumentServiceEnum(str, Enum):
     DOCUMENT_V0 = "DOCUMENT_V0"
     DOCUMENT_V1 = "DOCUMENT_V1"
+    DOCUMENT_V2 = "DOCUMENT_V2"
 
 
 class ParagraphServiceEnum(str, Enum):
     PARAGRAPH_V0 = "PARAGRAPH_V0"
     PARAGRAPH_V1 = "PARAGRAPH_V1"
+    PARAGRAPH_V2 = "PARAGRAPH_V2"
 
 
 class VectorServiceEnum(str, Enum):
     VECTOR_V0 = "VECTOR_V0"
     VECTOR_V1 = "VECTOR_V1"
 
 
 class RelationServiceEnum(str, Enum):
     RELATION_V0 = "RELATION_V0"
     RELATION_V1 = "RELATION_V1"
+    RELATION_V2 = "RELATION_V2"
 
 
 class ShardCreated(BaseModel):
     id: str
     document_service: DocumentServiceEnum
     paragraph_service: ParagraphServiceEnum
     vector_service: VectorServiceEnum
@@ -324,108 +403,739 @@
 
 
 class KnowledgeboxShards(BaseModel):
     kbid: str
     actual: int
     similarity: VectorSimilarity
     shards: List[ShardObject]
+    model: Optional[SemanticModelMetadata]
 
     @classmethod
     def from_message(cls: Type[_T], message: PBShards) -> _T:
         as_dict = MessageToDict(
             message,
             preserving_proto_field_name=True,
             including_default_value_fields=True,
         )
         as_dict["similarity"] = VectorSimilarity.from_message(message.similarity)
+        if message.HasField("model"):
+            as_dict["model"] = SemanticModelMetadata.from_message(message.model)
         return cls(**as_dict)
 
 
-class SearchRequest(BaseModel):
-    query: str = ""
-    advanced_query: Optional[str] = None
-    fields: List[str] = []
-    filters: List[str] = []
-    faceted: List[str] = []
-    sort: Optional[SortOptions] = None
-    page_number: int = 0
-    page_size: int = 20
-    min_score: float = 0.70
-    range_creation_start: Optional[datetime] = None
-    range_creation_end: Optional[datetime] = None
-    range_modification_start: Optional[datetime] = None
-    range_modification_end: Optional[datetime] = None
-    features: List[SearchOptions] = [
-        SearchOptions.PARAGRAPH,
-        SearchOptions.DOCUMENT,
-        SearchOptions.VECTOR,
-    ]
-    reload: bool = True
-    debug: bool = False
-    highlight: bool = False
-    show: List[ResourceProperties] = [ResourceProperties.BASIC]
-    field_type_filter: List[FieldTypeName] = list(FieldTypeName)
-    extracted: List[ExtractedDataTypeName] = list(ExtractedDataTypeName)
-    shards: List[str] = []
-    vector: Optional[List[float]] = None
-    vectorset: Optional[str] = None
-    with_duplicates: bool = False
-    with_status: Optional[ResourceProcessingStatus] = None
-    with_synonyms: bool = False
+class SearchParamDefaults:
+    query = ParamDefault(
+        default="", title="Query", description="The query to search for"
+    )
+    suggest_query = ParamDefault(
+        default=..., title="Query", description="The query to get suggestions for"
+    )
+    fields = ParamDefault(
+        default=[],
+        title="Fields",
+        description="The list of fields to search in. For instance: `a/title` to search only on title field. For more details on filtering by field, see: https://docs.nuclia.dev/docs/docs/using/search/#search-in-a-specific-field",  # noqa: E501
+    )
+    filters = ParamDefault(
+        default=[],
+        title="Filters",
+        description="The list of filters to apply. Filtering examples can be found here: https://docs.nuclia.dev/docs/docs/using/search/#filters",  # noqa: E501
+    )
+    resource_filters = ParamDefault(
+        default=[],
+        title="Resources filter",
+        description="List of resource ids to filter search results for. Only paragraphs from the specified resources will be returned.",  # noqa: E501
+    )
+    faceted = ParamDefault(
+        default=[],
+        title="Faceted",
+        description="The list of facets to calculate. The facets follow the same syntax as filters: https://docs.nuclia.dev/docs/docs/using/search/#filters",  # noqa: E501
+        max_items=50,
+    )
+    autofilter = ParamDefault(
+        default=False,
+        title="Automatic search filtering",
+        description="If set to true, the search will automatically add filters to the query. For example, it will filter results containing the entities detected in the query",  # noqa: E501
+    )
+    chat_query = ParamDefault(
+        default=...,
+        title="Query",
+        description="The query to get a generative answer for",
+    )
+    shards = ParamDefault(
+        default=[],
+        title="Shards",
+        description="The list of shard replicas to search in. If empty, random replicas will be selected.",
+    )
+    page_number = ParamDefault(
+        default=0,
+        title="Page number",
+        description="The page number of the results to return",
+    )
+    page_size = ParamDefault(
+        default=20,
+        title="Page size",
+        description="The number of results to return per page",
+    )
+    highlight = ParamDefault(
+        default=False,
+        title="Highlight",
+        description="If set to true, the query terms will be highlighted in the results between <mark>...</mark> tags",  # noqa: E501
+    )
+    with_duplicates = ParamDefault(
+        default=False,
+        title="With duplicate paragraphs",
+        description="Whether to return duplicate paragraphs on the same document",  # noqa: E501
+    )
+    with_status = ParamDefault(
+        default=None,
+        title="With processing status",
+        description="Filter results by resource processing status",
+    )
+    with_synonyms = ParamDefault(
+        default=False,
+        title="With custom synonyms",
+        description="Whether to return matches for custom knowledge box synonyms of the query terms. Note: only supported for `paragraph` and `document` search options.",  # noqa: E501
+    )
+    sort_order = ParamDefault(
+        default=SortOrder.DESC,
+        title="Sort order",
+        description="Order to sort results with",
+    )
+    sort_limit = ParamDefault(
+        default=None,
+        title="Sort limit",
+        description="",
+        gt=0,
+    )
+    sort_field = ParamDefault(
+        default=None,
+        title="Sort field",
+        description="Field to sort results with",
+    )
+    sort = ParamDefault(
+        default=None,
+        title="Sort options",
+        description="Options for results sorting",
+    )
+    search_features = ParamDefault(
+        default=None,
+        title="Search features",
+        description="List of search features to use. Each value corresponds to a lookup into on of the different indexes.",  # noqa
+    )
+    debug = ParamDefault(
+        default=False,
+        title="Debug mode",
+        description="If set, the response will include some extra metadata for debugging purposes, like the list of queried nodes.",  # noqa
+    )
+    show = ParamDefault(
+        default=[ResourceProperties.BASIC],
+        title="Show metadata",
+        description="Controls which types of metadata are serialized on resources of search results",
+    )
+    extracted = ParamDefault(
+        default=[],
+        title="Extracted metadata",
+        description="Controls which parts of the extracted metadata are serialized on search results",
+    )
+    field_type_filter = ParamDefault(
+        default=list(FieldTypeName),
+        title="Field type filter",
+        description="Filter search results to match paragraphs of a specific field type. E.g: `['conversation', 'text']`",  # noqa
+    )
+    range_creation_start = ParamDefault(
+        default=None,
+        title="Resource creation range start",
+        description="Resources created before this date will be filtered out of search results. Datetime are represented as a str in ISO 8601 format, like: 2008-09-15T15:53:00+05:00.",  # noqa
+    )
+    range_creation_end = ParamDefault(
+        default=None,
+        title="Resource creation range end",
+        description="Resources created after this date will be filtered out of search results. Datetime are represented as a str in ISO 8601 format, like: 2008-09-15T15:53:00+05:00.",  # noqa
+    )
+    range_modification_start = ParamDefault(
+        default=None,
+        title="Resource modification range start",
+        description="Resources modified before this date will be filtered out of search results. Datetime are represented as a str in ISO 8601 format, like: 2008-09-15T15:53:00+05:00.",  # noqa
+    )
+    range_modification_end = ParamDefault(
+        default=None,
+        title="Resource modification range end",
+        description="Resources modified after this date will be filtered out of search results. Datetime are represented as a str in ISO 8601 format, like: 2008-09-15T15:53:00+05:00.",  # noqa
+    )
+    vector = ParamDefault(
+        default=None,
+        title="Search Vector",
+        description="The vector to perform the search with. If not provided, NucliaDB will use Nuclia Predict API to create the vector off from the query.",  # noqa
+    )
+    vectorset = ParamDefault(
+        default=None,
+        title="Vectorset id",
+        description="Id of the vectorset to perform the vector search into.",
+    )
+    chat_context = ParamDefault(
+        default=None,
+        title="Chat history",
+        description="Use to rephrase the new LLM query by taking into account the chat conversation history",  # noqa
+    )
+    chat_features = ParamDefault(
+        default=[ChatOptions.VECTORS, ChatOptions.PARAGRAPHS, ChatOptions.RELATIONS],
+        title="Chat features",
+        description="Features enabled for the chat endpoint. Semantic search is done if `vectors` is included. If `paragraphs` is included, the results will include matching paragraphs from the bm25 index. If `relations` is included, a graph of entities related to the answer is returned.",  # noqa
+    )
+    suggest_features = ParamDefault(
+        default=[
+            SuggestOptions.PARAGRAPH,
+            SuggestOptions.ENTITIES,
+        ],
+        title="Suggest features",
+        description="Features enabled for the suggest endpoint.",
+    )
+    security = ParamDefault(
+        default=None,
+        title="Security",
+        description="Security metadata for the request. If not provided, the search request is done without the security lookup phase.",  # noqa
+    )
+    security_groups = ParamDefault(
+        default=[],
+        title="Security groups",
+        description="List of security groups to filter search results for. Only resources matching the query and containing the specified security groups will be returned. If empty, all resources will be considered for the search.",  # noqa
+    )
+    rephrase = ParamDefault(
+        default=False,
+        title="Rephrase query consuming LLMs",
+        description="Rephrase query consuming LLMs - it will make the query slower",  # noqa
+    )
+
+
+class Filter(BaseModel):
+    all: Optional[List[str]] = Field(default=None, min_items=1)
+    any: Optional[List[str]] = Field(default=None, min_items=1)
+    none: Optional[List[str]] = Field(default=None, min_items=1)
+    not_all: Optional[List[str]] = Field(default=None, min_items=1)
+
+    @root_validator(pre=True)
+    def validate_filter(cls, values):
+        if len({k for k, v in values.items() if v is not None}) != 1:
+            raise ValueError("Only one of 'all', 'any', 'none' or 'not_all' can be set")
+        return values
+
+
+class CatalogRequest(BaseModel):
+    query: str = SearchParamDefaults.query.to_pydantic_field()
+    filters: Union[List[str], List[Filter]] = Field(
+        default=[],
+        title="Filters",
+        description="The list of filters to apply. Filtering examples can be found here: https://docs.nuclia.dev/docs/docs/using/search/#filters",  # noqa: E501
+    )
+    faceted: List[str] = SearchParamDefaults.faceted.to_pydantic_field()
+    sort: Optional[SortOptions] = SearchParamDefaults.sort.to_pydantic_field()
+    page_number: int = SearchParamDefaults.page_number.to_pydantic_field()
+    page_size: int = SearchParamDefaults.page_size.to_pydantic_field()
+    shards: List[str] = SearchParamDefaults.shards.to_pydantic_field()
+    debug: bool = SearchParamDefaults.debug.to_pydantic_field()
+    with_status: Optional[ResourceProcessingStatus] = Field(
+        default=None,
+        title="With processing status",
+        description="Filter results by resource processing status",
+    )
+
+    @validator("faceted")
+    def nested_facets_not_supported(cls, facets):
+        return validate_facets(facets)
+
+
+class MinScore(BaseModel):
+    semantic: Optional[float] = Field(
+        default=None,
+        title="Minimum semantic score",
+        description="Minimum semantic similarity score used to filter vector index search. If not specified, the default minimum score of the semantic model associated to the Knowledge Box will be used. Check out the documentation for more information on how to use this parameter: https://docs.nuclia.dev/docs/docs/using/search/#minimum-score",  # noqa: E501
+    )
+    bm25: float = Field(
+        default=0,
+        title="Minimum bm25 score",
+        description="Minimum score used to filter bm25 index search. Check out the documentation for more information on how to use this parameter: https://docs.nuclia.dev/docs/docs/using/search/#minimum-score",
+        ge=0,
+    )
+
+
+class BaseSearchRequest(BaseModel):
+    query: str = SearchParamDefaults.query.to_pydantic_field()
+    fields: List[str] = SearchParamDefaults.fields.to_pydantic_field()
+    filters: Union[List[str], List[Filter]] = Field(
+        default=[],
+        title="Filters",
+        description="The list of filters to apply. Filtering examples can be found here: https://docs.nuclia.dev/docs/docs/using/search/#filters",  # noqa: E501
+    )
+    page_number: int = SearchParamDefaults.page_number.to_pydantic_field()
+    page_size: int = SearchParamDefaults.page_size.to_pydantic_field()
+    min_score: Optional[Union[float, MinScore]] = Field(
+        default=None,
+        title="Minimum score",
+        description="Minimum score to filter search results. Results with a lower score will be ignored. Accepts either a float or a dictionary with the minimum scores for the bm25 and vector indexes. If a float is provided, it is interpreted as the minimum score for vector index search.",  # noqa
+    )
+    range_creation_start: Optional[datetime] = (
+        SearchParamDefaults.range_creation_start.to_pydantic_field()
+    )
+    range_creation_end: Optional[datetime] = (
+        SearchParamDefaults.range_creation_end.to_pydantic_field()
+    )
+    range_modification_start: Optional[datetime] = (
+        SearchParamDefaults.range_modification_start.to_pydantic_field()
+    )
+    range_modification_end: Optional[datetime] = (
+        SearchParamDefaults.range_modification_end.to_pydantic_field()
+    )
+    features: List[SearchOptions] = (
+        SearchParamDefaults.search_features.to_pydantic_field(
+            default=[
+                SearchOptions.PARAGRAPH,
+                SearchOptions.DOCUMENT,
+                SearchOptions.VECTOR,
+            ]
+        )
+    )
+    debug: bool = SearchParamDefaults.debug.to_pydantic_field()
+    highlight: bool = SearchParamDefaults.highlight.to_pydantic_field()
+    show: List[ResourceProperties] = SearchParamDefaults.show.to_pydantic_field()
+    field_type_filter: List[FieldTypeName] = (
+        SearchParamDefaults.field_type_filter.to_pydantic_field()
+    )
+    extracted: List[ExtractedDataTypeName] = (
+        SearchParamDefaults.extracted.to_pydantic_field()
+    )
+    shards: List[str] = SearchParamDefaults.shards.to_pydantic_field()
+    vector: Optional[List[float]] = SearchParamDefaults.vector.to_pydantic_field()
+    vectorset: Optional[str] = SearchParamDefaults.vectorset.to_pydantic_field()
+    with_duplicates: bool = SearchParamDefaults.with_duplicates.to_pydantic_field()
+    with_synonyms: bool = SearchParamDefaults.with_synonyms.to_pydantic_field()
+    autofilter: bool = SearchParamDefaults.autofilter.to_pydantic_field()
+    resource_filters: List[str] = (
+        SearchParamDefaults.resource_filters.to_pydantic_field()
+    )
+    security: Optional[RequestSecurity] = (
+        SearchParamDefaults.security.to_pydantic_field()
+    )
+
+    rephrase: Optional[bool] = Field(
+        default=False,
+        title="Rephrase the query to improve search",
+        description="Consume LLM tokens to rephrase the query so the semantic search is better",
+    )
+
+
+class SearchRequest(BaseSearchRequest):
+    faceted: List[str] = SearchParamDefaults.faceted.to_pydantic_field()
+    sort: Optional[SortOptions] = SearchParamDefaults.sort.to_pydantic_field()
+
+    @validator("faceted")
+    def nested_facets_not_supported(cls, facets):
+        return validate_facets(facets)
 
 
 class Author(str, Enum):
     NUCLIA = "NUCLIA"
     USER = "USER"
 
 
-class Message(BaseModel):
+class ChatContextMessage(BaseModel):
     author: Author
     text: str
 
 
+# For bw compatibility
+Message = ChatContextMessage
+
+
+class UserPrompt(BaseModel):
+    prompt: str
+
+
+class Image(BaseModel):
+    content_type: str
+    b64encoded: str
+
+
 class ChatModel(BaseModel):
-    question: str
+    """
+    This is the model for the predict request payload on the chat endpoint
+    """
+
+    question: str = Field(description="Question to ask the generative model")
     user_id: str
     retrieval: bool = True
     system: Optional[str] = None
-    context: List[Message] = []
+    query_context: Dict[str, str] = Field(
+        default={},
+        description="The information retrieval context for the current query",
+    )
+    query_context_order: Optional[Dict[str, int]] = Field(
+        default=None,
+        description="The order of the query context elements. This is used to sort the context elements by relevance before sending them to the generative model",  # noqa
+    )
+    chat_history: List[ChatContextMessage] = Field(
+        default=[], description="The chat conversation history"
+    )
+    truncate: bool = Field(
+        default=True,
+        description="Truncate the chat context in case it doesn't fit the generative input",
+    )
+    user_prompt: Optional[UserPrompt] = Field(
+        default=None, description="Optional custom prompt input by the user"
+    )
+    citations: bool = Field(
+        default=False, description="Whether to include the citations in the answer"
+    )
+    generative_model: Optional[str] = Field(
+        default=None,
+        title="Generative model",
+        description="The generative model to use for the predict chat endpoint. If not provided, the model configured for the Knowledge Box is used.",
+    )
+
+    max_tokens: Optional[int] = Field(
+        default=None, description="Maximum characters to generate"
+    )
+
+    query_context_images: Dict[str, Image] = Field(
+        default={},
+        description="The information retrieval context for the current query, each image is a base64 encoded string",
+    )
 
 
 class RephraseModel(BaseModel):
     question: str
-    context: List[Message] = []
+    chat_history: List[ChatContextMessage] = []
     user_id: str
+    user_context: List[str] = []
+    generative_model: Optional[str] = Field(
+        default=None,
+        title="Generative model",
+        description="The generative model to use for the rephrase endpoint. If not provided, the model configured for the Knowledge Box is used.",
+    )
+
+
+class AskDocumentModel(BaseModel):
+    question: str = Field(description="The question to ask on the document")
+    blocks: List[List[str]] = Field(
+        description="The complete list of text blocks of a document"
+    )
+    user_id: str = Field(description="The id of the user associated to the request")
+
+
+class RagStrategyName:
+    FIELD_EXTENSION = "field_extension"
+    FULL_RESOURCE = "full_resource"
+    HIERARCHY = "hierarchy"
+
+
+class ImageRagStrategyName:
+    PAGE_IMAGE = "page_image"
+    TABLES = "tables"
+
+
+class RagStrategy(BaseModel):
+    name: str
+
+
+class ImageRagStrategy(BaseModel):
+    name: str
+
+
+ALLOWED_FIELD_TYPES: dict[str, str] = {
+    "l": "layout",
+    "t": "text",
+    "f": "file",
+    "u": "link",
+    "d": "datetime",
+    "k": "keywordset",
+    "a": "generic",
+    "c": "conversation",
+}
+
+
+class FieldExtensionStrategy(RagStrategy):
+    name: Literal["field_extension"]
+    fields: List[str] = Field(
+        title="Fields",
+        description="List of field ids to extend the context with. It will try to extend the retrieval context with the specified fields in the matching resources. The field ids have to be in the format `{field_type}/{field_name}`, like 'a/title', 'a/summary' for title and summary fields or 't/amend' for a text field named 'amend'.",  # noqa
+        min_items=1,
+        unique_items=True,
+    )
+
+    @root_validator(pre=True)
+    def fields_validator(cls, values):
+        if values.get("fields") is None:
+            return values
+
+        # Check that the fields are in the format {field_type}/{field_name}
+        for field in values.get("fields"):
+            try:
+                field_type, _ = field.strip("/").split("/")
+            except ValueError:
+                raise ValueError(
+                    f"Field '{field}' is not in the format {{field_type}}/{{field_name}}"
+                )
+            if field_type not in ALLOWED_FIELD_TYPES:
+                allowed_field_types_part = ", ".join(
+                    [
+                        f"'{fid}' for '{fname}' fields"
+                        for fid, fname in ALLOWED_FIELD_TYPES.items()
+                    ]
+                )
+                raise ValueError(
+                    f"Field '{field}' does not have a valid field type. "
+                    f"Valid field types are: {allowed_field_types_part}."
+                )
+
+        return values
+
+
+class FullResourceStrategy(RagStrategy):
+    name: Literal["full_resource"]
+    count: Optional[int] = Field(
+        title="Resources",
+        default=None,
+        description="How many full documents to retrieve",
+    )
+
+
+class HierarchyResourceStrategy(RagStrategy):
+    name: Literal["hierarchy"]
+    count: Optional[int] = Field(
+        title="Resources",
+        default=None,
+        description="Levels of distance that is added to the context",
+    )
+
+
+class TableImageStrategy(ImageRagStrategy):
+    name: Literal["tables"]
+
+
+class PageImageStrategy(ImageRagStrategy):
+    name: Literal["page_image"]
+    count: Optional[int] = Field(
+        title="Images",
+        default=None,
+        description="How many images to retrieve",
+    )
+
+
+class ParagraphImageStrategy(ImageRagStrategy):
+    name: Literal["paragraph_image"]
+
+
+RagStrategies = Annotated[
+    Union[FieldExtensionStrategy, FullResourceStrategy, HierarchyResourceStrategy],
+    Field(discriminator="name"),
+]
+RagImagesStrategies = Annotated[
+    Union[PageImageStrategy, ParagraphImageStrategy], Field(discriminator="name")
+]
+PromptContext = dict[str, str]
+PromptContextOrder = dict[str, int]
+PromptContextImages = dict[str, Image]
 
 
 class ChatRequest(BaseModel):
-    query: str = ""
-    fields: List[str] = []
-    filters: List[str] = []
-    min_score: float = 0.70
-    features: List[ChatOptions] = [
-        ChatOptions.PARAGRAPHS,
-        ChatOptions.RELATIONS,
-    ]
-    range_creation_start: Optional[datetime] = None
-    range_creation_end: Optional[datetime] = None
-    range_modification_start: Optional[datetime] = None
-    range_modification_end: Optional[datetime] = None
-    show: List[ResourceProperties] = [ResourceProperties.BASIC]
-    field_type_filter: List[FieldTypeName] = list(FieldTypeName)
-    extracted: List[ExtractedDataTypeName] = list(ExtractedDataTypeName)
-    shards: List[str] = []
-    context: Optional[List[Message]] = None
-
-
-class FindRequest(SearchRequest):
-    features: List[SearchOptions] = [
-        SearchOptions.PARAGRAPH,
-        SearchOptions.VECTOR,
-    ]
+    query: str = SearchParamDefaults.chat_query.to_pydantic_field()
+    fields: List[str] = SearchParamDefaults.fields.to_pydantic_field()
+    filters: Union[List[str], List[Filter]] = Field(
+        default=[],
+        title="Filters",
+        description="The list of filters to apply. Filtering examples can be found here: https://docs.nuclia.dev/docs/docs/using/search/#filters",  # noqa: E501
+    )
+    min_score: Optional[Union[float, MinScore]] = Field(
+        default=None,
+        title="Minimum score",
+        description="Minimum score to filter search results. Results with a lower score will be ignored. Accepts either a float or a dictionary with the minimum scores for the bm25 and vector indexes. If a float is provided, it is interpreted as the minimum score for vector index search.",  # noqa
+    )
+    features: List[ChatOptions] = SearchParamDefaults.chat_features.to_pydantic_field()
+    range_creation_start: Optional[datetime] = (
+        SearchParamDefaults.range_creation_start.to_pydantic_field()
+    )
+    range_creation_end: Optional[datetime] = (
+        SearchParamDefaults.range_creation_end.to_pydantic_field()
+    )
+    range_modification_start: Optional[datetime] = (
+        SearchParamDefaults.range_modification_start.to_pydantic_field()
+    )
+    range_modification_end: Optional[datetime] = (
+        SearchParamDefaults.range_modification_end.to_pydantic_field()
+    )
+    show: List[ResourceProperties] = SearchParamDefaults.show.to_pydantic_field()
+    field_type_filter: List[FieldTypeName] = (
+        SearchParamDefaults.field_type_filter.to_pydantic_field()
+    )
+    extracted: List[ExtractedDataTypeName] = (
+        SearchParamDefaults.extracted.to_pydantic_field()
+    )
+    shards: List[str] = SearchParamDefaults.shards.to_pydantic_field()
+    context: Optional[List[ChatContextMessage]] = (
+        SearchParamDefaults.chat_context.to_pydantic_field()
+    )
+    extra_context: Optional[List[str]] = Field(
+        default=None,
+        title="Extra query context",
+        description="""Additional context that is added to the retrieval context sent to the LLM.
+        It allows extending the chat feature with content that may not be in the Knowledge Box.""",
+    )
+    autofilter: bool = SearchParamDefaults.autofilter.to_pydantic_field()
+    highlight: bool = SearchParamDefaults.highlight.to_pydantic_field()
+    resource_filters: List[str] = (
+        SearchParamDefaults.resource_filters.to_pydantic_field()
+    )
+    prompt: Optional[str] = Field(
+        default=None,
+        title="Prompt",
+        description="Input here your prompt with the words {context} and {question} in brackets where you want those fields to be placed, in case you want them in your prompt. Context will be the data returned by the retrieval step.",  # noqa
+        min_length=1,
+    )
+    citations: bool = Field(
+        default=False,
+        description="Whether to include the citations for the answer in the response",
+    )
+    security: Optional[RequestSecurity] = (
+        SearchParamDefaults.security.to_pydantic_field()
+    )
+    rag_strategies: list[RagStrategies] = Field(
+        default=[],
+        title="RAG context building strategies",
+        description="Options for tweaking how the context for the LLM model is crafted. `full_resource` will add the full text of the matching resources to the context. `field_extension` will add the text of the matching resource's specified fields to the context. If empty, the default strategy is used.",  # noqa
+    )
+    rag_images_strategies: list[RagImagesStrategies] = Field(
+        default=[],
+        title="RAG image context building strategies",
+        description="Options for tweaking how the image based context for the LLM model is crafted. `page_image` will add the full page image of the matching resources to the context. If empty, the default strategy is used with the image of the paragraph.",  # noqa
+    )
+    debug: bool = SearchParamDefaults.debug.to_pydantic_field()
+
+    generative_model: Optional[str] = Field(
+        default=None,
+        title="Generative model",
+        description="The generative model to use for the chat endpoint. If not provided, the model configured for the Knowledge Box is used.",
+    )
+
+    max_tokens: Optional[int] = Field(
+        default=None,
+        title="Maximum tokens to generate",
+        description="The maximum amount of tokens to generate by the LLM",
+    )
+
+    rephrase: Optional[bool] = Field(
+        default=False,
+        title="Rephrase the query to improve search",
+        description="Consume LLM tokens to rephrase the query so the semantic search is better",
+    )
+
+    @root_validator(pre=True)
+    def rag_features_validator(cls, values):
+        chosen_strategies = []
+        for s in values.get("rag_strategies") or []:
+            if not isinstance(s, dict):
+                raise ValueError("RAG strategies must be defined using an object")
+            strategy = s.get("name", None)
+            if strategy is None:
+                raise ValueError(f"Invalid strategy '{s}'")
+            chosen_strategies.append(strategy)
+
+        # There must be at most one strategy of each type
+        if len(chosen_strategies) > len(set(chosen_strategies)):
+            raise ValueError("There must be at most one strategy of each type")
+
+        # If full resource strategy is chosen, it must be the only strategy
+        if (
+            RagStrategyName.FULL_RESOURCE in chosen_strategies
+            and len(chosen_strategies) > 1
+        ):
+            raise ValueError(
+                f"If '{RagStrategyName.FULL_RESOURCE}' strategy is chosen, it must be the only strategy"
+            )
+
+        return values
+
+
+class SummarizeResourceModel(BaseModel):
+    fields: Dict[str, str] = {}
+
+
+class SummaryKind(str, Enum):
+    SIMPLE = "simple"
+    EXTENDED = "extended"
+
+
+class SummarizeModel(BaseModel):
+    """
+    Model for the summarize predict api request payload
+    """
+
+    resources: Dict[str, SummarizeResourceModel] = {}
+    generative_model: Optional[str] = None
+    user_prompt: Optional[str] = None
+    summary_kind: SummaryKind = SummaryKind.SIMPLE
+
+
+class SummarizeRequest(BaseModel):
+    """
+    Model for the request payload of the summarize endpoint
+    """
+
+    generative_model: Optional[str] = Field(
+        default=None,
+        title="Generative model",
+        description="The generative model to use for the summarization. If not provided, the model configured for the Knowledge Box is used.",
+    )
+
+    user_prompt: Optional[str] = Field(
+        default=None,
+        title="User prompt",
+        description="Optional custom prompt input by the user",
+    )
+
+    resources: List[str] = Field(
+        ...,
+        min_items=1,
+        max_items=100,
+        title="Resources",
+        description="Uids or slugs of the resources to summarize. If the resources are not found, they will be ignored.",
+    )
+
+    summary_kind: SummaryKind = Field(
+        default=SummaryKind.SIMPLE,
+        title="Summary kind",
+        description="Option to customize how the summary will be",
+    )
+
+
+class SummarizedResource(BaseModel):
+    summary: str = Field(..., title="Summary", description="Summary of the resource")
+    tokens: int
+
+
+class SummarizedResponse(BaseModel):
+    resources: Dict[str, SummarizedResource] = Field(
+        default={},
+        title="Resources",
+        description="Individual resource summaries. The key is the resource id or slug.",
+    )
+    summary: str = Field(
+        default="",
+        title="Summary",
+        description="Global summary of all resources combined.",
+    )
+
+
+class FindRequest(BaseSearchRequest):
+    features: List[SearchOptions] = (
+        SearchParamDefaults.search_features.to_pydantic_field(
+            default=[
+                SearchOptions.PARAGRAPH,
+                SearchOptions.VECTOR,
+            ]
+        )
+    )
 
     @validator("features")
     def fulltext_not_supported(cls, v):
         if SearchOptions.DOCUMENT in v or SearchOptions.DOCUMENT == v:
             raise ValueError("fulltext search not supported")
         return v
 
@@ -444,61 +1154,159 @@
     start: int
     end: int
 
 
 class FindParagraph(BaseModel):
     score: float
     score_type: SCORE_TYPE
-    order: int = Field(0, ge=0)
+    order: int = Field(default=0, ge=0)
     text: str
     id: str
     labels: Optional[List[str]] = []
     position: Optional[TextPosition] = None
+    fuzzy_result: bool = False
+    page_with_visual: bool = Field(
+        default=False,
+        title="Page where this paragraph belongs is a visual page",
+        description="This flag informs if the page may have information that has not been extracted",
+    )
+    reference: Optional[str] = Field(
+        default=None,
+        title="Reference to the image that represents this text",
+        description="Reference to the extracted image that represents this paragraph",
+    )
+    is_a_table: bool = Field(
+        default=False,
+        title="Is a table",
+        description="The referenced image of the paragraph is a table",
+    )
 
 
 @dataclass
 class TempFindParagraph:
     rid: str
     field: str
     score: float
     start: int
     end: int
     id: str
+    split: Optional[str] = None
     paragraph: Optional[FindParagraph] = None
     vector_index: Optional[DocumentScored] = None
     paragraph_index: Optional[PBParagraphResult] = None
+    fuzzy_result: bool = False
+    page_with_visual: bool = False
+    reference: Optional[str] = None
+    is_a_table: bool = False
 
 
 class FindField(BaseModel):
     paragraphs: Dict[str, FindParagraph]
 
 
 class FindResource(Resource):
     fields: Dict[str, FindField]
 
     def updated_from(self, origin: Resource):
         for key in origin.__fields__.keys():
             self.__setattr__(key, getattr(origin, key))
 
 
-class KnowledgeboxFindResults(BaseModel):
+class KnowledgeboxFindResults(JsonBaseModel):
+    """Find on knowledgebox results"""
+
     resources: Dict[str, FindResource]
     relations: Optional[Relations] = None
-    facets: FacetsResult
     query: Optional[str] = None
     total: int = 0
     page_number: int = 0
     page_size: int = 20
     next_page: bool = False
-    nodes: Optional[List[Tuple[str, str, str]]]
-    shards: Optional[List[str]]
+    nodes: Optional[List[Dict[str, str]]] = Field(
+        default=None,
+        title="Nodes",
+        description="List of nodes queried in the search",
+    )
+    shards: Optional[List[str]] = Field(
+        default=None,
+        title="Shards",
+        description="The list of shard replica ids used for the search.",
+    )
+    autofilters: List[str] = ModelParamDefaults.applied_autofilters.to_pydantic_field()
+    min_score: Optional[Union[float, MinScore]] = Field(
+        title="Minimum result score",
+        description="The minimum scores that have been used for the search operation.",
+    )
+    best_matches: List[str] = Field(
+        default=[],
+        title="Best matches",
+        description="List of ids of best matching paragraphs. The list is sorted by decreasing relevance (most relevant first).",  # noqa
+    )
 
 
 class FeedbackTasks(str, Enum):
     CHAT = "CHAT"
 
 
 class FeedbackRequest(BaseModel):
-    ident: str
-    good: bool
-    task: FeedbackTasks
-    feedback: Optional[str]
+    ident: str = Field(
+        title="Request identifier",
+        description="Id of the request to provide feedback for. This id is returned in the response header `Nuclia-Learning-Id` of the chat endpoint.",  # noqa
+    )
+    good: bool = Field(title="Good", description="Whether the result was good or not")
+    task: FeedbackTasks = Field(
+        title="Task",
+        description="The task the feedback is for. For now, only `CHAT` task is available",
+    )
+    feedback: Optional[str] = Field(title="Feedback", description="Feedback text")
+
+
+TextBlocks = List[List[str]]
+
+
+class AskRequest(BaseModel):
+    question: str = Field(
+        ...,
+        title="Question",
+        description="Question asked to the document",
+        example="Does this document contain personal information?",
+    )
+
+
+class AskResponse(BaseModel):
+    answer: str = Field(
+        ...,
+        title="Answer",
+        description="Answer to the question received from the generative AI model",
+    )
+
+
+def validate_facets(facets):
+    """
+    Raises ValueError if provided facets contains nested facets, like:
+    ["/a/b", "/a/b/c"]
+    """
+    if len(facets) < 2:
+        return facets
+
+    # Sort facets alphabetically to make sure that nested facets appear right after their parents
+    sorted_facets = sorted(facets)
+    facet = sorted_facets.pop(0)
+    while True:
+        try:
+            next_facet = sorted_facets.pop(0)
+        except IndexError:
+            # No more facets to check
+            break
+        if next_facet == facet:
+            raise ValueError(
+                f"Facet {next_facet} is already present in facets. Faceted list must be unique."
+            )
+        if next_facet.startswith(facet):
+            if next_facet.replace(facet, "").startswith("/"):
+                raise ValueError(
+                    "Nested facets are not allowed: {child} is a child of {parent}".format(
+                        child=next_facet, parent=facet
+                    )
+                )
+        facet = next_facet
+    return facets
```

## nucliadb_models/text.py

```diff
@@ -17,16 +17,17 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from enum import Enum
 from typing import TYPE_CHECKING, Optional, Type, TypeVar
 
 from google.protobuf.json_format import MessageToDict
-from pydantic import BaseModel
+from pydantic import BaseModel, root_validator
 
+from nucliadb_models.utils import validate_json
 from nucliadb_protos import resources_pb2
 
 _T = TypeVar("_T")
 
 
 if TYPE_CHECKING:  # pragma: no cover
     TextFormatValue = resources_pb2.FieldText.Format.V
@@ -38,21 +39,24 @@
 
 
 class TextFormat(Enum):  # type: ignore
     PLAIN = "PLAIN"
     HTML = "HTML"
     RST = "RST"
     MARKDOWN = "MARKDOWN"
+    JSON = "JSON"
+    KEEP_MARKDOWN = "KEEP_MARKDOWN"
 
 
 TEXT_FORMAT_TO_MIMETYPE = {
     TextFormat.PLAIN: "text/plain",
     TextFormat.HTML: "text/html",
     TextFormat.RST: "text/x-rst",
     TextFormat.MARKDOWN: "text/markdown",
+    TextFormat.JSON: "application/json",
 }
 
 
 # Visualization classes (Those used on reader endpoints)
 
 
 class FieldText(BaseModel):
@@ -74,21 +78,29 @@
 # Creation and update classes (Those used on writer endpoints)
 
 
 class TextField(BaseModel):
     body: str
     format: TextFormat = TextFormat.PLAIN
 
+    @root_validator(pre=False, skip_on_failure=True)
+    def check_text_format(cls, values):
+        if values.get("format") == TextFormat.JSON:
+            validate_json(values.get("body", ""))
+        return values
+
 
 # Processing classes (Those used to sent to push endpoints)
 
 
 class PushTextFormat(TextFormatValue, Enum):  # type: ignore
     PLAIN = 0
     HTML = 1
     MARKDOWN = 2
     RST = 3
+    JSON = 4
+    KEEP_MARKDOWN = 5
 
 
 class Text(BaseModel):
     body: str
     format: PushTextFormat
```

## nucliadb_models/utils.py

```diff
@@ -13,20 +13,23 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
+import json
 import re
 
 import pydantic
 
 
 class FieldIdString(pydantic.ConstrainedStr):
+    """A string that is used as a field id."""
+
     regex = re.compile(r"^[a-zA-Z0-9:_-]+$")
 
     @classmethod
     def validate(cls, value: str) -> str:
         try:
             return super().validate(value)
         except pydantic.errors.StrRegexError:
@@ -46,15 +49,22 @@
     msg_template = (
         "Invalid slug: '{value}'. Slug must be a string with only "
         "letters, numbers, underscores, colons and dashes."
     )
 
 
 class SlugString(pydantic.ConstrainedStr):
-    regex = re.compile(r"^[a-z0-9:_-]+$")
+    regex = re.compile(r"^[a-zA-Z0-9:_-]+$")
 
     @classmethod
     def validate(cls, value: str) -> str:
         try:
             return super().validate(value)
         except pydantic.errors.StrRegexError:
             raise InvalidSlugError(value=value)
+
+
+def validate_json(value: str):
+    try:
+        json.loads(value)
+    except json.JSONDecodeError as exc:
+        raise ValueError("Invalid JSON") from exc
```

## nucliadb_models/vectors.py

```diff
@@ -1,42 +1,22 @@
 from enum import Enum
-from typing import Dict, List, Optional, Tuple, Type, TypeVar
+from typing import Dict, Optional, TypeVar
 
 from google.protobuf.json_format import MessageToDict
 from nucliadb_protos.utils_pb2 import VectorSimilarity as PBVectorSimilarity
 from nucliadb_protos.writer_pb2 import VectorSet as PBVectorSet
-from pydantic import BaseModel
+from pydantic import BaseModel, Field
 
-from nucliadb_models import FieldID
-from nucliadb_protos import resources_pb2
+from nucliadb_protos import knowledgebox_pb2
 
-UserVectorPosition = Tuple[int, int]
 _T = TypeVar("_T")
 
 # Managing vectors
 
 
-class UserVector(BaseModel):
-    vector: List[float]
-    positions: Optional[UserVectorPosition] = None
-
-
-class UserVectorList(BaseModel):
-    vectors: List[str]
-
-
-class UserVectorWrapper(BaseModel):
-    vectors: Optional[Dict[str, Dict[str, UserVector]]] = None  # vectorsets
-    vectors_to_delete: Optional[Dict[str, UserVectorList]] = None
-    field: FieldID
-
-
-UserVectorsWrapper = List[UserVectorWrapper]
-
-
 class VectorSimilarity(str, Enum):
     COSINE = "cosine"
     DOT = "dot"
 
     def to_pb(self) -> PBVectorSimilarity.ValueType:
         return VECTOR_SIMILARITY_ENUM_TO_PB[self.value]
 
@@ -48,52 +28,52 @@
 VECTOR_SIMILARITY_ENUM_TO_PB = {
     VectorSimilarity.COSINE.value: PBVectorSimilarity.COSINE,
     VectorSimilarity.DOT.value: PBVectorSimilarity.DOT,
 }
 VECTOR_SIMILARITY_PB_TO_ENUM = {v: k for k, v in VECTOR_SIMILARITY_ENUM_TO_PB.items()}
 
 
+class SemanticModelMetadata(BaseModel):
+    """
+    Metadata of the semantic model associated to the KB
+    """
+
+    similarity_function: VectorSimilarity = Field(
+        description="Vector similarity algorithm that is applied on search"
+    )
+    vector_dimension: Optional[int] = Field(
+        description="Dimension of the indexed vectors/embeddings"
+    )
+    default_min_score: Optional[float] = Field(
+        description="Default minimum similarity value at which results are ignored"
+    )
+
+    @classmethod
+    def from_message(cls, message: knowledgebox_pb2.SemanticModelMetadata):
+        as_dict = MessageToDict(
+            message,
+            preserving_proto_field_name=True,
+            including_default_value_fields=True,
+        )
+        as_dict["similarity_function"] = VectorSimilarity.from_message(
+            message.similarity_function
+        )
+        return cls(**as_dict)
+
+
 class VectorSet(BaseModel):
     dimension: int
-    similarity: Optional[VectorSimilarity]
+    similarity: Optional[VectorSimilarity] = None
 
     @classmethod
     def from_message(cls, message: PBVectorSet):
         as_dict = MessageToDict(
             message,
             preserving_proto_field_name=True,
             including_default_value_fields=True,
         )
         as_dict["similarity"] = VectorSimilarity.from_message(message.similarity)
         return cls(**as_dict)
 
 
 class VectorSets(BaseModel):
     vectorsets: Dict[str, VectorSet]
-
-
-# Resource rendering
-
-
-class GetUserVector(BaseModel):
-    vector: List[float]
-    labels: Optional[List[str]] = None
-    start: int
-    end: int
-
-
-class UserVectors(BaseModel):
-    vectors: Optional[Dict[str, GetUserVector]]
-
-
-class UserVectorSet(BaseModel):
-    vectors: Optional[Dict[str, UserVectors]]
-
-    @classmethod
-    def from_message(cls: Type[_T], message: resources_pb2.UserVectorSet) -> _T:
-        return cls(
-            **MessageToDict(
-                message,
-                preserving_proto_field_name=True,
-                including_default_value_fields=True,
-            )
-        )
```

## nucliadb_models/writer.py

```diff
@@ -13,98 +13,169 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import json
 from typing import Dict, List, Optional, Union
 
-from pydantic import BaseModel, validator
+from pydantic import BaseModel, Field, validator
 
 from nucliadb_models.conversation import InputConversationField
 from nucliadb_models.datetime import FieldDatetime
 from nucliadb_models.file import FileField
 from nucliadb_models.keywordset import FieldKeywordset
 from nucliadb_models.layout import InputLayoutField
 from nucliadb_models.link import LinkField
 from nucliadb_models.metadata import (
+    Extra,
     InputMetadata,
     Origin,
     UserFieldMetadata,
     UserMetadata,
 )
 from nucliadb_models.processing import PushProcessingOptions
+from nucliadb_models.security import ResourceSecurity
 from nucliadb_models.text import TextField
 from nucliadb_models.utils import FieldIdString, SlugString
-from nucliadb_models.vectors import UserVectorsWrapper
 
 GENERIC_MIME_TYPE = "application/generic"
 
 
+class FieldDefaults:
+    title = Field(None, title="Title")
+    summary = Field(None, title="Summary")
+    slug = Field(
+        None,
+        title="Slug",
+        description="The slug is the user-defined id for the resource",
+    )
+    icon = Field(
+        None,
+        title="Icon",
+        description="The icon should be a media type string: https://www.iana.org/assignments/media-types/media-types.xhtml",  # noqa
+    )
+
+    files = Field(
+        {},
+        title="Files",
+        description=f"Dictionary of file fields to be added to the resource. The keys correspond to the field id, and must comply with the regex: {FieldIdString.regex.pattern}",  # noqa
+    )
+    links = Field(
+        {},
+        title="Links",
+        description=f"Dictionary of link fields to be added to the resource. The keys correspond to the field id, and must comply with the regex: {FieldIdString.regex.pattern}",  # noqa
+    )
+    texts = Field(
+        {},
+        title="Texts",
+        description=f"Dictionary of text fields to be added to the resource. The keys correspond to the field id, and must comply with the regex: {FieldIdString.regex.pattern}",  # noqa
+    )
+    layouts = Field(
+        {},
+        title="Layouts",
+        description=f"Dictionary of layout fields to be added to the resource. The keys correspond to the field id, and must comply with the regex: {FieldIdString.regex.pattern}",  # noqa
+    )
+    conversations = Field(
+        {},
+        title="Conversations",
+        description=f"Dictionary of conversation fields to be added to the resource. The keys correspond to the field id, and must comply with the regex: {FieldIdString.regex.pattern}",  # noqa
+    )
+    keywordsets = Field(
+        {},
+        title="Keywordsets",
+        description=f"Dictionary of keywordset fields to be added to the resource. The keys correspond to the field id, and must comply with the regex: {FieldIdString.regex.pattern}",  # noqa
+    )
+    datetimes = Field(
+        {},
+        title="Datetimes",
+        description=f"Dictionary of datetime fields to be added to the resource. The keys correspond to the field id, and must comply with the regex: {FieldIdString.regex.pattern}",  # noqa
+    )
+
+
 class CreateResourcePayload(BaseModel):
-    title: Optional[str] = None
-    summary: Optional[str] = None
-    slug: Optional[SlugString] = None
-    icon: Optional[str] = None
+    title: Optional[str] = FieldDefaults.title
+    summary: Optional[str] = FieldDefaults.summary
+    slug: Optional[SlugString] = FieldDefaults.slug
+    icon: Optional[str] = FieldDefaults.icon
     thumbnail: Optional[str] = None
     layout: Optional[str] = None
     metadata: Optional[InputMetadata] = None
     usermetadata: Optional[UserMetadata] = None
     fieldmetadata: Optional[List[UserFieldMetadata]] = None
-    uservectors: Optional[UserVectorsWrapper] = None
     origin: Optional[Origin] = None
+    extra: Optional[Extra] = None
 
-    files: Dict[FieldIdString, FileField] = {}
-    links: Dict[FieldIdString, LinkField] = {}
-    texts: Dict[FieldIdString, TextField] = {}
-    layouts: Dict[FieldIdString, InputLayoutField] = {}
-    conversations: Dict[FieldIdString, InputConversationField] = {}
-    keywordsets: Dict[FieldIdString, FieldKeywordset] = {}
-    datetimes: Dict[FieldIdString, FieldDatetime] = {}
-
-    # Processing options
+    files: Dict[FieldIdString, FileField] = FieldDefaults.files
+    links: Dict[FieldIdString, LinkField] = FieldDefaults.links
+    texts: Dict[FieldIdString, TextField] = FieldDefaults.texts
+    layouts: Dict[FieldIdString, InputLayoutField] = FieldDefaults.layouts
+    conversations: Dict[FieldIdString, InputConversationField] = (
+        FieldDefaults.conversations
+    )
+    keywordsets: Dict[FieldIdString, FieldKeywordset] = FieldDefaults.keywordsets
+    datetimes: Dict[FieldIdString, FieldDatetime] = FieldDefaults.datetimes
     processing_options: Optional[PushProcessingOptions] = PushProcessingOptions()
+    security: Optional[ResourceSecurity] = Field(
+        default=None,
+        title="Security",
+        description="Security metadata for the resource. It can be used to have fine-grained control over who can access the resource.",  # noqa
+    )
 
     @validator("icon")
     def icon_check(cls, v):
         if v is None:
             return v
 
         if "/" not in v:
             raise ValueError("Icon should be a MIME string")
 
         if len(v.split("/")) != 2:
             raise ValueError("Icon needs two parts of MIME string")
 
         return v
 
+    @validator("extra")
+    def extra_check(cls, value):
+        limit = 400_000
+        if value and value.metadata and len(json.dumps(value.metadata)) > limit:
+            raise ValueError(
+                f"metadata should be less than {limit} bytes when serialized to JSON"
+            )
+        return value
+
 
 class UpdateResourcePayload(BaseModel):
-    title: Optional[str] = None
-    summary: Optional[str] = None
-    slug: Optional[SlugString] = None
-    icon: Optional[str] = None
+    title: Optional[str] = FieldDefaults.title
+    summary: Optional[str] = FieldDefaults.summary
+    slug: Optional[SlugString] = FieldDefaults.slug
     thumbnail: Optional[str] = None
     layout: Optional[str] = None
+    metadata: Optional[InputMetadata] = None
     usermetadata: Optional[UserMetadata] = None
-    uservectors: Optional[UserVectorsWrapper] = None
     fieldmetadata: Optional[List[UserFieldMetadata]] = None
     origin: Optional[Origin] = None
-
-    files: Dict[FieldIdString, FileField] = {}
-    links: Dict[FieldIdString, LinkField] = {}
-    texts: Dict[FieldIdString, TextField] = {}
-    layouts: Dict[FieldIdString, InputLayoutField] = {}
-    conversations: Dict[FieldIdString, InputConversationField] = {}
-    keywordsets: Dict[FieldIdString, FieldKeywordset] = {}
-    datetimes: Dict[FieldIdString, FieldDatetime] = {}
-
-    # Processing options
+    extra: Optional[Extra] = None
+    files: Dict[FieldIdString, FileField] = FieldDefaults.files
+    links: Dict[FieldIdString, LinkField] = FieldDefaults.links
+    texts: Dict[FieldIdString, TextField] = FieldDefaults.texts
+    layouts: Dict[FieldIdString, InputLayoutField] = FieldDefaults.layouts
+    conversations: Dict[FieldIdString, InputConversationField] = (
+        FieldDefaults.conversations
+    )
+    keywordsets: Dict[FieldIdString, FieldKeywordset] = FieldDefaults.keywordsets
+    datetimes: Dict[FieldIdString, FieldDatetime] = FieldDefaults.datetimes
     processing_options: Optional[PushProcessingOptions] = PushProcessingOptions()
+    security: Optional[ResourceSecurity] = Field(
+        default=None,
+        title="Security",
+        description="Security metadata for the resource. It can be used to have fine-grained control over who can access the resource.",  # noqa
+    )
 
 
 class ResourceCreated(BaseModel):
     uuid: str
     elapsed: Optional[float] = None
     seqid: Optional[int] = None
```

## Comparing `nucliadb_models-2.9.0.post267.dist-info/RECORD` & `nucliadb_models-3.0.0.post414.dist-info/RECORD`

 * *Files 18% similar despite different names*

```diff
@@ -1,27 +1,31 @@
-nucliadb_models/__init__.py,sha256=IUBE8gHRvnW2_aM0nJujs41x20M15tP-dHD5fWCqKAg,1273
-nucliadb_models/cluster.py,sha256=GYxrnCsFZeoG5-hR8TE7P5_FHpIHZ3mmkKBjYmNaP9s,2314
-nucliadb_models/common.py,sha256=sioXYUWkDkY1xNAsHJBkqHsxlcFOejqbBqyV6lyVf_A,6240
-nucliadb_models/conversation.py,sha256=WhIf3O650D2ZXqRN_o8v0ZyHmP-MP0oDk-kxQl8p69M,3982
+nucliadb_models/__init__.py,sha256=MJJNUFGx_iAhPI1RXHZ2Q-IELba7nhxWpAtonK_2aZY,1379
+nucliadb_models/common.py,sha256=SyckAlK1XIHWvUnDFKxHVB7YvNY2VqxDIrCuBcxc7Ts,7296
+nucliadb_models/configuration.py,sha256=A5d8PPpEQm27S_d2ExItlc4Z6NFLGzR6dnAg_IfBUUo,1786
+nucliadb_models/conversation.py,sha256=4MIkwIylRSjVOq4sep-14grHzsw5wwDbJcSjVc9XbIo,4242
 nucliadb_models/datetime.py,sha256=ub1pdVWATc2dO7UxWrH6F_Se0woeC_DrGkQOWbvPeWM,1682
-nucliadb_models/entities.py,sha256=05jENrE_5C89m38i92XyiUP_SbzPcp3j1hkAEmLbQbw,3023
-nucliadb_models/extracted.py,sha256=lC-73W_ZpLabgmv2Ntpe1HCet8clzJFARlchs59_oMk,8090
+nucliadb_models/entities.py,sha256=QnwOhVa53PfXtGZIFVTCpvFWcxN2wI2hjgkE3V-IPv4,3952
+nucliadb_models/export_import.py,sha256=d52MjLdMA9IuAMXdGsY_2y4ltpPZicIzkkex8aRFaDE,1255
+nucliadb_models/extracted.py,sha256=8yRHeH20cMPBXLdJEIA6GgXwnHwK1cX1gOroCNR5zfc,8873
 nucliadb_models/file.py,sha256=92b_t3tNc20ecqv11cYFVe9ds_jjWkcjScfTMtCif7s,2191
 nucliadb_models/keywordset.py,sha256=6xMBTZvhjOlPcioqC1wN3IhgWfoYt7OHtC3ucQ1IzKc,1735
-nucliadb_models/labels.py,sha256=kfcxPY7gvyQxfNzCZN0DcYM6m7mpIxBOjlU8QQjiDTw,1468
+nucliadb_models/labels.py,sha256=oAbcCsz5neNCLbdCa1nXO1HrZ1iSGIR48W443nub8Sc,3508
 nucliadb_models/layout.py,sha256=3H5XP4gNlO9AphlbpEDrVc-M0i_yMhwmzIybYzhcI44,3147
-nucliadb_models/link.py,sha256=HovN2zkCD8SbUvIuj33ek4HQ-Td1_mLjsds_V19igjI,2177
-nucliadb_models/metadata.py,sha256=n0X-txSJDMjZ5boCW1ovyQZ3uFiN_nMxI1sGtwI4-Co,11144
+nucliadb_models/link.py,sha256=X82foCo1ikYMnugdEGPoEBng4jokO8J5kfPaBa_yyEY,2581
+nucliadb_models/metadata.py,sha256=zvJJ5ZdvPrqvzF-0dJfwIcseFZcJpZfj1X1Asxuh7PA,12517
+nucliadb_models/notifications.py,sha256=43ZdEPRG85icakpYZEHLrEo_GMi93KRyBQdwcFwVEBI,4355
 nucliadb_models/processing.py,sha256=9bSD2fYnUiDThxgzpeq3DiK0AgfDu31F7YmxE3J5Qak,998
 nucliadb_models/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-nucliadb_models/resource.py,sha256=U0hfVjIo3FYGHizUw_4JCfkrdnTBjsbLDFgUsjW0EM0,7361
-nucliadb_models/search.py,sha256=u0qT5fkpoARrVNh8ODbg_mDWaCpKCAiOd0N9w4wyk9I,12454
+nucliadb_models/resource.py,sha256=MqE2vNvC07rI8n755b3f1S33aBY06HyrCFgvXgXtB3k,9610
+nucliadb_models/search.py,sha256=wkY_lq8SIUA26QaW61hzqLo9lUtsEkBSc5ZZ6XY1HjA,44060
+nucliadb_models/security.py,sha256=RewdzQ55nPZ9V7B0NX9KHeWg6B4Hg_RkeiFv2TQyrjs,1402
 nucliadb_models/synonyms.py,sha256=nvC2dmmFahhyK0cDHzDy-THQ07_LySbIPQphRDpn_7w,1904
-nucliadb_models/text.py,sha256=mYh4KmMvTqQu5mdLh45WX1q-4cJrlzbdD7NBxG2TLSc,2373
-nucliadb_models/utils.py,sha256=pweXqBd4jhRq1_KQK7DvMpi58I3LDUFK2VFzGvPGUiI,1960
-nucliadb_models/vectors.py,sha256=uvkRrFMv8sCaRoNFq_dUzNiBc4T3REKC4sN31Po_xGA,2553
-nucliadb_models/writer.py,sha256=B9PEvv1OE1EHAVOdqT45s8hveHek7tB0Ci0-7eoqusM,4372
-nucliadb_models-2.9.0.post267.dist-info/METADATA,sha256=4_1s9gndbpk_5uME0j1w3eOb-SCJo9zGwJMe29Mo90A,415
-nucliadb_models-2.9.0.post267.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-nucliadb_models-2.9.0.post267.dist-info/top_level.txt,sha256=UrY1I8oeovIRwkXLYplssTrxQdUjhSEFDFbnwaIV3tA,16
-nucliadb_models-2.9.0.post267.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
-nucliadb_models-2.9.0.post267.dist-info/RECORD,,
+nucliadb_models/text.py,sha256=p4a2nvoW9mJp7OIYQ_sqz2v00jdiK64-v3OQ35Oji84,2785
+nucliadb_models/trainset.py,sha256=bJMnL9vvbVqicERRsrogPXYjMmqelBSAWdTEWPW_xME,933
+nucliadb_models/utils.py,sha256=5wq_gqX71h8JZap-1ugnPuQXP3AnyRWv6fC66S8JCsU,2181
+nucliadb_models/vectors.py,sha256=KxyK3DPPAGFXS10c20q3ESS0EUOMe7IH0tmyROfTS64,2367
+nucliadb_models/writer.py,sha256=3JZDb150iJWI95jtGVi8GQ0nt4u50OJMUD_Usw1fwd4,7766
+nucliadb_models-3.0.0.post414.dist-info/METADATA,sha256=1EEoth9LsFGrv5vxeRamLebyLAptJ2g18i_uESNjEZI,657
+nucliadb_models-3.0.0.post414.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+nucliadb_models-3.0.0.post414.dist-info/top_level.txt,sha256=UrY1I8oeovIRwkXLYplssTrxQdUjhSEFDFbnwaIV3tA,16
+nucliadb_models-3.0.0.post414.dist-info/zip-safe,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+nucliadb_models-3.0.0.post414.dist-info/RECORD,,
```

