# Comparing `tmp/nucliadb_utils-2.9.0.post267-py3-none-any.whl.zip` & `tmp/nucliadb_utils-3.0.0.post414-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,71 +1,72 @@
-Zip file size: 101087 bytes, number of entries: 69
--rw-r--r--  2.0 unx      895 b- defN 23-May-15 13:40 nucliadb_utils/__init__.py
--rw-r--r--  2.0 unx     5877 b- defN 23-May-15 13:40 nucliadb_utils/authentication.py
--rw-r--r--  2.0 unx     4518 b- defN 23-May-15 13:40 nucliadb_utils/clandestined.py
--rw-r--r--  2.0 unx     1879 b- defN 23-May-15 13:40 nucliadb_utils/const.py
--rw-r--r--  2.0 unx     1138 b- defN 23-May-15 13:40 nucliadb_utils/exceptions.py
--rw-r--r--  2.0 unx     2276 b- defN 23-May-15 13:40 nucliadb_utils/featureflagging.py
--rw-r--r--  2.0 unx     2212 b- defN 23-May-15 13:40 nucliadb_utils/grpc.py
--rw-r--r--  2.0 unx     3474 b- defN 23-May-15 13:40 nucliadb_utils/indexing.py
--rw-r--r--  2.0 unx      867 b- defN 23-May-15 13:40 nucliadb_utils/keys.py
--rw-r--r--  2.0 unx     6093 b- defN 23-May-15 13:40 nucliadb_utils/nats.py
--rw-r--r--  2.0 unx     1173 b- defN 23-May-15 13:40 nucliadb_utils/partition.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-15 13:40 nucliadb_utils/py.typed
--rw-r--r--  2.0 unx     1713 b- defN 23-May-15 13:40 nucliadb_utils/run.py
--rw-r--r--  2.0 unx     4687 b- defN 23-May-15 13:40 nucliadb_utils/settings.py
--rw-r--r--  2.0 unx      890 b- defN 23-May-15 13:40 nucliadb_utils/store.py
--rw-r--r--  2.0 unx     6621 b- defN 23-May-15 13:40 nucliadb_utils/transaction.py
--rw-r--r--  2.0 unx    13338 b- defN 23-May-15 13:40 nucliadb_utils/utilities.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb_utils/audit/__init__.py
--rw-r--r--  2.0 unx     2419 b- defN 23-May-15 13:40 nucliadb_utils/audit/audit.py
--rw-r--r--  2.0 unx     2951 b- defN 23-May-15 13:40 nucliadb_utils/audit/basic.py
--rw-r--r--  2.0 unx     8134 b- defN 23-May-15 13:40 nucliadb_utils/audit/stream.py
--rw-r--r--  2.0 unx      900 b- defN 23-May-15 13:40 nucliadb_utils/cache/__init__.py
--rw-r--r--  2.0 unx      975 b- defN 23-May-15 13:40 nucliadb_utils/cache/exceptions.py
--rw-r--r--  2.0 unx     1216 b- defN 23-May-15 13:40 nucliadb_utils/cache/memcache.py
--rw-r--r--  2.0 unx     7089 b- defN 23-May-15 13:40 nucliadb_utils/cache/nats.py
--rw-r--r--  2.0 unx     1654 b- defN 23-May-15 13:40 nucliadb_utils/cache/pubsub.py
--rw-r--r--  2.0 unx     3136 b- defN 23-May-15 13:40 nucliadb_utils/cache/redis.py
--rw-r--r--  2.0 unx     1302 b- defN 23-May-15 13:40 nucliadb_utils/cache/settings.py
--rw-r--r--  2.0 unx     5906 b- defN 23-May-15 13:40 nucliadb_utils/cache/utility.py
--rw-r--r--  2.0 unx      833 b- defN 23-May-15 13:40 nucliadb_utils/fastapi/__init__.py
--rw-r--r--  2.0 unx     1737 b- defN 23-May-15 13:40 nucliadb_utils/fastapi/openapi.py
--rw-r--r--  2.0 unx     3631 b- defN 23-May-15 13:40 nucliadb_utils/fastapi/run.py
--rw-r--r--  2.0 unx     3786 b- defN 23-May-15 13:40 nucliadb_utils/fastapi/versioning.py
--rw-r--r--  2.0 unx      855 b- defN 23-May-15 13:40 nucliadb_utils/storages/__init__.py
--rw-r--r--  2.0 unx     2274 b- defN 23-May-15 13:40 nucliadb_utils/storages/exceptions.py
--rw-r--r--  2.0 unx    24880 b- defN 23-May-15 13:40 nucliadb_utils/storages/gcs.py
--rw-r--r--  2.0 unx    10180 b- defN 23-May-15 13:40 nucliadb_utils/storages/local.py
--rw-r--r--  2.0 unx     2099 b- defN 23-May-15 13:40 nucliadb_utils/storages/nuclia.py
--rw-r--r--  2.0 unx    17174 b- defN 23-May-15 13:40 nucliadb_utils/storages/pg.py
--rw-r--r--  2.0 unx    15029 b- defN 23-May-15 13:40 nucliadb_utils/storages/s3.py
--rw-r--r--  2.0 unx     1276 b- defN 23-May-15 13:40 nucliadb_utils/storages/settings.py
--rw-r--r--  2.0 unx    17691 b- defN 23-May-15 13:40 nucliadb_utils/storages/storage.py
--rw-r--r--  2.0 unx      994 b- defN 23-May-15 13:40 nucliadb_utils/tests/__init__.py
--rw-r--r--  2.0 unx    10691 b- defN 23-May-15 13:40 nucliadb_utils/tests/asyncbenchmark.py
--rw-r--r--  2.0 unx     1043 b- defN 23-May-15 13:40 nucliadb_utils/tests/clandestined.py
--rw-r--r--  2.0 unx     1686 b- defN 23-May-15 13:40 nucliadb_utils/tests/conftest.py
--rw-r--r--  2.0 unx     2758 b- defN 23-May-15 13:40 nucliadb_utils/tests/gcs.py
--rw-r--r--  2.0 unx     1513 b- defN 23-May-15 13:40 nucliadb_utils/tests/indexing.py
--rw-r--r--  2.0 unx     1310 b- defN 23-May-15 13:40 nucliadb_utils/tests/local.py
--rw-r--r--  2.0 unx     7699 b- defN 23-May-15 13:40 nucliadb_utils/tests/nats.py
--rw-r--r--  2.0 unx     2216 b- defN 23-May-15 13:40 nucliadb_utils/tests/s3.py
--rw-r--r--  2.0 unx      833 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/__init__.py
--rw-r--r--  2.0 unx     5159 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/test_authentication.py
--rw-r--r--  2.0 unx    14507 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/test_clandestined.py
--rw-r--r--  2.0 unx     1949 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/test_clandestined_collision.py
--rw-r--r--  2.0 unx     5303 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/test_clandestined_rendezvous_hash.py
--rw-r--r--  2.0 unx     3083 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/test_nats.py
--rw-r--r--  2.0 unx     1910 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/test_run.py
--rw-r--r--  2.0 unx     1189 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/test_tests.py
--rw-r--r--  2.0 unx     3408 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/test_transaction.py
--rw-r--r--  2.0 unx     5277 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/test_utilities.py
--rw-r--r--  2.0 unx      833 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/storages/__init__.py
--rw-r--r--  2.0 unx    16726 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/storages/test_pg.py
--rw-r--r--  2.0 unx     5638 b- defN 23-May-15 13:40 nucliadb_utils/tests/unit/storages/test_storage.py
--rw-r--r--  2.0 unx     1816 b- defN 23-May-15 13:42 nucliadb_utils-2.9.0.post267.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-15 13:42 nucliadb_utils-2.9.0.post267.dist-info/WHEEL
--rw-r--r--  2.0 unx       15 b- defN 23-May-15 13:42 nucliadb_utils-2.9.0.post267.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 23-May-15 13:41 nucliadb_utils-2.9.0.post267.dist-info/zip-safe
--rw-rw-r--  2.0 unx     6208 b- defN 23-May-15 13:42 nucliadb_utils-2.9.0.post267.dist-info/RECORD
-69 files, 299470 bytes uncompressed, 91139 bytes compressed:  69.6%
+Zip file size: 106965 bytes, number of entries: 70
+-rw-r--r--  2.0 unx      895 b- defN 24-Apr-10 13:42 nucliadb_utils/__init__.py
+-rw-r--r--  2.0 unx     2894 b- defN 24-Apr-10 13:42 nucliadb_utils/asyncio_utils.py
+-rw-r--r--  2.0 unx     6071 b- defN 24-Apr-10 13:42 nucliadb_utils/authentication.py
+-rw-r--r--  2.0 unx     2520 b- defN 24-Apr-10 13:42 nucliadb_utils/const.py
+-rw-r--r--  2.0 unx     2470 b- defN 24-Apr-10 13:42 nucliadb_utils/debug.py
+-rw-r--r--  2.0 unx     1094 b- defN 24-Apr-10 13:42 nucliadb_utils/exceptions.py
+-rw-r--r--  2.0 unx     3060 b- defN 24-Apr-10 13:42 nucliadb_utils/featureflagging.py
+-rw-r--r--  2.0 unx     3336 b- defN 24-Apr-10 13:42 nucliadb_utils/grpc.py
+-rw-r--r--  2.0 unx     1599 b- defN 24-Apr-10 13:42 nucliadb_utils/helpers.py
+-rw-r--r--  2.0 unx     3462 b- defN 24-Apr-10 13:42 nucliadb_utils/indexing.py
+-rw-r--r--  2.0 unx     8224 b- defN 24-Apr-10 13:42 nucliadb_utils/nats.py
+-rw-r--r--  2.0 unx     1173 b- defN 24-Apr-10 13:42 nucliadb_utils/partition.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 13:42 nucliadb_utils/py.typed
+-rw-r--r--  2.0 unx     1713 b- defN 24-Apr-10 13:42 nucliadb_utils/run.py
+-rw-r--r--  2.0 unx     7214 b- defN 24-Apr-10 13:42 nucliadb_utils/settings.py
+-rw-r--r--  2.0 unx     2702 b- defN 24-Apr-10 13:42 nucliadb_utils/signals.py
+-rw-r--r--  2.0 unx      890 b- defN 24-Apr-10 13:42 nucliadb_utils/store.py
+-rw-r--r--  2.0 unx     7212 b- defN 24-Apr-10 13:42 nucliadb_utils/transaction.py
+-rw-r--r--  2.0 unx    13981 b- defN 24-Apr-10 13:42 nucliadb_utils/utilities.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb_utils/audit/__init__.py
+-rw-r--r--  2.0 unx     2698 b- defN 24-Apr-10 13:42 nucliadb_utils/audit/audit.py
+-rw-r--r--  2.0 unx     3267 b- defN 24-Apr-10 13:42 nucliadb_utils/audit/basic.py
+-rw-r--r--  2.0 unx     9388 b- defN 24-Apr-10 13:42 nucliadb_utils/audit/stream.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb_utils/cache/__init__.py
+-rw-r--r--  2.0 unx      975 b- defN 24-Apr-10 13:42 nucliadb_utils/cache/exceptions.py
+-rw-r--r--  2.0 unx     7089 b- defN 24-Apr-10 13:42 nucliadb_utils/cache/nats.py
+-rw-r--r--  2.0 unx     1654 b- defN 24-Apr-10 13:42 nucliadb_utils/cache/pubsub.py
+-rw-r--r--  2.0 unx     1050 b- defN 24-Apr-10 13:42 nucliadb_utils/cache/settings.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb_utils/fastapi/__init__.py
+-rw-r--r--  2.0 unx     1737 b- defN 24-Apr-10 13:42 nucliadb_utils/fastapi/openapi.py
+-rw-r--r--  2.0 unx     3631 b- defN 24-Apr-10 13:42 nucliadb_utils/fastapi/run.py
+-rw-r--r--  2.0 unx     3774 b- defN 24-Apr-10 13:42 nucliadb_utils/fastapi/versioning.py
+-rw-r--r--  2.0 unx      872 b- defN 24-Apr-10 13:42 nucliadb_utils/storages/__init__.py
+-rw-r--r--  2.0 unx     2417 b- defN 24-Apr-10 13:42 nucliadb_utils/storages/exceptions.py
+-rw-r--r--  2.0 unx    27076 b- defN 24-Apr-10 13:42 nucliadb_utils/storages/gcs.py
+-rw-r--r--  2.0 unx    10037 b- defN 24-Apr-10 13:42 nucliadb_utils/storages/local.py
+-rw-r--r--  2.0 unx     2097 b- defN 24-Apr-10 13:42 nucliadb_utils/storages/nuclia.py
+-rw-r--r--  2.0 unx    18610 b- defN 24-Apr-10 13:42 nucliadb_utils/storages/pg.py
+-rw-r--r--  2.0 unx    18794 b- defN 24-Apr-10 13:42 nucliadb_utils/storages/s3.py
+-rw-r--r--  2.0 unx     1276 b- defN 24-Apr-10 13:42 nucliadb_utils/storages/settings.py
+-rw-r--r--  2.0 unx    20559 b- defN 24-Apr-10 13:42 nucliadb_utils/storages/storage.py
+-rw-r--r--  2.0 unx     1456 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/__init__.py
+-rw-r--r--  2.0 unx    10695 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/asyncbenchmark.py
+-rw-r--r--  2.0 unx     1876 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/conftest.py
+-rw-r--r--  2.0 unx     3098 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/gcs.py
+-rw-r--r--  2.0 unx     1513 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/indexing.py
+-rw-r--r--  2.0 unx     1343 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/local.py
+-rw-r--r--  2.0 unx     7701 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/nats.py
+-rw-r--r--  2.0 unx     1819 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/pg.py
+-rw-r--r--  2.0 unx     2330 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/s3.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/__init__.py
+-rw-r--r--  2.0 unx     2359 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/test_asyncio_utils.py
+-rw-r--r--  2.0 unx     5301 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/test_authentication.py
+-rw-r--r--  2.0 unx     1574 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/test_helpers.py
+-rw-r--r--  2.0 unx     4436 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/test_nats.py
+-rw-r--r--  2.0 unx     1910 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/test_run.py
+-rw-r--r--  2.0 unx     2647 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/test_signals.py
+-rw-r--r--  2.0 unx     1189 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/test_tests.py
+-rw-r--r--  2.0 unx     3917 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/test_transaction.py
+-rw-r--r--  2.0 unx     5099 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/test_utilities.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/storages/__init__.py
+-rw-r--r--  2.0 unx     1924 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/storages/test_aws.py
+-rw-r--r--  2.0 unx     3554 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/storages/test_gcs.py
+-rw-r--r--  2.0 unx    17000 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/storages/test_pg.py
+-rw-r--r--  2.0 unx     6892 b- defN 24-Apr-10 13:42 nucliadb_utils/tests/unit/storages/test_storage.py
+-rw-r--r--  2.0 unx     1975 b- defN 24-Apr-10 13:44 nucliadb_utils-3.0.0.post414.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-10 13:44 nucliadb_utils-3.0.0.post414.dist-info/WHEEL
+-rw-r--r--  2.0 unx       15 b- defN 24-Apr-10 13:44 nucliadb_utils-3.0.0.post414.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 24-Apr-10 13:43 nucliadb_utils-3.0.0.post414.dist-info/zip-safe
+-rw-rw-r--  2.0 unx     6273 b- defN 24-Apr-10 13:44 nucliadb_utils-3.0.0.post414.dist-info/RECORD
+70 files, 309667 bytes uncompressed, 96927 bytes compressed:  68.7%
```

## zipnote {}

```diff
@@ -1,32 +1,35 @@
 Filename: nucliadb_utils/__init__.py
 Comment: 
 
-Filename: nucliadb_utils/authentication.py
+Filename: nucliadb_utils/asyncio_utils.py
 Comment: 
 
-Filename: nucliadb_utils/clandestined.py
+Filename: nucliadb_utils/authentication.py
 Comment: 
 
 Filename: nucliadb_utils/const.py
 Comment: 
 
+Filename: nucliadb_utils/debug.py
+Comment: 
+
 Filename: nucliadb_utils/exceptions.py
 Comment: 
 
 Filename: nucliadb_utils/featureflagging.py
 Comment: 
 
 Filename: nucliadb_utils/grpc.py
 Comment: 
 
-Filename: nucliadb_utils/indexing.py
+Filename: nucliadb_utils/helpers.py
 Comment: 
 
-Filename: nucliadb_utils/keys.py
+Filename: nucliadb_utils/indexing.py
 Comment: 
 
 Filename: nucliadb_utils/nats.py
 Comment: 
 
 Filename: nucliadb_utils/partition.py
 Comment: 
@@ -36,14 +39,17 @@
 
 Filename: nucliadb_utils/run.py
 Comment: 
 
 Filename: nucliadb_utils/settings.py
 Comment: 
 
+Filename: nucliadb_utils/signals.py
+Comment: 
+
 Filename: nucliadb_utils/store.py
 Comment: 
 
 Filename: nucliadb_utils/transaction.py
 Comment: 
 
 Filename: nucliadb_utils/utilities.py
@@ -63,32 +69,23 @@
 
 Filename: nucliadb_utils/cache/__init__.py
 Comment: 
 
 Filename: nucliadb_utils/cache/exceptions.py
 Comment: 
 
-Filename: nucliadb_utils/cache/memcache.py
-Comment: 
-
 Filename: nucliadb_utils/cache/nats.py
 Comment: 
 
 Filename: nucliadb_utils/cache/pubsub.py
 Comment: 
 
-Filename: nucliadb_utils/cache/redis.py
-Comment: 
-
 Filename: nucliadb_utils/cache/settings.py
 Comment: 
 
-Filename: nucliadb_utils/cache/utility.py
-Comment: 
-
 Filename: nucliadb_utils/fastapi/__init__.py
 Comment: 
 
 Filename: nucliadb_utils/fastapi/openapi.py
 Comment: 
 
 Filename: nucliadb_utils/fastapi/run.py
@@ -126,17 +123,14 @@
 
 Filename: nucliadb_utils/tests/__init__.py
 Comment: 
 
 Filename: nucliadb_utils/tests/asyncbenchmark.py
 Comment: 
 
-Filename: nucliadb_utils/tests/clandestined.py
-Comment: 
-
 Filename: nucliadb_utils/tests/conftest.py
 Comment: 
 
 Filename: nucliadb_utils/tests/gcs.py
 Comment: 
 
 Filename: nucliadb_utils/tests/indexing.py
@@ -144,65 +138,74 @@
 
 Filename: nucliadb_utils/tests/local.py
 Comment: 
 
 Filename: nucliadb_utils/tests/nats.py
 Comment: 
 
-Filename: nucliadb_utils/tests/s3.py
+Filename: nucliadb_utils/tests/pg.py
 Comment: 
 
-Filename: nucliadb_utils/tests/unit/__init__.py
+Filename: nucliadb_utils/tests/s3.py
 Comment: 
 
-Filename: nucliadb_utils/tests/unit/test_authentication.py
+Filename: nucliadb_utils/tests/unit/__init__.py
 Comment: 
 
-Filename: nucliadb_utils/tests/unit/test_clandestined.py
+Filename: nucliadb_utils/tests/unit/test_asyncio_utils.py
 Comment: 
 
-Filename: nucliadb_utils/tests/unit/test_clandestined_collision.py
+Filename: nucliadb_utils/tests/unit/test_authentication.py
 Comment: 
 
-Filename: nucliadb_utils/tests/unit/test_clandestined_rendezvous_hash.py
+Filename: nucliadb_utils/tests/unit/test_helpers.py
 Comment: 
 
 Filename: nucliadb_utils/tests/unit/test_nats.py
 Comment: 
 
 Filename: nucliadb_utils/tests/unit/test_run.py
 Comment: 
 
+Filename: nucliadb_utils/tests/unit/test_signals.py
+Comment: 
+
 Filename: nucliadb_utils/tests/unit/test_tests.py
 Comment: 
 
 Filename: nucliadb_utils/tests/unit/test_transaction.py
 Comment: 
 
 Filename: nucliadb_utils/tests/unit/test_utilities.py
 Comment: 
 
 Filename: nucliadb_utils/tests/unit/storages/__init__.py
 Comment: 
 
+Filename: nucliadb_utils/tests/unit/storages/test_aws.py
+Comment: 
+
+Filename: nucliadb_utils/tests/unit/storages/test_gcs.py
+Comment: 
+
 Filename: nucliadb_utils/tests/unit/storages/test_pg.py
 Comment: 
 
 Filename: nucliadb_utils/tests/unit/storages/test_storage.py
 Comment: 
 
-Filename: nucliadb_utils-2.9.0.post267.dist-info/METADATA
+Filename: nucliadb_utils-3.0.0.post414.dist-info/METADATA
 Comment: 
 
-Filename: nucliadb_utils-2.9.0.post267.dist-info/WHEEL
+Filename: nucliadb_utils-3.0.0.post414.dist-info/WHEEL
 Comment: 
 
-Filename: nucliadb_utils-2.9.0.post267.dist-info/top_level.txt
+Filename: nucliadb_utils-3.0.0.post414.dist-info/top_level.txt
 Comment: 
 
-Filename: nucliadb_utils-2.9.0.post267.dist-info/zip-safe
+Filename: nucliadb_utils-3.0.0.post414.dist-info/zip-safe
 Comment: 
 
-Filename: nucliadb_utils-2.9.0.post267.dist-info/RECORD
+Filename: nucliadb_utils-3.0.0.post414.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nucliadb_utils/authentication.py

```diff
@@ -27,28 +27,31 @@
 from starlette.authentication import AuthCredentials, AuthenticationBackend, BaseUser
 from starlette.exceptions import HTTPException
 from starlette.requests import HTTPConnection, Request
 from starlette.responses import RedirectResponse, Response
 from starlette.websockets import WebSocket
 
 
-class STFUser(BaseUser):
+class NucliaUser(BaseUser):
     def __init__(self, username: str) -> None:
         self.username = username
 
     @property
     def is_authenticated(self) -> bool:
         return True
 
     @property
     def display_name(self) -> str:
         return self.username
 
 
-class STFAuthenticationBackend(AuthenticationBackend):
+STFUser = NucliaUser
+
+
+class NucliaCloudAuthenticationBackend(AuthenticationBackend):
     def __init__(
         self,
         roles_header: str = "X-NUCLIADB-ROLES",
         user_header: str = "X-NUCLIADB-USER",
     ) -> None:
         self.roles_header = roles_header
         self.user_header = user_header
@@ -61,19 +64,22 @@
         else:
             header_roles = request.headers[self.roles_header]
             roles = header_roles.split(";")
             auth_creds = AuthCredentials(roles)
 
         if self.user_header in request.headers:
             user = request.headers[self.user_header]
-            stf_user: BaseUser = STFUser(username=user)
+            nuclia_user: BaseUser = NucliaUser(username=user)
         else:
-            stf_user = STFUser(username="anonymous")
+            nuclia_user = NucliaUser(username="anonymous")
+
+        return auth_creds, nuclia_user
+
 
-        return auth_creds, stf_user
+STFAuthenticationBackend = NucliaCloudAuthenticationBackend
 
 
 def has_required_scope(conn: HTTPConnection, scopes: typing.Sequence[str]) -> bool:
     for scope in scopes:
         if scope in conn.auth.scopes:
             return True
     return False
@@ -143,15 +149,18 @@
 
             return async_wrapper
 
         else:
             # Handle sync request/response functions.
             @functools.wraps(func)
             def sync_wrapper(*args: typing.Any, **kwargs: typing.Any) -> Response:
-                request = kwargs.get("request", args[idx])
+                try:
+                    request = kwargs["request"]
+                except KeyError:
+                    request = args[idx]
                 assert isinstance(request, Request)
 
                 if not has_required_scope(request, scopes_list):
                     if redirect is not None:
                         return RedirectResponse(
                             url=request.url_for(redirect), status_code=303
                         )
```

## nucliadb_utils/const.py

```diff
@@ -47,12 +47,37 @@
         Indexing resources on the IndexNode
         """
 
         name = "node"
         subject = "node.{node}"
         group = "node-{node}"
 
+    class KB_EXPORTS:
+        """
+        Exporting kbs
+        """
+
+        name = "ndb-exports"
+        subject = "ndb-exports"
+        group = "ndb-exports"
+
+    class KB_IMPORTS:
+        """
+        Importing kbs
+        """
+
+        name = "ndb-imports"
+        subject = "ndb-imports"
+        group = "ndb-imports"
+
 
 class Features:
     WAIT_FOR_INDEX = "nucliadb_wait_for_resource_index"
-    SEPARATE_PROCESSED_MESSAGE_WRITES = "nucliadb_separate_processed_message_writes"
-    AUDITING_BW_COMPAT_SHARD_COUNTER = "nucliadb_auditing_bw_compat_shard_counter"
+    ASK_YOUR_DOCUMENTS = "nucliadb_ask_your_documents"
+    EXPERIMENTAL_KB = "nucliadb_experimental_kb"
+    READ_REPLICA_SEARCHES = "nucliadb_read_replica_searches"
+    VERSIONED_PRIVATE_PREDICT = "nucliadb_versioned_private_predict"
+    PREDICT_QUERY_ENDPOINT = "nucliadb_predict_query_endpoint"
+    BACK_PRESSURE = "nucliadb_back_pressure"
+    REBALANCE_KB = "nucliadb_rebalance_kb"
+    CORS_MIDDLEWARE = "nucliadb_cors_middleware_enabled"
+    VECTORSETS_V2 = "nucliadb_vectorsets_v2"
```

## nucliadb_utils/exceptions.py

```diff
@@ -25,13 +25,9 @@
 
 class LimitsExceededError(Exception):
     def __init__(self, status_code: int, detail: str):
         self.status_code = status_code
         self.detail = detail
 
 
-class ShardsNotFound(Exception):
-    pass
-
-
 class ConfigurationError(Exception):
     pass
```

## nucliadb_utils/featureflagging.py

```diff
@@ -20,50 +20,75 @@
 import json
 from typing import Any, Optional
 
 import mrflagly
 import pydantic
 
 from nucliadb_utils import const
+from nucliadb_utils.settings import nuclia_settings, running_settings
 
 
 class Settings(pydantic.BaseSettings):
-    environment: str = pydantic.Field(
-        "local", env=["environment", "running_environment"]
-    )
     flag_settings_url: Optional[str]
 
 
 DEFAULT_FLAG_DATA: dict[str, Any] = {
     # These are just defaults to use for local dev and tests
     const.Features.WAIT_FOR_INDEX: {
         "rollout": 0,
         "variants": {"environment": ["none"]},
     },
-    const.Features.SEPARATE_PROCESSED_MESSAGE_WRITES: {
+    const.Features.ASK_YOUR_DOCUMENTS: {
         "rollout": 0,
-        "variants": {"environment": ["none"]},
+        "variants": {"environment": ["stage", "local"]},
+    },
+    const.Features.EXPERIMENTAL_KB: {
+        "rollout": 0,
+        "variants": {"environment": ["local"]},
+    },
+    const.Features.READ_REPLICA_SEARCHES: {
+        "rollout": 0,
+        "variants": {"environment": ["local"]},
+    },
+    const.Features.VERSIONED_PRIVATE_PREDICT: {
+        "rollout": 0,
+        "variants": {"environment": ["local"]},
+    },
+    const.Features.PREDICT_QUERY_ENDPOINT: {
+        "rollout": 0,
+        "variants": {"environment": ["local"]},
+    },
+    const.Features.BACK_PRESSURE: {
+        "rollout": 0,
+        "variants": {"environment": ["local"]},
+    },
+    const.Features.REBALANCE_KB: {
+        "rollout": 0,
+        "variants": {"environment": ["local"]},
+    },
+    const.Features.CORS_MIDDLEWARE: {
+        "rollout": 0,
+        "variants": {"environment": ["local"]},
     },
-    const.Features.AUDITING_BW_COMPAT_SHARD_COUNTER: {
+    const.Features.VECTORSETS_V2: {
         "rollout": 0,
         "variants": {"environment": ["none"]},
     },
 }
 
 
 class FlagService:
     def __init__(self):
-        self.settings = Settings()
-        if self.settings.flag_settings_url is None:
+        settings = Settings()
+        if settings.flag_settings_url is None:
             self.flag_service = mrflagly.FlagService(data=json.dumps(DEFAULT_FLAG_DATA))
         else:
-            self.flag_service = mrflagly.FlagService(
-                url=self.settings.flag_settings_url
-            )
+            self.flag_service = mrflagly.FlagService(url=settings.flag_settings_url)
 
     def enabled(
         self, flag_key: str, default: bool = False, context: Optional[dict] = None
     ) -> bool:
         if context is None:
             context = {}
-        context["environment"] = self.settings.environment
+        context["environment"] = running_settings.running_environment
+        context["zone"] = nuclia_settings.nuclia_zone
         return self.flag_service.enabled(flag_key, default=default, context=context)
```

## nucliadb_utils/grpc.py

```diff
@@ -13,47 +13,85 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
+import json
 import logging
 from typing import Optional
 
-from grpc import aio  # type: ignore
-from grpc import ChannelCredentials
+from grpc import ChannelCredentials, aio
+
 from nucliadb_telemetry.grpc import GRPCTelemetry
+from nucliadb_telemetry.grpc_sentry import SentryInterceptor
 from nucliadb_telemetry.utils import get_telemetry
 
 logger = logging.getLogger(__name__)
 
+RETRY_OPTIONS = [
+    (
+        "grpc.service_config",
+        json.dumps(
+            {
+                "name": [{}],  # require to enable retrying all methods
+                "retryPolicy": {
+                    "maxAttempts": 4,
+                    "initialBackoff": "0.02s",
+                    "maxBackoff": "2s",
+                    "backoffMultiplier": 2,
+                    "retryableStatusCodes": [
+                        "UNAVAILABLE",
+                        "DEADLINE_EXCEEDED",
+                        "ABORTED",
+                        "CANCELLED",
+                    ],
+                },
+                "waitForReady": True,
+            }
+        ),
+    ),
+    ("grpc.max_metadata_size", 1 * 1024 * 1024),
+]
+
 
 def get_traced_grpc_channel(
     address: str,
     service_name: str,
     credentials: Optional[ChannelCredentials] = None,
     variant: str = "",
     max_send_message: int = 100,
 ) -> aio.Channel:
     tracer_provider = get_telemetry(service_name)
     if tracer_provider is not None:  # pragma: no cover
         telemetry_grpc = GRPCTelemetry(service_name + variant, tracer_provider)
         channel = telemetry_grpc.init_client(
-            address, max_send_message=max_send_message, credentials=credentials
+            address,
+            max_send_message=max_send_message,
+            credentials=credentials,
+            options=RETRY_OPTIONS,
         )
     else:
-        channel = aio.insecure_channel(address)
+        options = [
+            ("grpc.max_receive_message_length", max_send_message * 1024 * 1024),
+            ("grpc.max_send_message_length", max_send_message * 1024 * 1024),
+        ] + RETRY_OPTIONS
+        channel = aio.insecure_channel(address, options=options)
     return channel
 
 
 def get_traced_grpc_server(service_name: str, max_receive_message: int = 100):
     tracer_provider = get_telemetry(service_name)
     if tracer_provider is not None:  # pragma: no cover
         otgrpc = GRPCTelemetry(f"{service_name}_grpc", tracer_provider)
-        server = otgrpc.init_server(max_receive_message=max_receive_message)
+
+        server = otgrpc.init_server(
+            max_receive_message=max_receive_message,
+            interceptors=[SentryInterceptor()],
+        )
     else:
         options = [
-            ("grpc.max_receive_message_length", max_receive_message * 1024 * 1024),
+            ("grpc.max_receive_message_length", max_receive_message * 1024 * 1024)
         ]
         server = aio.server(options=options)
     return server
```

## nucliadb_utils/indexing.py

```diff
@@ -19,16 +19,16 @@
 #
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import nats
 from nats.aio.client import Client
 from nats.js.client import JetStreamContext
 from nucliadb_protos.nodewriter_pb2 import IndexMessage  # type: ignore
-from nucliadb_telemetry.jetstream import JetStreamContextTelemetry
 
+from nucliadb_telemetry.jetstream import JetStreamContextTelemetry
 from nucliadb_utils import const, logger
 from nucliadb_utils.nats import get_traced_jetstream
 
 
 class IndexingUtility:
     nc: Optional[Client] = None
     js: Optional[Union[JetStreamContext, JetStreamContextTelemetry]] = None
@@ -66,15 +66,15 @@
 
         options: Dict[str, Any] = {
             "error_cb": self.error_cb,
             "closed_cb": self.closed_cb,
             "reconnected_cb": self.reconnected_cb,
         }
 
-        if self.nats_creds is not None:
+        if self.nats_creds:
             options["user_credentials"] = self.nats_creds
 
         if len(self.nats_servers) > 0:
             options["servers"] = self.nats_servers
 
         self.nc = await nats.connect(**options)
         self.js = get_traced_jetstream(self.nc, service_name or "nucliadb_indexing")
```

## nucliadb_utils/nats.py

```diff
@@ -17,39 +17,90 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 import asyncio
 import logging
 import time
 from functools import cached_property
-from typing import Any, Awaitable, Callable, Optional
+from typing import Any, Awaitable, Callable, Optional, Union
 
 import nats
 import nats.errors
 import nats.js.api
 from nats.aio.client import Client as NATSClient
 from nats.aio.client import Msg
 from nats.aio.subscription import Subscription
 from nats.js.client import JetStreamContext
+
 from nucliadb_telemetry.jetstream import JetStreamContextTelemetry
 from nucliadb_telemetry.utils import get_telemetry
 
 logger = logging.getLogger(__name__)
 
 
-def get_traced_jetstream(nc: NATSClient, service_name: str) -> JetStreamContext:
+def get_traced_jetstream(
+    nc: NATSClient, service_name: str
+) -> Union[JetStreamContext, JetStreamContextTelemetry]:
     jetstream = nc.jetstream()
     tracer_provider = get_telemetry(service_name)
 
     if tracer_provider is not None and jetstream is not None:  # pragma: no cover
         logger.info(f"Configuring {service_name} jetstream with telemetry")
         jetstream = JetStreamContextTelemetry(jetstream, service_name, tracer_provider)
     return jetstream
 
 
+class MessageProgressUpdater:
+    """
+    Context manager to send progress updates to NATS.
+
+    This should allow lower ack_wait time settings without causing
+    messages to be redelivered.
+    """
+
+    _task: asyncio.Task
+
+    def __init__(self, msg: Msg, timeout: float):
+        self.msg = msg
+        self.timeout = timeout
+
+    def start(self):
+        seqid = self.msg.reply.split(".")[5]
+        task_name = f"MessageProgressUpdater: {id(self)} (seqid={seqid})"
+        self._task = asyncio.create_task(self._progress(), name=task_name)
+
+    async def end(self):
+        self._task.cancel()
+        try:
+            await self._task
+        except asyncio.CancelledError:  # pragma: no cover
+            pass
+        except Exception:  # pragma: no cover
+            pass
+
+    async def __aenter__(self):
+        self.start()
+        return self
+
+    async def __aexit__(self, exc_type, exc_value, traceback):
+        await self.end()
+
+    async def _progress(self):
+        while True:
+            try:
+                await asyncio.sleep(self.timeout)
+                if self.msg._ackd:  # all done, do not mark with in_progress
+                    return
+                await self.msg.in_progress()
+            except (RuntimeError, asyncio.CancelledError):
+                return
+            except Exception:  # pragma: no cover
+                logger.exception("Error sending task progress to NATS")
+
+
 class NatsConnectionManager:
     _nc: NATSClient
     _subscriptions: list[tuple[Subscription, Callable[[], Awaitable[None]]]]
     _unhealthy_timeout = (
         10  # needs to be unhealth for 10 seconds to be unhealthy and force exit
     )
 
@@ -87,15 +138,15 @@
         options: dict[str, Any] = {
             "error_cb": self.error_cb,
             "closed_cb": self.closed_cb,
             "reconnected_cb": self.reconnected_cb,
             "disconnected_cb": self.disconnected_cb,
         }
 
-        if self._nats_creds is not None:
+        if self._nats_creds:
             options["user_credentials"] = self._nats_creds
 
         if len(self._nats_servers) > 0:
             options["servers"] = self._nats_servers
 
         async with self._lock:
             self._nc = await nats.connect(**options)
@@ -104,21 +155,21 @@
         async with self._lock:
             for sub, _ in self._subscriptions:
                 try:
                     await sub.drain()
                 except nats.errors.ConnectionClosedError:  # pragma: no cover
                     pass
             try:
-                await asyncio.wait_for(self.nc.drain(), timeout=1)
+                await asyncio.wait_for(self._nc.drain(), timeout=1)
             except (
                 nats.errors.ConnectionClosedError,
                 asyncio.TimeoutError,
             ):  # pragma: no cover
                 pass
-            await self.nc.close()
+            await self._nc.close()
             self._subscriptions = []
 
     async def disconnected_cb(self) -> None:
         logger.info("Disconnected from NATS!")
         self._last_unhealthy = time.monotonic()
 
     async def reconnected_cb(self):
@@ -150,33 +201,50 @@
         logger.info("Connection is closed on NATS")
 
     @property
     def nc(self) -> NATSClient:
         return self._nc
 
     @cached_property
-    def js(self) -> JetStreamContext:
+    def js(self) -> Union[JetStreamContext, JetStreamContextTelemetry]:
         return get_traced_jetstream(self._nc, self._service_name)
 
     async def subscribe(
         self,
         *,
         subject: str,
         queue: str,
         stream: str,
         cb: Callable[[Msg], Awaitable[None]],
         subscription_lost_cb: Callable[[], Awaitable[None]],
         flow_control: bool = False,
+        manual_ack: bool = True,
         config: Optional[nats.js.api.ConsumerConfig] = None,
     ) -> Subscription:
         sub = await self.js.subscribe(
             subject=subject,
             queue=queue,
             stream=stream,
             cb=cb,
             flow_control=flow_control,
+            manual_ack=manual_ack,
             config=config,
         )
 
         self._subscriptions.append((sub, subscription_lost_cb))
 
         return sub
+
+    async def _remove_subscription(self, subscription: Subscription):
+        async with self._lock:
+            sub_index = None
+            for index, (sub, _) in enumerate(self._subscriptions):
+                if sub is not subscription:
+                    continue
+                sub_index = index
+                break
+            if sub_index is not None:
+                self._subscriptions.pop(sub_index)
+
+    async def unsubscribe(self, subscription: Subscription):
+        await subscription.unsubscribe()
+        await self._remove_subscription(subscription)
```

## nucliadb_utils/settings.py

```diff
@@ -21,30 +21,32 @@
 from typing import Dict, List, Optional
 
 from pydantic import BaseSettings, Field
 from pydantic.class_validators import root_validator
 
 
 class RunningSettings(BaseSettings):
-    debug: bool = True
+    debug: bool = False
     sentry_url: Optional[str] = None
     running_environment: str = Field(
-        "local", env=["environment", "running_environment"]
+        default="local",
+        env=["environment", "running_environment"],
+        description="Running environment. One of: local, test, stage, prod",
     )
     metrics_port: int = 3030
     metrics_host: str = "0.0.0.0"
     serving_port: int = 8080
     serving_host: str = "0.0.0.0"
 
 
 running_settings = RunningSettings()
 
 
 class HTTPSettings(BaseSettings):
-    cors_origins: List[str] = ["http://localhost:4200"]
+    cors_origins: List[str] = ["*"]
 
 
 http_settings = HTTPSettings()
 
 
 class FileBackendConfig(str, Enum):
     GCS = "gcs"
@@ -61,60 +63,91 @@
         for member in cls:
             if member.value == value.lower():
                 return member
 
 
 class StorageSettings(BaseSettings):
     file_backend: FileBackendConfig = Field(
-        FileBackendConfig.NOT_SET, description="File backend storage type"
+        default=FileBackendConfig.NOT_SET, description="File backend storage type"
     )
 
-    gcs_base64_creds: Optional[str] = None
-    gcs_bucket: Optional[str] = None
-    gcs_location: Optional[str] = None
-    gcs_project: Optional[str] = None
-    gcs_bucket_labels: Dict[str, str] = {}
+    gcs_base64_creds: Optional[str] = Field(
+        default=None,
+        description="GCS JSON credentials of a service account encoded in Base64: https://cloud.google.com/iam/docs/service-account-overview",  # noqa
+    )
+    gcs_bucket: Optional[str] = Field(
+        default=None,
+        description="GCS Bucket name where files are stored: https://cloud.google.com/storage/docs/buckets",
+    )
+    gcs_location: Optional[str] = Field(
+        default=None,
+        description="GCS Bucket location: https://cloud.google.com/storage/docs/locations",
+    )
+    gcs_project: Optional[str] = Field(
+        default=None,
+        description="Google Cloud Project ID: https://cloud.google.com/resource-manager/docs/creating-managing-projects",  # noqa
+    )
+    gcs_bucket_labels: Dict[str, str] = Field(
+        default={},
+        description="Map of labels with which GCS buckets will be labeled with: https://cloud.google.com/storage/docs/tags-and-labels",  # noqa
+    )
     gcs_endpoint_url: str = "https://www.googleapis.com"
 
     s3_client_id: Optional[str] = None
     s3_client_secret: Optional[str] = None
     s3_ssl: bool = True
     s3_verify_ssl: bool = True
     s3_max_pool_connections: int = 30
     s3_endpoint: Optional[str] = None
     s3_region_name: Optional[str] = None
-    s3_bucket: Optional[str] = None
+    s3_bucket: Optional[str] = Field(
+        default=None, description="KnowledgeBox S3 bucket name template"
+    )
+    s3_bucket_tags: Dict[str, str] = Field(
+        default={},
+        description="Map of tags with which S3 buckets will be tagged with: https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketTagging.html",  # noqa
+    )
 
     local_files: Optional[str] = Field(
-        None,
-        description="If using LOCAL `file_backend` storage, directory files should be stored",
+        default=None,
+        description="If using LOCAL `file_backend` storage, directory where files should be stored",
+    )
+    upload_token_expiration: int = Field(
+        default=3,
+        description="Number of days that uploaded files are kept in Nulia's processing engine",
     )
-    upload_token_expiration: Optional[int] = 3
 
     driver_pg_url: Optional[str] = None  # match same env var for k/v storage
+    driver_pg_connection_pool_max_size: int = 20  # match same env var for k/v storage
 
 
 storage_settings = StorageSettings()
 
 
 class NucliaSettings(BaseSettings):
     nuclia_service_account: Optional[str] = None
     nuclia_public_url: str = "https://{zone}.nuclia.cloud"
-    nuclia_cluster_url: str = "http://nucliadb_proxy.processing.svc.cluster.local:8080"
+    nuclia_processing_cluster_url: str = (
+        "http://processing-api.processing.svc.cluster.local:8080"
+    )
     nuclia_inner_predict_url: str = "http://predict.learning.svc.cluster.local:8080"
+    learning_internal_svc_base_url = "http://{service}.learning.svc.cluster.local:8080"
 
     nuclia_zone: str = "europe-1"
     onprem: bool = True
 
     nuclia_jwt_key: Optional[str] = None
     nuclia_hash_seed: int = 42
     nuclia_partitions: int = 1
 
     dummy_processing: bool = False
     dummy_predict: bool = False
+    dummy_learning_services: bool = False
+    local_predict: bool = False
+    local_predict_headers: Dict[str, str] = {}
 
     @root_validator(pre=True)
     def check_onprem_does_not_use_jwt_key(cls, values):
         if values.get("onprem") and values.get("jwt_key") is not None:
             raise ValueError("Invalid validation")
         return values
 
@@ -129,14 +162,17 @@
 nucliadb_settings = NucliaDBSettings()
 
 
 class TransactionSettings(BaseSettings):
     transaction_jetstream_auth: Optional[str] = None
     transaction_jetstream_servers: List[str] = ["nats://localhost:4222"]
     transaction_local: bool = False
+    transaction_commit_timeout: int = Field(
+        default=60, description="Transaction commit timeout in seconds"
+    )
 
 
 transaction_settings = TransactionSettings()
 
 
 class IndexingSettings(BaseSettings):
     index_jetstream_servers: List[str] = []
@@ -154,7 +190,29 @@
     audit_jetstream_auth: Optional[str] = None
     audit_partitions: int = 3
     audit_stream: str = "audit"
     audit_hash_seed: int = 1234
 
 
 audit_settings = AuditSettings()
+
+
+class NATSConsumerSettings(BaseSettings):
+    # Read about message ordering:
+    #   https://docs.nats.io/nats-concepts/subject_mapping#when-is-deterministic-partitioning-needed
+
+    # NATS MaxAckPending controls how many messages can we handle simultaneously
+    # (look at the implementation for how concurrent indexing is implemented)
+    #
+    # NOTE this parameter don't actually change already existing consumers! If
+    # you want to update the value, you should use nats-cli to do so
+    nats_max_ack_pending: int = 10
+    nats_max_deliver: int = 10000
+    nats_ack_wait: int = 10 * 60
+    nats_idle_heartbeat: float = 5.0
+
+
+nats_consumer_settings = NATSConsumerSettings()
+
+
+def is_onprem_nucliadb() -> bool:
+    return nuclia_settings.nuclia_service_account is not None
```

## nucliadb_utils/transaction.py

```diff
@@ -17,22 +17,27 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
 import uuid
 from asyncio import Event
 from functools import partial
-from typing import Any, Dict, List, Optional, Union
+from typing import Any, Optional, Union
 
 import nats
 from nats.aio.client import Client
 from nats.js.client import JetStreamContext
-from nucliadb_protos.writer_pb2 import BrokerMessage, Notification
-from nucliadb_telemetry.jetstream import JetStreamContextTelemetry
+from nucliadb_protos.writer_pb2 import (
+    BrokerMessage,
+    BrokerMessageBlobReference,
+    Notification,
+    OpStatusWriter,
+)
 
+from nucliadb_telemetry.jetstream import JetStreamContextTelemetry
 from nucliadb_utils import const, logger
 from nucliadb_utils.cache.pubsub import PubSubDriver
 from nucliadb_utils.nats import get_traced_jetstream
 from nucliadb_utils.utilities import get_pubsub, has_feature
 
 
 class WaitFor:
@@ -40,14 +45,18 @@
     seq: Optional[int] = None
 
     def __init__(self, uuid: str, seq: Optional[int] = None):
         self.uuid = uuid
         self.seq = seq
 
 
+class TransactionCommitTimeoutError(Exception):
+    pass
+
+
 class LocalTransactionUtility:
     async def commit(
         self,
         writer: BrokerMessage,
         partition: int,
         wait: bool = False,
         target_subject: Optional[str] = None,
@@ -55,15 +64,17 @@
         from nucliadb_utils.utilities import get_ingest
 
         ingest = get_ingest()
 
         async def iterator(writer):
             yield writer
 
-        await ingest.ProcessMessage(iterator(writer))  # type: ignore
+        resp = await ingest.ProcessMessage(iterator(writer))  # type: ignore
+        if resp.status != OpStatusWriter.Status.OK:
+            logger.error(f"Local transaction failed processing {writer}")
         return 0
 
     async def finalize(self):
         pass
 
     async def initialize(self):
         pass
@@ -72,19 +83,21 @@
 class TransactionUtility:
     nc: Client
     js: Union[JetStreamContext, JetStreamContextTelemetry]
     pubsub: PubSubDriver
 
     def __init__(
         self,
-        nats_servers: List[str],
+        nats_servers: list[str],
         nats_creds: Optional[str] = None,
+        commit_timeout: int = 60,
     ):
         self.nats_creds = nats_creds
         self.nats_servers = nats_servers
+        self.commit_timeout = commit_timeout
 
     async def disconnected_cb(self):
         logger.info("Got disconnected from NATS!")
 
     async def reconnected_cb(self):
         # See who we are connected to on reconnect.
         logger.info("Got reconnected to NATS {url}".format(url=self.nc.connected_url))
@@ -134,23 +147,23 @@
             handler=partial_received,
             key=const.PubSubChannels.RESOURCE_NOTIFY.format(kbid=kbid),
             subscription_id=request_id,
         )
         return waiting_event
 
     async def initialize(self, service_name: Optional[str] = None):
-        self.pubsub = await get_pubsub()
+        self.pubsub = await get_pubsub()  # type: ignore
 
-        options: Dict[str, Any] = {
+        options: dict[str, Any] = {
             "error_cb": self.error_cb,
             "closed_cb": self.closed_cb,
             "reconnected_cb": self.reconnected_cb,
         }
 
-        if self.nats_creds is not None:
+        if self.nats_creds:
             options["user_credentials"] = self.nats_creds
 
         if len(self.nats_servers) > 0:
             options["servers"] = self.nats_servers
 
         self.nc = await nats.connect(**options)
         self.js = get_traced_jetstream(self.nc, service_name or "nucliadb")
@@ -160,43 +173,50 @@
             await self.nc.drain()
         except nats.errors.ConnectionClosedError:
             pass
         await self.nc.close()
 
     async def commit(
         self,
-        writer: BrokerMessage,
+        writer: Union[BrokerMessage, BrokerMessageBlobReference],
         partition: int,
         wait: bool = False,
         target_subject: Optional[
             str
         ] = None,  # allow customizing where to send the message
+        headers: Optional[dict[str, str]] = None,
     ) -> int:
+        headers = headers or {}
         waiting_event: Optional[Event] = None
 
         waiting_for = WaitFor(uuid=writer.uuid)
         request_id = uuid.uuid4().hex
 
         if wait:
             waiting_event = await self.wait_for_commited(
                 writer.kbid, waiting_for, request_id=request_id
             )
 
         if target_subject is None:
             target_subject = const.Streams.INGEST.subject.format(partition=partition)
 
-        res = await self.js.publish(target_subject, writer.SerializeToString())
+        res = await self.js.publish(
+            target_subject, writer.SerializeToString(), headers=headers
+        )
 
         waiting_for.seq = res.seq
 
         if wait and waiting_event is not None:
             try:
-                await asyncio.wait_for(waiting_event.wait(), timeout=30.0)
+                await asyncio.wait_for(
+                    waiting_event.wait(), timeout=self.commit_timeout
+                )
             except asyncio.TimeoutError:
                 logger.warning("Took too much to commit")
+                raise TransactionCommitTimeoutError()
             finally:
                 await self.stop_waiting(writer.kbid, request_id=request_id)
 
         logger.info(
             f" - Pushed message to ingest.  kb: {writer.kbid}, resource: {writer.uuid}, nucliadb seqid: {res.seq}, partition: {partition}"  # noqa
         )
         return res.seq
```

## nucliadb_utils/utilities.py

```diff
@@ -30,24 +30,23 @@
 
 from nucliadb_utils import featureflagging
 from nucliadb_utils.audit.audit import AuditStorage
 from nucliadb_utils.audit.basic import BasicAuditStorage
 from nucliadb_utils.audit.stream import StreamAuditStorage
 from nucliadb_utils.cache.nats import NatsPubsub
 from nucliadb_utils.cache.pubsub import PubSubDriver
-from nucliadb_utils.cache.redis import RedisPubsub
 from nucliadb_utils.cache.settings import settings as cache_settings
-from nucliadb_utils.cache.utility import Cache
 from nucliadb_utils.exceptions import ConfigurationError
 from nucliadb_utils.indexing import IndexingUtility
 from nucliadb_utils.nats import NatsConnectionManager
 from nucliadb_utils.partition import PartitionUtility
 from nucliadb_utils.settings import (
     FileBackendConfig,
     audit_settings,
+    indexing_settings,
     nuclia_settings,
     storage_settings,
     transaction_settings,
 )
 from nucliadb_utils.storages.settings import settings as extended_storage_settings
 from nucliadb_utils.store import MAIN
 
@@ -63,39 +62,39 @@
 class Utility(str, Enum):
     INGEST = "ingest"
     CHANNEL = "channel"
     PARTITION = "partition"
     PREDICT = "predict"
     PROCESSING = "processing"
     TRANSACTION = "transaction"
-    CACHE = "cache"
-    NODES = "nodes"
+    SHARD_MANAGER = "shard_manager"
     COUNTER = "counter"
-    CHITCHAT = "chitchat"
     PUBSUB = "pubsub"
     INDEXING = "indexing"
     AUDIT = "audit"
     STORAGE = "storage"
     TRAIN = "train"
     TRAIN_SERVER = "train_server"
     FEATURE_FLAGS = "feature_flags"
     NATS_MANAGER = "nats_manager"
+    LOCAL_STORAGE = "local_storage"
+    NUCLIA_STORAGE = "nuclia_storage"
+    MAINDB_DRIVER = "driver"
 
 
 def get_utility(ident: Union[Utility, str]):
     return MAIN.get(ident)
 
 
 def set_utility(ident: Union[Utility, str], util: Any):
     MAIN[ident] = util
 
 
 def clean_utility(ident: Union[Utility, str]):
-    if ident in MAIN:
-        del MAIN[ident]
+    MAIN.pop(ident, None)
 
 
 async def get_storage(
     gcs_scopes: Optional[List[str]] = None, service_name: Optional[str] = None
 ) -> Storage:
     if Utility.STORAGE in MAIN:
         return MAIN[Utility.STORAGE]
@@ -110,14 +109,15 @@
             verify_ssl=storage_settings.s3_verify_ssl,
             deadletter_bucket=extended_storage_settings.s3_deadletter_bucket,
             indexing_bucket=extended_storage_settings.s3_indexing_bucket,
             use_ssl=storage_settings.s3_ssl,
             region_name=storage_settings.s3_region_name,
             max_pool_connections=storage_settings.s3_max_pool_connections,
             bucket=storage_settings.s3_bucket,
+            bucket_tags=storage_settings.s3_bucket_tags,
         )
         set_utility(Utility.STORAGE, s3util)
         await s3util.initialize()
         logger.info("Configuring S3 Storage")
 
     elif storage_settings.file_backend == FileBackendConfig.GCS:
         from nucliadb_utils.storages.gcs import GCSStorage
@@ -137,15 +137,18 @@
         set_utility(Utility.STORAGE, gcsutil)
         await gcsutil.initialize(service_name)
         logger.info("Configuring GCS Storage")
 
     elif storage_settings.file_backend == FileBackendConfig.PG:
         from nucliadb_utils.storages.pg import PostgresStorage
 
-        pgutil = PostgresStorage(storage_settings.driver_pg_url)  # type: ignore
+        pgutil = PostgresStorage(
+            storage_settings.driver_pg_url,  # type: ignore
+            connection_pool_max_size=storage_settings.driver_pg_connection_pool_max_size,
+        )
         set_utility(Utility.STORAGE, pgutil)
         await pgutil.initialize()
         logger.info("Configuring Postgres Storage")
 
     elif storage_settings.file_backend == FileBackendConfig.LOCAL:
         if storage_settings.local_files is None:
             raise ConfigurationError("LOCAL_FILES env var not configured")
@@ -162,76 +165,72 @@
             "Invalid storage settings, please configure FILE_BACKEND"
         )
 
     return MAIN[Utility.STORAGE]
 
 
 def get_local_storage() -> LocalStorage:
-    if "local_storage" not in MAIN:
+    if Utility.LOCAL_STORAGE not in MAIN:
         from nucliadb_utils.storages.local import LocalStorage
 
-        MAIN["local_storage"] = LocalStorage(
+        MAIN[Utility.LOCAL_STORAGE] = LocalStorage(
             local_testing_files=extended_storage_settings.local_testing_files
         )
         logger.info("Configuring Local Storage")
-    return MAIN.get("local_storage", None)
+    return MAIN.get(Utility.LOCAL_STORAGE, None)
 
 
 async def get_nuclia_storage() -> NucliaStorage:
-    if "nuclia_storage" not in MAIN:
+    if Utility.NUCLIA_STORAGE not in MAIN:
         from nucliadb_utils.storages.nuclia import NucliaStorage
 
-        MAIN["nuclia_storage"] = NucliaStorage(
+        MAIN[Utility.NUCLIA_STORAGE] = NucliaStorage(
             nuclia_public_url=nuclia_settings.nuclia_public_url,
             nuclia_zone=nuclia_settings.nuclia_zone,
             service_account=nuclia_settings.nuclia_service_account,
         )
         logger.info("Configuring Nuclia Storage")
-        await MAIN["nuclia_storage"].initialize()
-    return MAIN.get("nuclia_storage", None)
-
-
-async def get_cache() -> Optional[Cache]:
-    util: Optional[Cache] = get_utility(Utility.CACHE)
-    if util is None and cache_settings.cache_enabled:
-        driver = Cache()
-        set_utility(Utility.CACHE, driver)
-        logger.info("Configuring cache")
+        await MAIN[Utility.NUCLIA_STORAGE].initialize()
+    return MAIN.get(Utility.NUCLIA_STORAGE, None)
 
-    cache: Optional[Cache] = get_utility(Utility.CACHE)
-    if cache and not cache.initialized:
-        await cache.initialize()
-    return cache
 
-
-async def get_pubsub() -> PubSubDriver:
+async def get_pubsub() -> Optional[PubSubDriver]:
     driver: Optional[PubSubDriver] = get_utility(Utility.PUBSUB)
     if driver is None:
-        if cache_settings.cache_pubsub_driver == "redis":
-            driver = RedisPubsub(cache_settings.cache_pubsub_redis_url)
-            set_utility(Utility.PUBSUB, driver)
-            logger.info("Configuring redis pubsub")
-        elif cache_settings.cache_pubsub_driver == "nats":
+        if cache_settings.cache_pubsub_nats_url:
+            logger.info("Configuring nats pubsub")
             driver = NatsPubsub(
                 hosts=cache_settings.cache_pubsub_nats_url,
                 user_credentials_file=cache_settings.cache_pubsub_nats_auth,
             )
             set_utility(Utility.PUBSUB, driver)
-            logger.info("Configuring nats pubsub")
-        else:  # pragma: no cover
-            raise NotImplementedError("Invalid driver")
+        else:
+            return None
     if not driver.initialized:
         await driver.initialize()
     return driver
 
 
 def get_ingest() -> WriterStub:
     return get_utility(Utility.INGEST)  # type: ignore
 
 
+def start_partitioning_utility() -> PartitionUtility:
+    util = PartitionUtility(
+        partitions=nuclia_settings.nuclia_partitions,
+        seed=nuclia_settings.nuclia_hash_seed,
+    )
+    set_utility(Utility.PARTITION, util)
+    return util
+
+
+def stop_partitioning_utility():
+    clean_utility(Utility.PARTITION)
+
+
 def get_partitioning() -> PartitionUtility:
     return get_utility(Utility.PARTITION)  # type: ignore
 
 
 def clear_global_cache():
     MAIN.clear()
 
@@ -249,22 +248,28 @@
 
 
 async def start_transaction_utility(
     service_name: Optional[str] = None,
 ) -> TransactionUtility:
     from nucliadb_utils.transaction import LocalTransactionUtility, TransactionUtility
 
+    current = get_transaction_utility()
+    if current is not None:
+        logger.debug("Warning, transaction utility was already set, ignoring")
+        return current
+
     if transaction_settings.transaction_local:
-        transaction_utility: Union[
-            LocalTransactionUtility, TransactionUtility
-        ] = LocalTransactionUtility()
+        transaction_utility: Union[LocalTransactionUtility, TransactionUtility] = (
+            LocalTransactionUtility()
+        )
     elif transaction_settings.transaction_jetstream_servers is not None:
         transaction_utility = TransactionUtility(
             nats_creds=transaction_settings.transaction_jetstream_auth,
             nats_servers=transaction_settings.transaction_jetstream_servers,
+            commit_timeout=transaction_settings.transaction_commit_timeout,
         )
         await transaction_utility.initialize(service_name)
     set_utility(Utility.TRANSACTION, transaction_utility)
     return transaction_utility  # type: ignore
 
 
 def get_transaction_utility() -> TransactionUtility:
@@ -274,14 +279,31 @@
 async def stop_transaction_utility() -> None:
     transaction_utility = get_transaction_utility()
     if transaction_utility:
         await transaction_utility.finalize()
         clean_utility(Utility.TRANSACTION)
 
 
+async def start_indexing_utility(service_name: Optional[str] = None) -> IndexingUtility:
+    indexing_utility = IndexingUtility(
+        nats_creds=indexing_settings.index_jetstream_auth,
+        nats_servers=indexing_settings.index_jetstream_servers,
+    )
+    await indexing_utility.initialize(service_name)
+    set_utility(Utility.INDEXING, indexing_utility)
+    return indexing_utility
+
+
+async def stop_indexing_utility():
+    indexing_utility = get_indexing()
+    if indexing_utility:
+        await indexing_utility.finalize()
+        clean_utility(Utility.INDEXING)
+
+
 def get_indexing() -> IndexingUtility:
     return get_utility(Utility.INDEXING)
 
 
 def get_audit() -> Optional[AuditStorage]:
     return get_utility(Utility.AUDIT)
```

## nucliadb_utils/audit/audit.py

```diff
@@ -20,15 +20,15 @@
 from typing import List, Optional
 
 from google.protobuf.timestamp_pb2 import Timestamp
 from nucliadb_protos.audit_pb2 import (
     AuditField,
     AuditKBCounter,
     AuditRequest,
-    AuditShardCounter,
+    ChatContext,
 )
 from nucliadb_protos.nodereader_pb2 import SearchRequest
 from nucliadb_protos.resources_pb2 import FieldID
 
 
 class AuditStorage:
     async def report(
@@ -38,15 +38,14 @@
         audit_type: AuditRequest.AuditType.Value,  # type: ignore
         when: Optional[Timestamp] = None,
         user: Optional[str] = None,
         origin: Optional[str] = None,
         rid: Optional[str] = None,
         field_metadata: Optional[List[FieldID]] = None,
         audit_fields: Optional[List[AuditField]] = None,
-        counter: Optional[AuditShardCounter] = None,
         kb_counter: Optional[AuditKBCounter] = None,
     ):  # type: ignore
         raise NotImplementedError
 
     async def initialize(self):
         pass
 
@@ -74,9 +73,24 @@
         user: str,
         client: int,
         origin: str,
         timeit: float,
     ):
         raise NotImplementedError
 
+    async def chat(
+        self,
+        kbid: str,
+        user: str,
+        client: int,
+        origin: str,
+        timeit: float,
+        question: str,
+        rephrased_question: Optional[str],
+        context: List[ChatContext],
+        answer: Optional[str],
+        learning_id: str,
+    ):
+        raise NotImplementedError
+
     async def delete_kb(self, kbid):
         raise NotImplementedError
```

## nucliadb_utils/audit/basic.py

```diff
@@ -20,15 +20,15 @@
 from typing import List, Optional
 
 from google.protobuf.timestamp_pb2 import Timestamp
 from nucliadb_protos.audit_pb2 import (
     AuditField,
     AuditKBCounter,
     AuditRequest,
-    AuditShardCounter,
+    ChatContext,
 )
 from nucliadb_protos.nodereader_pb2 import SearchRequest
 from nucliadb_protos.resources_pb2 import FieldID
 from nucliadb_protos.writer_pb2 import BrokerMessage
 
 from nucliadb_utils import logger
 from nucliadb_utils.audit.audit import AuditStorage
@@ -45,19 +45,18 @@
         audit_type: AuditRequest.AuditType.Value,  # type: ignore
         when: Optional[Timestamp] = None,
         user: Optional[str] = None,
         origin: Optional[str] = None,
         rid: Optional[str] = None,
         field_metadata: Optional[List[FieldID]] = None,
         audit_fields: Optional[List[AuditField]] = None,
-        counter: Optional[AuditShardCounter] = None,
         kb_counter: Optional[AuditKBCounter] = None,
     ):  # type: ignore
         logger.debug(
-            f"AUDIT {audit_type} {kbid} {user} {origin} {rid} {audit_fields} {counter}"
+            f"AUDIT {audit_type} {kbid} {user} {origin} {rid} {audit_fields} {kb_counter}"
         )
 
     async def visited(self, kbid: str, uuid: str, user: str, origin: str):
         logger.debug(f"VISITED {kbid} {uuid} {user} {origin}")
 
     async def search(
         self,
@@ -75,11 +74,26 @@
         self,
         kbid: str,
         user: str,
         client: int,
         origin: str,
         timeit: float,
     ):
-        logger.debug(f"SEARCH {kbid} {user} {origin} {timeit}")
+        logger.debug(f"SUGGEST {kbid} {user} {origin} {timeit}")
+
+    async def chat(
+        self,
+        kbid: str,
+        user: str,
+        client_type: int,
+        origin: str,
+        timeit: float,
+        question: str,
+        rephrased_question: Optional[str],
+        context: List[ChatContext],
+        answer: Optional[str],
+        learning_id: str,
+    ):
+        logger.debug(f"CHAT {kbid} {user} {origin} {timeit}")
 
     async def delete_kb(self, kbid):
         logger.debug(f"KB DELETED {kbid}")
```

## nucliadb_utils/audit/stream.py

```diff
@@ -17,26 +17,27 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
 from datetime import datetime
 from typing import List, Optional
 
+import backoff
 import mmh3  # type: ignore
 import nats
 from google.protobuf.timestamp_pb2 import Timestamp
 from nucliadb_protos.audit_pb2 import (
     AuditField,
     AuditKBCounter,
     AuditRequest,
-    AuditShardCounter,
+    ChatContext,
 )
 from nucliadb_protos.nodereader_pb2 import SearchRequest
 from nucliadb_protos.resources_pb2 import FieldID
-from opentelemetry.trace import get_current_span
+from opentelemetry.trace import format_trace_id, get_current_span
 
 from nucliadb_utils import logger
 from nucliadb_utils.audit.audit import AuditStorage
 from nucliadb_utils.nats import get_traced_jetstream
 
 
 class StreamAuditStorage(AuditStorage):
@@ -84,15 +85,15 @@
     async def initialize(self):
         options = {
             "error_cb": self.error_cb,
             "closed_cb": self.closed_cb,
             "reconnected_cb": self.reconnected_cb,
         }
 
-        if self.nats_creds is not None:
+        if self.nats_creds:
             options["user_credentials"] = self.nats_creds
 
         if len(self.nats_servers) > 0:
             options["servers"] = self.nats_servers
 
         self.nc = await nats.connect(**options)
 
@@ -106,25 +107,33 @@
         if self.nc:
             await self.nc.flush()
             await self.nc.close()
             self.nc = None
 
     async def run(self):
         while True:
+            item_dequeued = False
             try:
                 audit = await self.queue.get()
+                item_dequeued = True
                 await self._send(audit)
             except (asyncio.CancelledError, KeyboardInterrupt, RuntimeError):
                 return
             except Exception:  # pragma: no cover
                 logger.exception("Could not send audit", stack_info=True)
+            finally:
+                if item_dequeued:
+                    self.queue.task_done()
 
     async def send(self, message: AuditRequest):
         self.queue.put_nowait(message)
 
+    @backoff.on_exception(
+        backoff.expo, (Exception,), jitter=backoff.random_jitter, max_tries=4
+    )
     async def _send(self, message: AuditRequest):
         if self.js is None:  # pragma: no cover
             raise AttributeError()
 
         partition = self.get_partition(message.kbid)
 
         res = await self.js.publish(
@@ -143,64 +152,60 @@
         audit_type: AuditRequest.AuditType.Value,  # type: ignore
         when: Optional[Timestamp] = None,
         user: Optional[str] = None,
         origin: Optional[str] = None,
         rid: Optional[str] = None,
         field_metadata: Optional[List[FieldID]] = None,
         audit_fields: Optional[List[AuditField]] = None,
-        counter: Optional[AuditShardCounter] = None,
         kb_counter: Optional[AuditKBCounter] = None,
     ):
         # Reports MODIFIED / DELETED / NEW events
         auditrequest = AuditRequest()
         auditrequest.kbid = kbid
         auditrequest.userid = user or ""
         auditrequest.rid = rid or ""
         auditrequest.origin = origin or ""
         auditrequest.type = audit_type
-        if when is None:
+        if when is None or when.SerializeToString() == b"":
             auditrequest.time.FromDatetime(datetime.now())
         else:
             auditrequest.time.CopyFrom(when)
 
         auditrequest.field_metadata.extend(field_metadata or [])
 
         if audit_fields:
             auditrequest.fields_audit.extend(audit_fields)
 
         if kb_counter:
             auditrequest.kb_counter.CopyFrom(kb_counter)
 
-        if counter:
-            auditrequest.counter.CopyFrom(counter)
-
-        auditrequest.trace_id = str(get_current_span().get_span_context().trace_id)
+        auditrequest.trace_id = get_trace_id()
 
         await self.send(auditrequest)
 
     async def visited(self, kbid: str, uuid: str, user: str, origin: str):
         auditrequest = AuditRequest()
         auditrequest.origin = origin
         auditrequest.userid = user
         auditrequest.rid = uuid
         auditrequest.kbid = kbid
         auditrequest.type = AuditRequest.VISITED
         auditrequest.time.FromDatetime(datetime.now())
 
-        auditrequest.trace_id = str(get_current_span().get_span_context().trace_id)
+        auditrequest.trace_id = get_trace_id()
 
         await self.send(auditrequest)
 
     async def delete_kb(self, kbid):
         # Search is a base64 encoded search
         auditrequest = AuditRequest()
         auditrequest.kbid = kbid
         auditrequest.type = AuditRequest.KB_DELETED
         auditrequest.time.FromDatetime(datetime.now())
-        auditrequest.trace_id = str(get_current_span().get_span_context().trace_id)
+        auditrequest.trace_id = get_trace_id()
         await self.send(auditrequest)
 
     async def search(
         self,
         kbid: str,
         user: str,
         client_type: int,
@@ -217,15 +222,15 @@
         auditrequest.kbid = kbid
         auditrequest.search.CopyFrom(search)
         auditrequest.timeit = timeit
         auditrequest.resources = resources
         auditrequest.type = AuditRequest.SEARCH
         auditrequest.time.FromDatetime(datetime.now())
 
-        auditrequest.trace_id = str(get_current_span().get_span_context().trace_id)
+        auditrequest.trace_id = get_trace_id()
         await self.send(auditrequest)
 
     async def suggest(
         self,
         kbid: str,
         user: str,
         client_type: int,
@@ -236,10 +241,48 @@
         auditrequest.origin = origin
         auditrequest.client_type = client_type  # type: ignore
         auditrequest.userid = user
         auditrequest.kbid = kbid
         auditrequest.timeit = timeit
         auditrequest.type = AuditRequest.SUGGEST
         auditrequest.time.FromDatetime(datetime.now())
-        auditrequest.trace_id = str(get_current_span().get_span_context().trace_id)
+        auditrequest.trace_id = get_trace_id()
 
         await self.send(auditrequest)
+
+    async def chat(
+        self,
+        kbid: str,
+        user: str,
+        client_type: int,
+        origin: str,
+        timeit: float,
+        question: str,
+        rephrased_question: Optional[str],
+        context: List[ChatContext],
+        answer: Optional[str],
+        learning_id: str,
+    ):
+        auditrequest = AuditRequest()
+        auditrequest.origin = origin
+        auditrequest.client_type = client_type  # type: ignore
+        auditrequest.userid = user
+        auditrequest.kbid = kbid
+        auditrequest.timeit = timeit
+        auditrequest.type = AuditRequest.CHAT
+        auditrequest.time.FromDatetime(datetime.now())
+        auditrequest.trace_id = get_trace_id()
+        auditrequest.chat.question = question
+        auditrequest.chat.context.extend(context)
+        auditrequest.chat.learning_id = learning_id
+        if rephrased_question is not None:
+            auditrequest.chat.rephrased_question = rephrased_question
+        if answer is not None:
+            auditrequest.chat.answer = answer
+        await self.send(auditrequest)
+
+
+def get_trace_id() -> str:
+    span = get_current_span()
+    if span is None:
+        return ""
+    return format_trace_id(span.get_span_context().trace_id)
```

## nucliadb_utils/cache/__init__.py

```diff
@@ -12,10 +12,7 @@
 # This program is distributed in the hope that it will be useful,
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
-
-CACHE_PREFIX = "gcache2-"
-KB_COUNTER_CACHE = "kb_{kbid}_counters"
```

## nucliadb_utils/cache/settings.py

```diff
@@ -19,17 +19,12 @@
 
 from typing import List, Optional
 
 from pydantic import BaseSettings
 
 
 class Settings(BaseSettings):
-    cache_memory_size: int = 209715200
-    cache_pubsub_channel: str = "nucliadb.cache.invalidations"
-    cache_pubsub_redis_url: Optional[str] = None
-    cache_pubsub_nats_url: List[str] = ["localhost:4222"]
+    cache_pubsub_nats_url: List[str] = []
     cache_pubsub_nats_auth: Optional[str] = None
-    cache_pubsub_driver: str = "nats"  # redis | nats
-    cache_enabled: bool = True
 
 
 settings = Settings()
```

## nucliadb_utils/fastapi/run.py

 * *Ordering differences only*

```diff
@@ -19,18 +19,18 @@
 
 import asyncio
 import os
 import sys
 
 import click
 from fastapi import FastAPI
-from nucliadb_telemetry.fastapi import application_metrics
 from uvicorn.config import Config  # type: ignore
 from uvicorn.server import Server  # type: ignore
 
+from nucliadb_telemetry.fastapi import application_metrics
 from nucliadb_utils import logger
 from nucliadb_utils.settings import running_settings
 
 STARTUP_FAILURE = 3
 
 
 def metrics_app() -> tuple[Server, Config]:
```

## nucliadb_utils/fastapi/versioning.py

```diff
@@ -82,16 +82,15 @@
                 unique_routes[route.path + "|" + method] = route
         for route in unique_routes.values():
             versioned_app.router.routes.append(route)
         parent_app.mount(prefix, versioned_app)
 
         @parent_app.get(f"{prefix}/openapi.json", name=semver, tags=["Versions"])
         @parent_app.get(f"{prefix}/docs", name=semver, tags=["Documentations"])
-        def noop() -> None:
-            ...
+        def noop() -> None: ...
 
     if enable_latest:
         prefix = "/latest"
         major, minor = version
         semver = version_format.format(major=major, minor=minor)
         versioned_app = FastAPI(
             title=app.title,
```

## nucliadb_utils/storages/__init__.py

```diff
@@ -13,8 +13,9 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-CHUNK_SIZE = 524288
+MB = 1024 * 1024
+CHUNK_SIZE = 5 * MB
```

## nucliadb_utils/storages/exceptions.py

```diff
@@ -66,7 +66,14 @@
         )
 
 
 class IndexDataNotFound(Exception):
     """
     Raised when the index data is not found in storage
     """
+
+
+class UnparsableResponse(Exception):
+    """
+    Raised when trying to parse a response from a storage API and it's not
+    possible
+    """
```

## nucliadb_utils/storages/gcs.py

```diff
@@ -18,29 +18,31 @@
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
 import asyncio
 import base64
 import json
+import socket
 from concurrent.futures import ThreadPoolExecutor
 from copy import deepcopy
 from datetime import datetime
 from typing import Any, AsyncIterator, Dict, List, Optional
 from urllib.parse import quote_plus
 
 import aiohttp
+import aiohttp.client_exceptions
 import backoff  # type: ignore
 import google.auth.transport.requests  # type: ignore
 import yarl
 from google.oauth2 import service_account  # type: ignore
 from nucliadb_protos.resources_pb2 import CloudFile
-from nucliadb_telemetry import metrics
-from nucliadb_telemetry.utils import setup_telemetry
 
+from nucliadb_telemetry import errors, metrics
+from nucliadb_telemetry.utils import setup_telemetry
 from nucliadb_utils import logger
 from nucliadb_utils.storages import CHUNK_SIZE
 from nucliadb_utils.storages.exceptions import (
     CouldNotCopyNotFound,
     CouldNotCreateBucket,
     InvalidOffset,
     ResumableUploadGone,
@@ -50,18 +52,23 @@
 storage_ops_observer = metrics.Observer("gcs_ops", labels={"type": ""})
 
 
 def strip_query_params(url: yarl.URL) -> str:
     return str(url.with_query(None))
 
 
-MAX_SIZE = 1073741824
+KB = 1024
+MB = 1024 * KB
+
+MIN_UPLOAD_SIZE = 256 * KB
+OBJECT_DATA_CHUNK_SIZE = 1 * MB
+
 
 DEFAULT_SCOPES = ["https://www.googleapis.com/auth/devstorage.read_write"]
-MAX_RETRIES = 5
+MAX_TRIES = 4
 
 POLICY_DELETE = {
     "lifecycle": {
         "rule": [
             {
                 "action": {"type": "Delete"},
                 "condition": {
@@ -73,21 +80,28 @@
 }
 
 
 class GoogleCloudException(Exception):
     pass
 
 
+class ReadingResponseContentException(GoogleCloudException):
+    pass
+
+
 RETRIABLE_EXCEPTIONS = (
     GoogleCloudException,
     aiohttp.client_exceptions.ClientPayloadError,
     aiohttp.client_exceptions.ClientConnectorError,
+    aiohttp.client_exceptions.ClientConnectionError,
     aiohttp.client_exceptions.ClientOSError,
     aiohttp.client_exceptions.ServerConnectionError,
+    aiohttp.client_exceptions.ServerDisconnectedError,
     CouldNotCreateBucket,
+    socket.gaierror,
 )
 
 
 class GCSStorageField(StorageField):
     storage: GCSStorage
 
     async def move(
@@ -98,15 +112,20 @@
         destination_bucket_name: str,
     ):
         await self.copy(
             origin_uri, destination_uri, origin_bucket_name, destination_bucket_name
         )
         await self.storage.delete_upload(origin_uri, origin_bucket_name)
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=4)
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     @storage_ops_observer.wrap({"type": "copy"})
     async def copy(
         self,
         origin_uri: str,
         destination_uri: str,
         origin_bucket_name: str,
         destination_bucket_name: str,
@@ -133,17 +152,39 @@
                     destination_bucket_name=destination_bucket_name,
                     text=text,
                 )
             else:
                 data = await resp.json()
                 assert data["resource"]["name"] == destination_uri
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=4)
     @storage_ops_observer.wrap({"type": "iter_data"})
     async def iter_data(self, headers=None):
+        attempt = 1
+        while True:
+            try:
+                async for chunk in self._inner_iter_data(headers=headers):
+                    yield chunk
+                break
+            except ReadingResponseContentException:
+                # Do not retry any exception that may happen in the middle of
+                # reading the response chunks, as that could lead to duplicated
+                # chunks and data corruption.
+                raise
+            except RETRIABLE_EXCEPTIONS as ex:
+                if attempt >= MAX_TRIES:
+                    raise
+                wait_time = 2 ** (attempt - 1)
+                logger.warning(
+                    f"Error downloading from GCP. Retrying ({attempt} of {MAX_TRIES}) after {wait_time} seconds. Error: {ex}"  # noqa
+                )
+                await asyncio.sleep(wait_time)
+                attempt += 1
+
+    @storage_ops_observer.wrap({"type": "inner_iter_data"})
+    async def _inner_iter_data(self, headers=None):
         if headers is None:
             headers = {}
 
         key = self.field.uri if self.field else self.key
         if self.field is None:
             bucket = self.bucket
         else:
@@ -166,34 +207,39 @@
                 elif api_resp.status == 401:
                     logger.warning(f"Invalid google cloud credentials error: {text}")
                     raise KeyError(
                         content={f"Google cloud invalid credentials : \n {text}"}
                     )
                 raise GoogleCloudException(f"{api_resp.status}: {text}")
             while True:
-                chunk = await api_resp.content.read(1024 * 1024)
+                try:
+                    chunk = await api_resp.content.read(OBJECT_DATA_CHUNK_SIZE)
+                except Exception as ex:
+                    raise ReadingResponseContentException() from ex
                 if len(chunk) > 0:
                     yield chunk
                 else:
                     break
 
-    async def range_supported(self) -> bool:
-        return True
-
     @storage_ops_observer.wrap({"type": "read_range"})
     async def read_range(self, start: int, end: int) -> AsyncIterator[bytes]:
         """
         Iterate through ranges of data
         """
         async for chunk in self.iter_data(
             headers={"Range": f"bytes={start}-{end - 1}"}
         ):
             yield chunk
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=4)
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     @storage_ops_observer.wrap({"type": "start_upload"})
     async def start(self, cf: CloudFile) -> CloudFile:
         """Init an upload.
 
         cf: New file to upload
         """
         if self.storage.session is None:
@@ -268,15 +314,15 @@
 
         return field
 
     @backoff.on_exception(
         backoff.constant,
         RETRIABLE_EXCEPTIONS,
         interval=1,
-        max_tries=4,
+        max_tries=MAX_TRIES,
         jitter=backoff.random_jitter,
     )
     @storage_ops_observer.wrap({"type": "append_data"})
     async def _append(self, cf: CloudFile, data: bytes):
         if self.field is None:
             raise AttributeError()
 
@@ -354,29 +400,34 @@
 
         self.field.uri = self.key
         self.field.ClearField("resumable_uri")
         self.field.ClearField("offset")
         self.field.ClearField("upload_uri")
         self.field.ClearField("parts")
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=4)
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     @storage_ops_observer.wrap({"type": "exists"})
     async def exists(self) -> Optional[Dict[str, str]]:
         """
         Existence can be checked either with a CloudFile data in the field attribute
         or own StorageField key and bucket. Field takes precendece
         """
         if self.storage.session is None:
             raise AttributeError()
         key = None
         bucket = None
-        if self.field is not None and self.field.uri is not None:
+        if self.field is not None and self.field.uri != "":
             key = self.field.uri
             bucket = self.field.bucket_name
-        elif self.key is not None:
+        elif self.key != "":
             key = self.key
             bucket = self.bucket
         else:
             return None
 
         url = "{}/{}/o/{}".format(
             self.storage.object_base_url,
@@ -465,15 +516,17 @@
         return self._credentials.token
 
     @storage_ops_observer.wrap({"type": "initialize"})
     async def initialize(self, service_name: Optional[str] = None):
         loop = asyncio.get_event_loop()
 
         await setup_telemetry(service_name or "GCS_SERVICE")
-        self.session = aiohttp.ClientSession(loop=loop)
+        self.session = aiohttp.ClientSession(
+            loop=loop, connector=aiohttp.TCPConnector(ttl_dns_cache=60 * 5)
+        )
 
         try:
             if self.deadletter_bucket is not None and self.deadletter_bucket != "":
                 await self.create_bucket(self.deadletter_bucket)
         except Exception:  # pragma: no cover
             logger.exception(
                 f"Could not create bucket {self.deadletter_bucket}", exc_info=True
@@ -493,20 +546,25 @@
     async def get_access_headers(self):
         if self._credentials is None:
             return {}
         loop = asyncio.get_event_loop()
         token = await loop.run_in_executor(self._executor, self._get_access_token)
         return {"AUTHORIZATION": f"Bearer {token}"}
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=4)
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     @storage_ops_observer.wrap({"type": "delete"})
     async def delete_upload(self, uri: str, bucket_name: str):
         if self.session is None:
             raise AttributeError()
-        if uri is not None:
+        if uri:
             url = "{}/{}/o/{}".format(
                 self.object_base_url, bucket_name, quote_plus(uri)
             )
             headers = await self.get_access_headers()
             async with self.session.delete(url, headers=headers) as resp:
                 try:
                     data = await resp.json()
@@ -637,22 +695,31 @@
             raise AttributeError()
         bucket_name = self.get_bucket_name(kbid)
         headers = await self.get_access_headers()
         url = f"{self.object_base_url}/{bucket_name}"
         deleted = False
         conflict = False
         async with self.session.delete(url, headers=headers) as resp:
-            if resp.status == 200:
+            if resp.status == 204:
                 logger.info(f"Deleted bucket: {bucket_name}")
                 deleted = True
-            if resp.status == 409:
-                logger.info(f"Conflict on deleting: {bucket_name}")
+            elif resp.status == 409:
+                details = await resp.text()
+                logger.info(f"Conflict on deleting bucket {bucket_name}: {details}")
                 conflict = True
-            if resp.status == 404:
+            elif resp.status == 404:
                 logger.info(f"Does not exit on deleting: {bucket_name}")
+            else:
+                details = await resp.text()
+                msg = f"Delete KB bucket returned an unexpected status {resp.status}: {details}"
+                logger.error(msg, extra={"kbid": kbid})
+                with errors.push_scope() as scope:
+                    scope.set_extra("kbid", kbid)
+                    scope.set_extra("status_code", resp.status)
+                    errors.capture_message(msg, "error", scope)
         return deleted, conflict
 
     async def iterate_bucket(self, bucket: str, prefix: str) -> AsyncIterator[Any]:
         if self.session is None:
             raise AttributeError()
         url = "{}/{}/o".format(self.object_base_url, bucket)
         headers = await self.get_access_headers()
```

## nucliadb_utils/storages/local.py

```diff
@@ -68,49 +68,45 @@
     ):
         origin_bucket_path = self.storage.get_bucket_path(origin_bucket_name)
         destination_bucket_path = self.storage.get_bucket_path(destination_bucket_name)
         origin_path = f"{origin_bucket_path}/{origin_uri}"
         destination_path = f"{destination_bucket_path}/{destination_uri}"
         shutil.copy(origin_path, destination_path)
 
-    def get_file_path(self, bucket: str, key: str):
-        return f"{self.storage.get_bucket_name(bucket)}/{key}"
-
     async def iter_data(self, headers=None):
         key = self.field.uri if self.field else self.key
         if self.field is None:
             bucket = self.bucket
         else:
             bucket = self.field.bucket_name
 
-        path = self.storage.get_bucket_path(bucket)
-
-        async with aiofiles.open(self.get_file_path(path, key)) as resp:
-            data = await resp.read(CHUNK_SIZE)
-            while data is not None:
-                yield data
+        path = self.storage.get_file_path(bucket, key)
+        async with aiofiles.open(path, mode="rb") as resp:
+            while True:
                 data = await resp.read(CHUNK_SIZE)
+                if not data:
+                    break
+                yield data
 
     async def read_range(self, start: int, end: int) -> AsyncIterator[bytes]:
         """
         Iterate through ranges of data
         """
         key = self.field.uri if self.field else self.key
         if self.field is None:
             bucket = self.bucket
         else:
             bucket = self.field.bucket_name
 
-        path = self.storage.get_bucket_path(bucket)
-
-        async with aiofiles.open(self.get_file_path(path, key), "rb") as resp:
+        path = self.storage.get_file_path(bucket, key)
+        async with aiofiles.open(path, "rb") as resp:
             await resp.seek(start)
             count = 0
             data = await resp.read(CHUNK_SIZE)
-            while data is not None and count < end:
+            while data and count < end:
                 if count + len(data) > end:
                     new_end = end - count
                     data = data[:new_end]
                 yield data
                 count += len(data)
                 data = await resp.read(CHUNK_SIZE)
 
@@ -140,16 +136,15 @@
                 md5=cf.md5,
                 content_type=cf.content_type,
                 bucket_name=self.bucket,
                 source=CloudFile.LOCAL,
             )
             upload_uri = self.key
 
-        path = self.storage.get_bucket_path(self.bucket)
-        init_url = f"{path}/{upload_uri}"
+        init_url = self.storage.get_file_path(self.bucket, upload_uri)
         metadata_init_url = self.metadata_key(init_url)
         metadata = json.dumps(
             {"FILENAME": cf.filename, "SIZE": cf.size, "CONTENT_TYPE": cf.content_type}
         )
 
         path_to_create = os.path.dirname(metadata_init_url)
         os.makedirs(path_to_create, exist_ok=True)
@@ -192,16 +187,15 @@
 
         await self._handler.close()
         self.field.uri = self.key
         self.field.ClearField("offset")
         self.field.ClearField("upload_uri")
 
     async def exists(self) -> Optional[Dict[str, str]]:
-        bucket_path = self.storage.get_bucket_path(self.bucket)
-        file_path = f"{bucket_path}/{self.key}"
+        file_path = self.storage.get_file_path(self.bucket, self.key)
         metadata_path = self.metadata_key(file_path)
         if os.path.exists(metadata_path):
             async with aiofiles.open(metadata_path, "r") as metadata:
                 return json.loads(await metadata.read())
         return None
 
     async def upload(self, iterator: AsyncIterator, origin: CloudFile) -> CloudFile:
@@ -233,14 +227,17 @@
 
     def get_bucket_name(self, kbid: str):
         return self.bucket_format.format(kbid=kbid)
 
     def get_bucket_path(self, bucket: str):
         return f"{self.local_testing_files}/{bucket}"
 
+    def get_file_path(self, bucket: str, key: str):
+        return f"{self.get_bucket_path(bucket)}/{key}"
+
     async def create_kb(self, kbid: str):
         bucket = self.get_bucket_name(kbid)
         path = self.get_bucket_path(bucket)
         try:
             os.makedirs(path, exist_ok=True)
             created = True
         except FileExistsError:
@@ -254,17 +251,17 @@
             shutil.rmtree(path)
             deleted = True
         except Exception:
             deleted = False
         return deleted
 
     async def delete_upload(self, uri: str, bucket_name: str):
-        path = self.get_bucket_path(bucket_name)
-        file_path = f"{path}/{uri}"
-        os.remove(file_path)
+        file_path = self.get_file_path(bucket_name, uri)
+        if os.path.exists(file_path):
+            os.remove(file_path)
 
     async def schedule_delete_kb(self, kbid: str):
         bucket = self.get_bucket_name(kbid)
         path = self.get_bucket_path(bucket)
         try:
             shutil.rmtree(path)
             deleted = True
@@ -276,16 +273,15 @@
         for key in glob.glob(f"{bucket}/{prefix}*"):
             item = {"name": key}
             yield item
 
     async def download(
         self, bucket_name: str, key: str, headers: Optional[Dict[str, str]] = None
     ):
-        path = self.get_bucket_path(bucket_name)
-        key_path = f"{path}/{key}"
+        key_path = self.get_file_path(bucket_name, key)
         if not os.path.exists(key_path):
             return
 
         async with aiofiles.open(key_path, mode="rb") as f:
             while True:
                 body = await f.read(self.chunk_size)
                 if body == b"" or body is None:
```

## nucliadb_utils/storages/nuclia.py

```diff
@@ -46,15 +46,15 @@
         url = f"{self.nuclia_public_url}/api/v1/processing/download?token={cf.uri}"
         async with self.session.get(
             url,
             headers={
                 "X-STF-NUAKEY": f"Bearer {self.service_account}",
             },
         ) as resp:
-            assert resp.status == 200
+            resp.raise_for_status()
             async for chunk in resp.content.iter_chunked(CHUNK_SIZE):
                 yield chunk
 
     async def initialize(self):
         self.session = aiohttp.ClientSession()
 
     async def finalize(self):
```

## nucliadb_utils/storages/pg.py

```diff
@@ -16,23 +16,26 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
 import asyncio
+import logging
 import uuid
 from typing import Any, AsyncIterator, Optional, TypedDict
 
 import asyncpg
 from nucliadb_protos.resources_pb2 import CloudFile
 
 from nucliadb_utils.storages import CHUNK_SIZE
 from nucliadb_utils.storages.storage import Storage, StorageField
 
+logger = logging.getLogger(__name__)
+
 # Table design notes
 # - No foreign key constraints ON PURPOSE
 # - No cascade handling ON PURPOSE
 CREATE_TABLE = """
 CREATE TABLE IF NOT EXISTS kb_files (
     kb_id TEXT,
     file_id TEXT,
@@ -108,17 +111,17 @@
             await self.connection.execute(
                 """
 INSERT INTO kb_files (kb_id, file_id, filename, size, content_type)
 VALUES ($1, $2, $3, $4, $5)
 """,
                 kb_id,
                 file_id,
-                filename,
+                filename or "",
                 size,
-                content_type,
+                content_type or "",
             )
 
     async def delete_file(self, kb_id: str, file_id: str) -> None:
         async with self.connection.transaction():
             await self.connection.execute(
                 """
 DELETE FROM kb_files
@@ -137,15 +140,21 @@
             )
 
     async def append_chunk(self, *, kb_id: str, file_id: str, data: bytes) -> None:
         async with self.connection.transaction():
             await self.connection.execute(
                 """
 INSERT INTO kb_files_fileparts (kb_id, file_id, part_id, data, size)
-VALUES ($1, $2, (SELECT COALESCE(MAX(part_id), 0) + 1 FROM kb_files_fileparts WHERE kb_id = $1 AND file_id = $2), $3, $4)
+VALUES (
+    $1, $2,
+    (
+        SELECT COALESCE(MAX(part_id), 0) + 1
+        FROM kb_files_fileparts WHERE kb_id = $1 AND file_id = $2
+    ),
+    $3, $4)
 """,
                 kb_id,
                 file_id,
                 data,
                 len(data),
             )
 
@@ -173,26 +182,45 @@
         *,
         origin_key: str,
         destination_key: str,
         origin_kb: str,
         destination_kb: str,
     ):
         async with self.connection.transaction():
+            # make sure to delete the destination first in
+            # case this is an overwrite of an existing
+            await self.connection.execute(
+                """
+delete from kb_files
+WHERE kb_id = $1 AND file_id = $2
+""",
+                destination_kb,
+                destination_key,
+            )
             await self.connection.execute(
                 """
 UPDATE kb_files
 SET kb_id = $1, file_id = $2
 WHERE kb_id = $3 AND file_id = $4
 """,
                 destination_kb,
                 destination_key,
                 origin_kb,
                 origin_key,
             )
-
+            # make sure to delete the destination first in
+            # case this is an overwrite of an existing
+            await self.connection.execute(
+                """
+delete from kb_files_fileparts
+WHERE kb_id = $1 AND file_id = $2
+""",
+                destination_kb,
+                destination_key,
+            )
             await self.connection.execute(
                 """
 UPDATE kb_files_fileparts
 SET kb_id = $1, file_id = $2
 WHERE kb_id = $3 AND file_id = $4
 """,
                 destination_kb,
@@ -451,35 +479,43 @@
         if self.field is None:
             raise AttributeError()
         count = 0
         async with self.storage.pool.acquire() as conn:
             dl = PostgresFileDataLayer(conn)
             async for chunk in iterable:
                 await dl.append_chunk(
-                    kb_id=self.bucket, file_id=cf.upload_uri, data=chunk
+                    kb_id=self.bucket,
+                    file_id=cf.upload_uri or self.field.upload_uri,
+                    data=chunk,
                 )
                 size = len(chunk)
                 count += size
                 self.field.offset += len(chunk)
         return count
 
     async def finish(self):
         async with self.storage.pool.acquire() as conn, conn.transaction():
             dl = PostgresFileDataLayer(conn)
             if self.field.old_uri not in ("", None):
                 # Already has a file
-                await dl.delete_file(self.bucket, self.field.bucket_name)
+                await dl.delete_file(self.bucket, self.field.uri)
 
             if self.field.upload_uri != self.key:
-                await dl.move(
-                    origin_key=self.field.upload_uri,
-                    destination_key=self.key,
-                    origin_kb=self.field.bucket_name,
-                    destination_kb=self.bucket,
-                )
+                try:
+                    await dl.move(
+                        origin_key=self.field.upload_uri,
+                        destination_key=self.key,
+                        origin_kb=self.field.bucket_name,
+                        destination_kb=self.bucket,
+                    )
+                except Exception:
+                    logger.exception(
+                        f"Error moving file {self.field.bucket_name}://{self.field.upload_uri} -> {self.bucket}://{self.key}"  # noqa
+                    )
+                    raise
 
         self.field.uri = self.key
         self.field.ClearField("offset")
         self.field.ClearField("upload_uri")
 
     async def exists(self) -> Optional[FileInfo]:  # type:ignore
         async with self.storage.pool.acquire() as conn:
@@ -497,28 +533,35 @@
 
 
 class PostgresStorage(Storage):
     field_klass = PostgresStorageField
     chunk_size = CHUNK_SIZE
     pool: asyncpg.pool.Pool
 
-    def __init__(self, dsn: str):
+    def __init__(self, dsn: str, connection_pool_max_size: int = 10):
         self.dsn = dsn
+        self.connection_pool_max_size = connection_pool_max_size
         self.source = CloudFile.POSTGRES
         self._lock = asyncio.Lock()
         self.initialized = False
 
     async def initialize(self):
         async with self._lock:
             if self.initialized is False:
-                self.pool = await asyncpg.create_pool(self.dsn)
+                self.pool = await asyncpg.create_pool(
+                    self.dsn,
+                    max_size=self.connection_pool_max_size,
+                )
 
                 # check if table exists
-                async with self.pool.acquire() as conn:
-                    await conn.execute(CREATE_TABLE)
+                try:
+                    async with self.pool.acquire() as conn:
+                        await conn.execute(CREATE_TABLE)
+                except asyncpg.exceptions.UniqueViolationError:  # pragma: no cover
+                    pass
 
             self.initialized = True
 
     async def finalize(self):
         async with self._lock:
             await self.pool.close()
             self.initialized = False
```

## nucliadb_utils/storages/s3.py

```diff
@@ -26,53 +26,74 @@
 import aiobotocore  # type: ignore
 import aiohttp
 import backoff  # type: ignore
 import botocore  # type: ignore
 from aiobotocore.session import AioSession, get_session  # type: ignore
 from nucliadb_protos.resources_pb2 import CloudFile
 
+from nucliadb_telemetry import errors
 from nucliadb_utils import logger
+from nucliadb_utils.storages.exceptions import UnparsableResponse
 from nucliadb_utils.storages.storage import Storage, StorageField
 
-MAX_SIZE = 1073741824
-
-MIN_UPLOAD_SIZE = 5 * 1024 * 1024
+MB = 1024 * 1024
+MIN_UPLOAD_SIZE = 5 * MB
 CHUNK_SIZE = MIN_UPLOAD_SIZE
-MAX_RETRIES = 5
+MAX_TRIES = 3
 
 RETRIABLE_EXCEPTIONS = (
     botocore.exceptions.ClientError,
     aiohttp.client_exceptions.ClientPayloadError,
     botocore.exceptions.BotoCoreError,
 )
 
 POLICY_DELETE = {
     "Rules": [
         {
-            "Expiration": {
-                "Days": 0,
-            },
+            "Expiration": {"Days": 1},
+            "ID": "FullDelete",
+            "Filter": {"Prefix": ""},
             "Status": "Enabled",
-        }
+            "NoncurrentVersionExpiration": {"NoncurrentDays": 1},
+            "AbortIncompleteMultipartUpload": {"DaysAfterInitiation": 1},
+        },
+        {
+            "Expiration": {"ExpiredObjectDeleteMarker": True},
+            "ID": "DeleteMarkers",
+            "Filter": {"Prefix": ""},
+            "Status": "Enabled",
+        },
     ]
 }
 
 
 class S3StorageField(StorageField):
     storage: S3Storage
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=3)
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     async def _download(self, uri, bucket, **kwargs):
         if "headers" in kwargs:
             for key, value in kwargs["headers"].items():
                 kwargs[key] = value
             del kwargs["headers"]
-        return await self.storage._s3aioclient.get_object(
-            Bucket=bucket, Key=uri, **kwargs
-        )
+        try:
+            return await self.storage._s3aioclient.get_object(
+                Bucket=bucket, Key=uri, **kwargs
+            )
+        except botocore.exceptions.ClientError as e:
+            error_code = parse_status_code(e)
+            if error_code == 404:
+                raise KeyError(f"S3 cloud file not found : {uri}")
+            else:
+                raise
 
     async def iter_data(self, **kwargs):
         # Suports field and key based iter
         uri = self.field.uri if self.field else self.key
         if self.field is None:
             bucket = self.bucket
         else:
@@ -105,48 +126,54 @@
             await self.storage._s3aioclient.abort_multipart_upload(
                 Bucket=bucket_name, Key=upload_file_id, UploadId=mpu["UploadId"]
             )
         except Exception:
             logger.warning("Could not abort multipart upload", exc_info=True)
 
     async def start(self, cf: CloudFile) -> CloudFile:
-        if self.field is not None and self.field.upload_uri is not None:
+        if self.field is not None and self.field.upload_uri != "":
             # Field has already a file beeing uploaded, cancel
             await self._abort_multipart()
-        elif self.field is not None and self.field.uri is not None:
+
+        if self.field is not None and self.field.uri != "":
             # If exist the file copy the old url to delete
             field = CloudFile(
                 filename=cf.filename,
                 size=cf.size,
                 content_type=cf.content_type,
                 bucket_name=self.bucket,
                 md5=cf.md5,
-                source=CloudFile.GCS,
+                source=CloudFile.S3,
                 old_uri=self.field.uri,
                 old_bucket=self.field.bucket_name,
             )
             upload_uri = f"{self.key}-{datetime.now().isoformat()}"
         else:
             field = CloudFile(
                 filename=cf.filename,
                 size=cf.size,
                 md5=cf.md5,
                 content_type=cf.content_type,
                 bucket_name=self.bucket,
-                source=CloudFile.GCS,
+                source=CloudFile.S3,
             )
             upload_uri = self.key
 
         field.offset = 1
         response = await self._create_multipart(self.bucket, upload_uri, field)
         field.resumable_uri = response["UploadId"]
         field.upload_uri = upload_uri
         return field
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=3)
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     async def _create_multipart(self, bucket_name: str, upload_id: str, cf: CloudFile):
         return await self.storage._s3aioclient.create_multipart_upload(
             Bucket=bucket_name,
             Key=upload_id,
             Metadata={
                 "FILENAME": cf.filename,
                 "SIZE": str(cf.size),
@@ -154,22 +181,37 @@
             },
         )
 
     async def append(self, cf: CloudFile, iterable: AsyncIterator) -> int:
         size = 0
         if self.field is None:
             raise AttributeError("No field configured")
+
+        upload_chunk = b""  # s3 strict about chunk size
         async for chunk in iterable:
             size += len(chunk)
-            part = await self._upload_part(cf, chunk)
+            upload_chunk += chunk
+            if len(upload_chunk) >= MIN_UPLOAD_SIZE:
+                part = await self._upload_part(cf, upload_chunk)
+                self.field.parts.append(part["ETag"])
+                self.field.offset += 1
+                upload_chunk = b""
+        if len(upload_chunk) > 0:
+            part = await self._upload_part(cf, upload_chunk)
             self.field.parts.append(part["ETag"])
             self.field.offset += 1
+
         return size
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=3)
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     async def _upload_part(self, cf: CloudFile, data: bytes):
         if self.field is None:
             raise AttributeError("No field configured")
         return await self.storage._s3aioclient.upload_part(
             Bucket=self.field.bucket_name,
             Key=self.field.upload_uri,
             PartNumber=self.field.offset,
@@ -183,31 +225,36 @@
         if self.field.old_uri not in ("", None):
             # delete existing file
             try:
                 await self.storage.delete_upload(
                     uri=self.field.old_uri, bucket=self.field.old_bucket
                 )
                 self.field.ClearField("old_uri")
-                self.field.CkearField("old_bucket")
+                self.field.ClearField("old_bucket")
             except botocore.exceptions.ClientError:
                 logger.error(
                     f"Referenced key {self.field.uri} could not be found", exc_info=True
                 )
                 logger.warning("Error deleting object", exc_info=True)
 
-        if self.field.resumable_uri is not None:
+        if self.field.resumable_uri != "":
             await self._complete_multipart_upload()
 
         self.field.uri = self.key
         self.field.ClearField("resumable_uri")
         self.field.ClearField("offset")
         self.field.ClearField("upload_uri")
         self.field.ClearField("parts")
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=3)
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     async def _complete_multipart_upload(self):
         # if blocks is 0, it means the file is of zero length so we need to
         # trick it to finish a multiple part with no data.
         if self.field.offset == 1:
             part = await self._upload_part(None, b"")
             self.field.parts.append(part["ETag"])
             self.field.offset += 1
@@ -228,18 +275,18 @@
         """
         Existence can be checked either with a CloudFile data in the field attribute
         or own StorageField key and bucket. Field takes precendece
         """
 
         key = None
         bucket = None
-        if self.field is not None and self.field.uri is not None:
+        if self.field is not None and self.field.uri != "":
             key = self.field.uri
             bucket = self.field.bucket_name
-        elif self.key is not None:
+        elif self.key != "":
             key = self.key
             bucket = self.bucket
         else:
             return None
 
         try:
             obj = await self.storage._s3aioclient.head_object(Bucket=bucket, Key=key)
@@ -249,16 +296,17 @@
                     "SIZE": metadata.get("size") or obj.get("ContentLength"),
                     "CONTENT_TYPE": metadata.get("content_type")
                     or obj.get("ContentType"),
                     "FILENAME": metadata.get("filename") or key.split("/")[-1],
                 }
             else:
                 return None
-        except botocore.exceptions.ClientError as ex:
-            if ex.response["Error"]["Code"] == "NoSuchKey":
+        except botocore.exceptions.ClientError as e:
+            error_code = parse_status_code(e)
+            if error_code == 404:
                 return None
             raise
 
     async def copy(
         self,
         origin_uri: str,
         destination_uri: str,
@@ -294,20 +342,24 @@
         indexing_bucket: Optional[str] = None,
         endpoint_url: Optional[str] = None,
         verify_ssl: bool = True,
         use_ssl: bool = True,
         region_name: Optional[str] = None,
         max_pool_connections: int = 30,
         bucket: Optional[str] = None,
+        bucket_tags: Optional[dict[str, str]] = None,
     ):
         self.source = CloudFile.S3
         self.deadletter_bucket = deadletter_bucket
         self.indexing_bucket = indexing_bucket
         self._aws_access_key = aws_client_id
         self._aws_secret_key = aws_client_secret
+        self._region_name = region_name
+
+        self._bucket_tags = bucket_tags
 
         self.opts = dict(
             aws_secret_access_key=self._aws_secret_key,
             aws_access_key_id=self._aws_access_key,
             endpoint_url=endpoint_url,
             verify=verify_ssl,
             use_ssl=use_ssl,
@@ -331,20 +383,31 @@
         return self._session
 
     async def initialize(self):
         session = AioSession()
         self._s3aioclient = await self._exit_stack.enter_async_context(
             session.create_client("s3", **self.opts)
         )
+        for bucket in (self.deadletter_bucket, self.indexing_bucket):
+            if bucket is not None:
+                await self._create_bucket_if_not_exists(bucket)
+
+    async def _create_bucket_if_not_exists(self, bucket_name: str) -> bool:
+        created = False
+        bucket_exists = await self.bucket_exists(bucket_name)
+        if not bucket_exists:
+            created = True
+            await self.create_bucket(bucket_name)
+        return created
 
     async def finalize(self):
         await self._exit_stack.__aexit__(None, None, None)
 
     async def delete_upload(self, uri: str, bucket: str):
-        if uri is not None:
+        if uri:
             try:
                 await self._s3aioclient.delete_object(Bucket=bucket, Key=uri)
             except botocore.exceptions.ClientError:
                 logger.warning("Error deleting object", exc_info=True)
         else:
             raise AttributeError("No valid uri")
 
@@ -355,41 +418,35 @@
         async for result in paginator.paginate(Bucket=bucket, Prefix=prefix):
             for item in result.get("Contents", []):
                 item["name"] = item["Key"]
                 yield item
 
     async def create_kb(self, kbid: str):
         bucket_name = self.get_bucket_name(kbid)
-        missing = False
-        created = False
-        try:
-            res = await self._s3aioclient.head_bucket(Bucket=bucket_name)
-            if res["ResponseMetadata"]["HTTPStatusCode"] == 404:
-                missing = True
-        except botocore.exceptions.ClientError as e:
-            error_code = int(e.response["Error"]["Code"])
-            if error_code == 404:
-                missing = True
+        return await self._create_bucket_if_not_exists(bucket_name)
 
-        if missing:
-            await self._s3aioclient.create_bucket(Bucket=bucket_name)
-            created = True
-        return created
+    async def bucket_exists(self, bucket_name: str) -> bool:
+        return await bucket_exists(self._s3aioclient, bucket_name)
+
+    async def create_bucket(self, bucket_name: str):
+        await create_bucket(
+            self._s3aioclient, bucket_name, self._bucket_tags, self._region_name
+        )
 
     async def schedule_delete_kb(self, kbid: str):
         bucket_name = self.get_bucket_name(kbid)
 
         missing = False
         deleted = False
         try:
             res = await self._s3aioclient.head_bucket(Bucket=bucket_name)
             if res["ResponseMetadata"]["HTTPStatusCode"] == 404:
                 missing = True
         except botocore.exceptions.ClientError as e:
-            error_code = int(e.response["Error"]["Code"])
+            error_code = parse_status_code(e)
             if error_code == 404:
                 missing = True
 
         if missing is False:
             await self._s3aioclient.put_bucket_lifecycle_configuration(
                 Bucket=bucket_name, LifecycleConfiguration=POLICY_DELETE
             )
@@ -403,21 +460,84 @@
         deleted = False
         conflict = False
         try:
             res = await self._s3aioclient.head_bucket(Bucket=bucket_name)
             if res["ResponseMetadata"]["HTTPStatusCode"] == 404:
                 missing = True
         except botocore.exceptions.ClientError as e:
-            error_code = int(e.response["Error"]["Code"])
+            error_code = parse_status_code(e)
             if error_code == 404:
                 missing = True
 
         if missing is False:
             try:
                 res = await self._s3aioclient.delete_bucket(Bucket=bucket_name)
             except botocore.exceptions.ClientError as e:
-                error_code = int(e.response["Error"]["Code"])
+                error_code = parse_status_code(e)
                 if error_code == 409:
                     conflict = True
                 if error_code in (200, 204):
                     deleted = True
         return deleted, conflict
+
+
+async def bucket_exists(client: AioSession, bucket_name: str) -> bool:
+    exists = True
+    try:
+        res = await client.head_bucket(Bucket=bucket_name)
+        if res["ResponseMetadata"]["HTTPStatusCode"] == 404:
+            exists = False
+    except botocore.exceptions.ClientError as e:
+        error_code = parse_status_code(e)
+        if error_code == 404:
+            exists = False
+    return exists
+
+
+async def create_bucket(
+    client: AioSession,
+    bucket_name: str,
+    bucket_tags: Optional[dict[str, str]] = None,
+    region_name: Optional[str] = None,
+):
+    bucket_creation_options = {}
+    if region_name is not None:
+        bucket_creation_options = {
+            "CreateBucketConfiguration": {"LocationConstraint": region_name}
+        }
+    # Create the bucket
+    await client.create_bucket(Bucket=bucket_name, **bucket_creation_options)
+
+    if bucket_tags is not None and len(bucket_tags) > 0:
+        # Set bucket tags
+        await client.put_bucket_tagging(
+            Bucket=bucket_name,
+            Tagging={
+                "TagSet": [
+                    {"Key": tag_key, "Value": tag_value}
+                    for tag_key, tag_value in bucket_tags.items()
+                ]
+            },
+        )
+
+
+def parse_status_code(error: botocore.exceptions.ClientError) -> int:
+    error_code = error.response["Error"]["Code"]
+    if error_code.isnumeric():
+        return int(error_code)
+
+    # See https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList
+    error_code_mappings = {
+        "NoSuchBucket": 404,
+        "NoSuchKey": 404,
+        "BucketNotEmpty": 409,
+    }
+
+    if error_code in error_code_mappings:
+        return error_code_mappings[error_code]
+
+    msg = f"Unexpected error status while parsing error response: {error_code}"
+    with errors.push_scope() as scope:
+        scope.set_extra("response", error.response)
+        errors.capture_message(msg, "error", scope)
+
+    raise UnparsableResponse(msg) from error
```

## nucliadb_utils/storages/storage.py

```diff
@@ -17,46 +17,51 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
 import abc
 import hashlib
+import uuid
 from io import BytesIO
 from typing import (
     Any,
     AsyncGenerator,
     AsyncIterator,
     Dict,
     List,
     Optional,
     Tuple,
     Type,
     Union,
+    cast,
 )
 
 from nucliadb_protos.noderesources_pb2 import Resource as BrainResource
-from nucliadb_protos.nodewriter_pb2 import IndexMessage, TypeMessage
+from nucliadb_protos.nodewriter_pb2 import IndexMessage
 from nucliadb_protos.resources_pb2 import CloudFile
 from nucliadb_protos.writer_pb2 import BrokerMessage
 
 from nucliadb_utils import logger
+from nucliadb_utils.helpers import async_gen_lookahead
 from nucliadb_utils.storages import CHUNK_SIZE
 from nucliadb_utils.storages.exceptions import IndexDataNotFound, InvalidCloudFile
 from nucliadb_utils.utilities import get_local_storage, get_nuclia_storage
 
 STORAGE_RESOURCE = "kbs/{kbid}/r/{uuid}"
 KB_RESOURCE_FIELD = "kbs/{kbid}/r/{uuid}/f/f/{field}"
 KB_LAYOUT_FIELD = "kbs/{kbid}/r/{uuid}/f/l/{field}/{ident}"
 KB_CONVERSATION_FIELD = "kbs/{kbid}/r/{uuid}/f/c/{field}/{ident}/{count}"
 STORAGE_FILE_EXTRACTED = "kbs/{kbid}/r/{uuid}/e/{field_type}/{field}/{key}"
 
 DEADLETTER = "deadletter/{partition}/{seqid}/{seq}"
 OLD_INDEXING_KEY = "index/{node}/{shard}/{txid}"
 INDEXING_KEY = "index/{kb}/{shard}/{resource}/{txid}"
+# temporary storage for large stream data
+MESSAGE_KEY = "message/{kbid}/{rid}/{mid}"
 
 
 class StorageField:
     storage: Storage
     bucket: str
     key: str
     field: Optional[CloudFile] = None
@@ -85,15 +90,15 @@
         """
         raise NotImplementedError()
         yield b""  # pragma: no cover
 
     async def delete(self) -> bool:
         deleted = False
         if self.field is not None:
-            await self.storage.delete_upload(self.bucket, self.field.uri)
+            await self.storage.delete_upload(self.field.uri, self.bucket)
             deleted = True
         return deleted
 
     async def exists(self) -> Optional[Dict[str, str]]:
         raise NotImplementedError
 
     def build_cf(self) -> CloudFile:
@@ -168,65 +173,51 @@
     async def indexing(
         self,
         message: BrainResource,
         txid: int,
         partition: Optional[str],
         kb: str,
         logical_shard: str,
-    ) -> IndexMessage:
+    ) -> str:
         if self.indexing_bucket is None:
             raise AttributeError()
         if txid < 0:
             txid = 0
 
         key = self.get_indexing_storage_key(
             kb=kb,
             logical_shard=logical_shard,
             resource_uid=message.resource.uuid,
             txid=txid,
         )
         await self.uploadbytes(self.indexing_bucket, key, message.SerializeToString())
-        response = IndexMessage()
-        response.txid = txid
-        response.typemessage = TypeMessage.CREATION
-        response.storage_key = key
-        response.kbid = kb
-        if partition:
-            response.partition = partition
-        return response
+
+        return key
 
     async def reindexing(
         self,
         message: BrainResource,
         reindex_id: str,
         partition: Optional[str],
         kb: str,
         logical_shard: str,
-    ) -> IndexMessage:
+    ) -> str:
         if self.indexing_bucket is None:
             raise AttributeError()
         key = self.get_indexing_storage_key(
             kb=kb,
             logical_shard=logical_shard,
             resource_uid=message.resource.uuid,
             txid=reindex_id,
         )
-        logger.info("Starting to serialize message")
         message_serialized = message.SerializeToString()
-        logger.info("Starting to upload bytes")
+        logger.debug("Starting to upload bytes")
         await self.uploadbytes(self.indexing_bucket, key, message_serialized)
-        logger.info("Finished to upload bytes")
-        response = IndexMessage()
-        response.reindex_id = reindex_id
-        response.typemessage = TypeMessage.CREATION
-        response.storage_key = key
-        response.kbid = kb
-        if partition:
-            response.partition = partition
-        return response
+        logger.debug("Finished to upload bytes")
+        return key
 
     async def get_indexing(self, payload: IndexMessage) -> BrainResource:
         if self.indexing_bucket is None:
             raise AttributeError()
         if payload.storage_key:
             key = payload.storage_key
         else:
@@ -285,36 +276,53 @@
             return False
         else:
             return True
 
     async def normalize_binary(
         self, file: CloudFile, destination: StorageField
     ):  # pragma: no cover
-        # this is covered by other tests
-        # see if file is in the same Cloud in the same bucket
         if file.source == self.source and file.uri != destination.key:
-            new_cf = await self.move(file, destination)
-
+            # This MAY BE the case for NucliaDB hosted deployment (Nuclia's cloud deployment):
+            # The data has been pushed to the bucket but with a different key.
+            logger.warning(
+                f"[Nuclia hosted] Source and destination keys differ!: {file.uri} != {destination.key}"
+            )
+            await self.move(file, destination)
+            new_cf = CloudFile()
+            new_cf.CopyFrom(file)
+            new_cf.bucket_name = destination.bucket
+            new_cf.uri = destination.key
         elif file.source == self.source:
+            # This is the case for NucliaDB hosted deployment (Nuclia's cloud deployment):
+            # The data is already stored in the right place by the processing
+            logger.debug(f"[Nuclia hosted]")
             return file
         elif file.source == CloudFile.EXPORT:
+            # This is for files coming from an export
+            logger.debug(f"[Exported file]: {file.uri}")
             new_cf = CloudFile()
             new_cf.CopyFrom(file)
             new_cf.bucket_name = destination.bucket
             new_cf.uri = destination.key
-            new_cf.source = self.source
+            new_cf.source = self.source  # type: ignore
         elif file.source == CloudFile.FLAPS:
+            # NucliaDB On-Prem: the data is stored in NUA, so we need to
+            # download it and upload it to NucliaDB's storage
+            logger.debug(f"[NucliaDB OnPrem]: {file.uri}")
             flaps_storage = await get_nuclia_storage()
             iterator = flaps_storage.download(file)
             new_cf = await self.uploaditerator(iterator, destination, file)
         elif file.source == CloudFile.LOCAL:
+            # For testing purposes: protobuffer is stored in a file in the local filesystem
+            logger.debug(f"[Local]: {file.uri}")
             local_storage = get_local_storage()
             iterator = local_storage.download(file.bucket_name, file.uri)
             new_cf = await self.uploaditerator(iterator, destination, file)
         elif file.source == CloudFile.EMPTY:
+            logger.warning(f"[Empty file]: {file.uri}")
             new_cf = CloudFile()
             new_cf.CopyFrom(file)
         else:
             raise InvalidCloudFile()
         return new_cf
 
     def conversation_field(
@@ -414,15 +422,16 @@
 
         generator = splitter(buffer)
         await self.uploaditerator(generator, destination, cf)
 
     async def uploaditerator(
         self, iterator: AsyncIterator, destination: StorageField, origin: CloudFile
     ) -> CloudFile:
-        return await destination.upload(iterator, origin)
+        safe_iterator = iterate_storage_compatible(iterator, self, origin)  # type: ignore
+        return await destination.upload(safe_iterator, origin)
 
     async def download(
         self, bucket: str, key: str, headers: Optional[Dict[str, str]] = None
     ):
         destination: StorageField = self.field_klass(
             storage=self, bucket=bucket, fullkey=key
         )
@@ -527,7 +536,63 @@
         raise NotImplementedError()
 
     async def delete_kb(self, kbid: str) -> Tuple[bool, bool]:
         raise NotImplementedError()
 
     async def schedule_delete_kb(self, kbid: str) -> bool:
         raise NotImplementedError()
+
+    async def set_stream_message(self, kbid: str, rid: str, data: bytes) -> str:
+        key = MESSAGE_KEY.format(kbid=kbid, rid=rid, mid=uuid.uuid4())
+        await self.uploadbytes(cast(str, self.indexing_bucket), key, data)
+        return key
+
+    async def get_stream_message(self, key: str) -> bytes:
+        bytes_buffer = await self.downloadbytes(cast(str, self.indexing_bucket), key)
+        if bytes_buffer.getbuffer().nbytes == 0:
+            raise KeyError(f'Stream message data not found for key "{key}"')
+        return bytes_buffer.read()
+
+    async def del_stream_message(self, key: str) -> None:
+        await self.delete_upload(key, cast(str, self.indexing_bucket))
+
+
+async def iter_and_add_size(
+    stream: AsyncGenerator[bytes, None], cf: CloudFile
+) -> AsyncGenerator[bytes, None]:
+    # This is needed because some storage types like GCS or S3 require
+    # the size of the file at least at the request done for the last chunk.
+    total_size = 0
+    async for chunk, is_last in async_gen_lookahead(stream):
+        total_size += len(chunk)
+        if is_last:
+            cf.size = total_size
+        yield chunk
+
+
+async def iter_in_chunk_size(
+    iterator: AsyncGenerator[bytes, None], chunk_size: int
+) -> AsyncGenerator[bytes, None]:
+    # This is needed to make sure bytes uploaded to the blob storage complies with a particular chunk size.
+    buffer = b""
+    async for chunk in iterator:
+        buffer += chunk
+        if len(buffer) >= chunk_size:
+            yield buffer[:chunk_size]
+            buffer = buffer[chunk_size:]
+    # The last chunk can be smaller than chunk size
+    if len(buffer) > 0:
+        yield buffer
+
+
+async def iterate_storage_compatible(
+    iterator: AsyncGenerator[bytes, None], storage: Storage, cf: CloudFile
+) -> AsyncGenerator[bytes, None]:
+    """
+    Makes sure to add the size to the cloudfile and split the data in
+    chunks that are compatible with the storage type of choice
+    """
+
+    async for chunk in iter_in_chunk_size(
+        iter_and_add_size(iterator, cf), chunk_size=storage.chunk_size
+    ):
+        yield chunk
```

## nucliadb_utils/tests/__init__.py

```diff
@@ -12,17 +12,31 @@
 # This program is distributed in the hope that it will be useful,
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
+import random
+import socket
 
 
-def free_port() -> int:
-    import socket
+def free_port(retries=20) -> int:
+    port_range_start = 1024  # Start of the port range (1024 is usually safe)
+    port_range_end = 65535  # End of the port range
 
+    for _ in range(retries):
+        port = random.randint(port_range_start, port_range_end)
+        try:
+            sock = socket.socket()
+            sock.bind(("", port))
+            sock.close()
+            return port
+        except OSError:
+            continue
+
+    # give up and let OS pick a free port
     sock = socket.socket()
     sock.bind(("", 0))
     port = sock.getsockname()[1]
     sock.close()
     return port
```

## nucliadb_utils/tests/asyncbenchmark.py

```diff
@@ -33,16 +33,15 @@
 from pytest_benchmark.utils import NameWrapper, format_time  # type: ignore
 
 T = TypeVar("T")
 
 from pytest_benchmark.stats import Metadata  # type: ignore
 
 
-class FixtureAlreadyUsed(Exception):
-    ...
+class FixtureAlreadyUsed(Exception): ...  # noqa
 
 
 class AsyncBenchmarkFixture(object):  # pragma: no cover
     _precisions: Dict[Callable, float] = {}
 
     def __init__(
         self,
```

## nucliadb_utils/tests/conftest.py

```diff
@@ -12,41 +12,45 @@
 # This program is distributed in the hope that it will be useful,
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
-import asyncpg
-import pytest
+import os
 
-from nucliadb_utils.storages.pg import PostgresStorage
-from nucliadb_utils.store import MAIN
+import pytest
+from pytest_lazy_fixtures import lazy_fixture
 
 pytest_plugins = [
     "pytest_docker_fixtures",
     "nucliadb_utils.tests.gcs",
     "nucliadb_utils.tests.nats",
     "nucliadb_utils.tests.s3",
+    "nucliadb_utils.tests.pg",
     "nucliadb_utils.tests.local",
-    "nucliadb_utils.tests.clandestined",
 ]
 
 
-@pytest.fixture(scope="function")
-async def pg_storage(pg):
-    dsn = f"postgresql://postgres:postgres@{pg[0]}:{pg[1]}/postgres"
-    storage = PostgresStorage(dsn)
-    MAIN["storage"] = storage
-    conn = await asyncpg.connect(dsn)
-    await conn.execute(
-        """
-DROP table IF EXISTS kb_files;
-DROP table IF EXISTS kb_files_fileparts;
-"""
-    )
-    await conn.close()
-    await storage.initialize()
-    yield storage
-    await storage.finalize()
-    if "storage" in MAIN:
-        del MAIN["storage"]
+def get_testing_storage_backend(default="gcs"):
+    return os.environ.get("TESTING_STORAGE_BACKEND", default)
+
+
+def lazy_storage_fixture():
+    backend = get_testing_storage_backend()
+    if backend == "gcs":
+        return [lazy_fixture.lf("gcs_storage")]
+    elif backend == "s3":
+        return [lazy_fixture.lf("s3_storage")]
+    elif backend == "pg":
+        return [lazy_fixture.lf("pg_storage")]
+    else:
+        print(f"Unknown storage backend {backend}, using gcs")
+        return [lazy_fixture.lf("gcs_storage")]
+
+
+@pytest.fixture(scope="function", params=lazy_storage_fixture())
+async def storage(request):
+    """
+    Generic storage fixture that allows us to run the same tests for different storage backends.
+    """
+    return request.param
```

## nucliadb_utils/tests/gcs.py

```diff
@@ -25,41 +25,51 @@
 import pytest
 import requests
 from pytest_docker_fixtures import images  # type: ignore
 from pytest_docker_fixtures.containers._base import BaseImage  # type: ignore
 
 from nucliadb_utils.storages.gcs import GCSStorage
 from nucliadb_utils.store import MAIN
+from nucliadb_utils.tests import free_port
+from nucliadb_utils.utilities import Utility  # type: ignore
 
 # IMPORTANT!
 # Without this, tests running in a remote docker host won't work
 DOCKER_ENV_GROUPS = re.search(r"//([^:]+)", docker.from_env().api.base_url)
 DOCKER_HOST: Optional[str] = DOCKER_ENV_GROUPS.group(1) if DOCKER_ENV_GROUPS else None  # type: ignore
 
 images.settings["gcs"] = {
     "image": "fsouza/fake-gcs-server",
     "version": "1.44.1",
-    "options": {
-        "command": f"-scheme http -external-url http://{DOCKER_HOST}:4443 -port 4443",
-        "ports": {"4443": "4443"},
-    },
+    "options": {},
 }
 
 
 class GCS(BaseImage):
     name = "gcs"
-    port = 4443
+
+    def __init__(self):
+        super().__init__()
+        self.port = free_port()
+
+    def get_image_options(self):
+        options = super().get_image_options()
+        options["ports"] = {str(self.port): str(self.port)}
+        options["command"] = (
+            f"-scheme http -external-url http://{DOCKER_HOST}:{self.port} -port {self.port}"
+        )
+        return options
 
     def check(self):
         try:
             response = requests.get(
                 f"http://{self.host}:{self.get_port()}/storage/v1/b"
             )
             return response.status_code == 200
-        except:  # pragma: no cover
+        except Exception:  # pragma: no cover
             return False
 
 
 @pytest.fixture(scope="session")
 def gcs():
     container = GCS()
     host, port = container.run()
@@ -77,13 +87,12 @@
         project="project",
         executor=ThreadPoolExecutor(1),
         deadletter_bucket="deadletters",
         indexing_bucket="indexing",
         labels={},
         url=gcs,
     )
-    MAIN["storage"] = storage
+    MAIN[Utility.STORAGE] = storage
     await storage.initialize()
     yield storage
     await storage.finalize()
-    if "storage" in MAIN:
-        del MAIN["storage"]
+    MAIN.pop(Utility.STORAGE, None)
```

## nucliadb_utils/tests/local.py

```diff
@@ -19,21 +19,21 @@
 #
 import tempfile
 
 import pytest
 
 from nucliadb_utils.storages.local import LocalStorage
 from nucliadb_utils.store import MAIN
+from nucliadb_utils.utilities import Utility
 
 
 @pytest.fixture(scope="function")
 async def local_storage():
     folder = tempfile.TemporaryDirectory()
     storage = LocalStorage(local_testing_files=folder.name)
 
-    MAIN["storage"] = storage
+    MAIN[Utility.STORAGE] = storage
     await storage.initialize()
     yield storage
     await storage.finalize()
     folder.cleanup()
-    if "storage" in MAIN:
-        del MAIN["storage"]
+    MAIN.pop(Utility.STORAGE, None)
```

## nucliadb_utils/tests/nats.py

```diff
@@ -199,15 +199,15 @@
             retries += 1
             time.sleep(0.1)
 
 
 @pytest.fixture(scope="session")
 def natsd_server():  # pragma: no cover
     if not os.path.isfile("nats-server"):
-        version = "v2.9.16"
+        version = "v2.10.12"
         arch = platform.machine()
         if arch == "x86_64":
             arch = "amd64"
         system = platform.system().lower()
 
         url = f"https://github.com/nats-io/nats-server/releases/download/{version}/nats-server-{version}-{system}-{arch}.zip"
 
@@ -232,15 +232,15 @@
     print("Started natsd")
     yield f"nats://localhost:{natsd_server.port}"
     natsd_server.stop()
 
 
 images.settings["nats"] = {
     "image": "nats",
-    "version": "2.9.16",
+    "version": "2.10.12",
     "options": {"command": ["-js"], "ports": {"4222": None}},
 }
 
 
 class NatsImage(BaseImage):  # pragma: no cover
     name = "nats"
     port = 4222
```

## nucliadb_utils/tests/s3.py

```diff
@@ -20,14 +20,15 @@
 import pytest
 import requests
 from pytest_docker_fixtures import images  # type: ignore
 from pytest_docker_fixtures.containers._base import BaseImage  # type: ignore
 
 from nucliadb_utils.storages.s3 import S3Storage
 from nucliadb_utils.store import MAIN
+from nucliadb_utils.utilities import Utility
 
 images.settings["s3"] = {
     "image": "localstack/localstack",
     "version": "0.12.18",
     "env": {"SERVICES": "s3"},
     "options": {
         "ports": {"4566": None, "4571": None},
@@ -39,15 +40,15 @@
     name = "s3"
     port = 4566
 
     def check(self):
         try:
             response = requests.get(f"http://{self.host}:{self.get_port()}")
             return response.status_code == 404
-        except:
+        except Exception:
             return False
 
 
 @pytest.fixture(scope="session")
 def s3():
     container = S3()
     host, port = container.run()
@@ -64,13 +65,14 @@
         deadletter_bucket="deadletter",
         indexing_bucket="indexing",
         endpoint_url=s3,
         verify_ssl=False,
         use_ssl=False,
         region_name=None,
         bucket="test-{kbid}",
+        bucket_tags={"testTag": "test"},
     )
     await storage.initialize()
-    MAIN["storage"] = storage
+    MAIN[Utility.STORAGE] = storage
     yield storage
     await storage.finalize()
-    del MAIN["storage"]
+    MAIN.pop(Utility.STORAGE, None)
```

## nucliadb_utils/tests/unit/test_authentication.py

```diff
@@ -23,46 +23,46 @@
 from fastapi import WebSocket
 from starlette.exceptions import HTTPException
 from starlette.requests import Request
 
 from nucliadb_utils import authentication
 
 
-class TestSTFAuthenticationBackend:
+class TestNucliaCloudAuthenticationBackend:
     @pytest.fixture()
     def backend(self):
-        return authentication.STFAuthenticationBackend()
+        return authentication.NucliaCloudAuthenticationBackend()
 
     @pytest.fixture()
     def req(self):
         return Mock(headers={})
 
     @pytest.mark.asyncio
     async def test_authenticate(
-        self, backend: authentication.STFAuthenticationBackend, req
+        self, backend: authentication.NucliaCloudAuthenticationBackend, req
     ):
         assert await backend.authenticate(req) is None
 
     @pytest.mark.asyncio
     async def test_authenticate_with_user(
-        self, backend: authentication.STFAuthenticationBackend, req
+        self, backend: authentication.NucliaCloudAuthenticationBackend, req
     ):
         req.headers = {
             backend.roles_header: "admin",
             backend.user_header: "user",
         }
 
         creds, user = await backend.authenticate(req)
 
         assert creds.scopes == ["admin"]
         assert user.username == "user"
 
     @pytest.mark.asyncio
     async def test_authenticate_with_anon(
-        self, backend: authentication.STFAuthenticationBackend, req
+        self, backend: authentication.NucliaCloudAuthenticationBackend, req
     ):
         req.headers = {backend.roles_header: "admin"}
 
         creds, user = await backend.authenticate(req)
 
         assert creds.scopes == ["admin"]
         assert user.username == "anonymous"
@@ -76,14 +76,20 @@
 
 class TestRequires:
     def test_requires_sync(self):
         req = Request({"type": "http", "auth": Mock(scopes=["admin"])})
 
         assert authentication.requires(["admin"])(lambda request: None)(req) is None
 
+        # test passed as kwargs
+        assert (
+            authentication.requires(["admin"])(lambda request: None)(request=req)
+            is None
+        )
+
     def test_requires_sync_returns_status(self):
         req = Request({"type": "http", "auth": Mock(scopes=["admin"])})
 
         with pytest.raises(HTTPException):
             assert authentication.requires(["foobar"])(lambda request: None)(req)
 
     def test_requires_sync_returns_redirect(self):
@@ -95,35 +101,32 @@
             )(req)
         assert resp.status_code == 303
 
     @pytest.mark.asyncio
     async def test_requires_async(self):
         req = Request({"type": "http", "auth": Mock(scopes=["admin"])})
 
-        async def noop(request):
-            ...
+        async def noop(request): ...
 
         assert await authentication.requires(["admin"])(noop)(request=req) is None
 
     @pytest.mark.asyncio
     async def test_requires_async_returns_status(self):
         req = Request({"type": "http", "auth": Mock(scopes=["admin"])})
 
-        async def noop(request):
-            ...
+        async def noop(request): ...
 
         with pytest.raises(HTTPException):
             assert await authentication.requires(["foobar"])(noop)(request=req)
 
     @pytest.mark.asyncio
     async def test_requires_async_returns_redirect(self):
         req = Request({"type": "http", "auth": Mock(scopes=["admin"])})
 
-        async def noop(request):
-            ...
+        async def noop(request): ...
 
         with patch.object(req, "url_for", return_value="http://foobar"):
             resp = await authentication.requires(["foobar"], redirect="/foobar")(noop)(
                 request=req
             )
         assert resp.status_code == 303
 
@@ -132,27 +135,25 @@
         ws = AsyncMock()
         req = WebSocket(
             {"type": "websocket", "auth": Mock(scopes=["admin"]), "websocket": ws},
             receive=AsyncMock(),
             send=AsyncMock(),
         )
 
-        async def noop(websocket):
-            ...
+        async def noop(websocket): ...
 
         assert await authentication.requires(["admin"])(noop)(req) is None
 
     @pytest.mark.asyncio
     async def test_requires_ws_fail(self):
         req = WebSocket(
             {"type": "websocket", "auth": Mock(scopes=["admin"])},
             receive=AsyncMock(),
             send=AsyncMock(),
         )
 
-        async def noop(websocket):
-            ...
+        async def noop(websocket): ...
 
         with patch.object(req, "close", return_value=None):
             assert await authentication.requires(["notallowed"])(noop)(req) is None
 
             req.close.assert_called_once()
```

## nucliadb_utils/tests/unit/test_nats.py

```diff
@@ -13,14 +13,15 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
+import asyncio
 import time
 from unittest.mock import AsyncMock, MagicMock, patch
 
 import pytest
 
 from nucliadb_utils import nats
 
@@ -35,15 +36,18 @@
         conn.close = AsyncMock()
         with patch("nucliadb_utils.nats.nats.connect", return_value=conn):
             yield conn
 
     @pytest.fixture()
     def js(self):
         conn = AsyncMock()
-        with patch("nucliadb_utils.nats.get_traced_jetstream", return_value=conn):
+        with patch(
+            "nucliadb_utils.nats.get_traced_jetstream",
+            return_value=conn,
+        ):
             yield conn
 
     @pytest.fixture()
     def manager(self, nats_conn, js):
         yield nats.NatsConnectionManager(service_name="test", nats_servers=["test"])
 
     async def test_initialize(self, manager: nats.NatsConnectionManager, nats_conn):
@@ -68,14 +72,15 @@
         )
 
         js.subscribe.assert_called_once_with(
             subject="subject",
             queue="queue",
             stream="stream",
             cb=cb,
+            manual_ack=True,
             flow_control=True,
             config=None,
         )
 
         await manager.reconnected_cb()
         lost_cb.assert_called_once()
 
@@ -96,7 +101,54 @@
         manager._last_unhealthy = time.monotonic() - 100
         assert not manager.healthy()
 
         manager._last_unhealthy = None
         manager._nc.is_connected = False
         assert manager.healthy()
         assert manager._last_unhealthy is not None
+
+    async def test_unsubscribe(
+        self, manager: nats.NatsConnectionManager, nats_conn, js
+    ):
+        await manager.initialize()
+
+        cb = AsyncMock()
+        lost_cb = AsyncMock()
+        sub = await manager.subscribe(
+            subject="subject",
+            queue="queue",
+            stream="stream",
+            cb=cb,
+            subscription_lost_cb=lost_cb,
+            flow_control=True,
+        )
+        assert len(manager._subscriptions) == 1
+
+        await manager.unsubscribe(sub)
+
+        sub.unsubscribe.assert_awaited_once()
+        assert len(manager._subscriptions) == 0
+
+        await manager.finalize()
+
+        nats_conn.drain.assert_called_once()
+        nats_conn.close.assert_called_once()
+
+
+async def test_message_progress_updater():
+    in_progress = AsyncMock()
+    msg = MagicMock(in_progress=in_progress, _ackd=False)
+
+    async with nats.MessageProgressUpdater(msg, 0.05):
+        await asyncio.sleep(0.07)
+
+    in_progress.assert_awaited_once()
+
+
+async def test_message_progress_updater_does_not_update_ack():
+    in_progress = AsyncMock()
+    msg = MagicMock(in_progress=in_progress, _ackd=True)
+
+    async with nats.MessageProgressUpdater(msg, 0.05):
+        await asyncio.sleep(0.07)
+
+    in_progress.assert_not_awaited()
```

## nucliadb_utils/tests/unit/test_transaction.py

```diff
@@ -13,20 +13,25 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
+import asyncio
 from unittest import mock
 
 import pytest
-from nucliadb_protos.writer_pb2 import Notification
+from nucliadb_protos.writer_pb2 import BrokerMessage, Notification
 
-from nucliadb_utils.transaction import TransactionUtility, WaitFor
+from nucliadb_utils.transaction import (
+    TransactionCommitTimeoutError,
+    TransactionUtility,
+    WaitFor,
+)
 
 
 @pytest.fixture()
 def pubsub():
     pubsub = mock.AsyncMock()
     pubsub.parse = lambda msg: msg.data
     with mock.patch("nucliadb_utils.transaction.get_pubsub", return_value=pubsub):
@@ -96,7 +101,19 @@
     _ = await txn.wait_for_commited(kbid, waiting_for, request_id=request_id)
 
     await txn.stop_waiting(kbid, request_id=request_id)
 
     # Unsubscribing twice with the same request_id should raise KeyError
     with pytest.raises(KeyError):
         await txn.stop_waiting(kbid, request_id=request_id)
+
+
+@pytest.mark.asyncio
+async def test_commit_timeout(txn: TransactionUtility, pubsub):
+    txn.js = mock.AsyncMock()
+    bm = BrokerMessage()
+
+    waiting_event = mock.Mock(wait=mock.Mock(side_effect=asyncio.TimeoutError))
+    txn.wait_for_commited = mock.AsyncMock(return_value=waiting_event)  # type: ignore
+
+    with pytest.raises(TransactionCommitTimeoutError):
+        await txn.commit(bm, 1, wait=True, target_subject="foo")
```

## nucliadb_utils/tests/unit/test_utilities.py

```diff
@@ -28,18 +28,18 @@
 
 @pytest.fixture(autouse=True)
 def reset_main():
     utilities.MAIN.clear()
 
 
 def test_clean_utility():
-    utilities.set_utility(utilities.Utility.CACHE, "test")
-    assert utilities.get_utility(utilities.Utility.CACHE) == "test"
-    utilities.clean_utility(utilities.Utility.CACHE)
-    assert utilities.get_utility(utilities.Utility.CACHE) is None
+    utilities.set_utility(utilities.Utility.PUBSUB, "test")
+    assert utilities.get_utility(utilities.Utility.PUBSUB) == "test"
+    utilities.clean_utility(utilities.Utility.PUBSUB)
+    assert utilities.get_utility(utilities.Utility.PUBSUB) is None
 
 
 @pytest.mark.asyncio
 async def test_get_storage_s3():
     s3 = AsyncMock()
     with patch.object(utilities.storage_settings, "file_backend", "s3"), patch(
         "nucliadb_utils.storages.s3.S3Storage", return_value=s3
@@ -90,20 +90,14 @@
 
 @pytest.mark.asyncio
 async def test_get_nuclia_storage():
     assert await utilities.get_nuclia_storage() is not None
 
 
 @pytest.mark.asyncio
-async def test_get_cache():
-    with patch("nucliadb_utils.utilities.Cache", return_value=AsyncMock()):
-        assert await utilities.get_cache() is not None
-
-
-@pytest.mark.asyncio
 async def test_get_pubsub():
     with patch("nucliadb_utils.utilities.NatsPubsub", return_value=AsyncMock()):
         assert await utilities.get_pubsub() is not None
 
 
 @pytest.mark.asyncio
 async def test_finalize_utilities():
```

## nucliadb_utils/tests/unit/storages/test_pg.py

```diff
@@ -192,21 +192,23 @@
         await data_layer.move(
             origin_key="origin_key",
             destination_key="destination_key",
             origin_kb="origin_kb",
             destination_kb="destination_kb",
         )
 
-        assert connection.execute.call_count == 2
+        assert connection.execute.call_count == 4
 
         connection.execute.assert_has_awaits(
             [
+                call(ANY, "destination_kb", "destination_key"),
                 call(
                     ANY, "destination_kb", "destination_key", "origin_kb", "origin_key"
                 ),
+                call(ANY, "destination_kb", "destination_key"),
                 call(
                     ANY, "destination_kb", "destination_key", "origin_kb", "origin_key"
                 ),
             ]
         )
 
     async def test_copy(self, data_layer: pg.PostgresFileDataLayer, connection):
@@ -340,25 +342,27 @@
         await storage_field.move(
             "origin_uri",
             "destination_uri",
             "origin_bucket_name",
             "destination_bucket_name",
         )
 
-        assert connection.execute.call_count == 2
+        assert connection.execute.call_count == 4
 
         connection.execute.assert_has_awaits(
             [
+                call(ANY, "destination_bucket_name", "destination_uri"),
                 call(
                     ANY,
                     "destination_bucket_name",
                     "destination_uri",
                     "origin_bucket_name",
                     "origin_uri",
                 ),
+                call(ANY, "destination_bucket_name", "destination_uri"),
                 call(
                     ANY,
                     "destination_bucket_name",
                     "destination_uri",
                     "origin_bucket_name",
                     "origin_uri",
                 ),
@@ -468,28 +472,28 @@
         field.upload_uri = "upload_uri"
         field.old_uri = "old_uri"
         storage_field.field = field
 
         await storage_field.finish()
         assert field.uri == storage_field.key
 
-        assert connection.execute.call_count == 4
+        assert connection.execute.call_count == 6
 
     async def test_upload(
         self,
         storage_field: pg.PostgresStorageField,
         connection,
         field,
     ):
         field.upload_uri = "upload_uri"
         storage_field.field = field
 
         await storage_field.upload(iter_result([b"test1", b"test2"]), field)
 
-        assert connection.execute.call_count == 7
+        assert connection.execute.call_count == 9
 
 
 class TestPostgresStorage:
     async def test_initialize(self, storage: pg.PostgresStorage, pool, connection):
         await storage.initialize()
 
         assert pool.acquire.call_count == 1
```

## nucliadb_utils/tests/unit/storages/test_storage.py

```diff
@@ -13,23 +13,29 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
+from math import ceil
 from unittest.mock import AsyncMock, MagicMock
 
 import pytest
 from nucliadb_protos.noderesources_pb2 import Resource as BrainResource
 from nucliadb_protos.noderesources_pb2 import ResourceID
 from nucliadb_protos.nodewriter_pb2 import IndexMessage
 from nucliadb_protos.resources_pb2 import CloudFile
 
-from nucliadb_utils.storages.storage import Storage, StorageField
+from nucliadb_utils.storages.storage import (
+    Storage,
+    StorageField,
+    iter_and_add_size,
+    iter_in_chunk_size,
+)
 
 
 class TestStorageField:
     @pytest.fixture
     def storage(self):
         yield AsyncMock(source=0)
 
@@ -40,15 +46,15 @@
     @pytest.fixture
     def storage_field(self, storage, field):
         yield StorageField(storage, "bucket", "fullkey", field)
 
     @pytest.mark.asyncio
     async def test_delete(self, storage_field: StorageField, storage):
         await storage_field.delete()
-        storage.delete_upload.assert_called_once_with("bucket", "uri")
+        storage.delete_upload.assert_called_once_with("uri", "bucket")
 
     def test_build_cf(self, storage_field: StorageField):
         cf = CloudFile()
         cf.bucket_name = "bucket"
         cf.uri = "fullkey"
         cf.filename = "payload.pb"
         assert storage_field.build_cf() == cf
@@ -160,7 +166,50 @@
         with pytest.raises(AttributeError):
             await storage.delete_indexing(
                 resource_uid="resource_uid",
                 txid=1,
                 kb="kb",
                 logical_shard="logical_shard",
             )
+
+
+async def testiter_and_add_size():
+    cf = CloudFile()
+
+    async def iter():
+        yield b"foo"
+        yield b"bar"
+
+    cf.size = 0
+    async for _ in iter_and_add_size(iter(), cf):
+        pass
+
+    assert cf.size == 6
+
+
+async def test_iter_in_chunk_size():
+    async def iterable(total_size, *, chunk_size=1):
+        data = b"0" * total_size
+        for i in range(ceil(total_size / chunk_size)):
+            chunk = data[i * chunk_size : (i + 1) * chunk_size]
+            yield chunk
+
+    chunks = [chunk async for chunk in iter_in_chunk_size(iterable(10), chunk_size=4)]
+    assert len(chunks) == 3
+    assert len(chunks[0]) == 4
+    assert len(chunks[1]) == 4
+    assert len(chunks[2]) == 2
+
+    chunks = [chunk async for chunk in iter_in_chunk_size(iterable(0), chunk_size=4)]
+    assert len(chunks) == 0
+
+    # Try with an iterable that yields chunks bigger than the chunk size
+    chunks = [
+        chunk
+        async for chunk in iter_in_chunk_size(
+            iterable(total_size=12, chunk_size=10), chunk_size=4
+        )
+    ]
+    assert len(chunks) == 3
+    assert len(chunks[0]) == 4
+    assert len(chunks[1]) == 4
+    assert len(chunks[2]) == 4
```

## Comparing `nucliadb_utils/cache/memcache.py` & `nucliadb_utils/tests/pg.py`

 * *Files 24% similar despite different names*

```diff
@@ -13,24 +13,45 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import Optional
+import asyncpg
+import pytest
+from pytest_docker_fixtures import images  # type: ignore
 
-from nucliadb_utils.cache.settings import settings
+from nucliadb_utils.storages.pg import PostgresStorage
+from nucliadb_utils.store import MAIN
+from nucliadb_utils.utilities import Utility
 
-try:
-    from memorylru import LRU  # type: ignore
-except ModuleNotFoundError:  # pragma: no cover
-    from lru import LRU  # type: ignore
+images.settings["postgresql"].update(
+    {
+        "version": "16.1",
+        "env": {
+            "POSTGRES_PASSWORD": "postgres",
+            "POSTGRES_DB": "postgres",
+            "POSTGRES_USER": "postgres",
+        },
+    }
+)
 
-_lru: Optional[LRU] = None
 
-
-def get_memory_cache() -> LRU:
-    global _lru
-    if _lru is None:
-        _lru = LRU(settings.cache_memory_size)
-    return _lru
+@pytest.fixture(scope="function")
+async def pg_storage(pg):
+    dsn = f"postgresql://postgres:postgres@{pg[0]}:{pg[1]}/postgres"
+    storage = PostgresStorage(dsn)
+    MAIN[Utility.STORAGE] = storage
+    conn = await asyncpg.connect(dsn)
+    await conn.execute(
+        """
+DROP table IF EXISTS kb_files;
+DROP table IF EXISTS kb_files_fileparts;
+"""
+    )
+    await conn.close()
+    await storage.initialize()
+    yield storage
+    await storage.finalize()
+    if Utility.STORAGE in MAIN:
+        del MAIN[Utility.STORAGE]
```

