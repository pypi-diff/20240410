# Comparing `tmp/nucliadb-2.9.0.post267-py3-none-any.whl.zip` & `tmp/nucliadb-3.0.0.post414-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,294 +1,453 @@
-Zip file size: 482932 bytes, number of entries: 292
--rw-r--r--  2.0 unx      891 b- defN 23-May-15 13:40 nucliadb/__init__.py
--rw-r--r--  2.0 unx     1027 b- defN 23-May-15 13:40 nucliadb/app.py
--rw-r--r--  2.0 unx     4846 b- defN 23-May-15 13:40 nucliadb/config.py
--rw-r--r--  2.0 unx     3727 b- defN 23-May-15 13:40 nucliadb/health.py
--rw-r--r--  2.0 unx     1420 b- defN 23-May-15 13:40 nucliadb/purge.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-15 13:40 nucliadb/py.typed
--rw-r--r--  2.0 unx     2522 b- defN 23-May-15 13:40 nucliadb/run.py
--rw-r--r--  2.0 unx     1913 b- defN 23-May-15 13:40 nucliadb/settings.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/http_clients/__init__.py
--rw-r--r--  2.0 unx     1100 b- defN 23-May-15 13:40 nucliadb/http_clients/exceptions.py
--rw-r--r--  2.0 unx     4144 b- defN 23-May-15 13:40 nucliadb/http_clients/processing.py
--rw-r--r--  2.0 unx     1011 b- defN 23-May-15 13:40 nucliadb/ingest/__init__.py
--rw-r--r--  2.0 unx     6963 b- defN 23-May-15 13:40 nucliadb/ingest/app.py
--rw-r--r--  2.0 unx      995 b- defN 23-May-15 13:40 nucliadb/ingest/cache.py
--rw-r--r--  2.0 unx     6563 b- defN 23-May-15 13:40 nucliadb/ingest/chitchat.py
--rw-r--r--  2.0 unx     2425 b- defN 23-May-15 13:40 nucliadb/ingest/partitions.py
--rw-r--r--  2.0 unx    15817 b- defN 23-May-15 13:40 nucliadb/ingest/processing.py
--rw-r--r--  2.0 unx     5308 b- defN 23-May-15 13:40 nucliadb/ingest/purge.py
--rw-r--r--  2.0 unx    18891 b- defN 23-May-15 13:40 nucliadb/ingest/serialize.py
--rw-r--r--  2.0 unx     3025 b- defN 23-May-15 13:40 nucliadb/ingest/settings.py
--rw-r--r--  2.0 unx     2531 b- defN 23-May-15 13:40 nucliadb/ingest/txn_utils.py
--rw-r--r--  2.0 unx     4917 b- defN 23-May-15 13:40 nucliadb/ingest/utils.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/ingest/consumer/__init__.py
--rw-r--r--  2.0 unx    12021 b- defN 23-May-15 13:40 nucliadb/ingest/consumer/auditing.py
--rw-r--r--  2.0 unx    10516 b- defN 23-May-15 13:40 nucliadb/ingest/consumer/consumer.py
--rw-r--r--  2.0 unx     1075 b- defN 23-May-15 13:40 nucliadb/ingest/consumer/metrics.py
--rw-r--r--  2.0 unx     6885 b- defN 23-May-15 13:40 nucliadb/ingest/consumer/pull.py
--rw-r--r--  2.0 unx     6069 b- defN 23-May-15 13:40 nucliadb/ingest/consumer/service.py
--rw-r--r--  2.0 unx     4188 b- defN 23-May-15 13:40 nucliadb/ingest/consumer/shard_creator.py
--rw-r--r--  2.0 unx     2656 b- defN 23-May-15 13:40 nucliadb/ingest/consumer/utils.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/ingest/fields/__init__.py
--rw-r--r--  2.0 unx    19273 b- defN 23-May-15 13:40 nucliadb/ingest/fields/base.py
--rw-r--r--  2.0 unx     6495 b- defN 23-May-15 13:40 nucliadb/ingest/fields/conversation.py
--rw-r--r--  2.0 unx     1223 b- defN 23-May-15 13:40 nucliadb/ingest/fields/date.py
--rw-r--r--  2.0 unx     1205 b- defN 23-May-15 13:40 nucliadb/ingest/fields/exceptions.py
--rw-r--r--  2.0 unx     5159 b- defN 23-May-15 13:40 nucliadb/ingest/fields/file.py
--rw-r--r--  2.0 unx     1523 b- defN 23-May-15 13:40 nucliadb/ingest/fields/generic.py
--rw-r--r--  2.0 unx     1235 b- defN 23-May-15 13:40 nucliadb/ingest/fields/keywordset.py
--rw-r--r--  2.0 unx     2250 b- defN 23-May-15 13:40 nucliadb/ingest/fields/layout.py
--rw-r--r--  2.0 unx     4084 b- defN 23-May-15 13:40 nucliadb/ingest/fields/link.py
--rw-r--r--  2.0 unx     1319 b- defN 23-May-15 13:40 nucliadb/ingest/fields/text.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/ingest/maindb/__init__.py
--rw-r--r--  2.0 unx     2631 b- defN 23-May-15 13:40 nucliadb/ingest/maindb/driver.py
--rw-r--r--  2.0 unx      980 b- defN 23-May-15 13:40 nucliadb/ingest/maindb/keys.py
--rw-r--r--  2.0 unx     7076 b- defN 23-May-15 13:40 nucliadb/ingest/maindb/local.py
--rw-r--r--  2.0 unx     5400 b- defN 23-May-15 13:40 nucliadb/ingest/maindb/pg.py
--rw-r--r--  2.0 unx     6146 b- defN 23-May-15 13:40 nucliadb/ingest/maindb/redis.py
--rw-r--r--  2.0 unx     6313 b- defN 23-May-15 13:40 nucliadb/ingest/maindb/tikv.py
--rw-r--r--  2.0 unx     3483 b- defN 23-May-15 13:40 nucliadb/ingest/orm/__init__.py
--rw-r--r--  2.0 unx     6783 b- defN 23-May-15 13:40 nucliadb/ingest/orm/abc.py
--rw-r--r--  2.0 unx    20650 b- defN 23-May-15 13:40 nucliadb/ingest/orm/brain.py
--rw-r--r--  2.0 unx    15267 b- defN 23-May-15 13:40 nucliadb/ingest/orm/entities.py
--rw-r--r--  2.0 unx     1448 b- defN 23-May-15 13:40 nucliadb/ingest/orm/exceptions.py
--rw-r--r--  2.0 unx    12586 b- defN 23-May-15 13:40 nucliadb/ingest/orm/grpc_node_binding.py
--rw-r--r--  2.0 unx     4606 b- defN 23-May-15 13:40 nucliadb/ingest/orm/grpc_node_dummy.py
--rw-r--r--  2.0 unx    21132 b- defN 23-May-15 13:40 nucliadb/ingest/orm/knowledgebox.py
--rw-r--r--  2.0 unx     1730 b- defN 23-May-15 13:40 nucliadb/ingest/orm/labels.py
--rw-r--r--  2.0 unx     5160 b- defN 23-May-15 13:40 nucliadb/ingest/orm/local_node.py
--rw-r--r--  2.0 unx     2614 b- defN 23-May-15 13:40 nucliadb/ingest/orm/local_shard.py
--rw-r--r--  2.0 unx    11289 b- defN 23-May-15 13:40 nucliadb/ingest/orm/node.py
--rw-r--r--  2.0 unx     4278 b- defN 23-May-15 13:40 nucliadb/ingest/orm/nodes_manager.py
--rw-r--r--  2.0 unx    51458 b- defN 23-May-15 13:40 nucliadb/ingest/orm/resource.py
--rw-r--r--  2.0 unx     4227 b- defN 23-May-15 13:40 nucliadb/ingest/orm/shard.py
--rw-r--r--  2.0 unx     1739 b- defN 23-May-15 13:40 nucliadb/ingest/orm/synonyms.py
--rw-r--r--  2.0 unx     3340 b- defN 23-May-15 13:40 nucliadb/ingest/orm/utils.py
--rw-r--r--  2.0 unx    20958 b- defN 23-May-15 13:40 nucliadb/ingest/orm/processor/__init__.py
--rw-r--r--  2.0 unx     1694 b- defN 23-May-15 13:40 nucliadb/ingest/orm/processor/sequence_manager.py
--rw-r--r--  2.0 unx     2057 b- defN 23-May-15 13:40 nucliadb/ingest/service/__init__.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/ingest/service/exceptions.py
--rw-r--r--  2.0 unx    34177 b- defN 23-May-15 13:40 nucliadb/ingest/service/writer.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/ingest/tests/__init__.py
--rw-r--r--  2.0 unx     1096 b- defN 23-May-15 13:40 nucliadb/ingest/tests/conftest.py
--rw-r--r--  2.0 unx    25872 b- defN 23-May-15 13:40 nucliadb/ingest/tests/fixtures.py
--rw-r--r--  2.0 unx     7549 b- defN 23-May-15 13:40 nucliadb/ingest/tests/tikv.py
--rw-r--r--  2.0 unx    62843 b- defN 23-May-15 13:40 nucliadb/ingest/tests/vectors.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/__init__.py
--rw-r--r--  2.0 unx     1740 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/test_chitchat.py
--rw-r--r--  2.0 unx      833 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/consumer/__init__.py
--rw-r--r--  2.0 unx     2549 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/consumer/test_auditing.py
--rw-r--r--  2.0 unx     5169 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/consumer/test_pull.py
--rw-r--r--  2.0 unx     2751 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/consumer/test_service.py
--rw-r--r--  2.0 unx     2013 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/consumer/test_shard_creator.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/ingest/__init__.py
--rw-r--r--  2.0 unx    25612 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/ingest/test_ingest.py
--rw-r--r--  2.0 unx     2339 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/ingest/test_processing_engine.py
--rw-r--r--  2.0 unx     8606 b- defN 23-May-15 13:40 nucliadb/ingest/tests/integration/ingest/test_relations.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/__init__.py
--rw-r--r--  2.0 unx     1179 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/test_cache.py
--rw-r--r--  2.0 unx     4246 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/test_chitchat.py
--rw-r--r--  2.0 unx     1432 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/test_partitions.py
--rw-r--r--  2.0 unx     5538 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/test_processing.py
--rw-r--r--  2.0 unx     3414 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/test_purge.py
--rw-r--r--  2.0 unx      978 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/test_settings.py
--rw-r--r--  2.0 unx     1403 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/test_txn_utils.py
--rw-r--r--  2.0 unx     2895 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/test_utils.py
--rw-r--r--  2.0 unx      833 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/consumer/__init__.py
--rw-r--r--  2.0 unx     2979 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/consumer/test_auditing.py
--rw-r--r--  2.0 unx     2041 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/consumer/test_pull.py
--rw-r--r--  2.0 unx     3868 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/consumer/test_shard_creator.py
--rw-r--r--  2.0 unx     1934 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/consumer/test_utils.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/orm/__init__.py
--rw-r--r--  2.0 unx     8991 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/orm/test_brain.py
--rw-r--r--  2.0 unx     2736 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/orm/test_cluster.py
--rw-r--r--  2.0 unx     4753 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/orm/test_node.py
--rw-r--r--  2.0 unx     3141 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/orm/test_processor.py
--rw-r--r--  2.0 unx     3772 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/orm/test_resource.py
--rw-r--r--  2.0 unx     1401 b- defN 23-May-15 13:40 nucliadb/ingest/tests/unit/orm/test_shard.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/models/__init__.py
--rw-r--r--  2.0 unx     1599 b- defN 23-May-15 13:40 nucliadb/models/responses.py
--rw-r--r--  2.0 unx      914 b- defN 23-May-15 13:40 nucliadb/one/__init__.py
--rw-r--r--  2.0 unx     4706 b- defN 23-May-15 13:40 nucliadb/one/app.py
--rw-r--r--  2.0 unx     2359 b- defN 23-May-15 13:40 nucliadb/one/lifecycle.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/one/tests/__init__.py
--rw-r--r--  2.0 unx     1164 b- defN 23-May-15 13:40 nucliadb/one/tests/conftest.py
--rw-r--r--  2.0 unx     3762 b- defN 23-May-15 13:40 nucliadb/one/tests/fixtures.py
--rw-r--r--  2.0 unx     2561 b- defN 23-May-15 13:40 nucliadb/one/tests/test_basic.py
--rw-r--r--  2.0 unx     2492 b- defN 23-May-15 13:40 nucliadb/one/tests/test_delete_field.py
--rw-r--r--  2.0 unx     6087 b- defN 23-May-15 13:40 nucliadb/one/tests/test_fieldmetadata.py
--rw-r--r--  2.0 unx     4594 b- defN 23-May-15 13:40 nucliadb/one/tests/test_services.py
--rw-r--r--  2.0 unx     4067 b- defN 23-May-15 13:40 nucliadb/one/tests/test_upload_download.py
--rw-r--r--  2.0 unx     1328 b- defN 23-May-15 13:40 nucliadb/reader/__init__.py
--rw-r--r--  2.0 unx     3356 b- defN 23-May-15 13:40 nucliadb/reader/app.py
--rw-r--r--  2.0 unx     1366 b- defN 23-May-15 13:40 nucliadb/reader/lifecycle.py
--rw-r--r--  2.0 unx     2186 b- defN 23-May-15 13:40 nucliadb/reader/openapi.py
--rw-r--r--  2.0 unx     1365 b- defN 23-May-15 13:40 nucliadb/reader/run.py
--rw-r--r--  2.0 unx      872 b- defN 23-May-15 13:40 nucliadb/reader/api/__init__.py
--rw-r--r--  2.0 unx     2433 b- defN 23-May-15 13:40 nucliadb/reader/api/models.py
--rw-r--r--  2.0 unx      995 b- defN 23-May-15 13:40 nucliadb/reader/api/v1/__init__.py
--rw-r--r--  2.0 unx     9790 b- defN 23-May-15 13:40 nucliadb/reader/api/v1/download.py
--rw-r--r--  2.0 unx     3570 b- defN 23-May-15 13:40 nucliadb/reader/api/v1/knowledgebox.py
--rw-r--r--  2.0 unx    10673 b- defN 23-May-15 13:40 nucliadb/reader/api/v1/resource.py
--rw-r--r--  2.0 unx     1011 b- defN 23-May-15 13:40 nucliadb/reader/api/v1/router.py
--rw-r--r--  2.0 unx    10766 b- defN 23-May-15 13:40 nucliadb/reader/api/v1/services.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/reader/tests/__init__.py
--rw-r--r--  2.0 unx     1101 b- defN 23-May-15 13:40 nucliadb/reader/tests/conftest.py
--rw-r--r--  2.0 unx     4635 b- defN 23-May-15 13:40 nucliadb/reader/tests/fixtures.py
--rw-r--r--  2.0 unx     2749 b- defN 23-May-15 13:40 nucliadb/reader/tests/test_list_resources.py
--rw-r--r--  2.0 unx     9946 b- defN 23-May-15 13:40 nucliadb/reader/tests/test_reader_file_download.py
--rw-r--r--  2.0 unx    11383 b- defN 23-May-15 13:40 nucliadb/reader/tests/test_reader_resource.py
--rw-r--r--  2.0 unx     6535 b- defN 23-May-15 13:40 nucliadb/reader/tests/test_reader_resource_field.py
--rw-r--r--  2.0 unx     1535 b- defN 23-May-15 13:40 nucliadb/search/__init__.py
--rw-r--r--  2.0 unx     4381 b- defN 23-May-15 13:40 nucliadb/search/app.py
--rw-r--r--  2.0 unx     2252 b- defN 23-May-15 13:40 nucliadb/search/lifecycle.py
--rw-r--r--  2.0 unx     2206 b- defN 23-May-15 13:40 nucliadb/search/openapi.py
--rw-r--r--  2.0 unx    11236 b- defN 23-May-15 13:40 nucliadb/search/predict.py
--rw-r--r--  2.0 unx     1366 b- defN 23-May-15 13:40 nucliadb/search/run.py
--rw-r--r--  2.0 unx     1108 b- defN 23-May-15 13:40 nucliadb/search/settings.py
--rw-r--r--  2.0 unx     1238 b- defN 23-May-15 13:40 nucliadb/search/utilities.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/search/api/__init__.py
--rw-r--r--  2.0 unx     1077 b- defN 23-May-15 13:40 nucliadb/search/api/v1/__init__.py
--rw-r--r--  2.0 unx     6845 b- defN 23-May-15 13:40 nucliadb/search/api/v1/chat.py
--rw-r--r--  2.0 unx     1831 b- defN 23-May-15 13:40 nucliadb/search/api/v1/feedback.py
--rw-r--r--  2.0 unx     9436 b- defN 23-May-15 13:40 nucliadb/search/api/v1/find.py
--rw-r--r--  2.0 unx     6938 b- defN 23-May-15 13:40 nucliadb/search/api/v1/knowledgebox.py
--rw-r--r--  2.0 unx     4781 b- defN 23-May-15 13:40 nucliadb/search/api/v1/resource.py
--rw-r--r--  2.0 unx      980 b- defN 23-May-15 13:40 nucliadb/search/api/v1/router.py
--rw-r--r--  2.0 unx    11487 b- defN 23-May-15 13:40 nucliadb/search/api/v1/search.py
--rw-r--r--  2.0 unx     4212 b- defN 23-May-15 13:40 nucliadb/search/api/v1/suggest.py
--rw-r--r--  2.0 unx      833 b- defN 23-May-15 13:40 nucliadb/search/requesters/__init__.py
--rw-r--r--  2.0 unx     8183 b- defN 23-May-15 13:40 nucliadb/search/requesters/utils.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/search/search/__init__.py
--rw-r--r--  2.0 unx     2352 b- defN 23-May-15 13:40 nucliadb/search/search/cache.py
--rw-r--r--  2.0 unx     5337 b- defN 23-May-15 13:40 nucliadb/search/search/fetch.py
--rw-r--r--  2.0 unx    13815 b- defN 23-May-15 13:40 nucliadb/search/search/find_merge.py
--rw-r--r--  2.0 unx    18899 b- defN 23-May-15 13:40 nucliadb/search/search/merge.py
--rw-r--r--  2.0 unx      947 b- defN 23-May-15 13:40 nucliadb/search/search/metrics.py
--rw-r--r--  2.0 unx    10051 b- defN 23-May-15 13:40 nucliadb/search/search/paragraphs.py
--rw-r--r--  2.0 unx     9905 b- defN 23-May-15 13:40 nucliadb/search/search/query.py
--rw-r--r--  2.0 unx     2647 b- defN 23-May-15 13:40 nucliadb/search/search/shards.py
--rw-r--r--  2.0 unx     2055 b- defN 23-May-15 13:40 nucliadb/search/search/synonyms.py
--rw-r--r--  2.0 unx     2410 b- defN 23-May-15 13:40 nucliadb/search/search/utils.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/search/tests/__init__.py
--rw-r--r--  2.0 unx     1172 b- defN 23-May-15 13:40 nucliadb/search/tests/conftest.py
--rw-r--r--  2.0 unx     7759 b- defN 23-May-15 13:40 nucliadb/search/tests/fixtures.py
--rw-r--r--  2.0 unx    15809 b- defN 23-May-15 13:40 nucliadb/search/tests/node.py
--rw-r--r--  2.0 unx      833 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/__init__.py
--rw-r--r--  2.0 unx     1560 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/test_app.py
--rw-r--r--  2.0 unx     1776 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/test_find_orderer.py
--rw-r--r--  2.0 unx     1329 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/test_openapi.py
--rw-r--r--  2.0 unx     8495 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/test_predict.py
--rw-r--r--  2.0 unx     1258 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/test_run.py
--rw-r--r--  2.0 unx      833 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/search/__init__.py
--rw-r--r--  2.0 unx     3736 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/search/test_fetch.py
--rw-r--r--  2.0 unx     3771 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/search/test_paragraphs.py
--rw-r--r--  2.0 unx     2512 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/search/test_synonyms.py
--rw-r--r--  2.0 unx      833 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/search/requesters/__init__.py
--rw-r--r--  2.0 unx     3444 b- defN 23-May-15 13:40 nucliadb/search/tests/unit/search/requesters/test_utils.py
--rw-r--r--  2.0 unx     2285 b- defN 23-May-15 13:40 nucliadb/static/favicon.ico
--rw-r--r--  2.0 unx     3833 b- defN 23-May-15 13:40 nucliadb/static/index.html
--rw-r--r--  2.0 unx     2639 b- defN 23-May-15 13:40 nucliadb/static/logo.svg
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/tests/__init__.py
--rw-r--r--  2.0 unx     1000 b- defN 23-May-15 13:40 nucliadb/tests/conftest.py
--rw-r--r--  2.0 unx    11567 b- defN 23-May-15 13:40 nucliadb/tests/fixtures.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/tests/benchmarks/__init__.py
--rw-r--r--  2.0 unx     2848 b- defN 23-May-15 13:40 nucliadb/tests/benchmarks/test_search.py
--rw-r--r--  2.0 unx      919 b- defN 23-May-15 13:40 nucliadb/tests/knowledgeboxes/__init__.py
--rw-r--r--  2.0 unx     7049 b- defN 23-May-15 13:40 nucliadb/tests/knowledgeboxes/philosophy_books.py
--rw-r--r--  2.0 unx     3131 b- defN 23-May-15 13:40 nucliadb/tests/knowledgeboxes/ten_dummy_resources.py
--rw-r--r--  2.0 unx     2507 b- defN 23-May-15 13:40 nucliadb/tests/utils/__init__.py
--rw-r--r--  2.0 unx     2533 b- defN 23-May-15 13:40 nucliadb/tests/utils/entities.py
--rw-r--r--  2.0 unx     1325 b- defN 23-May-15 13:40 nucliadb/train/__init__.py
--rw-r--r--  2.0 unx     3340 b- defN 23-May-15 13:40 nucliadb/train/app.py
--rw-r--r--  2.0 unx     3832 b- defN 23-May-15 13:40 nucliadb/train/generator.py
--rw-r--r--  2.0 unx     1638 b- defN 23-May-15 13:40 nucliadb/train/lifecycle.py
--rw-r--r--  2.0 unx     1198 b- defN 23-May-15 13:40 nucliadb/train/models.py
--rw-r--r--  2.0 unx     7906 b- defN 23-May-15 13:40 nucliadb/train/nodes.py
--rw-r--r--  2.0 unx     1364 b- defN 23-May-15 13:40 nucliadb/train/run.py
--rw-r--r--  2.0 unx     5755 b- defN 23-May-15 13:40 nucliadb/train/servicer.py
--rw-r--r--  2.0 unx     1415 b- defN 23-May-15 13:40 nucliadb/train/settings.py
--rw-r--r--  2.0 unx     3271 b- defN 23-May-15 13:40 nucliadb/train/upload.py
--rw-r--r--  2.0 unx     6419 b- defN 23-May-15 13:40 nucliadb/train/uploader.py
--rw-r--r--  2.0 unx     3005 b- defN 23-May-15 13:40 nucliadb/train/utils.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/train/api/__init__.py
--rw-r--r--  2.0 unx      958 b- defN 23-May-15 13:40 nucliadb/train/api/models.py
--rw-r--r--  2.0 unx     1479 b- defN 23-May-15 13:40 nucliadb/train/api/utils.py
--rw-r--r--  2.0 unx      928 b- defN 23-May-15 13:40 nucliadb/train/api/v1/__init__.py
--rw-r--r--  2.0 unx     2066 b- defN 23-May-15 13:40 nucliadb/train/api/v1/check.py
--rw-r--r--  2.0 unx      910 b- defN 23-May-15 13:40 nucliadb/train/api/v1/router.py
--rw-r--r--  2.0 unx     1905 b- defN 23-May-15 13:40 nucliadb/train/api/v1/shards.py
--rw-r--r--  2.0 unx     1825 b- defN 23-May-15 13:40 nucliadb/train/api/v1/trainset.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/train/generators/__init__.py
--rw-r--r--  2.0 unx     3289 b- defN 23-May-15 13:40 nucliadb/train/generators/field_classifier.py
--rw-r--r--  2.0 unx     3594 b- defN 23-May-15 13:40 nucliadb/train/generators/paragraph_classifier.py
--rw-r--r--  2.0 unx     4767 b- defN 23-May-15 13:40 nucliadb/train/generators/sentence_classifier.py
--rw-r--r--  2.0 unx    10256 b- defN 23-May-15 13:40 nucliadb/train/generators/token_classifier.py
--rw-r--r--  2.0 unx     2092 b- defN 23-May-15 13:40 nucliadb/train/generators/utils.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/train/tests/__init__.py
--rw-r--r--  2.0 unx     1088 b- defN 23-May-15 13:40 nucliadb/train/tests/conftest.py
--rw-r--r--  2.0 unx     8939 b- defN 23-May-15 13:40 nucliadb/train/tests/fixtures.py
--rw-r--r--  2.0 unx     8132 b- defN 23-May-15 13:40 nucliadb/train/tests/test_field_classification.py
--rw-r--r--  2.0 unx     2729 b- defN 23-May-15 13:40 nucliadb/train/tests/test_get_entities.py
--rw-r--r--  2.0 unx     1660 b- defN 23-May-15 13:40 nucliadb/train/tests/test_get_info.py
--rw-r--r--  2.0 unx     1382 b- defN 23-May-15 13:40 nucliadb/train/tests/test_get_ontology.py
--rw-r--r--  2.0 unx     2201 b- defN 23-May-15 13:40 nucliadb/train/tests/test_get_ontology_count.py
--rw-r--r--  2.0 unx     1416 b- defN 23-May-15 13:40 nucliadb/train/tests/test_list_fields.py
--rw-r--r--  2.0 unx     2944 b- defN 23-May-15 13:40 nucliadb/train/tests/test_list_paragraphs.py
--rw-r--r--  2.0 unx     1423 b- defN 23-May-15 13:40 nucliadb/train/tests/test_list_resources.py
--rw-r--r--  2.0 unx     2863 b- defN 23-May-15 13:40 nucliadb/train/tests/test_list_sentences.py
--rw-r--r--  2.0 unx     8129 b- defN 23-May-15 13:40 nucliadb/train/tests/test_paragraph_classification.py
--rw-r--r--  2.0 unx     8349 b- defN 23-May-15 13:40 nucliadb/train/tests/test_sentence_classification.py
--rw-r--r--  2.0 unx    10122 b- defN 23-May-15 13:40 nucliadb/train/tests/test_token_classification.py
--rw-r--r--  2.0 unx     1328 b- defN 23-May-15 13:40 nucliadb/writer/__init__.py
--rw-r--r--  2.0 unx     3331 b- defN 23-May-15 13:40 nucliadb/writer/app.py
--rw-r--r--  2.0 unx      972 b- defN 23-May-15 13:40 nucliadb/writer/exceptions.py
--rw-r--r--  2.0 unx     2219 b- defN 23-May-15 13:40 nucliadb/writer/lifecycle.py
--rw-r--r--  2.0 unx     2186 b- defN 23-May-15 13:40 nucliadb/writer/openapi.py
--rw-r--r--  2.0 unx     1366 b- defN 23-May-15 13:40 nucliadb/writer/run.py
--rw-r--r--  2.0 unx     1062 b- defN 23-May-15 13:40 nucliadb/writer/settings.py
--rw-r--r--  2.0 unx     1036 b- defN 23-May-15 13:40 nucliadb/writer/utilities.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/writer/api/__init__.py
--rw-r--r--  2.0 unx     1021 b- defN 23-May-15 13:40 nucliadb/writer/api/v1/__init__.py
--rw-r--r--  2.0 unx    22261 b- defN 23-May-15 13:40 nucliadb/writer/api/v1/field.py
--rw-r--r--  2.0 unx     5395 b- defN 23-May-15 13:40 nucliadb/writer/api/v1/knowledgebox.py
--rw-r--r--  2.0 unx    15201 b- defN 23-May-15 13:40 nucliadb/writer/api/v1/resource.py
--rw-r--r--  2.0 unx     1034 b- defN 23-May-15 13:40 nucliadb/writer/api/v1/router.py
--rw-r--r--  2.0 unx    14210 b- defN 23-May-15 13:40 nucliadb/writer/api/v1/services.py
--rw-r--r--  2.0 unx    25744 b- defN 23-May-15 13:40 nucliadb/writer/api/v1/upload.py
--rw-r--r--  2.0 unx     1618 b- defN 23-May-15 13:40 nucliadb/writer/layouts/__init__.py
--rw-r--r--  2.0 unx     2115 b- defN 23-May-15 13:40 nucliadb/writer/layouts/v1.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/writer/resource/__init__.py
--rw-r--r--  2.0 unx     1425 b- defN 23-May-15 13:40 nucliadb/writer/resource/audit.py
--rw-r--r--  2.0 unx     9324 b- defN 23-May-15 13:40 nucliadb/writer/resource/basic.py
--rw-r--r--  2.0 unx    15875 b- defN 23-May-15 13:40 nucliadb/writer/resource/field.py
--rw-r--r--  2.0 unx     1708 b- defN 23-May-15 13:40 nucliadb/writer/resource/origin.py
--rw-r--r--  2.0 unx     1151 b- defN 23-May-15 13:40 nucliadb/writer/resource/slug.py
--rw-r--r--  2.0 unx     4991 b- defN 23-May-15 13:40 nucliadb/writer/resource/vectors.py
--rw-r--r--  2.0 unx      835 b- defN 23-May-15 13:40 nucliadb/writer/tests/__init__.py
--rw-r--r--  2.0 unx     1144 b- defN 23-May-15 13:40 nucliadb/writer/tests/conftest.py
--rw-r--r--  2.0 unx     4204 b- defN 23-May-15 13:40 nucliadb/writer/tests/fixtures.py
--rw-r--r--  2.0 unx    15962 b- defN 23-May-15 13:40 nucliadb/writer/tests/test_fields.py
--rw-r--r--  2.0 unx    22630 b- defN 23-May-15 13:40 nucliadb/writer/tests/test_files.py
--rw-r--r--  2.0 unx     1932 b- defN 23-May-15 13:40 nucliadb/writer/tests/test_knowledgebox.py
--rw-r--r--  2.0 unx     4623 b- defN 23-May-15 13:40 nucliadb/writer/tests/test_reprocess_file_field.py
--rw-r--r--  2.0 unx    19059 b- defN 23-May-15 13:40 nucliadb/writer/tests/test_resources.py
--rw-r--r--  2.0 unx     5345 b- defN 23-May-15 13:40 nucliadb/writer/tests/test_service.py
--rw-r--r--  2.0 unx     3596 b- defN 23-May-15 13:40 nucliadb/writer/tests/test_tus.py
--rw-r--r--  2.0 unx     2028 b- defN 23-May-15 13:40 nucliadb/writer/tests/tus.py
--rw-r--r--  2.0 unx     1287 b- defN 23-May-15 13:40 nucliadb/writer/tests/utils.py
--rw-r--r--  2.0 unx     4772 b- defN 23-May-15 13:40 nucliadb/writer/tus/__init__.py
--rw-r--r--  2.0 unx     4757 b- defN 23-May-15 13:40 nucliadb/writer/tus/dm.py
--rw-r--r--  2.0 unx     2186 b- defN 23-May-15 13:40 nucliadb/writer/tus/exceptions.py
--rw-r--r--  2.0 unx    13818 b- defN 23-May-15 13:40 nucliadb/writer/tus/gcs.py
--rw-r--r--  2.0 unx     5811 b- defN 23-May-15 13:40 nucliadb/writer/tus/local.py
--rw-r--r--  2.0 unx     8571 b- defN 23-May-15 13:40 nucliadb/writer/tus/s3.py
--rw-r--r--  2.0 unx     4682 b- defN 23-May-15 13:40 nucliadb/writer/tus/storage.py
--rw-r--r--  2.0 unx     2580 b- defN 23-May-15 13:40 nucliadb/writer/tus/utils.py
--rw-r--r--  2.0 unx     2913 b- defN 23-May-15 13:42 nucliadb-2.9.0.post267.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-15 13:42 nucliadb-2.9.0.post267.dist-info/WHEEL
--rw-r--r--  2.0 unx      870 b- defN 23-May-15 13:42 nucliadb-2.9.0.post267.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        9 b- defN 23-May-15 13:42 nucliadb-2.9.0.post267.dist-info/top_level.txt
--rw-r--r--  2.0 unx        1 b- defN 23-May-15 13:42 nucliadb-2.9.0.post267.dist-info/zip-safe
--rw-rw-r--  2.0 unx    26698 b- defN 23-May-15 13:42 nucliadb-2.9.0.post267.dist-info/RECORD
-292 files, 1461176 bytes uncompressed, 440464 bytes compressed:  69.9%
+Zip file size: 745240 bytes, number of entries: 451
+-rw-r--r--  2.0 unx     1135 b- defN 24-Apr-10 13:42 migrations/0001_bootstrap.py
+-rw-r--r--  2.0 unx     1177 b- defN 24-Apr-10 13:42 migrations/0002_rollover_shards.py
+-rw-r--r--  2.0 unx     2436 b- defN 24-Apr-10 13:42 migrations/0003_allfields_key.py
+-rw-r--r--  2.0 unx     1177 b- defN 24-Apr-10 13:42 migrations/0004_rollover_shards.py
+-rw-r--r--  2.0 unx     1177 b- defN 24-Apr-10 13:42 migrations/0005_rollover_shards.py
+-rw-r--r--  2.0 unx     1024 b- defN 24-Apr-10 13:42 migrations/0006_rollover_shards.py
+-rw-r--r--  2.0 unx     1301 b- defN 24-Apr-10 13:42 migrations/0008_cleanup_leftover_rollover_metadata.py
+-rw-r--r--  2.0 unx     1378 b- defN 24-Apr-10 13:42 migrations/0009_upgrade_relations_and_texts_to_v2.py
+-rw-r--r--  2.0 unx     1610 b- defN 24-Apr-10 13:42 migrations/0010_fix_corrupt_indexes.py
+-rw-r--r--  2.0 unx     1843 b- defN 24-Apr-10 13:42 migrations/0011_materialize_labelset_ids.py
+-rw-r--r--  2.0 unx     1443 b- defN 24-Apr-10 13:42 migrations/0012_rollover_shards.py
+-rw-r--r--  2.0 unx     1024 b- defN 24-Apr-10 13:42 migrations/0013_rollover_shards.py
+-rw-r--r--  2.0 unx     1382 b- defN 24-Apr-10 13:42 migrations/0014_rollover_shards.py
+-rw-r--r--  2.0 unx     1462 b- defN 24-Apr-10 13:42 migrations/0015_targeted_rollover.py
+-rw-r--r--  2.0 unx     2506 b- defN 24-Apr-10 13:42 migrations/0016_upgrade_to_paragraphs_v2.py
+-rw-r--r--  2.0 unx     2100 b- defN 24-Apr-10 13:42 migrations/0017_multiple_writable_shards.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 migrations/__init__.py
+-rw-r--r--  2.0 unx      891 b- defN 24-Apr-10 13:42 nucliadb/__init__.py
+-rw-r--r--  2.0 unx     3733 b- defN 24-Apr-10 13:42 nucliadb/health.py
+-rw-r--r--  2.0 unx    10590 b- defN 24-Apr-10 13:42 nucliadb/learning_proxy.py
+-rw-r--r--  2.0 unx     4806 b- defN 24-Apr-10 13:42 nucliadb/metrics_exporter.py
+-rw-r--r--  2.0 unx     2272 b- defN 24-Apr-10 13:42 nucliadb/openapi.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Apr-10 13:42 nucliadb/py.typed
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/common/__init__.py
+-rw-r--r--  2.0 unx     4905 b- defN 24-Apr-10 13:42 nucliadb/common/locking.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/__init__.py
+-rw-r--r--  2.0 unx     4983 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/base.py
+-rw-r--r--  2.0 unx     1495 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/exceptions.py
+-rw-r--r--  2.0 unx     3658 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/grpc_node_dummy.py
+-rw-r--r--  2.0 unx     3429 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/index_node.py
+-rw-r--r--  2.0 unx    21640 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/manager.py
+-rw-r--r--  2.0 unx     8735 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/rebalance.py
+-rw-r--r--  2.0 unx    19449 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/rollover.py
+-rw-r--r--  2.0 unx     2713 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/settings.py
+-rw-r--r--  2.0 unx     5494 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/discovery/__init__.py
+-rw-r--r--  2.0 unx     6553 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/discovery/base.py
+-rw-r--r--  2.0 unx    12518 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/discovery/k8s.py
+-rw-r--r--  2.0 unx     1957 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/discovery/manual.py
+-rw-r--r--  2.0 unx     1737 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/discovery/single.py
+-rw-r--r--  2.0 unx     1139 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/discovery/types.py
+-rw-r--r--  2.0 unx     2548 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/discovery/utils.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/standalone/__init__.py
+-rw-r--r--  2.0 unx    13705 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/standalone/grpc_node_binding.py
+-rw-r--r--  2.0 unx     4683 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/standalone/index_node.py
+-rw-r--r--  2.0 unx     3444 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/standalone/service.py
+-rw-r--r--  2.0 unx     3386 b- defN 24-Apr-10 13:42 nucliadb/common/cluster/standalone/utils.py
+-rw-r--r--  2.0 unx     3498 b- defN 24-Apr-10 13:42 nucliadb/common/context/__init__.py
+-rw-r--r--  2.0 unx     1628 b- defN 24-Apr-10 13:42 nucliadb/common/context/fastapi.py
+-rw-r--r--  2.0 unx     1813 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/__init__.py
+-rw-r--r--  2.0 unx     1451 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/cluster.py
+-rw-r--r--  2.0 unx     5383 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/entities.py
+-rw-r--r--  2.0 unx      883 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/exceptions.py
+-rw-r--r--  2.0 unx     2940 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/kb.py
+-rw-r--r--  2.0 unx     5389 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/labels.py
+-rw-r--r--  2.0 unx     1599 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/processing.py
+-rw-r--r--  2.0 unx     7637 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/resources.py
+-rw-r--r--  2.0 unx     5633 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/rollover.py
+-rw-r--r--  2.0 unx     1681 b- defN 24-Apr-10 13:42 nucliadb/common/datamanagers/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/common/http_clients/__init__.py
+-rw-r--r--  2.0 unx     2146 b- defN 24-Apr-10 13:42 nucliadb/common/http_clients/auth.py
+-rw-r--r--  2.0 unx     1100 b- defN 24-Apr-10 13:42 nucliadb/common/http_clients/exceptions.py
+-rw-r--r--  2.0 unx     7155 b- defN 24-Apr-10 13:42 nucliadb/common/http_clients/processing.py
+-rw-r--r--  2.0 unx     1540 b- defN 24-Apr-10 13:42 nucliadb/common/http_clients/pypi.py
+-rw-r--r--  2.0 unx     1551 b- defN 24-Apr-10 13:42 nucliadb/common/http_clients/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/common/maindb/__init__.py
+-rw-r--r--  2.0 unx     3449 b- defN 24-Apr-10 13:42 nucliadb/common/maindb/driver.py
+-rw-r--r--  2.0 unx      946 b- defN 24-Apr-10 13:42 nucliadb/common/maindb/exceptions.py
+-rw-r--r--  2.0 unx     6769 b- defN 24-Apr-10 13:42 nucliadb/common/maindb/local.py
+-rw-r--r--  2.0 unx     8391 b- defN 24-Apr-10 13:42 nucliadb/common/maindb/pg.py
+-rw-r--r--  2.0 unx     6053 b- defN 24-Apr-10 13:42 nucliadb/common/maindb/redis.py
+-rw-r--r--  2.0 unx    14502 b- defN 24-Apr-10 13:42 nucliadb/common/maindb/tikv.py
+-rw-r--r--  2.0 unx     4109 b- defN 24-Apr-10 13:42 nucliadb/common/maindb/utils.py
+-rw-r--r--  2.0 unx      932 b- defN 24-Apr-10 13:42 nucliadb/export_import/__init__.py
+-rw-r--r--  2.0 unx     6171 b- defN 24-Apr-10 13:42 nucliadb/export_import/datamanager.py
+-rw-r--r--  2.0 unx     1780 b- defN 24-Apr-10 13:42 nucliadb/export_import/exceptions.py
+-rw-r--r--  2.0 unx     6570 b- defN 24-Apr-10 13:42 nucliadb/export_import/exporter.py
+-rw-r--r--  2.0 unx     4333 b- defN 24-Apr-10 13:42 nucliadb/export_import/importer.py
+-rw-r--r--  2.0 unx     2035 b- defN 24-Apr-10 13:42 nucliadb/export_import/models.py
+-rw-r--r--  2.0 unx     2571 b- defN 24-Apr-10 13:42 nucliadb/export_import/tasks.py
+-rw-r--r--  2.0 unx    16878 b- defN 24-Apr-10 13:42 nucliadb/export_import/utils.py
+-rw-r--r--  2.0 unx     1011 b- defN 24-Apr-10 13:42 nucliadb/ingest/__init__.py
+-rw-r--r--  2.0 unx     7277 b- defN 24-Apr-10 13:42 nucliadb/ingest/app.py
+-rw-r--r--  2.0 unx     1005 b- defN 24-Apr-10 13:42 nucliadb/ingest/cache.py
+-rw-r--r--  2.0 unx     2484 b- defN 24-Apr-10 13:42 nucliadb/ingest/partitions.py
+-rw-r--r--  2.0 unx    19541 b- defN 24-Apr-10 13:42 nucliadb/ingest/processing.py
+-rw-r--r--  2.0 unx    20277 b- defN 24-Apr-10 13:42 nucliadb/ingest/serialize.py
+-rw-r--r--  2.0 unx     3183 b- defN 24-Apr-10 13:42 nucliadb/ingest/settings.py
+-rw-r--r--  2.0 unx     2314 b- defN 24-Apr-10 13:42 nucliadb/ingest/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/ingest/consumer/__init__.py
+-rw-r--r--  2.0 unx    11563 b- defN 24-Apr-10 13:42 nucliadb/ingest/consumer/auditing.py
+-rw-r--r--  2.0 unx    11992 b- defN 24-Apr-10 13:42 nucliadb/ingest/consumer/consumer.py
+-rw-r--r--  2.0 unx     3788 b- defN 24-Apr-10 13:42 nucliadb/ingest/consumer/materializer.py
+-rw-r--r--  2.0 unx     1075 b- defN 24-Apr-10 13:42 nucliadb/ingest/consumer/metrics.py
+-rw-r--r--  2.0 unx     9543 b- defN 24-Apr-10 13:42 nucliadb/ingest/consumer/pull.py
+-rw-r--r--  2.0 unx     6935 b- defN 24-Apr-10 13:42 nucliadb/ingest/consumer/service.py
+-rw-r--r--  2.0 unx     4509 b- defN 24-Apr-10 13:42 nucliadb/ingest/consumer/shard_creator.py
+-rw-r--r--  2.0 unx     2656 b- defN 24-Apr-10 13:42 nucliadb/ingest/consumer/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/__init__.py
+-rw-r--r--  2.0 unx    18433 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/base.py
+-rw-r--r--  2.0 unx     6516 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/conversation.py
+-rw-r--r--  2.0 unx     1223 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/date.py
+-rw-r--r--  2.0 unx     1205 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/exceptions.py
+-rw-r--r--  2.0 unx     5159 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/file.py
+-rw-r--r--  2.0 unx     1547 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/generic.py
+-rw-r--r--  2.0 unx     1235 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/keywordset.py
+-rw-r--r--  2.0 unx     2250 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/layout.py
+-rw-r--r--  2.0 unx     4084 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/link.py
+-rw-r--r--  2.0 unx     1319 b- defN 24-Apr-10 13:42 nucliadb/ingest/fields/text.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/__init__.py
+-rw-r--r--  2.0 unx    27582 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/brain.py
+-rw-r--r--  2.0 unx    15758 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/entities.py
+-rw-r--r--  2.0 unx     1279 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/exceptions.py
+-rw-r--r--  2.0 unx    18514 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/knowledgebox.py
+-rw-r--r--  2.0 unx     1096 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/metrics.py
+-rw-r--r--  2.0 unx    60085 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/resource.py
+-rw-r--r--  2.0 unx     1739 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/synonyms.py
+-rw-r--r--  2.0 unx     3074 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/utils.py
+-rw-r--r--  2.0 unx    26935 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/processor/__init__.py
+-rw-r--r--  2.0 unx     1690 b- defN 24-Apr-10 13:42 nucliadb/ingest/orm/processor/sequence_manager.py
+-rw-r--r--  2.0 unx     2057 b- defN 24-Apr-10 13:42 nucliadb/ingest/service/__init__.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/ingest/service/exceptions.py
+-rw-r--r--  2.0 unx    36881 b- defN 24-Apr-10 13:42 nucliadb/ingest/service/writer.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/__init__.py
+-rw-r--r--  2.0 unx     1157 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/conftest.py
+-rw-r--r--  2.0 unx    24068 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/fixtures.py
+-rw-r--r--  2.0 unx    62843 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/vectors.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/__init__.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/consumer/__init__.py
+-rw-r--r--  2.0 unx     2549 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/consumer/test_auditing.py
+-rw-r--r--  2.0 unx     2591 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/consumer/test_materializer.py
+-rw-r--r--  2.0 unx     4465 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/consumer/test_pull.py
+-rw-r--r--  2.0 unx     2763 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/consumer/test_service.py
+-rw-r--r--  2.0 unx     2019 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/consumer/test_shard_creator.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/ingest/__init__.py
+-rw-r--r--  2.0 unx    27334 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/ingest/test_ingest.py
+-rw-r--r--  2.0 unx     3040 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/ingest/test_processing_engine.py
+-rw-r--r--  2.0 unx     8586 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/integration/ingest/test_relations.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/__init__.py
+-rw-r--r--  2.0 unx     1189 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/test_cache.py
+-rw-r--r--  2.0 unx     1432 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/test_partitions.py
+-rw-r--r--  2.0 unx     5807 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/test_processing.py
+-rw-r--r--  2.0 unx      978 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/test_settings.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/consumer/__init__.py
+-rw-r--r--  2.0 unx     3981 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/consumer/test_auditing.py
+-rw-r--r--  2.0 unx     2472 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/consumer/test_consumer.py
+-rw-r--r--  2.0 unx     2063 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/consumer/test_pull.py
+-rw-r--r--  2.0 unx     4217 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/consumer/test_shard_creator.py
+-rw-r--r--  2.0 unx     1934 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/consumer/test_utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/orm/__init__.py
+-rw-r--r--  2.0 unx     9876 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/orm/test_brain.py
+-rw-r--r--  2.0 unx     4045 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/orm/test_processor.py
+-rw-r--r--  2.0 unx     8965 b- defN 24-Apr-10 13:42 nucliadb/ingest/tests/unit/orm/test_resource.py
+-rw-r--r--  2.0 unx     2216 b- defN 24-Apr-10 13:42 nucliadb/middleware/__init__.py
+-rw-r--r--  2.0 unx     3912 b- defN 24-Apr-10 13:42 nucliadb/middleware/transaction.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/migrator/__init__.py
+-rw-r--r--  2.0 unx     2119 b- defN 24-Apr-10 13:42 nucliadb/migrator/command.py
+-rw-r--r--  2.0 unx     1334 b- defN 24-Apr-10 13:42 nucliadb/migrator/context.py
+-rw-r--r--  2.0 unx     5104 b- defN 24-Apr-10 13:42 nucliadb/migrator/datamanager.py
+-rw-r--r--  2.0 unx      889 b- defN 24-Apr-10 13:42 nucliadb/migrator/exceptions.py
+-rw-r--r--  2.0 unx     9237 b- defN 24-Apr-10 13:42 nucliadb/migrator/migrator.py
+-rw-r--r--  2.0 unx     1145 b- defN 24-Apr-10 13:42 nucliadb/migrator/models.py
+-rw-r--r--  2.0 unx     1110 b- defN 24-Apr-10 13:42 nucliadb/migrator/settings.py
+-rw-r--r--  2.0 unx     2346 b- defN 24-Apr-10 13:42 nucliadb/migrator/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/models/__init__.py
+-rw-r--r--  2.0 unx     1599 b- defN 24-Apr-10 13:42 nucliadb/models/responses.py
+-rw-r--r--  2.0 unx     6041 b- defN 24-Apr-10 13:42 nucliadb/purge/__init__.py
+-rw-r--r--  2.0 unx     9364 b- defN 24-Apr-10 13:42 nucliadb/purge/orphan_shards.py
+-rw-r--r--  2.0 unx     1328 b- defN 24-Apr-10 13:42 nucliadb/reader/__init__.py
+-rw-r--r--  2.0 unx     4112 b- defN 24-Apr-10 13:42 nucliadb/reader/app.py
+-rw-r--r--  2.0 unx     1366 b- defN 24-Apr-10 13:42 nucliadb/reader/lifecycle.py
+-rw-r--r--  2.0 unx     1031 b- defN 24-Apr-10 13:42 nucliadb/reader/openapi.py
+-rw-r--r--  2.0 unx     1447 b- defN 24-Apr-10 13:42 nucliadb/reader/run.py
+-rw-r--r--  2.0 unx      872 b- defN 24-Apr-10 13:42 nucliadb/reader/api/__init__.py
+-rw-r--r--  2.0 unx     2434 b- defN 24-Apr-10 13:42 nucliadb/reader/api/models.py
+-rw-r--r--  2.0 unx     1110 b- defN 24-Apr-10 13:42 nucliadb/reader/api/v1/__init__.py
+-rw-r--r--  2.0 unx    12368 b- defN 24-Apr-10 13:42 nucliadb/reader/api/v1/download.py
+-rw-r--r--  2.0 unx     6450 b- defN 24-Apr-10 13:42 nucliadb/reader/api/v1/export_import.py
+-rw-r--r--  2.0 unx     3632 b- defN 24-Apr-10 13:42 nucliadb/reader/api/v1/knowledgebox.py
+-rw-r--r--  2.0 unx     2093 b- defN 24-Apr-10 13:42 nucliadb/reader/api/v1/learning_collector.py
+-rw-r--r--  2.0 unx     4454 b- defN 24-Apr-10 13:42 nucliadb/reader/api/v1/learning_config.py
+-rw-r--r--  2.0 unx    13928 b- defN 24-Apr-10 13:42 nucliadb/reader/api/v1/resource.py
+-rw-r--r--  2.0 unx     1011 b- defN 24-Apr-10 13:42 nucliadb/reader/api/v1/router.py
+-rw-r--r--  2.0 unx    13882 b- defN 24-Apr-10 13:42 nucliadb/reader/api/v1/services.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/reader/reader/__init__.py
+-rw-r--r--  2.0 unx     8155 b- defN 24-Apr-10 13:42 nucliadb/reader/reader/notifications.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/reader/tests/__init__.py
+-rw-r--r--  2.0 unx     1224 b- defN 24-Apr-10 13:42 nucliadb/reader/tests/conftest.py
+-rw-r--r--  2.0 unx     4345 b- defN 24-Apr-10 13:42 nucliadb/reader/tests/fixtures.py
+-rw-r--r--  2.0 unx     2748 b- defN 24-Apr-10 13:42 nucliadb/reader/tests/test_list_resources.py
+-rw-r--r--  2.0 unx    10199 b- defN 24-Apr-10 13:42 nucliadb/reader/tests/test_reader_file_download.py
+-rw-r--r--  2.0 unx    10586 b- defN 24-Apr-10 13:42 nucliadb/reader/tests/test_reader_resource.py
+-rw-r--r--  2.0 unx     6535 b- defN 24-Apr-10 13:42 nucliadb/reader/tests/test_reader_resource_field.py
+-rw-r--r--  2.0 unx     1535 b- defN 24-Apr-10 13:42 nucliadb/search/__init__.py
+-rw-r--r--  2.0 unx     5264 b- defN 24-Apr-10 13:42 nucliadb/search/app.py
+-rw-r--r--  2.0 unx     1956 b- defN 24-Apr-10 13:42 nucliadb/search/lifecycle.py
+-rw-r--r--  2.0 unx     1016 b- defN 24-Apr-10 13:42 nucliadb/search/openapi.py
+-rw-r--r--  2.0 unx    19798 b- defN 24-Apr-10 13:42 nucliadb/search/predict.py
+-rw-r--r--  2.0 unx     1402 b- defN 24-Apr-10 13:42 nucliadb/search/run.py
+-rw-r--r--  2.0 unx     1193 b- defN 24-Apr-10 13:42 nucliadb/search/settings.py
+-rw-r--r--  2.0 unx     1037 b- defN 24-Apr-10 13:42 nucliadb/search/utilities.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/search/api/__init__.py
+-rw-r--r--  2.0 unx     1272 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/__init__.py
+-rw-r--r--  2.0 unx     9287 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/chat.py
+-rw-r--r--  2.0 unx     2665 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/feedback.py
+-rw-r--r--  2.0 unx     8804 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/find.py
+-rw-r--r--  2.0 unx     7026 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/knowledgebox.py
+-rw-r--r--  2.0 unx     3041 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/predict_proxy.py
+-rw-r--r--  2.0 unx      958 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/router.py
+-rw-r--r--  2.0 unx    18359 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/search.py
+-rw-r--r--  2.0 unx     5928 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/suggest.py
+-rw-r--r--  2.0 unx     2325 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/summarize.py
+-rw-r--r--  2.0 unx     1412 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/resource/__init__.py
+-rw-r--r--  2.0 unx     6095 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/resource/ask.py
+-rw-r--r--  2.0 unx     3521 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/resource/chat.py
+-rw-r--r--  2.0 unx     5293 b- defN 24-Apr-10 13:42 nucliadb/search/api/v1/resource/search.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/search/requesters/__init__.py
+-rw-r--r--  2.0 unx     9070 b- defN 24-Apr-10 13:42 nucliadb/search/requesters/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/search/search/__init__.py
+-rw-r--r--  2.0 unx     2746 b- defN 24-Apr-10 13:42 nucliadb/search/search/cache.py
+-rw-r--r--  2.0 unx     1154 b- defN 24-Apr-10 13:42 nucliadb/search/search/exceptions.py
+-rw-r--r--  2.0 unx     5465 b- defN 24-Apr-10 13:42 nucliadb/search/search/fetch.py
+-rw-r--r--  2.0 unx     6513 b- defN 24-Apr-10 13:42 nucliadb/search/search/filters.py
+-rw-r--r--  2.0 unx     4646 b- defN 24-Apr-10 13:42 nucliadb/search/search/find.py
+-rw-r--r--  2.0 unx    17152 b- defN 24-Apr-10 13:42 nucliadb/search/search/find_merge.py
+-rw-r--r--  2.0 unx    21118 b- defN 24-Apr-10 13:42 nucliadb/search/search/merge.py
+-rw-r--r--  2.0 unx     1130 b- defN 24-Apr-10 13:42 nucliadb/search/search/metrics.py
+-rw-r--r--  2.0 unx     8698 b- defN 24-Apr-10 13:42 nucliadb/search/search/paragraphs.py
+-rw-r--r--  2.0 unx     3026 b- defN 24-Apr-10 13:42 nucliadb/search/search/predict_proxy.py
+-rw-r--r--  2.0 unx    31760 b- defN 24-Apr-10 13:42 nucliadb/search/search/query.py
+-rw-r--r--  2.0 unx     3149 b- defN 24-Apr-10 13:42 nucliadb/search/search/shards.py
+-rw-r--r--  2.0 unx     4965 b- defN 24-Apr-10 13:42 nucliadb/search/search/summarize.py
+-rw-r--r--  2.0 unx     2438 b- defN 24-Apr-10 13:42 nucliadb/search/search/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/search/search/chat/__init__.py
+-rw-r--r--  2.0 unx     2058 b- defN 24-Apr-10 13:42 nucliadb/search/search/chat/images.py
+-rw-r--r--  2.0 unx    20039 b- defN 24-Apr-10 13:42 nucliadb/search/search/chat/prompt.py
+-rw-r--r--  2.0 unx    13853 b- defN 24-Apr-10 13:42 nucliadb/search/search/chat/query.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/search/tests/__init__.py
+-rw-r--r--  2.0 unx     1295 b- defN 24-Apr-10 13:42 nucliadb/search/tests/conftest.py
+-rw-r--r--  2.0 unx     6578 b- defN 24-Apr-10 13:42 nucliadb/search/tests/fixtures.py
+-rw-r--r--  2.0 unx    15529 b- defN 24-Apr-10 13:42 nucliadb/search/tests/node.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/__init__.py
+-rw-r--r--  2.0 unx     2649 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/test_app.py
+-rw-r--r--  2.0 unx     3374 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/test_find_merge.py
+-rw-r--r--  2.0 unx     1400 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/test_merge.py
+-rw-r--r--  2.0 unx    17080 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/test_predict.py
+-rw-r--r--  2.0 unx     1317 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/test_run.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/api/__init__.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/api/v1/__init__.py
+-rw-r--r--  2.0 unx     2966 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/api/v1/test_chat.py
+-rw-r--r--  2.0 unx     2865 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/api/v1/test_predict_proxy.py
+-rw-r--r--  2.0 unx     2636 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/api/v1/test_summarize.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/api/v1/resource/__init__.py
+-rw-r--r--  2.0 unx     2411 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/api/v1/resource/test_ask.py
+-rw-r--r--  2.0 unx     3004 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/api/v1/resource/test_chat.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/__init__.py
+-rw-r--r--  2.0 unx     8547 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/test_chat_prompt.py
+-rw-r--r--  2.0 unx     3736 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/test_fetch.py
+-rw-r--r--  2.0 unx     4306 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/test_filters.py
+-rw-r--r--  2.0 unx     4492 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/test_paragraphs.py
+-rw-r--r--  2.0 unx     3496 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/test_predict_proxy.py
+-rw-r--r--  2.0 unx     7001 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/test_query.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/requesters/__init__.py
+-rw-r--r--  2.0 unx     6455 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/requesters/test_utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/search/__init__.py
+-rw-r--r--  2.0 unx     1759 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/search/test_shards.py
+-rw-r--r--  2.0 unx     2511 b- defN 24-Apr-10 13:42 nucliadb/search/tests/unit/search/search/test_utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/standalone/__init__.py
+-rw-r--r--  2.0 unx     6746 b- defN 24-Apr-10 13:42 nucliadb/standalone/api_router.py
+-rw-r--r--  2.0 unx     6183 b- defN 24-Apr-10 13:42 nucliadb/standalone/app.py
+-rw-r--r--  2.0 unx     7800 b- defN 24-Apr-10 13:42 nucliadb/standalone/auth.py
+-rw-r--r--  2.0 unx     5309 b- defN 24-Apr-10 13:42 nucliadb/standalone/config.py
+-rw-r--r--  2.0 unx     6930 b- defN 24-Apr-10 13:42 nucliadb/standalone/introspect.py
+-rw-r--r--  2.0 unx     2488 b- defN 24-Apr-10 13:42 nucliadb/standalone/lifecycle.py
+-rw-r--r--  2.0 unx     1317 b- defN 24-Apr-10 13:42 nucliadb/standalone/purge.py
+-rw-r--r--  2.0 unx     5336 b- defN 24-Apr-10 13:42 nucliadb/standalone/run.py
+-rw-r--r--  2.0 unx     5781 b- defN 24-Apr-10 13:42 nucliadb/standalone/settings.py
+-rw-r--r--  2.0 unx     3132 b- defN 24-Apr-10 13:42 nucliadb/standalone/versions.py
+-rw-r--r--  2.0 unx     2285 b- defN 24-Apr-10 13:42 nucliadb/standalone/static/favicon.ico
+-rw-r--r--  2.0 unx     3833 b- defN 24-Apr-10 13:42 nucliadb/standalone/static/index.html
+-rw-r--r--  2.0 unx     2639 b- defN 24-Apr-10 13:42 nucliadb/standalone/static/logo.svg
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/standalone/tests/__init__.py
+-rw-r--r--  2.0 unx     1294 b- defN 24-Apr-10 13:42 nucliadb/standalone/tests/conftest.py
+-rw-r--r--  2.0 unx     1319 b- defN 24-Apr-10 13:42 nucliadb/standalone/tests/fixtures.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/standalone/tests/unit/__init__.py
+-rw-r--r--  2.0 unx     1722 b- defN 24-Apr-10 13:42 nucliadb/standalone/tests/unit/test_api_router.py
+-rw-r--r--  2.0 unx     5509 b- defN 24-Apr-10 13:42 nucliadb/standalone/tests/unit/test_auth.py
+-rw-r--r--  2.0 unx     1334 b- defN 24-Apr-10 13:42 nucliadb/standalone/tests/unit/test_introspect.py
+-rw-r--r--  2.0 unx     1472 b- defN 24-Apr-10 13:42 nucliadb/standalone/tests/unit/test_run.py
+-rw-r--r--  2.0 unx     1972 b- defN 24-Apr-10 13:42 nucliadb/standalone/tests/unit/test_versions.py
+-rw-r--r--  2.0 unx     1037 b- defN 24-Apr-10 13:42 nucliadb/tasks/__init__.py
+-rw-r--r--  2.0 unx     7655 b- defN 24-Apr-10 13:42 nucliadb/tasks/consumer.py
+-rw-r--r--  2.0 unx      924 b- defN 24-Apr-10 13:42 nucliadb/tasks/logger.py
+-rw-r--r--  2.0 unx     1378 b- defN 24-Apr-10 13:42 nucliadb/tasks/models.py
+-rw-r--r--  2.0 unx     3457 b- defN 24-Apr-10 13:42 nucliadb/tasks/producer.py
+-rw-r--r--  2.0 unx     1753 b- defN 24-Apr-10 13:42 nucliadb/tasks/registry.py
+-rw-r--r--  2.0 unx     1360 b- defN 24-Apr-10 13:42 nucliadb/tasks/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/__init__.py
+-rw-r--r--  2.0 unx     1229 b- defN 24-Apr-10 13:42 nucliadb/tests/conftest.py
+-rw-r--r--  2.0 unx    22365 b- defN 24-Apr-10 13:42 nucliadb/tests/fixtures.py
+-rw-r--r--  2.0 unx     7549 b- defN 24-Apr-10 13:42 nucliadb/tests/tikv.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/benchmarks/__init__.py
+-rw-r--r--  2.0 unx     3032 b- defN 24-Apr-10 13:42 nucliadb/tests/benchmarks/test_search.py
+-rw-r--r--  2.0 unx      919 b- defN 24-Apr-10 13:42 nucliadb/tests/knowledgeboxes/__init__.py
+-rw-r--r--  2.0 unx     7002 b- defN 24-Apr-10 13:42 nucliadb/tests/knowledgeboxes/philosophy_books.py
+-rw-r--r--  2.0 unx     3037 b- defN 24-Apr-10 13:42 nucliadb/tests/knowledgeboxes/ten_dummy_resources.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/migrations/__init__.py
+-rw-r--r--  2.0 unx     2878 b- defN 24-Apr-10 13:42 nucliadb/tests/migrations/test_migration_0017.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/__init__.py
+-rw-r--r--  2.0 unx     1487 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/test_field_ids.py
+-rw-r--r--  2.0 unx     2758 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/test_health.py
+-rw-r--r--  2.0 unx     1543 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/test_kb_slugs.py
+-rw-r--r--  2.0 unx     7892 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/test_learning_proxy.py
+-rw-r--r--  2.0 unx     2642 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/test_metrics_exporter.py
+-rw-r--r--  2.0 unx     1341 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/test_openapi.py
+-rw-r--r--  2.0 unx     3656 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/test_purge.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/__init__.py
+-rw-r--r--  2.0 unx     1268 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/test_context.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/cluster/__init__.py
+-rw-r--r--  2.0 unx    12036 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/cluster/test_cluster.py
+-rw-r--r--  2.0 unx     5441 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/cluster/test_kb_shard_manager.py
+-rw-r--r--  2.0 unx     9470 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/cluster/test_rollover.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/cluster/discovery/__init__.py
+-rw-r--r--  2.0 unx     5584 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/cluster/discovery/test_k8s.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/cluster/standalone/__init__.py
+-rw-r--r--  2.0 unx     3750 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/cluster/standalone/test_service.py
+-rw-r--r--  2.0 unx     2114 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/cluster/standalone/test_utils.py
+-rw-r--r--  2.0 unx      833 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/maindb/__init__.py
+-rw-r--r--  2.0 unx     3579 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/maindb/test_driver.py
+-rw-r--r--  2.0 unx     1869 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/maindb/test_tikv.py
+-rw-r--r--  2.0 unx     2998 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/common/maindb/test_utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/export_import/__init__.py
+-rw-r--r--  2.0 unx     1435 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/export_import/test_datamanager.py
+-rw-r--r--  2.0 unx     9632 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/export_import/test_utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/migrator/__init__.py
+-rw-r--r--  2.0 unx     3233 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/migrator/test_migrator.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/tasks/__init__.py
+-rw-r--r--  2.0 unx     1375 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/tasks/conftest.py
+-rw-r--r--  2.0 unx     2714 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/tasks/test_consumer.py
+-rw-r--r--  2.0 unx     3189 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/tasks/test_producer.py
+-rw-r--r--  2.0 unx     1827 b- defN 24-Apr-10 13:42 nucliadb/tests/unit/tasks/test_tasks.py
+-rw-r--r--  2.0 unx     2507 b- defN 24-Apr-10 13:42 nucliadb/tests/utils/__init__.py
+-rw-r--r--  2.0 unx     1797 b- defN 24-Apr-10 13:42 nucliadb/tests/utils/aiohttp_session.py
+-rw-r--r--  2.0 unx     2533 b- defN 24-Apr-10 13:42 nucliadb/tests/utils/entities.py
+-rw-r--r--  2.0 unx     6145 b- defN 24-Apr-10 13:42 nucliadb/tests/utils/broker_messages/__init__.py
+-rw-r--r--  2.0 unx     6795 b- defN 24-Apr-10 13:42 nucliadb/tests/utils/broker_messages/fields.py
+-rw-r--r--  2.0 unx     1196 b- defN 24-Apr-10 13:42 nucliadb/tests/utils/broker_messages/helpers.py
+-rw-r--r--  2.0 unx     1325 b- defN 24-Apr-10 13:42 nucliadb/train/__init__.py
+-rw-r--r--  2.0 unx     3932 b- defN 24-Apr-10 13:42 nucliadb/train/app.py
+-rw-r--r--  2.0 unx     3800 b- defN 24-Apr-10 13:42 nucliadb/train/generator.py
+-rw-r--r--  2.0 unx     1698 b- defN 24-Apr-10 13:42 nucliadb/train/lifecycle.py
+-rw-r--r--  2.0 unx     1198 b- defN 24-Apr-10 13:42 nucliadb/train/models.py
+-rw-r--r--  2.0 unx     5706 b- defN 24-Apr-10 13:42 nucliadb/train/nodes.py
+-rw-r--r--  2.0 unx     1400 b- defN 24-Apr-10 13:42 nucliadb/train/run.py
+-rw-r--r--  2.0 unx     5926 b- defN 24-Apr-10 13:42 nucliadb/train/servicer.py
+-rw-r--r--  2.0 unx     1415 b- defN 24-Apr-10 13:42 nucliadb/train/settings.py
+-rw-r--r--  2.0 unx     1496 b- defN 24-Apr-10 13:42 nucliadb/train/types.py
+-rw-r--r--  2.0 unx     3265 b- defN 24-Apr-10 13:42 nucliadb/train/upload.py
+-rw-r--r--  2.0 unx     6420 b- defN 24-Apr-10 13:42 nucliadb/train/uploader.py
+-rw-r--r--  2.0 unx     3179 b- defN 24-Apr-10 13:42 nucliadb/train/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/train/api/__init__.py
+-rw-r--r--  2.0 unx     1479 b- defN 24-Apr-10 13:42 nucliadb/train/api/utils.py
+-rw-r--r--  2.0 unx      928 b- defN 24-Apr-10 13:42 nucliadb/train/api/v1/__init__.py
+-rw-r--r--  2.0 unx     2065 b- defN 24-Apr-10 13:42 nucliadb/train/api/v1/check.py
+-rw-r--r--  2.0 unx      910 b- defN 24-Apr-10 13:42 nucliadb/train/api/v1/router.py
+-rw-r--r--  2.0 unx     1905 b- defN 24-Apr-10 13:42 nucliadb/train/api/v1/shards.py
+-rw-r--r--  2.0 unx     2129 b- defN 24-Apr-10 13:42 nucliadb/train/api/v1/trainset.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/train/generators/__init__.py
+-rw-r--r--  2.0 unx     3723 b- defN 24-Apr-10 13:42 nucliadb/train/generators/field_classifier.py
+-rw-r--r--  2.0 unx     6712 b- defN 24-Apr-10 13:42 nucliadb/train/generators/image_classifier.py
+-rw-r--r--  2.0 unx     2789 b- defN 24-Apr-10 13:42 nucliadb/train/generators/paragraph_classifier.py
+-rw-r--r--  2.0 unx     3590 b- defN 24-Apr-10 13:42 nucliadb/train/generators/paragraph_streaming.py
+-rw-r--r--  2.0 unx     5372 b- defN 24-Apr-10 13:42 nucliadb/train/generators/question_answer_streaming.py
+-rw-r--r--  2.0 unx     5160 b- defN 24-Apr-10 13:42 nucliadb/train/generators/sentence_classifier.py
+-rw-r--r--  2.0 unx    10446 b- defN 24-Apr-10 13:42 nucliadb/train/generators/token_classifier.py
+-rw-r--r--  2.0 unx     3877 b- defN 24-Apr-10 13:42 nucliadb/train/generators/utils.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/train/tests/__init__.py
+-rw-r--r--  2.0 unx     1125 b- defN 24-Apr-10 13:42 nucliadb/train/tests/conftest.py
+-rw-r--r--  2.0 unx    11053 b- defN 24-Apr-10 13:42 nucliadb/train/tests/fixtures.py
+-rw-r--r--  2.0 unx     4598 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_field_classification.py
+-rw-r--r--  2.0 unx     2729 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_get_entities.py
+-rw-r--r--  2.0 unx     1876 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_get_info.py
+-rw-r--r--  2.0 unx     1382 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_get_ontology.py
+-rw-r--r--  2.0 unx     2417 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_get_ontology_count.py
+-rw-r--r--  2.0 unx     7669 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_image_classification.py
+-rw-r--r--  2.0 unx     1416 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_list_fields.py
+-rw-r--r--  2.0 unx     2944 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_list_paragraphs.py
+-rw-r--r--  2.0 unx     1423 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_list_resources.py
+-rw-r--r--  2.0 unx     2947 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_list_sentences.py
+-rw-r--r--  2.0 unx     4645 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_paragraph_classification.py
+-rw-r--r--  2.0 unx     4320 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_paragraph_streaming.py
+-rw-r--r--  2.0 unx     8751 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_question_answer_streaming.py
+-rw-r--r--  2.0 unx     5051 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_sentence_classification.py
+-rw-r--r--  2.0 unx     5583 b- defN 24-Apr-10 13:42 nucliadb/train/tests/test_token_classification.py
+-rw-r--r--  2.0 unx     3324 b- defN 24-Apr-10 13:42 nucliadb/train/tests/utils.py
+-rw-r--r--  2.0 unx     1328 b- defN 24-Apr-10 13:42 nucliadb/writer/__init__.py
+-rw-r--r--  2.0 unx     4680 b- defN 24-Apr-10 13:42 nucliadb/writer/app.py
+-rw-r--r--  2.0 unx    18537 b- defN 24-Apr-10 13:42 nucliadb/writer/back_pressure.py
+-rw-r--r--  2.0 unx      972 b- defN 24-Apr-10 13:42 nucliadb/writer/exceptions.py
+-rw-r--r--  2.0 unx     1953 b- defN 24-Apr-10 13:42 nucliadb/writer/lifecycle.py
+-rw-r--r--  2.0 unx     1032 b- defN 24-Apr-10 13:42 nucliadb/writer/openapi.py
+-rw-r--r--  2.0 unx     1448 b- defN 24-Apr-10 13:42 nucliadb/writer/run.py
+-rw-r--r--  2.0 unx     2926 b- defN 24-Apr-10 13:42 nucliadb/writer/settings.py
+-rw-r--r--  2.0 unx     1036 b- defN 24-Apr-10 13:42 nucliadb/writer/utilities.py
+-rw-r--r--  2.0 unx     1948 b- defN 24-Apr-10 13:42 nucliadb/writer/vectors.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/writer/api/__init__.py
+-rw-r--r--  2.0 unx     1429 b- defN 24-Apr-10 13:42 nucliadb/writer/api/constants.py
+-rw-r--r--  2.0 unx     1095 b- defN 24-Apr-10 13:42 nucliadb/writer/api/v1/__init__.py
+-rw-r--r--  2.0 unx     6412 b- defN 24-Apr-10 13:42 nucliadb/writer/api/v1/export_import.py
+-rw-r--r--  2.0 unx    28320 b- defN 24-Apr-10 13:42 nucliadb/writer/api/v1/field.py
+-rw-r--r--  2.0 unx     5239 b- defN 24-Apr-10 13:42 nucliadb/writer/api/v1/knowledgebox.py
+-rw-r--r--  2.0 unx     2052 b- defN 24-Apr-10 13:42 nucliadb/writer/api/v1/learning_config.py
+-rw-r--r--  2.0 unx    19940 b- defN 24-Apr-10 13:42 nucliadb/writer/api/v1/resource.py
+-rw-r--r--  2.0 unx     1034 b- defN 24-Apr-10 13:42 nucliadb/writer/api/v1/router.py
+-rw-r--r--  2.0 unx    12733 b- defN 24-Apr-10 13:42 nucliadb/writer/api/v1/services.py
+-rw-r--r--  2.0 unx    31464 b- defN 24-Apr-10 13:42 nucliadb/writer/api/v1/upload.py
+-rw-r--r--  2.0 unx     1612 b- defN 24-Apr-10 13:42 nucliadb/writer/layouts/__init__.py
+-rw-r--r--  2.0 unx     2115 b- defN 24-Apr-10 13:42 nucliadb/writer/layouts/v1.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/writer/resource/__init__.py
+-rw-r--r--  2.0 unx     1425 b- defN 24-Apr-10 13:42 nucliadb/writer/resource/audit.py
+-rw-r--r--  2.0 unx    10968 b- defN 24-Apr-10 13:42 nucliadb/writer/resource/basic.py
+-rw-r--r--  2.0 unx    16319 b- defN 24-Apr-10 13:42 nucliadb/writer/resource/field.py
+-rw-r--r--  2.0 unx     2022 b- defN 24-Apr-10 13:42 nucliadb/writer/resource/origin.py
+-rw-r--r--  2.0 unx     1152 b- defN 24-Apr-10 13:42 nucliadb/writer/resource/slug.py
+-rw-r--r--  2.0 unx      835 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/__init__.py
+-rw-r--r--  2.0 unx     1200 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/conftest.py
+-rw-r--r--  2.0 unx     5971 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/fixtures.py
+-rw-r--r--  2.0 unx    15472 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/test_fields.py
+-rw-r--r--  2.0 unx    25913 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/test_files.py
+-rw-r--r--  2.0 unx     1729 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/test_knowledgebox.py
+-rw-r--r--  2.0 unx     4569 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/test_reprocess_file_field.py
+-rw-r--r--  2.0 unx    17449 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/test_resources.py
+-rw-r--r--  2.0 unx     5120 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/test_service.py
+-rw-r--r--  2.0 unx     6054 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/test_tus.py
+-rw-r--r--  2.0 unx     1287 b- defN 24-Apr-10 13:42 nucliadb/writer/tests/utils.py
+-rw-r--r--  2.0 unx     5226 b- defN 24-Apr-10 13:42 nucliadb/writer/tus/__init__.py
+-rw-r--r--  2.0 unx     5082 b- defN 24-Apr-10 13:42 nucliadb/writer/tus/dm.py
+-rw-r--r--  2.0 unx     2186 b- defN 24-Apr-10 13:42 nucliadb/writer/tus/exceptions.py
+-rw-r--r--  2.0 unx    14527 b- defN 24-Apr-10 13:42 nucliadb/writer/tus/gcs.py
+-rw-r--r--  2.0 unx     5849 b- defN 24-Apr-10 13:42 nucliadb/writer/tus/local.py
+-rw-r--r--  2.0 unx     4367 b- defN 24-Apr-10 13:42 nucliadb/writer/tus/pg.py
+-rw-r--r--  2.0 unx     9069 b- defN 24-Apr-10 13:42 nucliadb/writer/tus/s3.py
+-rw-r--r--  2.0 unx     4734 b- defN 24-Apr-10 13:42 nucliadb/writer/tus/storage.py
+-rw-r--r--  2.0 unx     2556 b- defN 24-Apr-10 13:42 nucliadb/writer/tus/utils.py
+-rw-r--r--  2.0 unx     4348 b- defN 24-Apr-10 13:44 nucliadb-3.0.0.post414.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-10 13:44 nucliadb-3.0.0.post414.dist-info/WHEEL
+-rw-r--r--  2.0 unx     1268 b- defN 24-Apr-10 13:44 nucliadb-3.0.0.post414.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       20 b- defN 24-Apr-10 13:44 nucliadb-3.0.0.post414.dist-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 24-Apr-10 13:43 nucliadb-3.0.0.post414.dist-info/zip-safe
+-rw-rw-r--  2.0 unx    42245 b- defN 24-Apr-10 13:44 nucliadb-3.0.0.post414.dist-info/RECORD
+451 files, 2218787 bytes uncompressed, 677700 bytes compressed:  69.5%
```

## zipnote {}

```diff
@@ -1,194 +1,353 @@
-Filename: nucliadb/__init__.py
+Filename: migrations/0001_bootstrap.py
+Comment: 
+
+Filename: migrations/0002_rollover_shards.py
+Comment: 
+
+Filename: migrations/0003_allfields_key.py
+Comment: 
+
+Filename: migrations/0004_rollover_shards.py
+Comment: 
+
+Filename: migrations/0005_rollover_shards.py
+Comment: 
+
+Filename: migrations/0006_rollover_shards.py
+Comment: 
+
+Filename: migrations/0008_cleanup_leftover_rollover_metadata.py
+Comment: 
+
+Filename: migrations/0009_upgrade_relations_and_texts_to_v2.py
+Comment: 
+
+Filename: migrations/0010_fix_corrupt_indexes.py
+Comment: 
+
+Filename: migrations/0011_materialize_labelset_ids.py
+Comment: 
+
+Filename: migrations/0012_rollover_shards.py
 Comment: 
 
-Filename: nucliadb/app.py
+Filename: migrations/0013_rollover_shards.py
 Comment: 
 
-Filename: nucliadb/config.py
+Filename: migrations/0014_rollover_shards.py
+Comment: 
+
+Filename: migrations/0015_targeted_rollover.py
+Comment: 
+
+Filename: migrations/0016_upgrade_to_paragraphs_v2.py
+Comment: 
+
+Filename: migrations/0017_multiple_writable_shards.py
+Comment: 
+
+Filename: migrations/__init__.py
+Comment: 
+
+Filename: nucliadb/__init__.py
 Comment: 
 
 Filename: nucliadb/health.py
 Comment: 
 
-Filename: nucliadb/purge.py
+Filename: nucliadb/learning_proxy.py
+Comment: 
+
+Filename: nucliadb/metrics_exporter.py
+Comment: 
+
+Filename: nucliadb/openapi.py
 Comment: 
 
 Filename: nucliadb/py.typed
 Comment: 
 
-Filename: nucliadb/run.py
+Filename: nucliadb/common/__init__.py
 Comment: 
 
-Filename: nucliadb/settings.py
+Filename: nucliadb/common/locking.py
 Comment: 
 
-Filename: nucliadb/http_clients/__init__.py
+Filename: nucliadb/common/cluster/__init__.py
 Comment: 
 
-Filename: nucliadb/http_clients/exceptions.py
+Filename: nucliadb/common/cluster/base.py
 Comment: 
 
-Filename: nucliadb/http_clients/processing.py
+Filename: nucliadb/common/cluster/exceptions.py
 Comment: 
 
-Filename: nucliadb/ingest/__init__.py
+Filename: nucliadb/common/cluster/grpc_node_dummy.py
 Comment: 
 
-Filename: nucliadb/ingest/app.py
+Filename: nucliadb/common/cluster/index_node.py
 Comment: 
 
-Filename: nucliadb/ingest/cache.py
+Filename: nucliadb/common/cluster/manager.py
 Comment: 
 
-Filename: nucliadb/ingest/chitchat.py
+Filename: nucliadb/common/cluster/rebalance.py
 Comment: 
 
-Filename: nucliadb/ingest/partitions.py
+Filename: nucliadb/common/cluster/rollover.py
 Comment: 
 
-Filename: nucliadb/ingest/processing.py
+Filename: nucliadb/common/cluster/settings.py
 Comment: 
 
-Filename: nucliadb/ingest/purge.py
+Filename: nucliadb/common/cluster/utils.py
 Comment: 
 
-Filename: nucliadb/ingest/serialize.py
+Filename: nucliadb/common/cluster/discovery/__init__.py
 Comment: 
 
-Filename: nucliadb/ingest/settings.py
+Filename: nucliadb/common/cluster/discovery/base.py
 Comment: 
 
-Filename: nucliadb/ingest/txn_utils.py
+Filename: nucliadb/common/cluster/discovery/k8s.py
 Comment: 
 
-Filename: nucliadb/ingest/utils.py
+Filename: nucliadb/common/cluster/discovery/manual.py
 Comment: 
 
-Filename: nucliadb/ingest/consumer/__init__.py
+Filename: nucliadb/common/cluster/discovery/single.py
 Comment: 
 
-Filename: nucliadb/ingest/consumer/auditing.py
+Filename: nucliadb/common/cluster/discovery/types.py
 Comment: 
 
-Filename: nucliadb/ingest/consumer/consumer.py
+Filename: nucliadb/common/cluster/discovery/utils.py
 Comment: 
 
-Filename: nucliadb/ingest/consumer/metrics.py
+Filename: nucliadb/common/cluster/standalone/__init__.py
 Comment: 
 
-Filename: nucliadb/ingest/consumer/pull.py
+Filename: nucliadb/common/cluster/standalone/grpc_node_binding.py
 Comment: 
 
-Filename: nucliadb/ingest/consumer/service.py
+Filename: nucliadb/common/cluster/standalone/index_node.py
 Comment: 
 
-Filename: nucliadb/ingest/consumer/shard_creator.py
+Filename: nucliadb/common/cluster/standalone/service.py
 Comment: 
 
-Filename: nucliadb/ingest/consumer/utils.py
+Filename: nucliadb/common/cluster/standalone/utils.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/__init__.py
+Filename: nucliadb/common/context/__init__.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/base.py
+Filename: nucliadb/common/context/fastapi.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/conversation.py
+Filename: nucliadb/common/datamanagers/__init__.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/date.py
+Filename: nucliadb/common/datamanagers/cluster.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/exceptions.py
+Filename: nucliadb/common/datamanagers/entities.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/file.py
+Filename: nucliadb/common/datamanagers/exceptions.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/generic.py
+Filename: nucliadb/common/datamanagers/kb.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/keywordset.py
+Filename: nucliadb/common/datamanagers/labels.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/layout.py
+Filename: nucliadb/common/datamanagers/processing.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/link.py
+Filename: nucliadb/common/datamanagers/resources.py
 Comment: 
 
-Filename: nucliadb/ingest/fields/text.py
+Filename: nucliadb/common/datamanagers/rollover.py
 Comment: 
 
-Filename: nucliadb/ingest/maindb/__init__.py
+Filename: nucliadb/common/datamanagers/utils.py
 Comment: 
 
-Filename: nucliadb/ingest/maindb/driver.py
+Filename: nucliadb/common/http_clients/__init__.py
 Comment: 
 
-Filename: nucliadb/ingest/maindb/keys.py
+Filename: nucliadb/common/http_clients/auth.py
 Comment: 
 
-Filename: nucliadb/ingest/maindb/local.py
+Filename: nucliadb/common/http_clients/exceptions.py
 Comment: 
 
-Filename: nucliadb/ingest/maindb/pg.py
+Filename: nucliadb/common/http_clients/processing.py
 Comment: 
 
-Filename: nucliadb/ingest/maindb/redis.py
+Filename: nucliadb/common/http_clients/pypi.py
 Comment: 
 
-Filename: nucliadb/ingest/maindb/tikv.py
+Filename: nucliadb/common/http_clients/utils.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/__init__.py
+Filename: nucliadb/common/maindb/__init__.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/abc.py
+Filename: nucliadb/common/maindb/driver.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/brain.py
+Filename: nucliadb/common/maindb/exceptions.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/entities.py
+Filename: nucliadb/common/maindb/local.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/exceptions.py
+Filename: nucliadb/common/maindb/pg.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/grpc_node_binding.py
+Filename: nucliadb/common/maindb/redis.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/grpc_node_dummy.py
+Filename: nucliadb/common/maindb/tikv.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/knowledgebox.py
+Filename: nucliadb/common/maindb/utils.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/labels.py
+Filename: nucliadb/export_import/__init__.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/local_node.py
+Filename: nucliadb/export_import/datamanager.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/local_shard.py
+Filename: nucliadb/export_import/exceptions.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/node.py
+Filename: nucliadb/export_import/exporter.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/nodes_manager.py
+Filename: nucliadb/export_import/importer.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/resource.py
+Filename: nucliadb/export_import/models.py
+Comment: 
+
+Filename: nucliadb/export_import/tasks.py
+Comment: 
+
+Filename: nucliadb/export_import/utils.py
+Comment: 
+
+Filename: nucliadb/ingest/__init__.py
+Comment: 
+
+Filename: nucliadb/ingest/app.py
+Comment: 
+
+Filename: nucliadb/ingest/cache.py
 Comment: 
 
-Filename: nucliadb/ingest/orm/shard.py
+Filename: nucliadb/ingest/partitions.py
+Comment: 
+
+Filename: nucliadb/ingest/processing.py
+Comment: 
+
+Filename: nucliadb/ingest/serialize.py
+Comment: 
+
+Filename: nucliadb/ingest/settings.py
+Comment: 
+
+Filename: nucliadb/ingest/utils.py
+Comment: 
+
+Filename: nucliadb/ingest/consumer/__init__.py
+Comment: 
+
+Filename: nucliadb/ingest/consumer/auditing.py
+Comment: 
+
+Filename: nucliadb/ingest/consumer/consumer.py
+Comment: 
+
+Filename: nucliadb/ingest/consumer/materializer.py
+Comment: 
+
+Filename: nucliadb/ingest/consumer/metrics.py
+Comment: 
+
+Filename: nucliadb/ingest/consumer/pull.py
+Comment: 
+
+Filename: nucliadb/ingest/consumer/service.py
+Comment: 
+
+Filename: nucliadb/ingest/consumer/shard_creator.py
+Comment: 
+
+Filename: nucliadb/ingest/consumer/utils.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/__init__.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/base.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/conversation.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/date.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/exceptions.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/file.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/generic.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/keywordset.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/layout.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/link.py
+Comment: 
+
+Filename: nucliadb/ingest/fields/text.py
+Comment: 
+
+Filename: nucliadb/ingest/orm/__init__.py
+Comment: 
+
+Filename: nucliadb/ingest/orm/brain.py
+Comment: 
+
+Filename: nucliadb/ingest/orm/entities.py
+Comment: 
+
+Filename: nucliadb/ingest/orm/exceptions.py
+Comment: 
+
+Filename: nucliadb/ingest/orm/knowledgebox.py
+Comment: 
+
+Filename: nucliadb/ingest/orm/metrics.py
+Comment: 
+
+Filename: nucliadb/ingest/orm/resource.py
 Comment: 
 
 Filename: nucliadb/ingest/orm/synonyms.py
 Comment: 
 
 Filename: nucliadb/ingest/orm/utils.py
 Comment: 
@@ -213,32 +372,29 @@
 
 Filename: nucliadb/ingest/tests/conftest.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/fixtures.py
 Comment: 
 
-Filename: nucliadb/ingest/tests/tikv.py
-Comment: 
-
 Filename: nucliadb/ingest/tests/vectors.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/integration/__init__.py
 Comment: 
 
-Filename: nucliadb/ingest/tests/integration/test_chitchat.py
-Comment: 
-
 Filename: nucliadb/ingest/tests/integration/consumer/__init__.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/integration/consumer/test_auditing.py
 Comment: 
 
+Filename: nucliadb/ingest/tests/integration/consumer/test_materializer.py
+Comment: 
+
 Filename: nucliadb/ingest/tests/integration/consumer/test_pull.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/integration/consumer/test_service.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/integration/consumer/test_shard_creator.py
@@ -258,41 +414,32 @@
 
 Filename: nucliadb/ingest/tests/unit/__init__.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/unit/test_cache.py
 Comment: 
 
-Filename: nucliadb/ingest/tests/unit/test_chitchat.py
-Comment: 
-
 Filename: nucliadb/ingest/tests/unit/test_partitions.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/unit/test_processing.py
 Comment: 
 
-Filename: nucliadb/ingest/tests/unit/test_purge.py
-Comment: 
-
 Filename: nucliadb/ingest/tests/unit/test_settings.py
 Comment: 
 
-Filename: nucliadb/ingest/tests/unit/test_txn_utils.py
-Comment: 
-
-Filename: nucliadb/ingest/tests/unit/test_utils.py
-Comment: 
-
 Filename: nucliadb/ingest/tests/unit/consumer/__init__.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/unit/consumer/test_auditing.py
 Comment: 
 
+Filename: nucliadb/ingest/tests/unit/consumer/test_consumer.py
+Comment: 
+
 Filename: nucliadb/ingest/tests/unit/consumer/test_pull.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/unit/consumer/test_shard_creator.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/unit/consumer/test_utils.py
@@ -300,66 +447,63 @@
 
 Filename: nucliadb/ingest/tests/unit/orm/__init__.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/unit/orm/test_brain.py
 Comment: 
 
-Filename: nucliadb/ingest/tests/unit/orm/test_cluster.py
-Comment: 
-
-Filename: nucliadb/ingest/tests/unit/orm/test_node.py
-Comment: 
-
 Filename: nucliadb/ingest/tests/unit/orm/test_processor.py
 Comment: 
 
 Filename: nucliadb/ingest/tests/unit/orm/test_resource.py
 Comment: 
 
-Filename: nucliadb/ingest/tests/unit/orm/test_shard.py
+Filename: nucliadb/middleware/__init__.py
 Comment: 
 
-Filename: nucliadb/models/__init__.py
+Filename: nucliadb/middleware/transaction.py
 Comment: 
 
-Filename: nucliadb/models/responses.py
+Filename: nucliadb/migrator/__init__.py
+Comment: 
+
+Filename: nucliadb/migrator/command.py
 Comment: 
 
-Filename: nucliadb/one/__init__.py
+Filename: nucliadb/migrator/context.py
 Comment: 
 
-Filename: nucliadb/one/app.py
+Filename: nucliadb/migrator/datamanager.py
 Comment: 
 
-Filename: nucliadb/one/lifecycle.py
+Filename: nucliadb/migrator/exceptions.py
 Comment: 
 
-Filename: nucliadb/one/tests/__init__.py
+Filename: nucliadb/migrator/migrator.py
 Comment: 
 
-Filename: nucliadb/one/tests/conftest.py
+Filename: nucliadb/migrator/models.py
 Comment: 
 
-Filename: nucliadb/one/tests/fixtures.py
+Filename: nucliadb/migrator/settings.py
 Comment: 
 
-Filename: nucliadb/one/tests/test_basic.py
+Filename: nucliadb/migrator/utils.py
 Comment: 
 
-Filename: nucliadb/one/tests/test_delete_field.py
+Filename: nucliadb/models/__init__.py
 Comment: 
 
-Filename: nucliadb/one/tests/test_fieldmetadata.py
+Filename: nucliadb/models/responses.py
 Comment: 
 
-Filename: nucliadb/one/tests/test_services.py
+Filename: nucliadb/purge/__init__.py
 Comment: 
 
-Filename: nucliadb/one/tests/test_upload_download.py
+Filename: nucliadb/purge/orphan_shards.py
 Comment: 
 
 Filename: nucliadb/reader/__init__.py
 Comment: 
 
 Filename: nucliadb/reader/app.py
 Comment: 
@@ -381,26 +525,41 @@
 
 Filename: nucliadb/reader/api/v1/__init__.py
 Comment: 
 
 Filename: nucliadb/reader/api/v1/download.py
 Comment: 
 
+Filename: nucliadb/reader/api/v1/export_import.py
+Comment: 
+
 Filename: nucliadb/reader/api/v1/knowledgebox.py
 Comment: 
 
+Filename: nucliadb/reader/api/v1/learning_collector.py
+Comment: 
+
+Filename: nucliadb/reader/api/v1/learning_config.py
+Comment: 
+
 Filename: nucliadb/reader/api/v1/resource.py
 Comment: 
 
 Filename: nucliadb/reader/api/v1/router.py
 Comment: 
 
 Filename: nucliadb/reader/api/v1/services.py
 Comment: 
 
+Filename: nucliadb/reader/reader/__init__.py
+Comment: 
+
+Filename: nucliadb/reader/reader/notifications.py
+Comment: 
+
 Filename: nucliadb/reader/tests/__init__.py
 Comment: 
 
 Filename: nucliadb/reader/tests/conftest.py
 Comment: 
 
 Filename: nucliadb/reader/tests/fixtures.py
@@ -456,65 +615,107 @@
 
 Filename: nucliadb/search/api/v1/find.py
 Comment: 
 
 Filename: nucliadb/search/api/v1/knowledgebox.py
 Comment: 
 
-Filename: nucliadb/search/api/v1/resource.py
+Filename: nucliadb/search/api/v1/predict_proxy.py
 Comment: 
 
 Filename: nucliadb/search/api/v1/router.py
 Comment: 
 
 Filename: nucliadb/search/api/v1/search.py
 Comment: 
 
 Filename: nucliadb/search/api/v1/suggest.py
 Comment: 
 
+Filename: nucliadb/search/api/v1/summarize.py
+Comment: 
+
+Filename: nucliadb/search/api/v1/utils.py
+Comment: 
+
+Filename: nucliadb/search/api/v1/resource/__init__.py
+Comment: 
+
+Filename: nucliadb/search/api/v1/resource/ask.py
+Comment: 
+
+Filename: nucliadb/search/api/v1/resource/chat.py
+Comment: 
+
+Filename: nucliadb/search/api/v1/resource/search.py
+Comment: 
+
 Filename: nucliadb/search/requesters/__init__.py
 Comment: 
 
 Filename: nucliadb/search/requesters/utils.py
 Comment: 
 
 Filename: nucliadb/search/search/__init__.py
 Comment: 
 
 Filename: nucliadb/search/search/cache.py
 Comment: 
 
+Filename: nucliadb/search/search/exceptions.py
+Comment: 
+
 Filename: nucliadb/search/search/fetch.py
 Comment: 
 
+Filename: nucliadb/search/search/filters.py
+Comment: 
+
+Filename: nucliadb/search/search/find.py
+Comment: 
+
 Filename: nucliadb/search/search/find_merge.py
 Comment: 
 
 Filename: nucliadb/search/search/merge.py
 Comment: 
 
 Filename: nucliadb/search/search/metrics.py
 Comment: 
 
 Filename: nucliadb/search/search/paragraphs.py
 Comment: 
 
+Filename: nucliadb/search/search/predict_proxy.py
+Comment: 
+
 Filename: nucliadb/search/search/query.py
 Comment: 
 
 Filename: nucliadb/search/search/shards.py
 Comment: 
 
-Filename: nucliadb/search/search/synonyms.py
+Filename: nucliadb/search/search/summarize.py
 Comment: 
 
 Filename: nucliadb/search/search/utils.py
 Comment: 
 
+Filename: nucliadb/search/search/chat/__init__.py
+Comment: 
+
+Filename: nucliadb/search/search/chat/images.py
+Comment: 
+
+Filename: nucliadb/search/search/chat/prompt.py
+Comment: 
+
+Filename: nucliadb/search/search/chat/query.py
+Comment: 
+
 Filename: nucliadb/search/tests/__init__.py
 Comment: 
 
 Filename: nucliadb/search/tests/conftest.py
 Comment: 
 
 Filename: nucliadb/search/tests/fixtures.py
@@ -525,62 +726,188 @@
 
 Filename: nucliadb/search/tests/unit/__init__.py
 Comment: 
 
 Filename: nucliadb/search/tests/unit/test_app.py
 Comment: 
 
-Filename: nucliadb/search/tests/unit/test_find_orderer.py
+Filename: nucliadb/search/tests/unit/test_find_merge.py
 Comment: 
 
-Filename: nucliadb/search/tests/unit/test_openapi.py
+Filename: nucliadb/search/tests/unit/test_merge.py
 Comment: 
 
 Filename: nucliadb/search/tests/unit/test_predict.py
 Comment: 
 
 Filename: nucliadb/search/tests/unit/test_run.py
 Comment: 
 
+Filename: nucliadb/search/tests/unit/api/__init__.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/api/v1/__init__.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/api/v1/test_chat.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/api/v1/test_predict_proxy.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/api/v1/test_summarize.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/api/v1/resource/__init__.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/api/v1/resource/test_ask.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/api/v1/resource/test_chat.py
+Comment: 
+
 Filename: nucliadb/search/tests/unit/search/__init__.py
 Comment: 
 
+Filename: nucliadb/search/tests/unit/search/test_chat_prompt.py
+Comment: 
+
 Filename: nucliadb/search/tests/unit/search/test_fetch.py
 Comment: 
 
+Filename: nucliadb/search/tests/unit/search/test_filters.py
+Comment: 
+
 Filename: nucliadb/search/tests/unit/search/test_paragraphs.py
 Comment: 
 
-Filename: nucliadb/search/tests/unit/search/test_synonyms.py
+Filename: nucliadb/search/tests/unit/search/test_predict_proxy.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/search/test_query.py
 Comment: 
 
 Filename: nucliadb/search/tests/unit/search/requesters/__init__.py
 Comment: 
 
 Filename: nucliadb/search/tests/unit/search/requesters/test_utils.py
 Comment: 
 
-Filename: nucliadb/static/favicon.ico
+Filename: nucliadb/search/tests/unit/search/search/__init__.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/search/search/test_shards.py
+Comment: 
+
+Filename: nucliadb/search/tests/unit/search/search/test_utils.py
+Comment: 
+
+Filename: nucliadb/standalone/__init__.py
+Comment: 
+
+Filename: nucliadb/standalone/api_router.py
+Comment: 
+
+Filename: nucliadb/standalone/app.py
+Comment: 
+
+Filename: nucliadb/standalone/auth.py
 Comment: 
 
-Filename: nucliadb/static/index.html
+Filename: nucliadb/standalone/config.py
 Comment: 
 
-Filename: nucliadb/static/logo.svg
+Filename: nucliadb/standalone/introspect.py
+Comment: 
+
+Filename: nucliadb/standalone/lifecycle.py
+Comment: 
+
+Filename: nucliadb/standalone/purge.py
+Comment: 
+
+Filename: nucliadb/standalone/run.py
+Comment: 
+
+Filename: nucliadb/standalone/settings.py
+Comment: 
+
+Filename: nucliadb/standalone/versions.py
+Comment: 
+
+Filename: nucliadb/standalone/static/favicon.ico
+Comment: 
+
+Filename: nucliadb/standalone/static/index.html
+Comment: 
+
+Filename: nucliadb/standalone/static/logo.svg
+Comment: 
+
+Filename: nucliadb/standalone/tests/__init__.py
+Comment: 
+
+Filename: nucliadb/standalone/tests/conftest.py
+Comment: 
+
+Filename: nucliadb/standalone/tests/fixtures.py
+Comment: 
+
+Filename: nucliadb/standalone/tests/unit/__init__.py
+Comment: 
+
+Filename: nucliadb/standalone/tests/unit/test_api_router.py
+Comment: 
+
+Filename: nucliadb/standalone/tests/unit/test_auth.py
+Comment: 
+
+Filename: nucliadb/standalone/tests/unit/test_introspect.py
+Comment: 
+
+Filename: nucliadb/standalone/tests/unit/test_run.py
+Comment: 
+
+Filename: nucliadb/standalone/tests/unit/test_versions.py
+Comment: 
+
+Filename: nucliadb/tasks/__init__.py
+Comment: 
+
+Filename: nucliadb/tasks/consumer.py
+Comment: 
+
+Filename: nucliadb/tasks/logger.py
+Comment: 
+
+Filename: nucliadb/tasks/models.py
+Comment: 
+
+Filename: nucliadb/tasks/producer.py
+Comment: 
+
+Filename: nucliadb/tasks/registry.py
+Comment: 
+
+Filename: nucliadb/tasks/utils.py
 Comment: 
 
 Filename: nucliadb/tests/__init__.py
 Comment: 
 
 Filename: nucliadb/tests/conftest.py
 Comment: 
 
 Filename: nucliadb/tests/fixtures.py
 Comment: 
 
+Filename: nucliadb/tests/tikv.py
+Comment: 
+
 Filename: nucliadb/tests/benchmarks/__init__.py
 Comment: 
 
 Filename: nucliadb/tests/benchmarks/test_search.py
 Comment: 
 
 Filename: nucliadb/tests/knowledgeboxes/__init__.py
@@ -588,20 +915,137 @@
 
 Filename: nucliadb/tests/knowledgeboxes/philosophy_books.py
 Comment: 
 
 Filename: nucliadb/tests/knowledgeboxes/ten_dummy_resources.py
 Comment: 
 
+Filename: nucliadb/tests/migrations/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/migrations/test_migration_0017.py
+Comment: 
+
+Filename: nucliadb/tests/unit/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/unit/test_field_ids.py
+Comment: 
+
+Filename: nucliadb/tests/unit/test_health.py
+Comment: 
+
+Filename: nucliadb/tests/unit/test_kb_slugs.py
+Comment: 
+
+Filename: nucliadb/tests/unit/test_learning_proxy.py
+Comment: 
+
+Filename: nucliadb/tests/unit/test_metrics_exporter.py
+Comment: 
+
+Filename: nucliadb/tests/unit/test_openapi.py
+Comment: 
+
+Filename: nucliadb/tests/unit/test_purge.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/test_context.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/cluster/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/cluster/test_cluster.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/cluster/test_kb_shard_manager.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/cluster/test_rollover.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/cluster/discovery/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/cluster/discovery/test_k8s.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/cluster/standalone/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/cluster/standalone/test_service.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/cluster/standalone/test_utils.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/maindb/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/maindb/test_driver.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/maindb/test_tikv.py
+Comment: 
+
+Filename: nucliadb/tests/unit/common/maindb/test_utils.py
+Comment: 
+
+Filename: nucliadb/tests/unit/export_import/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/unit/export_import/test_datamanager.py
+Comment: 
+
+Filename: nucliadb/tests/unit/export_import/test_utils.py
+Comment: 
+
+Filename: nucliadb/tests/unit/migrator/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/unit/migrator/test_migrator.py
+Comment: 
+
+Filename: nucliadb/tests/unit/tasks/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/unit/tasks/conftest.py
+Comment: 
+
+Filename: nucliadb/tests/unit/tasks/test_consumer.py
+Comment: 
+
+Filename: nucliadb/tests/unit/tasks/test_producer.py
+Comment: 
+
+Filename: nucliadb/tests/unit/tasks/test_tasks.py
+Comment: 
+
 Filename: nucliadb/tests/utils/__init__.py
 Comment: 
 
+Filename: nucliadb/tests/utils/aiohttp_session.py
+Comment: 
+
 Filename: nucliadb/tests/utils/entities.py
 Comment: 
 
+Filename: nucliadb/tests/utils/broker_messages/__init__.py
+Comment: 
+
+Filename: nucliadb/tests/utils/broker_messages/fields.py
+Comment: 
+
+Filename: nucliadb/tests/utils/broker_messages/helpers.py
+Comment: 
+
 Filename: nucliadb/train/__init__.py
 Comment: 
 
 Filename: nucliadb/train/app.py
 Comment: 
 
 Filename: nucliadb/train/generator.py
@@ -621,29 +1065,29 @@
 
 Filename: nucliadb/train/servicer.py
 Comment: 
 
 Filename: nucliadb/train/settings.py
 Comment: 
 
+Filename: nucliadb/train/types.py
+Comment: 
+
 Filename: nucliadb/train/upload.py
 Comment: 
 
 Filename: nucliadb/train/uploader.py
 Comment: 
 
 Filename: nucliadb/train/utils.py
 Comment: 
 
 Filename: nucliadb/train/api/__init__.py
 Comment: 
 
-Filename: nucliadb/train/api/models.py
-Comment: 
-
 Filename: nucliadb/train/api/utils.py
 Comment: 
 
 Filename: nucliadb/train/api/v1/__init__.py
 Comment: 
 
 Filename: nucliadb/train/api/v1/check.py
@@ -660,17 +1104,26 @@
 
 Filename: nucliadb/train/generators/__init__.py
 Comment: 
 
 Filename: nucliadb/train/generators/field_classifier.py
 Comment: 
 
+Filename: nucliadb/train/generators/image_classifier.py
+Comment: 
+
 Filename: nucliadb/train/generators/paragraph_classifier.py
 Comment: 
 
+Filename: nucliadb/train/generators/paragraph_streaming.py
+Comment: 
+
+Filename: nucliadb/train/generators/question_answer_streaming.py
+Comment: 
+
 Filename: nucliadb/train/generators/sentence_classifier.py
 Comment: 
 
 Filename: nucliadb/train/generators/token_classifier.py
 Comment: 
 
 Filename: nucliadb/train/generators/utils.py
@@ -696,14 +1149,17 @@
 
 Filename: nucliadb/train/tests/test_get_ontology.py
 Comment: 
 
 Filename: nucliadb/train/tests/test_get_ontology_count.py
 Comment: 
 
+Filename: nucliadb/train/tests/test_image_classification.py
+Comment: 
+
 Filename: nucliadb/train/tests/test_list_fields.py
 Comment: 
 
 Filename: nucliadb/train/tests/test_list_paragraphs.py
 Comment: 
 
 Filename: nucliadb/train/tests/test_list_resources.py
@@ -711,26 +1167,38 @@
 
 Filename: nucliadb/train/tests/test_list_sentences.py
 Comment: 
 
 Filename: nucliadb/train/tests/test_paragraph_classification.py
 Comment: 
 
+Filename: nucliadb/train/tests/test_paragraph_streaming.py
+Comment: 
+
+Filename: nucliadb/train/tests/test_question_answer_streaming.py
+Comment: 
+
 Filename: nucliadb/train/tests/test_sentence_classification.py
 Comment: 
 
 Filename: nucliadb/train/tests/test_token_classification.py
 Comment: 
 
+Filename: nucliadb/train/tests/utils.py
+Comment: 
+
 Filename: nucliadb/writer/__init__.py
 Comment: 
 
 Filename: nucliadb/writer/app.py
 Comment: 
 
+Filename: nucliadb/writer/back_pressure.py
+Comment: 
+
 Filename: nucliadb/writer/exceptions.py
 Comment: 
 
 Filename: nucliadb/writer/lifecycle.py
 Comment: 
 
 Filename: nucliadb/writer/openapi.py
@@ -741,26 +1209,38 @@
 
 Filename: nucliadb/writer/settings.py
 Comment: 
 
 Filename: nucliadb/writer/utilities.py
 Comment: 
 
+Filename: nucliadb/writer/vectors.py
+Comment: 
+
 Filename: nucliadb/writer/api/__init__.py
 Comment: 
 
+Filename: nucliadb/writer/api/constants.py
+Comment: 
+
 Filename: nucliadb/writer/api/v1/__init__.py
 Comment: 
 
+Filename: nucliadb/writer/api/v1/export_import.py
+Comment: 
+
 Filename: nucliadb/writer/api/v1/field.py
 Comment: 
 
 Filename: nucliadb/writer/api/v1/knowledgebox.py
 Comment: 
 
+Filename: nucliadb/writer/api/v1/learning_config.py
+Comment: 
+
 Filename: nucliadb/writer/api/v1/resource.py
 Comment: 
 
 Filename: nucliadb/writer/api/v1/router.py
 Comment: 
 
 Filename: nucliadb/writer/api/v1/services.py
@@ -789,17 +1269,14 @@
 
 Filename: nucliadb/writer/resource/origin.py
 Comment: 
 
 Filename: nucliadb/writer/resource/slug.py
 Comment: 
 
-Filename: nucliadb/writer/resource/vectors.py
-Comment: 
-
 Filename: nucliadb/writer/tests/__init__.py
 Comment: 
 
 Filename: nucliadb/writer/tests/conftest.py
 Comment: 
 
 Filename: nucliadb/writer/tests/fixtures.py
@@ -822,17 +1299,14 @@
 
 Filename: nucliadb/writer/tests/test_service.py
 Comment: 
 
 Filename: nucliadb/writer/tests/test_tus.py
 Comment: 
 
-Filename: nucliadb/writer/tests/tus.py
-Comment: 
-
 Filename: nucliadb/writer/tests/utils.py
 Comment: 
 
 Filename: nucliadb/writer/tus/__init__.py
 Comment: 
 
 Filename: nucliadb/writer/tus/dm.py
@@ -843,35 +1317,38 @@
 
 Filename: nucliadb/writer/tus/gcs.py
 Comment: 
 
 Filename: nucliadb/writer/tus/local.py
 Comment: 
 
+Filename: nucliadb/writer/tus/pg.py
+Comment: 
+
 Filename: nucliadb/writer/tus/s3.py
 Comment: 
 
 Filename: nucliadb/writer/tus/storage.py
 Comment: 
 
 Filename: nucliadb/writer/tus/utils.py
 Comment: 
 
-Filename: nucliadb-2.9.0.post267.dist-info/METADATA
+Filename: nucliadb-3.0.0.post414.dist-info/METADATA
 Comment: 
 
-Filename: nucliadb-2.9.0.post267.dist-info/WHEEL
+Filename: nucliadb-3.0.0.post414.dist-info/WHEEL
 Comment: 
 
-Filename: nucliadb-2.9.0.post267.dist-info/entry_points.txt
+Filename: nucliadb-3.0.0.post414.dist-info/entry_points.txt
 Comment: 
 
-Filename: nucliadb-2.9.0.post267.dist-info/top_level.txt
+Filename: nucliadb-3.0.0.post414.dist-info/top_level.txt
 Comment: 
 
-Filename: nucliadb-2.9.0.post267.dist-info/zip-safe
+Filename: nucliadb-3.0.0.post414.dist-info/zip-safe
 Comment: 
 
-Filename: nucliadb-2.9.0.post267.dist-info/RECORD
+Filename: nucliadb-3.0.0.post414.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nucliadb/health.py

```diff
@@ -17,16 +17,16 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
 import logging
 from typing import Awaitable, Callable, Optional
 
-from grpc import aio  # type: ignore
-from grpc_health.v1 import health, health_pb2, health_pb2_grpc  # type: ignore
+from grpc import aio
+from grpc_health.v1 import health, health_pb2, health_pb2_grpc
 
 from nucliadb_utils.cache.nats import NatsPubsub
 from nucliadb_utils.cache.pubsub import PubSubDriver
 from nucliadb_utils.utilities import Utility, get_nats_manager, get_utility
 
 logger = logging.getLogger(__name__)
 
@@ -37,18 +37,18 @@
     nats_manager = get_nats_manager()
     if nats_manager is None:
         return True
     return nats_manager.healthy()
 
 
 def nodes_health_check() -> bool:
-    from nucliadb.ingest import orm
+    from nucliadb.common.cluster import manager
     from nucliadb.ingest.settings import DriverConfig, settings
 
-    return len(orm.NODES) > 0 or settings.driver == DriverConfig.LOCAL
+    return len(manager.INDEX_NODES) > 0 or settings.driver == DriverConfig.LOCAL
 
 
 def pubsub_check() -> bool:
     driver: Optional[PubSubDriver] = get_utility(Utility.PUBSUB)
     if driver is None:
         return True
     if isinstance(driver, NatsPubsub):
@@ -104,15 +104,15 @@
     async def finalizer():
         health_task.cancel()
 
     return finalizer
 
 
 async def start_grpc_health_service(port: int) -> Callable[[], Awaitable[None]]:
-    aio.init_grpc_aio()
+    aio.init_grpc_aio()  # type: ignore
 
     server = aio.server()
     server.add_insecure_port(f"0.0.0.0:{port}")
 
     health_check_finalizer = setup_grpc_servicer(server)
 
     await server.start()
```

## nucliadb/ingest/app.py

```diff
@@ -14,94 +14,89 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
-from typing import Awaitable, Callable, Optional
+from typing import Awaitable, Callable
 
 import pkg_resources
 
 from nucliadb import health
+from nucliadb.common.cluster.discovery.utils import (
+    setup_cluster_discovery,
+    teardown_cluster_discovery,
+)
+from nucliadb.common.cluster.settings import settings as cluster_settings
+from nucliadb.common.cluster.utils import setup_cluster, teardown_cluster
+from nucliadb.common.context import ApplicationContext
+from nucliadb.export_import.tasks import get_exports_consumer, get_imports_consumer
 from nucliadb.ingest import SERVICE_NAME
-from nucliadb.ingest.chitchat import start_chitchat, stop_chitchat
 from nucliadb.ingest.consumer import service as consumer_service
 from nucliadb.ingest.partitions import assign_partitions
 from nucliadb.ingest.service import start_grpc
 from nucliadb.ingest.settings import settings
 from nucliadb_telemetry import errors
 from nucliadb_telemetry.logs import setup_logging
 from nucliadb_telemetry.utils import setup_telemetry
 from nucliadb_utils.fastapi.run import serve_metrics
-from nucliadb_utils.indexing import IndexingUtility
 from nucliadb_utils.run import run_until_exit
 from nucliadb_utils.settings import indexing_settings, transaction_settings
 from nucliadb_utils.utilities import (
-    Utility,
-    clean_utility,
-    get_indexing,
-    set_utility,
     start_audit_utility,
+    start_indexing_utility,
     start_nats_manager,
     start_transaction_utility,
     stop_audit_utility,
+    stop_indexing_utility,
     stop_nats_manager,
     stop_transaction_utility,
 )
 
 
-async def start_indexing_utility(service_name: Optional[str] = None):
-    if (
-        not indexing_settings.index_local
-        and indexing_settings.index_jetstream_servers is not None
-    ):
-        indexing_utility = IndexingUtility(
-            nats_creds=indexing_settings.index_jetstream_auth,
-            nats_servers=indexing_settings.index_jetstream_servers,
-        )
-        await indexing_utility.initialize(service_name)
-        set_utility(Utility.INDEXING, indexing_utility)
-
-
-async def stop_indexing_utility():
-    indexing_utility = get_indexing()
-    if indexing_utility:
-        await indexing_utility.finalize()
-        clean_utility(Utility.INDEXING)
-
-
 async def initialize() -> list[Callable[[], Awaitable[None]]]:
     await setup_telemetry(SERVICE_NAME)
 
+    await setup_cluster()
     await start_transaction_utility(SERVICE_NAME)
-    await start_indexing_utility(SERVICE_NAME)
+    if (
+        not cluster_settings.standalone_mode
+        and indexing_settings.index_jetstream_servers is not None
+    ):
+        await start_indexing_utility(SERVICE_NAME)
+
     await start_audit_utility(SERVICE_NAME)
 
     finalizers = [
         stop_transaction_utility,
         stop_indexing_utility,
         stop_audit_utility,
+        teardown_cluster,
     ]
 
     if not transaction_settings.transaction_local:
         # if we're running in standalone, we do not
         # want these services
         await start_nats_manager(
             SERVICE_NAME,
             transaction_settings.transaction_jetstream_servers,
             transaction_settings.transaction_jetstream_auth,
         )
         finalizers.append(stop_nats_manager)
 
-        await start_chitchat(SERVICE_NAME)
-        finalizers.append(stop_chitchat)
+        await setup_cluster_discovery()
+        finalizers.append(teardown_cluster_discovery)
 
     health.register_health_checks(
-        [health.nats_manager_healthy, health.nodes_health_check, health.pubsub_check]
+        [
+            health.nats_manager_healthy,
+            health.nodes_health_check,
+            health.pubsub_check,
+        ]
     )
 
     return finalizers
 
 
 async def initialize_grpc():  # pragma: no cover
     finalizers = await initialize()
@@ -151,25 +146,38 @@
         [grpc_health_finalizer, consumer, metrics_server.shutdown] + finalizers
     )
 
 
 async def main_subscriber_workers():  # pragma: no cover
     finalizers = await initialize()
 
+    context = ApplicationContext("subscriber-workers")
+    await context.initialize()
+
     metrics_server = await serve_metrics()
     grpc_health_finalizer = await health.start_grpc_health_service(settings.grpc_port)
     auditor_closer = await consumer_service.start_auditor()
     shard_creator_closer = await consumer_service.start_shard_creator()
+    materializer_closer = await consumer_service.start_materializer()
+
+    exports_consumer = get_exports_consumer()
+    await exports_consumer.initialize(context)
+    imports_consumer = get_imports_consumer()
+    await imports_consumer.initialize(context)
 
     await run_until_exit(
         [
+            imports_consumer.finalize,
+            exports_consumer.finalize,
             auditor_closer,
             shard_creator_closer,
+            materializer_closer,
             metrics_server.shutdown,
             grpc_health_finalizer,
+            context.finalize,
         ]
         + finalizers
     )
 
 
 def setup_configuration():  # pragma: no cover
     setup_logging()
```

## nucliadb/ingest/cache.py

```diff
@@ -13,13 +13,13 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from nucliadb.ingest.orm.node import READ_CONNECTIONS, WRITE_CONNECTIONS
+from nucliadb.common.cluster.index_node import READ_CONNECTIONS, WRITE_CONNECTIONS
 
 
 def clear_ingest_cache():
     READ_CONNECTIONS.clear()
     WRITE_CONNECTIONS.clear()
```

## nucliadb/ingest/partitions.py

```diff
@@ -34,15 +34,17 @@
         hostname = os.environ.get("HOSTNAME")
         if hostname is not None:
             sts_values = hostname.split("-")
             if len(sts_values) > 0:
                 try:
                     settings.replica_number = int(sts_values[-1])
                 except Exception:
-                    logger.error("Could not extract replica number from HOSTNAME")
+                    logger.error(
+                        f"Could not extract replica number from hostname: {hostname}"
+                    )
                     pass
 
         if settings.replica_number == -1:
             settings.replica_number = 0
     logger.info(f"PARTITIONS: Replica Number = {settings.replica_number}")
 
     # calculate assigned partitions based on total replicas and own replica number
```

## nucliadb/ingest/processing.py

```diff
@@ -15,39 +15,49 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import base64
 import datetime
+import logging
 import uuid
+from collections import defaultdict
 from contextlib import AsyncExitStack
 from enum import Enum
-from typing import TYPE_CHECKING, Any, Dict, Optional
+from typing import TYPE_CHECKING, Any, Optional, TypeVar
 
 import aiohttp
-import jwt  # type: ignore
+import backoff
+import jwt
 from nucliadb_protos.resources_pb2 import CloudFile
 from nucliadb_protos.resources_pb2 import FieldFile as FieldFilePB
 from pydantic import BaseModel, Field
 
 import nucliadb_models as models
 from nucliadb_models.resource import QueueType
 from nucliadb_telemetry import metrics
-from nucliadb_utils import logger
 from nucliadb_utils.exceptions import LimitsExceededError, SendToProcessError
 from nucliadb_utils.settings import nuclia_settings, storage_settings
 from nucliadb_utils.storages.storage import Storage
 from nucliadb_utils.utilities import Utility, set_utility
 
+logger = logging.getLogger(__name__)
+
+_T = TypeVar("_T")
+
 if TYPE_CHECKING:  # pragma: no cover
     SourceValue = CloudFile.Source.V
 else:
     SourceValue = int
 
+RETRIABLE_EXCEPTIONS = (aiohttp.client_exceptions.ClientConnectorError,)
+MAX_TRIES = 4
+
+
 processing_observer = metrics.Observer(
     "processing_engine",
     labels={"type": ""},
     error_mappings={
         "over_limits": LimitsExceededError,
         "processing_api_error": SendToProcessError,
     },
@@ -56,165 +66,132 @@
 
 class Source(SourceValue, Enum):  # type: ignore
     HTTP = 0
     INGEST = 1
 
 
 class ProcessingInfo(BaseModel):
-    seqid: int
+    seqid: Optional[int]
     account_seq: Optional[int]
-    queue: QueueType
+    queue: Optional[QueueType] = None
 
 
 class PushPayload(BaseModel):
     # There are multiple options of payload
     uuid: str
     slug: Optional[str] = None
     kbid: str
     source: Optional[Source] = None
     userid: str
 
-    genericfield: Dict[str, models.Text] = {}
+    title: Optional[str] = None
+
+    genericfield: dict[str, models.Text] = {}
 
     # New File
-    filefield: Dict[str, str] = {}
+    filefield: dict[str, str] = {}
 
     # New Link
-    linkfield: Dict[str, models.LinkUpload] = {}
+    linkfield: dict[str, models.LinkUpload] = {}
 
     # Diff on Text Field
-    textfield: Dict[str, models.Text] = {}
+    textfield: dict[str, models.Text] = {}
 
     # Diff on a Layout Field
-    layoutfield: Dict[str, models.LayoutDiff] = {}
+    layoutfield: dict[str, models.LayoutDiff] = {}
 
     # New conversations to process
-    conversationfield: Dict[str, models.PushConversation] = {}
+    conversationfield: dict[str, models.PushConversation] = {}
 
     # Only internal
     partition: int
 
     # List of available processing options (with default values)
     processing_options: Optional[models.PushProcessingOptions] = Field(
         default_factory=models.PushProcessingOptions
     )
 
 
 class PushResponse(BaseModel):
     seqid: Optional[int] = None
 
 
-DUMMY_JWT = "DUMMYJWT"
-
-
 async def start_processing_engine():
     if nuclia_settings.dummy_processing:
         processing_engine = DummyProcessingEngine()
     else:
         processing_engine = ProcessingEngine(
             nuclia_service_account=nuclia_settings.nuclia_service_account,
             nuclia_zone=nuclia_settings.nuclia_zone,
             onprem=nuclia_settings.onprem,
             nuclia_jwt_key=nuclia_settings.nuclia_jwt_key,
-            nuclia_cluster_url=nuclia_settings.nuclia_cluster_url,
+            nuclia_processing_cluster_url=nuclia_settings.nuclia_processing_cluster_url,
             nuclia_public_url=nuclia_settings.nuclia_public_url,
             driver=storage_settings.file_backend,
             days_to_keep=storage_settings.upload_token_expiration,
         )
     await processing_engine.initialize()
     set_utility(Utility.PROCESSING, processing_engine)
 
 
-class DummyProcessingEngine:
-    def __init__(self):
-        self.calls: list[list[Any]] = []
-
-    async def initialize(self):
-        pass
-
-    async def finalize(self):
-        pass
-
-    async def convert_filefield_to_str(self, file: models.FileField) -> str:
-        self.calls.append([file])
-        return DUMMY_JWT
-
-    def convert_external_filefield_to_str(self, file_field: models.FileField) -> str:
-        self.calls.append([file_field])
-        return DUMMY_JWT
-
-    async def convert_internal_filefield_to_str(
-        self, file: FieldFilePB, storage: Storage
-    ) -> str:
-        self.calls.append([file, storage])
-        return DUMMY_JWT
-
-    async def convert_internal_cf_to_str(self, cf: CloudFile, storage: Storage) -> str:
-        self.calls.append([cf, storage])
-        return DUMMY_JWT
-
-    async def send_to_process(
-        self, item: PushPayload, partition: int
-    ) -> ProcessingInfo:
-        self.calls.append([item, partition])
-        return ProcessingInfo(
-            seqid=len(self.calls), account_seq=0, queue=QueueType.SHARED
-        )
-
-
 class ProcessingEngine:
     def __init__(
         self,
         nuclia_service_account: Optional[str] = None,
         nuclia_zone: Optional[str] = None,
         nuclia_public_url: Optional[str] = None,
-        nuclia_cluster_url: Optional[str] = None,
+        nuclia_processing_cluster_url: Optional[str] = None,
         onprem: Optional[bool] = False,
         nuclia_jwt_key: Optional[str] = None,
         days_to_keep: int = 3,
         driver: str = "gcs",
     ):
         self.nuclia_service_account = nuclia_service_account
         self.nuclia_zone = nuclia_zone
         if nuclia_public_url is not None:
             self.nuclia_public_url: Optional[str] = nuclia_public_url.format(
                 zone=nuclia_zone
             )
         else:
             self.nuclia_public_url = None
 
-        if nuclia_cluster_url is not None:
-            self.nuclia_cluster_url: Optional[str] = nuclia_cluster_url
-        else:
-            self.nuclia_cluster_url = None
-
         self.onprem = onprem
         if self.onprem:
             self.nuclia_upload_url = (
                 f"{self.nuclia_public_url}/api/v1/processing/upload"
             )
         else:
             self.nuclia_upload_url = (
-                f"{self.nuclia_cluster_url}/api/v1/processing/upload"
+                f"{nuclia_processing_cluster_url}/api/v1/processing/upload"
             )
         self.nuclia_internal_push = (
-            f"{self.nuclia_cluster_url}/api/internal/processing/push"
+            f"{nuclia_processing_cluster_url}/api/v1/internal/processing/push"
+        )
+        self.nuclia_internal_delete = (
+            f"{nuclia_processing_cluster_url}/api/v1/internal/processing/requests"
+        )
+        self.nuclia_external_push_v2 = (
+            f"{self.nuclia_public_url}/api/v1/processing/push"
+        )
+        self.nuclia_external_delete = (
+            f"{self.nuclia_public_url}/api/v1/processing/requests"
         )
-        self.nuclia_external_push = f"{self.nuclia_public_url}/api/v1/processing/push"
 
         self.nuclia_jwt_key = nuclia_jwt_key
         self.days_to_keep = days_to_keep
         if driver == "gcs":
             self.driver = 0
         elif driver == "s3":
             self.driver = 1
-        elif driver == "local":
+        elif driver in ("local", "pg"):
             self.driver = 2
         else:
-            logger.error(f"Not valid driver to processing, fallback to local: {driver}")
+            logger.error(
+                f"Not a valid driver to processing, fallback to local: {driver}"
+            )
             self.driver = 2
         self._exit_stack = AsyncExitStack()
 
     async def initialize(self):
         self.session = aiohttp.ClientSession()
 
     async def finalize(self):
@@ -266,14 +243,20 @@
             "size": file.file.size,
             "content_type": file.file.content_type,
             "password": file.password,
             "language": file.language,
         }
         return jwt.encode(payload, self.nuclia_jwt_key, algorithm="HS256")
 
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     @processing_observer.wrap({"type": "file_field_upload"})
     async def convert_filefield_to_str(self, file: models.FileField) -> str:
         # Upload file without storing on Nuclia DB
         headers = {}
         headers["X-PASSWORD"] = file.password
         headers["X-LANGUAGE"] = file.language
         headers["X-FILENAME"] = base64.b64encode(file.file.filename.encode()).decode()  # type: ignore
@@ -286,14 +269,16 @@
         ) as resp:
             if resp.status == 200:
                 jwttoken = await resp.text()
                 return jwttoken
             elif resp.status == 402:
                 data = await resp.json()
                 raise LimitsExceededError(resp.status, data["detail"])
+            elif resp.status == 429:
+                raise LimitsExceededError(resp.status, "Rate limited")
             else:
                 text = await resp.text()
                 raise Exception(f"STATUS: {resp.status} - {text}")
 
     def convert_external_filefield_to_str(self, file_field: models.FileField) -> str:
         if self.nuclia_jwt_key is None:
             raise AttributeError("Nuclia JWT key not set")
@@ -314,14 +299,20 @@
             "filename": file_field.file.filename,
             "content_type": file_field.file.content_type,
             "language": file_field.language,
             "password": file_field.password,
         }
         return jwt.encode(payload, self.nuclia_jwt_key, algorithm="HS256")
 
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     @processing_observer.wrap({"type": "file_field_upload_internal"})
     async def convert_internal_filefield_to_str(
         self, file: FieldFilePB, storage: Storage
     ) -> str:
         """It's already an internal file that needs to be uploaded"""
         if self.onprem is False:
             # Upload the file to processing upload
@@ -344,19 +335,27 @@
                 self.nuclia_upload_url, data=iterator, headers=headers
             ) as resp:
                 if resp.status == 200:
                     jwttoken = await resp.text()
                 elif resp.status == 402:
                     data = await resp.json()
                     raise LimitsExceededError(resp.status, data["detail"])
+                elif resp.status == 429:
+                    raise LimitsExceededError(resp.status, "Rate limited")
                 else:
                     text = await resp.text()
                     raise Exception(f"STATUS: {resp.status} - {text}")
         return jwttoken
 
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     @processing_observer.wrap({"type": "cloud_file_upload"})
     async def convert_internal_cf_to_str(self, cf: CloudFile, storage: Storage) -> str:
         if self.onprem is False:
             # Upload the file to processing upload
             jwttoken = self.generate_file_token_from_cloudfile(cf)
         else:
             headers = {}
@@ -372,58 +371,150 @@
                 self.nuclia_upload_url, data=iterator, headers=headers
             ) as resp:
                 if resp.status == 200:
                     jwttoken = await resp.text()
                 elif resp.status == 402:
                     data = await resp.json()
                     raise LimitsExceededError(resp.status, data["detail"])
+                elif resp.status == 429:
+                    raise LimitsExceededError(resp.status, "Rate limited")
                 else:
                     text = await resp.text()
                     raise Exception(f"STATUS: {resp.status} - {text}")
 
         return jwttoken
 
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
     async def send_to_process(
         self, item: PushPayload, partition: int
     ) -> ProcessingInfo:
         op_type = "process_external" if self.onprem else "process_internal"
         with processing_observer({"type": op_type}):
             headers = {"CONTENT-TYPE": "application/json"}
             if self.onprem is False:
                 # Upload the payload
                 item.partition = partition
                 resp = await self.session.post(
-                    url=f"{self.nuclia_internal_push}",
-                    data=item.json(),
-                    headers=headers,
+                    url=self.nuclia_internal_push, data=item.json(), headers=headers
                 )
             else:
                 headers.update(
                     {"X-STF-NUAKEY": f"Bearer {self.nuclia_service_account}"}
                 )
                 # Upload the payload
                 resp = await self.session.post(
-                    url=self.nuclia_external_push + "?partition=" + str(partition),
-                    data=item.json(),
-                    headers=headers,
+                    url=self.nuclia_external_push_v2, data=item.json(), headers=headers
                 )
             if resp.status == 200:
                 data = await resp.json()
                 seqid = data.get("seqid")
                 account_seq = data.get("account_seq")
                 queue_type = data.get("queue")
             elif resp.status in (402, 413):
                 # 402 -> account limits exceeded
                 # 413 -> payload size exceeded
                 data = await resp.json()
                 raise LimitsExceededError(resp.status, data["detail"])
+            elif resp.status == 429:
+                raise LimitsExceededError(resp.status, "Rate limited")
             else:
-                raise SendToProcessError(f"{resp.status}: {await resp.text()}")
+                error_text = await resp.text()
+                logger.warning(f"Error sending to process: {resp.status} {error_text}")
+                raise SendToProcessError()
 
         logger.info(
             f"Pushed message to proxy. kb: {item.kbid}, resource: {item.uuid}, \
                 ingest seqid: {seqid}, partition: {partition}"
         )
 
         return ProcessingInfo(
-            seqid=seqid, account_seq=account_seq, queue=QueueType(queue_type)
+            seqid=seqid,
+            account_seq=account_seq,
+            queue=QueueType(queue_type) if queue_type is not None else None,
+        )
+
+    async def delete_from_processing(
+        self, *, kbid: str, resource_id: Optional[str] = None
+    ) -> None:
+        """
+        Delete a resource from processing. This prevents inflight resources from being processed
+        and wasting resources.
+
+        Ideally, this is done by publishing an event to NATS; however, since we also need to work
+        for hybrid on-prem installations, this is a simple way to handle it.
+
+        Long term, if we want to publish object events out to a NATS stream, we can implement
+        that instead of this method.
+        """
+        headers = {"CONTENT-TYPE": "application/json"}
+        data = {"kbid": kbid, "resource_id": resource_id}
+        if self.onprem is False:
+            # Upload the payload
+            url = self.nuclia_internal_delete
+        else:
+            url = self.nuclia_external_delete
+            headers.update({"X-NUCLIA-NUAKEY": f"Bearer {self.nuclia_service_account}"})
+
+        resp = await self.session.delete(url=url, json=data, headers=headers)
+        if resp.status != 200:
+            logger.warning(
+                "Error deleting from processing",
+                extra={"status": resp.status, "text": await resp.text()},
+            )
+
+
+class DummyProcessingEngine(ProcessingEngine):
+    def __init__(self):
+        self.calls: list[list[Any]] = []  # type: ignore
+        self.values = defaultdict(list)
+        self.onprem = True
+
+    async def initialize(self):
+        pass
+
+    async def finalize(self):
+        pass
+
+    async def convert_filefield_to_str(self, file: models.FileField) -> str:
+        self.calls.append([file])
+        index = len(self.values["convert_filefield_to_str"])
+        self.values["convert_filefield_to_str"].append(file)
+        return f"convert_filefield_to_str,{index}"
+
+    def convert_external_filefield_to_str(self, file_field: models.FileField) -> str:
+        self.calls.append([file_field])
+        index = len(self.values["convert_external_filefield_to_str"])
+        self.values["convert_external_filefield_to_str"].append(file_field)
+        return f"convert_external_filefield_to_str,{index}"
+
+    async def convert_internal_filefield_to_str(
+        self, file: FieldFilePB, storage: Storage
+    ) -> str:
+        self.calls.append([file, storage])
+        index = len(self.values["convert_internal_filefield_to_str"])
+        self.values["convert_internal_filefield_to_str"].append([file, storage])
+        return f"convert_internal_filefield_to_str,{index}"
+
+    async def convert_internal_cf_to_str(self, cf: CloudFile, storage: Storage) -> str:
+        self.calls.append([cf, storage])
+        index = len(self.values["convert_internal_cf_to_str"])
+        self.values["convert_internal_cf_to_str"].append([cf, storage])
+        return f"convert_internal_cf_to_str,{index}"
+
+    async def send_to_process(
+        self, item: PushPayload, partition: int
+    ) -> ProcessingInfo:
+        self.calls.append([item, partition])
+        self.values["send_to_process"].append([item, partition])
+        return ProcessingInfo(
+            seqid=len(self.calls), account_seq=0, queue=QueueType.SHARED
         )
+
+    async def delete_from_processing(
+        self, *, kbid: str, resource_id: Optional[str] = None
+    ) -> None:
+        self.calls.append([kbid, resource_id])
```

## nucliadb/ingest/serialize.py

```diff
@@ -14,25 +14,25 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
-from typing import List, Optional
+from typing import Optional
 
 import nucliadb_models as models
+from nucliadb.common.maindb.driver import Transaction
+from nucliadb.common.maindb.utils import get_driver
 from nucliadb.ingest.fields.base import Field
 from nucliadb.ingest.fields.conversation import Conversation
 from nucliadb.ingest.fields.file import File
 from nucliadb.ingest.fields.link import Link
-from nucliadb.ingest.maindb.driver import Transaction
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
 from nucliadb.ingest.orm.resource import Resource as ORMResource
-from nucliadb.ingest.utils import get_driver
 from nucliadb_models.common import FIELD_TYPES_MAP, FieldTypeName
 from nucliadb_models.resource import (
     ConversationFieldData,
     ConversationFieldExtractedData,
     DatetimeFieldData,
     DatetimeFieldExtractedData,
     Error,
@@ -50,23 +50,23 @@
     QueueType,
     Resource,
     ResourceData,
     TextFieldData,
     TextFieldExtractedData,
 )
 from nucliadb_models.search import ResourceProperties
-from nucliadb_models.vectors import UserVectorSet
+from nucliadb_models.security import ResourceSecurity
 from nucliadb_utils.utilities import get_storage
 
 
 async def set_resource_field_extracted_data(
     field: Field,
     field_data: ExtractedDataType,
     field_type_name: FieldTypeName,
-    wanted_extracted_data: List[ExtractedDataTypeName],
+    wanted_extracted_data: list[ExtractedDataTypeName],
 ) -> None:
     if field_data is None:
         return
 
     if ExtractedDataTypeName.TEXT in wanted_extracted_data:
         data_et = await field.get_extracted_text()
         if data_et is not None:
@@ -92,18 +92,18 @@
             )
 
     if ExtractedDataTypeName.VECTOR in wanted_extracted_data:
         data_vec = await field.get_vectors()
         if data_vec is not None:
             field_data.vectors = models.VectorObject.from_message(data_vec)
 
-    if ExtractedDataTypeName.USERVECTORS in wanted_extracted_data:
-        user_data_vec = await field.get_user_vectors()
-        if user_data_vec is not None:
-            field_data.uservectors = UserVectorSet.from_message(user_data_vec)
+    if ExtractedDataTypeName.QA in wanted_extracted_data:
+        qa = await field.get_question_answers()
+        if qa is not None:
+            field_data.question_answers = models.QuestionAnswers.from_message(qa)
 
     if (
         isinstance(field, File)
         and isinstance(field_data, FileFieldExtractedData)
         and ExtractedDataTypeName.FILE in wanted_extracted_data
     ):
         data_fed = await field.get_file_extracted_data()
@@ -119,28 +119,48 @@
         if data_led is not None:
             field_data.link = models.LinkExtractedData.from_message(data_led)
 
 
 async def serialize(
     kbid: str,
     rid: Optional[str],
-    show: List[ResourceProperties],
-    field_type_filter: List[FieldTypeName],
-    extracted: List[ExtractedDataTypeName],
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    extracted: list[ExtractedDataTypeName],
     service_name: Optional[str] = None,
     slug: Optional[str] = None,
 ) -> Optional[Resource]:
-    driver = await get_driver()
-    txn = await driver.begin()
+    driver = get_driver()
+    async with driver.transaction(wait_for_abort=False, read_only=True) as txn:
+        return await managed_serialize(
+            txn,
+            kbid,
+            rid,
+            show,
+            field_type_filter,
+            extracted,
+            service_name=service_name,
+            slug=slug,
+        )
 
+
+async def managed_serialize(
+    txn: Transaction,
+    kbid: str,
+    rid: Optional[str],
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    extracted: list[ExtractedDataTypeName],
+    service_name: Optional[str] = None,
+    slug: Optional[str] = None,
+) -> Optional[Resource]:
     orm_resource = await get_orm_resource(
         txn, kbid, rid=rid, slug=slug, service_name=service_name
     )
     if orm_resource is None:
-        await txn.abort()
         return None
 
     resource = Resource(id=orm_resource.uuid)
 
     include_values = ResourceProperties.VALUES in show
 
     include_extracted_data = (
@@ -203,16 +223,28 @@
             ]
 
     if ResourceProperties.ORIGIN in show:
         await orm_resource.get_origin()
         if orm_resource.origin is not None:
             resource.origin = models.Origin.from_message(orm_resource.origin)
 
+    if ResourceProperties.EXTRA in show:
+        await orm_resource.get_extra()
+        if orm_resource.extra is not None:
+            resource.extra = models.Extra.from_message(orm_resource.extra)
+
     include_errors = ResourceProperties.ERRORS in show
 
+    if ResourceProperties.SECURITY in show:
+        await orm_resource.get_security()
+        resource.security = ResourceSecurity(access_groups=[])
+        if orm_resource.security is not None:
+            for gid in orm_resource.security.access_groups:
+                resource.security.access_groups.append(gid)
+
     if field_type_filter and (include_values or include_extracted_data):
         await orm_resource.get_fields()
         resource.data = ResourceData()
         for (field_type, field_id), field in orm_resource.fields.items():
             field_type_name = FIELD_TYPES_MAP[field_type]
             if field_type_name not in field_type_filter:
                 continue
@@ -224,17 +256,20 @@
 
             if field_type_name is FieldTypeName.TEXT:
                 if resource.data.texts is None:
                     resource.data.texts = {}
                 if field.id not in resource.data.texts:
                     resource.data.texts[field.id] = TextFieldData()
                 if include_value:
-                    resource.data.texts[field.id].value = models.FieldText.from_message(
-                        value  # type: ignore
+                    serialized_value = (
+                        models.FieldText.from_message(value)
+                        if value is not None
+                        else None
                     )
+                    resource.data.texts[field.id].value = serialized_value
                 if include_errors:
                     error = await field.get_error()
                     if error is not None:
                         resource.data.texts[field.id].error = Error(
                             body=error.error, code=error.code
                         )
                 if include_extracted_data:
@@ -247,17 +282,22 @@
                     )
             elif field_type_name is FieldTypeName.FILE:
                 if resource.data.files is None:
                     resource.data.files = {}
                 if field.id not in resource.data.files:
                     resource.data.files[field.id] = FileFieldData()
                 if include_value:
-                    resource.data.files[field.id].value = models.FieldFile.from_message(
-                        value  # type: ignore
-                    )
+                    if value is not None:
+                        resource.data.files[
+                            field.id
+                        ].value = models.FieldFile.from_message(
+                            value  # type: ignore
+                        )
+                    else:
+                        resource.data.files[field.id].value = None
 
                 if include_errors:
                     error = await field.get_error()
                     if error is not None:
                         resource.data.files[field.id].error = Error(
                             body=error.error, code=error.code
                         )
@@ -309,17 +349,17 @@
                 if include_errors:
                     error = await field.get_error()
                     if error is not None:
                         resource.data.layouts[field.id].error = Error(
                             body=error.error, code=error.code
                         )
                 if include_extracted_data:
-                    resource.data.layouts[
-                        field.id
-                    ].extracted = LayoutFieldExtractedData()
+                    resource.data.layouts[field.id].extracted = (
+                        LayoutFieldExtractedData()
+                    )
                     await set_resource_field_extracted_data(
                         field,
                         resource.data.layouts[field.id].extracted,
                         field_type_name,
                         extracted,
                     )
             elif field_type_name is FieldTypeName.CONVERSATION:
@@ -331,21 +371,21 @@
                     error = await field.get_error()
                     if error is not None:
                         resource.data.conversations[field.id].error = Error(
                             body=error.error, code=error.code
                         )
                 if include_value and isinstance(field, Conversation):
                     value = await field.get_metadata()
-                    resource.data.conversations[
-                        field.id
-                    ].value = models.FieldConversation.from_message(value)
+                    resource.data.conversations[field.id].value = (
+                        models.FieldConversation.from_message(value)
+                    )
                 if include_extracted_data:
-                    resource.data.conversations[
-                        field.id
-                    ].extracted = ConversationFieldExtractedData()
+                    resource.data.conversations[field.id].extracted = (
+                        ConversationFieldExtractedData()
+                    )
                     await set_resource_field_extracted_data(
                         field,
                         resource.data.conversations[field.id].extracted,
                         field_type_name,
                         extracted,
                     )
             elif field_type_name is FieldTypeName.DATETIME:
@@ -362,17 +402,17 @@
                 if include_value:
                     resource.data.datetimes[
                         field.id
                     ].value = models.FieldDatetime.from_message(
                         value  # type: ignore
                     )
                 if include_extracted_data:
-                    resource.data.datetimes[
-                        field.id
-                    ].extracted = DatetimeFieldExtractedData()
+                    resource.data.datetimes[field.id].extracted = (
+                        DatetimeFieldExtractedData()
+                    )
                     await set_resource_field_extracted_data(
                         field,
                         resource.data.datetimes[field.id].extracted,
                         field_type_name,
                         extracted,
                     )
             elif field_type_name is FieldTypeName.KEYWORDSET:
@@ -389,17 +429,17 @@
                 if include_value:
                     resource.data.keywordsets[
                         field.id
                     ].value = models.FieldKeywordset.from_message(
                         value  # type: ignore
                     )
                 if include_extracted_data:
-                    resource.data.keywordsets[
-                        field.id
-                    ].extracted = KeywordsetFieldExtractedData()
+                    resource.data.keywordsets[field.id].extracted = (
+                        KeywordsetFieldExtractedData()
+                    )
                     await set_resource_field_extracted_data(
                         field,
                         resource.data.keywordsets[field.id].extracted,
                         field_type_name,
                         extracted,
                     )
             elif field_type_name is FieldTypeName.GENERIC:
@@ -417,15 +457,14 @@
                         )
                 if include_extracted_data:
                     resource.data.generics[field.id].extracted = TextFieldExtractedData(
                         text=models.ExtractedText(
                             text=resource.data.generics[field.id].value
                         )
                     )
-    await txn.abort()
     return resource
 
 
 async def get_orm_resource(
     txn: Transaction,
     kbid: str,
     rid: Optional[str],
@@ -452,11 +491,11 @@
     return orm_resource
 
 
 async def get_resource_uuid_by_slug(
     kbid: str, slug: str, service_name: Optional[str] = None
 ) -> Optional[str]:
     storage = await get_storage(service_name=service_name)
-    driver = await get_driver()
-    txn = await driver.begin()
-    kb = KnowledgeBox(txn, storage, kbid)
-    return await kb.get_resource_uuid_by_slug(slug)
+    driver = get_driver()
+    async with driver.transaction() as txn:
+        kb = KnowledgeBox(txn, storage, kbid)
+        return await kb.get_resource_uuid_by_slug(slug)
```

## nucliadb/ingest/settings.py

```diff
@@ -14,15 +14,15 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from enum import Enum
-from typing import Dict, List, Optional
+from typing import Optional
 
 from pydantic import BaseSettings, Field
 
 
 class DriverConfig(str, Enum):
     REDIS = "redis"
     TIKV = "tikv"
@@ -37,67 +37,62 @@
         """
         for member in cls:
             if member.value == value.lower():
                 return member
 
 
 class DriverSettings(BaseSettings):
-    driver: DriverConfig = Field(DriverConfig.NOT_SET, description="K/V storage driver")
-    driver_redis_url: Optional[str] = None
-    driver_tikv_url: Optional[List[str]] = []
-    driver_local_url: Optional[str] = None
-    driver_pg_url: Optional[str] = None
+    driver: DriverConfig = Field(
+        default=DriverConfig.NOT_SET, description="K/V storage driver"
+    )
+    driver_redis_url: Optional[str] = Field(
+        default=None, description="Redis URL. Example: redis://localhost:6379"
+    )
+    driver_tikv_url: Optional[list[str]] = Field(
+        default=None,
+        description=(
+            "TiKV PD (Placement Driver) URLs. The URL to the cluster manager of"
+            "TiKV. Example: '[\"tikv-pd.svc:2379\"]'"
+        ),
+    )
+    driver_local_url: Optional[str] = Field(
+        default=None,
+        description="Local path to store data on file system. Example: /nucliadb/data/main",
+    )
+    driver_pg_url: Optional[str] = Field(
+        default=None,
+        description="PostgreSQL DSN. The connection string to the PG server. Example: postgres://username:password@postgres:5432/nucliadb.",  # noqa
+    )
+    driver_pg_connection_pool_max_size: int = Field(
+        default=20,
+        description="PostgreSQL max pool size. The maximum number of connections to the PostgreSQL server.",
+    )
+    driver_tikv_connection_pool_size: int = Field(
+        default=3,
+        description="TiKV max pool size. The maximum number of connections to the TiKV server.",
+    )
 
 
 class Settings(DriverSettings):
     grpc_port: int = 8030
 
-    partitions: List[str] = ["1"]
+    partitions: list[str] = ["1"]
 
-    pull_time_error_backoff: int = 100
+    pull_time_error_backoff: int = 30
+    pull_api_timeout: int = 60
     disable_pull_worker: bool = False
 
+    # ingest consumer sts replica settings
     replica_number: int = -1
-    total_replicas: int = 1
+    total_replicas: int = 1  # number of ingest processor replicas in the cluster
     nuclia_partitions: int = 50
 
-    # NODE INFORMATION
-
-    node_replicas: int = 2  # TODO discuss default value
-
-    chitchat_binding_host: str = "0.0.0.0"
-    chitchat_binding_port: int = 31337
-    chitchat_enabled: bool = True
-
-    # chitchat_peers_addr: List[str] = ["localhost:3001"] # TODO is it seed list?
-    # swim_host_key: str = "ingest.key" # TODO: ask if it's swim specific keys?
-
-    logging_config: Optional[str] = None
-
-    node_writer_port: int = 10000
-    node_reader_port: int = 10001
-    node_sidecar_port: int = 10002
-
-    # Only for testing proposes
-    writer_port_map: Dict[str, int] = {}
-    reader_port_map: Dict[str, int] = {}
-    sidecar_port_map: Dict[str, int] = {}
-
-    # Node limits
-    max_shard_fields: int = 200000  # max number of fields to target per shard
-    max_node_replicas: int = (
-        600  # max number of shard replicas a single node will manage
-    )
-
-    local_reader_threads: int = 5
-    local_writer_threads: int = 5
-
     max_receive_message_length: int = 4
 
     # Search query timeouts
     relation_search_timeout: float = 10.0
     relation_types_timeout: float = 10.0
 
-    nodes_load_ingest: bool = False
+    max_concurrent_ingest_processing: int = 5
 
 
 settings = Settings()
```

## nucliadb/ingest/utils.py

```diff
@@ -17,114 +17,36 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from typing import Optional
 
 from nucliadb_protos.writer_pb2_grpc import WriterStub
 
-from nucliadb.ingest.chitchat import ChitchatMonitor
-from nucliadb.ingest.maindb.driver import Driver
-from nucliadb.ingest.settings import settings
-from nucliadb_utils.exceptions import ConfigurationError
+from nucliadb.common.maindb.utils import setup_driver
 from nucliadb_utils.grpc import get_traced_grpc_channel
 from nucliadb_utils.settings import nucliadb_settings
-from nucliadb_utils.store import MAIN
 from nucliadb_utils.utilities import Utility, clean_utility, get_utility, set_utility
 
-try:
-    from nucliadb.ingest.maindb.redis import RedisDriver
-
-    REDIS = True
-except ImportError:  # pragma: no cover
-    REDIS = False
-
-try:
-    from nucliadb.ingest.maindb.tikv import TiKVDriver
-
-    TIKV = True
-except ImportError:  # pragma: no cover
-    TIKV = False
-
-
-try:
-    from nucliadb.ingest.maindb.pg import PGDriver
-
-    PG = True
-except ImportError:  # pragma: no cover
-    PG = False
-
-try:
-    from nucliadb.ingest.maindb.local import LocalDriver
-
-    FILES = True
-except ImportError:  # pragma: no cover
-    FILES = False
-
-_DRIVER_UTIL_NAME = "driver"
-
-
-async def get_driver() -> Driver:
-    if _DRIVER_UTIL_NAME in MAIN:
-        return MAIN[_DRIVER_UTIL_NAME]
-
-    if settings.driver == "redis":
-        if not REDIS:
-            raise ConfigurationError("`redis` python package not installed.")
-        if settings.driver_redis_url is None:
-            raise ConfigurationError("No DRIVER_REDIS_URL env var defined.")
-
-        redis_driver = RedisDriver(settings.driver_redis_url)
-        MAIN[_DRIVER_UTIL_NAME] = redis_driver
-    elif settings.driver == "tikv":
-        if not TIKV:
-            raise ConfigurationError("`tikv_client` python package not installed.")
-        if settings.driver_tikv_url is None:
-            raise ConfigurationError("No DRIVER_TIKV_URL env var defined.")
-
-        tikv_driver = TiKVDriver(settings.driver_tikv_url)
-        MAIN[_DRIVER_UTIL_NAME] = tikv_driver
-    elif settings.driver == "pg":
-        if not PG:
-            raise ConfigurationError("`asyncpg` python package not installed.")
-        if settings.driver_pg_url is None:
-            raise ConfigurationError("No DRIVER_PG_URL env var defined.")
-        pg_driver = PGDriver(settings.driver_pg_url)
-        MAIN[_DRIVER_UTIL_NAME] = pg_driver
-    elif settings.driver == "local":
-        if not FILES:
-            raise ConfigurationError("`aiofiles` python package not installed.")
-        if settings.driver_local_url is None:
-            raise ConfigurationError("No DRIVER_LOCAL_URL env var defined.")
-
-        local_driver = LocalDriver(settings.driver_local_url)
-        MAIN[_DRIVER_UTIL_NAME] = local_driver
-    else:
-        raise ConfigurationError(
-            f"Invalid DRIVER defined configured: {settings.driver}"
-        )
-
-    driver: Driver = MAIN[_DRIVER_UTIL_NAME]
-    if not driver.initialized:
-        await driver.initialize()
-    return driver
-
 
 async def start_ingest(service_name: Optional[str] = None):
+    await setup_driver()
+
     actual_service = get_utility(Utility.INGEST)
     if actual_service is not None:
         return
 
     if nucliadb_settings.nucliadb_ingest is not None:
         # Its distributed lets create a GRPC client
         # We want Jaeger telemetry enabled
         channel = get_traced_grpc_channel(
             nucliadb_settings.nucliadb_ingest, service_name or "ingest"
         )
         set_utility(Utility.CHANNEL, channel)
-        set_utility(Utility.INGEST, WriterStub(channel))
+        ingest = WriterStub(channel)  # type: ignore
+        set_utility(Utility.INGEST, ingest)
     else:
         # Its not distributed create a ingest
         from nucliadb.ingest.service.writer import WriterServicer
 
         service = WriterServicer()
         await service.initialize()
         set_utility(Utility.INGEST, service)
@@ -134,11 +56,7 @@
     if get_utility(Utility.CHANNEL):
         await get_utility(Utility.CHANNEL).close()
         clean_utility(Utility.CHANNEL)
         clean_utility(Utility.INGEST)
     if get_utility(Utility.INGEST):
         util = get_utility(Utility.INGEST)
         await util.finalize()
-
-
-def get_chitchat() -> Optional[ChitchatMonitor]:
-    return get_utility(Utility.CHITCHAT)
```

## nucliadb/ingest/consumer/auditing.py

```diff
@@ -19,25 +19,27 @@
 #
 
 import asyncio
 import logging
 import uuid
 from functools import partial
 
-from nucliadb.ingest.maindb.driver import Driver
+from nucliadb_protos.resources_pb2 import FieldType
+
+from nucliadb.common.cluster.exceptions import ShardsNotFound
+from nucliadb.common.cluster.manager import choose_node
+from nucliadb.common.cluster.utils import get_shard_manager
+from nucliadb.common.maindb.driver import Driver
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
-from nucliadb.ingest.orm.nodes_manager import NodesManager
 from nucliadb.ingest.orm.resource import Resource
-from nucliadb_protos import audit_pb2, noderesources_pb2, nodesidecar_pb2, writer_pb2
+from nucliadb_protos import audit_pb2, nodereader_pb2, noderesources_pb2, writer_pb2
 from nucliadb_utils import const
 from nucliadb_utils.audit.audit import AuditStorage
 from nucliadb_utils.cache.pubsub import PubSubDriver
-from nucliadb_utils.exceptions import ShardsNotFound
 from nucliadb_utils.storages.storage import Storage
-from nucliadb_utils.utilities import has_feature
 
 from . import metrics
 from .utils import DelayedTaskHandler
 
 logger = logging.getLogger(__name__)
 
 
@@ -64,15 +66,15 @@
         audit: AuditStorage,
         pubsub: PubSubDriver,
         check_delay: float = 5.0,
     ):
         self.driver = driver
         self.audit = audit
         self.pubsub = pubsub
-        self.node_manager = NodesManager(driver)
+        self.shard_manager = get_shard_manager()
         self.task_handler = DelayedTaskHandler(check_delay)
 
     async def initialize(self) -> None:
         self.loop = asyncio.get_running_loop()
         self.subscription_id = str(uuid.uuid4())
         await self.task_handler.initialize()
         await self.pubsub.subscribe(
@@ -100,45 +102,34 @@
             notification.kbid, partial(self.process_kb, notification.kbid)
         )
         metrics.total_messages.inc({"action": "scheduled", "type": "audit_counter"})
 
     @metrics.handler_histo.wrap({"type": "audit_counter"})
     async def process_kb(self, kbid: str) -> None:
         try:
-            shard_groups: list[
-                writer_pb2.ShardObject
-            ] = await self.node_manager.get_shards_by_kbid(kbid)
+            shard_groups: list[writer_pb2.ShardObject] = (
+                await self.shard_manager.get_shards_by_kbid(kbid)
+            )
         except ShardsNotFound:
             logger.warning(f"No shards found for kbid {kbid}, skipping")
             return
 
         logger.info({"message": "Processing counter audit for kbid", "kbid": kbid})
 
         total_fields = 0
         total_paragraphs = 0
 
         for shard_obj in shard_groups:
-            node, shard_id, _ = self.node_manager.choose_node(shard_obj)
-            shard_counter: nodesidecar_pb2.Counter = await node.sidecar.GetCount(
-                noderesources_pb2.ShardId(id=shard_id)  # type: ignore
+            node, shard_id = choose_node(shard_obj)
+            shard: nodereader_pb2.Shard = await node.reader.GetShard(
+                nodereader_pb2.GetShardRequest(shard_id=noderesources_pb2.ShardId(id=shard_id))  # type: ignore
             )
 
-            total_fields += shard_counter.resources
-            total_paragraphs += shard_counter.paragraphs
-
-            if has_feature(const.Features.AUDITING_BW_COMPAT_SHARD_COUNTER):
-                await self.audit.report(
-                    kbid=kbid,
-                    audit_type=audit_pb2.AuditRequest.AuditType.MODIFIED,
-                    counter=audit_pb2.AuditShardCounter(
-                        shard=shard_obj.shard,
-                        fields=shard_counter.resources,
-                        paragraphs=shard_counter.paragraphs,
-                    ),
-                )
+            total_fields += shard.fields
+            total_paragraphs += shard.paragraphs
 
         await self.audit.report(
             kbid=kbid,
             audit_type=audit_pb2.AuditRequest.AuditType.INDEXED,
             kb_counter=audit_pb2.AuditKBCounter(
                 fields=total_fields, paragraphs=total_paragraphs
             ),
@@ -161,30 +152,31 @@
         audit: AuditStorage,
         pubsub: PubSubDriver,
     ):
         self.driver = driver
         self.storage = storage
         self.audit = audit
         self.pubsub = pubsub
-        self.node_manager = NodesManager(driver)
 
     async def initialize(self) -> None:
         self.subscription_id = str(uuid.uuid4())
         await self.pubsub.subscribe(
             handler=self.handle_message,
             key=const.PubSubChannels.RESOURCE_NOTIFY.format(kbid="*"),
             group="audit-writes",
             subscription_id=self.subscription_id,
         )
 
     async def finalize(self) -> None:
         await self.pubsub.unsubscribe(self.subscription_id)
 
     def iterate_auditable_fields(
-        self, resource_keys: list[tuple[int, str]], message: writer_pb2.BrokerMessage
+        self,
+        resource_keys: list[tuple[FieldType.ValueType, str]],
+        message: writer_pb2.BrokerMessage,
     ):
         """
         Generator that emits the combined list of field ids from both
         the existing resource and message that needs to be considered
         in the audit of fields.
         """
         yielded = set()
@@ -240,82 +232,84 @@
                 continue
 
             yield (field_id, field_type)
 
     async def collect_audit_fields(
         self, message: writer_pb2.BrokerMessage
     ) -> list[audit_pb2.AuditField]:
+        if message.type == writer_pb2.BrokerMessage.MessageType.DELETE:
+            # If we are fully deleting a resource we won't iterate the delete_fields (if any).
+            # Make no sense as we already collected all resource fields as deleted
+            return []
+
         audit_storage_fields: list[audit_pb2.AuditField] = []
         async with self.driver.transaction() as txn:
             kb = KnowledgeBox(txn, self.storage, message.kbid)
             resource = Resource(txn, self.storage, kb, message.uuid)
             field_keys = await resource.get_fields_ids()
 
-            if message.type != writer_pb2.BrokerMessage.MessageType.DELETE:
-                for field_id, field_type in self.iterate_auditable_fields(
-                    field_keys, message
-                ):
-                    auditfield = audit_pb2.AuditField()
-                    auditfield.field_type = field_type
-                    auditfield.field_id = field_id
-                    if field_type is writer_pb2.FieldType.FILE:
-                        auditfield.filename = message.files[field_id].file.filename
-                    # The field did exist, so we are overwriting it, with a modified file
-                    # in case of a file
-                    auditfield.action = audit_pb2.AuditField.FieldAction.MODIFIED
-                    if field_type is writer_pb2.FieldType.FILE:
-                        auditfield.size = message.files[field_id].file.size
-
-                    audit_storage_fields.append(auditfield)
-
-            if (
-                message.delete_fields
-                and message.type is not writer_pb2.BrokerMessage.MessageType.DELETE
+            for field_id, field_type in self.iterate_auditable_fields(
+                field_keys, message
             ):
-                # If we are fully deleting a resource we won't iterate the delete_fields (if any)
-                # Make no sense as we already collected all resource fields as deleted
-                for fieldid in message.delete_fields:
-                    field = await resource.get_field(
-                        fieldid.field, writer_pb2.FieldType.FILE, load=True
-                    )
-                    audit_field = audit_pb2.AuditField()
-                    audit_field.action = audit_pb2.AuditField.FieldAction.DELETED
-                    audit_field.field_id = fieldid.field
-                    audit_field.field_type = fieldid.field_type
-                    if fieldid.field_type is writer_pb2.FieldType.FILE:
-                        val = await field.get_value()
-                        audit_field.size = 0
-                        if val is not None:
-                            audit_field.filename = val.file.filename
-                    audit_storage_fields.append(audit_field)
+                auditfield = audit_pb2.AuditField()
+                auditfield.field_type = field_type
+                auditfield.field_id = field_id
+                if field_type is writer_pb2.FieldType.FILE:
+                    auditfield.filename = message.files[field_id].file.filename
+                # The field did exist, so we are overwriting it, with a modified file
+                # in case of a file
+                auditfield.action = audit_pb2.AuditField.FieldAction.MODIFIED
+                if field_type is writer_pb2.FieldType.FILE:
+                    auditfield.size = message.files[field_id].file.size
+
+                audit_storage_fields.append(auditfield)
+
+            for fieldid in message.delete_fields or []:
+                field = await resource.get_field(
+                    fieldid.field, writer_pb2.FieldType.FILE, load=True
+                )
+                audit_field = audit_pb2.AuditField()
+                audit_field.action = audit_pb2.AuditField.FieldAction.DELETED
+                audit_field.field_id = fieldid.field
+                audit_field.field_type = fieldid.field_type
+                if fieldid.field_type is writer_pb2.FieldType.FILE:
+                    val = await field.get_value()
+                    audit_field.size = 0
+                    if val is not None:
+                        audit_field.filename = val.file.filename
+                audit_storage_fields.append(audit_field)
 
         return audit_storage_fields
 
     async def handle_message(self, raw_data) -> None:
         data = self.pubsub.parse(raw_data)
         notification = writer_pb2.Notification()
         notification.ParseFromString(data)
 
         if notification.write_type == notification.WriteType.UNSET:
             metrics.total_messages.inc({"action": "ignored", "type": "audit_fields"})
             return
 
+        message = notification.message
+        if message.source == message.MessageSource.PROCESSOR:
+            metrics.total_messages.inc({"action": "ignored", "type": "audit_fields"})
+            return
+
         logger.info(
             {"message": "Processing field audit for kbid", "kbid": notification.kbid}
         )
 
         metrics.total_messages.inc({"action": "scheduled", "type": "audit_fields"})
         with metrics.handler_histo({"type": "audit_fields"}):
-            message = notification.message
             audit_fields = await self.collect_audit_fields(message)
             field_metadata = [fi.field for fi in message.field_metadata]
-
+            when = message.audit.when if message.audit.HasField("when") else None
             await self.audit.report(
                 kbid=message.kbid,
-                when=message.audit.when,
+                when=when,
                 user=message.audit.user,
                 rid=message.uuid,
                 origin=message.audit.origin,
                 field_metadata=field_metadata,
                 audit_type=AUDIT_TYPES.get(notification.write_type),
                 audit_fields=audit_fields,
             )
```

## nucliadb/ingest/consumer/consumer.py

```diff
@@ -16,32 +16,33 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
 import logging
 import time
-from typing import Optional
+from typing import Optional, Union
 
+import backoff
 import nats
 import nats.js.api
 from nats.aio.client import Msg
-from nucliadb_protos.writer_pb2 import BrokerMessage
+from nucliadb_protos.writer_pb2 import BrokerMessage, BrokerMessageBlobReference
 
+from nucliadb.common.cluster.exceptions import ShardsNotFound
+from nucliadb.common.maindb.driver import Driver
+from nucliadb.common.maindb.exceptions import ConflictError
 from nucliadb.ingest import logger
-from nucliadb.ingest.maindb.driver import Driver
 from nucliadb.ingest.orm.exceptions import DeadletteredError, SequenceOrderViolation
 from nucliadb.ingest.orm.processor import Processor, sequence_manager
-from nucliadb_telemetry import errors, metrics
-from nucliadb_telemetry.utils import set_info_on_span
+from nucliadb_telemetry import context, errors, metrics
 from nucliadb_utils import const
-from nucliadb_utils.cache import KB_COUNTER_CACHE
-from nucliadb_utils.cache.utility import Cache
-from nucliadb_utils.exceptions import ShardsNotFound
-from nucliadb_utils.nats import NatsConnectionManager
+from nucliadb_utils.cache.pubsub import PubSubDriver
+from nucliadb_utils.nats import MessageProgressUpdater, NatsConnectionManager
+from nucliadb_utils.settings import nats_consumer_settings
 from nucliadb_utils.storages.storage import Storage
 
 consumer_observer = metrics.Observer(
     "message_processor",
     labels={"source": ""},
     buckets=[
         0.01,
@@ -67,25 +68,25 @@
 class IngestConsumer:
     def __init__(
         self,
         driver: Driver,
         partition: str,
         storage: Storage,
         nats_connection_manager: NatsConnectionManager,
-        cache: Optional[Cache] = None,
+        pubsub: Optional[PubSubDriver] = None,
+        lock: Optional[Union[asyncio.Lock, asyncio.Semaphore]] = None,
     ):
         self.driver = driver
         self.partition = partition
-        self.cache = cache
         self.nats_connection_manager = nats_connection_manager
-        self.ack_wait = 10 * 60
+        self.storage = storage
         self.initialized = False
 
-        self.lock = asyncio.Lock()
-        self.processor = Processor(driver, storage, cache, partition)
+        self.lock = lock or asyncio.Lock()
+        self.processor = Processor(driver, storage, pubsub, partition)
 
     async def initialize(self):
         await self.setup_nats_subscription()
         self.initialized = True
 
     async def setup_nats_subscription(self):
         last_seqid = await sequence_manager.get_last_seqid(self.driver, self.partition)
@@ -99,63 +100,88 @@
             flow_control=True,
             cb=self.subscription_worker,
             subscription_lost_cb=self.setup_nats_subscription,
             config=nats.js.api.ConsumerConfig(
                 deliver_policy=nats.js.api.DeliverPolicy.BY_START_SEQUENCE,
                 opt_start_seq=last_seqid,
                 ack_policy=nats.js.api.AckPolicy.EXPLICIT,
-                # Read about message ordering:
-                #   https://docs.nats.io/nats-concepts/subject_mapping#when-is-deterministic-partitioning-needed
-                max_ack_pending=1,  # required for strict message ordering
-                max_deliver=10000,
-                ack_wait=self.ack_wait,
-                idle_heartbeat=5.0,
+                max_ack_pending=nats_consumer_settings.nats_max_ack_pending,
+                max_deliver=nats_consumer_settings.nats_max_deliver,
+                ack_wait=nats_consumer_settings.nats_ack_wait,
+                idle_heartbeat=nats_consumer_settings.nats_idle_heartbeat,
             ),
         )
         logger.info(
             f"Subscribed to {subject} on stream {const.Streams.INGEST.name} from {last_seqid}"
         )
 
+    @backoff.on_exception(
+        backoff.expo, (ConflictError,), jitter=backoff.random_jitter, max_tries=4
+    )
     async def _process(self, pb: BrokerMessage, seqid: int):
         await self.processor.process(pb, seqid, self.partition)
 
+    async def get_broker_message(self, msg: Msg) -> BrokerMessage:
+        pb_data = msg.data
+        if msg.headers is not None and msg.headers.get("X-MESSAGE-TYPE") == "PROXY":
+            # this is a message that is referencing a blob because
+            # it was too big to be sent through NATS
+            ref_pb = BrokerMessageBlobReference()
+            ref_pb.ParseFromString(pb_data)
+            pb_data = await self.storage.get_stream_message(ref_pb.storage_key)
+        pb = BrokerMessage()
+        pb.ParseFromString(pb_data)
+        return pb
+
+    async def clean_broker_message(self, msg: Msg) -> None:
+        if msg.headers is not None and msg.headers.get("X-MESSAGE-TYPE") == "PROXY":
+            ref_pb = BrokerMessageBlobReference()
+            ref_pb.ParseFromString(msg.data)
+            try:
+                await self.storage.del_stream_message(ref_pb.storage_key)
+            except Exception:  # pragma: no cover
+                logger.warning("Could not delete blob reference", exc_info=True)
+
     async def subscription_worker(self, msg: Msg):
         subject = msg.subject
         reply = msg.reply
         seqid = int(reply.split(".")[5])
-        logger.info(
-            f"Message received: subject:{subject}, seqid: {seqid}, reply: {reply}"
-        )
         message_source = "<msg source not set>"
         start = time.monotonic()
 
-        async with self.lock:
+        async with MessageProgressUpdater(
+            msg, nats_consumer_settings.nats_ack_wait * 0.66
+        ), self.lock:
+            logger.info(
+                f"Message processing: subject:{subject}, seqid: {seqid}, reply: {reply}"
+            )
             try:
-                pb = BrokerMessage()
-                pb.ParseFromString(msg.data)
+                pb = await self.get_broker_message(msg)
                 if pb.source == pb.MessageSource.PROCESSOR:
                     message_source = "processing"
                 elif pb.source == pb.MessageSource.WRITER:
                     message_source = "writer"
                 if pb.HasField("audit"):
                     audit_time = pb.audit.when.ToDatetime().isoformat()
                 else:
                     audit_time = ""
 
                 logger.debug(
                     f"Received from {message_source} on {pb.kbid}/{pb.uuid} seq {seqid} partition {self.partition} at {time}"  # noqa
                 )
-                set_info_on_span({"nuclia.kbid": pb.kbid, "nuclia.rid": pb.uuid})
+                context.add_context({"kbid": pb.kbid, "rid": pb.uuid})
 
                 try:
                     with consumer_observer(
                         {
-                            "source": "writer"
-                            if pb.source == pb.MessageSource.WRITER
-                            else "processor"
+                            "source": (
+                                "writer"
+                                if pb.source == pb.MessageSource.WRITER
+                                else "processor"
+                            )
                         }
                     ):
                         await self._process(pb, seqid)
                 except SequenceOrderViolation as err:
                     log_func = logger.error
                     if seqid == err.last_seqid:  # pragma: no cover
                         # Occasional retries of the last processed message may happen
@@ -172,18 +198,14 @@
                     logger.log(
                         log_level,
                         f"Successfully processed {message_type_name} message from \
                             {message_source}. kb: {pb.kbid}, resource: {pb.uuid}, \
                                 nucliadb seqid: {seqid}, partition: {self.partition} as {audit_time}, \
                                     total time: {time_to_process:.2f}s",
                     )
-                    if self.cache is not None:
-                        await self.cache.delete(
-                            KB_COUNTER_CACHE.format(kbid=pb.kbid), invalidate=True
-                        )
             except DeadletteredError as e:
                 # Messages that have been sent to deadletter at some point
                 # We don't want to process it again so it's ack'd
                 errors.capture_exception(e)
                 logger.info(
                     f"An error happend while processing a message from {message_source}. "
                     f"A copy of the message has been stored on {self.processor.storage.deadletter_bucket}. "
@@ -200,24 +222,25 @@
                     f"This message has been dropped and won't be retried again"
                     f"Check sentry for more details: {str(e)}"
                 )
                 await msg.ack()
             except Exception as e:
                 # Unhandled exceptions that need to be retried after a small delay
                 errors.capture_exception(e)
-                logger.info(
+                logger.exception(
                     f"An error happend while processing a message from {message_source}. "
                     "Message has not been ACKd and will be retried. "
                     f"Check sentry for more details: {str(e)}"
                 )
-                await asyncio.sleep(2)
+                await msg.nak()
                 raise e
             else:
                 # Successful processing
                 await msg.ack()
+                await self.clean_broker_message(msg)
 
 
 class IngestProcessedConsumer(IngestConsumer):
     """
     Consumer designed to write processed resources to the database.
 
     This is so that we can have a single consumer for both the regular writer and writes
@@ -234,23 +257,26 @@
             queue=const.Streams.INGEST_PROCESSED.group,
             stream=const.Streams.INGEST_PROCESSED.name,
             flow_control=True,
             cb=self.subscription_worker,
             subscription_lost_cb=self.setup_nats_subscription,
             config=nats.js.api.ConsumerConfig(
                 ack_policy=nats.js.api.AckPolicy.EXPLICIT,
-                max_ack_pending=10,
-                max_deliver=10000,
-                ack_wait=self.ack_wait,
-                idle_heartbeat=5.0,
+                max_ack_pending=100,  # custom ack pending here
+                max_deliver=nats_consumer_settings.nats_max_deliver,
+                ack_wait=nats_consumer_settings.nats_ack_wait,
+                idle_heartbeat=nats_consumer_settings.nats_idle_heartbeat,
             ),
         )
         logger.info(
             f"Subscribed to {subject} on stream {const.Streams.INGEST_PROCESSED.name}"
         )
 
+    @backoff.on_exception(
+        backoff.expo, (ConflictError,), jitter=backoff.random_jitter, max_tries=4
+    )
     async def _process(self, pb: BrokerMessage, seqid: int):
         """
         We are setting `transaction_check` to False here because we can not mix
         transaction ids from regular ingest writes and writes coming from processor.
         """
         await self.processor.process(pb, seqid, self.partition, transaction_check=False)
```

## nucliadb/ingest/consumer/pull.py

```diff
@@ -15,81 +15,103 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
 import base64
-from typing import List, Optional
+from typing import Optional
 
 import nats
+import nats.errors
 from aiohttp.client_exceptions import ClientConnectorError
-from nats.aio.subscription import Subscription
-from nucliadb_protos.writer_pb2 import BrokerMessage
+from nucliadb_protos.writer_pb2 import BrokerMessage, BrokerMessageBlobReference
 
-from nucliadb.http_clients.processing import ProcessingHTTPClient
+from nucliadb.common import datamanagers
+from nucliadb.common.http_clients.processing import ProcessingHTTPClient, get_nua_api_id
+from nucliadb.common.maindb.driver import Driver
 from nucliadb.ingest import logger, logger_activity
-from nucliadb.ingest.maindb.driver import Driver
 from nucliadb.ingest.orm.exceptions import ReallyStopPulling
 from nucliadb.ingest.orm.processor import Processor
 from nucliadb_telemetry import errors
 from nucliadb_utils import const
-from nucliadb_utils.cache.utility import Cache
+from nucliadb_utils.cache.pubsub import PubSubDriver
 from nucliadb_utils.settings import nuclia_settings
 from nucliadb_utils.storages.storage import Storage
-from nucliadb_utils.utilities import get_transaction_utility, has_feature
+from nucliadb_utils.utilities import get_storage, get_transaction_utility
 
 
 class PullWorker:
     """
     The pull worker is responsible for pulling messages from the pull processing
     http endpoint and injecting them into the processing write queue.
 
     The processing pull endpoint is also described as the "processing proxy" at times.
     """
 
-    subscriptions: List[Subscription]
-
     def __init__(
         self,
         driver: Driver,
         partition: str,
         storage: Storage,
         pull_time_error_backoff: int,
-        cache: Optional[Cache] = None,
+        pubsub: Optional[PubSubDriver] = None,
         local_subscriber: bool = False,
         pull_time_empty_backoff: float = 5.0,
+        pull_api_timeout: int = 60,
     ):
         self.partition = partition
         self.pull_time_error_backoff = pull_time_error_backoff
         self.pull_time_empty_backoff = pull_time_empty_backoff
+        self.pull_api_timeout = pull_api_timeout
         self.local_subscriber = local_subscriber
-        self.cache = cache
 
-        self.processor = Processor(driver, storage, cache, partition)
+        self.processor = Processor(driver, storage, pubsub, partition)
+
+    def __str__(self) -> str:
+        return f"PullWorker(partition={self.partition})"
+
+    def __repr__(self) -> str:
+        return str(self)
 
     async def handle_message(self, payload: str) -> None:
         pb = BrokerMessage()
-        pb.ParseFromString(base64.b64decode(payload))
+        data = base64.b64decode(payload)
+        pb.ParseFromString(data)
 
         logger.debug(
             f"Resource: {pb.uuid} KB: {pb.kbid} ProcessingID: {pb.processing_id}"
         )
 
         if not self.local_subscriber:
             transaction_utility = get_transaction_utility()
             if transaction_utility is None:
                 raise Exception("No transaction utility defined")
-            subject = None
-            if has_feature(const.Features.SEPARATE_PROCESSED_MESSAGE_WRITES):
-                # send messsages to queue that is managed by separated consumer
-                subject = const.Streams.INGEST_PROCESSED.subject
-            await transaction_utility.commit(
-                writer=pb, partition=int(self.partition), target_subject=subject
-            )
+            try:
+                await transaction_utility.commit(
+                    writer=pb,
+                    partition=int(self.partition),
+                    # send to separate processor
+                    target_subject=const.Streams.INGEST_PROCESSED.subject,
+                )
+            except nats.errors.MaxPayloadError:
+                storage = await get_storage()
+                stored_key = await storage.set_stream_message(
+                    kbid=pb.kbid, rid=pb.uuid, data=data
+                )
+                referenced_pb = BrokerMessageBlobReference(
+                    uuid=pb.uuid, kbid=pb.kbid, storage_key=stored_key
+                )
+                await transaction_utility.commit(
+                    writer=referenced_pb,
+                    partition=int(self.partition),
+                    # send to separate processor
+                    target_subject=const.Streams.INGEST_PROCESSED.subject,
+                    headers={"X-MESSAGE-TYPE": "PROXY"},
+                )
         else:
             # No nats defined == monolitic nucliadb
             await self.processor.process(
                 pb,
                 0,  # Fake sequence id as in local mode there's no transactions
                 partition=self.partition,
                 transaction_check=False,
@@ -111,33 +133,65 @@
                 await asyncio.sleep(10)
 
     async def _loop(self):
         headers = {}
         data = None
         if nuclia_settings.nuclia_service_account is not None:
             headers["X-STF-NUAKEY"] = f"Bearer {nuclia_settings.nuclia_service_account}"
+            # parse jwt sub to get pull type id
+            try:
+                pull_type_id = get_nua_api_id()
+            except Exception as exc:
+                logger.exception(
+                    "Could not read NUA API Key. Can not start pull worker"
+                )
+                raise ReallyStopPulling() from exc
+        else:
+            pull_type_id = "main"
 
         async with ProcessingHTTPClient() as processing_http_client:
             logger.info(f"Collecting from NucliaDB Cloud {self.partition} partition")
-
             while True:
                 try:
-                    data = await processing_http_client.pull(self.partition)
+                    async with datamanagers.with_transaction() as txn:
+                        cursor = await datamanagers.processing.get_pull_offset(
+                            txn, pull_type_id=pull_type_id, partition=self.partition
+                        )
+
+                    data = await processing_http_client.pull(
+                        self.partition,
+                        cursor=cursor,
+                        timeout=self.pull_api_timeout,
+                    )
                     if data.status == "ok":
                         logger.info(
-                            f"Message received from proxy, partition: {self.partition}"
+                            "Message received from proxy",
+                            extra={"partition": self.partition, "cursor": data.cursor},
                         )
                         try:
-                            await self.handle_message(data.payload)
+                            if data.payload is not None:
+                                await self.handle_message(data.payload)
+                            for payload in data.payloads:
+                                # If using cursors and multiple messages are returned, it will be in the
+                                # `payloads` property
+                                await self.handle_message(payload)
                         except Exception as e:
                             errors.capture_exception(e)
                             logger.exception(
-                                "Error while pulling and processing message"
+                                "Error while pulling and processing message/s"
                             )
                             raise e
+                        async with datamanagers.with_transaction() as txn:
+                            await datamanagers.processing.set_pull_offset(
+                                txn,
+                                pull_type_id=pull_type_id,
+                                partition=self.partition,
+                                offset=data.cursor,
+                            )
+                            await txn.commit()
                     elif data.status == "empty":
                         logger_activity.debug(
                             f"No messages waiting in partition #{self.partition}"
                         )
                         await asyncio.sleep(self.pull_time_empty_backoff)
                     else:
                         logger.info(f"Proxy pull answered with error: {data}")
@@ -158,15 +212,17 @@
                         f"Could not connect to processing engine, \
                          {processing_http_client.base_url} verify your internet connection"
                     )
                     await asyncio.sleep(self.pull_time_error_backoff)
 
                 except nats.errors.MaxPayloadError as e:
                     if data is not None:
+                        payload_length = 0
+                        if data.payload:
+                            payload_length = len(base64.b64decode(data.payload))
                         logger.error(
-                            f"Message too big to transaction: {len(data['payload'])}"
+                            f"Message too big for transaction: {payload_length}"
                         )
                     raise e
-
                 except Exception:
                     logger.exception("Unhandled error pulling messages from processing")
                     await asyncio.sleep(self.pull_time_error_backoff)
```

## nucliadb/ingest/consumer/service.py

```diff
@@ -18,31 +18,31 @@
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
 import sys
 from functools import partial
 from typing import Awaitable, Callable, Optional
 
+from nucliadb.common.cluster import manager
+from nucliadb.common.maindb.utils import setup_driver
 from nucliadb.ingest import SERVICE_NAME, logger
 from nucliadb.ingest.consumer.consumer import IngestConsumer, IngestProcessedConsumer
 from nucliadb.ingest.consumer.pull import PullWorker
-from nucliadb.ingest.orm import NODES
 from nucliadb.ingest.settings import settings
-from nucliadb.ingest.utils import get_driver
 from nucliadb_utils.exceptions import ConfigurationError
 from nucliadb_utils.settings import running_settings, transaction_settings
 from nucliadb_utils.utilities import (
     get_audit,
-    get_cache,
     get_nats_manager,
     get_pubsub,
     get_storage,
 )
 
 from .auditing import IndexAuditHandler, ResourceWritesAuditHandler
+from .materializer import MaterializerHandler
 from .shard_creator import ShardCreatorHandler
 
 
 def _handle_task_result(task: asyncio.Task) -> None:
     e = task.exception()
     if e:
         logger.exception(
@@ -56,59 +56,67 @@
         task.cancel()
     await asyncio.gather(*tasks, return_exceptions=True)
 
 
 async def start_pull_workers(
     service_name: Optional[str] = None,
 ) -> Callable[[], Awaitable[None]]:
-    driver = await get_driver()
-    cache = await get_cache()
+    driver = await setup_driver()
+    pubsub = await get_pubsub()
     storage = await get_storage(service_name=service_name or SERVICE_NAME)
     tasks = []
     for partition in settings.partitions:
         worker = PullWorker(
             driver=driver,
             partition=partition,
             storage=storage,
             pull_time_error_backoff=settings.pull_time_error_backoff,
-            cache=cache,
+            pubsub=pubsub,
             local_subscriber=transaction_settings.transaction_local,
+            pull_api_timeout=settings.pull_api_timeout,
         )
         task = asyncio.create_task(worker.loop())
         task.add_done_callback(_handle_task_result)
         tasks.append(task)
 
     return partial(_exit_tasks, tasks)
 
 
 async def start_ingest_consumers(
     service_name: Optional[str] = None,
 ) -> Callable[[], Awaitable[None]]:
     if transaction_settings.transaction_local:
         raise ConfigurationError("Can not start ingest consumers in local mode")
 
-    while len(NODES) == 0 and running_settings.running_environment not in (
+    while len(
+        manager.get_index_nodes()
+    ) == 0 and running_settings.running_environment not in (
         "local",
         "test",
     ):
         logger.warning("Initializion delayed 1s to receive some Nodes on the cluster")
         await asyncio.sleep(1)
 
-    driver = await get_driver()
-    cache = await get_cache()
+    driver = await setup_driver()
+    pubsub = await get_pubsub()
     storage = await get_storage(service_name=service_name or SERVICE_NAME)
     nats_connection_manager = get_nats_manager()
 
+    max_concurrent_processing = asyncio.Semaphore(
+        settings.max_concurrent_ingest_processing
+    )
+
     for partition in settings.partitions:
         consumer = IngestConsumer(
             driver=driver,
             partition=partition,
             storage=storage,
-            cache=cache,
+            pubsub=pubsub,
             nats_connection_manager=nats_connection_manager,
+            lock=max_concurrent_processing,
         )
         await consumer.initialize()
 
     return nats_connection_manager.finalize
 
 
 async def start_ingest_processed_consumer(
@@ -120,43 +128,46 @@
     We are not maintaining transactionability based on the nats sequence id from this
     consumer and we will start off by not separating writes by partition AND
     allowing NATS to manage the queue group for us.
     """
     if transaction_settings.transaction_local:
         raise ConfigurationError("Can not start ingest consumers in local mode")
 
-    while len(NODES) == 0 and running_settings.running_environment not in (
+    while len(
+        manager.get_index_nodes()
+    ) == 0 and running_settings.running_environment not in (
         "local",
         "test",
     ):
         logger.warning("Initializion delayed 1s to receive some Nodes on the cluster")
         await asyncio.sleep(1)
 
-    driver = await get_driver()
-    cache = await get_cache()
+    driver = await setup_driver()
+    pubsub = await get_pubsub()
     storage = await get_storage(service_name=service_name or SERVICE_NAME)
     nats_connection_manager = get_nats_manager()
 
     consumer = IngestProcessedConsumer(
         driver=driver,
         partition="-1",
         storage=storage,
-        cache=cache,
+        pubsub=pubsub,
         nats_connection_manager=nats_connection_manager,
     )
     await consumer.initialize()
 
     return nats_connection_manager.finalize
 
 
 async def start_auditor() -> Callable[[], Awaitable[None]]:
-    driver = await get_driver()
+    driver = await setup_driver()
     audit = get_audit()
     assert audit is not None
     pubsub = await get_pubsub()
+    assert pubsub is not None, "Pubsub is not configured"
     storage = await get_storage(service_name=SERVICE_NAME)
     index_auditor = IndexAuditHandler(driver=driver, audit=audit, pubsub=pubsub)
     resource_writes_auditor = ResourceWritesAuditHandler(
         driver=driver, storage=storage, audit=audit, pubsub=pubsub
     )
 
     await index_auditor.initialize()
@@ -164,15 +175,27 @@
 
     return partial(
         asyncio.gather, index_auditor.finalize(), resource_writes_auditor.finalize()  # type: ignore
     )
 
 
 async def start_shard_creator() -> Callable[[], Awaitable[None]]:
-    driver = await get_driver()
+    driver = await setup_driver()
     pubsub = await get_pubsub()
+    assert pubsub is not None, "Pubsub is not configured"
     storage = await get_storage(service_name=SERVICE_NAME)
 
     shard_creator = ShardCreatorHandler(driver=driver, storage=storage, pubsub=pubsub)
     await shard_creator.initialize()
 
     return shard_creator.finalize
+
+
+async def start_materializer() -> Callable[[], Awaitable[None]]:
+    driver = await setup_driver()
+    pubsub = await get_pubsub()
+    assert pubsub is not None, "Pubsub is not configured"
+    storage = await get_storage(service_name=SERVICE_NAME)
+    materializer = MaterializerHandler(driver=driver, storage=storage, pubsub=pubsub)
+    await materializer.initialize()
+
+    return materializer.finalize
```

## nucliadb/ingest/consumer/shard_creator.py

```diff
@@ -18,20 +18,19 @@
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
 import logging
 import uuid
 from functools import partial
 
-from nucliadb.ingest.maindb.driver import Driver
-from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
-from nucliadb.ingest.orm.nodes_manager import NodesManager
-from nucliadb.ingest.orm.utils import get_node_klass
-from nucliadb.ingest.settings import settings
-from nucliadb_protos import noderesources_pb2, nodesidecar_pb2, writer_pb2
+from nucliadb.common import datamanagers, locking
+from nucliadb.common.cluster.manager import choose_node
+from nucliadb.common.cluster.utils import get_shard_manager
+from nucliadb.common.maindb.driver import Driver
+from nucliadb_protos import nodereader_pb2, noderesources_pb2, writer_pb2
 from nucliadb_utils import const
 from nucliadb_utils.cache.pubsub import PubSubDriver
 from nucliadb_utils.storages.storage import Storage
 
 from . import metrics
 from .utils import DelayedTaskHandler
 
@@ -53,15 +52,15 @@
         storage: Storage,
         pubsub: PubSubDriver,
         check_delay: float = 10.0,
     ):
         self.driver = driver
         self.storage = storage
         self.pubsub = pubsub
-        self.node_manager = NodesManager(driver)
+        self.shard_manager = get_shard_manager()
         self.task_handler = DelayedTaskHandler(check_delay)
 
     async def initialize(self) -> None:
         self.subscription_id = str(uuid.uuid4())
         await self.task_handler.initialize()
         await self.pubsub.subscribe(
             handler=self.handle_message,
@@ -87,24 +86,31 @@
             notification.kbid, partial(self.process_kb, notification.kbid)
         )
         metrics.total_messages.inc({"type": "shard_creator", "action": "scheduled"})
 
     @metrics.handler_histo.wrap({"type": "shard_creator"})
     async def process_kb(self, kbid: str) -> None:
         logger.info({"message": "Processing notification for kbid", "kbid": kbid})
-        kb_shards = await self.node_manager.get_shards_by_kbid_inner(kbid)
-        current_shard: writer_pb2.ShardObject = kb_shards.shards[kb_shards.actual]
-
-        node_klass = get_node_klass()
-
-        node, shard_id, _ = self.node_manager.choose_node(current_shard)
-        shard_counter: nodesidecar_pb2.Counter = await node.sidecar.GetCount(
-            noderesources_pb2.ShardId(id=shard_id)  # type: ignore
-        )
-        if shard_counter.resources > settings.max_shard_fields:
-            logger.warning({"message": "Adding shard", "kbid": kbid})
-            async with self.driver.transaction() as txn:
-                kb = KnowledgeBox(txn, self.storage, kbid)
-                similarity = await kb.get_similarity()
+        async with self.driver.transaction(read_only=True) as txn:
+            kb_shards = await datamanagers.cluster.get_kb_shards(txn, kbid=kbid)
+            current_shard = await self.shard_manager.get_current_active_shard(txn, kbid)
+
+        if kb_shards is None or current_shard is None:
+            logger.error(
+                "Processing a notification for a nonexistent", extra={"kbid": kbid}
+            )
+            return
 
-                await node_klass.create_shard_by_kbid(txn, kbid, similarity=similarity)
-                await txn.commit()
+        # TODO: when multiple shards are allowed, this should either handle the
+        # written shard or attempt to rebalance everything
+        async with locking.distributed_lock(locking.NEW_SHARD_LOCK.format(kbid=kbid)):
+            # remember, a lock will do at least 1+ reads and 1 write.
+            # with heavy writes, this adds some simple k/v pressure
+            node, shard_id = choose_node(current_shard)
+            shard: nodereader_pb2.Shard = await node.reader.GetShard(
+                nodereader_pb2.GetShardRequest(shard_id=noderesources_pb2.ShardId(id=shard_id))  # type: ignore
+            )
+            await self.shard_manager.maybe_create_new_shard(
+                kbid,
+                shard.paragraphs,
+                kb_shards.release_channel,
+            )
```

## nucliadb/ingest/fields/base.py

```diff
@@ -17,61 +17,57 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
 import enum
 from datetime import datetime
-from typing import Any, Dict, List, Optional, Tuple, Type
+from typing import Any, Optional, Type
 
 from nucliadb_protos.resources_pb2 import (
     CloudFile,
     ExtractedTextWrapper,
     ExtractedVectorsWrapper,
     FieldComputedMetadata,
     FieldComputedMetadataWrapper,
+    FieldQuestionAnswerWrapper,
     LargeComputedMetadata,
     LargeComputedMetadataWrapper,
-    UserVectorsWrapper,
-)
-from nucliadb_protos.utils_pb2 import (
-    ExtractedText,
-    UserVectorSet,
-    UserVectorsList,
-    VectorObject,
+    QuestionAnswers,
 )
+from nucliadb_protos.utils_pb2 import ExtractedText, VectorObject
 from nucliadb_protos.writer_pb2 import Error
 
 from nucliadb.ingest.fields.exceptions import InvalidFieldClass, InvalidPBClass
 from nucliadb_utils.storages.storage import Storage, StorageField
 
 KB_RESOURCE_FIELD = "/kbs/{kbid}/r/{uuid}/f/{type}/{field}"
 KB_RESOURCE_ERROR = "/kbs/{kbid}/r/{uuid}/f/{type}/{field}/error"
 
 SUBFIELDFIELDS = ["l", "c"]
 
 
 class FieldTypes(str, enum.Enum):
     FIELD_TEXT = "extracted_text"
     FIELD_VECTORS = "extracted_vectors"
-    USER_FIELD_VECTORS = "user_vectors"
     FIELD_METADATA = "metadata"
     FIELD_LARGE_METADATA = "large_metadata"
     THUMBNAIL = "thumbnail"
+    QUESTION_ANSWERS = "question_answers"
 
 
 class Field:
     pbklass: Optional[Type] = None
     type: str = "x"
     value: Optional[Any]
     extracted_text: Optional[ExtractedText]
     extracted_vectors: Optional[VectorObject]
     computed_metadata: Optional[FieldComputedMetadata]
     large_computed_metadata: Optional[LargeComputedMetadata]
-    extracted_user_vectors: Optional[UserVectorSet]
+    question_answers: Optional[QuestionAnswers]
 
     def __init__(
         self,
         id: str,
         resource: Any,
         pb: Optional[Any] = None,
         value: Optional[Any] = None,
@@ -80,15 +76,15 @@
             raise InvalidFieldClass()
 
         self.value = None
         self.extracted_text: Optional[ExtractedText] = None
         self.extracted_vectors = None
         self.computed_metadata = None
         self.large_computed_metadata = None
-        self.extracted_user_vectors = None
+        self.question_answers = None
 
         self.id: str = id
         self.resource: Any = resource
 
         if value is not None:
             newpb = self.pbklass()
             newpb.ParseFromString(value)
@@ -107,14 +103,18 @@
     def uuid(self) -> str:
         return self.resource.uuid
 
     @property
     def storage(self) -> Storage:
         return self.resource.storage
 
+    @property
+    def resource_unique_id(self) -> str:
+        return f"{self.uuid}/{self.type}/{self.id}"
+
     def get_storage_field(self, field_type: FieldTypes) -> StorageField:
         return self.storage.file_extracted(
             self.kbid, self.uuid, self.type, self.id, field_type.value
         )
 
     async def db_get_value(self):
         if self.value is None:
@@ -149,14 +149,22 @@
         async for key in self.resource.txn.keys(field_base_key):
             keys_to_delete.append(key)
         for key in keys_to_delete:
             await self.resource.txn.delete(key)
         await self.delete_extracted_text()
         await self.delete_vectors()
         await self.delete_metadata()
+        await self.delete_question_answers()
+
+    async def delete_question_answers(self) -> None:
+        sf = self.get_storage_field(FieldTypes.QUESTION_ANSWERS)
+        try:
+            await self.storage.delete_upload(sf.key, sf.bucket)
+        except KeyError:
+            pass
 
     async def delete_extracted_text(self) -> None:
         sf = self.get_storage_field(FieldTypes.FIELD_TEXT)
         try:
             await self.storage.delete_upload(sf.key, sf.bucket)
         except KeyError:
             pass
@@ -192,14 +200,36 @@
         await self.resource.txn.set(
             KB_RESOURCE_ERROR.format(
                 kbid=self.kbid, uuid=self.uuid, type=self.type, field=self.id
             ),
             error.SerializeToString(),
         )
 
+    async def get_question_answers(self) -> Optional[QuestionAnswers]:
+        if self.question_answers is None:
+            sf = self.get_storage_field(FieldTypes.QUESTION_ANSWERS)
+            payload = await self.storage.download_pb(sf, QuestionAnswers)
+            if payload is not None:
+                self.question_answers = payload
+        return self.question_answers
+
+    async def set_question_answers(self, payload: FieldQuestionAnswerWrapper) -> None:
+        sf = self.get_storage_field(FieldTypes.QUESTION_ANSWERS)
+
+        if payload.HasField("file"):
+            raw_payload = await self.storage.downloadbytescf(payload.file)
+            pb = QuestionAnswers()
+            pb.ParseFromString(raw_payload.read())
+            raw_payload.flush()
+            self.question_answers = pb
+        else:
+            self.question_answers = payload.question_answers
+
+        await self.storage.upload_pb(sf, self.question_answers)
+
     async def set_extracted_text(self, payload: ExtractedTextWrapper) -> None:
         if self.type in SUBFIELDFIELDS:
             try:
                 actual_payload: Optional[ExtractedText] = await self.get_extracted_text(
                     force=True
                 )
             except KeyError:
@@ -246,15 +276,15 @@
         if await sf.exists() is not None:
             return sf.build_cf()
         else:
             return None
 
     async def set_vectors(
         self, payload: ExtractedVectorsWrapper
-    ) -> Tuple[Optional[VectorObject], bool, List[str]]:
+    ) -> tuple[Optional[VectorObject], bool, list[str]]:
         if self.type in SUBFIELDFIELDS:
             try:
                 actual_payload: Optional[VectorObject] = await self.get_vectors(
                     force=True
                 )
             except KeyError:
                 actual_payload = None
@@ -300,73 +330,29 @@
         if self.extracted_vectors is None or force:
             sf = self.get_storage_field(FieldTypes.FIELD_VECTORS)
             payload = await self.storage.download_pb(sf, VectorObject)
             if payload is not None:
                 self.extracted_vectors = payload
         return self.extracted_vectors
 
-    async def set_user_vectors(
-        self, user_vectors: UserVectorsWrapper
-    ) -> Tuple[UserVectorSet, Dict[str, UserVectorsList]]:
-        try:
-            actual_payload: Optional[UserVectorSet] = await self.get_user_vectors(
-                force=True
-            )
-        except KeyError:
-            actual_payload = None
-
-        sf = self.get_storage_field(FieldTypes.USER_FIELD_VECTORS)
-
-        vectors_to_delete: Dict[str, UserVectorsList] = {}
-        if actual_payload is not None:
-            for vectorset, user_vector in user_vectors.vectors.vectors.items():
-                for key, vector in user_vector.vectors.items():
-                    if key in actual_payload.vectors[vectorset].vectors.keys():
-                        if vectorset not in vectors_to_delete:
-                            vectors_to_delete[vectorset] = UserVectorsList()
-                        vectors_to_delete[vectorset].vectors.append(key)
-                    actual_payload.vectors[vectorset].vectors[key].CopyFrom(vector)
-            for vectorset, delete_vectors in user_vectors.vectors_to_delete.items():
-                for vector_to_delete in delete_vectors.vectors:
-                    if (
-                        actual_payload.vectors.get(vectorset).vectors.get(
-                            vector_to_delete
-                        )
-                        is not None
-                    ):
-                        del actual_payload.vectors[vectorset].vectors[vector_to_delete]
-        else:
-            actual_payload = user_vectors.vectors
-        await self.storage.upload_pb(sf, actual_payload)
-        self.extracted_user_vectors = actual_payload
-        return actual_payload, vectors_to_delete
-
-    async def get_user_vectors(self, force=False) -> Optional[UserVectorSet]:
-        if self.extracted_user_vectors is None or force:
-            sf = self.get_storage_field(FieldTypes.USER_FIELD_VECTORS)
-            payload = await self.storage.download_pb(sf, UserVectorSet)
-            if payload is not None:
-                self.extracted_user_vectors = payload
-        return self.extracted_user_vectors
-
     async def get_vectors_cf(self) -> Optional[CloudFile]:
         sf = self.get_storage_field(FieldTypes.FIELD_VECTORS)
         if await sf.exists() is not None:
             return sf.build_cf()
         else:
             return None
 
     async def set_field_metadata(
         self, payload: FieldComputedMetadataWrapper
-    ) -> Tuple[FieldComputedMetadata, List[str], Dict[str, List[str]]]:
+    ) -> tuple[FieldComputedMetadata, list[str], dict[str, list[str]]]:
         if self.type in SUBFIELDFIELDS:
             try:
-                actual_payload: Optional[
-                    FieldComputedMetadata
-                ] = await self.get_field_metadata(force=True)
+                actual_payload: Optional[FieldComputedMetadata] = (
+                    await self.get_field_metadata(force=True)
+                )
             except KeyError:
                 actual_payload = None
         else:
             actual_payload = None
 
         sf = self.get_storage_field(FieldTypes.FIELD_METADATA)
 
@@ -428,17 +414,17 @@
             return sf.build_cf()
         else:
             return None
 
     async def set_large_field_metadata(self, payload: LargeComputedMetadataWrapper):
         if self.type in SUBFIELDFIELDS:
             try:
-                actual_payload: Optional[
-                    LargeComputedMetadata
-                ] = await self.get_large_field_metadata(force=True)
+                actual_payload: Optional[LargeComputedMetadata] = (
+                    await self.get_large_field_metadata(force=True)
+                )
             except KeyError:
                 actual_payload = None
         else:
             actual_payload = None
 
         sf = self.get_storage_field(FieldTypes.FIELD_LARGE_METADATA)
```

## nucliadb/ingest/fields/conversation.py

```diff
@@ -14,15 +14,15 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import uuid
-from typing import Any, Dict, Optional
+from typing import Any, Optional
 
 from nucliadb_protos.resources_pb2 import CloudFile
 from nucliadb_protos.resources_pb2 import Conversation as PBConversation
 from nucliadb_protos.resources_pb2 import FieldConversation
 
 from nucliadb.ingest.fields.base import Field
 from nucliadb_utils.storages.storage import StorageField
@@ -35,40 +35,40 @@
 class PageNotFound(Exception):
     pass
 
 
 class Conversation(Field):
     pbklass = PBConversation
     type: str = "c"
-    value: Dict[int, PBConversation]
+    value: dict[int, PBConversation]
     metadata: Optional[FieldConversation]
 
     _created: bool = False
 
     def __init__(
         self,
         id: str,
         resource: Any,
         pb: Optional[Any] = None,
-        value: Optional[Dict[int, PBConversation]] = None,
+        value: Optional[dict[int, PBConversation]] = None,
     ):
         super(Conversation, self).__init__(id, resource, pb, value)
         self.value = {}
         self.metadata = None
 
     async def set_value(self, payload: PBConversation):
         last_page: Optional[PBConversation] = None
         metadata = await self.get_metadata()
-        if self._created is False:
+        if self._created is False and metadata.pages > 0:
             try:
                 last_page = await self.db_get_value(page=metadata.pages)
             except PageNotFound:
                 pass
 
-        # Make sure files are on our region
+        # Make sure message attachment files are on our region
         for message in payload.messages:
             new_message_files = []
             for count, file in enumerate(message.content.attachments):
                 if self.storage.needs_move(file, self.kbid):
                     if message.ident == "":
                         message_ident = uuid.uuid4().hex
                     else:
@@ -102,32 +102,29 @@
             if len(messages) > 0:
                 metadata.pages += 1
                 last_page = PBConversation()
 
         await self.db_set_metadata(metadata)
 
     async def get_value(self, page: Optional[int] = None) -> Optional[PBConversation]:
-        # If now page was requested, force fetch of metadata
+        # If no page was requested, force fetch of metadata
         # and set the page to the last page
         if page is None and self.metadata is None:
-            await self.db_get_metadata()
+            await self.get_metadata()
 
         try:
             if page is not None and page > 0:
                 pageobj = await self.db_get_value(page)
             else:
                 pageobj = None
             return pageobj
         except PageNotFound:
             return None
 
     async def get_metadata(self) -> FieldConversation:
-        return await self.db_get_metadata()
-
-    async def db_get_metadata(self) -> FieldConversation:
         if self.metadata is None:
             payload = await self.resource.txn.get(
                 KB_RESOURCE_FIELD_METADATA.format(
                     kbid=self.kbid, uuid=self.uuid, type=self.type, field=self.id
                 )
             )
             self.metadata = FieldConversation()
@@ -137,14 +134,17 @@
                 self.metadata.size = PAGE_SIZE
                 self.metadata.pages = 0
                 self.metadata.total = 0
                 self._created = True
         return self.metadata
 
     async def db_get_value(self, page: int = 1):
+        if page == 0:
+            raise ValueError(f"Conversation pages start at index 1")
+
         if self.value.get(page) is None:
             field_key = KB_RESOURCE_FIELD.format(
                 kbid=self.kbid,
                 uuid=self.uuid,
                 type=self.type,
                 field=self.id,
                 page=page,
```

## nucliadb/ingest/fields/generic.py

```diff
@@ -15,31 +15,31 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from nucliadb.ingest.fields.base import Field
 
-VALID_GLOBAL = ("title", "summary")
+VALID_GENERIC_FIELDS = ("title", "summary")
 
 
 class Generic(Field):
     pbklass = str
     value: str
     type: str = "a"
 
     async def set_value(self, payload: str):
-        if self.id not in VALID_GLOBAL:
+        if self.id not in VALID_GENERIC_FIELDS:
             raise AttributeError(self.id)
 
         if self.resource.basic is None:
             await self.resource.get_basic()
 
         setattr(self.resource.basic, self.id, payload)
 
     async def get_value(self) -> str:
-        if self.id not in VALID_GLOBAL:
+        if self.id not in VALID_GENERIC_FIELDS:
             raise AttributeError(self.id)
         if self.resource.basic is None:
             await self.resource.get_basic()
 
         return getattr(self.resource.basic, self.id)
```

## nucliadb/ingest/orm/__init__.py

```diff
@@ -13,84 +13,7 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from __future__ import annotations
-
-from dataclasses import dataclass
-from datetime import datetime
-from typing import TYPE_CHECKING, Any, Optional
-
-from nucliadb.ingest.orm.exceptions import NodeClusterSmall
-from nucliadb.ingest.settings import settings
-from nucliadb_utils.clandestined import Cluster  # type: ignore
-
-if TYPE_CHECKING:  # pragma: no cover
-    from nucliadb.ingest.orm.node import Node
-
-NODES: dict[str, Node] = {}
-
-
-@dataclass
-class ScoredNode:
-    id: str
-    shard_count: int
-
-
-class ClusterObject:
-    date: datetime
-    cluster: Optional[Cluster] = None
-    local_node: Any
-
-    def __init__(self):
-        self.local_node = None
-        self.date = datetime.now()
-
-    def get_local_node(self):
-        return self.local_node
-
-    def find_nodes(self, avoid_nodes: Optional[list[str]] = None) -> list[str]:
-        """
-        Returns a list of node ids sorted by increasing shard count and load score.
-        It will avoid the node ids in `avoid_nodes` from the computation if possible.
-        It raises an exception if it can't find enough nodes for the configured replicas.
-        """
-        target_replicas = settings.node_replicas
-        available_nodes = [
-            ScoredNode(id=node_id, shard_count=node.shard_count)
-            for node_id, node in NODES.items()
-        ]
-        if len(available_nodes) < target_replicas:
-            raise NodeClusterSmall(
-                f"Not enough nodes. Total: {len(available_nodes)}, Required: {target_replicas}"
-            )
-
-        if settings.max_node_replicas >= 0:
-            available_nodes = list(
-                filter(
-                    lambda x: x.shard_count < settings.max_node_replicas, available_nodes  # type: ignore
-                )
-            )
-            if len(available_nodes) < target_replicas:
-                raise NodeClusterSmall(
-                    f"Could not find enough nodes with available shards. Available: {len(available_nodes)}, Required: {target_replicas}"  # noqa
-                )
-
-        # Sort available nodes by increasing shard_count
-        sorted_nodes = sorted(available_nodes, key=lambda x: x.shard_count)
-        available_node_ids = [node.id for node in sorted_nodes]
-
-        avoid_nodes = avoid_nodes or []
-        # get preferred nodes first
-        preferred_nodes = [nid for nid in available_node_ids if nid not in avoid_nodes]
-        # now, add to the end of the last nodes
-        preferred_node_order = preferred_nodes + [
-            nid for nid in available_node_ids if nid not in preferred_nodes
-        ]
-
-        return preferred_node_order[:target_replicas]
-
-
-NODE_CLUSTER = ClusterObject()
```

## nucliadb/ingest/orm/brain.py

```diff
@@ -13,177 +13,226 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import logging
 from copy import deepcopy
-from typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple, Union
+from dataclasses import dataclass
+from typing import TYPE_CHECKING, Optional, Union
 
-from google.protobuf.internal.containers import MessageMap
 from nucliadb_protos.noderesources_pb2 import IndexParagraph as BrainParagraph
 from nucliadb_protos.noderesources_pb2 import ParagraphMetadata
 from nucliadb_protos.noderesources_pb2 import Position as TextPosition
+from nucliadb_protos.noderesources_pb2 import Representation
 from nucliadb_protos.noderesources_pb2 import Resource as PBBrainResource
 from nucliadb_protos.noderesources_pb2 import ResourceID
 from nucliadb_protos.resources_pb2 import (
     Basic,
     ExtractedText,
     FieldComputedMetadata,
     FieldKeywordset,
     FieldMetadata,
     Metadata,
     Origin,
     Paragraph,
     UserFieldMetadata,
     UserMetadata,
 )
-from nucliadb_protos.utils_pb2 import (
-    Relation,
-    RelationNode,
-    UserVectorSet,
-    UserVectorsList,
-    VectorObject,
-)
+from nucliadb_protos.utils_pb2 import Relation, RelationNode, VectorObject
 
 from nucliadb.ingest import logger
-from nucliadb.ingest.orm.labels import BASE_TAGS, flat_resource_tags
 from nucliadb.ingest.orm.utils import compute_paragraph_key
+from nucliadb_models.labels import BASE_LABELS, flatten_resource_labels
 from nucliadb_models.metadata import ResourceProcessingStatus
+from nucliadb_protos import utils_pb2
 
 if TYPE_CHECKING:  # pragma: no cover
     StatusValue = Union[Metadata.Status.V, int]
 else:
     StatusValue = int
 
-FilePagePositions = Dict[int, Tuple[int, int]]
+FilePagePositions = dict[int, tuple[int, int]]
 
 
 METADATA_STATUS_PB_TYPE_TO_NAME_MAP = {
     Metadata.Status.ERROR: ResourceProcessingStatus.ERROR.name,
     Metadata.Status.PROCESSED: ResourceProcessingStatus.PROCESSED.name,
     Metadata.Status.PENDING: ResourceProcessingStatus.PENDING.name,
     Metadata.Status.BLOCKED: ResourceProcessingStatus.BLOCKED.name,
     Metadata.Status.EXPIRED: ResourceProcessingStatus.EXPIRED.name,
 }
 
 
+@dataclass
+class ParagraphClassifications:
+    valid: dict[str, list[str]]
+    denied: dict[str, list[str]]
+
+
 class ResourceBrain:
     def __init__(self, rid: str):
         self.rid = rid
         ridobj = ResourceID(uuid=rid)
         self.brain: PBBrainResource = PBBrainResource(resource=ridobj)
-        self.tags: Dict[str, List[str]] = deepcopy(BASE_TAGS)
+        self.labels: dict[str, list[str]] = deepcopy(BASE_LABELS)
 
     def apply_field_text(self, field_key: str, text: str):
         self.brain.texts[field_key].text = text
 
+    def _get_paragraph_user_classifications(
+        self, basic_user_field_metadata: Optional[UserFieldMetadata]
+    ) -> ParagraphClassifications:
+        pc = ParagraphClassifications(valid={}, denied={})
+        if basic_user_field_metadata is None:
+            return pc
+        for annotated_paragraph in basic_user_field_metadata.paragraphs:
+            for classification in annotated_paragraph.classifications:
+                paragraph_key = compute_paragraph_key(self.rid, annotated_paragraph.key)
+                classif_label = f"/l/{classification.labelset}/{classification.label}"
+                if classification.cancelled_by_user:
+                    pc.denied.setdefault(paragraph_key, []).append(classif_label)
+                else:
+                    pc.valid.setdefault(paragraph_key, []).append(classif_label)
+        return pc
+
     def apply_field_metadata(
         self,
         field_key: str,
         metadata: FieldComputedMetadata,
-        replace_field: List[str],
-        replace_splits: Dict[str, List[str]],
+        replace_field: list[str],
+        replace_splits: dict[str, list[str]],
         page_positions: Optional[FilePagePositions],
         extracted_text: Optional[ExtractedText],
         basic_user_field_metadata: Optional[UserFieldMetadata] = None,
     ):
         # To check for duplicate paragraphs
-        unique_paragraphs: Set[str] = set()
-
-        # Expose also user classes
+        unique_paragraphs: set[str] = set()
 
-        if basic_user_field_metadata is not None:
-            paragraphs = {
-                compute_paragraph_key(self.rid, paragraph.key): paragraph
-                for paragraph in basic_user_field_metadata.paragraphs
-            }
-        else:
-            paragraphs = {}
+        # Expose also user classifications
+        paragraph_classifications = self._get_paragraph_user_classifications(
+            basic_user_field_metadata
+        )
 
         # We should set paragraphs and labels
+        paragraph_pages = ParagraphPages(page_positions) if page_positions else None
         for subfield, metadata_split in metadata.split_metadata.items():
             # For each split of this field
             for index, paragraph in enumerate(metadata_split.paragraphs):
                 key = f"{self.rid}/{field_key}/{subfield}/{paragraph.start}-{paragraph.end}"
 
-                denied_classifications = []
-                if key in paragraphs:
-                    denied_classifications = [
-                        f"/l/{classification.labelset}/{classification.label}"
-                        for classification in paragraphs[key].classifications
-                        if classification.cancelled_by_user is True
-                    ]
+                denied_classifications = paragraph_classifications.denied.get(key, [])
                 position = TextPosition(
                     index=index,
                     start=paragraph.start,
                     end=paragraph.end,
                     start_seconds=paragraph.start_seconds,
                     end_seconds=paragraph.end_seconds,
                 )
-                if page_positions:
-                    position.page_number = get_page_number(
-                        paragraph.start, page_positions
-                    )
+                page_with_visual = False
+                if paragraph.HasField("page"):
+                    position.page_number = paragraph.page.page
+                    page_with_visual = paragraph.page.page_with_visual
+                    position.in_page = True
+                elif paragraph_pages:
+                    position.page_number = paragraph_pages.get(paragraph.start)
+                    position.in_page = True
+                else:
+                    position.in_page = False
+
+                representation = Representation()
+                if paragraph.HasField("representation"):
+                    representation.file = paragraph.representation.reference_file
+                    representation.is_a_table = paragraph.representation.is_a_table
+
                 p = BrainParagraph(
                     start=paragraph.start,
                     end=paragraph.end,
                     field=field_key,
                     split=subfield,
                     index=index,
                     repeated_in_field=is_paragraph_repeated_in_field(
                         paragraph,
                         extracted_text,
                         unique_paragraphs,
                         split=subfield,
                     ),
-                    metadata=ParagraphMetadata(position=position),
+                    metadata=ParagraphMetadata(
+                        position=position,
+                        page_with_visual=page_with_visual,
+                        representation=representation,
+                    ),
+                )
+                p.labels.append(
+                    f"/k/{Paragraph.TypeParagraph.Name(paragraph.kind).lower()}"
                 )
                 for classification in paragraph.classifications:
                     label = f"/l/{classification.labelset}/{classification.label}"
                     if label not in denied_classifications:
                         p.labels.append(label)
 
+                # Add user annotated labels to paragraphs
+                extend_unique(p.labels, paragraph_classifications.valid.get(key, []))  # type: ignore
+
                 self.brain.paragraphs[field_key].paragraphs[key].CopyFrom(p)
 
         for index, paragraph in enumerate(metadata.metadata.paragraphs):
             key = f"{self.rid}/{field_key}/{paragraph.start}-{paragraph.end}"
-            denied_classifications = []
-            if key in paragraphs:
-                denied_classifications = [
-                    f"/l/{classification.labelset}/{classification.label}"
-                    for classification in paragraphs[key].classifications
-                    if classification.cancelled_by_user is True
-                ]
-
+            denied_classifications = paragraph_classifications.denied.get(key, [])
             position = TextPosition(
                 index=index,
                 start=paragraph.start,
                 end=paragraph.end,
                 start_seconds=paragraph.start_seconds,
                 end_seconds=paragraph.end_seconds,
             )
-            if page_positions:
-                position.page_number = get_page_number(paragraph.start, page_positions)
+            page_with_visual = False
+            if paragraph.HasField("page"):
+                position.page_number = paragraph.page.page
+                position.in_page = True
+                page_with_visual = paragraph.page.page_with_visual
+            elif paragraph_pages:
+                position.page_number = paragraph_pages.get(paragraph.start)
+                position.in_page = True
+            else:
+                position.in_page = False
+
+            representation = Representation()
+            if paragraph.HasField("representation"):
+                representation.file = paragraph.representation.reference_file
+                representation.is_a_table = paragraph.representation.is_a_table
+
             p = BrainParagraph(
                 start=paragraph.start,
                 end=paragraph.end,
                 field=field_key,
                 index=index,
                 repeated_in_field=is_paragraph_repeated_in_field(
                     paragraph, extracted_text, unique_paragraphs
                 ),
-                metadata=ParagraphMetadata(position=position),
+                metadata=ParagraphMetadata(
+                    position=position,
+                    page_with_visual=page_with_visual,
+                    representation=representation,
+                ),
+            )
+            p.labels.append(
+                f"/k/{Paragraph.TypeParagraph.Name(paragraph.kind).lower()}"
             )
+
             for classification in paragraph.classifications:
                 label = f"/l/{classification.labelset}/{classification.label}"
                 if label not in denied_classifications:
                     p.labels.append(label)
 
+            # Add user annotated labels to paragraphs
+            extend_unique(p.labels, paragraph_classifications.valid.get(key, []))  # type: ignore
+
             self.brain.paragraphs[field_key].paragraphs[key].CopyFrom(p)
 
         for relations in metadata.metadata.relations:
             for relation in relations.relations:
                 self.brain.relations.append(relation)
 
         for split, sentences in replace_splits.items():
@@ -205,66 +254,87 @@
                 )
 
         for paragraph in metadata.metadata.paragraphs:
             self.brain.sentences_to_delete.append(
                 f"{self.rid}/{field_key}/{paragraph.start}-{paragraph.end}"
             )
 
-    def apply_user_vectors(
-        self,
-        field_key: str,
-        user_vectors: UserVectorSet,
-        vectors_to_delete: MessageMap[str, UserVectorsList],
-    ):
-        for vectorset, vectors in user_vectors.vectors.items():
-            for vector_id, user_vector in vectors.vectors.items():
-                self.brain.vectors[vectorset].vectors[
-                    f"{self.rid}/{field_key}/{vector_id}/{user_vector.start}-{user_vector.end}"
-                ].CopyFrom(user_vector)
-
-        for vectorset, vectorslist in vectors_to_delete.items():
-            for vector in vectorslist.vectors:
-                self.brain.vectors_to_delete[vectorset].vectors.append(
-                    f"{self.rid}/{field_key}/{vector}"
-                )
-
     def apply_field_vectors(
         self,
         field_key: str,
         vo: VectorObject,
         replace_field: bool,
-        replace_splits: List[str],
+        replace_splits: list[str],
     ):
         for subfield, vectors in vo.split_vectors.items():
             # For each split of this field
 
             for index, vector in enumerate(vectors.vectors):
                 sparagraph = self.brain.paragraphs[field_key].paragraphs[
                     f"{self.rid}/{field_key}/{subfield}/{vector.start_paragraph}-{vector.end_paragraph}"
                 ]
                 ssentence = sparagraph.sentences[
                     f"{self.rid}/{field_key}/{subfield}/{index}/{vector.start}-{vector.end}"
                 ]
+
+                ssentence.ClearField("vector")  # clear first to prevent duplicates
                 ssentence.vector.extend(vector.vector)
 
+                # we only care about start/stop position of the paragraph for a given sentence here
+                # the key has the sentence position
+                ssentence.metadata.position.start = vector.start_paragraph
+                ssentence.metadata.position.end = vector.end_paragraph
+
+                ssentence.metadata.position.page_number = (
+                    sparagraph.metadata.position.page_number
+                )
+                ssentence.metadata.position.in_page = (
+                    sparagraph.metadata.position.in_page
+                )
+                ssentence.metadata.page_with_visual = (
+                    sparagraph.metadata.page_with_visual
+                )
+
+                ssentence.metadata.representation.file = (
+                    sparagraph.metadata.representation.file
+                )
+                ssentence.metadata.representation.is_a_table = (
+                    sparagraph.metadata.representation.is_a_table
+                )
+                ssentence.metadata.position.index = sparagraph.metadata.position.index
+
         for index, vector in enumerate(vo.vectors.vectors):
-            paragraph = self.brain.paragraphs[field_key].paragraphs[
-                f"{self.rid}/{field_key}/{vector.start_paragraph}-{vector.end_paragraph}"
-            ]
-            sentence = paragraph.sentences[
-                f"{self.rid}/{field_key}/{index}/{vector.start}-{vector.end}"
-            ]
+            para_key = f"{self.rid}/{field_key}/{vector.start_paragraph}-{vector.end_paragraph}"
+            paragraph = self.brain.paragraphs[field_key].paragraphs[para_key]
+            sent_key = f"{self.rid}/{field_key}/{index}/{vector.start}-{vector.end}"
+            sentence = paragraph.sentences[sent_key]
+
+            sentence.ClearField("vector")  # clear first to prevent duplicates
             sentence.vector.extend(vector.vector)
-            sentence.metadata.position.start = vector.start
-            sentence.metadata.position.end = vector.end
+
+            # we only care about start/stop position of the paragraph for a given sentence here
+            # the key has the sentence position
+            sentence.metadata.position.start = vector.start_paragraph
+            sentence.metadata.position.end = vector.end_paragraph
 
             # does it make sense to copy forward paragraph values here?
             sentence.metadata.position.page_number = (
                 paragraph.metadata.position.page_number
             )
+            sentence.metadata.position.in_page = paragraph.metadata.position.in_page
+
+            sentence.metadata.page_with_visual = paragraph.metadata.page_with_visual
+
+            sentence.metadata.representation.file = (
+                paragraph.metadata.representation.file
+            )
+            sentence.metadata.representation.is_a_table = (
+                paragraph.metadata.representation.is_a_table
+            )
+
             sentence.metadata.position.index = paragraph.metadata.position.index
 
         for split in replace_splits:
             self.brain.sentences_to_delete.append(f"{self.rid}/{field_key}/{split}")
 
         if replace_field:
             self.brain.sentences_to_delete.append(f"{self.rid}/{field_key}")
@@ -280,82 +350,89 @@
             self.brain.sentences_to_delete.append(
                 f"{self.rid}/{field_key}/{vector.start}-{vector.end}"
             )
 
     def set_processing_status(
         self, basic: Basic, previous_status: Optional[Metadata.Status.ValueType]
     ):
+        """
+        We purposefully overwrite what we index as a status and DO NOT reflect
+        actual status with what we index.
+
+        This seems to be is on purpose so the frontend of the product can operate
+        on 2 statuses only -- PENDING and PROCESSED.
+        """
         # The value of brain.status will either be PROCESSED or PENDING
         status = basic.metadata.status
         if previous_status is not None and previous_status != Metadata.Status.PENDING:
             # Already processed once, so it stays as PROCESSED
             self.brain.status = PBBrainResource.PROCESSED
             return
         # previos_status is None or PENDING
         if status == Metadata.Status.PENDING:
             # Stays in pending
             self.brain.status = PBBrainResource.PENDING
         else:
             # Means it has just been processed
             self.brain.status = PBBrainResource.PROCESSED
 
+    def set_security(self, security: utils_pb2.Security):
+        self.brain.security.CopyFrom(security)
+
     def get_processing_status_tag(self, metadata: Metadata) -> str:
         if not metadata.useful:
             return "EMPTY"
         return METADATA_STATUS_PB_TYPE_TO_NAME_MAP[metadata.status]
 
-    def set_global_tags(self, basic: Basic, uuid: str, origin: Optional[Origin]):
-        self.brain.metadata.created.CopyFrom(basic.created)
-        self.brain.metadata.modified.CopyFrom(basic.modified)
+    def set_resource_metadata(self, basic: Basic, origin: Optional[Origin]):
+        self._set_resource_dates(basic, origin)
+        self._set_resource_labels(basic, origin)
+        self._set_resource_relations(basic, origin)
+
+    def _set_resource_dates(self, basic: Basic, origin: Optional[Origin]):
+        if basic.created.seconds > 0:
+            self.brain.metadata.created.CopyFrom(basic.created)
+        else:
+            logging.warning(f"Basic metadata has no created field for {self.rid}")
+            self.brain.metadata.created.GetCurrentTime()
+        if basic.modified.seconds > 0:
+            self.brain.metadata.modified.CopyFrom(basic.modified)
+        else:
+            if basic.created.seconds > 0:
+                self.brain.metadata.modified.CopyFrom(basic.created)
+            else:
+                self.brain.metadata.modified.GetCurrentTime()
+
+        if origin is not None:
+            # overwrite created/modified if provided on origin
+            if origin.HasField("created") and origin.created.seconds > 0:
+                self.brain.metadata.created.CopyFrom(origin.created)
+            if origin.HasField("modified") and origin.modified.seconds > 0:
+                self.brain.metadata.modified.CopyFrom(origin.modified)
 
+    def _set_resource_relations(self, basic: Basic, origin: Optional[Origin]):
         relationnodedocument = RelationNode(
-            value=uuid, ntype=RelationNode.NodeType.RESOURCE
+            value=self.rid, ntype=RelationNode.NodeType.RESOURCE
         )
         if origin is not None:
-            if origin.source_id:
-                self.tags["o"] = [origin.source_id]
-            # origin tags
-            for tag in origin.tags:
-                self.tags["t"].append(tag)
-            # origin source
-            if origin.source_id != "":
-                self.tags["u"].append(f"s/{origin.source_id}")
-
             # origin contributors
             for contrib in origin.colaborators:
-                self.tags["u"].append(f"o/{contrib}")
                 relationnodeuser = RelationNode(
                     value=contrib, ntype=RelationNode.NodeType.USER
                 )
                 self.brain.relations.append(
                     Relation(
                         relation=Relation.COLAB,
                         source=relationnodedocument,
                         to=relationnodeuser,
                     )
                 )
 
-        # icon
-        self.tags["n"].append(f"i/{basic.icon}")
-
-        # processing status
-        status_tag = self.get_processing_status_tag(basic.metadata)
-        self.tags["n"].append(f"s/{status_tag}")
-
-        # main language
-        if basic.metadata.language != "":
-            self.tags["s"].append(f"p/{basic.metadata.language}")
-
-        # all language
-        for lang in basic.metadata.languages:
-            self.tags["s"].append(f"s/{lang}")
-
         # labels
         for classification in basic.usermetadata.classifications:
-            self.tags["l"].append(f"{classification.labelset}/{classification.label}")
             relation_node_label = RelationNode(
                 value=f"{classification.labelset}/{classification.label}",
                 ntype=RelationNode.NodeType.LABEL,
             )
             self.brain.relations.append(
                 Relation(
                     relation=Relation.ABOUT,
@@ -363,42 +440,82 @@
                     to=relation_node_label,
                 )
             )
 
         # relations
         self.brain.relations.extend(basic.usermetadata.relations)
 
-        self.compute_tags()
+    def _set_resource_labels(self, basic: Basic, origin: Optional[Origin]):
+        if origin is not None:
+            if origin.source_id:
+                self.labels["o"] = [origin.source_id]
+            # origin tags
+            for tag in origin.tags:
+                self.labels["t"].append(tag)
+            # origin source
+            if origin.source_id != "":
+                self.labels["u"].append(f"s/{origin.source_id}")
+
+            if origin.path:
+                self.labels["p"].append(origin.path.lstrip("/"))
+
+            # origin contributors
+            for contrib in origin.colaborators:
+                self.labels["u"].append(f"o/{contrib}")
+
+            for key, value in origin.metadata.items():
+                self.labels["m"].append(f"{key[:255]}/{value[:255]}")
+
+        # icon
+        self.labels["n"].append(f"i/{basic.icon}")
+
+        # processing status
+        status_tag = self.get_processing_status_tag(basic.metadata)
+        self.labels["n"].append(f"s/{status_tag}")
+
+        # main language
+        if basic.metadata.language:
+            self.labels["s"].append(f"p/{basic.metadata.language}")
+
+        # all language
+        for lang in basic.metadata.languages:
+            self.labels["s"].append(f"s/{lang}")
 
-    def process_meta(
+        # labels
+        for classification in basic.usermetadata.classifications:
+            self.labels["l"].append(f"{classification.labelset}/{classification.label}")
+
+        self.compute_labels()
+
+    def process_field_metadata(
         self,
         field_key: str,
         metadata: FieldMetadata,
-        tags: Dict[str, List[str]],
+        labels: dict[str, list[str]],
         relation_node_document: RelationNode,
-        user_canceled_labels: List[str],
+        user_canceled_labels: list[str],
     ):
         for classification in metadata.classifications:
             label = f"{classification.labelset}/{classification.label}"
             if label not in user_canceled_labels:
-                tags["l"].append(label)
+                labels["l"].append(label)
                 relation_node_label = RelationNode(
                     value=label,
                     ntype=RelationNode.NodeType.LABEL,
                 )
                 self.brain.relations.append(
                     Relation(
                         relation=Relation.ABOUT,
                         source=relation_node_document,
                         to=relation_node_label,
                     )
                 )
 
         for klass_entity, _ in metadata.positions.items():
-            tags["e"].append(klass_entity)
+            labels["e"].append(klass_entity)
             entity_array = klass_entity.split("/")
             if len(entity_array) == 1:
                 raise AttributeError(f"Entity should be with type {klass_entity}")
             elif len(entity_array) > 1:
                 klass = entity_array[0]
                 entity = "/".join(entity_array[1:])
             relation_node_entity = RelationNode(
@@ -411,18 +528,18 @@
             )
             self.brain.relations.append(rel)
 
     def process_keywordset_fields(self, field_key: str, field: FieldKeywordset):
         # all field keywords
         if field:
             for keyword in field.keywords:
-                self.tags["f"].append(f"{field_key}/{keyword.value}")
-                self.tags["fg"].append(keyword.value)
+                self.labels["f"].append(f"{field_key}/{keyword.value}")
+                self.labels["fg"].append(keyword.value)
 
-    def apply_field_tags_globally(
+    def apply_field_labels(
         self,
         field_key: str,
         metadata: Optional[FieldComputedMetadata],
         uuid: str,
         basic_user_metadata: Optional[UserMetadata] = None,
         basic_user_fieldmetadata: Optional[UserFieldMetadata] = None,
     ):
@@ -434,55 +551,69 @@
             ]
         else:
             user_canceled_labels = []
 
         relation_node_resource = RelationNode(
             value=uuid, ntype=RelationNode.NodeType.RESOURCE
         )
-        tags: Dict[str, List[str]] = {"l": [], "e": []}
+        labels: dict[str, list[str]] = {"l": [], "e": []}
         if metadata is not None:
             for meta in metadata.split_metadata.values():
-                self.process_meta(
-                    field_key, meta, tags, relation_node_resource, user_canceled_labels
+                self.process_field_metadata(
+                    field_key,
+                    meta,
+                    labels,
+                    relation_node_resource,
+                    user_canceled_labels,
                 )
-            self.process_meta(
+            self.process_field_metadata(
                 field_key,
                 metadata.metadata,
-                tags,
+                labels,
                 relation_node_resource,
                 user_canceled_labels,
             )
 
         if basic_user_fieldmetadata is not None:
             for token in basic_user_fieldmetadata.token:
                 if token.cancelled_by_user is False:
-                    tags["e"].append(f"{token.klass}/{token.token}")
+                    labels["e"].append(f"{token.klass}/{token.token}")
                     relation_node_entity = RelationNode(
                         value=token.token,
                         ntype=RelationNode.NodeType.ENTITY,
                         subtype=token.klass,
                     )
                     rel = Relation(
                         relation=Relation.ENTITY,
                         source=relation_node_resource,
                         to=relation_node_entity,
                     )
                     self.brain.relations.append(rel)
             for paragraph_annotation in basic_user_fieldmetadata.paragraphs:
                 for classification in paragraph_annotation.classifications:
                     if not classification.cancelled_by_user:
-                        self.brain.paragraphs[field_key].paragraphs[
-                            paragraph_annotation.key
-                        ].labels.append(
-                            f"/l/{classification.labelset}/{classification.label}"
-                        )
-        self.brain.texts[field_key].labels.extend(flat_resource_tags(tags))
+                        label = f"/l/{classification.labelset}/{classification.label}"
+                        # FIXME: this condition avoid adding duplicate labels
+                        # while importing a kb. We shouldn't add duplicates on
+                        # the first place
+                        if (
+                            label
+                            not in self.brain.paragraphs[field_key]
+                            .paragraphs[paragraph_annotation.key]
+                            .labels
+                        ):
+                            self.brain.paragraphs[field_key].paragraphs[
+                                paragraph_annotation.key
+                            ].labels.append(label)
+        extend_unique(
+            self.brain.texts[field_key].labels, flatten_resource_labels(labels)  # type: ignore
+        )
 
-    def compute_tags(self):
-        self.brain.labels.extend(flat_resource_tags(self.tags))
+    def compute_labels(self):
+        extend_unique(self.brain.labels, flatten_resource_labels(self.labels))
 
 
 def get_paragraph_text(
     extracted_text: ExtractedText, start: int, end: int, split: Optional[str] = None
 ) -> str:
     if split is not None:
         text = extracted_text.split_text[split]
@@ -490,15 +621,15 @@
         text = extracted_text.text
     return text[start:end]
 
 
 def is_paragraph_repeated_in_field(
     paragraph: Paragraph,
     extracted_text: Optional[ExtractedText],
-    unique_paragraphs: Set[str],
+    unique_paragraphs: set[str],
     split: Optional[str] = None,
 ) -> bool:
     if extracted_text is None:
         return False
 
     paragraph_text = get_paragraph_text(
         extracted_text, start=paragraph.start, end=paragraph.end, split=split
@@ -510,17 +641,41 @@
         repeated_in_field = True
     else:
         repeated_in_field = False
         unique_paragraphs.add(paragraph_text)
     return repeated_in_field
 
 
-def get_page_number(start_index: int, page_positions: FilePagePositions) -> int:
-    page_number = 0
-    for page_number, (page_start, page_end) in page_positions.items():
-        if page_start <= start_index <= page_end:
-            return int(page_number)
-        if start_index <= page_end:
-            logger.info("There is a wrong page start")
-            return int(page_number)
-    logger.error("Could not found a page")
-    return int(page_number)
+class ParagraphPages:
+    """
+    Class to get the page number for a given paragraph in an optimized way.
+    """
+
+    def __init__(self, positions: FilePagePositions):
+        self.positions = positions
+        self._materialized = self._materialize_page_numbers(positions)
+
+    def _materialize_page_numbers(self, positions: FilePagePositions) -> list[int]:
+        page_numbers_by_index = []
+        for page_number, (page_start, page_end) in positions.items():
+            page_numbers_by_index.extend([page_number] * (page_end - page_start + 1))
+        return page_numbers_by_index
+
+    def get(self, paragraph_start_index: int) -> int:
+        try:
+            return self._materialized[paragraph_start_index]
+        except IndexError:
+            logger.error(
+                f"Could not find a page for the given index: {paragraph_start_index}. Page positions: {self.positions}"  # noqa
+            )
+            if len(self._materialized) > 0:
+                return self._materialized[-1]
+            return 0
+
+
+def extend_unique(a: list, b: list):
+    """
+    Prevents extending with duplicate elements
+    """
+    for item in b:
+        if item not in a:
+            a.append(item)
```

## nucliadb/ingest/orm/entities.py

```diff
@@ -14,101 +14,118 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
-from typing import AsyncGenerator, Dict, List, Optional, Set, Tuple
+import asyncio
+from typing import AsyncGenerator, Optional
 
 from nucliadb_protos.knowledgebox_pb2 import (
     DeletedEntitiesGroups,
     EntitiesGroup,
     EntitiesGroupSummary,
     Entity,
 )
 from nucliadb_protos.nodereader_pb2 import (
+    Faceted,
     RelationNodeFilter,
     RelationPrefixSearchRequest,
     RelationSearchRequest,
     RelationSearchResponse,
-    TypeList,
+    SearchRequest,
+    SearchResponse,
 )
-from nucliadb_protos.noderesources_pb2 import ShardId
-from nucliadb_protos.nodewriter_pb2 import SetGraph
-from nucliadb_protos.utils_pb2 import JoinGraph, RelationNode
+from nucliadb_protos.utils_pb2 import RelationNode
 from nucliadb_protos.writer_pb2 import GetEntitiesResponse
 
-from nucliadb.ingest.maindb.driver import Transaction
-from nucliadb.ingest.maindb.keys import (
-    KB_DELETED_ENTITIES_GROUPS,
-    KB_ENTITIES,
-    KB_ENTITIES_GROUP,
-)
-from nucliadb.ingest.orm.exceptions import (
+from nucliadb.common import datamanagers
+from nucliadb.common.cluster.base import AbstractIndexNode
+from nucliadb.common.cluster.exceptions import (
     AlreadyExists,
     EntitiesGroupNotFound,
     NodeError,
 )
+from nucliadb.common.cluster.utils import get_shard_manager
+from nucliadb.common.datamanagers.entities import (
+    KB_DELETED_ENTITIES_GROUPS,
+    KB_ENTITIES,
+    KB_ENTITIES_GROUP,
+)
+from nucliadb.common.maindb.driver import Transaction
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
-from nucliadb.ingest.orm.node import Node
-from nucliadb.ingest.orm.nodes_manager import NodesManager
 from nucliadb.ingest.settings import settings
-from nucliadb.ingest.utils import get_driver
 from nucliadb_telemetry import errors
 
+from .exceptions import EntityManagementException
+
+MAX_DUPLICATES = 300
+MAX_DELETED = 300
+
 
 class EntitiesManager:
     def __init__(
         self,
         knowledgebox: KnowledgeBox,
         txn: Transaction,
+        use_read_replica_nodes: bool = False,
     ):
         self.kb = knowledgebox
         self.txn = txn
         self.kbid = self.kb.kbid
+        self.use_read_replica_nodes = use_read_replica_nodes
 
     async def create_entities_group(self, group: str, entities: EntitiesGroup):
         if await self.entities_group_exists(group):
             raise AlreadyExists(f"Entities group {group} already exists")
 
         await self.store_entities_group(group, entities)
-        await self.index_entities_group(group, entities)
 
     async def get_entities(self, entities: GetEntitiesResponse):
         async for group, eg in self.iterate_entities_groups(exclude_deleted=True):
             entities.groups[group].CopyFrom(eg)
 
     async def get_entities_group(self, group: str) -> Optional[EntitiesGroup]:
         deleted = await self.is_entities_group_deleted(group)
         if deleted:
             return None
         return await self.get_entities_group_inner(group)
 
-    async def get_entities_groups(self) -> Dict[str, EntitiesGroup]:
+    async def get_entities_groups(self) -> dict[str, EntitiesGroup]:
         groups = {}
         async for group, eg in self.iterate_entities_groups(exclude_deleted=True):
             groups[group] = eg
         return groups
 
-    async def list_entities_groups(self) -> Dict[str, EntitiesGroupSummary]:
+    async def list_entities_groups(self) -> dict[str, EntitiesGroupSummary]:
         groups = {}
-        async for group in self.iterate_entities_groups_names(exclude_deleted=True):
-            stored = await self.get_stored_entities_group(group)
-            if stored is not None:
-                groups[group] = EntitiesGroupSummary(
-                    title=stored.title, color=stored.color, custom=stored.custom
-                )
-            else:
-                # We don't want to search for each indexed group, as we are
-                # providing a quick summary
-                groups[group] = EntitiesGroupSummary()
+        max_simultaneous = asyncio.Semaphore(10)
+
+        async def _composition(group: str):
+            async with max_simultaneous:
+                stored = await self.get_stored_entities_group(group)
+                if stored is not None:
+                    groups[group] = EntitiesGroupSummary(
+                        title=stored.title, color=stored.color, custom=stored.custom
+                    )
+                else:
+                    # We don't want to search for each indexed group, as we are
+                    # providing a quick summary
+                    groups[group] = EntitiesGroupSummary()
+
+        tasks = [
+            asyncio.create_task(_composition(group))
+            async for group in self.iterate_entities_groups_names(exclude_deleted=True)
+        ]
+        if tasks:
+            await asyncio.wait(tasks)
         return groups
 
-    async def update_entities(self, group: str, entities: Dict[str, Entity]):
+    async def update_entities(self, group: str, entities: dict[str, Entity]):
         """Update entities on an entity group. New entities are appended and existing
         are overwriten. Existing entities not appearing in `entities` are left
         intact. Use `delete_entities` to delete them instead.
 
         """
         if not await self.entities_group_exists(group):
             raise EntitiesGroupNotFound(f"Entities group '{group}' doesn't exist")
@@ -117,16 +134,14 @@
         if entities_group is None:
             entities_group = EntitiesGroup()
 
         for name, entity in entities.items():
             entities_group.entities[name].CopyFrom(entity)
 
         await self.store_entities_group(group, entities_group)
-        # XXX: this is indexing everything. We could do it better indexing only diffs
-        await self.index_entities_group(group, entities_group)
 
     async def set_entities_group(self, group: str, entities: EntitiesGroup):
         indexed = await self.get_indexed_entities_group(group)
         if indexed is None:
             updated = entities
         else:
             updated = EntitiesGroup()
@@ -134,19 +149,17 @@
 
             for name, entity in indexed.entities.items():
                 if name not in updated.entities:
                     updated.entities[name].CopyFrom(entity)
                     updated.entities[name].deleted = True
 
         await self.store_entities_group(group, updated)
-        await self.index_entities_group(group, updated)
 
     async def set_entities_group_force(self, group: str, entitiesgroup: EntitiesGroup):
         await self.store_entities_group(group, entitiesgroup)
-        await self.index_entities_group(group, entitiesgroup)
 
     async def set_entities_group_metadata(
         self, group: str, *, title: Optional[str] = None, color: Optional[str] = None
     ):
         entities_group = await self.get_stored_entities_group(group)
         if entities_group is None:
             entities_group = EntitiesGroup()
@@ -154,29 +167,26 @@
         if title:
             entities_group.title = title
         if color:
             entities_group.color = color
 
         await self.store_entities_group(group, entities_group)
 
-    async def delete_entities(self, group: str, delete: List[str]):
+    async def delete_entities(self, group: str, delete: list[str]):
         stored = await self.get_stored_entities_group(group)
-        indexed = await self.get_indexed_entities_group(group)
-
-        if stored is None and indexed is None:
-            return
 
         stored = stored or EntitiesGroup()
-        indexed = indexed or EntitiesGroup()
         for name in delete:
-            if name in stored.entities or name in indexed.entities:
+            if name not in stored.entities:
+                entity = stored.entities[name]
+                entity.value = name
+            else:
                 entity = stored.entities[name]
-                entity.deleted = True
+            entity.deleted = True
         await self.store_entities_group(group, stored)
-        # TODO: we should remove indexed entities here
 
     async def delete_entities_group(self, group: str):
         await self.delete_stored_entities_group(group)
         await self.mark_entities_group_as_deleted(group)
 
     # Private API
 
@@ -189,45 +199,42 @@
         elif stored is not None and indexed is not None:
             entities_group = self.merge_entities_groups(indexed, stored)
         else:
             entities_group = stored or indexed  # type: ignore
         return entities_group
 
     async def get_stored_entities_group(self, group: str) -> Optional[EntitiesGroup]:
-        key = KB_ENTITIES_GROUP.format(kbid=self.kbid, id=group)
-        payload = await self.txn.get(key)
-        if not payload:
-            return None
-
-        eg = EntitiesGroup()
-        eg.ParseFromString(payload)
-        return eg
+        return await datamanagers.entities.get_entities_group(
+            self.txn, kbid=self.kbid, group=group
+        )
 
     async def get_indexed_entities_group(self, group: str) -> Optional[EntitiesGroup]:
-        driver = await get_driver()
-        nodes_manager = NodesManager(driver=driver)
+        shard_manager = get_shard_manager()
 
         async def do_entities_search(
-            node: Node, shard_id: str, node_id: str
+            node: AbstractIndexNode, shard_id: str
         ) -> RelationSearchResponse:
             request = RelationSearchRequest(
                 shard_id=shard_id,
                 prefix=RelationPrefixSearchRequest(
                     prefix="",
                     node_filters=[
                         RelationNodeFilter(
                             node_type=RelationNode.NodeType.ENTITY, node_subtype=group
                         )
                     ],
                 ),
             )
             return await node.reader.RelationSearch(request)  # type: ignore
 
-        results = await nodes_manager.apply_for_all_shards(
-            self.kbid, do_entities_search, settings.relation_search_timeout
+        results = await shard_manager.apply_for_all_shards(
+            self.kbid,
+            do_entities_search,
+            settings.relation_search_timeout,
+            use_read_replica_nodes=self.use_read_replica_nodes,
         )
         for result in results:
             if isinstance(result, Exception):
                 errors.capture_exception(result)
                 raise NodeError("Error while querying relation index")
 
         entities = {}
@@ -237,16 +244,16 @@
             )
 
         if not entities:
             return None
         eg = EntitiesGroup(entities=entities)
         return eg
 
-    async def get_deleted_entities_groups(self) -> Set[str]:
-        deleted: Set[str] = set()
+    async def get_deleted_entities_groups(self) -> set[str]:
+        deleted: set[str] = set()
         key = KB_DELETED_ENTITIES_GROUPS.format(kbid=self.kbid)
         payload = await self.txn.get(key)
         if payload:
             deg = DeletedEntitiesGroups()
             deg.ParseFromString(payload)
             deleted.update(deg.entities_groups)
         return deleted
@@ -260,24 +267,28 @@
         if indexed is not None:
             return True
 
         return False
 
     async def iterate_entities_groups(
         self, exclude_deleted: bool
-    ) -> AsyncGenerator[Tuple[str, EntitiesGroup], None]:
+    ) -> AsyncGenerator[tuple[str, EntitiesGroup], None]:
         async for group in self.iterate_entities_groups_names(exclude_deleted):
             eg = await self.get_entities_group_inner(group)
             if eg is None:
                 continue
             yield group, eg
 
     async def iterate_entities_groups_names(
-        self, exclude_deleted: bool
+        self,
+        exclude_deleted: bool,
     ) -> AsyncGenerator[str, None]:
+        # Start the task to get indexed groups
+        indexed_task = asyncio.create_task(self.get_indexed_entities_groups_names())
+
         if exclude_deleted:
             deleted_groups = await self.get_deleted_entities_groups()
 
         visited_groups = set()
 
         # stored groups
         entities_key = KB_ENTITIES.format(kbid=self.kbid)
@@ -285,112 +296,131 @@
             group = key.split("/")[-1]
             if exclude_deleted and group in deleted_groups:
                 continue
             yield group
             visited_groups.add(group)
 
         # indexed groups
-        indexed_groups = await self.get_indexed_entities_groups_names()
+        indexed_groups = await indexed_task
         for group in indexed_groups:
             if (exclude_deleted and group in deleted_groups) or group in visited_groups:
                 continue
             yield group
             visited_groups.add(group)
 
-    async def get_indexed_entities_groups_names(self) -> Set[str]:
-        driver = await get_driver()
-        nodes_manager = NodesManager(driver=driver)
+    async def get_indexed_entities_groups_names(
+        self,
+    ) -> set[str]:
+        shard_manager = get_shard_manager()
 
         async def query_indexed_entities_group_names(
-            node: Node, shard_id: str, node_id: str
-        ) -> TypeList:
-            return await node.reader.RelationTypes(ShardId(id=shard_id))  # type: ignore
+            node: AbstractIndexNode, shard_id: str
+        ) -> set[str]:
+            request = SearchRequest(
+                shard=shard_id,
+                result_per_page=0,
+                body="",
+                document=True,
+                paragraph=False,
+                faceted=Faceted(labels=["/e"]),
+            )
+            response: SearchResponse = await node.reader.Search(request)  # type: ignore
+            try:
+                facetresults = response.document.facets["/e"].facetresults
+                return {facet.tag.split("/")[-1] for facet in facetresults}
+            except KeyError:
+                # No entities found
+                return set()
 
-        results = await nodes_manager.apply_for_all_shards(
+        results = await shard_manager.apply_for_all_shards(
             self.kbid,
             query_indexed_entities_group_names,
             settings.relation_types_timeout,
+            use_read_replica_nodes=self.use_read_replica_nodes,
         )
         for result in results:
             if isinstance(result, Exception):
                 errors.capture_exception(result)
                 raise NodeError("Error while looking for relations types")
 
-        indexed_groups = set()
-        for relation_types in results:
-            for item in relation_types.list:
-                if item.with_type != RelationNode.NodeType.ENTITY:
-                    continue
-                group = item.with_subtype
-                indexed_groups.add(group)
-        return indexed_groups
+        if not results:
+            return set()
+        return set.union(*results)
 
     async def store_entities_group(self, group: str, eg: EntitiesGroup):
-        key = KB_ENTITIES_GROUP.format(kbid=self.kbid, id=group)
-        await self.txn.set(key, eg.SerializeToString())
+        meta_cache = await datamanagers.entities.get_entities_meta_cache(
+            self.txn, kbid=self.kbid
+        )
+        duplicates = {}
+        deleted = []
+        duplicate_count = 0
+        for entity in eg.entities.values():
+            if entity.deleted:
+                deleted.append(entity.value)
+                continue
+            if len(entity.represents) == 0:
+                continue
+            duplicates[entity.value] = list(entity.represents)
+            duplicate_count += len(duplicates[entity.value])
+
+        if duplicate_count > MAX_DUPLICATES:
+            raise EntityManagementException(
+                f"Too many duplicates: {duplicate_count}. Max of {MAX_DUPLICATES} currently allowed"
+            )
+        if len(deleted) > MAX_DELETED:
+            raise EntityManagementException(
+                f"Too many deleted entities: {len(deleted)}. Max of {MAX_DELETED} currently allowed"
+            )
+
+        meta_cache.set_duplicates(group, duplicates)
+        meta_cache.set_deleted(group, deleted)
+        await datamanagers.entities.set_entities_meta_cache(
+            self.txn, kbid=self.kbid, cache=meta_cache
+        )
+
+        await datamanagers.entities.set_entities_group(
+            self.txn, kbid=self.kbid, group_id=group, entities=eg
+        )
         # if it was preivously deleted, we must unmark it
         await self.unmark_entities_group_as_deleted(group)
 
     async def is_entities_group_deleted(self, group: str):
         deleted_groups = await self.get_deleted_entities_groups()
         return group in deleted_groups
 
     async def delete_stored_entities_group(self, group: str):
         entities_key = KB_ENTITIES_GROUP.format(kbid=self.kbid, id=group)
         await self.txn.delete(entities_key)
 
     async def mark_entities_group_as_deleted(self, group: str):
-        deleted_groups_key = KB_DELETED_ENTITIES_GROUPS.format(kbid=self.kbid)
-        payload = await self.txn.get(deleted_groups_key)
-        deg = DeletedEntitiesGroups()
-        if payload:
-            deg.ParseFromString(payload)
-        if group not in deg.entities_groups:
-            deg.entities_groups.append(group)
-            await self.txn.set(deleted_groups_key, deg.SerializeToString())
+        await datamanagers.entities.mark_group_as_deleted(
+            self.txn, kbid=self.kbid, group=group
+        )
 
     async def unmark_entities_group_as_deleted(self, group: str):
-        key = KB_DELETED_ENTITIES_GROUPS.format(kbid=self.kbid)
-        payload = await self.txn.get(key)
-        if not payload:
-            return
-        deg = DeletedEntitiesGroups()
-        deg.ParseFromString(payload)
-        if group in deg.entities_groups:
-            deg.entities_groups.remove(group)
-            await self.txn.set(key, deg.SerializeToString())
+        await datamanagers.entities.unmark_group_as_deleted(
+            self.txn, kbid=self.kbid, group=group
+        )
 
     @staticmethod
     def merge_entities_groups(indexed: EntitiesGroup, stored: EntitiesGroup):
         """Create a new EntitiesGroup with the merged entities from `stored` and
         `indexed`. The values of `stored` take priority when `stored` and
         `indexed` share entities. That's also true for common fields.
 
         """
-        merged_entities: Dict[str, Entity] = {}
+        merged_entities: dict[str, Entity] = {}
         merged_entities.update(indexed.entities)
         merged_entities.update(stored.entities)
 
+        for entity, edata in list(stored.entities.items()):
+            # filter out deleted entities
+            if edata.deleted:
+                del merged_entities[entity]
+
         merged = EntitiesGroup(
             entities=merged_entities,
             title=stored.title or indexed.title or "",
             color=stored.color or indexed.color or "",
             custom=False,  # if there are indexed entities, can't be a custom group
         )
         return merged
-
-    async def index_entities_group(self, group: str, entities: EntitiesGroup):
-        # TODO properly indexing of SYNONYM relations
-        graph_nodes = {
-            i: RelationNode(
-                value=entity.value,
-                ntype=RelationNode.NodeType.ENTITY,
-                subtype=group,
-            )
-            for i, (name, entity) in enumerate(entities.entities.items())
-        }
-
-        jg = JoinGraph(nodes=graph_nodes, edges=[])
-
-        async for node, shard_id in self.kb.iterate_kb_nodes():
-            sg = SetGraph(shard_id=ShardId(id=shard_id), graph=jg)
-            await node.writer.JoinGraph(sg)  # type: ignore
```

## nucliadb/ingest/orm/exceptions.py

```diff
@@ -15,54 +15,36 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
 
-class AlreadyExists(Exception):
-    pass
-
-
 class NotFound(Exception):
     pass
 
 
-class NodeClusterSmall(Exception):
-    pass
-
-
 class KnowledgeBoxConflict(Exception):
     pass
 
 
-class ShardNotFound(NotFound):
-    pass
-
-
-class KnowledgeBoxNotFound(NotFound):
-    pass
-
-
-class NodesUnsync(Exception):
-    pass
-
-
-class NodeError(Exception):
-    pass
-
-
 class DeadletteredError(Exception):
     pass
 
 
 class ReallyStopPulling(Exception):
     pass
 
 
 class SequenceOrderViolation(Exception):
     def __init__(self, last_seqid: int):
         self.last_seqid = last_seqid
 
 
-class EntitiesGroupNotFound(NotFound):
+class ResourceNotIndexable(Exception):
+    """
+    Unable to index resource
+    """
+
+
+class EntityManagementException(Exception):
     pass
```

## nucliadb/ingest/orm/knowledgebox.py

```diff
@@ -14,66 +14,56 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from datetime import datetime
-from typing import AsyncGenerator, AsyncIterator, Optional, Sequence, Tuple, Union
+from typing import AsyncGenerator, AsyncIterator, Optional, Sequence
 from uuid import uuid4
 
 from grpc import StatusCode
-from grpc.aio import AioRpcError  # type: ignore
-from nucliadb_protos.knowledgebox_pb2 import KnowledgeBoxConfig, Labels, LabelSet
+from grpc.aio import AioRpcError
+from nucliadb_protos.knowledgebox_pb2 import (
+    KnowledgeBoxConfig,
+    Labels,
+    LabelSet,
+    SemanticModelMetadata,
+)
 from nucliadb_protos.knowledgebox_pb2 import Synonyms as PBSynonyms
 from nucliadb_protos.knowledgebox_pb2 import VectorSet, VectorSets
 from nucliadb_protos.resources_pb2 import Basic
-from nucliadb_protos.utils_pb2 import VectorSimilarity
-from nucliadb_protos.writer_pb2 import GetLabelSetResponse, GetVectorSetsResponse
-from nucliadb_protos.writer_pb2 import Shards
-from nucliadb_protos.writer_pb2 import Shards as PBShards
+from nucliadb_protos.utils_pb2 import ReleaseChannel
 
+from nucliadb.common import datamanagers
+from nucliadb.common.cluster.base import AbstractIndexNode
+from nucliadb.common.cluster.exceptions import ShardNotFound, ShardsNotFound
+from nucliadb.common.cluster.manager import get_index_node
+from nucliadb.common.cluster.utils import get_shard_manager
+from nucliadb.common.maindb.driver import Driver, Transaction
 from nucliadb.ingest import SERVICE_NAME, logger
-from nucliadb.ingest.maindb.driver import Driver, Transaction
-from nucliadb.ingest.orm.abc import AbstractNode
-from nucliadb.ingest.orm.exceptions import (
-    KnowledgeBoxConflict,
-    KnowledgeBoxNotFound,
-    ShardNotFound,
-)
-from nucliadb.ingest.orm.local_node import LocalNode
-from nucliadb.ingest.orm.node import KB_SHARDS, Node
+from nucliadb.ingest.orm.exceptions import KnowledgeBoxConflict
 from nucliadb.ingest.orm.resource import (
     KB_RESOURCE_SLUG,
     KB_RESOURCE_SLUG_BASE,
     Resource,
 )
-from nucliadb.ingest.orm.shard import Shard
 from nucliadb.ingest.orm.synonyms import Synonyms
-from nucliadb.ingest.orm.utils import (
-    compute_paragraph_key,
-    get_basic,
-    get_node_klass,
-    set_basic,
-)
-from nucliadb_utils.exceptions import ShardsNotFound
-from nucliadb_utils.settings import indexing_settings
+from nucliadb.ingest.orm.utils import compute_paragraph_key, get_basic, set_basic
+from nucliadb.migrator.utils import get_latest_version
+from nucliadb_protos import writer_pb2
 from nucliadb_utils.storages.storage import Storage
 from nucliadb_utils.utilities import get_audit, get_storage
 
+# XXX Eventually all these keys should be moved to datamanagers.kb
 KB_RESOURCE = "/kbs/{kbid}/r/{uuid}"
 
 KB_KEYS = "/kbs/{kbid}/"
-KB_UUID = "/kbs/{kbid}/config"
-KB_LABELSET = "/kbs/{kbid}/labels/{id}"
-KB_LABELS = "/kbs/{kbid}/labels"
+
 KB_VECTORSET = "/kbs/{kbid}/vectorsets"
-KB_RESOURCE_SHARD = "/kbs/{kbid}/r/{uuid}/shard"
-KB_SLUGS_BASE = "/kbslugs/"
-KB_SLUGS = KB_SLUGS_BASE + "{slug}"
 
 KB_TO_DELETE_BASE = "/kbtodelete/"
 KB_TO_DELETE_STORAGE_BASE = "/storagetodelete/"
 
 KB_TO_DELETE = f"{KB_TO_DELETE_BASE}{{kbid}}"
 KB_TO_DELETE_STORAGE = f"{KB_TO_DELETE_STORAGE_BASE}{{kbid}}"
 
@@ -84,145 +74,111 @@
         self.storage = storage
         self.kbid = kbid
         self._config: Optional[KnowledgeBoxConfig] = None
         self.synonyms = Synonyms(self.txn, self.kbid)
 
     async def get_config(self) -> Optional[KnowledgeBoxConfig]:
         if self._config is None:
-            payload = await self.txn.get(KB_UUID.format(kbid=self.kbid))
-            if payload is not None:
-                response = KnowledgeBoxConfig()
-                response.ParseFromString(payload)
-                self._config = response
-                return response
+            async with datamanagers.with_transaction() as txn:
+                config = await datamanagers.kb.get_config(txn, kbid=self.kbid)
+            if config is not None:
+                self._config = config
+                return config
             else:
                 return None
         else:
             return self._config
 
     @classmethod
-    async def get_kb(cls, txn: Transaction, uuid: str) -> Optional[KnowledgeBoxConfig]:
-        payload = await txn.get(KB_UUID.format(kbid=uuid))
-        if payload is not None:
-            response = KnowledgeBoxConfig()
-            response.ParseFromString(payload)
-            return response
-        else:
-            return None
-
-    @classmethod
-    async def exist_kb(cls, txn: Transaction, uuid: str) -> bool:
-        payload = await txn.get(KB_UUID.format(kbid=uuid))
-        if payload is not None:
-            return True
-        else:
-            return False
-
-    @classmethod
     async def delete_kb(cls, txn: Transaction, slug: str = "", kbid: str = ""):
         # Mark storage to be deleted
         # Mark keys to be deleted
-        if kbid == "" and slug == "":
+        logger.info(f"Deleting KB kbid={kbid} slug={slug}")
+        if not kbid and not slug:
             raise AttributeError()
 
-        if kbid == "" and slug != "":
-            kbid_bytes = await txn.get(KB_SLUGS.format(slug=slug))
+        if slug and not kbid:
+            kbid_bytes = await txn.get(datamanagers.kb.KB_SLUGS.format(slug=slug))
             if kbid_bytes is None:
-                raise KnowledgeBoxNotFound()
+                raise datamanagers.exceptions.KnowledgeBoxNotFound()
             kbid = kbid_bytes.decode()
 
-        if slug == "" and kbid != "":
-            kbconfig_bytes = await txn.get(KB_UUID.format(kbid=kbid))
+        if kbid and not slug:
+            kbconfig_bytes = await txn.get(datamanagers.kb.KB_UUID.format(kbid=kbid))
             if kbconfig_bytes is None:
-                raise KnowledgeBoxNotFound()
+                raise datamanagers.exceptions.KnowledgeBoxNotFound()
             pbconfig = KnowledgeBoxConfig()
             pbconfig.ParseFromString(kbconfig_bytes)
             slug = pbconfig.slug
 
         # Delete main anchor
-        subtxn = await txn.driver.begin()
-        key_match = KB_SLUGS.format(slug=slug)
-        await subtxn.delete(key_match)
-
-        when = datetime.now().isoformat()
-        await subtxn.set(KB_TO_DELETE.format(kbid=kbid), when.encode())
-        await subtxn.commit()
+        async with txn.driver.transaction() as subtxn:
+            key_match = datamanagers.kb.KB_SLUGS.format(slug=slug)
+            logger.info(f"Deleting KB with slug: {slug}")
+            await subtxn.delete(key_match)
+
+            when = datetime.now().isoformat()
+            await subtxn.set(KB_TO_DELETE.format(kbid=kbid), when.encode())
+            await subtxn.commit()
 
         audit_util = get_audit()
         if audit_util is not None:
             await audit_util.delete_kb(kbid)
         return kbid
 
     @classmethod
-    async def get_kb_uuid(cls, txn: Transaction, slug: str) -> Optional[str]:
-        uuid = await txn.get(KB_SLUGS.format(slug=slug))
-        if uuid is not None:
-            return uuid.decode()
-        else:
-            return None
-
-    @classmethod
-    async def get_kbs(
-        cls, txn: Transaction, slug: str, count: int = -1
-    ) -> AsyncIterator[Tuple[str, str]]:
-        async for key in txn.keys(KB_SLUGS.format(slug=slug), count=count):
-            slug = key.replace(KB_SLUGS_BASE, "")
-            uuid = await cls.get_kb_uuid(txn, slug)
-            if uuid is None:
-                logger.error(f"KB with slug ({slug}) but without uuid?")
-                continue
-            yield (uuid, slug)
-
-    @classmethod
     async def create(
         cls,
         txn: Transaction,
         slug: str,
+        semantic_model: SemanticModelMetadata,
         uuid: Optional[str] = None,
         config: Optional[KnowledgeBoxConfig] = None,
-        similarity: VectorSimilarity.ValueType = VectorSimilarity.COSINE,
-    ) -> Tuple[str, bool]:
+        release_channel: ReleaseChannel.ValueType = ReleaseChannel.STABLE,
+    ) -> tuple[str, bool]:
         failed = False
-        exist = await cls.get_kb_uuid(txn, slug)
+        exist = await datamanagers.kb.get_kb_uuid(txn, slug=slug)
         if exist:
             raise KnowledgeBoxConflict()
         if uuid is None or uuid == "":
             uuid = str(uuid4())
 
         if slug == "":
             slug = uuid
 
         await txn.set(
-            KB_SLUGS.format(
-                slug=slug,
-            ),
+            datamanagers.kb.KB_SLUGS.format(slug=slug),
             uuid.encode(),
         )
         if config is None:
             config = KnowledgeBoxConfig()
 
+        config.migration_version = get_latest_version()
         config.slug = slug
         await txn.set(
-            KB_UUID.format(
-                kbid=uuid,
-            ),
+            datamanagers.kb.KB_UUID.format(kbid=uuid),
             config.SerializeToString(),
         )
         # Create Storage
         storage = await get_storage(service_name=SERVICE_NAME)
 
         created = await storage.create_kb(uuid)
         if created is False:
             logger.error(f"{uuid} KB could not be created")
             failed = True
 
         if failed is False:
+            shard_manager = get_shard_manager()
             try:
-                node_klass = get_node_klass()
-                await node_klass.create_shard_by_kbid(txn, uuid, similarity=similarity)
+                await shard_manager.create_shard_by_kbid(
+                    txn,
+                    uuid,
+                    semantic_model=semantic_model,
+                    release_channel=release_channel,
+                )
             except Exception as e:
                 await storage.delete_kb(uuid)
                 raise e
 
         if failed:
             await storage.delete_kb(uuid)
 
@@ -232,61 +188,53 @@
     async def update(
         cls,
         txn: Transaction,
         uuid: str,
         slug: Optional[str] = None,
         config: Optional[KnowledgeBoxConfig] = None,
     ) -> str:
-        exist = await cls.get_kb(txn, uuid)
+        exist = await datamanagers.kb.get_config(txn, kbid=uuid)
         if not exist:
-            raise KnowledgeBoxNotFound()
+            raise datamanagers.exceptions.KnowledgeBoxNotFound()
 
         if slug:
-            await txn.delete(
-                KB_SLUGS.format(
-                    slug=exist.slug,
-                )
-            )
+            await txn.delete(datamanagers.kb.KB_SLUGS.format(slug=exist.slug))
             await txn.set(
-                KB_SLUGS.format(
-                    slug=slug,
-                ),
+                datamanagers.kb.KB_SLUGS.format(slug=slug),
                 uuid.encode(),
             )
             if config:
                 config.slug = slug
             else:
                 exist.slug = slug
 
         if config and exist != config:
             exist.MergeFrom(config)
 
         await txn.set(
-            KB_UUID.format(
-                kbid=uuid,
-            ),
+            datamanagers.kb.KB_UUID.format(kbid=uuid),
             exist.SerializeToString(),
         )
 
         return uuid
 
-    async def iterate_kb_nodes(self) -> AsyncIterator[Tuple[AbstractNode, str]]:
-        shards_obj = await self.get_shards_object()
+    async def iterate_kb_nodes(self) -> AsyncIterator[tuple[AbstractIndexNode, str]]:
+        async with datamanagers.with_transaction() as txn:
+            shards_obj = await datamanagers.cluster.get_kb_shards(txn, kbid=self.kbid)
+            if shards_obj is None:
+                raise ShardsNotFound(self.kbid)
 
         for shard in shards_obj.shards:
             for replica in shard.replicas:
-                node_klass = get_node_klass()
-                node: Optional[Union[LocalNode, Node]] = await node_klass.get(
-                    replica.node
-                )
+                node = get_index_node(replica.node)
                 if node is not None:
                     yield node, replica.shard.id
 
     # Vectorset
-    async def get_vectorsets(self, response: GetVectorSetsResponse):
+    async def get_vectorsets(self, response: writer_pb2.GetVectorSetsResponse):
         vectorset_key = KB_VECTORSET.format(kbid=self.kbid)
         payload = await self.txn.get(vectorset_key)
         if payload is not None:
             response.vectorsets.ParseFromString(payload)
 
     async def del_vectorset(self, id: str):
         vectorset_key = KB_VECTORSET.format(kbid=self.kbid)
@@ -312,38 +260,36 @@
         async for node, shard in self.iterate_kb_nodes():
             await node.set_vectorset(shard, id, similarity=vs.similarity)
         payload = vts.SerializeToString()
         await self.txn.set(vectorset_key, payload)
 
     # Labels
     async def set_labelset(self, id: str, labelset: LabelSet):
-        labelset_key = KB_LABELSET.format(kbid=self.kbid, id=id)
-        await self.txn.set(labelset_key, labelset.SerializeToString())
+        await datamanagers.labels.set_labelset(
+            self.txn, kbid=self.kbid, labelset_id=id, labelset=labelset
+        )
 
     async def get_labels(self) -> Labels:
-        labels_key = KB_LABELS.format(kbid=self.kbid)
-        labels = Labels()
-        async for key in self.txn.keys(labels_key, count=-1):
-            labelset = await self.txn.get(key)
-            id = key.split("/")[-1]
-            if labelset is not None:
-                ls = LabelSet()
-                ls.ParseFromString(labelset)
-                labels.labelset[id].CopyFrom(ls)
-        return labels
-
-    async def get_labelset(self, labelset: str, labelset_response: GetLabelSetResponse):
-        labelset_key = KB_LABELSET.format(kbid=self.kbid, id=labelset)
-        payload = await self.txn.get(labelset_key)
-        if payload is not None:
-            labelset_response.labelset.ParseFromString(payload)
+        return await datamanagers.labels.get_labels(self.txn, kbid=self.kbid)
+
+    async def get_labelset(
+        self, labelset: str, labelset_response: writer_pb2.GetLabelSetResponse
+    ):
+        ls = await datamanagers.labels.get_labelset(
+            self.txn,
+            kbid=self.kbid,
+            labelset_id=labelset,
+        )
+        if ls is not None:
+            labelset_response.labelset.CopyFrom(ls)
 
     async def del_labelset(self, id: str):
-        labelset_key = KB_LABELSET.format(kbid=self.kbid, id=id)
-        await self.txn.delete(labelset_key)
+        await datamanagers.labels.delete_labelset(
+            self.txn, kbid=self.kbid, labelset_id=id
+        )
 
     async def get_synonyms(self, synonyms: PBSynonyms):
         pbsyn = await self.synonyms.get()
         if pbsyn is not None:
             synonyms.CopyFrom(pbsyn)
 
     async def set_synonyms(self, synonyms: PBSynonyms):
@@ -367,115 +313,96 @@
         KB_TO_DELETE_STORAGE key, so then purge cronjob will keep trying
         to delete once the emptying have been completed.
         """
         storage = await get_storage(service_name=SERVICE_NAME)
         exists = await storage.schedule_delete_kb(kbid)
         if exists is False:
             logger.error(f"{kbid} KB does not exists on Storage")
-        txn = await driver.begin()
-        storage_to_delete = KB_TO_DELETE_STORAGE.format(kbid=kbid)
-        await txn.set(storage_to_delete, b"")
-
-        # Delete KB Shards
-        shards_match = KB_SHARDS.format(kbid=kbid)
-        payload = await txn.get(shards_match)
 
-        if payload is None:
-            logger.warning(f"Shards not found for kbid={kbid}")
-        else:
-            shards_obj = Shards()
-            shards_obj.ParseFromString(payload)  # type: ignore
+        async with driver.transaction() as txn:
+            storage_to_delete = KB_TO_DELETE_STORAGE.format(kbid=kbid)
+            await txn.set(storage_to_delete, b"")
+
+            # Delete KB Shards
+            shards_match = datamanagers.cluster.KB_SHARDS.format(kbid=kbid)
+            payload = await txn.get(shards_match)
 
-            if not indexing_settings.index_local:
-                await Node.load_active_nodes()
+            if payload is None:
+                logger.warning(f"Shards not found for kbid={kbid}")
+            else:
+                shards_obj = writer_pb2.Shards()
+                shards_obj.ParseFromString(payload)  # type: ignore
 
-            for shard in shards_obj.shards:
-                # Delete the shard on nodes
-                for replica in shard.replicas:
-                    node_klass = get_node_klass()
-                    node: Optional[Union[LocalNode, Node]] = await node_klass.get(
-                        replica.node
-                    )
-                    if node is None:
-                        logger.info(f"No node {replica.node} found lets continue")
-                        continue
-                    try:
-                        await node.delete_shard(replica.shard.id)
-                        logger.debug(
-                            f"Succeded deleting shard from nodeid={replica.node} at {node.address}"
-                        )
-                    except AioRpcError as exc:
-                        if exc.code() == StatusCode.NOT_FOUND:
+                for shard in shards_obj.shards:
+                    # Delete the shard on nodes
+                    for replica in shard.replicas:
+                        node = get_index_node(replica.node)
+                        if node is None:
+                            logger.error(
+                                f"No node {replica.node} found, let's continue. Some shards may stay orphaned",
+                                extra={"kbid": kbid},
+                            )
                             continue
-                        await txn.abort()
-                        raise ShardNotFound(f"{exc.details()} @ {node.address}")
+                        try:
+                            await node.delete_shard(replica.shard.id)
+                            logger.debug(
+                                f"Succeded deleting shard from nodeid={replica.node} at {node.address}",
+                                extra={"kbid": kbid, "node_id": replica.node},
+                            )
+                        except AioRpcError as exc:
+                            if exc.code() == StatusCode.NOT_FOUND:
+                                continue
+                            raise ShardNotFound(f"{exc.details()} @ {node.address}")
 
-        await txn.commit()
+            await txn.commit()
         await cls.delete_all_kb_keys(driver, kbid)
 
     @classmethod
     async def delete_all_kb_keys(
         cls, driver: Driver, kbid: str, chunk_size: int = 1_000
     ):
         prefix = KB_KEYS.format(kbid=kbid)
         while True:
-            txn = await driver.begin()
-            all_keys = [key async for key in txn.keys(match=prefix, count=-1)]
-            await txn.abort()
+            async with driver.transaction() as txn:
+                all_keys = [key async for key in txn.keys(match=prefix, count=-1)]
 
             if len(all_keys) == 0:
                 break
 
             # We commit deletions in chunks because otherwise
             # tikv complains if there is too much data to commit
             for chunk_of_keys in chunker(all_keys, chunk_size):
-                txn = await driver.begin()
-                for key in chunk_of_keys:
-                    await txn.delete(key)
-                await txn.commit()
-
-    async def get_resource_shard(self, shard_id: str, node_klass) -> Optional[Shard]:
-        pb = await self.get_shards_object()
+                async with driver.transaction() as txn:
+                    for key in chunk_of_keys:
+                        await txn.delete(key)
+                    await txn.commit()
+
+    async def get_resource_shard(
+        self, shard_id: str
+    ) -> Optional[writer_pb2.ShardObject]:
+        async with datamanagers.with_transaction() as txn:
+            pb = await datamanagers.cluster.get_kb_shards(txn, kbid=self.kbid)
+            if pb is None:
+                logger.warning("Shards not found for kbid", extra={"kbid": self.kbid})
+                return None
         for shard in pb.shards:
             if shard.shard == shard_id:
-                return node_klass.create_shard_klass(shard_id, shard)
+                return shard
         return None
 
-    async def get_shards_object(self) -> PBShards:
-        key = KB_SHARDS.format(kbid=self.kbid)
-        payload = await self.txn.get(key)
-        if payload is None:
-            await self.txn.abort()
-            raise ShardsNotFound(self.kbid)
-        pb = PBShards()
-        pb.ParseFromString(payload)
-        return pb
-
-    async def get_similarity(self) -> VectorSimilarity.ValueType:
-        try:
-            shards_obj = await self.get_shards_object()
-            return shards_obj.similarity
-        except ShardsNotFound:
-            logger.warning(
-                f"Config for kb not found: {self.kbid} while trying to get the similarity. \
-                    Defaulting to cosine distance."
-            )
-            return VectorSimilarity.COSINE
-
     async def get(self, uuid: str) -> Optional[Resource]:
         raw_basic = await get_basic(self.txn, self.kbid, uuid)
         if raw_basic:
-            config = await self.get_config()
             return Resource(
                 txn=self.txn,
                 storage=self.storage,
                 kb=self,
                 uuid=uuid,
                 basic=Resource.parse_basic(raw_basic),
-                disable_vectors=config.disable_vectors if config is not None else True,
+                disable_vectors=False,
             )
         else:
             return None
 
     async def delete_resource(self, uuid: str):
         raw_basic = await get_basic(self.txn, self.kbid, uuid)
         if raw_basic:
@@ -493,87 +420,73 @@
             try:
                 await self.txn.delete(slug_key)
             except Exception:
                 pass
 
         await self.storage.delete_resource(self.kbid, uuid)
 
-    async def set_resource_shard_id(self, uuid: str, shard: str):
-        await self.txn.set(
-            KB_RESOURCE_SHARD.format(kbid=self.kbid, uuid=uuid), shard.encode()
-        )
-
-    async def get_resource_shard_id(self, uuid: str) -> Optional[str]:
-        shard = await self.txn.get(KB_RESOURCE_SHARD.format(kbid=self.kbid, uuid=uuid))
-        if shard is not None:
-            return shard.decode()
-        else:
-            return None
-
     async def get_resource_uuid_by_slug(self, slug: str) -> Optional[str]:
         uuid = await self.txn.get(KB_RESOURCE_SLUG.format(kbid=self.kbid, slug=slug))
         if uuid is not None:
             return uuid.decode()
         else:
             return None
 
     async def get_unique_slug(self, uuid: str, slug: str) -> str:
         key = KB_RESOURCE_SLUG.format(kbid=self.kbid, slug=slug)
         key_ok = False
         while key_ok is False:
             found = await self.txn.get(key)
-            if found is not None and found.decode() != uuid:
+            if found and found.decode() != uuid:
                 slug += ".c"
                 key = KB_RESOURCE_SLUG.format(kbid=self.kbid, slug=slug)
             else:
                 key_ok = True
         return slug
 
     @classmethod
     async def resource_slug_exists(
         self, txn: Transaction, kbid: str, slug: str
     ) -> bool:
         key = KB_RESOURCE_SLUG.format(kbid=kbid, slug=slug)
-        return await txn.get(key) is not None
+        encoded_slug: Optional[bytes] = await txn.get(key)
+        return encoded_slug not in (None, b"")
 
     async def add_resource(
         self, uuid: str, slug: str, basic: Optional[Basic] = None
     ) -> Resource:
         if basic is None:
             basic = Basic()
         if slug == "":
             slug = uuid
         slug = await self.get_unique_slug(uuid, slug)
         basic.slug = slug
         fix_paragraph_annotation_keys(uuid, basic)
         await set_basic(self.txn, self.kbid, uuid, basic)
-        config = await self.get_config()
         return Resource(
             storage=self.storage,
             txn=self.txn,
             kb=self,
             uuid=uuid,
             basic=basic,
-            disable_vectors=config.disable_vectors if config is not None else False,
+            disable_vectors=False,
         )
 
     async def iterate_resources(self) -> AsyncGenerator[Resource, None]:
         base = KB_RESOURCE_SLUG_BASE.format(kbid=self.kbid)
-        config = await self.get_config()
         async for key in self.txn.keys(match=base, count=-1):
-            uuid = await self.get_resource_uuid_by_slug(key.split("/")[-1])
+            slug = key.split("/")[-1]
+            uuid = await self.get_resource_uuid_by_slug(slug)
             if uuid is not None:
                 yield Resource(
                     self.txn,
                     self.storage,
                     self,
                     uuid,
-                    disable_vectors=config.disable_vectors
-                    if config is not None
-                    else False,
+                    disable_vectors=False,
                 )
 
 
 def chunker(seq: Sequence, size: int):
     return (seq[pos : pos + size] for pos in range(0, len(seq), size))
```

## nucliadb/ingest/orm/resource.py

```diff
@@ -16,134 +16,149 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
 import asyncio
+import logging
 from concurrent.futures import ThreadPoolExecutor
 from functools import partial
-from typing import TYPE_CHECKING, Any, AsyncIterator, Dict, List, Optional, Tuple, Type
+from typing import TYPE_CHECKING, Any, AsyncIterator, Optional, Type
 
+from nucliadb_protos.resources_pb2 import AllFieldIDs as PBAllFieldIDs
 from nucliadb_protos.resources_pb2 import Basic
 from nucliadb_protos.resources_pb2 import Basic as PBBasic
 from nucliadb_protos.resources_pb2 import CloudFile
 from nucliadb_protos.resources_pb2 import Conversation as PBConversation
+from nucliadb_protos.resources_pb2 import Extra as PBExtra
 from nucliadb_protos.resources_pb2 import (
     ExtractedTextWrapper,
     ExtractedVectorsWrapper,
     FieldClassifications,
     FieldComputedMetadataWrapper,
     FieldID,
     FieldMetadata,
+    FieldQuestionAnswerWrapper,
     FieldText,
     FieldType,
     FileExtractedData,
     LargeComputedMetadataWrapper,
     LinkExtractedData,
 )
 from nucliadb_protos.resources_pb2 import Metadata
 from nucliadb_protos.resources_pb2 import Metadata as PBMetadata
 from nucliadb_protos.resources_pb2 import Origin as PBOrigin
-from nucliadb_protos.resources_pb2 import ParagraphAnnotation
+from nucliadb_protos.resources_pb2 import Paragraph, ParagraphAnnotation
 from nucliadb_protos.resources_pb2 import Relations as PBRelations
-from nucliadb_protos.resources_pb2 import UserVectorsWrapper
 from nucliadb_protos.train_pb2 import EnabledMetadata
 from nucliadb_protos.train_pb2 import Position as TrainPosition
 from nucliadb_protos.train_pb2 import (
     TrainField,
     TrainMetadata,
     TrainParagraph,
     TrainResource,
     TrainSentence,
 )
 from nucliadb_protos.utils_pb2 import Relation as PBRelation
 from nucliadb_protos.writer_pb2 import BrokerMessage
 
+from nucliadb.common.maindb.driver import Transaction
 from nucliadb.ingest.fields.base import Field
 from nucliadb.ingest.fields.conversation import Conversation
 from nucliadb.ingest.fields.date import Datetime
 from nucliadb.ingest.fields.file import File
-from nucliadb.ingest.fields.generic import VALID_GLOBAL, Generic
+from nucliadb.ingest.fields.generic import VALID_GENERIC_FIELDS, Generic
 from nucliadb.ingest.fields.keywordset import Keywordset
 from nucliadb.ingest.fields.layout import Layout
 from nucliadb.ingest.fields.link import Link
 from nucliadb.ingest.fields.text import Text
-from nucliadb.ingest.maindb.driver import Transaction
 from nucliadb.ingest.orm.brain import FilePagePositions, ResourceBrain
+from nucliadb.ingest.orm.metrics import processor_observer
 from nucliadb.ingest.orm.utils import get_basic, set_basic
 from nucliadb_models.common import CloudLink
 from nucliadb_models.writer import GENERIC_MIME_TYPE
+from nucliadb_protos import utils_pb2, writer_pb2
 from nucliadb_utils.storages.storage import Storage
 
 if TYPE_CHECKING:  # pragma: no cover
     from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
 
+logger = logging.getLogger(__name__)
+
 KB_RESOURCE_ORIGIN = "/kbs/{kbid}/r/{uuid}/origin"
+KB_RESOURCE_EXTRA = "/kbs/{kbid}/r/{uuid}/extra"
+KB_RESOURCE_SECURITY = "/kbs/{kbid}/r/{uuid}/security"
 KB_RESOURCE_METADATA = "/kbs/{kbid}/r/{uuid}/metadata"
 KB_RESOURCE_RELATIONS = "/kbs/{kbid}/r/{uuid}/relations"
 KB_RESOURCE_FIELDS = "/kbs/{kbid}/r/{uuid}/f/"
+KB_RESOURCE_ALL_FIELDS = "/kbs/{kbid}/r/{uuid}/allfields"
 KB_RESOURCE_SLUG_BASE = "/kbs/{kbid}/s/"
 KB_RESOURCE_SLUG = f"{KB_RESOURCE_SLUG_BASE}{{slug}}"
 KB_RESOURCE_CONVERSATION = "/kbs/{kbid}/r/{uuid}/c/{page}"
 GLOBAL_FIELD = "a"
-KB_FIELDS: Dict[int, Type] = {
+KB_FIELDS: dict[int, Type] = {
     FieldType.LAYOUT: Layout,
     FieldType.TEXT: Text,
     FieldType.FILE: File,
     FieldType.LINK: Link,
     FieldType.DATETIME: Datetime,
     FieldType.KEYWORDSET: Keywordset,
     FieldType.GENERIC: Generic,
     FieldType.CONVERSATION: Conversation,
 }
 
-KB_REVERSE: Dict[str, int] = {
+KB_REVERSE: dict[str, FieldType.ValueType] = {
     "l": FieldType.LAYOUT,
     "t": FieldType.TEXT,
     "f": FieldType.FILE,
     "u": FieldType.LINK,
     "d": FieldType.DATETIME,
     "k": FieldType.KEYWORDSET,
     "a": FieldType.GENERIC,
     "c": FieldType.CONVERSATION,
 }
 
-KB_REVERSE_REVERSE = {v: k for k, v in KB_REVERSE.items()}
+FIELD_TYPE_TO_ID = {v: k for k, v in KB_REVERSE.items()}
 
 _executor = ThreadPoolExecutor(10)
 
 
 PB_TEXT_FORMAT_TO_MIMETYPE = {
     FieldText.Format.PLAIN: "text/plain",
     FieldText.Format.HTML: "text/html",
     FieldText.Format.RST: "text/x-rst",
     FieldText.Format.MARKDOWN: "text/markdown",
+    FieldText.Format.JSON: "application/json",
+    FieldText.Format.KEEP_MARKDOWN: "text/markdown",
 }
 
+BASIC_IMMUTABLE_FIELDS = ("icon",)
+
 
 class Resource:
     def __init__(
         self,
         txn: Transaction,
         storage: Storage,
         kb: KnowledgeBox,
         uuid: str,
         basic: Optional[PBBasic] = None,
         disable_vectors: bool = True,
     ):
-        self.fields: Dict[Tuple[int, str], Field] = {}
-        self.conversations: Dict[int, PBConversation] = {}
+        self.fields: dict[tuple[FieldType.ValueType, str], Field] = {}
+        self.conversations: dict[int, PBConversation] = {}
         self.relations: Optional[PBRelations] = None
-        self.all_fields_keys: List[Tuple[int, str]] = []
+        self.all_fields_keys: Optional[list[tuple[FieldType.ValueType, str]]] = None
         self.origin: Optional[PBOrigin] = None
+        self.extra: Optional[PBExtra] = None
+        self.security: Optional[utils_pb2.Security] = None
         self.modified: bool = False
-        self.slug_modified: bool = False
         self._indexer: Optional[ResourceBrain] = None
-        self._modified_extracted_text: List[FieldID] = []
+        self._modified_extracted_text: list[FieldID] = []
 
         self.txn = txn
         self.storage = storage
         self.kb = kb
         self.uuid = uuid
         self.basic = basic
         self.disable_vectors = disable_vectors
@@ -192,25 +207,32 @@
         return self.basic
 
     def set_processing_status(self, current_basic: PBBasic, basic_in_payload: PBBasic):
         self._previous_status = current_basic.metadata.status
         if basic_in_payload.HasField("metadata") and basic_in_payload.metadata.useful:
             current_basic.metadata.status = basic_in_payload.metadata.status
 
+    @processor_observer.wrap({"type": "set_basic"})
     async def set_basic(
         self,
         payload: PBBasic,
-        slug: Optional[str] = None,
-        deleted_fields: Optional[List[FieldID]] = None,
+        deleted_fields: Optional[list[FieldID]] = None,
     ):
-        """
-        Some basic fields are computed off field metadata. This means we need to recompute upon field deletions.
-        """
         await self.get_basic()
-        if self.basic is not None and self.basic != payload:
+
+        if self.basic is None:
+            self.basic = payload
+
+        elif self.basic != payload:
+            for field in BASIC_IMMUTABLE_FIELDS:
+                # Immutable basic fields that are already set are cleared
+                # from the payload so that they are not overwritten
+                if getattr(self.basic, field, "") != "":
+                    payload.ClearField(field)  # type: ignore
+
             self.basic.MergeFrom(payload)
 
             self.set_processing_status(self.basic, payload)
 
             # We force the usermetadata classification to be the one defined
             if payload.HasField("usermetadata"):
                 self.basic.usermetadata.CopyFrom(payload.usermetadata)
@@ -252,76 +274,120 @@
                             replace_field=[],
                             replace_splits={},
                             page_positions=page_positions,
                             extracted_text=await field_obj.get_extracted_text(),
                             basic_user_field_metadata=user_field_metadata,
                         )
 
-        else:
-            self.basic = payload
-        if slug is not None and slug != "":
-            slug = await self.kb.get_unique_slug(self.uuid, slug)
-            self.basic.slug = slug
-            self.slug_modified = True
+        # Some basic fields are computed off field metadata.
+        # This means we need to recompute upon field deletions.
         if deleted_fields is not None and len(deleted_fields) > 0:
             remove_field_classifications(self.basic, deleted_fields=deleted_fields)
+
         await set_basic(self.txn, self.kb.kbid, self.uuid, self.basic)
         self.modified = True
 
     # Origin
     async def get_origin(self) -> Optional[PBOrigin]:
         if self.origin is None:
             pb = PBOrigin()
             payload = await self.txn.get(
                 KB_RESOURCE_ORIGIN.format(kbid=self.kb.kbid, uuid=self.uuid)
             )
             if payload is None:
                 return None
+
             pb.ParseFromString(payload)
             self.origin = pb
         return self.origin
 
     async def set_origin(self, payload: PBOrigin):
         await self.txn.set(
             KB_RESOURCE_ORIGIN.format(kbid=self.kb.kbid, uuid=self.uuid),
             payload.SerializeToString(),
         )
         self.modified = True
         self.origin = payload
 
+    # Extra
+    async def get_extra(self) -> Optional[PBExtra]:
+        if self.extra is None:
+            pb = PBExtra()
+            payload = await self.txn.get(
+                KB_RESOURCE_EXTRA.format(kbid=self.kb.kbid, uuid=self.uuid)
+            )
+            if payload is None:
+                return None
+            pb.ParseFromString(payload)
+            self.extra = pb
+        return self.extra
+
+    async def set_extra(self, payload: PBExtra):
+        key = KB_RESOURCE_EXTRA.format(kbid=self.kb.kbid, uuid=self.uuid)
+        await self.txn.set(
+            key,
+            payload.SerializeToString(),
+        )
+        self.modified = True
+        self.extra = payload
+
+    # Security
+    async def get_security(self) -> Optional[utils_pb2.Security]:
+        if self.security is None:
+            pb = utils_pb2.Security()
+            key = KB_RESOURCE_SECURITY.format(kbid=self.kb.kbid, uuid=self.uuid)
+            payload = await self.txn.get(key)
+            if payload is None:
+                return None
+            pb.ParseFromString(payload)
+            self.security = pb
+        return self.security
+
+    async def set_security(self, payload: utils_pb2.Security) -> None:
+        key = KB_RESOURCE_SECURITY.format(kbid=self.kb.kbid, uuid=self.uuid)
+        await self.txn.set(
+            key,
+            payload.SerializeToString(),
+        )
+        self.modified = True
+        self.security = payload
+
     # Relations
     async def get_relations(self) -> Optional[PBRelations]:
         if self.relations is None:
             pb = PBRelations()
             payload = await self.txn.get(
                 KB_RESOURCE_RELATIONS.format(kbid=self.kb.kbid, uuid=self.uuid)
             )
             if payload is None:
                 return None
             pb.ParseFromString(payload)
             self.relations = pb
         return self.relations
 
-    async def set_relations(self, payload: List[PBRelation]):
+    async def set_relations(self, payload: list[PBRelation]):
         relations = PBRelations()
         for relation in payload:
             relations.relations.append(relation)
         await self.txn.set(
             KB_RESOURCE_RELATIONS.format(kbid=self.kb.kbid, uuid=self.uuid),
             relations.SerializeToString(),
         )
         self.modified = True
         self.relations = relations
 
+    @processor_observer.wrap({"type": "generate_index_message"})
     async def generate_index_message(self) -> ResourceBrain:
         brain = ResourceBrain(rid=self.uuid)
         origin = await self.get_origin()
         basic = await self.get_basic()
         if basic is not None:
-            brain.set_global_tags(basic, self.uuid, origin)
+            brain.set_resource_metadata(basic, origin)
+        await self.compute_security(brain)
+        await self.compute_global_tags(brain)
         fields = await self.get_fields(force=True)
         for (type_id, field_id), field in fields.items():
             fieldid = FieldID(field_type=type_id, field=field_id)  # type: ignore
             await self.compute_global_text_field(fieldid, brain)
 
             field_metadata = await field.get_field_metadata()
             field_key = self.generate_field_id(fieldid)
@@ -351,61 +417,52 @@
                     basic_user_field_metadata=user_field_metadata,
                 )
 
             if self.disable_vectors is False:
                 vo = await field.get_vectors()
                 if vo is not None:
                     brain.apply_field_vectors(field_key, vo, False, [])
-
-                vu = await field.get_user_vectors()
-                if vu is not None:
-                    vectors_to_delete = {}  # type: ignore
-                    brain.apply_user_vectors(field_key, vu, vectors_to_delete)  # type: ignore
         return brain
 
     async def generate_field_vectors(
-        self, bm: BrokerMessage, type_id: int, field_id: str, field: Field
+        self,
+        bm: BrokerMessage,
+        type_id: FieldType.ValueType,
+        field_id: str,
+        field: Field,
     ):
         vo = await field.get_vectors()
         if vo is None:
             return
         evw = ExtractedVectorsWrapper()
         evw.field.field = field_id
         evw.field.field_type = type_id  # type: ignore
         evw.vectors.CopyFrom(vo)
         bm.field_vectors.append(evw)
 
-    async def generate_user_vectors(
-        self, bm: BrokerMessage, type_id: int, field_id: str, field: Field
-    ):
-        uv = await field.get_user_vectors()
-        if uv is None:
-            return
-        uvw = UserVectorsWrapper()
-        uvw.field.field = field_id
-        uvw.field.field_type = type_id  # type: ignore
-        uvw.vectors.CopyFrom(uv)
-        bm.user_vectors.append(uvw)
-
     async def generate_field_large_computed_metadata(
-        self, bm: BrokerMessage, type_id: int, field_id: str, field: Field
+        self,
+        bm: BrokerMessage,
+        type_id: FieldType.ValueType,
+        field_id: str,
+        field: Field,
     ):
         lcm = await field.get_large_field_metadata()
         if lcm is None:
             return
         lcmw = LargeComputedMetadataWrapper()
         lcmw.field.field = field_id
         lcmw.field.field_type = type_id  # type: ignore
         lcmw.real.CopyFrom(lcm)
         bm.field_large_metadata.append(lcmw)
 
     async def generate_field_computed_metadata(
         self,
         bm: BrokerMessage,
-        type_id: int,
+        type_id: FieldType.ValueType,
         field_id: str,
         field: Field,
     ):
         fcmw = FieldComputedMetadataWrapper()
         fcmw.field.field = field_id
         fcmw.field.field_type = type_id  # type: ignore
 
@@ -414,54 +471,76 @@
             fcmw.metadata.CopyFrom(field_metadata)
             fcmw.field.field = field_id
             fcmw.field.field_type = type_id  # type: ignore
             bm.field_metadata.append(fcmw)
             # Make sure cloud files are removed for exporting
 
     async def generate_extracted_text(
-        self, bm: BrokerMessage, type_id: int, field_id: str, field: Field
+        self,
+        bm: BrokerMessage,
+        type_id: FieldType.ValueType,
+        field_id: str,
+        field: Field,
     ):
         etw = ExtractedTextWrapper()
         etw.field.field = field_id
         etw.field.field_type = type_id  # type: ignore
         extracted_text = await field.get_extracted_text()
         if extracted_text is not None:
             etw.body.CopyFrom(extracted_text)
             bm.extracted_text.append(etw)
 
     async def generate_field(
         self,
         bm: BrokerMessage,
-        type_id: int,
+        type_id: FieldType.ValueType,
         field_id: str,
         field: Field,
     ):
         # Used for exporting a field
         if type_id == FieldType.TEXT:
             value = await field.get_value()
             bm.texts[field_id].CopyFrom(value)
         elif type_id == FieldType.LINK:
             value = await field.get_value()
             bm.links[field_id].CopyFrom(value)
         elif type_id == FieldType.FILE:
             value = await field.get_value()
             bm.files[field_id].CopyFrom(value)
         elif type_id == FieldType.CONVERSATION:
-            value = await field.get_value()
+            value = await self.get_full_conversation(field)  # type: ignore
             bm.conversations[field_id].CopyFrom(value)
         elif type_id == FieldType.KEYWORDSET:
             value = await field.get_value()
             bm.keywordsets[field_id].CopyFrom(value)
         elif type_id == FieldType.DATETIME:
             value = await field.get_value()
             bm.datetimes[field_id].CopyFrom(value)
         elif type_id == FieldType.LAYOUT:
             value = await field.get_value()
             bm.layouts[field_id].CopyFrom(value)
 
+    async def get_full_conversation(
+        self,
+        conversation_field: Conversation,
+    ) -> Optional[PBConversation]:
+        """
+        Messages of a conversations may be stored across several pages.
+        This method fetches them all and returns a single complete conversation.
+        """
+        full_conv = PBConversation()
+        n_page = 1
+        while True:
+            page = await conversation_field.get_value(page=n_page)
+            if page is None:
+                break
+            full_conv.messages.extend(page.messages)
+            n_page += 1
+        return full_conv
+
     async def generate_broker_message(self) -> BrokerMessage:
         # full means downloading all the pointers
         # minuts the ones to external files that are not PB
         # Go for all fields and recreate brain
         bm = BrokerMessage()
         bm.kbid = self.kb.kbid
         bm.uuid = self.uuid
@@ -497,126 +576,229 @@
                 link_extracted_data = await field.get_link_extracted_data()
                 if link_extracted_data is not None:
                     bm.link_extracted_data.append(link_extracted_data)
 
             # Field vectors
             await self.generate_field_vectors(bm, type_id, field_id, field)
 
-            # User vectors
-            await self.generate_user_vectors(bm, type_id, field_id, field)
-
             # Large metadata
             await self.generate_field_large_computed_metadata(
                 bm, type_id, field_id, field
             )
 
         return bm
 
     # Fields
-    async def get_fields(self, force: bool = False) -> Dict[Tuple[int, str], Field]:
+    async def get_fields(
+        self, force: bool = False
+    ) -> dict[tuple[FieldType.ValueType, str], Field]:
         # Get all fields
         for type, field in await self.get_fields_ids(force=force):
             if (type, field) not in self.fields:
                 self.fields[(type, field)] = await self.get_field(field, type)
         return self.fields
 
-    async def get_fields_ids(self, force: bool = False) -> List[Tuple[int, str]]:
-        # Get all fields
+    async def _deprecated_scan_fields_ids(
+        self,
+    ) -> AsyncIterator[tuple[FieldType.ValueType, str]]:
+        logger.warning("Scanning fields ids. This is not optimal.")
+        prefix = KB_RESOURCE_FIELDS.format(kbid=self.kb.kbid, uuid=self.uuid)
+        allfields = set()
+        async for key in self.txn.keys(prefix, count=-1):
+            # The [6:8] `slicing purpose is to match exactly the two
+            # splitted parts corresponding to type and field, and nothing else!
+            type, field = key.split("/")[6:8]
+            type_id = KB_REVERSE.get(type)
+            if type_id is None:
+                raise AttributeError("Invalid field type")
+            result = (type_id, field)
+            if result not in allfields:
+                # fields can have errors that are stored in a subkey:
+                # - field key       -> kbs/kbid/r/ruuid/f/myfield
+                # - field error key -> kbs/kbid/r/ruuid/f/myfield/errors
+                # and that would return duplicates here.
+                yield result
+            allfields.add(result)
+
+    async def _inner_get_fields_ids(self) -> list[tuple[FieldType.ValueType, str]]:
+        # Use a set to make sure we don't have duplicate field ids
+        result = set()
+        all_fields = await self.get_all_field_ids()
+        if all_fields is not None:
+            for f in all_fields.fields:
+                result.add((f.field_type, f.field))
+        # We make sure that title and summary are set to be added
         basic = await self.get_basic()
-        if len(self.all_fields_keys) == 0 or force:
-            result = []
-            fields = KB_RESOURCE_FIELDS.format(kbid=self.kb.kbid, uuid=self.uuid)
-            async for key in self.txn.keys(fields, count=-1):
-                # The [6:8] `slicing purpose is to match exactly the two
-                # splitted parts corresponding to type and field, and nothing else!
-                type, field = key.split("/")[6:8]
-                type_id = KB_REVERSE.get(type)
-                if type_id is None:
-                    raise AttributeError("Invalid field type")
-                result.append((type_id, field))
-
-            for generic in VALID_GLOBAL:
-                # We make sure that title and summary are set to be added
+        if basic is not None:
+            for generic in VALID_GENERIC_FIELDS:
                 append = True
-                if generic == "title" and (basic is None or basic.title == ""):
+                if generic == "title" and basic.title == "":
                     append = False
-                elif generic == "summary" and (basic is None or basic.summary == ""):
+                elif generic == "summary" and basic.summary == "":
                     append = False
                 if append:
-                    result.append((FieldType.GENERIC, generic))
+                    result.add((FieldType.GENERIC, generic))
+        return list(result)
 
-            self.all_fields_keys = result
+    async def get_fields_ids(
+        self, force: bool = False
+    ) -> list[tuple[FieldType.ValueType, str]]:
+        """
+        Get all ids of the fields of the resource and cache them.
+        """
+        # Get all fields
+        if self.all_fields_keys is None or force is True:
+            self.all_fields_keys = await self._inner_get_fields_ids()
         return self.all_fields_keys
 
-    async def get_field(self, key: str, type: int, load: bool = True):
+    async def get_field(self, key: str, type: FieldType.ValueType, load: bool = True):
         field = (type, key)
         if field not in self.fields:
             field_obj: Field = KB_FIELDS[type](id=key, resource=self)
             if load:
                 await field_obj.get_value()
             self.fields[field] = field_obj
         return self.fields[field]
 
-    async def set_field(self, type: int, key: str, payload: Any):
+    async def set_field(self, type: FieldType.ValueType, key: str, payload: Any):
         field = (type, key)
         if field not in self.fields:
             field_obj: Field = KB_FIELDS[type](id=key, resource=self)
             self.fields[field] = field_obj
         else:
             field_obj = self.fields[field]
         await field_obj.set_value(payload)
+        if self.all_fields_keys is None:
+            self.all_fields_keys = []
         self.all_fields_keys.append(field)
-
         self.modified = True
         return field_obj
 
-    async def delete_field(self, type: int, key: str):
+    async def delete_field(self, type: FieldType.ValueType, key: str):
         field = (type, key)
         if field in self.fields:
             field_obj = self.fields[field]
             del self.fields[field]
         else:
             field_obj = KB_FIELDS[type](id=key, resource=self)
 
+        if self.all_fields_keys is not None:
+            if field in self.all_fields_keys:
+                self.all_fields_keys.remove(field)
+
         field_key = self.generate_field_id(FieldID(field_type=type, field=key))  # type: ignore
         vo = await field_obj.get_vectors()
         if vo is not None:
             self.indexer.delete_vectors(field_key=field_key, vo=vo)
 
         metadata = await field_obj.get_field_metadata()
         if metadata is not None:
             self.indexer.delete_metadata(field_key=field_key, metadata=metadata)
 
         await field_obj.delete()
 
+    def has_field(self, type: FieldType.ValueType, field: str) -> bool:
+        return (type, field) in self.fields
+
+    async def get_all_field_ids(self) -> Optional[PBAllFieldIDs]:
+        key = KB_RESOURCE_ALL_FIELDS.format(kbid=self.kb.kbid, uuid=self.uuid)
+        payload = await self.txn.get(key)
+        if payload is None:
+            return None
+        all_fields = PBAllFieldIDs()
+        all_fields.ParseFromString(payload)
+        return all_fields
+
+    async def set_all_field_ids(self, all_fields: PBAllFieldIDs):
+        key = KB_RESOURCE_ALL_FIELDS.format(kbid=self.kb.kbid, uuid=self.uuid)
+        await self.txn.set(key, all_fields.SerializeToString())
+
+    async def update_all_field_ids(
+        self,
+        *,
+        updated: Optional[list[FieldID]] = None,
+        deleted: Optional[list[FieldID]] = None,
+        errors: Optional[list[writer_pb2.Error]] = None,
+    ):
+        needs_update = False
+        all_fields = await self.get_all_field_ids()
+        if all_fields is None:
+            needs_update = True
+            all_fields = PBAllFieldIDs()
+
+        for field in updated or []:
+            if field not in all_fields.fields:
+                all_fields.fields.append(field)
+                needs_update = True
+
+        for error in errors or []:
+            field_id = FieldID(field_type=error.field_type, field=error.field)
+            if field_id not in all_fields.fields:
+                all_fields.fields.append(field_id)
+                needs_update = True
+
+        for field in deleted or []:
+            if field in all_fields.fields:
+                all_fields.fields.remove(field)
+                needs_update = True
+
+        if needs_update:
+            await self.set_all_field_ids(all_fields)
+
+    @processor_observer.wrap({"type": "apply_fields"})
     async def apply_fields(self, message: BrokerMessage):
+        message_updated_fields = []
         for field, layout in message.layouts.items():
-            await self.set_field(FieldType.LAYOUT, field, layout)
+            fid = FieldID(field_type=FieldType.LAYOUT, field=field)
+            await self.set_field(fid.field_type, fid.field, layout)
+            message_updated_fields.append(fid)
 
         for field, text in message.texts.items():
-            await self.set_field(FieldType.TEXT, field, text)
+            fid = FieldID(field_type=FieldType.TEXT, field=field)
+            await self.set_field(fid.field_type, fid.field, text)
+            message_updated_fields.append(fid)
 
         for field, keywordset in message.keywordsets.items():
-            await self.set_field(FieldType.KEYWORDSET, field, keywordset)
+            fid = FieldID(field_type=FieldType.KEYWORDSET, field=field)
+            await self.set_field(fid.field_type, fid.field, keywordset)
+            message_updated_fields.append(fid)
 
         for field, datetimeobj in message.datetimes.items():
-            await self.set_field(FieldType.DATETIME, field, datetimeobj)
+            fid = FieldID(field_type=FieldType.DATETIME, field=field)
+            await self.set_field(fid.field_type, fid.field, datetimeobj)
+            message_updated_fields.append(fid)
 
         for field, link in message.links.items():
-            await self.set_field(FieldType.LINK, field, link)
+            fid = FieldID(field_type=FieldType.LINK, field=field)
+            await self.set_field(fid.field_type, fid.field, link)
+            message_updated_fields.append(fid)
 
         for field, file in message.files.items():
-            await self.set_field(FieldType.FILE, field, file)
+            fid = FieldID(field_type=FieldType.FILE, field=field)
+            await self.set_field(fid.field_type, fid.field, file)
+            message_updated_fields.append(fid)
 
         for field, conversation in message.conversations.items():
-            await self.set_field(FieldType.CONVERSATION, field, conversation)
+            fid = FieldID(field_type=FieldType.CONVERSATION, field=field)
+            await self.set_field(fid.field_type, fid.field, conversation)
+            message_updated_fields.append(fid)
 
         for fieldid in message.delete_fields:
             await self.delete_field(fieldid.field_type, fieldid.field)
 
+        if (
+            len(message_updated_fields)
+            or len(message.delete_fields)
+            or len(message.errors)
+        ):
+            await self.update_all_field_ids(
+                updated=message_updated_fields, deleted=message.delete_fields, errors=message.errors  # type: ignore
+            )
+
+    @processor_observer.wrap({"type": "apply_extracted"})
     async def apply_extracted(self, message: BrokerMessage):
         errors = False
         field_obj: Field
         for error in message.errors:
             field_obj = await self.get_field(error.field, error.field_type, load=False)
             await field_obj.set_error(error)
             errors = True
@@ -631,34 +813,43 @@
         if errors:
             self.basic.metadata.status = PBMetadata.Status.ERROR
         elif errors is False and message.source is message.MessageSource.PROCESSOR:
             self.basic.metadata.status = PBMetadata.Status.PROCESSED
 
         maybe_update_basic_icon(self.basic, get_text_field_mimetype(message))
 
+        for question_answers in message.question_answers:
+            await self._apply_question_answers(question_answers)
+
         for extracted_text in message.extracted_text:
             await self._apply_extracted_text(extracted_text)
 
+        extracted_languages = []
+
         for link_extracted_data in message.link_extracted_data:
             await self._apply_link_extracted_data(link_extracted_data)
+            await self.maybe_update_title_metadata(link_extracted_data)
+            extracted_languages.append(link_extracted_data.language)
 
         for file_extracted_data in message.file_extracted_data:
             await self._apply_file_extracted_data(file_extracted_data)
+            extracted_languages.append(file_extracted_data.language)
 
         # Metadata should go first
         for field_metadata in message.field_metadata:
             await self._apply_field_computed_metadata(field_metadata)
+            extracted_languages.extend(extract_field_metadata_languages(field_metadata))
+
+        update_basic_languages(self.basic, extracted_languages)
 
         # Upload to binary storage
         # Vector indexing
         if self.disable_vectors is False:
             for field_vectors in message.field_vectors:
                 await self._apply_extracted_vectors(field_vectors)
-            for user_vectors in message.user_vectors:
-                await self._apply_user_vectors(user_vectors)
 
         # Only uploading to binary storage
         for field_large_metadata in message.field_large_metadata:
             await self._apply_field_large_metadata(field_large_metadata)
 
         for relation in message.relations:
             self.indexer.brain.relations.append(relation)
@@ -674,45 +865,80 @@
             extracted_text.field.field, extracted_text.field.field_type, load=False
         )
         await field_obj.set_extracted_text(extracted_text)
         self._modified_extracted_text.append(
             extracted_text.field,
         )
 
+    async def _apply_question_answers(
+        self, question_answers: FieldQuestionAnswerWrapper
+    ):
+        field = question_answers.field
+        field_obj = await self.get_field(field.field, field.field_type, load=False)
+        await field_obj.set_question_answers(question_answers)
+
     async def _apply_link_extracted_data(self, link_extracted_data: LinkExtractedData):
         assert self.basic is not None
         field_link: Link = await self.get_field(
             link_extracted_data.field,
             FieldType.LINK,
             load=False,
         )
         maybe_update_basic_thumbnail(self.basic, link_extracted_data.link_thumbnail)
 
         await field_link.set_link_extracted_data(link_extracted_data)
 
         maybe_update_basic_icon(self.basic, "application/stf-link")
 
-        if (
-            self.basic.title.startswith("http") and link_extracted_data.title != ""
-        ) or (self.basic.title == "" and link_extracted_data.title != ""):
-            # If the title was http something or empty replace
-            self.basic.title = link_extracted_data.title
-
         maybe_update_basic_summary(self.basic, link_extracted_data.description)
 
+    async def maybe_update_title_metadata(self, link_extracted_data: LinkExtractedData):
+        assert self.basic is not None
+        if not link_extracted_data.title:
+            return
+        if not (self.basic.title.startswith("http") or self.basic.title == ""):
+            return
+
+        title = link_extracted_data.title
+        self.basic.title = title
+        # Extracted text
+        field = await self.get_field("title", FieldType.GENERIC, load=False)
+        etw = ExtractedTextWrapper()
+        etw.body.text = title
+        await field.set_extracted_text(etw)
+
+        # Field computed metadata
+        fcmw = FieldComputedMetadataWrapper()
+        fcmw.field.field = "title"
+        fcmw.field.field_type = FieldType.GENERIC
+
+        # Merge with any existing field computed metadata
+        fcm = await field.get_field_metadata(force=True)
+        if fcm is not None:
+            fcmw.metadata.CopyFrom(fcm)
+
+        fcmw.metadata.metadata.ClearField("paragraphs")
+        paragraph = Paragraph(
+            start=0, end=len(title), kind=Paragraph.TypeParagraph.TITLE
+        )
+        fcmw.metadata.metadata.paragraphs.append(paragraph)
+
+        await field.set_field_metadata(fcmw)
+
     async def _apply_file_extracted_data(self, file_extracted_data: FileExtractedData):
         assert self.basic is not None
         field_file: File = await self.get_field(
             file_extracted_data.field,
             FieldType.FILE,
             load=False,
         )
+        # uri can change after extraction
+        await field_file.set_file_extracted_data(file_extracted_data)
         maybe_update_basic_icon(self.basic, file_extracted_data.icon)
         maybe_update_basic_thumbnail(self.basic, file_extracted_data.file_thumbnail)
-        await field_file.set_file_extracted_data(file_extracted_data)
 
     async def _apply_field_computed_metadata(
         self, field_metadata: FieldComputedMetadataWrapper
     ):
         assert self.basic is not None
         maybe_update_basic_summary(self.basic, field_metadata.metadata.metadata.summary)
 
@@ -740,34 +966,44 @@
                 for fm in self.basic.fieldmetadata
                 if fm.field.field == field_metadata.field.field
                 and fm.field.field_type == field_metadata.field.field_type
             ),
             None,
         )
 
+        extracted_text = await field_obj.get_extracted_text()
         apply_field_metadata = partial(
             self.indexer.apply_field_metadata,
             field_key,
             metadata,
             replace_field=replace_field,
             replace_splits=replace_splits,
             page_positions=page_positions,
-            extracted_text=await field_obj.get_extracted_text(),
+            extracted_text=extracted_text,
             basic_user_field_metadata=user_field_metadata,
         )
         loop = asyncio.get_running_loop()
         await loop.run_in_executor(_executor, apply_field_metadata)
 
         maybe_update_basic_thumbnail(
             self.basic, field_metadata.metadata.metadata.thumbnail
         )
 
         add_field_classifications(self.basic, field_metadata)
 
     async def _apply_extracted_vectors(self, field_vectors: ExtractedVectorsWrapper):
+        if not self.has_field(
+            field_vectors.field.field_type, field_vectors.field.field
+        ):
+            # skipping because field does not exist
+            logger.warning(
+                f'Field "{field_vectors.field.field}" does not exist, skipping vectors'
+            )
+            return
+
         field_obj = await self.get_field(
             field_vectors.field.field,
             field_vectors.field.field_type,
             load=False,
         )
         (
             vo,
@@ -784,82 +1020,68 @@
                 replace_splits_sentences,
             )
             loop = asyncio.get_running_loop()
             await loop.run_in_executor(_executor, apply_field_vectors)
         else:
             raise AttributeError("VO not found on set")
 
-    async def _apply_user_vectors(self, user_vectors: UserVectorsWrapper):
-        field_obj = await self.get_field(
-            user_vectors.field.field,
-            user_vectors.field.field_type,
-            load=False,
-        )
-        uv, vectors_to_delete = await field_obj.set_user_vectors(user_vectors)
-        field_key = self.generate_field_id(user_vectors.field)
-        if uv is not None:
-            # We need to make sure that the vectors replaced are not on the new vectors
-            # So we extend the vectors to delete with the one replaced by the update
-            for vectorset, vectors in vectors_to_delete.items():
-                for vector in vectors.vectors:
-                    if vector not in user_vectors.vectors_to_delete[vectorset].vectors:
-                        user_vectors.vectors_to_delete[vectorset].vectors.append(vector)
-            self.indexer.apply_user_vectors(
-                field_key, uv, user_vectors.vectors_to_delete
-            )
-        else:
-            raise AttributeError("User Vectors not found on set")
-
     async def _apply_field_large_metadata(
         self, field_large_metadata: LargeComputedMetadataWrapper
     ):
         field_obj = await self.get_field(
             field_large_metadata.field.field,
             field_large_metadata.field.field_type,
             load=False,
         )
         await field_obj.set_large_field_metadata(field_large_metadata)
 
     def generate_field_id(self, field: FieldID) -> str:
-        return f"{KB_REVERSE_REVERSE[field.field_type]}/{field.field}"
+        return f"{FIELD_TYPE_TO_ID[field.field_type]}/{field.field}"
 
+    async def compute_security(self, brain: ResourceBrain):
+        security = await self.get_security()
+        if security is None:
+            return
+        brain.set_security(security)
+
+    @processor_observer.wrap({"type": "compute_global_tags"})
     async def compute_global_tags(self, brain: ResourceBrain):
         origin = await self.get_origin()
         basic = await self.get_basic()
         if basic is None:
             raise KeyError("Resource not found")
 
         brain.set_processing_status(basic=basic, previous_status=self._previous_status)
-        brain.set_global_tags(basic=basic, origin=origin, uuid=self.uuid)
+        brain.set_resource_metadata(basic=basic, origin=origin)
         for type, field in await self.get_fields_ids(force=True):
             fieldobj = await self.get_field(field, type, load=False)
             fieldid = FieldID(field_type=type, field=field)  # type: ignore
             fieldkey = self.generate_field_id(fieldid)
             extracted_metadata = await fieldobj.get_field_metadata()
             valid_user_field_metadata = None
             for user_field_metadata in basic.fieldmetadata:
                 if (
                     user_field_metadata.field.field == field
                     and user_field_metadata.field.field_type == type
                 ):
                     valid_user_field_metadata = user_field_metadata
                     break
-            brain.apply_field_tags_globally(
+            brain.apply_field_labels(
                 fieldkey,
                 extracted_metadata,
                 self.uuid,
                 basic.usermetadata,
                 valid_user_field_metadata,
             )
             if type == FieldType.KEYWORDSET:
                 field_data = await fieldobj.db_get_value()
                 brain.process_keywordset_fields(fieldkey, field_data)
 
+    @processor_observer.wrap({"type": "compute_global_text"})
     async def compute_global_text(self):
-        # For each extracted
         for type, field in await self.get_fields_ids(force=True):
             fieldid = FieldID(field_type=type, field=field)
             await self.compute_global_text_field(fieldid, self.indexer)
 
     async def compute_global_text_field(self, fieldid: FieldID, brain: ResourceBrain):
         fieldobj = await self.get_field(fieldid.field, fieldid.field_type, load=False)
         fieldkey = self.generate_field_id(fieldid)
@@ -867,44 +1089,35 @@
         if extracted_text is None:
             return
         field_text = extracted_text.text
         for _, split in extracted_text.split_text.items():
             field_text += f" {split} "
         brain.apply_field_text(fieldkey, field_text)
 
-    async def get_all(self):
-        if self.basic is None:
-            self.basic = await self.get_basic()
-
-        if self.origin is None:
-            self.origin = await self.get_origin()
-
-        if self.relations is None:
-            self.relations = await self.get_relations()
-
     def clean(self):
         self._indexer = None
+        self.txn = None
 
     async def iterate_sentences(
         self, enabled_metadata: EnabledMetadata
     ) -> AsyncIterator[TrainSentence]:  # pragma: no cover
         fields = await self.get_fields(force=True)
         metadata = TrainMetadata()
-        userdefinedparagraphclass: Dict[str, ParagraphAnnotation] = {}
+        userdefinedparagraphclass: dict[str, ParagraphAnnotation] = {}
         if enabled_metadata.labels:
             if self.basic is None:
                 self.basic = await self.get_basic()
             if self.basic is not None:
                 metadata.labels.resource.extend(self.basic.usermetadata.classifications)
                 for fieldmetadata in self.basic.fieldmetadata:
                     field_id = self.generate_field_id(fieldmetadata.field)
                     for annotationparagraph in fieldmetadata.paragraphs:
-                        userdefinedparagraphclass[
-                            annotationparagraph.key
-                        ] = annotationparagraph
+                        userdefinedparagraphclass[annotationparagraph.key] = (
+                            annotationparagraph
+                        )
 
         for (type_id, field_id), field in fields.items():
             fieldid = FieldID(field_type=type_id, field=field_id)  # type: ignore
             field_key = self.generate_field_id(fieldid)
             fm = await field.get_field_metadata()
             extracted_text = None
             vo = None
@@ -914,26 +1127,26 @@
                 vo = await field.get_vectors()
 
             extracted_text = await field.get_extracted_text()
 
             if fm is None:
                 continue
 
-            field_metadatas: List[Tuple[Optional[str], FieldMetadata]] = [
+            field_metadatas: list[tuple[Optional[str], FieldMetadata]] = [
                 (None, fm.metadata)
             ]
             for subfield_metadata, splitted_metadata in fm.split_metadata.items():
                 field_metadatas.append((subfield_metadata, splitted_metadata))
 
             for subfield, field_metadata in field_metadatas:
                 if enabled_metadata.labels:
                     metadata.labels.ClearField("field")
                     metadata.labels.field.extend(field_metadata.classifications)
 
-                entities: Dict[str, str] = {}
+                entities: dict[str, str] = {}
                 if enabled_metadata.entities:
                     entities.update(field_metadata.ner)
 
                 precomputed_vectors = {}
                 if vo is not None:
                     if subfield is not None:
                         vectors = vo.split_vectors[subfield]
@@ -999,51 +1212,51 @@
                         yield pb_sentence
 
     async def iterate_paragraphs(
         self, enabled_metadata: EnabledMetadata
     ) -> AsyncIterator[TrainParagraph]:
         fields = await self.get_fields(force=True)
         metadata = TrainMetadata()
-        userdefinedparagraphclass: Dict[str, ParagraphAnnotation] = {}
+        userdefinedparagraphclass: dict[str, ParagraphAnnotation] = {}
         if enabled_metadata.labels:
             if self.basic is None:
                 self.basic = await self.get_basic()
             if self.basic is not None:
                 metadata.labels.resource.extend(self.basic.usermetadata.classifications)
                 for fieldmetadata in self.basic.fieldmetadata:
                     field_id = self.generate_field_id(fieldmetadata.field)
                     for annotationparagraph in fieldmetadata.paragraphs:
-                        userdefinedparagraphclass[
-                            annotationparagraph.key
-                        ] = annotationparagraph
+                        userdefinedparagraphclass[annotationparagraph.key] = (
+                            annotationparagraph
+                        )
 
         for (type_id, field_id), field in fields.items():
             fieldid = FieldID(field_type=type_id, field=field_id)  # type: ignore
             field_key = self.generate_field_id(fieldid)
             fm = await field.get_field_metadata()
             extracted_text = None
             text = None
 
             extracted_text = await field.get_extracted_text()
 
             if fm is None:
                 continue
 
-            field_metadatas: List[Tuple[Optional[str], FieldMetadata]] = [
+            field_metadatas: list[tuple[Optional[str], FieldMetadata]] = [
                 (None, fm.metadata)
             ]
             for subfield_metadata, splitted_metadata in fm.split_metadata.items():
                 field_metadatas.append((subfield_metadata, splitted_metadata))
 
             for subfield, field_metadata in field_metadatas:
                 if enabled_metadata.labels:
                     metadata.labels.ClearField("field")
                     metadata.labels.field.extend(field_metadata.classifications)
 
-                entities: Dict[str, str] = {}
+                entities: dict[str, str] = {}
                 if enabled_metadata.entities:
                     entities.update(field_metadata.ner)
 
                 if extracted_text is not None:
                     if subfield is not None:
                         text = extracted_text.split_text[subfield]
                     else:
@@ -1101,15 +1314,15 @@
 
             if enabled_metadata.text:
                 extracted_text = await field.get_extracted_text()
 
             if fm is None:
                 continue
 
-            field_metadatas: List[Tuple[Optional[str], FieldMetadata]] = [
+            field_metadatas: list[tuple[Optional[str], FieldMetadata]] = [
                 (None, fm.metadata)
             ]
             for subfield_metadata, splitted_metadata in fm.split_metadata.items():
                 field_metadatas.append((subfield_metadata, splitted_metadata))
 
             for subfield, splitted_metadata in field_metadatas:
                 if enabled_metadata.labels:
@@ -1157,15 +1370,15 @@
                 metadata.text += extracted_text.text
                 for text in extracted_text.split_text.values():
                     metadata.text += f" {text}"
 
             if fm is None:
                 continue
 
-            field_metadatas: List[Tuple[Optional[str], FieldMetadata]] = [
+            field_metadatas: list[tuple[Optional[str], FieldMetadata]] = [
                 (None, fm.metadata)
             ]
             for subfield_metadata, splitted_metadata in fm.split_metadata.items():
                 field_metadatas.append((subfield_metadata, splitted_metadata))
 
             for _, splitted_metadata in field_metadatas:
                 if enabled_metadata.labels:
@@ -1192,15 +1405,15 @@
     if file_extracted_data is None:
         return positions
     for index, position in enumerate(file_extracted_data.file_pages_previews.positions):
         positions[index] = (position.start, position.end)
     return positions
 
 
-def remove_field_classifications(basic: PBBasic, deleted_fields: List[FieldID]):
+def remove_field_classifications(basic: PBBasic, deleted_fields: list[FieldID]):
     """
     Clean classifications of fields that have been deleted
     """
     field_classifications = [
         fc
         for fc in basic.computedmetadata.field_classifications
         if fc.field not in deleted_fields
@@ -1222,15 +1435,15 @@
     fcfs.field.CopyFrom(fcmw.field)
     fcfs.classifications.extend(fcmw.metadata.metadata.classifications)
     basic.computedmetadata.field_classifications.append(fcfs)
     return True
 
 
 def add_entities_to_metadata(
-    entities: Dict[str, str], local_text: str, metadata: TrainMetadata
+    entities: dict[str, str], local_text: str, metadata: TrainMetadata
 ) -> None:
     for entity_key, entity_value in entities.items():
         if entity_key not in local_text:
             # Add the entity only if found in text
             continue
         metadata.entities[entity_key] = entity_value
 
@@ -1269,12 +1482,42 @@
 ) -> bool:
     if basic.thumbnail or thumbnail is None:
         return False
     basic.thumbnail = CloudLink.format_reader_download_uri(thumbnail.uri)
     return True
 
 
+def update_basic_languages(basic: Basic, languages: list[str]) -> bool:
+    if len(languages) == 0:
+        return False
+
+    updated = False
+    for language in languages:
+        if not language:
+            continue
+
+        if basic.metadata.language == "":
+            basic.metadata.language = language
+            updated = True
+
+        if language not in basic.metadata.languages:
+            basic.metadata.languages.append(language)
+            updated = True
+
+    return updated
+
+
 def get_text_field_mimetype(bm: BrokerMessage) -> Optional[str]:
     if len(bm.texts) == 0:
         return None
     text_format = next(iter(bm.texts.values())).format
     return PB_TEXT_FORMAT_TO_MIMETYPE[text_format]
+
+
+def extract_field_metadata_languages(
+    field_metadata: FieldComputedMetadataWrapper,
+) -> list[str]:
+    languages: set[str] = set()
+    languages.add(field_metadata.metadata.metadata.language)
+    for _, splitted_metadata in field_metadata.metadata.split_metadata.items():
+        languages.add(splitted_metadata.language)
+    return list(languages)
```

## nucliadb/ingest/orm/synonyms.py

```diff
@@ -17,15 +17,15 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from typing import Optional
 
 from nucliadb_protos.knowledgebox_pb2 import Synonyms as PBSynonyms
 
-from nucliadb.ingest.maindb.driver import Transaction
+from nucliadb.common.maindb.driver import Transaction
 
 KB_SYNONYMS = "/kbs/{kbid}/synonyms"
 
 
 class Synonyms:
     def __init__(self, txn: Transaction, kbid: str):
         self.txn = txn
```

## nucliadb/ingest/orm/utils.py

```diff
@@ -26,33 +26,23 @@
     FieldComputedMetadataWrapper,
     FieldType,
     Metadata,
     Paragraph,
 )
 from nucliadb_protos.writer_pb2 import BrokerMessage
 
-from nucliadb.ingest.maindb.driver import Transaction
-from nucliadb.ingest.orm.local_node import LocalNode
-from nucliadb.ingest.orm.node import Node
+from nucliadb.common.maindb.driver import Transaction
 from nucliadb.ingest.processing import PushPayload
 from nucliadb.ingest.settings import settings as ingest_settings
 from nucliadb_models.text import PushTextFormat, Text
-from nucliadb_utils.settings import indexing_settings
 
 KB_RESOURCE_BASIC_FS = "/kbs/{kbid}/r/{uuid}/basic"  # Only used on FS driver
 KB_RESOURCE_BASIC = "/kbs/{kbid}/r/{uuid}"
 
 
-def get_node_klass():
-    if indexing_settings.index_local:
-        return LocalNode
-    else:
-        return Node
-
-
 async def set_basic(txn: Transaction, kbid: str, uuid: str, basic: Basic):
     if ingest_settings.driver == "local":
         await txn.set(
             KB_RESOURCE_BASIC_FS.format(kbid=kbid, uuid=uuid),
             basic.SerializeToString(),
         )
     else:
```

## nucliadb/ingest/orm/processor/__init__.py

```diff
@@ -13,79 +13,114 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import asyncio
 import logging
-from typing import Dict, List, Optional, Tuple
+from typing import Optional
 
-from nucliadb_protos.knowledgebox_pb2 import KnowledgeBox as KnowledgeBoxPB
-from nucliadb_protos.knowledgebox_pb2 import (
-    KnowledgeBoxConfig,
-    KnowledgeBoxID,
-    KnowledgeBoxResponseStatus,
-)
-from nucliadb_protos.resources_pb2 import Metadata as PBMetadata
-from nucliadb_protos.utils_pb2 import VectorSimilarity
-from nucliadb_protos.writer_pb2 import BrokerMessage, Notification
+import aiohttp.client_exceptions
 
-from nucliadb.ingest.maindb.driver import Driver, Transaction
+from nucliadb.common import datamanagers, locking
+from nucliadb.common.cluster.settings import settings as cluster_settings
+from nucliadb.common.cluster.utils import get_shard_manager
+from nucliadb.common.maindb.driver import Driver, Transaction
+from nucliadb.common.maindb.exceptions import ConflictError
 from nucliadb.ingest.orm.exceptions import (
     DeadletteredError,
     KnowledgeBoxConflict,
-    KnowledgeBoxNotFound,
+    ResourceNotIndexable,
     SequenceOrderViolation,
 )
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
+from nucliadb.ingest.orm.metrics import processor_observer
 from nucliadb.ingest.orm.processor import sequence_manager
 from nucliadb.ingest.orm.resource import Resource
-from nucliadb.ingest.orm.shard import Shard
-from nucliadb.ingest.orm.utils import get_node_klass, set_basic
+from nucliadb_protos import (
+    knowledgebox_pb2,
+    noderesources_pb2,
+    nodewriter_pb2,
+    resources_pb2,
+    utils_pb2,
+    writer_pb2,
+)
 from nucliadb_telemetry import errors
 from nucliadb_utils import const
-from nucliadb_utils.cache.utility import Cache
+from nucliadb_utils.cache.pubsub import PubSubDriver
 from nucliadb_utils.storages.storage import Storage
 from nucliadb_utils.utilities import get_storage
 
 logger = logging.getLogger(__name__)
 
 
+MESSAGE_TO_NOTIFICATION_SOURCE = {
+    writer_pb2.BrokerMessage.MessageSource.WRITER: writer_pb2.NotificationSource.WRITER,
+    writer_pb2.BrokerMessage.MessageSource.PROCESSOR: writer_pb2.NotificationSource.PROCESSOR,
+}
+
+
+def validate_indexable_resource(resource: noderesources_pb2.Resource) -> None:
+    """
+    It would be more optimal to move this to another layer but it'd also make the code
+    more difficult to grok and test because we'd need to move processable check and throw
+    an exception in the middle of a bunch of processing logic.
+
+    As it is implemented right now, we just do the check if a resource is indexable right
+    before we actually try to index it and not buried it somewhere else in the code base.
+
+    This is still an edge case.
+    """
+    num_paragraphs = 0
+    for _, fparagraph in resource.paragraphs.items():
+        # this count should not be very expensive to do since we don't have
+        # a lot of different fields and we just do a count on a dict
+        num_paragraphs += len(fparagraph.paragraphs)
+
+    if num_paragraphs > cluster_settings.max_resource_paragraphs:
+        raise ResourceNotIndexable(
+            "Resource has too many paragraphs. "
+            f"Supported: {cluster_settings.max_resource_paragraphs} , Number: {num_paragraphs}"
+        )
+
+
 class Processor:
     """
     This class is responsible for processing messages from the broker
     and attempts to manage sequencing correctly with a txn id implementation.
 
     The "txn" in this implementation is oriented around the sequence id of
     messages coming through the message broker.
 
     Not all writes are going to have a transaction id. For example, writes
     coming from processor can be coming through a different channel
     and can not use the txn id
     """
 
-    messages: Dict[str, List[BrokerMessage]]
+    messages: dict[str, list[writer_pb2.BrokerMessage]]
 
     def __init__(
         self,
         driver: Driver,
         storage: Storage,
-        cache: Optional[Cache] = None,
+        pubsub: Optional[PubSubDriver] = None,
         partition: Optional[str] = None,
     ):
         self.messages = {}
         self.driver = driver
         self.storage = storage
         self.partition = partition
-        self.cache = cache
+        self.pubsub = pubsub
+        self.shard_manager = get_shard_manager()
 
     async def process(
         self,
-        message: BrokerMessage,
+        message: writer_pb2.BrokerMessage,
         seqid: int,
         partition: Optional[str] = None,
         transaction_check: bool = True,
     ) -> None:
         partition = partition if self.partition is None else self.partition
         if partition is None:
             raise AttributeError("Can't process message from unknown partition")
@@ -94,283 +129,353 @@
         # that the current message doesn't violate the sequence order for the
         # current partition
         if transaction_check:
             last_seqid = await sequence_manager.get_last_seqid(self.driver, partition)
             if last_seqid is not None and seqid <= last_seqid:
                 raise SequenceOrderViolation(last_seqid)
 
-        if message.type == BrokerMessage.MessageType.DELETE:
+        if message.type == writer_pb2.BrokerMessage.MessageType.DELETE:
             await self.delete_resource(message, seqid, partition, transaction_check)
-        elif message.type == BrokerMessage.MessageType.AUTOCOMMIT:
+        elif message.type == writer_pb2.BrokerMessage.MessageType.AUTOCOMMIT:
             await self.txn([message], seqid, partition, transaction_check)
-        elif message.type == BrokerMessage.MessageType.MULTI:
+        elif message.type == writer_pb2.BrokerMessage.MessageType.MULTI:
             # XXX Not supported right now
             # MULTI, COMMIT and ROLLBACK are all not supported in transactional mode right now
             # This concept is probably not tenable with current architecture because
             # of how nats works and how we would need to manage rollbacks.
             # XXX Should this be removed?
             await self.multi(message, seqid)
-        elif message.type == BrokerMessage.MessageType.COMMIT:
+        elif message.type == writer_pb2.BrokerMessage.MessageType.COMMIT:
             await self.commit(message, seqid, partition)
-        elif message.type == BrokerMessage.MessageType.ROLLBACK:
+        elif message.type == writer_pb2.BrokerMessage.MessageType.ROLLBACK:
             await self.rollback(message, seqid, partition)
 
-    async def get_resource_uuid(self, kb: KnowledgeBox, message: BrokerMessage) -> str:
+    async def get_resource_uuid(
+        self, kb: KnowledgeBox, message: writer_pb2.BrokerMessage
+    ) -> str:
         if message.uuid is None:
             uuid = await kb.get_resource_uuid_by_slug(message.slug)
         else:
             uuid = message.uuid
         return uuid
 
+    @processor_observer.wrap({"type": "delete_resource"})
     async def delete_resource(
         self,
-        message: BrokerMessage,
+        message: writer_pb2.BrokerMessage,
         seqid: int,
         partition: str,
         transaction_check: bool = True,
     ) -> None:
         txn = await self.driver.begin()
-        kb = KnowledgeBox(txn, self.storage, message.kbid)
+        try:
+            kb = KnowledgeBox(txn, self.storage, message.kbid)
 
-        uuid = await self.get_resource_uuid(kb, message)
-        shard_id = await kb.get_resource_shard_id(uuid)
-        if shard_id is None:
-            logger.warning(f"Resource {uuid} does not exist")
-        else:
-            node_klass = get_node_klass()
-            shard: Optional[Shard] = await kb.get_resource_shard(shard_id, node_klass)
-            if shard is None:
-                raise AttributeError("Shard not available")
-            await shard.delete_resource(message.uuid, seqid, partition, message.kbid)
-            try:
-                await kb.delete_resource(message.uuid)
-            except Exception as exc:
-                await txn.abort()
-                await self.notify_abort(
-                    partition=partition,
-                    seqid=seqid,
-                    multi=message.multiid,
-                    kbid=message.kbid,
-                    rid=message.uuid,
+            uuid = await self.get_resource_uuid(kb, message)
+            async with locking.distributed_lock(
+                locking.RESOURCE_INDEX_LOCK.format(kbid=message.kbid, resource_id=uuid)
+            ):
+                # we need to have a lock at indexing time because we don't know if
+                # a resource was in the process of being moved when a delete occurred
+                shard_id = await datamanagers.resources.get_resource_shard_id(
+                    txn, kbid=message.kbid, rid=uuid
                 )
-                raise exc
-        if txn.open:
-            if transaction_check:
-                await sequence_manager.set_last_seqid(txn, partition, seqid)
-            await txn.commit()
+            if shard_id is None:
+                logger.warning(f"Resource {uuid} does not exist")
+            else:
+                shard = await kb.get_resource_shard(shard_id)
+                if shard is None:
+                    raise AttributeError("Shard not available")
+
+                await self.shard_manager.delete_resource(
+                    shard, message.uuid, seqid, partition, message.kbid
+                )
+                try:
+                    await kb.delete_resource(message.uuid)
+                except Exception as exc:
+                    await txn.abort()
+                    await self.notify_abort(
+                        partition=partition,
+                        seqid=seqid,
+                        multi=message.multiid,
+                        kbid=message.kbid,
+                        rid=message.uuid,
+                        source=message.source,
+                    )
+                    raise exc
+        finally:
+            if txn.open:
+                if transaction_check:
+                    await sequence_manager.set_last_seqid(txn, partition, seqid)
+                await txn.commit()
         await self.notify_commit(
             partition=partition,
             seqid=seqid,
             multi=message.multiid,
             message=message,
-            write_type=Notification.WriteType.DELETED,
+            write_type=writer_pb2.Notification.WriteType.DELETED,
         )
 
+    @processor_observer.wrap({"type": "commit_slug"})
     async def commit_slug(self, resource: Resource) -> None:
         # Slug may have conflicts as its not partitioned properly,
         # so we commit it in a different transaction to make it as short as possible
         prev_txn = resource.txn
-        async with self.driver.transaction() as txn:
-            resource.txn = txn
-            await resource.set_slug()
-            await txn.commit()
-        resource.txn = prev_txn
+        try:
+            async with self.driver.transaction() as txn:
+                resource.txn = txn
+                await resource.set_slug()
+                await txn.commit()
+        finally:
+            resource.txn = prev_txn
 
+    @processor_observer.wrap({"type": "txn"})
     async def txn(
         self,
-        messages: List[BrokerMessage],
+        messages: list[writer_pb2.BrokerMessage],
         seqid: int,
         partition: str,
         transaction_check: bool = True,
     ) -> None:
         if len(messages) == 0:
             return None
 
         txn = await self.driver.begin()
         kbid = messages[0].kbid
-        if not await KnowledgeBox.exist_kb(txn, kbid):
+        if not await datamanagers.kb.exists_kb(txn, kbid=kbid):
             logger.warning(f"KB {kbid} is deleted: skiping txn")
             if transaction_check:
                 await sequence_manager.set_last_seqid(txn, partition, seqid)
             await txn.commit()
             return None
 
-        multi = messages[0].multiid
-        kb = KnowledgeBox(txn, self.storage, kbid)
-        uuid = await self.get_resource_uuid(kb, messages[0])
-        resource: Optional[Resource] = None
-        handled_exception = None
-        created = False
-        shard: Optional[Shard] = None
-
         try:
+            multi = messages[0].multiid
+            kb = KnowledgeBox(txn, self.storage, kbid)
+            uuid = await self.get_resource_uuid(kb, messages[0])
+            resource: Optional[Resource] = None
+            handled_exception = None
+            created = False
+
             for message in messages:
                 if resource is not None:
                     assert resource.uuid == message.uuid
                 result = await self.apply_resource(message, kb, resource)
 
                 if result is None:
                     continue
 
                 resource, _created = result
                 created = created or _created
 
             if resource:
                 await resource.compute_global_text()
                 await resource.compute_global_tags(resource.indexer)
+                await resource.compute_security(resource.indexer)
                 if message.reindex:
                     # when reindexing, let's just generate full new index message
                     resource.replace_indexer(await resource.generate_index_message())
 
             if resource and resource.modified:
-                shard_id = await kb.get_resource_shard_id(uuid)
-                node_klass = get_node_klass()
-
-                if shard_id is not None:
-                    shard = await kb.get_resource_shard(shard_id, node_klass)
-
-                if shard is None:
-                    # It's a new resource, get current active shard to place
-                    # new resource on
-                    shard = await node_klass.get_current_active_shard(txn, kbid)
-                    if shard is None:
-                        # no shard available, create a new one
-                        similarity = await kb.get_similarity()
-                        shard = await node_klass.create_shard_by_kbid(
-                            txn, kbid, similarity=similarity
-                        )
-                    await kb.set_resource_shard_id(uuid, shard.sharduuid)
-
-                if shard is not None:
-                    await shard.add_resource(
-                        resource.indexer.brain, seqid, partition=partition, kb=kbid
-                    )
-                else:
-                    raise AttributeError("Shard is not available")
+                await self.index_resource(  # noqa
+                    resource=resource,
+                    txn=txn,
+                    uuid=uuid,
+                    kbid=kbid,
+                    seqid=seqid,
+                    partition=partition,
+                    kb=kb,
+                    source=messages_source(messages),
+                )
 
                 if transaction_check:
                     await sequence_manager.set_last_seqid(txn, partition, seqid)
                 await txn.commit()
 
-                if created or resource.slug_modified:
+                if created:
                     await self.commit_slug(resource)
 
                 await self.notify_commit(
                     partition=partition,
                     seqid=seqid,
                     multi=multi,
                     message=message,
-                    write_type=Notification.WriteType.CREATED
-                    if created
-                    else Notification.WriteType.MODIFIED,
+                    write_type=(
+                        writer_pb2.Notification.WriteType.CREATED
+                        if created
+                        else writer_pb2.Notification.WriteType.MODIFIED
+                    ),
                 )
             elif resource and resource.modified is False:
                 await txn.abort()
                 await self.notify_abort(
-                    partition=partition, seqid=seqid, multi=multi, kbid=kbid, rid=uuid
+                    partition=partition,
+                    seqid=seqid,
+                    multi=multi,
+                    kbid=kbid,
+                    rid=uuid,
+                    source=message.source,
                 )
-                logger.warning(f"This message did not modify the resource")
+                logger.warning("This message did not modify the resource")
+        except (
+            asyncio.TimeoutError,
+            asyncio.CancelledError,
+            aiohttp.client_exceptions.ClientError,
+            ConflictError,
+        ):  # pragma: no cover
+            # Unhandled exceptions here that should bubble and hard fail
+            # XXX We swallow too many exceptions here!
+            await self.notify_abort(
+                partition=partition,
+                seqid=seqid,
+                multi=multi,
+                kbid=kbid,
+                rid=uuid,
+                source=message.source,
+            )
+            raise
         except Exception as exc:
             # As we are in the middle of a transaction, we cannot let the exception raise directly
             # as we need to do some cleanup. The exception will be reraised at the end of the function
             # and then handled by the top caller, so errors can be handled in the same place.
             await self.deadletter(messages, partition, seqid)
             await self.notify_abort(
-                partition=partition, seqid=seqid, multi=multi, kbid=kbid, rid=uuid
+                partition=partition,
+                seqid=seqid,
+                multi=multi,
+                kbid=kbid,
+                rid=uuid,
+                source=message.source,
             )
             handled_exception = exc
         finally:
             if resource is not None:
                 resource.clean()
             # txn should be already commited or aborted, but in the event of an exception
             # it could be left open. Make sure to close it if it's still open
             if txn.open:
                 await txn.abort()
 
         if handled_exception is not None:
             if seqid == -1:
                 raise handled_exception
             else:
-                await self._mark_resource_error(resource, partition, seqid, shard, kbid)
+                if resource is not None:
+                    await self._mark_resource_error(kb, resource, partition, seqid)
                 raise DeadletteredError() from handled_exception
 
         return None
 
-    async def _mark_resource_error(
+    @processor_observer.wrap({"type": "index_resource"})
+    async def index_resource(
         self,
-        resource: Optional[Resource],
-        partition: str,
-        seqid: int,
-        shard: Optional[Shard],
+        resource: Resource,
+        txn: Transaction,
+        uuid: str,
         kbid: str,
+        seqid: int,
+        partition: str,
+        kb: KnowledgeBox,
+        source: nodewriter_pb2.IndexMessageSource.ValueType,
     ) -> None:
-        """
-        Unhandled error processing, try to mark resource as error
-        """
-        if shard is None:
-            logger.warning(
-                "Unable to mark resource as error, shard is None. "
-                "This should not happen so you did something special to get here."
+        validate_indexable_resource(resource.indexer.brain)
+
+        async with locking.distributed_lock(
+            locking.RESOURCE_INDEX_LOCK.format(kbid=kbid, resource_id=uuid)
+        ):
+            # we need to have a lock at indexing time because we don't know if
+            # a resource was move to another shard while it was being indexed
+            shard_id = await datamanagers.resources.get_resource_shard_id(
+                txn, kbid=kbid, rid=uuid
             )
-            return
-        if resource is None or resource.basic is None:
-            logger.info(
-                f"Skip when resource does not even have basic metadata: {resource}"
+
+        shard = None
+        if shard_id is not None:
+            shard = await kb.get_resource_shard(shard_id)
+
+        if shard is None:
+            # It's a new resource, get current active shard to place
+            # new resource on
+            shard = await self.shard_manager.get_current_active_shard(txn, kbid)
+            if shard is None:
+                # no shard available, create a new one
+                model = await datamanagers.kb.get_model_metadata(txn, kbid=kbid)
+                config = await kb.get_config()
+                if config is not None:
+                    release_channel = config.release_channel
+                else:
+                    release_channel = utils_pb2.ReleaseChannel.STABLE
+
+                shard = await self.shard_manager.create_shard_by_kbid(
+                    txn,
+                    kbid,
+                    semantic_model=model,
+                    release_channel=release_channel,
+                )
+            await datamanagers.resources.set_resource_shard_id(
+                txn, kbid=kbid, rid=uuid, shard=shard.shard
             )
-            return
-        txn = None
-        try:
-            async with self.driver.transaction() as txn:
-                resource.basic.metadata.status = PBMetadata.Status.ERROR
-                await set_basic(txn, resource.kb.kbid, resource.uuid, resource.basic)
-                await txn.commit()
 
-            await shard.add_resource(
-                resource.indexer.brain, seqid, partition=partition, kb=kbid
+        if shard is not None:
+            index_message = resource.indexer.brain
+            await self.shard_manager.add_resource(
+                shard,
+                index_message,
+                seqid,
+                partition=partition,
+                kb=kbid,
+                source=source,
             )
-        except Exception:
-            logger.warning("Error while marking resource as error", exc_info=True)
+        else:
+            raise AttributeError("Shard is not available")
 
-    async def multi(self, message: BrokerMessage, seqid: int) -> None:
+    async def multi(self, message: writer_pb2.BrokerMessage, seqid: int) -> None:
         self.messages.setdefault(message.multiid, []).append(message)
 
-    async def commit(self, message: BrokerMessage, seqid: int, partition: str) -> None:
+    async def commit(
+        self, message: writer_pb2.BrokerMessage, seqid: int, partition: str
+    ) -> None:
         if message.multiid not in self.messages:
             # Error
             logger.error(f"Closed multi {message.multiid}")
             await self.deadletter([message], partition, seqid)
         else:
             await self.txn(self.messages[message.multiid], seqid, partition)
 
     async def rollback(
-        self, message: BrokerMessage, seqid: int, partition: str
+        self, message: writer_pb2.BrokerMessage, seqid: int, partition: str
     ) -> None:
         # Error
         logger.error(f"Closed multi {message.multiid}")
         del self.messages[message.multiid]
         await self.notify_abort(
             partition=partition,
             seqid=seqid,
             multi=message.multiid,
             kbid=message.kbid,
             rid=message.uuid,
+            source=message.source,
         )
 
     async def deadletter(
-        self, messages: List[BrokerMessage], partition: str, seqid: int
+        self, messages: list[writer_pb2.BrokerMessage], partition: str, seqid: int
     ) -> None:
         for seq, message in enumerate(messages):
             await self.storage.deadletter(message, seq, seqid, partition)
 
+    @processor_observer.wrap({"type": "apply_resource"})
     async def apply_resource(
         self,
-        message: BrokerMessage,
+        message: writer_pb2.BrokerMessage,
         kb: KnowledgeBox,
         resource: Optional[Resource] = None,
-    ) -> Optional[Tuple[Resource, bool]]:
+    ) -> Optional[tuple[Resource, bool]]:
+        """
+        Convert a broker message into a resource object, and apply it to the database
+        """
         created = False
 
         if resource is None:
             # Make sure we load the resource in case it already exists on db
             if message.uuid is None and message.slug:
                 uuid = await kb.get_resource_uuid_by_slug(message.slug)
             else:
@@ -380,181 +485,232 @@
         if resource is None and message.source is message.MessageSource.WRITER:
             # It's a new resource
             resource = await kb.add_resource(uuid, message.slug, message.basic)
             created = True
         elif resource is not None:
             # It's an update of an existing resource, can come either from writer or
             # from processing
-            if (
-                message.HasField("basic")
-                or message.slug != ""
-                or len(message.delete_fields) > 0
-            ):
-                await resource.set_basic(
-                    message.basic,
-                    slug=message.slug,
-                    deleted_fields=message.delete_fields,  # type: ignore
-                )
+            await self.maybe_update_resource_basic(resource, message)
         elif resource is None and message.source is message.MessageSource.PROCESSOR:
             # It's a new resource, and somehow we received the message coming from processing before
             # the "fast" one, this shouldn't happen
             logger.info(
                 f"Secondary message for resource {message.uuid} and resource does not exist, ignoring"
             )
             return None
 
-        if message.HasField("origin") and resource:
+        if resource is None:
+            return None
+
+        if message.HasField("origin"):
             await resource.set_origin(message.origin)
 
-        if resource:
-            await resource.apply_fields(message)
-            await resource.apply_extracted(message)
-            return (resource, created)
+        if message.HasField("extra"):
+            await resource.set_extra(message.extra)
 
-        return None
+        if message.HasField("security"):
+            await resource.set_security(message.security)
+
+        await resource.apply_fields(message)
+        await resource.apply_extracted(message)
+        return (resource, created)
+
+    async def maybe_update_resource_basic(
+        self, resource: Resource, message: writer_pb2.BrokerMessage
+    ) -> None:
+        basic_field_updates = message.HasField("basic")
+        deleted_fields = len(message.delete_fields) > 0
+        if not (basic_field_updates or deleted_fields):
+            return
+
+        await resource.set_basic(
+            message.basic,
+            deleted_fields=message.delete_fields,  # type: ignore
+        )
 
     async def notify_commit(
         self,
         *,
         partition: str,
         seqid: int,
         multi: str,
-        message: BrokerMessage,
-        write_type: Notification.WriteType.Value,  # type: ignore
+        message: writer_pb2.BrokerMessage,
+        write_type: writer_pb2.Notification.WriteType.ValueType,
     ):
-        notification = Notification(
+        notification = writer_pb2.Notification(
             partition=int(partition),
             seqid=seqid,
             multi=multi,
             uuid=message.uuid,
             kbid=message.kbid,
-            action=Notification.Action.COMMIT,
+            action=writer_pb2.Notification.Action.COMMIT,
+            write_type=write_type,
+            source=MESSAGE_TO_NOTIFICATION_SOURCE[message.source],
             # including the message here again might feel a bit unusual but allows
             # us to react to these notifications with the original payload
-            write_type=write_type,
             message=message,
+            processing_errors=len(message.errors) > 0,
         )
 
         await self.notify(
             const.PubSubChannels.RESOURCE_NOTIFY.format(kbid=message.kbid),
             notification.SerializeToString(),
         )
 
     async def notify_abort(
-        self, *, partition: str, seqid: int, multi: str, kbid: str, rid: str
+        self,
+        *,
+        partition: str,
+        seqid: int,
+        multi: str,
+        kbid: str,
+        rid: str,
+        source: writer_pb2.BrokerMessage.MessageSource.ValueType,
     ):
-        message = Notification(
+        message = writer_pb2.Notification(
             partition=int(partition),
             seqid=seqid,
             multi=multi,
             uuid=rid,
             kbid=kbid,
-            action=Notification.ABORT,
+            action=writer_pb2.Notification.ABORT,
+            source=MESSAGE_TO_NOTIFICATION_SOURCE[source],
         )
         await self.notify(
             const.PubSubChannels.RESOURCE_NOTIFY.format(kbid=kbid),
             message.SerializeToString(),
         )
 
     async def notify(self, channel, payload: bytes):
-        if self.cache is not None and self.cache.pubsub is not None:
-            await self.cache.pubsub.publish(channel, payload)
+        if self.pubsub is not None:
+            await self.pubsub.publish(channel, payload)
+
+    async def _mark_resource_error(
+        self, kb: KnowledgeBox, resource: Optional[Resource], partition: str, seqid: int
+    ) -> None:
+        """
+        Unhandled error processing, try to mark resource as error
+        """
+        if resource is None or resource.basic is None:
+            logger.info(
+                f"Skip when resource does not even have basic metadata: {resource}"
+            )
+            return
+        try:
+            async with self.driver.transaction() as txn:
+                kb.txn = resource.txn = txn
+
+                shard_id = await datamanagers.resources.get_resource_shard_id(
+                    txn, kbid=kb.kbid, rid=resource.uuid
+                )
+                shard = None
+                if shard_id is not None:
+                    shard = await kb.get_resource_shard(shard_id)
+                if shard is None:
+                    logger.warning(
+                        "Unable to mark resource as error, shard is None. "
+                        "This should not happen so you did something special to get here."
+                    )
+                    return
+
+                resource.basic.metadata.status = resources_pb2.Metadata.Status.ERROR
+                await resource.set_basic(resource.basic)
+                await txn.commit()
+
+            resource.indexer.set_processing_status(
+                basic=resource.basic, previous_status=resource._previous_status
+            )
+            await self.shard_manager.add_resource(
+                shard, resource.indexer.brain, seqid, partition=partition, kb=kb.kbid
+            )
+        except Exception:
+            logger.warning("Error while marking resource as error", exc_info=True)
 
     # KB tools
     # XXX: Why are these utility functions here?
     async def get_kb_obj(
-        self, txn: Transaction, kbid: KnowledgeBoxID
+        self, txn: Transaction, kbid: knowledgebox_pb2.KnowledgeBoxID
     ) -> Optional[KnowledgeBox]:
         uuid: Optional[str] = kbid.uuid
         if uuid == "":
-            uuid = await KnowledgeBox.get_kb_uuid(txn, kbid.slug)
+            uuid = await datamanagers.kb.get_kb_uuid(txn, slug=kbid.slug)
 
         if uuid is None:
             return None
 
-        if not (await KnowledgeBox.exist_kb(txn, uuid)):
+        if not (await datamanagers.kb.exists_kb(txn, kbid=uuid)):
             return None
 
         storage = await get_storage()
         kbobj = KnowledgeBox(txn, storage, uuid)
         return kbobj
 
-    async def get_kb(self, slug: str = "", uuid: Optional[str] = "") -> KnowledgeBoxPB:
-        txn = await self.driver.begin()
-
-        if uuid == "" and slug != "":
-            uuid = await KnowledgeBox.get_kb_uuid(txn, slug)
-
-        response = KnowledgeBoxPB()
-        if uuid is None:
-            response.status = KnowledgeBoxResponseStatus.NOTFOUND
-            await txn.abort()
-            return response
-
-        config = await KnowledgeBox.get_kb(txn, uuid)
-
-        await txn.abort()
-
-        if config is None:
-            response.status = KnowledgeBoxResponseStatus.NOTFOUND
-            return response
-
-        response.uuid = uuid
-        response.slug = config.slug
-        response.config.CopyFrom(config)
-        return response
-
-    async def get_kb_uuid(self, slug: str) -> Optional[str]:
-        txn = await self.driver.begin()
-        uuid = await KnowledgeBox.get_kb_uuid(txn, slug)
-        await txn.abort()
-        return uuid
-
+    @processor_observer.wrap({"type": "create_kb"})
     async def create_kb(
         self,
         slug: str,
-        config: Optional[KnowledgeBoxConfig],
+        config: Optional[knowledgebox_pb2.KnowledgeBoxConfig],
+        semantic_model: knowledgebox_pb2.SemanticModelMetadata,
         forceuuid: Optional[str] = None,
-        similarity: VectorSimilarity.ValueType = VectorSimilarity.COSINE,
+        release_channel: utils_pb2.ReleaseChannel.ValueType = utils_pb2.ReleaseChannel.STABLE,
     ) -> str:
         async with self.driver.transaction() as txn:
             try:
                 uuid, failed = await KnowledgeBox.create(
-                    txn, slug, config=config, uuid=forceuuid, similarity=similarity
+                    txn,
+                    slug,
+                    semantic_model,
+                    uuid=forceuuid,
+                    config=config,
+                    release_channel=release_channel,
                 )
                 if failed:
                     raise Exception("Failed to create KB")
                 await txn.commit()
                 return uuid
             except KnowledgeBoxConflict:
                 raise
             except Exception as e:
                 errors.capture_exception(e)
                 raise e
 
     async def update_kb(
-        self, kbid: str, slug: str, config: Optional[KnowledgeBoxConfig]
+        self,
+        kbid: str,
+        slug: str,
+        config: Optional[knowledgebox_pb2.KnowledgeBoxConfig],
     ) -> str:
-        txn = await self.driver.begin()
-        try:
+        async with self.driver.transaction() as txn:
             uuid = await KnowledgeBox.update(txn, kbid, slug, config=config)
-        except Exception as e:
-            await txn.abort()
-            raise e
-        await txn.commit()
+            await txn.commit()
         return uuid
 
-    async def list_kb(self, prefix: str):
-        txn = await self.driver.begin()
-        async for kbid, slug in KnowledgeBox.get_kbs(txn, prefix):
-            yield slug
-        await txn.abort()
-
     async def delete_kb(self, kbid: str = "", slug: str = "") -> str:
-        txn = await self.driver.begin()
-        try:
+        async with self.driver.transaction() as txn:
             uuid = await KnowledgeBox.delete_kb(txn, kbid=kbid, slug=slug)
-        except (AttributeError, KeyError, KnowledgeBoxNotFound) as exc:
-            await txn.abort()
-            raise exc
-        await txn.commit()
+            await txn.commit()
         return uuid
+
+
+def messages_source(messages: list[writer_pb2.BrokerMessage]):
+    from_writer = all(
+        (
+            message.source == writer_pb2.BrokerMessage.MessageSource.WRITER
+            for message in messages
+        )
+    )
+    from_processor = all(
+        (
+            message.source == writer_pb2.BrokerMessage.MessageSource.PROCESSOR
+            for message in messages
+        )
+    )
+    if from_writer:
+        source = nodewriter_pb2.IndexMessageSource.WRITER
+    elif from_processor:
+        source = nodewriter_pb2.IndexMessageSource.PROCESSOR
+    else:  # pragma: nocover
+        msg = "Processor received multiple broker messages with different sources in the same txn!"
+        logger.error(msg)
+        errors.capture_exception(Exception(msg))
+        source = nodewriter_pb2.IndexMessageSource.PROCESSOR
+    return source
```

## nucliadb/ingest/orm/processor/sequence_manager.py

```diff
@@ -15,30 +15,30 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from typing import Optional
 
-from nucliadb.ingest.maindb.driver import Driver, Transaction
+from nucliadb.common.maindb.driver import Driver, Transaction
 
 TXNID = "/internal/worker/{worker}"
 
 
 async def get_last_seqid(driver: Driver, worker: str) -> Optional[int]:
     """
     Get last stored sequence id for a worker.
 
     This is oriented towards the ingest consumer and processor,
     which is the only one that should be writing to this key.
     """
     async with driver.transaction() as txn:
         key = TXNID.format(worker=worker)
         last_seq = await txn.get(key)
-        if last_seq is None:
+        if not last_seq:
             return None
         else:
             return int(last_seq)
 
 
 async def set_last_seqid(txn: Transaction, worker: str, seqid: int) -> None:
     """
```

## nucliadb/ingest/service/__init__.py

```diff
@@ -15,27 +15,27 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from typing import Optional
 
-from grpc import aio  # type: ignore
+from grpc import aio
 
 from nucliadb import health
 from nucliadb.ingest import logger
 from nucliadb.ingest.service.writer import WriterServicer
 from nucliadb.ingest.settings import settings
 from nucliadb_protos import writer_pb2_grpc
 from nucliadb_telemetry.utils import setup_telemetry
 from nucliadb_utils.grpc import get_traced_grpc_server
 
 
 async def start_grpc(service_name: Optional[str] = None):
-    aio.init_grpc_aio()
+    aio.init_grpc_aio()  # type: ignore
 
     await setup_telemetry(service_name or "ingest")
     server = get_traced_grpc_server(
         service_name or "ingest",
         max_receive_message=settings.max_receive_message_length,
     )
```

## nucliadb/ingest/service/writer.py

```diff
@@ -13,41 +13,38 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import json
 import uuid
 from io import BytesIO
 from typing import AsyncIterator, Optional
 
 from nucliadb_protos.knowledgebox_pb2 import (
-    CleanedKnowledgeBoxResponse,
     DeleteKnowledgeBoxResponse,
     GCKnowledgeBoxResponse,
-    KnowledgeBox,
     KnowledgeBoxID,
     KnowledgeBoxNew,
-    KnowledgeBoxPrefix,
     KnowledgeBoxResponseStatus,
     KnowledgeBoxUpdate,
     Labels,
     NewKnowledgeBoxResponse,
+    SemanticModelMetadata,
     UpdateKnowledgeBoxResponse,
 )
-from nucliadb_protos.noderesources_pb2 import ShardCleaned
 from nucliadb_protos.resources_pb2 import CloudFile
 from nucliadb_protos.writer_pb2 import (
     BinaryData,
     BrokerMessage,
     DelEntitiesRequest,
     DelLabelsRequest,
     DelVectorSetRequest,
-    ExportRequest,
     ExtractedVectorsWrapper,
     FileRequest,
     FileUploaded,
     GetEntitiesGroupRequest,
     GetEntitiesGroupResponse,
     GetEntitiesRequest,
     GetEntitiesResponse,
@@ -73,155 +70,185 @@
     ResourceIdResponse,
     SetEntitiesRequest,
     SetLabelsRequest,
     SetSynonymsRequest,
     SetVectorSetRequest,
     SetVectorsRequest,
     SetVectorsResponse,
-)
-from nucliadb_protos.writer_pb2 import Shards as PBShards
-from nucliadb_protos.writer_pb2 import (
     UpdateEntitiesGroupRequest,
     UpdateEntitiesGroupResponse,
     UploadBinaryData,
     WriterStatusRequest,
     WriterStatusResponse,
 )
 
+from nucliadb import learning_proxy
+from nucliadb.common import datamanagers
+from nucliadb.common.cluster.exceptions import AlreadyExists, EntitiesGroupNotFound
+from nucliadb.common.cluster.manager import get_index_nodes
+from nucliadb.common.cluster.utils import get_shard_manager
+from nucliadb.common.datamanagers.exceptions import KnowledgeBoxNotFound
+from nucliadb.common.maindb.driver import Transaction
+from nucliadb.common.maindb.utils import setup_driver
 from nucliadb.ingest import SERVICE_NAME, logger
-from nucliadb.ingest.maindb.driver import Transaction
 from nucliadb.ingest.orm.entities import EntitiesManager
-from nucliadb.ingest.orm.exceptions import (
-    AlreadyExists,
-    EntitiesGroupNotFound,
-    KnowledgeBoxConflict,
-    KnowledgeBoxNotFound,
-)
+from nucliadb.ingest.orm.exceptions import KnowledgeBoxConflict
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox as KnowledgeBoxORM
-from nucliadb.ingest.orm.knowledgebox import KnowledgeBox as KnowledgeBoxObj
-from nucliadb.ingest.orm.node import Node
 from nucliadb.ingest.orm.processor import Processor, sequence_manager
 from nucliadb.ingest.orm.resource import Resource as ResourceORM
-from nucliadb.ingest.orm.shard import Shard
-from nucliadb.ingest.orm.utils import get_node_klass
 from nucliadb.ingest.settings import settings
-from nucliadb.ingest.utils import get_driver
-from nucliadb_protos import writer_pb2_grpc
+from nucliadb_protos import utils_pb2, writer_pb2, writer_pb2_grpc
 from nucliadb_telemetry import errors
-from nucliadb_utils.cache import KB_COUNTER_CACHE
-from nucliadb_utils.keys import KB_SHARDS
+from nucliadb_utils import const
+from nucliadb_utils.settings import is_onprem_nucliadb, running_settings
 from nucliadb_utils.storages.storage import Storage, StorageField
 from nucliadb_utils.utilities import (
-    get_cache,
     get_partitioning,
+    get_pubsub,
     get_storage,
     get_transaction_utility,
+    has_feature,
 )
 
 
 class WriterServicer(writer_pb2_grpc.WriterServicer):
     def __init__(self):
         self.partitions = settings.partitions
 
     async def initialize(self):
-        storage = await get_storage(service_name=SERVICE_NAME)
-        driver = await get_driver()
-        cache = await get_cache()
-        self.proc = Processor(driver=driver, storage=storage, cache=cache)
-
-    async def finalize(self):
-        ...
-
-    async def GetKnowledgeBox(self, request: KnowledgeBoxID, context=None) -> KnowledgeBox:  # type: ignore
-        response: KnowledgeBox = await self.proc.get_kb(
-            slug=request.slug, uuid=request.uuid
+        self.storage = await get_storage(service_name=SERVICE_NAME)
+        self.driver = await setup_driver()
+        self.proc = Processor(
+            driver=self.driver, storage=self.storage, pubsub=await get_pubsub()
         )
-        return response
+        self.shards_manager = get_shard_manager()
 
-    async def CleanAndUpgradeKnowledgeBoxIndex(  # type: ignore
-        self, request: KnowledgeBoxID, context=None
-    ) -> CleanedKnowledgeBoxResponse:
-        try:
-            txn = await self.proc.driver.begin()
-            node_klass = get_node_klass()
-            all_shards = await node_klass.get_all_shards(txn, request.uuid)
-
-            updated_shards = PBShards()
-            updated_shards.CopyFrom(all_shards)
-
-            for logic_shard in all_shards.shards:
-                shard = node_klass.create_shard_klass(logic_shard.shard, logic_shard)
-                replicas_cleaned = await shard.clean_and_upgrade()
-                for replica_id, shard_cleaned in replicas_cleaned.items():
-                    update_shards_with_updated_replica(
-                        updated_shards, replica_id, shard_cleaned
-                    )
-
-            key = KB_SHARDS.format(kbid=request.uuid)
-            await txn.set(key, updated_shards.SerializeToString())
-            await txn.commit()
-            return CleanedKnowledgeBoxResponse()
-        except Exception as e:
-            errors.capture_exception(e)
-            logger.error("Error in ingest gRPC servicer", exc_info=True)
-            await txn.abort()
-            raise
+    async def finalize(self): ...
 
     async def SetVectors(  # type: ignore
         self, request: SetVectorsRequest, context=None
     ) -> SetVectorsResponse:
         response = SetVectorsResponse()
         response.found = True
 
-        txn = await self.proc.driver.begin()
-        storage = await get_storage(service_name=SERVICE_NAME)
-
-        kbobj = KnowledgeBoxORM(txn, storage, request.kbid)
-        resobj = ResourceORM(txn, storage, kbobj, request.rid)
+        async with self.driver.transaction() as txn:
+            kbobj = KnowledgeBoxORM(txn, self.storage, request.kbid)
+            resobj = ResourceORM(txn, self.storage, kbobj, request.rid)
 
-        field = await resobj.get_field(
-            request.field.field, request.field.field_type, load=True
-        )
-        if field.value is None:
-            await txn.abort()
-            response.found = False
-            return response
+            field = await resobj.get_field(
+                request.field.field, request.field.field_type, load=True
+            )
+            if field.value is None:
+                response.found = False
+                return response
 
-        evw = ExtractedVectorsWrapper()
-        evw.field.CopyFrom(request.field)
-        evw.vectors.CopyFrom(request.vectors)
-        logger.debug(f"Setting {len(request.vectors.vectors.vectors)} vectors")
+            evw = ExtractedVectorsWrapper()
+            evw.field.CopyFrom(request.field)
+            evw.vectors.CopyFrom(request.vectors)
+            logger.debug(f"Setting {len(request.vectors.vectors.vectors)} vectors")
 
-        try:
-            await field.set_vectors(evw)
-            await txn.commit()
-        except Exception as e:
-            errors.capture_exception(e)
-            logger.error("Error in ingest gRPC servicer", exc_info=True)
-            await txn.abort()
+            try:
+                await field.set_vectors(evw)
+                await txn.commit()
+            except Exception as e:
+                errors.capture_exception(e)
+                logger.error("Error in ingest gRPC servicer", exc_info=True)
 
-        return response
+            return response
 
     async def NewKnowledgeBox(  # type: ignore
         self, request: KnowledgeBoxNew, context=None
     ) -> NewKnowledgeBoxResponse:
         try:
-            kbid = await self.proc.create_kb(
-                request.slug,
-                request.config,
-                forceuuid=request.forceuuid,
-                similarity=request.similarity,
-            )
+            kbid = await self.create_kb(request)
+            logger.info("KB created successfully", extra={"kbid": kbid})
         except KnowledgeBoxConflict:
+            logger.warning("KB already exists", extra={"slug": request.slug})
             return NewKnowledgeBoxResponse(status=KnowledgeBoxResponseStatus.CONFLICT)
-        except Exception:
-            logger.exception("Could not create KB", exc_info=True)
+        except Exception as exc:
+            errors.capture_exception(exc)
+            logger.exception(
+                "Unexpected error creating KB",
+                exc_info=True,
+                extra={"slug": request.slug},
+            )
             return NewKnowledgeBoxResponse(status=KnowledgeBoxResponseStatus.ERROR)
         return NewKnowledgeBoxResponse(status=KnowledgeBoxResponseStatus.OK, uuid=kbid)
 
+    async def create_kb(self, request: KnowledgeBoxNew) -> str:
+        if is_onprem_nucliadb():
+            return await self._create_kb_onprem(request)
+        else:
+            return await self._create_kb_hosted(request)
+
+    async def _create_kb_onprem(self, request: KnowledgeBoxNew) -> str:
+        """
+        First, try to get the learning configuration for the new knowledge box.
+        From there we need to extract the semantic model metadata and pass it to the create_kb method.
+        If the kb creation fails, rollback the learning configuration for the kbid that was just created.
+        """
+        kbid = request.forceuuid or str(uuid.uuid4())
+        release_channel = get_release_channel(request)
+        request.config.release_channel = release_channel
+        lconfig = await learning_proxy.get_configuration(kbid)
+        lconfig_created = False
+        if lconfig is None:
+            if request.learning_config:
+                # We parse the desired configuration from the request and set it
+                config = json.loads(request.learning_config)
+            else:
+                # We set an empty configuration so that learning chooses the default values.
+                config = {}
+                logger.warning(
+                    "No learning configuration provided. Default will be used.",
+                    extra={"kbid": kbid},
+                )
+            lconfig = await learning_proxy.set_configuration(kbid, config=config)
+            lconfig_created = True
+        else:
+            logger.info("Learning configuration already exists", extra={"kbid": kbid})
+        try:
+            await self.proc.create_kb(
+                request.slug,
+                request.config,
+                parse_model_metadata_from_learning_config(lconfig),
+                forceuuid=kbid,
+                release_channel=release_channel,
+            )
+            return kbid
+        except Exception:
+            # Rollback learning config for the kbid that was just created
+            try:
+                if lconfig_created:
+                    await learning_proxy.delete_configuration(kbid)
+            except Exception:
+                logger.warning(
+                    "Could not rollback learning configuration",
+                    exc_info=True,
+                    extra={"kbid": kbid},
+                )
+            raise
+
+    async def _create_kb_hosted(self, request: KnowledgeBoxNew) -> str:
+        """
+        For the hosted case, we assume that the learning configuration
+        is already set and we are given the model metadata in the request.
+        """
+        kbid = request.forceuuid or str(uuid.uuid4())
+        release_channel = get_release_channel(request)
+        request.config.release_channel = release_channel
+        await self.proc.create_kb(
+            request.slug,
+            request.config,
+            parse_model_metadata_from_request(request),
+            forceuuid=kbid,
+            release_channel=release_channel,
+        )
+        return kbid
+
     async def UpdateKnowledgeBox(  # type: ignore
         self, request: KnowledgeBoxUpdate, context=None
     ) -> UpdateKnowledgeBoxResponse:
         try:
             kbid = await self.proc.update_kb(request.uuid, request.slug, request.config)
         except KnowledgeBoxNotFound:
             return UpdateKnowledgeBoxResponse(
@@ -234,176 +261,173 @@
             status=KnowledgeBoxResponseStatus.OK, uuid=kbid
         )
 
     async def DeleteKnowledgeBox(  # type: ignore
         self, request: KnowledgeBoxID, context=None
     ) -> DeleteKnowledgeBoxResponse:
         try:
-            await self.proc.delete_kb(request.uuid, request.slug)
+            await self.delete_kb(request)
         except KnowledgeBoxNotFound:
             logger.warning(f"KB not found: kbid={request.uuid}, slug={request.slug}")
         except Exception:
             logger.exception("Could not delete KB", exc_info=True)
             return DeleteKnowledgeBoxResponse(status=KnowledgeBoxResponseStatus.ERROR)
         return DeleteKnowledgeBoxResponse(status=KnowledgeBoxResponseStatus.OK)
 
-    async def ListKnowledgeBox(  # type: ignore
-        self, request: KnowledgeBoxPrefix, context=None
-    ) -> AsyncIterator[KnowledgeBoxID]:  # type: ignore
-        async for slug in self.proc.list_kb(request.prefix):
-            uuid = await self.proc.get_kb_uuid(slug)
-            yield KnowledgeBoxID(uuid=uuid, slug=slug)
+    async def delete_kb(self, request: KnowledgeBoxID) -> None:
+        kbid = request.uuid
+        await self.proc.delete_kb(kbid, request.slug)
+        # learning configuration is automatically removed in nuclia backend for
+        # hosted users, we only need to remove it for onprem
+        if is_onprem_nucliadb():
+            try:
+                await learning_proxy.delete_configuration(kbid)
+                logger.info("Learning configuration deleted", extra={"kbid": kbid})
+            except Exception:
+                logger.exception(
+                    "Unexpected error deleting learning configuration",
+                    exc_info=True,
+                    extra={"kbid": kbid},
+                )
 
     async def GCKnowledgeBox(  # type: ignore
         self, request: KnowledgeBoxID, context=None
     ) -> GCKnowledgeBoxResponse:
         response = GCKnowledgeBoxResponse()
         return response
 
     async def ProcessMessage(  # type: ignore
         self, request_stream: AsyncIterator[BrokerMessage], context=None
     ):
         response = OpStatusWriter()
-        cache = await get_cache()
         async for message in request_stream:
             try:
                 await self.proc.process(
                     message, -1, partition=self.partitions[0], transaction_check=False
                 )
             except Exception:
                 logger.exception("Error processing", stack_info=True)
                 response.status = OpStatusWriter.Status.ERROR
                 break
             response.status = OpStatusWriter.Status.OK
             logger.info(f"Processed {message.uuid}")
-            if cache is not None:
-                await cache.delete(
-                    KB_COUNTER_CACHE.format(kbid=message.kbid), invalidate=True
-                )
         return response
 
     async def SetLabels(self, request: SetLabelsRequest, context=None) -> OpStatusWriter:  # type: ignore
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb)
-        response = OpStatusWriter()
-        if kbobj is not None:
-            try:
-                await kbobj.set_labelset(request.id, request.labelset)
-                await txn.commit()
-                response.status = OpStatusWriter.Status.OK
-            except Exception as e:
-                errors.capture_exception(e)
-                logger.error("Error in ingest gRPC servicer", exc_info=True)
-                response.status = OpStatusWriter.Status.ERROR
-                await txn.abort()
-        else:
-            await txn.abort()
-            response.status = OpStatusWriter.Status.NOTFOUND
-        return response
+        async with self.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb)
+            response = OpStatusWriter()
+            if kbobj is not None:
+                try:
+                    await kbobj.set_labelset(request.id, request.labelset)
+                    await txn.commit()
+                    response.status = OpStatusWriter.Status.OK
+                except Exception as e:
+                    errors.capture_exception(e)
+                    logger.error("Error in ingest gRPC servicer", exc_info=True)
+                    response.status = OpStatusWriter.Status.ERROR
+            else:
+                response.status = OpStatusWriter.Status.NOTFOUND
+            return response
 
     async def DelLabels(self, request: DelLabelsRequest, context=None) -> OpStatusWriter:  # type: ignore
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb)
-        response = OpStatusWriter()
-        if kbobj is not None:
-            try:
-                await kbobj.del_labelset(request.id)
-                await txn.commit()
-                response.status = OpStatusWriter.Status.OK
-            except Exception as e:
-                errors.capture_exception(e)
-                logger.error("Error in ingest gRPC servicer", exc_info=True)
-                response.status = OpStatusWriter.Status.ERROR
-                await txn.abort()
-        else:
-            await txn.abort()
-            response.status = OpStatusWriter.Status.NOTFOUND
+        async with self.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb)
+            response = OpStatusWriter()
+            if kbobj is not None:
+                try:
+                    await kbobj.del_labelset(request.id)
+                    await txn.commit()
+                    response.status = OpStatusWriter.Status.OK
+                except Exception as e:
+                    errors.capture_exception(e)
+                    logger.error("Error in ingest gRPC servicer", exc_info=True)
+                    response.status = OpStatusWriter.Status.ERROR
+            else:
+                response.status = OpStatusWriter.Status.NOTFOUND
 
-        return response
+            return response
 
     async def GetLabels(self, request: GetLabelsRequest, context=None) -> GetLabelsResponse:  # type: ignore
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb)
-        labels: Optional[Labels] = None
-        if kbobj is not None:
-            labels = await kbobj.get_labels()
-        await txn.abort()
+        async with self.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb)
+            labels: Optional[Labels] = None
+            if kbobj is not None:
+                labels = await kbobj.get_labels()
         response = GetLabelsResponse()
         if kbobj is None:
             response.status = GetLabelsResponse.Status.NOTFOUND
         else:
             response.kb.uuid = kbobj.kbid
             if labels is not None:
                 response.labels.CopyFrom(labels)
 
         return response
 
     async def GetLabelSet(  # type: ignore
         self, request: GetLabelSetRequest, context=None
     ) -> GetLabelSetResponse:
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb)
-        response = GetLabelSetResponse()
-        if kbobj is not None:
-            await kbobj.get_labelset(request.labelset, response)
-            response.kb.uuid = kbobj.kbid
-            response.status = GetLabelSetResponse.Status.OK
-        await txn.abort()
-        if kbobj is None:
-            response.status = GetLabelSetResponse.Status.NOTFOUND
-        return response
+        async with self.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb)
+            response = GetLabelSetResponse()
+            if kbobj is not None:
+                await kbobj.get_labelset(request.labelset, response)
+                response.kb.uuid = kbobj.kbid
+                response.status = GetLabelSetResponse.Status.OK
+            else:
+                response.status = GetLabelSetResponse.Status.NOTFOUND
+            return response
 
     async def GetVectorSets(  # type: ignore
         self, request: GetVectorSetsRequest, context=None
     ) -> GetVectorSetsResponse:
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb)
-        response = GetVectorSetsResponse()
-        if kbobj is not None:
-            await kbobj.get_vectorsets(response)
-            response.kb.uuid = kbobj.kbid
-            response.status = GetVectorSetsResponse.Status.OK
-        await txn.abort()
-        if kbobj is None:
-            response.status = GetVectorSetsResponse.Status.NOTFOUND
-        return response
+        async with self.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb)
+            response = GetVectorSetsResponse()
+            if kbobj is not None:
+                await kbobj.get_vectorsets(response)
+                response.kb.uuid = kbobj.kbid
+                response.status = GetVectorSetsResponse.Status.OK
+            else:
+                response.status = GetVectorSetsResponse.Status.NOTFOUND
+            return response
 
     async def DelVectorSet(  # type: ignore
         self, request: DelVectorSetRequest, context=None
     ) -> OpStatusWriter:
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb)
-        response = OpStatusWriter()
-        if kbobj is not None:
-            await kbobj.del_vectorset(request.vectorset)
-            response.status = OpStatusWriter.Status.OK
-        await txn.commit()
-        if kbobj is None:
-            response.status = OpStatusWriter.Status.NOTFOUND
-        return response
+        async with self.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb)
+            response = OpStatusWriter()
+            if kbobj is not None:
+                await kbobj.del_vectorset(request.vectorset)
+                response.status = OpStatusWriter.Status.OK
+                await txn.commit()
+            else:
+                response.status = OpStatusWriter.Status.NOTFOUND
+            return response
 
     async def SetVectorSet(  # type: ignore
         self, request: SetVectorSetRequest, context=None
     ) -> OpStatusWriter:
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb)
-        response = OpStatusWriter()
-        if kbobj is not None:
-            await kbobj.set_vectorset(request.id, request.vectorset)
-            response.status = OpStatusWriter.Status.OK
-        await txn.commit()
-        if kbobj is None:
-            response.status = OpStatusWriter.Status.NOTFOUND
-        return response
+        async with self.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb)
+            response = OpStatusWriter()
+            if kbobj is not None:
+                await kbobj.set_vectorset(request.id, request.vectorset)
+                response.status = OpStatusWriter.Status.OK
+                await txn.commit()
+            else:
+                response.status = OpStatusWriter.Status.NOTFOUND
+            return response
 
     async def NewEntitiesGroup(  # type: ignore
         self, request: NewEntitiesGroupRequest, context=None
     ) -> NewEntitiesGroupResponse:
         response = NewEntitiesGroupResponse()
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, request.kb)
             if kbobj is None:
                 response.status = NewEntitiesGroupResponse.Status.KB_NOT_FOUND
                 return response
 
             entities_manager = EntitiesManager(kbobj, txn)
             try:
@@ -418,15 +442,15 @@
             response.status = NewEntitiesGroupResponse.Status.OK
             return response
 
     async def GetEntities(  # type: ignore
         self, request: GetEntitiesRequest, context=None
     ) -> GetEntitiesResponse:
         response = GetEntitiesResponse()
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, request.kb)
 
             if kbobj is None:
                 response.status = GetEntitiesResponse.Status.NOTFOUND
                 return response
 
             entities_manager = EntitiesManager(kbobj, txn)
@@ -441,22 +465,22 @@
                 response.status = GetEntitiesResponse.Status.OK
             return response
 
     async def ListEntitiesGroups(  # type: ignore
         self, request: ListEntitiesGroupsRequest, context=None
     ) -> ListEntitiesGroupsResponse:
         response = ListEntitiesGroupsResponse()
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, request.kb)
 
             if kbobj is None:
                 response.status = ListEntitiesGroupsResponse.Status.NOTFOUND
                 return response
 
-            entities_manager = EntitiesManager(kbobj, txn)
+            entities_manager = EntitiesManager(kbobj, txn, use_read_replica_nodes=True)
             try:
                 entities_groups = await entities_manager.list_entities_groups()
             except Exception as e:
                 errors.capture_exception(e)
                 logger.error("Error in ingest gRPC servicer", exc_info=True)
                 response.status = ListEntitiesGroupsResponse.Status.ERROR
             else:
@@ -466,45 +490,44 @@
 
             return response
 
     async def GetEntitiesGroup(  # type: ignore
         self, request: GetEntitiesGroupRequest, context=None
     ) -> GetEntitiesGroupResponse:
         response = GetEntitiesGroupResponse()
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, request.kb)
-
             if kbobj is None:
                 response.status = GetEntitiesGroupResponse.Status.KB_NOT_FOUND
                 return response
 
             entities_manager = EntitiesManager(kbobj, txn)
             try:
                 entities_group = await entities_manager.get_entities_group(
                     request.group
                 )
             except Exception as e:
                 errors.capture_exception(e)
                 logger.error("Error in ingest gRPC servicer", exc_info=True)
                 response.status = GetEntitiesGroupResponse.Status.ERROR
             else:
+                response.kb.uuid = kbobj.kbid
                 if entities_group is None:
                     response.status = (
                         GetEntitiesGroupResponse.Status.ENTITIES_GROUP_NOT_FOUND
                     )
                 else:
-                    response.kb.uuid = kbobj.kbid
                     response.status = GetEntitiesGroupResponse.Status.OK
                     response.group.CopyFrom(entities_group)
 
             return response
 
     async def SetEntities(self, request: SetEntitiesRequest, context=None) -> OpStatusWriter:  # type: ignore
         response = OpStatusWriter()
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, request.kb)
             if kbobj is None:
                 response.status = OpStatusWriter.Status.NOTFOUND
                 return response
 
             entities_manager = EntitiesManager(kbobj, txn)
             try:
@@ -520,15 +543,15 @@
                 await txn.commit()
             return response
 
     async def UpdateEntitiesGroup(  # type: ignore
         self, request: UpdateEntitiesGroupRequest, context=None
     ) -> UpdateEntitiesGroupResponse:
         response = UpdateEntitiesGroupResponse()
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, request.kb)
             if kbobj is None:
                 response.status = UpdateEntitiesGroupResponse.Status.KB_NOT_FOUND
                 return response
 
             entities_manager = EntitiesManager(kbobj, txn)
 
@@ -549,15 +572,15 @@
 
             await txn.commit()
             response.status = UpdateEntitiesGroupResponse.Status.OK
             return response
 
     async def DelEntities(self, request: DelEntitiesRequest, context=None) -> OpStatusWriter:  # type: ignore
         response = OpStatusWriter()
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, request.kb)
             if kbobj is None:
                 response.status = OpStatusWriter.Status.NOTFOUND
                 return response
 
             entities_manager = EntitiesManager(kbobj, txn)
             try:
@@ -573,15 +596,15 @@
 
     async def GetSynonyms(  # type: ignore
         self, request: KnowledgeBoxID, context=None
     ) -> GetSynonymsResponse:
         kbid = request
         response = GetSynonymsResponse()
         txn: Transaction
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, kbid)
             if kbobj is None:
                 response.status.status = OpStatusWriter.Status.NOTFOUND
                 return response
             try:
                 await kbobj.get_synonyms(response.synonyms)
                 response.status.status = OpStatusWriter.Status.OK
@@ -594,15 +617,15 @@
 
     async def SetSynonyms(  # type: ignore
         self, request: SetSynonymsRequest, context=None
     ) -> OpStatusWriter:
         kbid = request.kbid
         response = OpStatusWriter()
         txn: Transaction
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, kbid)
             if kbobj is None:
                 response.status = OpStatusWriter.Status.NOTFOUND
                 return response
             try:
                 await kbobj.set_synonyms(request.synonyms)
                 await txn.commit()
@@ -616,15 +639,15 @@
 
     async def DelSynonyms(  # type: ignore
         self, request: KnowledgeBoxID, context=None
     ) -> OpStatusWriter:
         kbid = request
         response = OpStatusWriter()
         txn: Transaction
-        async with self.proc.driver.transaction() as txn:
+        async with self.driver.transaction() as txn:
             kbobj = await self.proc.get_kb_obj(txn, kbid)
             if kbobj is None:
                 response.status = OpStatusWriter.Status.NOTFOUND
                 return response
             try:
                 await kbobj.delete_synonyms()
                 await txn.commit()
@@ -637,186 +660,169 @@
                 return response
 
     async def Status(  # type: ignore
         self, request: WriterStatusRequest, context=None
     ) -> WriterStatusResponse:
         logger.info("Status Call")
         response = WriterStatusResponse()
-        txn = await self.proc.driver.begin()
-        async for (kbid, slug) in KnowledgeBoxObj.get_kbs(txn, slug="", count=-1):
-            response.knowledgeboxes.append(slug)
-
-        for partition in settings.partitions:
-            seq_id = await sequence_manager.get_last_seqid(self.proc.driver, partition)
-            if seq_id is not None:
-                response.msgid[partition] = seq_id
+        async with self.driver.transaction() as txn:
+            async for _, slug in datamanagers.kb.get_kbs(txn):
+                response.knowledgeboxes.append(slug)
+
+            for partition in settings.partitions:
+                seq_id = await sequence_manager.get_last_seqid(self.driver, partition)
+                if seq_id is not None:
+                    response.msgid[partition] = seq_id
 
-        await txn.abort()
-        return response
+            return response
 
     async def ListMembers(  # type: ignore
         self, request: ListMembersRequest, context=None
     ) -> ListMembersResponse:
         response = ListMembersResponse()
-        response.members.extend(await Node.list_members())
+        response.members.extend(
+            [
+                writer_pb2.Member(
+                    id=n.id,
+                    listen_address=n.address,
+                    is_self=False,
+                    dummy=n.dummy,
+                    shard_count=n.shard_count,
+                    primary_id=n.primary_id or "",
+                )
+                for n in get_index_nodes(include_secondary=True)
+            ]
+        )
         return response
 
     async def GetResourceId(  # type: ignore
         self, request: ResourceIdRequest, context=None
     ) -> ResourceIdResponse:
         response = ResourceIdResponse()
         response.uuid = ""
-        txn = await self.proc.driver.begin()
-        storage = await get_storage(service_name=SERVICE_NAME)
-
-        kbobj = KnowledgeBoxORM(txn, storage, request.kbid)
-        rid = await kbobj.get_resource_uuid_by_slug(request.slug)
-        if rid:
-            response.uuid = rid
-        await txn.abort()
-        return response
+        async with self.driver.transaction() as txn:
+            kbobj = KnowledgeBoxORM(txn, self.storage, request.kbid)
+            rid = await kbobj.get_resource_uuid_by_slug(request.slug)
+            if rid:
+                response.uuid = rid
+            return response
 
     async def ResourceFieldExists(  # type: ignore
         self, request: ResourceFieldId, context=None
     ) -> ResourceFieldExistsResponse:
         response = ResourceFieldExistsResponse()
         response.found = False
         resobj = None
-        txn = await self.proc.driver.begin()
-        storage = await get_storage(service_name=SERVICE_NAME)
-
-        kbobj = KnowledgeBoxORM(txn, storage, request.kbid)
-        resobj = ResourceORM(txn, storage, kbobj, request.rid)
+        async with self.driver.transaction() as txn:
+            kbobj = KnowledgeBoxORM(txn, self.storage, request.kbid)
+            resobj = ResourceORM(txn, self.storage, kbobj, request.rid)
+
+            if request.field != "":
+                field = await resobj.get_field(
+                    request.field, request.field_type, load=True
+                )
+                if field.value is not None:
+                    response.found = True
+                else:
+                    response.found = False
+                return response
 
-        if request.field != "":
-            field = await resobj.get_field(request.field, request.field_type, load=True)
-            if field.value is not None:
-                response.found = True
-            else:
-                response.found = False
-            await txn.abort()
-            return response
+            if request.rid != "":
+                if await resobj.exists():
+                    response.found = True
+                else:
+                    response.found = False
+                return response
 
-        if request.rid != "":
-            if await resobj.exists():
-                response.found = True
-            else:
-                response.found = False
-            await txn.abort()
-            return response
+            if request.kbid != "":
+                config = await datamanagers.kb.get_config(txn, kbid=request.kbid)
+                if config is not None:
+                    response.found = True
+                else:
+                    response.found = False
+                return response
 
-        if request.kbid != "":
-            config = await KnowledgeBoxORM.get_kb(txn, request.kbid)
-            if config is not None:
-                response.found = True
-            else:
-                response.found = False
-            await txn.abort()
             return response
 
-        await txn.abort()
-        return response
-
     async def Index(self, request: IndexResource, context=None) -> IndexStatus:  # type: ignore
-        txn = await self.proc.driver.begin()
-        storage = await get_storage(service_name=SERVICE_NAME)
-
-        kbobj = KnowledgeBoxORM(txn, storage, request.kbid)
-        resobj = ResourceORM(txn, storage, kbobj, request.rid)
-        bm = await resobj.generate_broker_message()
-        transaction = get_transaction_utility()
-        partitioning = get_partitioning()
-        partition = partitioning.generate_partition(request.kbid, request.rid)
-        await transaction.commit(bm, partition)
+        async with self.driver.transaction() as txn:
+            kbobj = KnowledgeBoxORM(txn, self.storage, request.kbid)
+            resobj = ResourceORM(txn, self.storage, kbobj, request.rid)
+            bm = await resobj.generate_broker_message()
+            transaction = get_transaction_utility()
+            partitioning = get_partitioning()
+            partition = partitioning.generate_partition(request.kbid, request.rid)
+            await transaction.commit(bm, partition)
 
-        response = IndexStatus()
-        await txn.abort()
-        return response
+            response = IndexStatus()
+            return response
 
     async def ReIndex(self, request: IndexResource, context=None) -> IndexStatus:  # type: ignore
         try:
-            txn = await self.proc.driver.begin()
-            storage = await get_storage(service_name=SERVICE_NAME)
-            kbobj = KnowledgeBoxORM(txn, storage, request.kbid)
-            resobj = ResourceORM(txn, storage, kbobj, request.rid)
-            resobj.disable_vectors = not request.reindex_vectors
-
-            brain = await resobj.generate_index_message()
-            shard_id = await kbobj.get_resource_shard_id(request.rid)
-            shard: Optional[Shard] = None
-            node_klass = get_node_klass()
-            if shard_id is not None:
-                shard = await kbobj.get_resource_shard(shard_id, node_klass)
+            async with self.driver.transaction() as txn:
+                kbobj = KnowledgeBoxORM(txn, self.storage, request.kbid)
+                resobj = ResourceORM(txn, self.storage, kbobj, request.rid)
+                resobj.disable_vectors = not request.reindex_vectors
+
+                brain = await resobj.generate_index_message()
+                shard_id = await datamanagers.resources.get_resource_shard_id(
+                    txn, kbid=request.kbid, rid=request.rid
+                )
+                shard: Optional[writer_pb2.ShardObject] = None
+                if shard_id is not None:
+                    shard = await kbobj.get_resource_shard(shard_id)
 
-            if shard is None:
-                shard = await node_klass.get_current_active_shard(txn, request.kbid)
                 if shard is None:
-                    # no shard currently exists, create one
-                    similarity = await kbobj.get_similarity()
-                    shard = await node_klass.create_shard_by_kbid(
-                        txn, request.kbid, similarity=similarity
+                    shard = await self.shards_manager.get_current_active_shard(
+                        txn, request.kbid
                     )
+                    if shard is None:
+                        # no shard currently exists, create one
+                        model = await datamanagers.kb.get_model_metadata(
+                            txn, kbid=request.kbid
+                        )
+                        shard = await self.shards_manager.create_shard_by_kbid(
+                            txn, request.kbid, semantic_model=model
+                        )
 
-                await kbobj.set_resource_shard_id(request.rid, shard.sharduuid)
+                    await datamanagers.resources.set_resource_shard_id(
+                        txn, kbid=request.kbid, rid=request.rid, shard=shard.shard
+                    )
 
-            if shard is not None:
-                counter = await shard.add_resource(
-                    brain.brain,
-                    0,
-                    partition=self.partitions[0],
-                    kb=request.kbid,
-                    reindex_id=uuid.uuid4().hex,
-                )
-
-                if counter is not None and counter.fields > settings.max_shard_fields:
-                    # check to see if we've exceeded the max resources per shard
-                    # and create a new shard for new resources to land on
-                    similarity = await kbobj.get_similarity()
-                    shard = await node_klass.create_shard_by_kbid(
-                        txn, request.kbid, similarity=similarity
+                if shard is not None:
+                    await self.shards_manager.add_resource(
+                        shard,
+                        brain.brain,
+                        0,
+                        partition=self.partitions[0],
+                        kb=request.kbid,
+                        reindex_id=uuid.uuid4().hex,
                     )
 
-            response = IndexStatus()
-            await txn.abort()
-            return response
+                response = IndexStatus()
+                return response
         except Exception as e:
             errors.capture_exception(e)
             logger.error("Error in ingest gRPC servicer", exc_info=True)
-            await txn.abort()
-            raise
-
-    async def Export(self, request: ExportRequest, context=None):
-        try:
-            txn = await self.proc.driver.begin()
-            storage = await get_storage(service_name=SERVICE_NAME)
-
-            kbobj = KnowledgeBoxORM(txn, storage, request.kbid)
-            async for resource in kbobj.iterate_resources():
-                yield await resource.generate_broker_message()
-            await txn.abort()
-        except Exception:
-            logger.exception("Export", stack_info=True)
             raise
 
     async def DownloadFile(self, request: FileRequest, context=None):
-        storage = await get_storage(service_name=SERVICE_NAME)
-        async for data in storage.download(request.bucket, request.key):
+        async for data in self.storage.download(request.bucket, request.key):
             yield BinaryData(data=data)
 
     async def UploadFile(self, request: AsyncIterator[UploadBinaryData], context=None) -> FileUploaded:  # type: ignore
-        storage = await get_storage(service_name=SERVICE_NAME)
         data: UploadBinaryData
 
         destination: Optional[StorageField] = None
         cf = CloudFile()
         data = await request.__anext__()
         if data.HasField("metadata"):
-            bucket = storage.get_bucket_name(data.metadata.kbid)
-            destination = storage.field_klass(
-                storage=storage, bucket=bucket, fullkey=data.metadata.key
+            bucket = self.storage.get_bucket_name(data.metadata.kbid)
+            destination = self.storage.field_klass(
+                storage=self.storage, bucket=bucket, fullkey=data.metadata.key
             )
             cf.content_type = data.metadata.content_type
             cf.filename = data.metadata.filename
             cf.size = data.metadata.size
         else:
             raise AttributeError("Metadata not found")
 
@@ -841,23 +847,67 @@
             buf.seek(0)
             data = buf.read()
             if len(data):
                 yield data
 
         if destination is None:
             raise AttributeError("No destination file")
-        await storage.uploaditerator(generate_buffer(storage, request), destination, cf)
+        await self.storage.uploaditerator(
+            generate_buffer(self.storage, request), destination, cf
+        )
         result = FileUploaded()
         return result
 
 
-def update_shards_with_updated_replica(
-    shards: PBShards, replica_id: str, updated_replica: ShardCleaned
-):
-    for logic_shard in shards.shards:
-        for replica in logic_shard.replicas:
-            if replica.shard.id == replica_id:
-                replica.shard.document_service = updated_replica.document_service
-                replica.shard.vector_service = updated_replica.vector_service
-                replica.shard.paragraph_service = updated_replica.paragraph_service
-                replica.shard.relation_service = updated_replica.relation_service
-                return
+LEARNING_SIMILARITY_FUNCTION_TO_PROTO = {
+    "cosine": utils_pb2.VectorSimilarity.COSINE,
+    "dot": utils_pb2.VectorSimilarity.DOT,
+}
+
+
+def parse_model_metadata_from_learning_config(
+    lconfig: learning_proxy.LearningConfiguration,
+) -> SemanticModelMetadata:
+    model = SemanticModelMetadata()
+    model.similarity_function = LEARNING_SIMILARITY_FUNCTION_TO_PROTO[
+        lconfig.semantic_vector_similarity
+    ]
+    if lconfig.semantic_vector_size is not None:
+        model.vector_dimension = lconfig.semantic_vector_size
+    else:
+        logger.warning("Vector dimension not set!")
+    if lconfig.semantic_threshold is not None:
+        model.default_min_score = lconfig.semantic_threshold
+    else:
+        logger.warning("Default min score not set!")
+    return model
+
+
+def parse_model_metadata_from_request(
+    request: KnowledgeBoxNew,
+) -> SemanticModelMetadata:
+    model = SemanticModelMetadata()
+    model.similarity_function = request.similarity
+    if request.HasField("vector_dimension"):
+        model.vector_dimension = request.vector_dimension
+    else:
+        logger.warning(
+            "Vector dimension not set. Will be detected automatically on the first vector set."
+        )
+    if request.HasField("default_min_score"):
+        model.default_min_score = request.default_min_score
+    else:
+        logger.warning("Default min score not set!")
+    return model
+
+
+def get_release_channel(request: KnowledgeBoxNew) -> utils_pb2.ReleaseChannel.ValueType:
+    """
+    Set channel to Experimental if specified in the grpc request or if the requested
+    slug has the experimental_kb feature enabled in stage environment.
+    """
+    release_channel = request.release_channel
+    if running_settings.running_environment == "stage" and has_feature(
+        const.Features.EXPERIMENTAL_KB, context={"slug": request.slug}
+    ):
+        release_channel = utils_pb2.ReleaseChannel.EXPERIMENTAL
+    return release_channel
```

## nucliadb/ingest/tests/conftest.py

```diff
@@ -17,12 +17,14 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 pytest_plugins = [
     "pytest_docker_fixtures",
     "nucliadb_utils.tests.nats",
     "nucliadb.ingest.tests.fixtures",
-    "nucliadb.ingest.tests.tikv",
+    "nucliadb.tests.fixtures",
+    "nucliadb.tests.tikv",
+    "nucliadb_utils.tests.conftest",
     "nucliadb_utils.tests.gcs",
     "nucliadb_utils.tests.s3",
     "nucliadb_telemetry.tests.telemetry",
 ]
```

## nucliadb/ingest/tests/fixtures.py

```diff
@@ -14,55 +14,50 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import logging
-import os
 import uuid
 from dataclasses import dataclass
 from datetime import datetime
 from os.path import dirname, getsize
-from shutil import rmtree
-from tempfile import mkdtemp
-from typing import AsyncIterator, List, Optional
+from typing import Optional
 from unittest.mock import AsyncMock, patch
 
 import nats
 import pytest
-from grpc import aio  # type: ignore
+from grpc import aio
+from nucliadb_protos.knowledgebox_pb2 import SemanticModelMetadata
 from nucliadb_protos.writer_pb2 import BrokerMessage
-from redis import asyncio as aioredis
 
+from nucliadb.common.cluster import manager
+from nucliadb.common.cluster.settings import settings as cluster_settings
+from nucliadb.common.maindb.driver import Driver
 from nucliadb.ingest.consumer import service as consumer_service
-from nucliadb.ingest.maindb.driver import Driver
-from nucliadb.ingest.maindb.local import LocalDriver
-from nucliadb.ingest.maindb.redis import RedisDriver
-from nucliadb.ingest.maindb.tikv import TiKVDriver
+from nucliadb.ingest.fields.base import Field
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
-from nucliadb.ingest.orm.node import NODES, Node
 from nucliadb.ingest.orm.processor import Processor
+from nucliadb.ingest.orm.resource import KB_REVERSE, Resource
 from nucliadb.ingest.service.writer import WriterServicer
-from nucliadb.ingest.settings import DriverConfig, settings
+from nucliadb.ingest.settings import settings
 from nucliadb.ingest.tests.vectors import V1, V2, V3
-from nucliadb_models.cluster import MemberType
+from nucliadb.learning_proxy import LearningConfiguration
 from nucliadb_protos import resources_pb2 as rpb
 from nucliadb_protos import utils_pb2 as upb
 from nucliadb_protos import writer_pb2_grpc
 from nucliadb_utils import const
 from nucliadb_utils.audit.basic import BasicAuditStorage
 from nucliadb_utils.audit.stream import StreamAuditStorage
 from nucliadb_utils.cache.nats import NatsPubsub
-from nucliadb_utils.cache.settings import settings as cache_settings
-from nucliadb_utils.cache.utility import Cache
 from nucliadb_utils.indexing import IndexingUtility
 from nucliadb_utils.settings import indexing_settings, transaction_settings
 from nucliadb_utils.storages.settings import settings as storage_settings
-from nucliadb_utils.store import MAIN
+from nucliadb_utils.storages.storage import Storage
 from nucliadb_utils.utilities import (
     Utility,
     clean_utility,
     clear_global_cache,
     get_utility,
     set_utility,
     start_nats_manager,
@@ -71,22 +66,22 @@
     stop_transaction_utility,
 )
 
 logger = logging.getLogger(__name__)
 
 
 @pytest.fixture(scope="function")
-async def processor(maindb_driver, gcs_storage, cache):
-    proc = Processor(maindb_driver, gcs_storage, cache, partition="1")
+async def processor(maindb_driver, storage, pubsub):
+    proc = Processor(maindb_driver, storage, pubsub, partition="1")
     yield proc
 
 
 @pytest.fixture(scope="function")
-async def stream_processor(maindb_driver, gcs_storage, cache):
-    proc = Processor(maindb_driver, gcs_storage, cache, partition="1")
+async def stream_processor(maindb_driver, storage, pubsub):
+    proc = Processor(maindb_driver, storage, pubsub, partition="1")
     yield proc
 
 
 @pytest.fixture(scope="function")
 async def local_files():
     storage_settings.local_testing_files = f"{dirname(__file__)}"
 
@@ -96,66 +91,41 @@
     servicer: WriterServicer
     channel: aio.Channel
     host: str
     serv: aio.Server
 
 
 @pytest.fixture(scope="function")
-async def redis_config(redis):
-    settings.driver_redis_url = f"redis://{redis[0]}:{redis[1]}"
-    cache_settings.cache_pubsub_redis_url = f"redis://{redis[0]}:{redis[1]}"
-    default_driver = settings.driver
-    default_driver_pubsub = cache_settings.cache_pubsub_driver
-
-    cache_settings.cache_pubsub_driver = "redis"
-    settings.driver = "redis"
-
-    storage_settings.local_testing_files = f"{dirname(__file__)}"
-    driver = aioredis.from_url(f"redis://{redis[0]}:{redis[1]}")
-    await driver.flushall()
-
-    yield
-
-    settings.driver_redis_url = None
-    cache_settings.cache_pubsub_redis_url = None
-    settings.driver = default_driver
-    cache_settings.cache_pubsub_driver = default_driver_pubsub
-    await driver.flushall()
-    await driver.close(close_connection_pool=True)
-
-    pubsub = get_utility(Utility.PUBSUB)
-    if pubsub is not None:
-        await pubsub.finalize()
-    clear_global_cache()
-
-
-@pytest.fixture(scope="function")
 async def ingest_consumers(
-    redis_config, transaction_utility, gcs_storage, fake_node, nats_manager
+    redis_config, transaction_utility, storage, fake_node, nats_manager
 ):
     ingest_consumers_finalizer = await consumer_service.start_ingest_consumers()
 
     yield
 
     await ingest_consumers_finalizer()
+    clear_global_cache()
 
 
 @pytest.fixture(scope="function")
 async def ingest_processed_consumer(
-    redis_config, transaction_utility, gcs_storage, fake_node, nats_manager
+    redis_config, transaction_utility, storage, fake_node, nats_manager
 ):
     ingest_consumer_finalizer = await consumer_service.start_ingest_processed_consumer()
 
     yield
 
     await ingest_consumer_finalizer()
+    clear_global_cache()
 
 
 @pytest.fixture(scope="function")
-async def grpc_servicer(redis_config, ingest_consumers, ingest_processed_consumer):
+async def grpc_servicer(
+    maindb_driver, ingest_consumers, ingest_processed_consumer, learning_config
+):
     servicer = WriterServicer()
     await servicer.initialize()
 
     server = aio.server()
     port = server.add_insecure_port("[::]:0")
     writer_pb2_grpc.add_WriterServicer_to_server(servicer, server)
     await server.start()
@@ -168,151 +138,81 @@
     )
     await servicer.finalize()
     await _channel.close()
     await server.stop(None)
 
 
 @pytest.fixture(scope="function")
-async def local_driver() -> AsyncIterator[Driver]:
-    path = mkdtemp()
-    settings.driver = DriverConfig.LOCAL
-    settings.driver_local_url = path
-    driver: Driver = LocalDriver(url=path)
-    await driver.initialize()
-    yield driver
-    await driver.finalize()
-    rmtree(path)
-    settings.driver_local_url = None
-    MAIN.pop("driver", None)
-
-
-@pytest.fixture(scope="function")
-async def tikv_driver(tikvd: List[str]) -> AsyncIterator[Driver]:
-    if os.environ.get("TESTING_TIKV_LOCAL", None):
-        url = "localhost:2379"
-    else:
-        url = f"{tikvd[0]}:{tikvd[2]}"
-    settings.driver = DriverConfig.TIKV
-    settings.driver_tikv_url = [url]
-
-    driver: Driver = TiKVDriver(url=[url])
-    await driver.initialize()
-
-    yield driver
-
-    txn = await driver.begin()
-    async for key in txn.keys(""):
-        await txn.delete(key)
-    await txn.commit()
-    await driver.finalize()
-    settings.driver_tikv_url = []
-    MAIN.pop("driver", None)
-
-
-@pytest.fixture(scope="function")
-async def redis_driver(redis: List[str]) -> AsyncIterator[RedisDriver]:
-    url = f"redis://{redis[0]}:{redis[1]}"
-    settings.driver = DriverConfig.REDIS
-    settings.driver_redis_url = f"redis://{redis[0]}:{redis[1]}"
-
-    driver = RedisDriver(url=url)
-    await driver.initialize()
-
-    assert driver.redis is not None
-    await driver.redis.flushall()
-    logging.info(f"Redis driver ready at {url}")
-
-    yield driver
-
-    await driver.finalize()
-    settings.driver_redis_url = None
-    MAIN.pop("driver", None)
-
-
-@pytest.fixture(scope="function")
-async def maindb_driver(redis_driver):
-    yield redis_driver
-
-
-@pytest.fixture(scope="function")
-async def txn(maindb_driver):
-    txn = await maindb_driver.begin()
-    yield txn
-    await txn.abort()
-
-
-@pytest.fixture(scope="function")
 async def pubsub(natsd):
     pubsub = get_utility(Utility.PUBSUB)
     if pubsub is None:
         pubsub = NatsPubsub(hosts=[natsd])
         await pubsub.initialize()
         set_utility(Utility.PUBSUB, pubsub)
 
     yield pubsub
+    clean_utility(Utility.PUBSUB)
     await pubsub.finalize()
-    set_utility(Utility.PUBSUB, None)
-
-
-@pytest.fixture(scope="function")
-async def cache(pubsub):
-    che = Cache(pubsub)
-    await che.initialize()
-    yield che
-    await che.finalize()
 
 
 @pytest.fixture(scope="function")
-async def fake_node(_natsd_reset, indexing_utility_ingest):
-    NODES.clear()
-    uuid1 = str(uuid.uuid4())
-    uuid2 = str(uuid.uuid4())
-    await Node.set(
-        uuid1,
-        address="nohost:9999",
-        type=MemberType.IO,
+async def fake_node(indexing_utility, shard_manager):
+    manager.INDEX_NODES.clear()
+    manager.add_index_node(
+        id=str(uuid.uuid4()),
+        address="nohost",
         shard_count=0,
+        available_disk=100,
         dummy=True,
     )
-    await Node.set(
-        uuid2,
-        address="nohost:9999",
-        type=MemberType.IO,
+    manager.add_index_node(
+        id=str(uuid.uuid4()),
+        address="nohost",
         shard_count=0,
-        dummy=True,
-    )
-    indexing_utility = IndexingUtility(
-        nats_creds=indexing_settings.index_jetstream_auth,
-        nats_servers=indexing_settings.index_jetstream_servers,
+        available_disk=100,
         dummy=True,
     )
 
-    old_index_local = indexing_settings.index_local
-    indexing_settings.index_local = False
-    set_utility(Utility.INDEXING, indexing_utility)
-    yield
-    clean_utility(Utility.INDEXING)
-    indexing_settings.index_local = old_index_local
-    if await Node.get(uuid1):
-        await Node.destroy(uuid1)
-    if await Node.get(uuid2):
-        await Node.destroy(uuid2)
+    with patch.object(cluster_settings, "standalone_mode", False):
+        yield
+
+    manager.INDEX_NODES.clear()
+
+
+@pytest.fixture()
+def learning_config():
+    lconfig = LearningConfiguration(
+        semantic_model="multilingual",
+        semantic_threshold=None,
+        semantic_vector_size=None,
+        semantic_vector_similarity="cosine",
+    )
+    with patch("nucliadb.ingest.service.writer.learning_proxy") as mocked:
+        mocked.set_configuration = AsyncMock(return_value=None)
+        mocked.get_configuration = AsyncMock(return_value=lconfig)
+        mocked.delete_configuration = AsyncMock(return_value=None)
+        yield mocked
 
 
 @pytest.fixture(scope="function")
-async def knowledgebox_ingest(gcs_storage, redis_driver: RedisDriver):
+async def knowledgebox_ingest(
+    storage, maindb_driver: Driver, shard_manager, learning_config
+):
     kbid = str(uuid.uuid4())
     kbslug = str(uuid.uuid4())
-    txn = await redis_driver.begin()
-    await KnowledgeBox.create(txn, kbslug, kbid)
-    await txn.commit()
+    async with maindb_driver.transaction() as txn:
+        model = SemanticModelMetadata(similarity_function=upb.VectorSimilarity.COSINE)
+        await KnowledgeBox.create(txn, kbslug, model, uuid=kbid)
+        await txn.commit()
+
     yield kbid
-    txn = await redis_driver.begin()
-    await KnowledgeBox.delete_kb(txn, kbslug, kbid)
-    await txn.commit()
+
+    async with maindb_driver.transaction() as txn:
+        await KnowledgeBox.delete_kb(txn, kbslug, kbid)
+        await txn.commit()
 
 
 @pytest.fixture(scope="function")
 async def audit():
     return BasicAuditStorage()
 
 
@@ -328,77 +228,74 @@
     )
     await audit.initialize()
     yield audit
     await audit.finalize()
 
 
 @pytest.fixture(scope="function")
-async def indexing_utility_ingest(natsd):
-    nc = await nats.connect(servers=[natsd])
-    js = nc.jetstream()
-    try:
-        await js.delete_consumer("node", "node-1")
-    except nats.js.errors.NotFoundError:
-        pass
-
-    try:
-        await js.delete_stream(name="node")
-    except nats.js.errors.NotFoundError:
-        pass
-
-    await js.add_stream(name="node", subjects=["node.*"])
-    indexing_settings.index_jetstream_servers = [natsd]
-    await nc.drain()
-    await nc.close()
+async def indexing_utility(natsd, _clean_natsd):
+    indexing_utility = IndexingUtility(
+        nats_creds=indexing_settings.index_jetstream_auth,
+        nats_servers=indexing_settings.index_jetstream_servers,
+        dummy=True,
+    )
+    await indexing_utility.initialize()
+    set_utility(Utility.INDEXING, indexing_utility)
 
     yield
 
+    clean_utility(Utility.INDEXING)
+    await indexing_utility.finalize()
+
 
 @pytest.fixture(scope="function")
-async def _natsd_reset(natsd, event_loop):
+async def _clean_natsd(natsd):
     nc = await nats.connect(servers=[natsd])
     js = nc.jetstream()
-    try:
-        await js.delete_consumer(
-            const.Streams.INGEST.name,
-            const.Streams.INGEST.group.format(partition="1"),
-        )
-    except nats.js.errors.NotFoundError:
-        pass
-    try:
-        await js.delete_consumer(
-            const.Streams.INGEST_PROCESSED.name,
-            const.Streams.INGEST_PROCESSED.group,
-        )
-    except nats.js.errors.NotFoundError:
-        pass
 
-    try:
-        await js.delete_stream(name="nucliadb")
-    except nats.js.errors.NotFoundError:
-        pass
-
-    await js.add_stream(
-        name="nucliadb",
-        subjects=[const.Streams.INGEST.subject.format(partition=">")],
-    )
+    consumers = [
+        (const.Streams.INGEST.name, const.Streams.INGEST.group.format(partition="1")),
+        (const.Streams.INGEST_PROCESSED.name, const.Streams.INGEST_PROCESSED.group),
+        (const.Streams.INDEX.name, const.Streams.INDEX.group.format(node="1")),
+    ]
+    for stream, consumer in consumers:
+        try:
+            await js.delete_consumer(stream, consumer)
+        except nats.js.errors.NotFoundError:
+            pass
+
+    streams = [
+        (const.Streams.INGEST.name, const.Streams.INGEST.subject.format(partition=">")),
+        (const.Streams.INDEX.name, const.Streams.INDEX.subject.format(node="*")),
+    ]
+    for stream, subject in streams:
+        try:
+            await js.delete_stream(stream)
+        except nats.js.errors.NotFoundError:
+            pass
+
+        await js.add_stream(name=stream, subjects=[subject])
+
     await nc.drain()
     await nc.close()
+
+    indexing_settings.index_jetstream_servers = [natsd]
+
     yield
 
 
 @pytest.fixture(scope="function")
 async def nats_manager(natsd):
     ncm = await start_nats_manager("service_name", [natsd], None)
     yield ncm
     await stop_nats_manager()
 
 
 @pytest.fixture(scope="function")
-async def transaction_utility(natsd):
+async def transaction_utility(natsd, pubsub):
     transaction_settings.transaction_jetstream_servers = [natsd]
     util = await start_transaction_utility()
     yield util
     await stop_transaction_utility()
 
 
 THUMBNAIL = rpb.CloudFile(
@@ -423,42 +320,49 @@
     md5="01cca3f53edb934a445a3112c6caa652",
 )
 
 
 # HELPERS
 
 
+async def make_field(field, extracted_text):
+    await field.set_extracted_text(make_extracted_text(field.id, body=extracted_text))
+    await field.set_field_metadata(make_field_metadata(field.id))
+    await field.set_large_field_metadata(make_field_large_metadata(field.id))
+    await field.set_vectors(make_extracted_vectors(field.id))
+
+
 def make_extracted_text(field_id, body: str):
     ex1 = rpb.ExtractedTextWrapper()
     ex1.field.CopyFrom(rpb.FieldID(field_type=rpb.FieldType.TEXT, field=field_id))
     ex1.body.text = body
     return ex1
 
 
 def make_field_metadata(field_id):
     ex1 = rpb.FieldComputedMetadataWrapper()
     ex1.field.CopyFrom(rpb.FieldID(field_type=rpb.FieldType.TEXT, field=field_id))
     ex1.metadata.metadata.links.append("https://nuclia.com")
 
     p1 = rpb.Paragraph(start=0, end=20)
-    p1.sentences.append(rpb.Sentence(start=0, end=10, key="test"))
-    p1.sentences.append(rpb.Sentence(start=11, end=20, key="test"))
+    p1.sentences.append(rpb.Sentence(start=0, end=20, key=""))
     cl1 = rpb.Classification(labelset="labelset1", label="label1")
-    p1.classifications.append(cl1)
+    cl2 = rpb.Classification(labelset="paragraph-labelset", label="label1")
+    p1.classifications.append(cl2)
     ex1.metadata.metadata.paragraphs.append(p1)
     ex1.metadata.metadata.classifications.append(cl1)
     # ex1.metadata.metadata.ner["Ramon"] = "PEOPLE"
     ex1.metadata.metadata.last_index.FromDatetime(datetime.now())
     ex1.metadata.metadata.last_understanding.FromDatetime(datetime.now())
     ex1.metadata.metadata.last_extract.FromDatetime(datetime.now())
     ex1.metadata.metadata.last_summary.FromDatetime(datetime.now())
     ex1.metadata.metadata.thumbnail.CopyFrom(THUMBNAIL)
     ex1.metadata.metadata.positions["ENTITY/document"].entity = "document"
     ex1.metadata.metadata.positions["ENTITY/document"].position.extend(
-        [rpb.Position(start=0, end=5), rpb.Position(start=23, end=28)]
+        [rpb.Position(start=0, end=5), rpb.Position(start=13, end=18)]
     )
     return ex1
 
 
 def make_field_large_metadata(field_id):
     ex1 = rpb.LargeComputedMetadataWrapper()
     ex1.field.CopyFrom(rpb.FieldID(field_type=rpb.FieldType.TEXT, field=field_id))
@@ -469,30 +373,27 @@
     ex1.real.metadata.tokens["tok"] = 3
     return ex1
 
 
 def make_extracted_vectors(field_id):
     ex1 = rpb.ExtractedVectorsWrapper()
     ex1.field.CopyFrom(rpb.FieldID(field_type=rpb.FieldType.TEXT, field=field_id))
-    v1 = rpb.Vector(start=1, end=2, vector=b"ansjkdn")
+    v1 = rpb.Vector(start=0, end=20, vector=b"ansjkdn")
     ex1.vectors.vectors.vectors.append(v1)
     return ex1
 
 
 @pytest.fixture(scope="function")
-async def test_resource(
-    gcs_storage, redis_driver, cache, knowledgebox_ingest, fake_node
-):
+async def test_resource(storage, maindb_driver, knowledgebox_ingest, fake_node):
     """
     Create a resource that has every possible bit of information
     """
     resource = await create_resource(
-        storage=gcs_storage,
-        driver=redis_driver,
-        cache=cache,
+        storage=storage,
+        driver=maindb_driver,
         knowledgebox_ingest=knowledgebox_ingest,
     )
     yield resource
     resource.clean()
 
 
 @pytest.fixture(scope="function")
@@ -611,15 +512,17 @@
     ev.vectors.vectors.vectors.append(v3)
 
     message1.field_vectors.append(ev)
     message1.source = BrokerMessage.MessageSource.WRITER
     return message1
 
 
-async def create_resource(storage, driver: Driver, cache, knowledgebox_ingest: str):
+async def create_resource(
+    storage: Storage, driver: Driver, knowledgebox_ingest: str
+) -> Resource:
     txn = await driver.begin()
 
     rid = str(uuid.uuid4())
     kb_obj = KnowledgeBox(txn, storage, kbid=knowledgebox_ingest)
     test_resource = await kb_obj.add_resource(uuid=rid, slug="slug")
     await test_resource.set_slug()
 
@@ -684,26 +587,39 @@
 
     await test_resource.set_origin(o2)
 
     # 2.  FIELDS
     #
     # Add an example of each of the files, containing all possible metadata
 
+    # Title
+    title_field = await test_resource.get_field(
+        "title", rpb.FieldType.GENERIC, load=False
+    )
+    await make_field(title_field, "MyText")
+
+    # Summary
+    summary_field = await test_resource.get_field(
+        "summary", rpb.FieldType.GENERIC, load=False
+    )
+    await make_field(summary_field, "MyText")
+
     # 2.1 FILE FIELD
 
     t2 = rpb.FieldFile(
         language="es",
     )
     t2.added.FromDatetime(datetime.now())
     t2.file.CopyFrom(TEST_CLOUDFILE)
 
-    await test_resource.set_field(rpb.FieldType.FILE, "file1", t2)
+    file_field = await test_resource.set_field(rpb.FieldType.FILE, "file1", t2)
+    await add_field_id(test_resource, file_field)
+    await make_field(file_field, "MyText")
 
     # 2.2 LINK FIELD
-
     li2 = rpb.FieldLink(
         uri="htts://nuclia.cloud",
         language="ca",
     )
     li2.added.FromDatetime(datetime.now())
     li2.headers["AUTHORIZATION"] = "Bearer xxxxx"
     linkfield = await test_resource.set_field(rpb.FieldType.LINK, "link1", li2)
@@ -714,56 +630,51 @@
     ex1.title = "My Title"
     ex1.field = "link1"
 
     ex1.link_preview.CopyFrom(THUMBNAIL)
     ex1.link_thumbnail.CopyFrom(THUMBNAIL)
 
     await linkfield.set_link_extracted_data(ex1)
-
-    await linkfield.set_extracted_text(make_extracted_text(linkfield.id, body="MyText"))
-    await linkfield.set_field_metadata(make_field_metadata(linkfield.id))
-    await linkfield.set_large_field_metadata(make_field_large_metadata(linkfield.id))
-    await linkfield.set_vectors(make_extracted_vectors(linkfield.id))
+    await add_field_id(test_resource, linkfield)
+    await make_field(linkfield, "MyText")
 
     # 2.3 TEXT FIELDS
 
     t23 = rpb.FieldText(body="This is my text field", format=rpb.FieldText.Format.PLAIN)
     textfield = await test_resource.set_field(rpb.FieldType.TEXT, "text1", t23)
-
-    await textfield.set_extracted_text(make_extracted_text(textfield.id, body="MyText"))
-    await textfield.set_field_metadata(make_field_metadata(textfield.id))
-    await textfield.set_large_field_metadata(make_field_large_metadata(textfield.id))
-    await textfield.set_vectors(make_extracted_vectors(textfield.id))
+    await add_field_id(test_resource, textfield)
+    await make_field(textfield, "MyText")
 
     # 2.4 LAYOUT FIELD
 
     l2 = rpb.FieldLayout(format=rpb.FieldLayout.Format.NUCLIAv1)
     l2.body.blocks["field1"].x = 0
     l2.body.blocks["field1"].y = 0
     l2.body.blocks["field1"].cols = 1
     l2.body.blocks["field1"].rows = 1
     l2.body.blocks["field1"].type = rpb.Block.TypeBlock.TITLE
     l2.body.blocks["field1"].payload = "{}"
     l2.body.blocks["field1"].file.CopyFrom(TEST_CLOUDFILE)
 
     layoutfield = await test_resource.set_field(rpb.FieldType.LAYOUT, "layout1", l2)
+    await add_field_id(test_resource, layoutfield)
 
     await layoutfield.set_extracted_text(
         make_extracted_text(layoutfield.id, body="MyText")
     )
     await layoutfield.set_field_metadata(make_field_metadata(layoutfield.id))
     await layoutfield.set_large_field_metadata(
         make_field_large_metadata(layoutfield.id)
     )
     await layoutfield.set_vectors(make_extracted_vectors(layoutfield.id))
 
     # 2.5 CONVERSATION FIELD
 
     def make_message(
-        text: str, files: Optional[List[rpb.CloudFile]] = None
+        text: str, files: Optional[list[rpb.CloudFile]] = None
     ) -> rpb.Message:
         msg = rpb.Message(
             who="myself",
         )
         msg.timestamp.FromDatetime(datetime.now())
         msg.content.text = text
         msg.content.format = rpb.MessageContent.Format.PLAIN
@@ -778,62 +689,76 @@
     for i in range(300):
         new_message = make_message(f"{i} hello")
         if i == 33:
             new_message = make_message(f"{i} hello", files=[TEST_CLOUDFILE, THUMBNAIL])
         c2.messages.append(new_message)
 
     convfield = await test_resource.set_field(rpb.FieldType.CONVERSATION, "conv1", c2)
-    await convfield.set_extracted_text(make_extracted_text(convfield.id, body="MyText"))
+    await add_field_id(test_resource, convfield)
+    await make_field(convfield, extracted_text="MyText")
 
     # 2.6 KEYWORDSET FIELD
 
     k2 = rpb.FieldKeywordset(
         keywords=[rpb.Keyword(value="kw1"), rpb.Keyword(value="kw2")]
     )
-    await test_resource.set_field(rpb.FieldType.KEYWORDSET, "keywordset1", k2)
+    kws_field = await test_resource.set_field(
+        rpb.FieldType.KEYWORDSET, "keywordset1", k2
+    )
+    await add_field_id(test_resource, kws_field)
+    await make_field(kws_field, "MyText")
 
     # 2.7 DATETIMES FIELD
 
     d2 = rpb.FieldDatetime()
     d2.value.FromDatetime(datetime.now())
-    await test_resource.set_field(rpb.FieldType.DATETIME, "datetime1", d2)
-
-    # 3 USER VECTORS
+    datetime_field = await test_resource.set_field(
+        rpb.FieldType.DATETIME, "datetime1", d2
+    )
+    await add_field_id(test_resource, datetime_field)
+    await make_field(datetime_field, "MyText")
 
     field_obj = await test_resource.get_field("datetime1", type=rpb.FieldType.DATETIME)
-    user_vectors = rpb.UserVectorsWrapper()
-    user_vectors.vectors.vectors["vectorset1"].vectors["vector1"].vector.extend(
-        (0.1, 0.2, 0.3)
-    )
-    await field_obj.set_user_vectors(user_vectors)
+
+    # Q/A
+    question_answers = rpb.FieldQuestionAnswerWrapper()
+    for i in range(10):
+        qa = rpb.QuestionAnswer()
+
+        qa.question.text = f"My question {i}"
+        qa.question.language = "catalan"
+        qa.question.ids_paragraphs.extend([f"id1/{i}", f"id2/{i}"])
+
+        answer = rpb.Answers()
+        answer.text = f"My answer {i}"
+        answer.language = "catalan"
+        answer.ids_paragraphs.extend([f"id1/{i}", f"id2/{i}"])
+        qa.answers.append(answer)
+        question_answers.question_answers.question_answer.append(qa)
+
+    await field_obj.set_question_answers(question_answers)
 
     await txn.commit()
     return test_resource
 
 
-@pytest.fixture(scope="function")
-def metrics_registry():
-    import prometheus_client.registry  # type: ignore
-
-    for collector in prometheus_client.registry.REGISTRY._names_to_collectors.values():
-        if not hasattr(collector, "_metrics"):
-            continue
-        collector._metrics.clear()
-    yield prometheus_client.registry.REGISTRY
+async def add_field_id(resource: Resource, field: Field):
+    field_type = KB_REVERSE[field.type]
+    field_id = rpb.FieldID(field_type=field_type, field=field.id)
+    await resource.update_all_field_ids(updated=[field_id])
 
 
 @pytest.fixture(scope="function")
 async def entities_manager_mock():
     """EntitiesManager mock for ingest gRPC API disabling indexed entities
     functionality. As tests doesn't startup a node, with this mock we allow
     testing ingest's gRPC API while the whole entities functionality is properly
     tested in tests nos using this fixture.
 
     """
     klass = "nucliadb.ingest.service.writer.EntitiesManager"
     with patch(f"{klass}.get_indexed_entities_group", AsyncMock(return_value=None)):
-        with patch(f"{klass}.index_entities_group", AsyncMock(return_value=None)):
-            with patch(
-                f"nucliadb.ingest.orm.entities.NodesManager.apply_for_all_shards",
-                AsyncMock(return_value=[]),
-            ):
-                yield
+        with patch(
+            "nucliadb.common.cluster.manager.KBShardManager.apply_for_all_shards",
+            AsyncMock(return_value=[]),
+        ):
+            yield
```

## nucliadb/ingest/tests/integration/consumer/test_pull.py

```diff
@@ -57,43 +57,45 @@
 
 
 @pytest.fixture()
 async def pull_processor_api():
     app = FastAPI()
     messages: list[BrokerMessage] = []  # type: ignore
 
-    @app.get("/api/internal/processing/pull")
+    @app.get("/api/v1/internal/processing/pull")
     async def pull():
         if len(messages) == 0:
             return {"status": "empty"}
         message = messages.pop()
         return {
             "status": "ok",
             "payload": base64.b64encode(message.SerializeToString()).decode(),
+            "msgid": len(messages),
         }
 
     port = free_port()
     config = Config(app, host="0.0.0.0", port=port, http="auto")
     server = Server(config=config)
 
     await start_server(server, config)
 
     url = f"http://127.0.0.1:{port}"
     with patch(
-        "nucliadb.http_clients.processing.nuclia_settings.nuclia_cluster_url", url
+        "nucliadb.common.http_clients.processing.nuclia_settings.nuclia_processing_cluster_url",
+        url,
     ):
         yield PullProcessorAPI(url=url, messages=messages)
 
     await server.shutdown()
 
 
 @pytest.fixture()
-async def pull_worker(redis_driver, pull_processor_api: PullProcessorAPI):
+async def pull_worker(maindb_driver, pull_processor_api: PullProcessorAPI):
     worker = PullWorker(
-        driver=redis_driver,
+        driver=maindb_driver,
         partition="1",
         storage=None,  # type: ignore
         pull_time_error_backoff=5,
         pull_time_empty_backoff=0.1,
     )
 
     task = asyncio.create_task(worker.loop())
@@ -132,26 +134,11 @@
     assert consumer_info2.delivered.stream_seq == 0
 
     # add message that should go to first consumer
     pull_processor_api.messages.append(create_broker_message(knowledgebox_ingest))
     await wait_for_messages(pull_processor_api.messages)
 
     consumer_info1 = await nats_manager.js.consumer_info(
-        const.Streams.INGEST.name, const.Streams.INGEST.group.format(partition="1")
+        const.Streams.INGEST.name, const.Streams.INGEST_PROCESSED.group
     )
 
     assert consumer_info1.delivered.stream_seq == 1
-
-    # check next message goes to other consumer when ff is enabled
-    # remove this type of test when ff removed
-    with patch("nucliadb.ingest.consumer.pull.has_feature", return_value=True):
-        pull_processor_api.messages.append(create_broker_message(knowledgebox_ingest))
-        await wait_for_messages(pull_processor_api.messages)
-
-    consumer_info1 = await nats_manager.js.consumer_info(
-        const.Streams.INGEST.name, const.Streams.INGEST.group.format(partition="1")
-    )
-    consumer_info2 = await nats_manager.js.consumer_info(
-        const.Streams.INGEST_PROCESSED.name, const.Streams.INGEST_PROCESSED.group
-    )
-    assert consumer_info1.delivered.stream_seq == 1  # still on first
-    assert consumer_info2.delivered.stream_seq == 2
```

## nucliadb/ingest/tests/integration/consumer/test_service.py

```diff
@@ -41,14 +41,15 @@
 
 async def test_separated_ingest_consumer(
     ingest_consumers,
     ingest_processed_consumer,
     knowledgebox_ingest,
     transaction_utility: TransactionUtility,
     nats_manager: NatsConnectionManager,
+    pubsub,
 ):
     bm_normal = create_broker_message(knowledgebox_ingest)
     bm_processed = create_broker_message(knowledgebox_ingest)
     bm_processed.source == BrokerMessage.MessageSource.PROCESSOR
 
     await transaction_utility.commit(bm_normal, partition=1, wait=True)
```

## nucliadb/ingest/tests/integration/consumer/test_shard_creator.py

```diff
@@ -28,31 +28,31 @@
 
 pytestmark = pytest.mark.asyncio
 
 
 async def test_shard_auto_create(
     maindb_driver,
     pubsub,
-    gcs_storage,
+    storage,
     fake_node,
     knowledgebox_ingest,
 ):
-    from nucliadb.ingest.settings import settings
+    from nucliadb.common.cluster.settings import settings
 
-    settings.max_shard_fields = 1
+    settings.max_shard_paragraphs = 1
 
     sc = shard_creator.ShardCreatorHandler(
         driver=maindb_driver,
-        storage=gcs_storage,
+        storage=storage,
         pubsub=pubsub,
         check_delay=0.05,
     )
     await sc.initialize()
 
-    original_kb_shards = await sc.node_manager.get_shards_by_kbid_inner(
+    original_kb_shards = await sc.shard_manager.get_shards_by_kbid_inner(
         knowledgebox_ingest
     )
 
     await pubsub.publish(
         const.PubSubChannels.RESOURCE_NOTIFY.format(kbid=knowledgebox_ingest),
         writer_pb2.Notification(
             kbid=knowledgebox_ingest,
@@ -60,9 +60,9 @@
         ).SerializeToString(),
     )
 
     await asyncio.sleep(0.2)
 
     await sc.finalize()
 
-    kb_shards = await sc.node_manager.get_shards_by_kbid_inner(knowledgebox_ingest)
+    kb_shards = await sc.shard_manager.get_shards_by_kbid_inner(knowledgebox_ingest)
     assert len(kb_shards.shards) == len(original_kb_shards.shards) + 1
```

## nucliadb/ingest/tests/integration/ingest/test_ingest.py

```diff
@@ -14,44 +14,46 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import base64
-import traceback
 import uuid
 from datetime import datetime
 from os.path import dirname, getsize
 from unittest.mock import patch
 from uuid import uuid4
 
 import nats
 import pytest
 from nats.aio.client import Client
 from nats.js import JetStreamContext
 from nucliadb_protos.audit_pb2 import AuditField, AuditRequest
 from nucliadb_protos.resources_pb2 import (
     TEXT,
+    Answers,
     Classification,
     CloudFile,
     Entity,
     ExtractedTextWrapper,
     ExtractedVectorsWrapper,
     FieldComputedMetadataWrapper,
     FieldID,
+    FieldQuestionAnswerWrapper,
     FieldType,
     FileExtractedData,
     LargeComputedMetadataWrapper,
 )
 from nucliadb_protos.resources_pb2 import Metadata as PBMetadata
-from nucliadb_protos.resources_pb2 import Origin, Paragraph
+from nucliadb_protos.resources_pb2 import Origin, Paragraph, QuestionAnswer
 from nucliadb_protos.utils_pb2 import Vector
 from nucliadb_protos.writer_pb2 import BrokerMessage
 
+from nucliadb.common import datamanagers
 from nucliadb.ingest import SERVICE_NAME
 from nucliadb.ingest.consumer.auditing import (
     IndexAuditHandler,
     ResourceWritesAuditHandler,
 )
 from nucliadb.ingest.orm.exceptions import DeadletteredError
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
@@ -64,39 +66,39 @@
 EXAMPLE_VECTOR = base64.b64decode(
     "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"  # noqa
 )
 
 
 @pytest.fixture(autouse=True)
 async def audit_consumers(
-    maindb_driver, gcs_storage, cache, stream_audit: StreamAuditStorage
+    maindb_driver, storage, pubsub, stream_audit: StreamAuditStorage
 ):
     index_auditor = IndexAuditHandler(
         driver=maindb_driver,
         audit=stream_audit,
-        pubsub=cache.pubsub,
+        pubsub=pubsub,
     )
     resource_writes_auditor = ResourceWritesAuditHandler(
         driver=maindb_driver,
-        storage=gcs_storage,
+        storage=storage,
         audit=stream_audit,
-        pubsub=cache.pubsub,
+        pubsub=pubsub,
     )
 
     await index_auditor.initialize()
     await resource_writes_auditor.initialize()
     yield
     await index_auditor.finalize()
     await resource_writes_auditor.finalize()
 
 
 @pytest.fixture()
 def kbid(
     local_files,
-    gcs_storage: Storage,
+    storage: Storage,
     txn,
     cache,
     fake_node,
     processor,
     knowledgebox_ingest,
 ):
     yield knowledgebox_ingest
@@ -199,96 +201,98 @@
 
     pb = await storage.get_indexing(index._calls[1][1])
     assert pb.texts["a/summary"].text == "My summary"  # type: ignore
 
 
 @pytest.mark.asyncio
 async def test_ingest_error_message(
-    kbid: str, gcs_storage: Storage, txn, cache, processor
+    kbid: str, storage: Storage, processor, maindb_driver
 ):
     filename = f"{dirname(__file__)}/assets/resource.pb"
     with open(filename, "r") as f:
         data = base64.b64decode(f.read())
     message0: BrokerMessage = BrokerMessage()
     message0.ParseFromString(data)
     message0.kbid = kbid
     message0.source = BrokerMessage.MessageSource.WRITER
+
     await processor.process(message=message0, seqid=1)
 
     filename = f"{dirname(__file__)}/assets/error.pb"
     with open(filename, "r") as f:
         data = base64.b64decode(f.read())
     message1: BrokerMessage = BrokerMessage()
     message1.ParseFromString(data)
     message1.kbid = kbid
     message1.ClearField("field_vectors")
     message1.source = BrokerMessage.MessageSource.WRITER
-    await processor.process(message=message1, seqid=2)
-
-    kb_obj = KnowledgeBox(txn, gcs_storage, kbid=kbid)
-    r = await kb_obj.get(message1.uuid)
-    assert r is not None
-    field_obj = await r.get_field("wikipedia_ml", TEXT)
-    ext1 = await field_obj.get_extracted_text()
-    lfm1 = await field_obj.get_large_field_metadata()
-    fm1 = await field_obj.get_field_metadata()
-    basic = await r.get_basic()
-    assert basic is not None
-    assert basic.slug == message1.slug
-    assert basic.summary == message0.basic.summary
 
-    assert ext1.text == message1.extracted_text[0].body.text
+    await processor.process(message=message1, seqid=2)
 
-    assert lfm1 is not None
-    assert fm1 is not None
-    assert field_obj.value.body == message0.texts["wikipedia_ml"].body
+    async with maindb_driver.transaction() as txn:
+        kb_obj = KnowledgeBox(txn, storage, kbid=kbid)
+        r = await kb_obj.get(message1.uuid)
+        assert r is not None
+        field_obj = await r.get_field("wikipedia_ml", TEXT)
+        ext1 = await field_obj.get_extracted_text()
+        lfm1 = await field_obj.get_large_field_metadata()
+        fm1 = await field_obj.get_field_metadata()
+        basic = await r.get_basic()
+        assert basic is not None
+        assert basic.slug == message1.slug
+        assert basic.summary == message0.basic.summary
+
+        assert ext1.text == message1.extracted_text[0].body.text
+
+        assert lfm1 is not None
+        assert fm1 is not None
+        assert field_obj.value.body == message0.texts["wikipedia_ml"].body
 
 
 @pytest.mark.asyncio
 async def test_ingest_messages_origin(
     local_files,
-    gcs_storage: Storage,
-    txn,
-    cache,
+    storage: Storage,
     fake_node,
     processor,
     knowledgebox_ingest,
 ):
     rid = "43ece3e4-b706-4c74-b41b-3637f6d28197"
     message1: BrokerMessage = BrokerMessage(
         kbid=knowledgebox_ingest,
         uuid=rid,
         slug="slug1",
         type=BrokerMessage.AUTOCOMMIT,
     )
     message1.source = BrokerMessage.MessageSource.WRITER
     await processor.process(message=message1, seqid=1)
 
-    storage = await get_storage(service_name=SERVICE_NAME)
-    kb = KnowledgeBox(txn, storage, knowledgebox_ingest)
-    res = Resource(txn, storage, kb, rid)
-    origin = await res.get_origin()
+    async with processor.driver.transaction() as txn:
+        storage = await get_storage(service_name=SERVICE_NAME)
+        kb = KnowledgeBox(txn, storage, knowledgebox_ingest)
+        res = Resource(txn, storage, kb, rid)
+        origin = await res.get_origin()
 
     # should not be set
     assert origin is None
 
     # now set the origin
     message1.origin.CopyFrom(
         Origin(
             source=Origin.Source.API,
             filename="file.png",
             url="http://www.google.com",
         )
     )
     await processor.process(message=message1, seqid=2)
 
-    await txn.abort()  # force clearing txn cache from last pull
-    kb = KnowledgeBox(txn, storage, knowledgebox_ingest)
-    res = Resource(txn, storage, kb, rid)
-    origin = await res.get_origin()
+    async with processor.driver.transaction() as txn:
+        kb = KnowledgeBox(txn, storage, knowledgebox_ingest)
+        res = Resource(txn, storage, kb, rid)
+        origin = await res.get_origin()
 
     assert origin is not None
     assert origin.url == "http://www.google.com"
     assert origin.source == Origin.Source.API
     assert origin.filename == "file.png"
 
 
@@ -341,22 +345,22 @@
     auditreq.ParseFromString(msg[0].data)
     return auditreq
 
 
 @pytest.mark.asyncio
 async def test_ingest_audit_stream_files_only(
     local_files,
-    gcs_storage: Storage,
+    storage: Storage,
     txn,
     cache,
     fake_node,
     knowledgebox_ingest,
     stream_processor,
     stream_audit: StreamAuditStorage,
-    redis_driver,
+    maindb_driver,
 ):
     from nucliadb_utils.settings import audit_settings
 
     # Prepare a test audit stream to receive our messages
     partition = stream_audit.get_partition(knowledgebox_ingest)
     client: Client = await nats.connect(stream_audit.nats_servers)
     jetstream: JetStreamContext = client.jetstream()
@@ -469,48 +473,101 @@
     await stream_processor.process(message=message, seqid=4)
     auditreq = await get_audit_messages(psub)
 
     assert auditreq.type == AuditRequest.AuditType.DELETED
 
     # Test 5: Delete knowledgebox
 
-    txn = await redis_driver.begin()
-    kb = await KnowledgeBox.get_kb(txn, knowledgebox_ingest)
+    txn = await maindb_driver.begin()
+    kb = await datamanagers.kb.get_config(txn, kbid=knowledgebox_ingest)
 
     set_utility(Utility.AUDIT, stream_audit)
     await KnowledgeBox.delete_kb(txn, kb.slug, knowledgebox_ingest)  # type: ignore
 
     auditreq = await get_audit_messages(psub)
     assert auditreq.kbid == knowledgebox_ingest
     assert auditreq.type == AuditRequest.AuditType.KB_DELETED
 
     try:
         int(auditreq.trace_id)
     except ValueError:
         assert False, "Invalid trace ID"
 
     # Currently where not updating audit counters on delete operations
-    assert not auditreq.HasField("counter")
+    assert not auditreq.HasField("kb_counter")
 
     await txn.abort()
 
     await client.drain()
     await client.close()
 
 
 @pytest.mark.asyncio
+async def test_qa(
+    local_files,
+    storage: Storage,
+    cache,
+    fake_node,
+    stream_processor,
+    stream_audit: StreamAuditStorage,
+    test_resource: Resource,
+):
+    kbid = test_resource.kb.kbid
+    rid = test_resource.uuid
+    driver = stream_processor.driver
+    message = make_message(kbid, rid)
+    message.account_seq = 2
+    message.files["qa"].file.uri = "http://something"
+    message.files["qa"].file.size = 123
+    message.files["qa"].file.source = CloudFile.Source.LOCAL
+
+    qaw = FieldQuestionAnswerWrapper()
+    qaw.field.field_type = FieldType.FILE
+    qaw.field.field = "qa"
+
+    for i in range(10):
+        qa = QuestionAnswer()
+
+        qa.question.text = f"My question {i}"
+        qa.question.language = "catalan"
+        qa.question.ids_paragraphs.extend([f"id1/{i}", f"id2/{i}"])
+
+        answer = Answers()
+        answer.text = f"My answer {i}"
+        answer.language = "catalan"
+        answer.ids_paragraphs.extend([f"id1/{i}", f"id2/{i}"])
+        qa.answers.append(answer)
+        qaw.question_answers.question_answer.append(qa)
+
+    message.question_answers.append(qaw)
+
+    await stream_processor.process(message=message, seqid=1)
+
+    async with driver.transaction() as txn:
+        kb_obj = KnowledgeBox(txn, storage, kbid=kbid)
+        r = await kb_obj.get(message.uuid)
+        assert r is not None
+        res = await r.get_field(key="qa", type=FieldType.FILE)
+        res_qa = await res.get_question_answers()
+
+    assert qaw.question_answers == res_qa
+
+    # delete op
+    message = make_message(kbid, rid, message_type=BrokerMessage.MessageType.DELETE)
+    await stream_processor.process(message=message, seqid=2)
+
+
+@pytest.mark.asyncio
 async def test_ingest_audit_stream_mixed(
     local_files,
-    gcs_storage: Storage,
-    txn,
+    storage: Storage,
     cache,
     fake_node,
     stream_processor,
     stream_audit: StreamAuditStorage,
-    redis_driver,
     test_resource: Resource,
 ):
     from nucliadb_utils.settings import audit_settings
 
     kbid = test_resource.kb.kbid
     rid = test_resource.uuid
     # Prepare a test audit stream to receive our messages
@@ -561,88 +618,74 @@
 
     message = make_message(kbid, rid, message_type=BrokerMessage.MessageType.DELETE)
     await stream_processor.process(message=message, seqid=2)
     auditreq = await get_audit_messages(psub)
 
     assert auditreq.type == AuditRequest.AuditType.DELETED
 
-    await txn.abort()
-
     await client.drain()
     await client.close()
 
 
 @pytest.mark.asyncio
 async def test_ingest_account_seq_stored(
     local_files,
-    gcs_storage: Storage,
-    txn,
-    cache,
+    storage: Storage,
     fake_node,
     stream_processor,
-    redis_driver,
     test_resource: Resource,
 ):
+    driver = stream_processor.driver
     kbid = test_resource.kb.kbid
     rid = test_resource.uuid
 
     message = make_message(kbid, rid)
     message.account_seq = 2
     add_filefields(message, [("file_1", "file.png")])
     await stream_processor.process(message=message, seqid=1)
 
-    kb_obj = KnowledgeBox(txn, gcs_storage, kbid=kbid)
-    r = await kb_obj.get(message.uuid)
-    assert r is not None
+    async with driver.transaction() as txn:
+        kb_obj = KnowledgeBox(txn, storage, kbid=kbid)
+        r = await kb_obj.get(message.uuid)
+        assert r is not None
+        basic = await r.get_basic()
 
-    basic = await r.get_basic()
     assert basic is not None
     assert basic.last_account_seq == 2
     assert basic.queue == 0
 
-    await txn.abort()
-
 
 @pytest.mark.asyncio
-async def test_ingest_txn_missing_kb(
+async def test_ingest_processor_handles_missing_kb(
     local_files,
-    gcs_storage: Storage,
-    txn,
-    cache,
+    storage: Storage,
     fake_node,
     stream_processor,
-    redis_driver,
     test_resource: Resource,
 ):
     kbid = str(uuid4())
     rid = str(uuid4())
     message = make_message(kbid, rid)
     message.account_seq = 1
-    try:
-        await stream_processor.process(message=message, seqid=1)
-    except Exception:
-        assert (
-            False
-        ), f"Processing should not fail due to a missing Knowledgebox:\n\n{str(traceback.format_exc())}"
-
-    await txn.abort()
+    await stream_processor.process(message=message, seqid=1)
 
 
 @pytest.mark.asyncio
 async def test_ingest_autocommit_deadletter_marks_resource(
-    kbid: str, processor: Processor, txn, gcs_storage, cache
+    kbid: str, processor: Processor, storage, maindb_driver
 ):
     rid = str(uuid.uuid4())
     message = make_message(kbid, rid)
 
     with patch.object(processor, "notify_commit") as mock_notify, pytest.raises(
         DeadletteredError
     ):
         # cause an error to force deadletter handling
         mock_notify.side_effect = Exception("test")
         await processor.process(message=message, seqid=1)
 
-    kb_obj = KnowledgeBox(txn, gcs_storage, kbid=kbid)
-    resource = await kb_obj.get(message.uuid)
+    async with maindb_driver.transaction() as txn:
+        kb_obj = KnowledgeBox(txn, storage, kbid=kbid)
+        resource = await kb_obj.get(message.uuid)
 
     mock_notify.assert_called_once()
     assert resource.basic.metadata.status == PBMetadata.Status.ERROR  # type: ignore
```

## nucliadb/ingest/tests/integration/ingest/test_processing_engine.py

```diff
@@ -17,15 +17,16 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
 from uuid import uuid4
 
 import pytest
-from aioresponses import aioresponses
+
+from nucliadb.tests.utils.aiohttp_session import get_mocked_session
 
 
 @pytest.mark.parametrize("onprem", [True, False])
 @pytest.mark.parametrize(
     "mock_payload",
     [
         {"seqid": 1, "account_seq": 1, "queue": "private"},
@@ -43,28 +44,52 @@
     """
 
     from nucliadb.ingest.processing import ProcessingEngine, PushPayload
 
     fake_nuclia_proxy_url = "http://fake_proxy"
     processing_engine = ProcessingEngine(
         onprem=onprem,
-        nuclia_cluster_url=fake_nuclia_proxy_url,
+        nuclia_processing_cluster_url=fake_nuclia_proxy_url,
         nuclia_public_url=fake_nuclia_proxy_url,
     )
     await processing_engine.initialize()
 
     payload = PushPayload(
         uuid=str(uuid4()), kbid=str(uuid4()), userid=str(uuid4()), partition=0
     )
 
-    with aioresponses() as m:
-        m.post(
-            f"{fake_nuclia_proxy_url}/api/internal/processing/push",
-            payload=mock_payload,
-        )
-        m.post(
-            f"{fake_nuclia_proxy_url}/api/v1/processing/push?partition=0",
-            payload=mock_payload,
-        )
+    processing_engine.session = get_mocked_session(
+        "POST", 200, json=mock_payload, context_manager=False
+    )
+    await processing_engine.send_to_process(payload, partition=0)
+
+    await processing_engine.finalize()
+
+
+@pytest.mark.parametrize("onprem", [True, False])
+@pytest.mark.asyncio
+async def test_delete_from_processing(onprem):
+    """
+    Test that send_to_process does not fail
+    """
+
+    from nucliadb.ingest.processing import ProcessingEngine
+
+    fake_nuclia_proxy_url = "http://fake_proxy"
+    processing_engine = ProcessingEngine(
+        onprem=onprem,
+        nuclia_processing_cluster_url=fake_nuclia_proxy_url,
+        nuclia_public_url=fake_nuclia_proxy_url,
+    )
+    await processing_engine.initialize()
+
+    processing_engine.session = get_mocked_session(
+        "POST",
+        200,
+        json={"kbid": "kbid", "resource_id": "resource_id"},
+        context_manager=False,
+    )
+    await processing_engine.delete_from_processing(
+        kbid="kbid", resource_id="resource_id"
+    )
 
-        await processing_engine.send_to_process(payload, partition=0)
     await processing_engine.finalize()
```

## nucliadb/ingest/tests/integration/ingest/test_relations.py

```diff
@@ -32,15 +32,15 @@
 
 from nucliadb.ingest import SERVICE_NAME
 from nucliadb_utils.utilities import get_indexing, get_storage
 
 
 @pytest.mark.asyncio
 async def test_ingest_relations_indexing(
-    fake_node, local_files, gcs_storage, knowledgebox_ingest, processor
+    fake_node, local_files, storage, knowledgebox_ingest, processor
 ):
     rid = str(uuid.uuid4())
     bm = BrokerMessage(
         kbid=knowledgebox_ingest, uuid=rid, slug="slug-1", type=BrokerMessage.AUTOCOMMIT
     )
 
     e0 = RelationNode(value="E0", ntype=RelationNode.NodeType.ENTITY, subtype="")
@@ -73,15 +73,15 @@
     assert pb.relations[0] == r0
     assert pb.relations[1] == r1
     assert pb.relations[2] == r2
 
 
 @pytest.mark.asyncio
 async def test_ingest_label_relation_extraction(
-    fake_node, local_files, gcs_storage, knowledgebox_ingest, processor
+    fake_node, local_files, storage, knowledgebox_ingest, processor
 ):
     rid = str(uuid.uuid4())
     bm = BrokerMessage(
         kbid=knowledgebox_ingest, uuid=rid, slug="slug-1", type=BrokerMessage.AUTOCOMMIT
     )
 
     labels = [
@@ -105,15 +105,15 @@
         assert pb.relations[i].relation == Relation.RelationType.ABOUT
         assert pb.relations[i].source.value == rid
         assert pb.relations[i].to.value == f"{labelset}/{label}"
 
 
 @pytest.mark.asyncio
 async def test_ingest_colab_relation_extraction(
-    fake_node, local_files, gcs_storage, knowledgebox_ingest, processor
+    fake_node, local_files, storage, knowledgebox_ingest, processor
 ):
     rid = str(uuid.uuid4())
     bm = BrokerMessage(
         kbid=knowledgebox_ingest, uuid=rid, slug="slug-1", type=BrokerMessage.AUTOCOMMIT
     )
 
     collaborators = ["Alice", "Bob", "Trudy"]
@@ -130,15 +130,15 @@
         assert pb.relations[i].relation == Relation.RelationType.COLAB
         assert pb.relations[i].source.value == rid
         assert pb.relations[i].to.value == collaborator
 
 
 @pytest.mark.asyncio
 async def test_ingest_field_metadata_relation_extraction(
-    fake_node, local_files, gcs_storage, knowledgebox_ingest, processor
+    fake_node, local_files, storage, knowledgebox_ingest, processor
 ):
     rid = str(uuid.uuid4())
     bm = BrokerMessage(
         kbid=knowledgebox_ingest,
         uuid=rid,
         slug="slug-1",
         type=BrokerMessage.AUTOCOMMIT,
@@ -202,15 +202,15 @@
     ]
     for generated_relation in generated_relations:
         assert generated_relation in pb.relations
 
 
 @pytest.mark.asyncio
 async def test_ingest_field_relations_relation_extraction(
-    fake_node, local_files, gcs_storage, knowledgebox_ingest, processor
+    fake_node, local_files, storage, knowledgebox_ingest, processor
 ):
     rid = str(uuid.uuid4())
     bm = BrokerMessage(
         kbid=knowledgebox_ingest, uuid=rid, slug="slug-1", type=BrokerMessage.AUTOCOMMIT
     )
 
     relationnode = RelationNode(
```

## nucliadb/ingest/tests/unit/test_cache.py

```diff
@@ -13,16 +13,16 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+from nucliadb.common.cluster.index_node import READ_CONNECTIONS, WRITE_CONNECTIONS
 from nucliadb.ingest.cache import clear_ingest_cache
-from nucliadb.ingest.orm.node import READ_CONNECTIONS, WRITE_CONNECTIONS
 
 
 def test_clear_ingest_cache():
     READ_CONNECTIONS["addr1"] = "conn1"
     WRITE_CONNECTIONS["addr2"] = "conn2"
 
     clear_ingest_cache()
```

## nucliadb/ingest/tests/unit/test_processing.py

```diff
@@ -13,26 +13,27 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from unittest.mock import AsyncMock, Mock
+from unittest.mock import Mock
 
 import pytest
 from nucliadb_protos.resources_pb2 import CloudFile
 
 from nucliadb.ingest.processing import (
     DummyProcessingEngine,
     ProcessingEngine,
     PushPayload,
 )
+from nucliadb.tests.utils.aiohttp_session import get_mocked_session
 from nucliadb_models import File, FileField
-from nucliadb_utils.exceptions import LimitsExceededError
+from nucliadb_utils.exceptions import LimitsExceededError, SendToProcessError
 
 TEST_FILE = FileField(
     password="mypassword", file=File(filename="myfile.pdf", payload="")
 )
 
 TEST_CLOUD_FILE = CloudFile(
     uri="file.png",
@@ -51,44 +52,25 @@
     engine = DummyProcessingEngine()
     await engine.initialize()
     await engine.finalize()
     await engine.convert_filefield_to_str(None)
     engine.convert_external_filefield_to_str(None)
     await engine.convert_internal_filefield_to_str(None, None)
     await engine.convert_internal_cf_to_str(None, None)
-    await engine.send_to_process(None, 1)
+    await engine.send_to_process(Mock(kbid="foo"), 1)
 
 
 @pytest.fixture(scope="function")
 def engine():
-    return ProcessingEngine(
+    pe = ProcessingEngine(
         onprem=True,
-        nuclia_cluster_url="cluster_url",
+        nuclia_processing_cluster_url="cluster_url",
         nuclia_public_url="public_url",
     )
-
-
-def get_mocked_session(
-    http_method: str, status: int, text=None, json=None, context_manager=True
-):
-    response = Mock(status=status)
-    if text:
-        response.text = AsyncMock(return_value=text)
-    if json:
-        response.json = AsyncMock(return_value=json)
-    if context_manager:
-        # For when async with self.session.post() as response: is called
-        session = Mock()
-        http_method_mock = AsyncMock(__aenter__=AsyncMock(return_value=response))
-        getattr(session, http_method.lower()).return_value = http_method_mock
-    else:
-        # For when await self.session.post() is called
-        session = AsyncMock()
-        getattr(session, http_method.lower()).return_value = response
-    return session
+    yield pe
 
 
 async def test_convert_filefield_to_str_200(engine):
     engine.session = get_mocked_session("POST", 200, text="jwt")
 
     assert await engine.convert_filefield_to_str(TEST_FILE) == "jwt"
 
@@ -97,14 +79,22 @@
     engine.session = get_mocked_session("POST", 402, json={"detail": "limits exceeded"})
 
     with pytest.raises(LimitsExceededError) as exc:
         await engine.convert_filefield_to_str(TEST_FILE)
     assert exc.value.status_code == 402
 
 
+async def test_convert_filefield_to_str_429(engine):
+    engine.session = get_mocked_session("POST", 429, json={"detail": "limits exceeded"})
+
+    with pytest.raises(LimitsExceededError) as exc:
+        await engine.convert_filefield_to_str(TEST_FILE)
+    assert exc.value.status_code == 429
+
+
 async def test_convert_filefield_to_str_500(engine):
     engine.session = get_mocked_session("POST", 500, text="error")
 
     with pytest.raises(Exception) as exc:
         await engine.convert_filefield_to_str(TEST_FILE)
     assert str(exc.value) == "STATUS: 500 - error"
 
@@ -119,14 +109,22 @@
     engine.session = get_mocked_session("POST", 402, json={"detail": "limits exceeded"})
 
     with pytest.raises(LimitsExceededError) as exc:
         await engine.convert_internal_cf_to_str(TEST_CLOUD_FILE, Mock())
     assert exc.value.status_code == 402
 
 
+async def test_convert_internal_cf_to_str_429(engine):
+    engine.session = get_mocked_session("POST", 429, json={"detail": "limits exceeded"})
+
+    with pytest.raises(LimitsExceededError) as exc:
+        await engine.convert_internal_cf_to_str(TEST_CLOUD_FILE, Mock())
+    assert exc.value.status_code == 429
+
+
 async def test_convert_internal_cf_to_str_500(engine):
     engine.session = get_mocked_session("POST", 500, text="error")
 
     with pytest.raises(Exception) as exc:
         await engine.convert_internal_cf_to_str(TEST_CLOUD_FILE, Mock())
     assert str(exc.value) == "STATUS: 500 - error"
 
@@ -150,15 +148,24 @@
     )
 
     with pytest.raises(LimitsExceededError) as exc:
         await engine.send_to_process(TEST_ITEM, 1)
     assert exc.value.status_code == status
 
 
+async def test_send_to_process_limits_exceeded_429(engine):
+    engine.session = get_mocked_session(
+        "POST", 429, json={"detail": "limits exceeded"}, context_manager=False
+    )
+
+    with pytest.raises(LimitsExceededError) as exc:
+        await engine.send_to_process(TEST_ITEM, 1)
+    assert exc.value.status_code == 429
+
+
 async def test_send_to_process_500(engine):
     engine.session = get_mocked_session(
         "POST", 500, text="error", context_manager=False
     )
 
-    with pytest.raises(Exception) as exc:
+    with pytest.raises(SendToProcessError):
         await engine.send_to_process(TEST_ITEM, 1)
-    assert str(exc.value) == "500: error"
```

## nucliadb/ingest/tests/unit/consumer/test_auditing.py

```diff
@@ -19,66 +19,83 @@
 #
 
 import asyncio
 from unittest.mock import AsyncMock, MagicMock, patch
 
 import pytest
 from nucliadb_protos.audit_pb2 import AuditKBCounter, AuditRequest
-from nucliadb_protos.nodesidecar_pb2 import Counter
-from nucliadb_protos.writer_pb2 import Notification, ShardObject
+from nucliadb_protos.writer_pb2 import BrokerMessage, Notification, ShardObject
 
 from nucliadb.ingest.consumer import auditing
+from nucliadb_protos import nodereader_pb2
 
 pytestmark = pytest.mark.asyncio
 
 
 @pytest.fixture()
 def pubsub():
     mock = AsyncMock()
     mock.parse = lambda x: x
     yield mock
 
 
 @pytest.fixture()
-def sidecar():
+def reader():
     yield AsyncMock()
 
 
 @pytest.fixture()
-def nodes_manager(sidecar):
+def shard_manager(reader):
     nm = MagicMock()
-    node = MagicMock(sidecar=sidecar)
-    nm.choose_node.return_value = node, "shard_id", None
+    node = MagicMock(reader=reader)
     nm.get_shards_by_kbid = AsyncMock(return_value=[ShardObject()])
-    with patch("nucliadb.ingest.consumer.auditing.NodesManager", return_value=nm):
+    with patch(
+        "nucliadb.ingest.consumer.auditing.get_shard_manager", return_value=nm
+    ), patch(
+        "nucliadb.ingest.consumer.auditing.choose_node",
+        return_value=(node, "shard_id"),
+    ):
         yield nm
 
 
 @pytest.fixture()
 def audit():
     yield AsyncMock()
 
 
 @pytest.fixture()
-async def index_audit_handler(pubsub, audit, nodes_manager):
+async def index_audit_handler(pubsub, audit, shard_manager):
     iah = auditing.IndexAuditHandler(
         driver=AsyncMock(transaction=MagicMock(return_value=AsyncMock())),
         audit=audit,
         pubsub=pubsub,
         check_delay=0.05,
     )
     await iah.initialize()
     yield iah
     await iah.finalize()
 
 
+@pytest.fixture()
+async def writes_audit_handler(pubsub, audit, shard_manager):
+    rwah = auditing.ResourceWritesAuditHandler(
+        driver=AsyncMock(transaction=MagicMock(return_value=AsyncMock())),
+        storage=AsyncMock(),
+        audit=audit,
+        pubsub=pubsub,
+    )
+    await rwah.initialize()
+    yield rwah
+    await rwah.finalize()
+
+
 async def test_handle_message(
-    index_audit_handler: auditing.IndexAuditHandler, sidecar, audit
+    index_audit_handler: auditing.IndexAuditHandler, reader, audit
 ):
-    sidecar.GetCount.return_value = Counter(resources=5, paragraphs=6)
+    reader.GetShard.return_value = nodereader_pb2.Shard(fields=5, paragraphs=6)
 
     notif = Notification(
         kbid="kbid",
         action=Notification.Action.INDEXED,
     )
     await index_audit_handler.handle_message(notif.SerializeToString())
 
@@ -99,7 +116,25 @@
         action=Notification.Action.COMMIT,
     )
     await index_audit_handler.handle_message(notif.SerializeToString())
 
     await index_audit_handler.finalize()
 
     audit.report.assert_not_called()
+
+
+async def test_resource_handle_message_processor_messages_are_not_audited(
+    writes_audit_handler: auditing.ResourceWritesAuditHandler, audit
+):
+    message = BrokerMessage()
+    message.source = BrokerMessage.MessageSource.PROCESSOR
+    notif = Notification(
+        kbid="kbid",
+        action=Notification.Action.COMMIT,
+        message=message,
+        write_type=Notification.WriteType.MODIFIED,
+    )
+    await writes_audit_handler.handle_message(notif.SerializeToString())
+
+    await writes_audit_handler.finalize()
+
+    audit.report.assert_not_called()
```

## nucliadb/ingest/tests/unit/consumer/test_pull.py

```diff
@@ -49,12 +49,12 @@
     def worker(self, processor):
         yield PullWorker(
             driver=AsyncMock(),
             partition="1",
             storage=AsyncMock(),
             pull_time_error_backoff=100,
             zone="zone",
-            nuclia_cluster_url="nuclia_cluster_url",
+            nuclia_processing_cluster_url="nuclia_processing_cluster_url",
             nuclia_public_url="nuclia_public_url",
             audit=None,
             onprem=False,
         )
```

## nucliadb/ingest/tests/unit/consumer/test_shard_creator.py

```diff
@@ -15,117 +15,125 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
 import asyncio
-from unittest.mock import ANY, AsyncMock, MagicMock, patch
+from unittest.mock import AsyncMock, MagicMock, patch
 
 import pytest
-from nucliadb_protos.nodesidecar_pb2 import Counter
 from nucliadb_protos.writer_pb2 import Notification, ShardObject, Shards
 
+from nucliadb.common.cluster.settings import settings
 from nucliadb.ingest.consumer import shard_creator
-from nucliadb.ingest.settings import settings
+from nucliadb_protos import nodereader_pb2
 
 pytestmark = pytest.mark.asyncio
 
 
 @pytest.fixture()
 def pubsub():
     mock = AsyncMock()
     mock.parse = lambda x: x
     yield mock
 
 
 @pytest.fixture()
-def sidecar():
+def reader():
     yield AsyncMock()
 
 
 @pytest.fixture()
-def nodes_manager(sidecar):
-    nm = MagicMock()
-    node = MagicMock(sidecar=sidecar)
-    nm.choose_node.return_value = node, "shard_id", None
-    shards = Shards(shards=[ShardObject()], actual=0)
-    nm.get_shards_by_kbid_inner = AsyncMock(return_value=shards)
-    with patch("nucliadb.ingest.consumer.shard_creator.NodesManager", return_value=nm):
-        yield nm
+def kbdm():
+    mock = MagicMock()
+    mock.get_model_metadata = AsyncMock(return_value="model")
+    with patch("nucliadb.common.cluster.manager.datamanagers.kb", return_value=mock):
+        yield mock
 
 
 @pytest.fixture()
-def node_klass():
-    nc = AsyncMock()
+def shard_manager(reader):
+    sm = MagicMock()
+    node = MagicMock(reader=reader)
+    shards = Shards(shards=[ShardObject(read_only=False)], actual=0)
+    sm.get_current_active_shard = AsyncMock(return_value=shards.shards[0])
+    sm.maybe_create_new_shard = AsyncMock()
     with patch(
-        "nucliadb.ingest.consumer.shard_creator.get_node_klass", return_value=nc
+        "nucliadb.ingest.consumer.shard_creator.get_shard_manager", return_value=sm
+    ), patch(
+        "nucliadb.ingest.consumer.shard_creator.choose_node",
+        return_value=(node, "shard_id"),
+    ), patch(
+        "nucliadb.ingest.consumer.shard_creator.datamanagers.cluster.get_kb_shards",
+        AsyncMock(return_value=shards),
+    ), patch(
+        "nucliadb.ingest.consumer.shard_creator.locking.distributed_lock",
+        return_value=AsyncMock(),
     ):
-        yield nc
+        yield sm
 
 
 @pytest.fixture()
-def kb():
-    kb = AsyncMock()
-    with patch("nucliadb.ingest.consumer.shard_creator.KnowledgeBox", return_value=kb):
-        yield kb
-
-
-@pytest.fixture()
-async def shard_creator_handler(pubsub, nodes_manager, node_klass, kb):
+async def shard_creator_handler(pubsub, shard_manager):
     sc = shard_creator.ShardCreatorHandler(
         driver=AsyncMock(transaction=MagicMock(return_value=AsyncMock())),
         storage=AsyncMock(),
         pubsub=pubsub,
         check_delay=0.05,
     )
     await sc.initialize()
     yield sc
     await sc.finalize()
 
 
 async def test_handle_message_create_new_shard(
-    shard_creator_handler: shard_creator.ShardCreatorHandler, sidecar, node_klass, kb
+    shard_creator_handler: shard_creator.ShardCreatorHandler,
+    reader,
+    kbdm,
+    shard_manager,
 ):
-    sidecar.GetCount.return_value = Counter(resources=settings.max_shard_fields + 1)
+    reader.GetShard.return_value = nodereader_pb2.Shard(
+        paragraphs=settings.max_shard_paragraphs + 1
+    )
 
     notif = Notification(
         kbid="kbid",
         action=Notification.Action.INDEXED,
     )
     await shard_creator_handler.handle_message(notif.SerializeToString())
-
     await asyncio.sleep(0.06)
-
-    node_klass.create_shard_by_kbid.assert_called_with(
-        ANY, "kbid", similarity=kb.get_similarity.return_value
+    shard_manager.maybe_create_new_shard.assert_called_with(
+        "kbid", settings.max_shard_paragraphs + 1, 0
     )
 
 
 async def test_handle_message_do_not_create(
-    shard_creator_handler: shard_creator.ShardCreatorHandler, sidecar, node_klass, kb
+    shard_creator_handler: shard_creator.ShardCreatorHandler, reader, shard_manager
 ):
-    sidecar.GetCount.return_value = Counter(resources=settings.max_shard_fields - 1)
+    reader.GetShard.return_value = nodereader_pb2.Shard(
+        paragraphs=settings.max_shard_paragraphs - 1
+    )
 
     notif = Notification(
         kbid="kbid",
         action=Notification.Action.INDEXED,
     )
     await shard_creator_handler.handle_message(notif.SerializeToString())
 
     await shard_creator_handler.finalize()
 
-    node_klass.create_shard_by_kbid.assert_not_called()
+    shard_manager.create_shard_by_kbid.assert_not_called()
 
 
 async def test_handle_message_ignore_not_indexed(
-    shard_creator_handler: shard_creator.ShardCreatorHandler, node_klass
+    shard_creator_handler: shard_creator.ShardCreatorHandler, shard_manager
 ):
     notif = Notification(
         kbid="kbid",
         action=Notification.Action.COMMIT,
     )
     await shard_creator_handler.handle_message(notif.SerializeToString())
 
     await shard_creator_handler.finalize()
 
-    node_klass.create_shard_by_kbid.assert_not_called()
+    shard_manager.create_shard_by_kbid.assert_not_called()
```

## nucliadb/ingest/tests/unit/orm/test_brain.py

```diff
@@ -28,15 +28,16 @@
     FieldID,
     FieldType,
     Metadata,
     Paragraph,
     Sentence,
 )
 
-from nucliadb.ingest.orm.brain import ResourceBrain, get_page_number
+from nucliadb.ingest.orm.brain import ParagraphPages, ResourceBrain
+from nucliadb_protos import resources_pb2
 
 
 def test_apply_field_metadata_marks_duplicated_paragraphs():
     # Simulate a field with two paragraphs that contain the same text
     br = ResourceBrain(rid=str(uuid4()))
     field_key = "text1"
     fcmw = FieldComputedMetadataWrapper()
@@ -116,22 +117,24 @@
             # Only the first time that a paragraph is found should be set to false
             assert paragraph.repeated_in_field is False
         else:
             assert paragraph.repeated_in_field is True
 
 
 def test_get_page_number():
-    page_positions = {
-        0: (0, 99),
-        1: (100, 199),
-        2: (200, 299),
-    }
-    assert get_page_number(10, page_positions) == 0
-    assert get_page_number(100, page_positions) == 1
-    assert get_page_number(500, page_positions) == 2
+    page_numbers = ParagraphPages(
+        {
+            0: (0, 99),
+            1: (100, 199),
+            2: (200, 299),
+        }
+    )
+    assert page_numbers.get(10) == 0
+    assert page_numbers.get(100) == 1
+    assert page_numbers.get(500) == 2
 
 
 @pytest.mark.parametrize(
     "new_status,previous_status,expected_brain_status",
     [
         # No previous_status
         (Metadata.Status.PENDING, None, PBResource.PENDING),
@@ -212,7 +215,33 @@
     assert len(br.brain.paragraphs[field_key].paragraphs) == 2
     for paragraph in br.brain.paragraphs[field_key].paragraphs.values():
         assert paragraph.metadata.position.page_number == 2
         assert paragraph.metadata.position.start == 40
         assert paragraph.metadata.position.end == 54
         assert paragraph.metadata.position.start_seconds == [0]
         assert paragraph.metadata.position.end_seconds == [10]
+
+
+def test_set_resource_metadata_promotes_origin_dates():
+    resource_brain = ResourceBrain("rid")
+    basic = Basic()
+    basic.created.seconds = 1
+    basic.modified.seconds = 2
+    origin = resources_pb2.Origin()
+    origin.created.seconds = 3
+    origin.modified.seconds = 4
+
+    resource_brain.set_resource_metadata(basic, origin)
+
+    assert resource_brain.brain.metadata.created.seconds == 3
+    assert resource_brain.brain.metadata.modified.seconds == 4
+
+
+def test_set_resource_metadata_handles_timestamp_not_present():
+    resource_brain = ResourceBrain("rid")
+    basic = Basic()
+    resource_brain.set_resource_metadata(basic, None)
+    created = resource_brain.brain.metadata.created.seconds
+    modified = resource_brain.brain.metadata.modified.seconds
+    assert created > 0
+    assert modified > 0
+    assert modified >= created
```

## nucliadb/ingest/tests/unit/orm/test_processor.py

```diff
@@ -17,15 +17,18 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 from unittest.mock import AsyncMock, MagicMock, Mock, patch
 
 import pytest
 
-from nucliadb.ingest.orm.processor import Processor
+from nucliadb.common.cluster.settings import settings as cluster_settings
+from nucliadb.ingest.orm.exceptions import ResourceNotIndexable
+from nucliadb.ingest.orm.processor import Processor, validate_indexable_resource
+from nucliadb_protos import noderesources_pb2
 
 
 @pytest.fixture()
 def txn():
     yield AsyncMock()
 
 
@@ -33,79 +36,96 @@
 def driver(txn):
     mock = MagicMock()
     mock.transaction.return_value.__aenter__.return_value = txn
     yield mock
 
 
 @pytest.fixture()
-def processor(driver):
-    yield Processor(driver, None)
+def sm():
+    mock = AsyncMock()
+    mock.add_resource = AsyncMock()
+    with patch("nucliadb.ingest.orm.processor.get_shard_manager", return_value=mock):
+        yield mock
 
 
 @pytest.fixture()
-def shard():
-    yield AsyncMock()
+def processor(driver, sm):
+    yield Processor(driver, None)
 
 
 @pytest.fixture()
 def resource():
-    yield MagicMock()
+    mock = MagicMock()
+    mock.set_basic = AsyncMock()
+    yield mock
 
 
-async def test_mark_resource_error(processor: Processor, txn, shard, resource):
-    with patch("nucliadb.ingest.orm.processor.set_basic") as set_basic:
-        await processor._mark_resource_error(
-            resource, partition="partition", seqid=1, shard=shard, kbid="kbid"
-        )
+@pytest.fixture()
+def kb():
+    mock = MagicMock(kbid="kbid")
+    mock.get_resource_shard_id = AsyncMock()
+    mock.get_resource_shard = AsyncMock()
+    yield mock
 
-    txn.commit.assert_called_once()
-    set_basic.assert_called_once_with(
-        txn, resource.kb.kbid, resource.uuid, resource.basic
-    )
 
-    shard.add_resource.assert_called_once_with(
-        resource.indexer.brain, 1, partition="partition", kb="kbid"
+async def test_commit_slug(processor: Processor, txn, resource):
+    another_txn = Mock()
+    resource.txn = another_txn
+    resource.set_slug = AsyncMock()
+
+    await processor.commit_slug(resource)
+
+    resource.set_slug.assert_awaited_once()
+    txn.commit.assert_awaited_once()
+    assert resource.txn is another_txn
+
+
+async def test_mark_resource_error(processor: Processor, txn, resource, kb, sm):
+    await processor._mark_resource_error(kb, resource, partition="partition", seqid=1)
+    txn.commit.assert_called_once()
+    resource.set_basic.assert_awaited_once()
+    sm.add_resource.assert_awaited_once_with(
+        kb.get_resource_shard.return_value,
+        resource.indexer.brain,
+        1,
+        partition="partition",
+        kb="kbid",
     )
 
 
 async def test_mark_resource_error_handle_error(
-    processor: Processor, shard, resource, txn
+    processor: Processor, kb, resource, txn
 ):
-    with patch("nucliadb.ingest.orm.processor.set_basic") as set_basic:
-        set_basic.side_effect = Exception("test")
-        await processor._mark_resource_error(
-            resource, partition="partition", seqid=1, shard=shard, kbid="kbid"
-        )
-
+    resource.set_basic.side_effect = Exception("test")
+    await processor._mark_resource_error(kb, resource, partition="partition", seqid=1)
     txn.commit.assert_not_called()
 
 
 async def test_mark_resource_error_skip_no_shard(
-    processor: Processor, resource, driver
+    processor: Processor, resource, driver, kb, txn
 ):
-    await processor._mark_resource_error(
-        resource, partition="partition", seqid=1, shard=None, kbid="kbid"
-    )
-
-    driver.transaction.assert_not_called()
+    kb.get_resource_shard.return_value = None
+    await processor._mark_resource_error(kb, resource, partition="partition", seqid=1)
+    txn.commit.assert_not_called()
 
 
 async def test_mark_resource_error_skip_no_resource(
-    processor: Processor, shard, driver
+    processor: Processor, kb, driver, txn
 ):
-    await processor._mark_resource_error(
-        None, partition="partition", seqid=1, shard=shard, kbid="kbid"
-    )
-
-    driver.transaction.assert_not_called()
+    await processor._mark_resource_error(kb, None, partition="partition", seqid=1)
+    txn.commit.assert_not_called()
 
 
-async def test_commit_slug(processor: Processor, txn, resource):
-    another_txn = Mock()
-    resource.txn = another_txn
-    resource.set_slug = AsyncMock()
+def test_validate_indexable_resource():
+    resource = noderesources_pb2.Resource()
+    resource.paragraphs["test"].paragraphs["test"].sentences["test"].vector.append(1.0)
+    validate_indexable_resource(resource)
 
-    await processor.commit_slug(resource)
 
-    resource.set_slug.assert_awaited_once()
-    txn.commit.assert_awaited_once()
-    assert resource.txn is another_txn
+def test_validate_indexable_resource_throws_error_for_max():
+    resource = noderesources_pb2.Resource()
+    for i in range(cluster_settings.max_resource_paragraphs + 1):
+        resource.paragraphs["test"].paragraphs[f"test{i}"].sentences[
+            "test"
+        ].vector.append(1.0)
+    with pytest.raises(ResourceNotIndexable):
+        validate_indexable_resource(resource)
```

## nucliadb/ingest/tests/unit/orm/test_resource.py

```diff
@@ -13,32 +13,37 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from unittest.mock import AsyncMock
+from unittest.mock import AsyncMock, MagicMock
 
 import pytest
 from nucliadb_protos.resources_pb2 import (
+    AllFieldIDs,
     Basic,
     CloudFile,
+    FieldID,
     FieldText,
+    FieldType,
     FileExtractedData,
     PagePositions,
 )
 from nucliadb_protos.writer_pb2 import BrokerMessage
 
 from nucliadb.ingest.orm.resource import (
+    Resource,
     get_file_page_positions,
     get_text_field_mimetype,
     maybe_update_basic_icon,
     maybe_update_basic_summary,
     maybe_update_basic_thumbnail,
+    update_basic_languages,
 )
 
 
 @pytest.mark.asyncio
 async def test_get_file_page_positions():
     extracted_data = FileExtractedData()
     extracted_data.file_pages_previews.positions.extend(
@@ -62,14 +67,38 @@
     assert maybe_update_basic_summary(basic, summary) is updated
     if updated:
         assert basic.summary == summary
     else:
         assert basic.summary != summary
 
 
+def test_update_basic_languages():
+    basic = Basic()
+    # Languages are updated the first time
+    assert update_basic_languages(basic, ["en"]) is True
+    assert basic.metadata.language == "en"
+    assert basic.metadata.languages == ["en"]
+
+    # Languages are not updated
+    assert update_basic_languages(basic, ["en"]) is False
+    assert basic.metadata.language == "en"
+    assert basic.metadata.languages == ["en"]
+
+    # Main language is not updated but new language is added
+    assert update_basic_languages(basic, ["de"]) is True
+    assert basic.metadata.language == "en"
+    assert basic.metadata.languages == ["en", "de"]
+
+    # Null values
+    assert update_basic_languages(basic, [""]) is False
+    assert update_basic_languages(basic, [None]) is False  # type: ignore
+    assert basic.metadata.language == "en"
+    assert basic.metadata.languages == ["en", "de"]
+
+
 @pytest.mark.parametrize(
     "basic,thumbnail,updated",
     [
         (Basic(), CloudFile(uri="new_thumbnail_url"), True),
         (
             Basic(thumbnail="old_thumbnail_url"),
             CloudFile(uri="new_thumbnail_url"),
@@ -90,14 +119,15 @@
     "text_format,mimetype",
     [
         (None, None),
         (FieldText.Format.PLAIN, "text/plain"),
         (FieldText.Format.HTML, "text/html"),
         (FieldText.Format.RST, "text/x-rst"),
         (FieldText.Format.MARKDOWN, "text/markdown"),
+        (FieldText.Format.KEEP_MARKDOWN, "text/markdown"),
     ],
 )
 def test_get_text_field_mimetype(text_format, mimetype):
     message = BrokerMessage()
     if text_format is not None:
         message.texts["foo"].body = "foo"
         message.texts["foo"].format = text_format
@@ -114,7 +144,132 @@
         (Basic(icon="application/octet-stream"), "text/html", True),
     ],
 )
 def test_maybe_update_basic_icon(basic, icon, updated):
     assert maybe_update_basic_icon(basic, icon) == updated
     if updated:
         assert basic.icon == icon
+
+
+class Transaction:
+    def __init__(self):
+        self.kv = {}
+
+    async def get(self, key):
+        return self.kv.get(key)
+
+    async def set(self, key, value):
+        self.kv[key] = value
+
+
+@pytest.fixture(scope="function")
+def txn():
+    return Transaction()
+
+
+@pytest.fixture(scope="function")
+def storage():
+    mock = AsyncMock()
+    return mock
+
+
+@pytest.fixture(scope="function")
+def kb():
+    mock = AsyncMock()
+    return mock
+
+
+async def test_get_fields_ids_caches_keys(txn, storage, kb):
+    resource = Resource(txn, storage, kb, "rid")
+    cached_field_keys = [(0, "foo"), (1, "bar")]
+    new_field_keys = [(2, "baz")]
+    resource._inner_get_fields_ids = AsyncMock(return_value=new_field_keys)  # type: ignore
+    resource.all_fields_keys = cached_field_keys
+
+    assert await resource.get_fields_ids() == cached_field_keys
+    resource._inner_get_fields_ids.assert_not_awaited()
+
+    assert await resource.get_fields_ids(force=True) == new_field_keys
+    resource._inner_get_fields_ids.assert_awaited_once()
+    assert resource.all_fields_keys == new_field_keys
+
+    # If the all_field_keys is an empty list,
+    # we should not be calling the inner_get_fields_ids
+    resource.all_fields_keys = []
+    resource._inner_get_fields_ids.reset_mock()
+    assert await resource.get_fields_ids() == []
+    resource._inner_get_fields_ids.assert_not_awaited()
+
+
+async def test_get_set_all_field_ids(txn, storage, kb):
+    resource = Resource(txn, storage, kb, "rid")
+
+    assert await resource.get_all_field_ids() is None
+
+    all_fields = AllFieldIDs()
+    all_fields.fields.append(FieldID(field_type=FieldType.TEXT, field="text"))
+
+    await resource.set_all_field_ids(all_fields)
+
+    assert await resource.get_all_field_ids() == all_fields
+
+
+async def test_update_all_fields_key(txn, storage, kb):
+    resource = Resource(txn, storage, kb, "rid")
+
+    await resource.update_all_field_ids(updated=[], deleted=[])
+
+    # Initial value is Empty
+    assert (await resource.get_all_field_ids()) == AllFieldIDs()
+
+    all_fields = AllFieldIDs()
+    all_fields.fields.append(FieldID(field_type=FieldType.TEXT, field="text1"))
+    all_fields.fields.append(FieldID(field_type=FieldType.TEXT, field="text2"))
+
+    await resource.update_all_field_ids(updated=all_fields.fields)
+
+    # Check updates
+    assert await resource.get_all_field_ids() == all_fields
+
+    file_field = FieldID(field_type=FieldType.FILE, field="file")
+    await resource.update_all_field_ids(updated=[file_field])
+
+    result = await resource.get_all_field_ids()
+    assert list(result.fields) == list(all_fields.fields) + [file_field]
+
+    # Check deletes
+    await resource.update_all_field_ids(deleted=[file_field])
+
+    assert await resource.get_all_field_ids() == all_fields
+
+
+async def test_apply_fields_calls_update_all_field_ids(txn, storage, kb):
+    resource = Resource(txn, storage, kb, "rid")
+    resource.update_all_field_ids = AsyncMock()  # type: ignore
+    resource.set_field = AsyncMock()  # type: ignore
+
+    bm = MagicMock()
+    bm.layouts = {"layout": MagicMock()}
+    bm.texts = {"text": MagicMock()}
+    bm.keywordsets = {"keywordset": MagicMock()}
+    bm.datetimes = {"datetime": MagicMock()}
+    bm.links = {"link": MagicMock()}
+    bm.files = {"file": MagicMock()}
+    bm.conversations = {"conversation": MagicMock()}
+    bm.delete_fields.append(FieldID(field_type=FieldType.LAYOUT, field="to_delete"))
+
+    await resource.apply_fields(bm)
+
+    resource.update_all_field_ids.assert_awaited_once()
+
+    resource.update_all_field_ids.call_args[1]["updated"] == [
+        FieldID(field_type=FieldType.LAYOUT, field="layout"),
+        FieldID(field_type=FieldType.TEXT, field="text"),
+        FieldID(field_type=FieldType.KEYWORDSET, field="keywordset"),
+        FieldID(field_type=FieldType.DATETIME, field="datetime"),
+        FieldID(field_type=FieldType.LINK, field="link"),
+        FieldID(field_type=FieldType.FILE, field="file"),
+        FieldID(field_type=FieldType.CONVERSATION, field="conversation"),
+    ]
+    resource.update_all_field_ids.call_args[1]["deleted"] == [
+        FieldID(field_type=FieldType.LAYOUT, field="to_delete"),
+    ]
```

## nucliadb/reader/app.py

```diff
@@ -13,44 +13,58 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+
 import pkg_resources
 from fastapi import FastAPI
 from fastapi.responses import JSONResponse
 from starlette.middleware import Middleware
 from starlette.middleware.authentication import AuthenticationMiddleware
 from starlette.middleware.cors import CORSMiddleware
 from starlette.requests import ClientDisconnect, Request
 from starlette.responses import HTMLResponse
 
+from nucliadb.common.context.fastapi import set_app_context
 from nucliadb.reader import API_PREFIX
 from nucliadb.reader.api.v1.router import api as api_v1
 from nucliadb.reader.lifecycle import finalize, initialize
 from nucliadb_telemetry import errors
-from nucliadb_utils.authentication import STFAuthenticationBackend
+from nucliadb_utils import const
+from nucliadb_utils.authentication import NucliaCloudAuthenticationBackend
 from nucliadb_utils.fastapi.openapi import extend_openapi
 from nucliadb_utils.fastapi.versioning import VersionedFastAPI
 from nucliadb_utils.settings import http_settings, running_settings
+from nucliadb_utils.utilities import has_feature
+
+middleware = []
 
-middleware = [
-    Middleware(
-        CORSMiddleware,
-        allow_origins=http_settings.cors_origins,
-        allow_methods=["*"],
-        allow_headers=["*"],
-    ),
-    Middleware(
-        AuthenticationMiddleware,
-        backend=STFAuthenticationBackend(),
-    ),
-]
+if has_feature(const.Features.CORS_MIDDLEWARE, default=False):
+    middleware.append(
+        Middleware(
+            CORSMiddleware,
+            allow_origins=http_settings.cors_origins,
+            allow_methods=["*"],
+            # Authorization will be exluded from * in the future, (CORS non-wildcard request-header).
+            # Browsers already showing deprecation notices, so it needs to be specified explicitly
+            allow_headers=["*", "Authorization"],
+        )
+    )
+
+middleware.extend(
+    [
+        Middleware(
+            AuthenticationMiddleware,
+            backend=NucliaCloudAuthenticationBackend(),
+        )
+    ]
+)
 
 errors.setup_error_handling(pkg_resources.get_distribution("nucliadb").version)
 
 on_startup = [initialize]
 on_shutdown = [finalize]
 
 
@@ -77,29 +91,33 @@
     exception_handlers={
         Exception: global_exception_handler,
         ClientDisconnect: client_disconnect_handler,
     },
 )
 
 
-base_app = FastAPI(title="NucliaDB Reader API", **fastapi_settings)  # type: ignore
+def create_application() -> FastAPI:
+    base_app = FastAPI(title="NucliaDB Reader API", **fastapi_settings)  # type: ignore
 
-base_app.include_router(api_v1)
+    base_app.include_router(api_v1)
 
-extend_openapi(base_app)
+    extend_openapi(base_app)
 
-application = VersionedFastAPI(
-    base_app,
-    version_format="{major}",
-    prefix_format=f"/{API_PREFIX}/v{{major}}",
-    default_version=(1, 0),
-    enable_latest=False,
-    kwargs=fastapi_settings,
-)
+    application = VersionedFastAPI(
+        base_app,
+        version_format="{major}",
+        prefix_format=f"/{API_PREFIX}/v{{major}}",
+        default_version=(1, 0),
+        enable_latest=False,
+        kwargs=fastapi_settings,
+    )
 
+    async def homepage(request: Request) -> HTMLResponse:
+        return HTMLResponse("NucliaDB Reader Service")
 
-async def homepage(request: Request) -> HTMLResponse:
-    return HTMLResponse("NucliaDB Reader Service")
+    # Use raw starlette routes to avoid unnecessary overhead
+    application.add_route("/", homepage)
 
+    # Inject application context into the fastapi app's state
+    set_app_context(application)
 
-# Use raw starlette routes to avoid unnecessary overhead
-application.add_route("/", homepage)
+    return application
```

## nucliadb/reader/openapi.py

```diff
@@ -14,60 +14,14 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
-import datetime
-import json
-import sys
-
-from fastapi.openapi.utils import get_openapi
-from starlette.routing import Mount
-
-from nucliadb.reader import API_PREFIX
-
-
-def is_versioned_route(route):
-    return isinstance(route, Mount) and route.path.startswith(f"/{API_PREFIX}/v")
-
-
-def extract_openapi(application, version, commit_id):
-    app = [
-        route.app
-        for route in application.routes
-        if is_versioned_route(route) and route.app.version == version
-    ][0]
-    document = get_openapi(
-        title=app.title,
-        version=app.version,
-        openapi_version=app.openapi_version,
-        description=app.description,
-        terms_of_service=app.terms_of_service,
-        contact=app.contact,
-        license_info=app.license_info,
-        routes=app.routes,
-        tags=app.openapi_tags,
-        servers=app.servers,
-    )
-
-    document["x-metadata"] = {
-        "nucliadb_reader": {
-            "commit": commit_id,
-            "last_updated": datetime.datetime.utcnow().isoformat(),
-        }
-    }
-    return document
+from nucliadb import openapi
 
 
 def command_extract_openapi():
-    from nucliadb.reader.app import application
-
-    openapi_json_path = sys.argv[1]
-    api_version = sys.argv[2]
-    commit_id = sys.argv[3]
+    from nucliadb.reader.app import create_application
 
-    json.dump(
-        extract_openapi(application, api_version, commit_id),
-        open(openapi_json_path, "w"),
-    )
+    openapi.command_extract_openapi(create_application(), "nucliadb_reader")
```

## nucliadb/reader/run.py

```diff
@@ -14,24 +14,26 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 from nucliadb.reader import SERVICE_NAME
-from nucliadb.reader.app import application
+from nucliadb.reader.app import create_application
 from nucliadb_telemetry.fastapi import instrument_app
 from nucliadb_telemetry.logs import setup_logging
 from nucliadb_telemetry.utils import get_telemetry
 from nucliadb_utils.fastapi.run import run_fastapi_with_metrics
 
 
 def run():
     setup_logging()
+    application = create_application()
     instrument_app(
         application,
         tracer_provider=get_telemetry(SERVICE_NAME),
         excluded_urls=["/"],
         metrics=True,
+        trace_id_on_responses=True,
     )
 
     run_fastapi_with_metrics(application)
```

## nucliadb/reader/api/models.py

```diff
@@ -13,15 +13,15 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import TYPE_CHECKING, Any, Dict, Optional, Union
+from typing import TYPE_CHECKING, Any, Optional, Union
 
 from pydantic import BaseModel
 
 import nucliadb_models as models
 from nucliadb_models.common import FIELD_TYPES_MAP, FieldTypeName
 from nucliadb_models.resource import (
     ConversationFieldExtractedData,
@@ -53,20 +53,20 @@
 
 
 class ResourceField(BaseModel):
     field_type: FieldTypeName
     field_id: str
     value: ValueType
     extracted: ExtractedDataType
-    error: Optional[Error]
+    error: Optional[Error] = None
 
 
 FIELD_NAMES_TO_PB_TYPE_MAP = {v: k for k, v in FIELD_TYPES_MAP.items()}
 
-FIELD_NAME_TO_EXTRACTED_DATA_FIELD_MAP: Dict[FieldTypeName, Any] = {
+FIELD_NAME_TO_EXTRACTED_DATA_FIELD_MAP: dict[FieldTypeName, Any] = {
     FieldTypeName.TEXT: TextFieldExtractedData,
     FieldTypeName.FILE: FileFieldExtractedData,
     FieldTypeName.LINK: LinkFieldExtractedData,
     FieldTypeName.DATETIME: DatetimeFieldExtractedData,
     FieldTypeName.KEYWORDSET: KeywordsetFieldExtractedData,
     FieldTypeName.LAYOUT: LayoutFieldExtractedData,
     FieldTypeName.CONVERSATION: ConversationFieldExtractedData,
```

## nucliadb/reader/api/v1/__init__.py

```diff
@@ -14,11 +14,14 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from . import download  # noqa
+from . import export_import  # noqa
 from . import knowledgebox  # noqa
+from . import learning_collector  # noqa
+from . import learning_config  # noqa
 from . import resource  # noqa
 from . import services  # noqa
 from .router import api  # noqa
```

## nucliadb/reader/api/v1/download.py

```diff
@@ -13,25 +13,26 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import urllib.parse
 from enum import Enum
-from typing import Optional, Tuple
+from typing import Optional
 
 from fastapi import HTTPException
 from fastapi.requests import Request
 from fastapi.responses import Response
 from fastapi_versioning import version
 from starlette.datastructures import Headers
 from starlette.responses import StreamingResponse
 
-from nucliadb.ingest.orm.resource import KB_REVERSE_REVERSE
+from nucliadb.ingest.orm.resource import FIELD_TYPE_TO_ID
 from nucliadb.ingest.serialize import get_resource_uuid_by_slug
 from nucliadb.reader import SERVICE_NAME, logger
 from nucliadb.reader.api.models import FIELD_NAMES_TO_PB_TYPE_MAP
 from nucliadb_models.common import FieldTypeName
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_utils.authentication import requires_one
 from nucliadb_utils.storages.storage import StorageField  # type: ignore
@@ -47,58 +48,110 @@
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/{{field_type}}/{{field_id}}/download/extracted/{{download_field:path}}",  # noqa
     tags=["Resource fields"],
     status_code=200,
     name="Download extracted binary file (by slug)",
 )
+@requires_one([NucliaDBRoles.READER])
+@version(1)
+async def download_extract_file_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_type: FieldTypeName,
+    field_id: str,
+    download_field: str,
+) -> Response:
+    return await _download_extract_file(
+        request, kbid, field_type, field_id, download_field, rslug=rslug
+    )
+
+
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/{{field_type}}/{{field_id}}/download/extracted/{{download_field:path}}",  # noqa
     tags=["Resource fields"],
     status_code=200,
     name="Download extracted binary file (by id)",
 )
 @requires_one([NucliaDBRoles.READER])
 @version(1)
-async def download_extract_file(
+async def download_extract_file_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_type: FieldTypeName,
+    field_id: str,
+    download_field: str,
+) -> Response:
+    return await _download_extract_file(
+        request, kbid, field_type, field_id, download_field, rid=rid
+    )
+
+
+async def _download_extract_file(
     request: Request,
     kbid: str,
     field_type: FieldTypeName,
     field_id: str,
     download_field: str,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
 ) -> Response:
     rid = await _get_resource_uuid_from_params(kbid, rid, rslug)
 
     storage = await get_storage(service_name=SERVICE_NAME)
 
     pb_field_type = FIELD_NAMES_TO_PB_TYPE_MAP[field_type]
-    field_type_letter = KB_REVERSE_REVERSE[pb_field_type]
+    field_type_letter = FIELD_TYPE_TO_ID[pb_field_type]
 
     sf = storage.file_extracted(kbid, rid, field_type_letter, field_id, download_field)
 
     return await download_api(sf, request.headers)
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/file/{{field_id}}/download/field",
     tags=["Resource fields"],
     status_code=200,
     name="Download field binary field (by slug)",
 )
+@requires_one([NucliaDBRoles.READER])
+@version(1)
+async def download_field_file_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    inline: bool = False,
+) -> Response:
+    return await _download_field_file(
+        request, kbid, field_id, rslug=rslug, inline=inline
+    )
+
+
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/file/{{field_id}}/download/field",
     tags=["Resource fields"],
     status_code=200,
     name="Download field binary field (by id)",
 )
 @requires_one([NucliaDBRoles.READER])
 @version(1)
-async def download_field_file(
+async def download_field_file_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    inline: bool = False,
+) -> Response:
+    return await _download_field_file(request, kbid, field_id, rid=rid, inline=inline)
+
+
+async def _download_field_file(
     request: Request,
     kbid: str,
     field_id: str,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
     inline: bool = False,
 ) -> Response:
@@ -113,23 +166,49 @@
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/layout/{{field_id}}/download/field/{{download_field}}",
     tags=["Resource fields"],
     status_code=200,
     name="Download layout binary field (by slug)",
 )
+@requires_one([NucliaDBRoles.READER])
+@version(1)
+async def download_field_layout_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    download_field: str,
+) -> Response:
+    return await _download_field_layout(
+        request, kbid, field_id, download_field, rslug=rslug
+    )
+
+
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/layout/{{field_id}}/download/field/{{download_field}}",
     tags=["Resource fields"],
     status_code=200,
     name="Download layout binary field (by id)",
 )
 @requires_one([NucliaDBRoles.READER])
 @version(1)
-async def download_field_layout(
+async def download_field_layout_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    download_field: str,
+) -> Response:
+    return await _download_field_layout(
+        request, kbid, field_id, download_field, rid=rid
+    )
+
+
+async def _download_field_layout(
     request: Request,
     kbid: str,
     field_id: str,
     download_field: str,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
 ) -> Response:
@@ -144,23 +223,51 @@
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/conversation/{{field_id}}/download/field/{{message_id}}/{{file_num}}",  # noqa
     tags=["Resource fields"],
     status_code=200,
     name="Download conversation binary field (by slug)",
 )
+@requires_one([NucliaDBRoles.READER])
+@version(1)
+async def download_field_conversation_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    message_id: str,
+    file_num: int,
+) -> Response:
+    return await _download_field_conversation(
+        request, kbid, field_id, message_id, file_num, rslug=rslug
+    )
+
+
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/conversation/{{field_id}}/download/field/{{message_id}}/{{file_num}}",  # noqa
     tags=["Resource fields"],
     status_code=200,
     name="Download conversation binary field (by id)",
 )
 @requires_one([NucliaDBRoles.READER])
 @version(1)
-async def download_field_conversation(
+async def download_field_conversation_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    message_id: str,
+    file_num: int,
+) -> Response:
+    return await _download_field_conversation(
+        request, kbid, field_id, message_id, file_num, rid=rid
+    )
+
+
+async def _download_field_conversation(
     request: Request,
     kbid: str,
     field_id: str,
     message_id: str,
     file_num: int,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
@@ -178,14 +285,16 @@
     metadata = await sf.exists()
     if metadata is None:
         raise HTTPException(status_code=404, detail="Specified file doesn't exist")
 
     file_size = int(metadata.get("SIZE", -1))
     content_type = metadata.get("CONTENT_TYPE", "application/octet-stream")
     filename = metadata.get("FILENAME", "file")
+    filename = safe_http_header_encode(filename)
+
     status_code = 200
 
     content_disposition = "inline" if inline else f'attachment; filename="{filename}"'
     extra_headers = {
         "Accept-Ranges": "bytes",
         "Content-Type": content_type,
         "Content-Disposition": content_disposition,
@@ -257,15 +366,15 @@
         rid = await get_resource_uuid_by_slug(kbid, rslug, service_name=SERVICE_NAME)  # type: ignore
         if rid is None:
             raise HTTPException(status_code=404, detail="Resource does not exist")
 
     return rid
 
 
-def parse_media_range(range_request: str, file_size: int) -> Tuple[int, int, int]:
+def parse_media_range(range_request: str, file_size: int) -> tuple[int, int, int]:
     # Implemented following this docpage: https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests
     ranges = range_request.split("bytes=")[-1].split(", ")
     if len(ranges) > 1:
         raise NotImplementedError()
     start_str, _, end_str = ranges[0].partition("-")
     start = int(start_str)
     max_range_size = file_size - 1
@@ -274,7 +383,11 @@
         end = max_range_size
         range_size = file_size - start
     else:
         end = int(end_str)
         end = min(end, max_range_size)
         range_size = (end - start) + 1
     return start, end, range_size
+
+
+def safe_http_header_encode(text):
+    return urllib.parse.quote(text)
```

## nucliadb/reader/api/v1/knowledgebox.py

```diff
@@ -17,16 +17,16 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from fastapi import HTTPException
 from fastapi_versioning import version  # type: ignore
 from starlette.requests import Request
 
-from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
-from nucliadb.ingest.utils import get_driver
+from nucliadb.common import datamanagers
+from nucliadb.common.maindb.utils import get_driver
 from nucliadb.reader.api.v1.router import KB_PREFIX, KBS_PREFIX, api
 from nucliadb_models.resource import (
     KnowledgeBoxConfig,
     KnowledgeBoxList,
     KnowledgeBoxObj,
     KnowledgeBoxObjSummary,
     NucliaDBRoles,
@@ -41,63 +41,63 @@
     response_model=KnowledgeBoxList,
     tags=["Knowledge Boxes"],
     include_in_schema=False,
 )
 @requires(NucliaDBRoles.MANAGER)
 @version(1)
 async def get_kbs(request: Request, prefix: str = "") -> KnowledgeBoxList:
-    driver = await get_driver()
+    driver = get_driver()
     async with driver.transaction() as txn:
         response = KnowledgeBoxList()
-        async for kbid, slug in KnowledgeBox.get_kbs(txn, prefix):
-            response.kbs.append(KnowledgeBoxObjSummary(slug=slug or None, uuid=kbid))
+        async for kbid, slug in datamanagers.kb.get_kbs(txn, prefix=prefix):
+            response.kbs.append(KnowledgeBoxObjSummary(slug=slug or None, uuid=kbid))  # type: ignore
         return response
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}",
     status_code=200,
     name="Get Knowledge Box",
     response_model=KnowledgeBoxObj,
     tags=["Knowledge Boxes"],
 )
 @requires_one([NucliaDBRoles.MANAGER, NucliaDBRoles.READER])
 @version(1)
 async def get_kb(request: Request, kbid: str) -> KnowledgeBoxObj:
-    driver = await get_driver()
+    driver = get_driver()
     async with driver.transaction() as txn:
-        kb_config = await KnowledgeBox.get_kb(txn, kbid)
+        kb_config = await datamanagers.kb.get_config(txn, kbid=kbid)
         if kb_config is None:
             raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
 
         return KnowledgeBoxObj(
             uuid=kbid,
-            slug=kb_config.slug,
+            slug=kb_config.slug,  # type: ignore
             config=KnowledgeBoxConfig.from_message(kb_config),
         )
 
 
 @api.get(
     f"/{KB_PREFIX}/s/{{slug}}",
     status_code=200,
     name="Get Knowledge Box (by slug)",
     response_model=KnowledgeBoxObj,
     tags=["Knowledge Boxes"],
 )
 @requires_one([NucliaDBRoles.MANAGER, NucliaDBRoles.READER])
 @version(1)
 async def get_kb_by_slug(request: Request, slug: str) -> KnowledgeBoxObj:
-    driver = await get_driver()
+    driver = get_driver()
     async with driver.transaction() as txn:
-        kbid = await KnowledgeBox.get_kb_uuid(txn, slug)
+        kbid = await datamanagers.kb.get_kb_uuid(txn, slug=slug)
         if kbid is None:
             raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
 
-        kb_config = await KnowledgeBox.get_kb(txn, kbid)
+        kb_config = await datamanagers.kb.get_config(txn, kbid=kbid)
         if kb_config is None:
             raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
 
         return KnowledgeBoxObj(
             uuid=kbid,
-            slug=kb_config.slug,
+            slug=kb_config.slug,  # type: ignore
             config=KnowledgeBoxConfig.from_message(kb_config),
         )
```

## nucliadb/reader/api/v1/resource.py

```diff
@@ -13,27 +13,30 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import List, Literal, Optional, Union
-from typing import get_args as typing_get_args
+from typing import Optional, Union
 
 from fastapi import Header, HTTPException, Query, Request, Response
 from fastapi_versioning import version
 
 import nucliadb_models as models
+from nucliadb.common.maindb.utils import get_driver
 from nucliadb.ingest.fields.conversation import Conversation
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox as ORMKnowledgeBox
 from nucliadb.ingest.orm.resource import KB_RESOURCE_SLUG_BASE
 from nucliadb.ingest.orm.resource import Resource as ORMResource
-from nucliadb.ingest.serialize import serialize, set_resource_field_extracted_data
-from nucliadb.ingest.utils import get_driver
+from nucliadb.ingest.serialize import (
+    managed_serialize,
+    serialize,
+    set_resource_field_extracted_data,
+)
 from nucliadb.reader import SERVICE_NAME  # type: ignore
 from nucliadb.reader.api import DEFAULT_RESOURCE_LIST_PAGE_SIZE
 from nucliadb.reader.api.models import (
     FIELD_NAME_TO_EXTRACTED_DATA_FIELD_MAP,
     FIELD_NAMES_TO_PB_TYPE_MAP,
     ResourceField,
 )
@@ -63,30 +66,30 @@
 )
 @requires_one([NucliaDBRoles.READER])
 @version(1)
 async def list_resources(
     request: Request,
     response: Response,
     kbid: str,
-    page: int = Query(0),
-    size: int = Query(DEFAULT_RESOURCE_LIST_PAGE_SIZE),
+    page: int = Query(0, description="Requested page number (0-based)"),
+    size: int = Query(DEFAULT_RESOURCE_LIST_PAGE_SIZE, description="Page size"),
 ) -> ResourceList:
     # Get all resource id's fast by scanning all existing slugs
 
     # Get counters from maindb
-    driver = await get_driver()
+    driver = get_driver()
     txn = await driver.begin()
 
     # Filter parameters for serializer
-    show: List[ResourceProperties] = [ResourceProperties.BASIC]
-    field_types: List[FieldTypeName] = []
-    extracted: List[ExtractedDataTypeName] = []
+    show: list[ResourceProperties] = [ResourceProperties.BASIC]
+    field_types: list[FieldTypeName] = []
+    extracted: list[ExtractedDataTypeName] = []
 
     try:
-        resources: List[Resource] = []
+        resources: list[Resource] = []
         max_items_to_iterate = (page + 1) * size
         first_wanted_item_index = (page * size) + 1  # 1-based index
         current_key_index = 0
 
         # ask for one item more than we need, in order to know if it's the last page
         keys_generator = txn.keys(
             match=KB_RESOURCE_SLUG_BASE.format(kbid=kbid),
@@ -102,16 +105,17 @@
             # Don't fetch keys once we got all items for this
             if len(resources) == size:
                 await keys_generator.aclose()
                 break
 
             # Fetch and Add wanted item
             rid = await txn.get(key)
-            if rid is not None:
-                result = await serialize(
+            if rid:
+                result = await managed_serialize(
+                    txn,
                     kbid,
                     rid.decode(),
                     show,
                     field_types,
                     extracted,
                     service_name=SERVICE_NAME,
                 )
@@ -131,51 +135,107 @@
     return ResourceList(
         resources=resources,
         pagination=ResourcePagination(page=page, size=size, last=is_last_page),
     )
 
 
 @api.get(
-    f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}",
+    f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}",
     status_code=200,
-    name="Get Resource (by slug)",
+    name="Get Resource (by id)",
     response_model=Resource,
     response_model_exclude_unset=True,
     tags=["Resources"],
 )
+@requires(NucliaDBRoles.READER)
+@version(1)
+async def get_resource_by_uuid(
+    request: Request,
+    kbid: str,
+    rid: str,
+    show: list[ResourceProperties] = Query([ResourceProperties.BASIC]),
+    field_type_filter: list[FieldTypeName] = Query(
+        list(FieldTypeName), alias="field_type"
+    ),
+    extracted: list[ExtractedDataTypeName] = Query(
+        [
+            ExtractedDataTypeName.TEXT,
+            ExtractedDataTypeName.METADATA,
+            ExtractedDataTypeName.LINK,
+            ExtractedDataTypeName.FILE,
+        ]
+    ),
+    x_nucliadb_user: str = Header(""),
+    x_forwarded_for: str = Header(""),
+):
+    return await _get_resource(
+        rid=rid,
+        kbid=kbid,
+        show=show,
+        field_type_filter=field_type_filter,
+        extracted=extracted,
+        x_nucliadb_user=x_nucliadb_user,
+        x_forwarded_for=x_forwarded_for,
+    )
+
+
 @api.get(
-    f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}",
+    f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}",
     status_code=200,
-    name="Get Resource (by id)",
+    name="Get Resource (by slug)",
     response_model=Resource,
     response_model_exclude_unset=True,
     tags=["Resources"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
-async def get_resource(
+async def get_resource_by_slug(
     request: Request,
     kbid: str,
-    rid: Optional[str] = None,
-    rslug: Optional[str] = None,
-    show: List[ResourceProperties] = Query([ResourceProperties.BASIC]),
-    field_type_filter: List[FieldTypeName] = Query(
+    rslug: str,
+    show: list[ResourceProperties] = Query([ResourceProperties.BASIC]),
+    field_type_filter: list[FieldTypeName] = Query(
         list(FieldTypeName), alias="field_type"
     ),
-    extracted: List[ExtractedDataTypeName] = Query(
+    extracted: list[ExtractedDataTypeName] = Query(
         [
             ExtractedDataTypeName.TEXT,
             ExtractedDataTypeName.METADATA,
             ExtractedDataTypeName.LINK,
             ExtractedDataTypeName.FILE,
         ]
     ),
     x_nucliadb_user: str = Header(""),
     x_forwarded_for: str = Header(""),
 ) -> Resource:
+    return await _get_resource(
+        rslug=rslug,
+        kbid=kbid,
+        show=show,
+        field_type_filter=field_type_filter,
+        extracted=extracted,
+        x_nucliadb_user=x_nucliadb_user,
+        x_forwarded_for=x_forwarded_for,
+    )
+
+
+async def _get_resource(
+    *,
+    rslug: Optional[str] = None,
+    rid: Optional[str] = None,
+    kbid: str,
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    extracted: list[ExtractedDataTypeName],
+    x_nucliadb_user: str,
+    x_forwarded_for: str,
+) -> Resource:
+    if all([rslug, rid]) or not any([rslug, rid]):
+        raise ValueError("Either rid or rslug must be provided, but not both")
+
     audit = get_audit()
     if audit is not None:
         audit_id = rid if rid else rslug
         await audit.visited(kbid, audit_id, x_nucliadb_user, x_forwarded_for)  # type: ignore
 
     result = await serialize(
         kbid,
@@ -187,127 +247,178 @@
         slug=rslug,
     )
     if result is None:
         raise HTTPException(status_code=404, detail="Resource does not exist")
     return result
 
 
-PageShortcuts = Literal["last", "first"]
-PAGE_SHORTCUTS = typing_get_args(PageShortcuts)
-
-
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/{{field_type}}/{{field_id}}",
     status_code=200,
     name="Get Resource field (by slug)",
     response_model=ResourceField,
     response_model_exclude_unset=True,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.READER)
+@version(1)
+async def get_resource_field_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_type: FieldTypeName,
+    field_id: str,
+    show: list[ResourceFieldProperties] = Query([ResourceFieldProperties.VALUE]),
+    extracted: list[ExtractedDataTypeName] = Query(
+        [
+            ExtractedDataTypeName.TEXT,
+            ExtractedDataTypeName.METADATA,
+            ExtractedDataTypeName.LINK,
+            ExtractedDataTypeName.FILE,
+        ]
+    ),
+    # not working with latest pydantic/fastapi
+    # page: Union[Literal["last", "first"], int] = Query("last"),
+    page: Union[str, int] = Query("last"),
+) -> Response:
+    return await _get_resource_field(
+        kbid,
+        rslug=rslug,
+        field_type=field_type,
+        field_id=field_id,
+        show=show,
+        extracted=extracted,
+        page=page,
+    )
+
+
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/{{field_type}}/{{field_id}}",
     status_code=200,
     name="Get Resource field (by id)",
     response_model=ResourceField,
     response_model_exclude_unset=True,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
-async def get_resource_field(
+async def get_resource_field_rid_prefix(
     request: Request,
     kbid: str,
+    rid: str,
     field_type: FieldTypeName,
     field_id: str,
-    rid: Optional[str] = None,
-    rslug: Optional[str] = None,
-    show: List[ResourceFieldProperties] = Query([ResourceFieldProperties.VALUE]),
-    extracted: List[ExtractedDataTypeName] = Query(
+    show: list[ResourceFieldProperties] = Query([ResourceFieldProperties.VALUE]),
+    extracted: list[ExtractedDataTypeName] = Query(
         [
             ExtractedDataTypeName.TEXT,
             ExtractedDataTypeName.METADATA,
             ExtractedDataTypeName.LINK,
             ExtractedDataTypeName.FILE,
         ]
     ),
-    page: Union[Literal["last", "first"], int] = Query("last"),
+    # not working with latest pydantic/fastapi
+    # page: Union[Literal["last", "first"], int] = Query("last"),
+    page: Union[str, int] = Query("last"),
 ) -> Response:
-    storage = await get_storage(service_name=SERVICE_NAME)
-    driver = await get_driver()
+    return await _get_resource_field(
+        kbid,
+        rid=rid,
+        field_type=field_type,
+        field_id=field_id,
+        show=show,
+        extracted=extracted,
+        page=page,
+    )
 
-    txn = await driver.begin()
+
+async def _get_resource_field(
+    kbid: str,
+    field_type: FieldTypeName,
+    field_id: str,
+    show: list[ResourceFieldProperties],
+    extracted: list[ExtractedDataTypeName],
+    page: Union[str, int],
+    rid: Optional[str] = None,
+    rslug: Optional[str] = None,
+) -> Response:
+    storage = await get_storage(service_name=SERVICE_NAME)
+    driver = get_driver()
 
     pb_field_id = FIELD_NAMES_TO_PB_TYPE_MAP[field_type]
 
-    kb = ORMKnowledgeBox(txn, storage, kbid)
+    async with driver.transaction() as txn:
+        kb = ORMKnowledgeBox(txn, storage, kbid)
 
-    if rid is None:
-        assert rslug is not None, "Either rid or rslug must be defined"
-        rid = await kb.get_resource_uuid_by_slug(rslug)
         if rid is None:
-            await txn.abort()
-            raise HTTPException(status_code=404, detail="Resource does not exist")
+            assert rslug is not None, "Either rid or rslug must be defined"
+            rid = await kb.get_resource_uuid_by_slug(rslug)
+            if rid is None:
+                raise HTTPException(status_code=404, detail="Resource does not exist")
+
+        resource = ORMResource(txn, storage, kb, rid)
+        field = await resource.get_field(field_id, pb_field_id, load=True)
+        if field is None:
+            raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
 
-    resource = ORMResource(txn, storage, kb, rid)
-    field = await resource.get_field(field_id, pb_field_id, load=True)
-    if field is None:
-        await txn.abort()
-        raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
-
-    resource_field = ResourceField(field_id=field_id, field_type=field_type)
-
-    if ResourceFieldProperties.VALUE in show:
-        value = await field.get_value()
-
-        if isinstance(value, resources_pb2.FieldText):
-            value = await field.get_value()
-            resource_field.value = models.FieldText.from_message(value)
+        resource_field = ResourceField(field_id=field_id, field_type=field_type)  # type: ignore
 
-        if isinstance(value, resources_pb2.FieldFile):
+        if ResourceFieldProperties.VALUE in show:
             value = await field.get_value()
-            resource_field.value = models.FieldFile.from_message(value)
-
-        if isinstance(value, resources_pb2.FieldLink):
-            value = await field.get_value()
-            resource_field.value = models.FieldLink.from_message(value)
-
-        if isinstance(value, resources_pb2.FieldLayout):
-            value = await field.get_value()
-            resource_field.value = models.FieldLayout.from_message(value)
-
-        if isinstance(value, resources_pb2.FieldDatetime):
-            value = await field.get_value()
-            resource_field.value = models.FieldDatetime.from_message(value)
-
-        if isinstance(value, resources_pb2.FieldKeywordset):
-            value = await field.get_value()
-            resource_field.value = models.FieldKeywordset.from_message(value)
-
-        if isinstance(field, Conversation):
-            if page == "first":
-                page = 1
-            elif page == "last":
-                conversation_metadata = await field.get_metadata()
-                page = conversation_metadata.pages
-
-            value = await field.get_value(page=page)
-            resource_field.value = models.Conversation.from_message(value)
-
-    if ResourceFieldProperties.EXTRACTED in show and extracted:
-        resource_field.extracted = FIELD_NAME_TO_EXTRACTED_DATA_FIELD_MAP[field_type]()
-        await set_resource_field_extracted_data(
-            field,
-            resource_field.extracted,
-            field_type,
-            extracted,
-        )
 
-    if ResourceFieldProperties.ERROR in show:
-        error = await field.get_error()
-        if error is not None:
-            resource_field.error = Error(body=error.error, code=error.code)
+            if isinstance(value, resources_pb2.FieldText):
+                value = await field.get_value()
+                resource_field.value = models.FieldText.from_message(value)
+
+            if isinstance(value, resources_pb2.FieldFile):
+                value = await field.get_value()
+                resource_field.value = models.FieldFile.from_message(value)
+
+            if isinstance(value, resources_pb2.FieldLink):
+                value = await field.get_value()
+                resource_field.value = models.FieldLink.from_message(value)
+
+            if isinstance(value, resources_pb2.FieldLayout):
+                value = await field.get_value()
+                resource_field.value = models.FieldLayout.from_message(value)
+
+            if isinstance(value, resources_pb2.FieldDatetime):
+                value = await field.get_value()
+                resource_field.value = models.FieldDatetime.from_message(value)
+
+            if isinstance(value, resources_pb2.FieldKeywordset):
+                value = await field.get_value()
+                resource_field.value = models.FieldKeywordset.from_message(value)
+
+            if isinstance(field, Conversation):
+                if page == "first":
+                    page_to_fetch = 1
+                elif page == "last":
+                    conversation_metadata = await field.get_metadata()
+                    page_to_fetch = conversation_metadata.pages
+                else:
+                    page_to_fetch = int(page)
+
+                value = await field.get_value(page=page_to_fetch)
+                if value is not None:
+                    resource_field.value = models.Conversation.from_message(value)
+
+        if ResourceFieldProperties.EXTRACTED in show and extracted:
+            resource_field.extracted = FIELD_NAME_TO_EXTRACTED_DATA_FIELD_MAP[
+                field_type
+            ]()
+            await set_resource_field_extracted_data(
+                field,
+                resource_field.extracted,
+                field_type,
+                extracted,
+            )
+
+        if ResourceFieldProperties.ERROR in show:
+            error = await field.get_error()
+            if error is not None:
+                resource_field.error = Error(body=error.error, code=error.code)
 
-    await txn.abort()
     return Response(
         content=resource_field.json(exclude_unset=True, by_alias=True),
         media_type="application/json",
     )
```

## nucliadb/reader/api/v1/services.py

```diff
@@ -13,105 +13,92 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import asyncio
+from typing import Optional, Union
+
 from fastapi import HTTPException
+from fastapi.responses import StreamingResponse
 from fastapi_versioning import version  # type: ignore
 from google.protobuf.json_format import MessageToDict
 from nucliadb_protos.knowledgebox_pb2 import KnowledgeBoxID
 from nucliadb_protos.writer_pb2 import (
     GetEntitiesGroupRequest,
     GetEntitiesGroupResponse,
-    GetEntitiesRequest,
-    GetEntitiesResponse,
     GetLabelSetRequest,
     GetLabelSetResponse,
     GetLabelsRequest,
     GetLabelsResponse,
     GetSynonymsResponse,
     GetVectorSetsRequest,
     GetVectorSetsResponse,
     ListEntitiesGroupsRequest,
     ListEntitiesGroupsResponse,
     OpStatusWriter,
 )
 from starlette.requests import Request
 
+from nucliadb.common import datamanagers
+from nucliadb.common.cluster.settings import in_standalone_mode
+from nucliadb.common.context.fastapi import get_app_context
+from nucliadb.common.http_clients import processing
+from nucliadb.common.maindb.utils import get_driver
+from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
+from nucliadb.models.responses import HTTPClientError
+from nucliadb.reader import SERVICE_NAME
 from nucliadb.reader.api.v1.router import KB_PREFIX, api
-from nucliadb_models.entities import EntitiesGroup, KnowledgeBoxEntities
+from nucliadb.reader.reader.notifications import kb_notifications_stream
+from nucliadb_models.entities import (
+    EntitiesGroup,
+    EntitiesGroupSummary,
+    KnowledgeBoxEntities,
+)
 from nucliadb_models.labels import KnowledgeBoxLabels, LabelSet
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_models.synonyms import KnowledgeBoxSynonyms
 from nucliadb_models.vectors import VectorSet, VectorSets
-from nucliadb_telemetry.utils import set_info_on_span
+from nucliadb_utils import const
 from nucliadb_utils.authentication import requires
-from nucliadb_utils.utilities import get_ingest
+from nucliadb_utils.utilities import get_ingest, get_storage, has_feature
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/entitiesgroups",
     status_code=200,
     name="Get Knowledge Box Entities",
     response_model=KnowledgeBoxEntities,
     tags=["Knowledge Box Services"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
 async def get_entities(
     request: Request, kbid: str, show_entities: bool = False
-) -> KnowledgeBoxEntities:
+) -> Union[KnowledgeBoxEntities, HTTPClientError]:
     if show_entities:
-        return await get_all_entities(kbid)
-    else:
-        return await list_entities_groups(kbid)
-
-
-async def get_all_entities(kbid: str):
-    """WARNING: this function is really costly due to how entities are retrieved
-    from the node."""
-    ingest = get_ingest()
-    e_request: GetEntitiesRequest = GetEntitiesRequest()
-    e_request.kb.uuid = kbid
-    set_info_on_span({"nuclia.kbid": kbid})
-
-    kbobj: GetEntitiesResponse = await ingest.GetEntities(e_request)  # type: ignore
-    if kbobj.status == GetEntitiesResponse.Status.OK:
-        response = KnowledgeBoxEntities(uuid=kbid)
-        for key, group in kbobj.groups.items():
-            entities_group = EntitiesGroup.from_message(group)
-            if "" in entities_group.entities:
-                del entities_group.entities[""]
-            response.groups[key] = entities_group
-        return response
-    elif kbobj.status == GetEntitiesResponse.Status.NOTFOUND:
-        raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
-    elif kbobj.status == GetEntitiesResponse.Status.ERROR:
-        raise HTTPException(
-            status_code=500, detail="Error while getting entities groups"
-        )
-    else:
-        raise HTTPException(
-            status_code=500, detail="Error on getting Knowledge box entities"
+        return HTTPClientError(
+            status_code=400,
+            detail="show_entities param is not supported. Please use /entitiesgroup/{group} instead.",
         )
+    return await list_entities_groups(kbid)
 
 
 async def list_entities_groups(kbid: str):
     ingest = get_ingest()
     e_request: ListEntitiesGroupsRequest = ListEntitiesGroupsRequest()
     e_request.kb.uuid = kbid
-    set_info_on_span({"nuclia.kbid": kbid})
 
     entities_groups = await ingest.ListEntitiesGroups(e_request)  # type: ignore
     if entities_groups.status == ListEntitiesGroupsResponse.Status.OK:
         response = KnowledgeBoxEntities(uuid=kbid)
         for key, eg_summary in entities_groups.groups.items():
-            entities_group = EntitiesGroup.from_summary_message(eg_summary)
+            entities_group = EntitiesGroupSummary.from_message(eg_summary)
             response.groups[key] = entities_group
         return response
     elif entities_groups.status == ListEntitiesGroupsResponse.Status.NOTFOUND:
         raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
     elif entities_groups.status == ListEntitiesGroupsResponse.Status.ERROR:
         raise HTTPException(
             status_code=500, detail="Error while listing entities groups"
@@ -121,26 +108,25 @@
             status_code=500, detail="Error on listing Knowledge box entities"
         )
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/entitiesgroup/{{group}}",
     status_code=200,
-    name="Get Knowledge Box Entities",
+    name="Get a Knowledge Box Entities Group",
     response_model=EntitiesGroup,
     tags=["Knowledge Box Services"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
 async def get_entity(request: Request, kbid: str, group: str) -> EntitiesGroup:
     ingest = get_ingest()
     l_request: GetEntitiesGroupRequest = GetEntitiesGroupRequest()
     l_request.kb.uuid = kbid
     l_request.group = group
-    set_info_on_span({"nuclia.kbid": kbid})
 
     kbobj: GetEntitiesGroupResponse = await ingest.GetEntitiesGroup(l_request)  # type: ignore
     if kbobj.status == GetEntitiesGroupResponse.Status.OK:
         response = EntitiesGroup.from_message(kbobj.group)
         return response
     elif kbobj.status == GetEntitiesGroupResponse.Status.KB_NOT_FOUND:
         raise HTTPException(
@@ -155,25 +141,24 @@
             status_code=500, detail="Error on getting entities group on a Knowledge box"
         )
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/labelsets",
     status_code=200,
-    name="Get Knowledge Box Labels",
+    name="Get Knowledge Box Label Sets",
     response_model=KnowledgeBoxLabels,
     tags=["Knowledge Box Services"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
-async def get_labels(request: Request, kbid: str) -> KnowledgeBoxLabels:
+async def get_labelsets(request: Request, kbid: str) -> KnowledgeBoxLabels:
     ingest = get_ingest()
     l_request: GetLabelsRequest = GetLabelsRequest()
     l_request.kb.uuid = kbid
-    set_info_on_span({"nuclia.kbid": kbid})
 
     kbobj: GetLabelsResponse = await ingest.GetLabels(l_request)  # type: ignore
     if kbobj.status == GetLabelsResponse.Status.OK:
         response = KnowledgeBoxLabels(uuid=kbid)
         for labelset, labelset_data in kbobj.labels.labelset.items():
             labelset_response = LabelSet(
                 **MessageToDict(
@@ -191,26 +176,25 @@
             status_code=500, detail="Error on getting Knowledge box labels"
         )
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/labelset/{{labelset}}",
     status_code=200,
-    name="Get Knowledge Box Label",
+    name="Get a Knowledge Box Label Set",
     response_model=LabelSet,
     tags=["Knowledge Box Services"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
-async def get_label(request: Request, kbid: str, labelset: str) -> LabelSet:
+async def get_labelset(request: Request, kbid: str, labelset: str) -> LabelSet:
     ingest = get_ingest()
     l_request: GetLabelSetRequest = GetLabelSetRequest()
     l_request.kb.uuid = kbid
     l_request.labelset = labelset
-    set_info_on_span({"nuclia.kbid": kbid})
 
     kbobj: GetLabelSetResponse = await ingest.GetLabelSet(l_request)  # type: ignore
     if kbobj.status == GetLabelSetResponse.Status.OK:
         response = LabelSet(
             **MessageToDict(
                 kbobj.labelset,
                 preserving_proto_field_name=True,
@@ -225,28 +209,31 @@
             status_code=500, detail="Error on getting labelset on a Knowledge box"
         )
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/vectorsets",
     status_code=200,
-    name="Get Knowledge Box VectorSet",
+    name="Get Knowledge Box Vector Sets",
     tags=["Knowledge Box Services"],
     response_model=VectorSets,
     openapi_extra={"x-operation_order": 1},
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
 async def get_vectorsets(request: Request, kbid: str):
+    if not has_feature(const.Features.VECTORSETS_V2, context={"kbid": kbid}):
+        raise HTTPException(
+            status_code=404,
+            detail="Vectorsets API is not yet implemented",
+        )
     ingest = get_ingest()
     pbrequest: GetVectorSetsRequest = GetVectorSetsRequest()
     pbrequest.kb.uuid = kbid
 
-    set_info_on_span({"nuclia.kbid": kbid})
-
     vectorsets: GetVectorSetsResponse = await ingest.GetVectorSets(pbrequest)  # type: ignore
     if vectorsets.status == GetVectorSetsResponse.Status.OK:
         result = VectorSets(vectorsets={})
         for key, vector in vectorsets.vectorsets.vectorsets.items():
             result.vectorsets[key] = VectorSet.from_message(vector)
         return result
     elif vectorsets.status == GetVectorSetsResponse.Status.NOTFOUND:
@@ -264,19 +251,122 @@
     tags=["Knowledge Box Services"],
     response_model=KnowledgeBoxSynonyms,
     openapi_extra={"x-operation_order": 2},
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
 async def get_custom_synonyms(request: Request, kbid: str):
-    set_info_on_span({"nuclia.kbid": kbid})
     ingest = get_ingest()
     pbrequest = KnowledgeBoxID(uuid=kbid)
     pbresponse: GetSynonymsResponse = await ingest.GetSynonyms(pbrequest)  # type: ignore
     if pbresponse.status.status == OpStatusWriter.Status.OK:
         return KnowledgeBoxSynonyms.from_message(pbresponse.synonyms)
     elif pbresponse.status.status == OpStatusWriter.Status.NOTFOUND:
         raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
     elif pbresponse.status.status == OpStatusWriter.Status.ERROR:
         raise HTTPException(
             status_code=500, detail="Error getting synonyms of a Knowledge box"
         )
+
+
+@api.get(
+    f"/{KB_PREFIX}/{{kbid}}/notifications",
+    status_code=200,
+    name="Knowledge Box Notifications Stream",
+    description="Provides a stream of activity notifications for the given Knowledge Box. The stream will be automatically closed after 2 minutes.",  # noqa: E501
+    tags=["Knowledge Box Services"],
+    response_description="Each line of the response is a Base64-encoded JSON object representing a notification. Refer to [the internal documentation](https://github.com/nuclia/nucliadb/blob/main/docs/tutorials/KB_NOTIFICATIONS.md) for a more detailed explanation of each notification type.",  # noqa: E501
+    response_model=None,
+    responses={"404": {"description": "Knowledge Box not found"}},
+)
+@requires(NucliaDBRoles.READER)
+@version(1)
+async def notifications_endpoint(
+    request: Request, kbid: str
+) -> Union[StreamingResponse, HTTPClientError]:
+    if in_standalone_mode():
+        return HTTPClientError(
+            status_code=404,
+            detail="Notifications are only available in the cloud offering of NucliaDB.",
+        )
+
+    context = get_app_context(request.app)
+
+    if not await exists_kb(kbid=kbid):
+        return HTTPClientError(status_code=404, detail="Knowledge Box not found")
+
+    response = StreamingResponse(
+        content=kb_notifications_stream(context, kbid),
+        status_code=200,
+        media_type="binary/octet-stream",
+    )
+
+    return response
+
+
+async def exists_kb(kbid: str) -> bool:
+    async with datamanagers.with_transaction(read_only=True) as txn:
+        return await datamanagers.kb.exists_kb(txn, kbid=kbid)
+
+
+@api.get(
+    f"/{KB_PREFIX}/{{kbid}}/processing-status",
+    status_code=200,
+    name="Knowledge Box Processing Status",
+    description="Provides the status of the processing of the given Knowledge Box.",
+    tags=["Knowledge Box Services"],
+    response_model=processing.RequestsResults,
+    responses={
+        "404": {"description": "Knowledge Box not found"},
+    },
+)
+@requires(NucliaDBRoles.READER)
+@version(1)
+async def processing_status(
+    request: Request,
+    kbid: str,
+    cursor: Optional[str] = None,
+    scheduled: Optional[bool] = None,
+    limit: int = 20,
+) -> Union[processing.RequestsResults, HTTPClientError]:
+    if not await exists_kb(kbid=kbid):
+        return HTTPClientError(status_code=404, detail="Knowledge Box not found")
+
+    async with processing.ProcessingHTTPClient() as client:
+        results = await client.requests(
+            cursor=cursor, scheduled=scheduled, kbid=kbid, limit=limit
+        )
+
+    storage = await get_storage(service_name=SERVICE_NAME)
+    driver = get_driver()
+
+    async with driver.transaction(wait_for_abort=False, read_only=True) as txn:
+        kb = KnowledgeBox(txn, storage, kbid)
+
+        max_simultaneous = asyncio.Semaphore(10)
+
+        async def _composition(
+            result: processing.RequestsResult,
+        ) -> Optional[processing.RequestsResult]:
+            async with max_simultaneous:
+                resource = await kb.get(result.resource_id)
+                if resource is None:
+                    return None
+
+                basic = await resource.get_basic()
+                if basic is None:
+                    return None
+
+                result.title = basic.title
+                return result
+
+        result_items = [
+            item
+            for item in await asyncio.gather(
+                *[_composition(result) for result in results.results]
+            )
+            if item is not None
+        ]
+
+    # overwrite result with only resources that exist in the database.
+    results.results = result_items
+    return results
```

## nucliadb/reader/tests/conftest.py

```diff
@@ -15,14 +15,17 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 pytest_plugins = [
     "pytest_docker_fixtures",
-    "nucliadb.ingest.tests.fixtures",
+    "nucliadb.tests.fixtures",
+    "nucliadb.tests.tikv",
+    "nucliadb.ingest.tests.fixtures",  # should be refactored out
     "nucliadb.reader.tests.fixtures",
     "nucliadb_utils.tests.nats",
+    "nucliadb_utils.tests.conftest",
     "nucliadb_utils.tests.gcs",
     "nucliadb_utils.tests.s3",
     "nucliadb_utils.tests.asyncbenchmark",
 ]
```

## nucliadb/reader/tests/fixtures.py

```diff
@@ -16,24 +16,23 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import uuid
 from datetime import datetime
 from enum import Enum
-from typing import List, Optional
+from typing import Optional
 
 import pytest
 from httpx import AsyncClient
 from nucliadb_protos.writer_pb2 import BrokerMessage
-from starlette.routing import Mount
 
 from nucliadb.ingest.orm.resource import KB_RESOURCE_SLUG_BASE
 from nucliadb.reader import API_PREFIX
-from nucliadb_utils.utilities import Utility, clear_global_cache, set_utility
+from nucliadb_utils.utilities import clear_global_cache
 
 
 @pytest.fixture(scope="function")
 def test_settings_reader(cache, gcs, fake_node, maindb_driver):  # type: ignore
     from nucliadb_utils.settings import (
         FileBackendConfig,
         running_settings,
@@ -43,32 +42,25 @@
     running_settings.debug = False
     print(f"Driver ready at {maindb_driver.url}")
 
     storage_settings.gcs_endpoint_url = gcs
     storage_settings.file_backend = FileBackendConfig.GCS
     storage_settings.gcs_bucket = "test"
 
-    set_utility(Utility.CACHE, cache)
     yield
 
 
 @pytest.fixture(scope="function")
-async def reader_api(test_settings_reader: None, local_files, event_loop):  # type: ignore
-    from nucliadb.reader.app import application
+async def reader_api(test_settings_reader: None, local_files):  # type: ignore
+    from nucliadb.reader.app import create_application
 
-    async def handler(req, exc):  # type: ignore
-        raise exc
-
-    # Little hack to raise exeptions from VersionedFastApi
-    for route in application.routes:
-        if isinstance(route, Mount):
-            route.app.middleware_stack.handler = handler  # type: ignore
+    application = create_application()
 
     def make_client_fixture(
-        roles: Optional[List[Enum]] = None,
+        roles: Optional[list[Enum]] = None,
         user: str = "",
         version: str = "1",
     ) -> AsyncClient:
         roles = roles or []
         client_base_url = "http://test"
         client_base_url = f"{client_base_url}/{API_PREFIX}/v{version}"
 
@@ -104,32 +96,31 @@
     message1.basic.modified.FromDatetime(datetime.utcnow())
     message1.source = BrokerMessage.MessageSource.WRITER
 
     return message1
 
 
 @pytest.fixture(scope="function")
-async def test_pagination_resources(
-    processor, knowledgebox_ingest, test_settings_reader
-):
+async def test_resources(processor, knowledgebox_ingest, test_settings_reader):
     """
     Create a set of resources with only basic information to test pagination
     """
-
+    resources = []
     amount = 10
     for i in range(1, 10 + 1):
         message = broker_simple_resource(knowledgebox_ingest, i)
         await processor.process(message=message, seqid=i)
+        resources.append(message.uuid)
         # Give processed data some time to reach the node
 
     from time import time
 
-    from nucliadb.ingest.utils import get_driver
+    from nucliadb.common.maindb.utils import get_driver
 
-    driver = await get_driver()
+    driver = get_driver()
 
     t0 = time()
 
     while time() - t0 < 30:  # wait max 30 seconds for it
         txn = await driver.begin()
         count = 0
         async for key in txn.keys(
@@ -138,8 +129,8 @@
             count += 1
 
         await txn.abort()
         if count == amount:
             break
         print(f"got {count}, retrying")
 
-    yield knowledgebox_ingest
+    yield knowledgebox_ingest, resources
```

## nucliadb/reader/tests/test_list_resources.py

```diff
@@ -42,21 +42,21 @@
 @pytest.mark.asyncio
 @pytest.mark.parametrize(
     "page, size, expected_resources_count, expected_is_last_page",
     PAGINATION_TEST_SCENARIOS,
 )
 async def test_list_resources(
     reader_api: Callable[..., AsyncClient],
-    test_pagination_resources: str,
+    test_resources: tuple[str, list[str]],
     page: Optional[int],
     size: Optional[int],
     expected_resources_count: int,
     expected_is_last_page: bool,
 ) -> None:
-    kbid = test_pagination_resources
+    kbid = test_resources[0]
 
     query_params = {}
     if page is not None:
         query_params["page"] = page
 
     if size is not None:
         query_params["size"] = size
```

## nucliadb/reader/tests/test_reader_file_download.py

```diff
@@ -23,15 +23,15 @@
 import pytest
 from httpx import AsyncClient
 from nucliadb_protos.resources_pb2 import FieldType
 
 import nucliadb.ingest.tests.fixtures
 from nucliadb.ingest.orm.resource import Resource
 from nucliadb.ingest.tests.fixtures import TEST_CLOUDFILE, THUMBNAIL
-from nucliadb.reader.api.v1.download import parse_media_range
+from nucliadb.reader.api.v1.download import parse_media_range, safe_http_header_encode
 from nucliadb.reader.api.v1.router import KB_PREFIX, RESOURCE_PREFIX, RSLUG_PREFIX
 from nucliadb_models.resource import NucliaDBRoles
 
 BASE = ("field_id", "field_type")
 VALUE = ("value",)
 EXTRACTED = ("extracted",)
 
@@ -260,7 +260,14 @@
         resp = await client.get(download_url)
         assert resp.status_code == 200
         assert resp.headers["Content-Disposition"].startswith("attachment; filename=")
 
         resp = await client.get(f"{download_url}?inline=true")
         assert resp.status_code == 200
         assert resp.headers["Content-Disposition"] == "inline"
+
+
+@pytest.mark.parametrize("text", ["ÇŞĞIİÖÜ"])
+def test_safe_http_header_encode(text):
+    safe_text = safe_http_header_encode(text)
+    # This is how startette encodes the headers
+    safe_text.lower().encode("latin-1")
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## nucliadb/reader/tests/test_reader_resource.py

```diff
@@ -347,33 +347,7 @@
         assert resp.status_code == 200
 
         resource = resp.json()
         metadata = resource["data"]["texts"]["text1"]["extracted"]["metadata"][
             "metadata"
         ]
         assert metadata["positions"]["ENTITY/document"]["entity"] == "document"
-
-
-@pytest.mark.asyncio
-async def test_get_resource_extracted_uservectors(
-    reader_api: Callable[..., AsyncClient], test_resource: Resource
-):
-    rsc = test_resource
-    kbid = rsc.kb.kbid
-    rid = rsc.uuid
-
-    async with reader_api(roles=[NucliaDBRoles.READER]) as client:
-        resp = await client.get(
-            f"/{KB_PREFIX}/{kbid}/{RESOURCE_PREFIX}/{rid}",
-            params={
-                "show": ["extracted"],
-                "extracted": [
-                    "uservectors",
-                ],
-            },
-        )
-        assert resp.status_code == 200
-
-        resource = resp.json()
-        assert resource["data"]["datetimes"]["datetime1"]["extracted"]["uservectors"][
-            "vectors"
-        ]["vectorset1"]["vectors"]["vector1"]["vector"] == [0.1, 0.2, 0.3]
```

## nucliadb/search/app.py

```diff
@@ -22,38 +22,54 @@
 from fastapi.responses import JSONResponse
 from starlette.middleware import Middleware
 from starlette.middleware.authentication import AuthenticationMiddleware
 from starlette.middleware.cors import CORSMiddleware
 from starlette.requests import ClientDisconnect, Request
 from starlette.responses import HTMLResponse
 
-from nucliadb.ingest.orm import NODES
+from nucliadb.common.cluster import manager
+from nucliadb.middleware import ProcessTimeHeaderMiddleware
+from nucliadb.middleware.transaction import ReadOnlyTransactionMiddleware
 from nucliadb.search import API_PREFIX
 from nucliadb.search.api.v1.router import api as api_v1
 from nucliadb.search.lifecycle import finalize, initialize
 from nucliadb.search.settings import settings
 from nucliadb_telemetry import errors
-from nucliadb_utils.authentication import STFAuthenticationBackend
+from nucliadb_utils import const
+from nucliadb_utils.authentication import NucliaCloudAuthenticationBackend
 from nucliadb_utils.fastapi.openapi import extend_openapi
 from nucliadb_utils.fastapi.versioning import VersionedFastAPI
 from nucliadb_utils.settings import http_settings, running_settings
+from nucliadb_utils.utilities import has_feature
 
-middleware = [
-    Middleware(
-        CORSMiddleware,
-        allow_origins=http_settings.cors_origins,
-        allow_methods=["*"],
-        allow_headers=["*"],
-    ),
-    Middleware(
-        AuthenticationMiddleware,
-        backend=STFAuthenticationBackend(),
-    ),
-]
+middleware = []
 
+if has_feature(const.Features.CORS_MIDDLEWARE, default=False):
+    middleware.append(
+        Middleware(
+            CORSMiddleware,
+            allow_origins=http_settings.cors_origins,
+            allow_methods=["*"],
+            # Authorization will be exluded from * in the future, (CORS non-wildcard request-header).
+            # Browsers already showing deprecation notices, so it needs to be specified explicitly
+            allow_headers=["*", "Authorization"],
+        )
+    )
+
+middleware.extend(
+    [
+        Middleware(
+            AuthenticationMiddleware, backend=NucliaCloudAuthenticationBackend()
+        ),
+        Middleware(ReadOnlyTransactionMiddleware),
+    ]
+)
+
+if running_settings.debug:
+    middleware.append(Middleware(ProcessTimeHeaderMiddleware))
 
 errors.setup_error_handling(pkg_resources.get_distribution("nucliadb").version)
 
 
 on_startup = [initialize]
 on_shutdown = [finalize]
 
@@ -82,15 +98,14 @@
         Exception: global_exception_handler,
         ClientDisconnect: client_disconnect_handler,
     },
 )
 
 
 base_app = FastAPI(title="NucliaDB Search API", **fastapi_settings)  # type: ignore
-
 base_app.include_router(api_v1)
 
 extend_openapi(base_app)
 
 application = VersionedFastAPI(
     base_app,
     version_format="{major}",
@@ -101,41 +116,43 @@
 )
 
 
 async def homepage(request: Request) -> HTMLResponse:
     return HTMLResponse("NucliaDB Search Service")
 
 
-async def chitchat_members(request: Request) -> JSONResponse:
+async def node_members(request: Request) -> JSONResponse:
     return JSONResponse(
         [
             {
-                "id": node_id,
+                "id": node.id,
                 "listen_address": node.address,
                 "type": node.label,
                 "shard_count": node.shard_count,
+                "available_disk": node.available_disk,
                 "dummy": node.dummy,
+                "primary_id": node.primary_id,
             }
-            for node_id, node in NODES.items()
+            for node in manager.get_index_nodes(include_secondary=True)
         ]
     )
 
 
 async def alive(request: Request) -> JSONResponse:
-    if len(NODES) == 0 and settings.driver != "local":
+    if len(manager.get_index_nodes()) == 0 and settings.driver != "local":
         return JSONResponse({"status": "error"}, status_code=503)
     else:
         return JSONResponse({"status": "ok"})
 
 
 async def ready(request: Request) -> JSONResponse:
     """
     Right now, they are the same, but we might want to add more
     """
     return await alive(request)
 
 
 # Use raw starlette routes to avoid unnecessary overhead
 application.add_route("/", homepage)
-application.add_route("/chitchat/members", chitchat_members)
+application.add_route("/node/members", node_members)
 application.add_route("/health/alive", alive)
 application.add_route("/health/ready", ready)
```

## nucliadb/search/lifecycle.py

```diff
@@ -13,54 +13,46 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from nucliadb.ingest.chitchat import start_chitchat, stop_chitchat
-from nucliadb.ingest.orm.nodes_manager import NodesManager
-from nucliadb.ingest.utils import get_driver  # type: ignore
+from nucliadb.common.cluster.utils import setup_cluster, teardown_cluster
+from nucliadb.common.maindb.utils import setup_driver  # type: ignore
 from nucliadb.ingest.utils import start_ingest, stop_ingest
 from nucliadb.search import SERVICE_NAME
 from nucliadb.search.predict import start_predict_engine
-from nucliadb.search.search import paragraphs
 from nucliadb_telemetry.utils import clean_telemetry, setup_telemetry
 from nucliadb_utils.utilities import (
     Utility,
     clean_utility,
     finalize_utilities,
     get_utility,
-    set_utility,
     start_audit_utility,
     stop_audit_utility,
 )
 
 
 async def initialize() -> None:
     await setup_telemetry(SERVICE_NAME)
 
     await start_ingest(SERVICE_NAME)
     await start_predict_engine()
 
-    driver = await get_driver()
-    set_utility(Utility.NODES, NodesManager(driver=driver))
-    await start_chitchat(SERVICE_NAME)
-
-    await paragraphs.initialize_cache()
+    await setup_driver()
+    await setup_cluster()
 
     await start_audit_utility(SERVICE_NAME)
 
 
 async def finalize() -> None:
     await stop_ingest()
     if get_utility(Utility.PARTITION):
         clean_utility(Utility.PARTITION)
     if get_utility(Utility.PREDICT):
         clean_utility(Utility.PREDICT)
-    if get_utility(Utility.NODES):
-        clean_utility(Utility.NODES)
 
     await finalize_utilities()
     await stop_audit_utility()
-    await stop_chitchat()
+    await teardown_cluster()
     await clean_telemetry(SERVICE_NAME)
```

## nucliadb/search/openapi.py

```diff
@@ -14,60 +14,15 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
-import datetime
-import json
-import sys
 
-from fastapi.openapi.utils import get_openapi
-from starlette.routing import Mount
+from nucliadb import openapi
 
-from nucliadb.search import API_PREFIX
 
-
-def is_versioned_route(route):
-    return isinstance(route, Mount) and route.path.startswith(f"/{API_PREFIX}/v")
-
-
-def extract_openapi(application, version, commit_id):
-    app = [
-        route.app
-        for route in application.routes
-        if is_versioned_route(route) and route.app.version == version
-    ][0]
-    document = get_openapi(
-        title=app.title,
-        version=app.version,
-        openapi_version=app.openapi_version,
-        description=app.description,
-        terms_of_service=app.terms_of_service,
-        contact=app.contact,
-        license_info=app.license_info,
-        routes=app.routes,
-        tags=app.openapi_tags,
-        servers=app.servers,
-    )
-
-    document["x-metadata"] = {
-        "nucliadb_search": {
-            "commit": commit_id,
-            "last_updated": datetime.datetime.utcnow().isoformat(),
-        }
-    }
-    return document
-
-
-def command_extract_openapi():  # pragma: no cover
+def command_extract_openapi():
     from nucliadb.search.app import application
 
-    openapi_json_path = sys.argv[1]
-    api_version = sys.argv[2]
-    commit_id = sys.argv[3]
-
-    json.dump(
-        extract_openapi(application, api_version, commit_id),
-        open(openapi_json_path, "w"),
-    )
+    openapi.command_extract_openapi(application, "nucliadb_search")
```

## nucliadb/search/predict.py

```diff
@@ -15,312 +15,585 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import json
 import os
-from typing import AsyncIterator, Dict, List, Optional, Tuple
+from enum import Enum
+from typing import AsyncIterator, Optional
+from unittest.mock import AsyncMock, Mock
 
 import aiohttp
+import backoff
 from nucliadb_protos.utils_pb2 import RelationNode
 
 from nucliadb.ingest.tests.vectors import Q, Qm2023
 from nucliadb.search import logger
-from nucliadb_models.search import ChatModel, FeedbackRequest, RephraseModel
+from nucliadb_models.search import (
+    AskDocumentModel,
+    ChatModel,
+    FeedbackRequest,
+    Ner,
+    QueryInfo,
+    RephraseModel,
+    SentenceSearch,
+    SummarizedResource,
+    SummarizedResponse,
+    SummarizeModel,
+    TokenSearch,
+)
 from nucliadb_telemetry import metrics
+from nucliadb_utils import const
 from nucliadb_utils.exceptions import LimitsExceededError
 from nucliadb_utils.settings import nuclia_settings
-from nucliadb_utils.utilities import Utility, set_utility
+from nucliadb_utils.utilities import Utility, has_feature, set_utility
 
 
 class SendToPredictError(Exception):
     pass
 
 
+class ProxiedPredictAPIError(Exception):
+    def __init__(self, status: int, detail: str = ""):
+        self.status = status
+        self.detail = detail
+
+
 class PredictVectorMissing(Exception):
     pass
 
 
+class NUAKeyMissingError(Exception):
+    pass
+
+
+class RephraseError(Exception):
+    pass
+
+
+class RephraseMissingContextError(Exception):
+    pass
+
+
 DUMMY_RELATION_NODE = [
     RelationNode(value="Ferran", ntype=RelationNode.NodeType.ENTITY, subtype="PERSON"),
     RelationNode(
         value="Joan Antoni", ntype=RelationNode.NodeType.ENTITY, subtype="PERSON"
     ),
 ]
 
 DUMMY_REPHRASE_QUERY = "This is a rephrased query"
 DUMMY_LEARNING_ID = "00"
 
 
 PUBLIC_PREDICT = "/api/v1/predict"
 PRIVATE_PREDICT = "/api/internal/predict"
+VERSIONED_PRIVATE_PREDICT = "/api/v1/internal/predict"
 SENTENCE = "/sentence"
 TOKENS = "/tokens"
+QUERY = "/query"
+SUMMARIZE = "/summarize"
 CHAT = "/chat"
+ASK_DOCUMENT = "/ask_document"
 REPHRASE = "/rephrase"
 FEEDBACK = "/feedback"
 
+NUCLIA_LEARNING_ID_HEADER = "NUCLIA-LEARNING-ID"
+
 
 predict_observer = metrics.Observer(
     "predict_engine",
     labels={"type": ""},
     error_mappings={
         "over_limits": LimitsExceededError,
         "predict_api_error": SendToPredictError,
         "empty_vectors": PredictVectorMissing,
     },
 )
 
 
+RETRIABLE_EXCEPTIONS = (aiohttp.client_exceptions.ClientConnectorError,)
+MAX_TRIES = 2
+
+
+class AnswerStatusCode(str, Enum):
+    SUCCESS = "0"
+    ERROR = "-1"
+    NO_CONTEXT = "-2"
+
+
 async def start_predict_engine():
     if nuclia_settings.dummy_predict:
         predict_util = DummyPredictEngine()
     else:
         predict_util = PredictEngine(
             nuclia_settings.nuclia_inner_predict_url,
             nuclia_settings.nuclia_public_url,
             nuclia_settings.nuclia_service_account,
             nuclia_settings.nuclia_zone,
             nuclia_settings.onprem,
+            nuclia_settings.local_predict,
+            nuclia_settings.local_predict_headers,
         )
     await predict_util.initialize()
     set_utility(Utility.PREDICT, predict_util)
 
 
-def convert_relations(data: Dict[str, List[Dict[str, str]]]) -> List[RelationNode]:
+def convert_relations(data: dict[str, list[dict[str, str]]]) -> list[RelationNode]:
     result = []
     for token in data["tokens"]:
         text = token["text"]
         klass = token["ner"]
         result.append(
             RelationNode(value=text, ntype=RelationNode.NodeType.ENTITY, subtype=klass)
         )
     return result
 
 
-class DummyPredictEngine:
-    def __init__(self):
-        self.calls = []
-
-    async def initialize(self):
-        pass
-
-    async def finalize(self):
-        pass
-
-    async def send_feedback(
-        self,
-        kbid: str,
-        item: FeedbackRequest,
-        x_nucliadb_user: str,
-        x_ndb_client: str,
-        x_forwarded_for: str,
-    ):
-        self.calls.append(item)
-        return
-
-    async def rephrase_query(self, kbid: str, item: RephraseModel) -> str:
-        self.calls.append(item)
-        return DUMMY_REPHRASE_QUERY
-
-    async def chat_query(
-        self, kbid: str, item: ChatModel
-    ) -> Tuple[str, AsyncIterator[bytes]]:
-        self.calls.append(item)
-
-        async def generate():
-            for i in [b"valid ", b"answer ", b" to"]:
-                yield i
-
-        return (DUMMY_LEARNING_ID, generate())
-
-    async def convert_sentence_to_vector(self, kbid: str, sentence: str) -> List[float]:
-        self.calls.append(sentence)
-        if (
-            os.environ.get("TEST_SENTENCE_ENCODER") == "multilingual-2023-02-21"
-        ):  # pragma: no cover
-            return Qm2023
-        else:
-            return Q
-
-    async def detect_entities(self, kbid: str, sentence: str) -> List[RelationNode]:
-        self.calls.append(sentence)
-        dummy_data = os.environ.get("TEST_RELATIONS", None)
-        if dummy_data is not None:  # pragma: no cover
-            return convert_relations(json.loads(dummy_data))
-        else:
-            return DUMMY_RELATION_NODE
-
-
 class PredictEngine:
     def __init__(
         self,
         cluster_url: Optional[str] = None,
         public_url: Optional[str] = None,
         nuclia_service_account: Optional[str] = None,
         zone: Optional[str] = None,
         onprem: bool = False,
+        local_predict: bool = False,
+        local_predict_headers: Optional[dict[str, str]] = None,
     ):
         self.nuclia_service_account = nuclia_service_account
         self.cluster_url = cluster_url
         if public_url is not None:
             self.public_url: Optional[str] = public_url.format(zone=zone)
         else:
             self.public_url = None
         self.zone = zone
         self.onprem = onprem
+        self.local_predict = local_predict
+        self.local_predict_headers = local_predict_headers
 
     async def initialize(self):
         self.session = aiohttp.ClientSession()
 
     async def finalize(self):
         await self.session.close()
 
-    async def check_response(self, resp, expected: int = 200) -> None:
-        if resp.status == expected:
+    def check_nua_key_is_configured_for_onprem(self):
+        if self.onprem and (
+            self.nuclia_service_account is None and self.local_predict is False
+        ):
+            raise NUAKeyMissingError()
+
+    def get_predict_url(self, endpoint: str, kbid: str) -> str:
+        if not endpoint.startswith("/"):
+            endpoint = "/" + endpoint
+        if self.onprem:
+            # On-prem NucliaDB uses the public URL for the predict API. Examples:
+            # /api/v1/predict/chat/{kbid}
+            # /api/v1/predict/rephrase/{kbid}
+            return f"{self.public_url}{PUBLIC_PREDICT}{endpoint}/{kbid}"
+        else:
+            if has_feature(const.Features.VERSIONED_PRIVATE_PREDICT):
+                return f"{self.cluster_url}{VERSIONED_PRIVATE_PREDICT}{endpoint}"
+            else:
+                return f"{self.cluster_url}{PRIVATE_PREDICT}{endpoint}"
+
+    def get_predict_headers(self, kbid: str) -> dict[str, str]:
+        if self.onprem:
+            headers = {"X-STF-NUAKEY": f"Bearer {self.nuclia_service_account}"}
+            if self.local_predict_headers is not None:
+                headers.update(self.local_predict_headers)
+            return headers
+        else:
+            return {"X-STF-KBID": kbid}
+
+    async def check_response(
+        self, resp: aiohttp.ClientResponse, expected_status: int = 200
+    ) -> None:
+        if resp.status == expected_status:
             return
+
         if resp.status == 402:
             data = await resp.json()
             raise LimitsExceededError(402, data["detail"])
-        else:
-            raise SendToPredictError(f"{resp.status}: {await resp.read()}")
+
+        try:
+            data = await resp.json()
+            try:
+                detail = data["detail"]
+            except (KeyError, TypeError):
+                detail = data
+        except (
+            json.decoder.JSONDecodeError,
+            aiohttp.client_exceptions.ContentTypeError,
+        ):
+            detail = await resp.text()
+        logger.error(f"Predict API error at {resp.url}: {detail}")
+        raise ProxiedPredictAPIError(status=resp.status, detail=detail)
+
+    @backoff.on_exception(
+        backoff.expo,
+        RETRIABLE_EXCEPTIONS,
+        jitter=backoff.random_jitter,
+        max_tries=MAX_TRIES,
+    )
+    async def make_request(self, method: str, **request_args):
+        func = getattr(self.session, method.lower())
+        return await func(**request_args)
 
     @predict_observer.wrap({"type": "feedback"})
     async def send_feedback(
         self,
         kbid: str,
         item: FeedbackRequest,
         x_nucliadb_user: str,
         x_ndb_client: str,
         x_forwarded_for: str,
     ):
+        try:
+            self.check_nua_key_is_configured_for_onprem()
+        except NUAKeyMissingError:
+            logger.warning(
+                "Nuclia Service account is not defined so could not send the feedback"
+            )
+            return
+
         data = item.dict()
         data["user_id"] = x_nucliadb_user
         data["client"] = x_ndb_client
         data["forwarded"] = x_forwarded_for
 
-        if self.onprem is False:
-            # Upload the payload
-            resp = await self.session.post(
-                url=f"{self.cluster_url}{PRIVATE_PREDICT}{FEEDBACK}",
-                json=data,
-                headers={"X-STF-KBID": kbid},
-            )
-        else:
-            if self.nuclia_service_account is None:
-                logger.warning(
-                    "Nuclia Service account is not defined so could not send the feedback"
-                )
-                return
-            # Upload the payload
-            headers = {"X-STF-NUAKEY": f"Bearer {self.nuclia_service_account}"}
-            resp = await self.session.post(
-                url=f"{self.public_url}{PUBLIC_PREDICT}{FEEDBACK}",
-                json=data,
-                headers=headers,
-            )
-        await self.check_response(resp, expected=204)
+        resp = await self.make_request(
+            "POST",
+            url=self.get_predict_url(FEEDBACK, kbid),
+            json=data,
+            headers=self.get_predict_headers(kbid),
+        )
+        await self.check_response(resp, expected_status=204)
 
     @predict_observer.wrap({"type": "rephrase"})
     async def rephrase_query(self, kbid: str, item: RephraseModel) -> str:
-        if self.onprem is False:
-            # Upload the payload
-            resp = await self.session.post(
-                url=f"{self.cluster_url}{PRIVATE_PREDICT}{REPHRASE}",
-                json=item.dict(),
-                headers={"X-STF-KBID": kbid},
-            )
-        else:
-            if self.nuclia_service_account is None:
-                error = (
-                    "Nuclia Service account is not defined so could not rephrase query"
-                )
-                logger.warning(error)
-                raise SendToPredictError(error)
-            # Upload the payload
-            headers = {"X-STF-NUAKEY": f"Bearer {self.nuclia_service_account}"}
-            resp = await self.session.post(
-                url=f"{self.public_url}{PUBLIC_PREDICT}{REPHRASE}",
-                json=item.dict(),
-                headers=headers,
-            )
-        await self.check_response(resp, expected=200)
-        return await resp.text()
+        try:
+            self.check_nua_key_is_configured_for_onprem()
+        except NUAKeyMissingError:
+            error = "Nuclia Service account is not defined so could not rephrase query"
+            logger.warning(error)
+            raise SendToPredictError(error)
+
+        resp = await self.make_request(
+            "POST",
+            url=self.get_predict_url(REPHRASE, kbid),
+            json=item.dict(),
+            headers=self.get_predict_headers(kbid),
+        )
+        await self.check_response(resp, expected_status=200)
+        return await _parse_rephrase_response(resp)
 
     @predict_observer.wrap({"type": "chat"})
     async def chat_query(
         self, kbid: str, item: ChatModel
-    ) -> Tuple[str, AsyncIterator[bytes]]:
-        if self.onprem is False:
-            # Upload the payload
-            resp = await self.session.post(
-                url=f"{self.cluster_url}{PRIVATE_PREDICT}{CHAT}",
-                json=item.dict(),
-                headers={"X-STF-KBID": kbid},
-            )
-        else:
-            if self.nuclia_service_account is None:
-                error = "Nuclia Service account is not defined so could not retrieve vectors for the query"
-                logger.warning(error)
-                raise SendToPredictError(error)
-            # Upload the payload
-            headers = {"X-STF-NUAKEY": f"Bearer {self.nuclia_service_account}"}
-            resp = await self.session.post(
-                url=f"{self.public_url}{PUBLIC_PREDICT}{CHAT}",
-                json=item.dict(),
-                headers=headers,
+    ) -> tuple[str, AsyncIterator[bytes]]:
+        try:
+            self.check_nua_key_is_configured_for_onprem()
+        except NUAKeyMissingError:
+            error = "Nuclia Service account is not defined so the chat operation could not be performed"
+            logger.warning(error)
+            raise SendToPredictError(error)
+
+        resp = await self.make_request(
+            "POST",
+            url=self.get_predict_url(CHAT, kbid),
+            json=item.dict(),
+            headers=self.get_predict_headers(kbid),
+            timeout=None,
+        )
+        await self.check_response(resp, expected_status=200)
+        ident = resp.headers.get(NUCLIA_LEARNING_ID_HEADER)
+        return ident, get_answer_generator(resp)
+
+    @predict_observer.wrap({"type": "ask_document"})
+    async def ask_document(
+        self, kbid: str, question: str, blocks: list[list[str]], user_id: str
+    ) -> str:
+        try:
+            self.check_nua_key_is_configured_for_onprem()
+        except NUAKeyMissingError:
+            error = "Nuclia Service account is not defined so could not ask document"
+            logger.warning(error)
+            raise SendToPredictError(error)
+
+        item = AskDocumentModel(question=question, blocks=blocks, user_id=user_id)
+        resp = await self.make_request(
+            "POST",
+            url=self.get_predict_url(ASK_DOCUMENT, kbid),
+            json=item.dict(),
+            headers=self.get_predict_headers(kbid),
+            timeout=None,
+        )
+        await self.check_response(resp, expected_status=200)
+        return await resp.text()
+
+    @predict_observer.wrap({"type": "query"})
+    async def query(
+        self,
+        kbid: str,
+        sentence: str,
+        generative_model: Optional[str] = None,
+        rephrase: Optional[bool] = False,
+    ) -> QueryInfo:
+        try:
+            self.check_nua_key_is_configured_for_onprem()
+        except NUAKeyMissingError:
+            error = (
+                "Nuclia Service account is not defined so could not ask query endpoint"
             )
-        await self.check_response(resp, expected=200)
-        ident = resp.headers.get("NUCLIA-LEARNING-ID")
-        return ident, resp.content.iter_any()
+            logger.warning(error)
+            raise SendToPredictError(error)
+
+        params = {
+            "text": sentence,
+            "rephrase": str(rephrase),
+        }
+        if generative_model is not None:
+            params["generative_model"] = generative_model
+
+        resp = await self.make_request(
+            "GET",
+            url=self.get_predict_url(QUERY, kbid),
+            params=params,
+            headers=self.get_predict_headers(kbid),
+        )
+        await self.check_response(resp, expected_status=200)
+        data = await resp.json()
+        return QueryInfo(**data)
 
     @predict_observer.wrap({"type": "sentence"})
-    async def convert_sentence_to_vector(self, kbid: str, sentence: str) -> List[float]:
-        if self.onprem is False:
-            # Upload the payload
-            resp = await self.session.get(
-                url=f"{self.cluster_url}{PRIVATE_PREDICT}{SENTENCE}?text={sentence}",
-                headers={"X-STF-KBID": kbid},
+    async def convert_sentence_to_vector(self, kbid: str, sentence: str) -> list[float]:
+        try:
+            self.check_nua_key_is_configured_for_onprem()
+        except NUAKeyMissingError:
+            logger.warning(
+                "Nuclia Service account is not defined so could not retrieve vectors for the query"
             )
-        else:
-            if self.nuclia_service_account is None:
-                logger.warning(
-                    "Nuclia Service account is not defined so could not retrieve vectors for the query"
-                )
-                return []
-            # Upload the payload
-            headers = {"X-STF-NUAKEY": f"Bearer {self.nuclia_service_account}"}
-            resp = await self.session.get(
-                url=f"{self.public_url}{PUBLIC_PREDICT}{SENTENCE}?text={sentence}",
-                headers=headers,
-            )
-        await self.check_response(resp, expected=200)
+            return []
+
+        resp = await self.make_request(
+            "GET",
+            url=self.get_predict_url(SENTENCE, kbid),
+            params={"text": sentence},
+            headers=self.get_predict_headers(kbid),
+        )
+        await self.check_response(resp, expected_status=200)
         data = await resp.json()
         if len(data["data"]) == 0:
             raise PredictVectorMissing()
         return data["data"]
 
     @predict_observer.wrap({"type": "entities"})
-    async def detect_entities(self, kbid: str, sentence: str) -> List[RelationNode]:
-        if self.onprem is False:
-            # Upload the payload
-            resp = await self.session.get(
-                url=f"{self.cluster_url}{PRIVATE_PREDICT}{TOKENS}?text={sentence}",
-                headers={"X-STF-KBID": kbid},
+    async def detect_entities(self, kbid: str, sentence: str) -> list[RelationNode]:
+        try:
+            self.check_nua_key_is_configured_for_onprem()
+        except NUAKeyMissingError:
+            logger.warning(
+                "Nuclia Service account is not defined so could not retrieve entities from the query"
+            )
+            return []
+
+        resp = await self.make_request(
+            "GET",
+            url=self.get_predict_url(TOKENS, kbid),
+            params={"text": sentence},
+            headers=self.get_predict_headers(kbid),
+        )
+        await self.check_response(resp, expected_status=200)
+        data = await resp.json()
+        return convert_relations(data)
+
+    @predict_observer.wrap({"type": "summarize"})
+    async def summarize(self, kbid: str, item: SummarizeModel) -> SummarizedResponse:
+        try:
+            self.check_nua_key_is_configured_for_onprem()
+        except NUAKeyMissingError:
+            error = "Nuclia Service account is not defined. Summarize operation could not be performed"
+            logger.warning(error)
+            raise SendToPredictError(error)
+        resp = await self.make_request(
+            "POST",
+            url=self.get_predict_url(SUMMARIZE, kbid),
+            json=item.dict(),
+            headers=self.get_predict_headers(kbid),
+            timeout=None,
+        )
+        await self.check_response(resp, expected_status=200)
+        data = await resp.json()
+        return SummarizedResponse.parse_obj(data)
+
+
+class DummyPredictEngine(PredictEngine):
+    def __init__(self):
+        self.onprem = True
+        self.cluster_url = "http://localhost:8000"
+        self.public_url = "http://localhost:8000"
+        self.calls = []
+        self.generated_answer = [
+            b"valid ",
+            b"answer ",
+            b" to",
+            AnswerStatusCode.SUCCESS.encode(),
+        ]
+        self.max_context = 1000
+
+    async def initialize(self):
+        pass
+
+    async def finalize(self):
+        pass
+
+    def get_predict_headers(self, kbid: str) -> dict[str, str]:
+        return {}
+
+    async def make_request(self, method: str, **request_args):
+        response = Mock(status=200)
+        response.json = AsyncMock(return_value={"foo": "bar"})
+        response.headers = {NUCLIA_LEARNING_ID_HEADER: DUMMY_LEARNING_ID}
+        return response
+
+    async def send_feedback(
+        self,
+        kbid: str,
+        item: FeedbackRequest,
+        x_nucliadb_user: str,
+        x_ndb_client: str,
+        x_forwarded_for: str,
+    ):
+        self.calls.append(("send_feedback", item))
+        return
+
+    async def rephrase_query(self, kbid: str, item: RephraseModel) -> str:
+        self.calls.append(("rephrase_query", item))
+        return DUMMY_REPHRASE_QUERY
+
+    async def chat_query(
+        self, kbid: str, item: ChatModel
+    ) -> tuple[str, AsyncIterator[bytes]]:
+        self.calls.append(("chat_query", item))
+
+        async def generate():
+            for i in self.generated_answer:
+                yield i
+
+        return (DUMMY_LEARNING_ID, generate())
+
+    async def ask_document(
+        self, kbid: str, query: str, blocks: list[list[str]], user_id: str
+    ) -> str:
+        self.calls.append(("ask_document", (query, blocks, user_id)))
+        answer = os.environ.get("TEST_ASK_DOCUMENT") or "Answer to your question"
+        return answer
+
+    async def query(
+        self,
+        kbid: str,
+        sentence: str,
+        generative_model: Optional[str] = None,
+        rephrase: Optional[bool] = False,
+    ) -> QueryInfo:
+        self.calls.append(("query", sentence))
+        if (
+            os.environ.get("TEST_SENTENCE_ENCODER") == "multilingual-2023-02-21"
+        ):  # pragma: no cover
+            return QueryInfo(
+                language="en",
+                stop_words=[],
+                semantic_threshold=0.7,
+                visual_llm=True,
+                max_context=self.max_context,
+                entities=TokenSearch(
+                    tokens=[Ner(text="text", ner="PERSON", start=0, end=2)], time=0.0
+                ),
+                sentence=SentenceSearch(data=Qm2023, time=0.0),
+                query=sentence,
             )
         else:
-            if self.nuclia_service_account is None:  # pragma: no cover
-                logger.warning(
-                    "Nuclia Service account is not defined so could not retrieve entities from the query"
-                )
-                return []
-            # Upload the payload
-            headers = {"X-STF-NUAKEY": f"Bearer {self.nuclia_service_account}"}
-            resp = await self.session.get(
-                url=f"{self.public_url}{PUBLIC_PREDICT}{TOKENS}?text={sentence}",
-                headers=headers,
+            return QueryInfo(
+                language="en",
+                stop_words=[],
+                semantic_threshold=0.7,
+                visual_llm=True,
+                max_context=self.max_context,
+                entities=TokenSearch(
+                    tokens=[Ner(text="text", ner="PERSON", start=0, end=2)], time=0.0
+                ),
+                sentence=SentenceSearch(data=Q, time=0.0),
+                query=sentence,
             )
-        await self.check_response(resp, expected=200)
-        data = await resp.json()
 
-        return convert_relations(data)
+    async def convert_sentence_to_vector(self, kbid: str, sentence: str) -> list[float]:
+        self.calls.append(("convert_sentence_to_vector", sentence))
+        if (
+            os.environ.get("TEST_SENTENCE_ENCODER") == "multilingual-2023-02-21"
+        ):  # pragma: no cover
+            return Qm2023
+        else:
+            return Q
+
+    async def detect_entities(self, kbid: str, sentence: str) -> list[RelationNode]:
+        self.calls.append(("detect_entities", sentence))
+        dummy_data = os.environ.get("TEST_RELATIONS", None)
+        if dummy_data is not None:  # pragma: no cover
+            return convert_relations(json.loads(dummy_data))
+        else:
+            return DUMMY_RELATION_NODE
+
+    async def summarize(self, kbid: str, item: SummarizeModel) -> SummarizedResponse:
+        self.calls.append(("summarize", (kbid, item)))
+        response = SummarizedResponse(
+            summary="global summary",
+        )
+        for rid in item.resources.keys():
+            rsummary = []
+            for field_id, field_text in item.resources[rid].fields.items():
+                rsummary.append(f"{field_id}: {field_text}")
+            response.resources[rid] = SummarizedResource(
+                summary="\n\n".join(rsummary), tokens=10
+            )
+        return response
+
+
+def get_answer_generator(response: aiohttp.ClientResponse):
+    """
+    Returns an async generator that yields the chunks of the response
+    in the same way as received from the server.
+    See: https://docs.aiohttp.org/en/stable/streams.html#aiohttp.StreamReader.iter_chunks
+    """
+
+    async def _iter_answer_chunks(gen):
+        buffer = b""
+        async for chunk, end_of_chunk in gen:
+            buffer += chunk
+            if end_of_chunk:
+                yield buffer
+                buffer = b""
+
+    return _iter_answer_chunks(response.content.iter_chunks())
+
+
+async def _parse_rephrase_response(
+    resp: aiohttp.ClientResponse,
+) -> str:
+    """
+    Predict api is returning a json payload that is a string with the following format:
+    <rephrased_query><status_code>
+    where status_code is "0" for success, "-1" for error and "-2" for no context
+    it will raise an exception if the status code is not 0
+    """
+    content = await resp.json()
+    if content.endswith("0"):
+        return content[:-1]
+    elif content.endswith("-1"):
+        raise RephraseError(content[:-2])
+    elif content.endswith("-2"):
+        raise RephraseMissingContextError(content[:-2])
+    else:
+        # bw compatibility
+        return content
```

## nucliadb/search/run.py

```diff
@@ -29,10 +29,11 @@
 def run():
     setup_logging()
     instrument_app(
         application,
         tracer_provider=get_telemetry(SERVICE_NAME),
         excluded_urls=["/"],
         metrics=True,
+        trace_id_on_responses=True,
     )
 
     run_fastapi_with_metrics(application)
```

## nucliadb/search/settings.py

```diff
@@ -13,20 +13,23 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import Optional
+
+from pydantic import Field
 
 from nucliadb.ingest.settings import DriverSettings
 
 
 class Settings(DriverSettings):
     search_timeout: float = 10.0
-
-    search_cache_redis_host: Optional[str] = None
-    search_cache_redis_port: Optional[int] = None
+    slow_find_log_threshold: float = Field(
+        default=3.0,
+        title="Slow query log threshold",
+        description="The threshold in seconds for logging slow queries",
+    )
 
 
 settings = Settings()
```

## nucliadb/search/utilities.py

```diff
@@ -13,19 +13,13 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from nucliadb.ingest.orm.nodes_manager import NodesManager
-from nucliadb.ingest.utils import get_driver  # noqa
 from nucliadb.search.predict import PredictEngine
 from nucliadb_utils.utilities import Utility, get_utility
 
 
 def get_predict() -> PredictEngine:
     return get_utility(Utility.PREDICT)  # type: ignore
-
-
-def get_nodes() -> NodesManager:
-    return get_utility(Utility.NODES)  # type: ignore
```

## nucliadb/search/api/v1/__init__.py

```diff
@@ -17,11 +17,15 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from . import chat  # noqa
 from . import feedback  # noqa
 from . import find  # noqa
 from . import knowledgebox  # noqa
-from . import resource  # noqa
+from . import predict_proxy  # noqa
 from . import search  # noqa
 from . import suggest  # noqa
+from . import summarize  # noqa
+from .resource import ask as ask_resource  # noqa
+from .resource import chat as chat_resource  # noqa
+from .resource import search as search_resource  # noqa
 from .router import api  # noqa
```

## nucliadb/search/api/v1/chat.py

```diff
@@ -14,195 +14,245 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import base64
-from typing import AsyncIterator, List, Union
+import json
+from typing import Any, Optional, Union
 
+import pydantic
 from fastapi import Body, Header, Request, Response
+from fastapi.openapi.models import Example
 from fastapi_versioning import version
-from nucliadb_protos.nodereader_pb2 import RelationSearchRequest, RelationSearchResponse
 from starlette.responses import StreamingResponse
 
+from nucliadb.common.datamanagers.exceptions import KnowledgeBoxNotFound
 from nucliadb.models.responses import HTTPClientError
-from nucliadb.search.api.v1.find import find
+from nucliadb.search import logger, predict
 from nucliadb.search.api.v1.router import KB_PREFIX, api
-from nucliadb.search.predict import PredictEngine
-from nucliadb.search.requesters.utils import Method, node_query
-from nucliadb.search.search.merge import merge_relations_results
-from nucliadb.search.utilities import get_predict
+from nucliadb.search.predict import AnswerStatusCode
+from nucliadb.search.search.chat.query import (
+    START_OF_CITATIONS,
+    chat,
+    get_relations_results,
+)
+from nucliadb.search.search.exceptions import (
+    IncompleteFindResultsError,
+    InvalidQueryError,
+)
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_models.search import (
-    Author,
-    ChatModel,
     ChatOptions,
     ChatRequest,
-    FindRequest,
     KnowledgeboxFindResults,
-    Message,
     NucliaDBClientType,
-    SearchOptions,
+    PromptContext,
+    PromptContextOrder,
+    Relations,
 )
+from nucliadb_telemetry.errors import capture_exception
 from nucliadb_utils.authentication import requires
 from nucliadb_utils.exceptions import LimitsExceededError
 
 END_OF_STREAM = "_END_"
 
+
+class SyncChatResponse(pydantic.BaseModel):
+    answer: str
+    relations: Optional[Relations]
+    results: KnowledgeboxFindResults
+    status: AnswerStatusCode
+    citations: dict[str, Any] = {}
+    prompt_context: Optional[PromptContext] = None
+    prompt_context_order: Optional[PromptContextOrder] = None
+
+
 CHAT_EXAMPLES = {
-    "search_and_chat": {
-        "summary": "Ask who won the league final",
-        "description": "You can ask a question to your knowledge box",  # noqa
-        "value": {
+    "search_and_chat": Example(
+        summary="Ask who won the league final",
+        description="You can ask a question to your knowledge box",  # noqa
+        value={
             "query": "Who won the league final?",
         },
-    },
+    ),
+    "search_and_chat_with_custom_prompt": Example(
+        summary="Ask for the gold price evolution in 2023 in a very conscise way",
+        description="You can ask a question and specify a custom prompt to tweak the tone of the response",  # noqa
+        value={
+            "query": "How has the price of gold evolved during 2023?",
+            "prompt": "Given this context: {context}. Answer this {question} in a concise way using the provided context",  # noqa
+        },
+    ),
 }
 
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/chat",
     status_code=200,
     name="Chat Knowledge Box",
+    summary="Chat on a Knowledge Box",
     description="Chat on a Knowledge Box",
     tags=["Search"],
+    response_model=None,
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
-async def chat_post_knowledgebox(
+async def chat_knowledgebox_endpoint(
     request: Request,
-    response: Response,
     kbid: str,
-    item: ChatRequest = Body(examples=CHAT_EXAMPLES),
+    item: ChatRequest = Body(openapi_examples=CHAT_EXAMPLES),
     x_ndb_client: NucliaDBClientType = Header(NucliaDBClientType.API),
     x_nucliadb_user: str = Header(""),
     x_forwarded_for: str = Header(""),
-) -> Union[StreamingResponse, HTTPClientError]:
+    x_synchronous: bool = Header(
+        False,
+        description="When set to true, outputs response as JSON in a non-streaming way. "
+        "This is slower and requires waiting for entire answer to be ready.",
+    ),
+) -> Union[StreamingResponse, HTTPClientError, Response]:
     try:
-        return await chat(
-            response, kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
+        return await create_chat_response(
+            kbid, item, x_nucliadb_user, x_ndb_client, x_forwarded_for, x_synchronous
+        )
+    except KnowledgeBoxNotFound:
+        return HTTPClientError(
+            status_code=404,
+            detail=f"Knowledge Box '{kbid}' not found.",
         )
     except LimitsExceededError as exc:
         return HTTPClientError(status_code=exc.status_code, detail=exc.detail)
-
-
-async def chat(
-    response: Response,
-    kbid: str,
-    item: ChatRequest,
-    x_ndb_client: NucliaDBClientType,
-    x_nucliadb_user: str,
-    x_forwarded_for: str,
-):
-    predict = get_predict()
-
-    if item.context is not None and len(item.context) > 0:
-        # There is context lets do a query
-        req = ChatModel(
-            question=item.query,
-            context=item.context,
-            user_id=x_nucliadb_user,
-            retrieval=False,
+    except predict.ProxiedPredictAPIError as err:
+        return HTTPClientError(
+            status_code=err.status,
+            detail=err.detail,
         )
+    except IncompleteFindResultsError:
+        return HTTPClientError(
+            status_code=529,
+            detail="Temporary error on information retrieval. Please try again.",
+        )
+    except predict.RephraseMissingContextError:
+        return HTTPClientError(
+            status_code=412,
+            detail="Unable to rephrase the query with the provided context.",
+        )
+    except predict.RephraseError as err:
+        return HTTPClientError(
+            status_code=529,
+            detail=f"Temporary error while rephrasing the query. Please try again later. Error: {err}",
+        )
+    except InvalidQueryError as exc:
+        return HTTPClientError(status_code=412, detail=str(exc))
 
-        new_query = await predict.rephrase_query(kbid, req)
-    else:
-        new_query = item.query
-
-    # ONLY PARAGRAPHS I VECTORS
-    find_request = FindRequest()
-    find_request.features = [
-        SearchOptions.PARAGRAPH,
-        SearchOptions.VECTOR,
-    ]
-    find_request.query = new_query
-    find_request.fields = item.fields
-    find_request.filters = item.filters
-    find_request.field_type_filter = item.field_type_filter
-    find_request.min_score = item.min_score
-    find_request.range_creation_start = item.range_creation_start
-    find_request.range_creation_end = item.range_creation_end
-    find_request.range_modification_start = item.range_modification_start
-    find_request.range_modification_end = item.range_modification_end
-    find_request.show = item.show
-    find_request.extracted = item.extracted
-    find_request.shards = item.shards
 
-    results = await find(
-        response, kbid, find_request, x_ndb_client, x_nucliadb_user, x_forwarded_for
+async def create_chat_response(
+    kbid: str,
+    chat_request: ChatRequest,
+    user_id: str,
+    client_type: NucliaDBClientType,
+    origin: str,
+    x_synchronous: bool,
+) -> Response:
+    chat_result = await chat(
+        kbid,
+        chat_request,
+        user_id,
+        client_type,
+        origin,
     )
+    if x_synchronous:
+        streamed_answer = b""
+        async for chunk in chat_result.answer_stream:
+            streamed_answer += chunk
 
-    flattened_text = " \n\n ".join(
-        [
-            paragraph.text
-            for result in results.resources.values()
-            for field in result.fields.values()
-            for paragraph in field.paragraphs.values()
-        ]
-    )
-    if item.context is None:
-        context = []
-    else:
-        context = item.context
-    context.append(Message(author=Author.NUCLIA, text=flattened_text))
+        answer, citations = parse_streamed_answer(
+            streamed_answer, chat_request.citations
+        )
 
-    chat_model = ChatModel(
-        user_id=x_nucliadb_user, context=context, question=item.query
-    )
+        relations_results = None
+        if ChatOptions.RELATIONS in chat_request.features:
+            # XXX should use query parser here
+            relations_results = await get_relations_results(
+                kbid=kbid, chat_request=chat_request, text_answer=answer
+            )
 
-    ident, generator = await predict.chat_query(kbid, chat_model)
+        sync_chat_resp = SyncChatResponse(
+            answer=answer,
+            relations=relations_results,
+            results=chat_result.find_results,
+            status=chat_result.status_code.value,
+            citations=citations,
+        )
+        if chat_request.debug:
+            sync_chat_resp.prompt_context = chat_result.prompt_context
+            sync_chat_resp.prompt_context_order = chat_result.prompt_context_order
+        return Response(
+            content=sync_chat_resp.json(exclude_unset=True),
+            headers={
+                "NUCLIA-LEARNING-ID": chat_result.nuclia_learning_id or "unknown",
+                "Access-Control-Expose-Headers": "NUCLIA-LEARNING-ID",
+                "Content-Type": "application/json",
+            },
+        )
+    else:
 
-    async def generate_answer(
-        results: KnowledgeboxFindResults,
-        kbid: str,
-        predict: PredictEngine,
-        generator: AsyncIterator[bytes],
-        features: List[ChatOptions],
-    ):
-        if ChatOptions.PARAGRAPHS in features:
-            bytes_results = base64.b64encode(results.json().encode())
+        async def _streaming_response():
+            bytes_results = base64.b64encode(chat_result.find_results.json().encode())
             yield len(bytes_results).to_bytes(length=4, byteorder="big", signed=False)
             yield bytes_results
 
-        answer = []
-        async for data in generator:
-            answer.append(data)
-            yield data
+            streamed_answer = b""
+            async for chunk in chat_result.answer_stream:
+                streamed_answer += chunk
+                yield chunk
+
+            answer, _ = parse_streamed_answer(streamed_answer, chat_request.citations)
+
+            yield END_OF_STREAM.encode()
+            if ChatOptions.RELATIONS in chat_request.features:
+                # XXX should use query parser here
+                relations_results = await get_relations_results(
+                    kbid=kbid, chat_request=chat_request, text_answer=answer
+                )
+                yield base64.b64encode(relations_results.json().encode())
 
-        if ChatOptions.RELATIONS in features:
-            yield END_OF_STREAM
+        return StreamingResponse(
+            _streaming_response(),
+            media_type="application/octet-stream",
+            headers={
+                "NUCLIA-LEARNING-ID": chat_result.nuclia_learning_id or "unknown",
+                "Access-Control-Expose-Headers": "NUCLIA-LEARNING-ID",
+            },
+        )
 
-            text_answer = b"".join(answer)
 
-            detected_entities = await predict.detect_entities(
-                kbid, text_answer.decode()
-            )
-            relation_request = RelationSearchRequest()
-            relation_request.subgraph.entry_points.extend(detected_entities)
-            relation_request.subgraph.depth = 1
-
-            relations_results: List[RelationSearchResponse]
-            (
-                relations_results,
-                incomplete_results,
-                queried_nodes,
-                queried_shards,
-            ) = await node_query(kbid, Method.RELATIONS, relation_request, item.shards)
-            yield base64.b64encode(
-                (
-                    await merge_relations_results(
-                        relations_results, relation_request.subgraph
-                    )
-                )
-                .json()
-                .encode()
+def parse_streamed_answer(
+    streamed_bytes: bytes, requested_citations: bool
+) -> tuple[str, dict[str, Any]]:
+    try:
+        text_answer, tail = streamed_bytes.split(START_OF_CITATIONS, 1)
+    except ValueError:
+        if requested_citations:
+            logger.warning(
+                "Citations were requested but not found in the answer. "
+                "Returning the answer without citations."
             )
-
-    return StreamingResponse(
-        generate_answer(results, kbid, predict, generator, item.features),
-        media_type="plain/text",
-        headers={
-            "NUCLIA-LEARNING-ID": ident,
-            "Access-Control-Expose-Headers": "NUCLIA-LEARNING-ID",
-        },
-    )
+        return streamed_bytes.decode("utf-8"), {}
+    if not requested_citations:
+        logger.warning(
+            "Citations were not requested but found in the answer. "
+            "Returning the answer without citations."
+        )
+        return text_answer.decode("utf-8"), {}
+    try:
+        citations_length = int.from_bytes(tail[:4], byteorder="big", signed=False)
+        citations_bytes = tail[4 : 4 + citations_length]
+        citations = json.loads(base64.b64decode(citations_bytes).decode())
+        return text_answer.decode("utf-8"), citations
+    except Exception as exc:
+        capture_exception(exc)
+        logger.exception(
+            "Error parsing citations. Returning the answer without citations."
+        )
+        return text_answer.decode("utf-8"), {}
```

## nucliadb/search/api/v1/feedback.py

```diff
@@ -14,39 +14,65 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
+
 from fastapi import Header, Request, Response
 from fastapi_versioning import version
 
+from nucliadb.models.responses import HTTPClientError
+from nucliadb.search import logger, predict
 from nucliadb.search.api.v1.router import KB_PREFIX, api
 from nucliadb.search.utilities import get_predict
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_models.search import FeedbackRequest, NucliaDBClientType
+from nucliadb_telemetry import errors
 from nucliadb_utils.authentication import requires
 
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/feedback",
     status_code=200,
-    name="Feedback Knowledge Box",
-    description="Feedback on a Knowledge Box",
+    name="Send Feedback",
+    description="Send feedback for a search operation in a Knowledge Box",
     tags=["Search"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
-async def feedback_knowledgebox(
+async def send_feedback_endpoint(
     request: Request,
     response: Response,
     kbid: str,
     item: FeedbackRequest,
     x_ndb_client: NucliaDBClientType = Header(NucliaDBClientType.API),
     x_nucliadb_user: str = Header(""),
     x_forwarded_for: str = Header(""),
 ):
+    try:
+        return await send_feedback(
+            kbid, item, x_nucliadb_user, x_ndb_client, x_forwarded_for
+        )
+    except predict.ProxiedPredictAPIError as err:
+        return HTTPClientError(
+            status_code=err.status,
+            detail=err.detail,
+        )
+    except Exception as ex:
+        errors.capture_exception(ex)
+        logger.exception("Unexpected error sending feedback", extra={"kbid": kbid})
+        return HTTPClientError(status_code=500, detail=f"Internal server error")
+
+
+async def send_feedback(
+    kbid: str,
+    item: FeedbackRequest,
+    x_nucliadb_user: str,
+    x_ndb_client: NucliaDBClientType,
+    x_forwarded_for: str,
+):
     predict = get_predict()
     await predict.send_feedback(
         kbid, item, x_nucliadb_user, x_ndb_client, x_forwarded_for
     )
```

## nucliadb/search/api/v1/find.py

```diff
@@ -15,53 +15,52 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import json
 from datetime import datetime
-from time import time
-from typing import List, Optional, Union
+from typing import Optional, Union
 
 from fastapi import Body, Header, Query, Request, Response
+from fastapi.openapi.models import Example
 from fastapi_versioning import version
 from pydantic.error_wrappers import ValidationError
 
+from nucliadb.common.datamanagers.exceptions import KnowledgeBoxNotFound
 from nucliadb.models.responses import HTTPClientError
+from nucliadb.search import predict
 from nucliadb.search.api.v1.router import KB_PREFIX, api
-from nucliadb.search.requesters.utils import Method, node_query
-from nucliadb.search.search.find_merge import find_merge_results
-from nucliadb.search.search.query import global_query_to_pb, pre_process_query
-from nucliadb.search.search.utils import parse_sort_options
+from nucliadb.search.api.v1.utils import fastapi_query
+from nucliadb.search.search.exceptions import InvalidQueryError
+from nucliadb.search.search.find import find
+from nucliadb.search.search.utils import min_score_from_query_params
 from nucliadb_models.common import FieldTypeName
-from nucliadb_models.metadata import ResourceProcessingStatus
 from nucliadb_models.resource import ExtractedDataTypeName, NucliaDBRoles
 from nucliadb_models.search import (
     FindRequest,
     KnowledgeboxFindResults,
     NucliaDBClientType,
     ResourceProperties,
     SearchOptions,
-    SortField,
-    SortOptions,
-    SortOrder,
+    SearchParamDefaults,
 )
+from nucliadb_models.security import RequestSecurity
 from nucliadb_utils.authentication import requires
 from nucliadb_utils.exceptions import LimitsExceededError
-from nucliadb_utils.utilities import get_audit
 
 FIND_EXAMPLES = {
-    "find_hybrid_search": {
-        "summary": "Do a hybrid search on a Knowledge Box",
-        "description": "Perform a hybrid search that will return text and semantic results matching the query",
-        "value": {
+    "find_hybrid_search": Example(
+        summary="Do a hybrid search on a Knowledge Box",
+        description="Perform a hybrid search that will return text and semantic results matching the query",
+        value={
             "query": "How can I be an effective product manager?",
             "features": [SearchOptions.PARAGRAPH, SearchOptions.VECTOR],
         },
-    }
+    )
 }
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/find",
     status_code=200,
     name="Find Knowledge Box",
@@ -72,91 +71,104 @@
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
 async def find_knowledgebox(
     request: Request,
     response: Response,
     kbid: str,
-    query: str = Query(default=""),
-    advanced_query: Optional[str] = Query(default=None),
-    fields: List[str] = Query(default=[]),
-    filters: List[str] = Query(default=[]),
-    faceted: List[str] = Query(default=[]),
-    sort_field: Optional[SortField] = Query(default=None),
-    sort_limit: Optional[int] = Query(default=None, gt=0),
-    sort_order: SortOrder = Query(default=SortOrder.DESC),
-    page_number: int = Query(default=0),
-    page_size: int = Query(default=20),
-    min_score: float = Query(default=0.70),
-    range_creation_start: Optional[datetime] = Query(default=None),
-    range_creation_end: Optional[datetime] = Query(default=None),
-    range_modification_start: Optional[datetime] = Query(default=None),
-    range_modification_end: Optional[datetime] = Query(default=None),
-    features: List[SearchOptions] = Query(
+    query: str = fastapi_query(SearchParamDefaults.query),
+    fields: list[str] = fastapi_query(SearchParamDefaults.fields),
+    filters: list[str] = fastapi_query(SearchParamDefaults.filters),
+    page_number: int = fastapi_query(SearchParamDefaults.page_number),
+    page_size: int = fastapi_query(SearchParamDefaults.page_size),
+    min_score: Optional[float] = Query(
+        default=None,
+        description="Minimum similarity score to filter vector index results. If not specified, the default minimum score of the semantic model associated to the Knowledge Box will be used. Check out the documentation for more information on how to use this parameter: https://docs.nuclia.dev/docs/docs/using/search/#minimum-score",  # noqa: E501
+        deprecated=True,
+    ),
+    min_score_semantic: Optional[float] = Query(
+        default=None,
+        description="Minimum semantic similarity score to filter vector index results. If not specified, the default minimum score of the semantic model associated to the Knowledge Box will be used. Check out the documentation for more information on how to use this parameter: https://docs.nuclia.dev/docs/docs/using/search/#minimum-score",  # noqa: E501
+    ),
+    min_score_bm25: float = Query(
+        default=0,
+        description="Minimum bm25 score to filter paragraph and document index results",
+        ge=0,
+    ),
+    range_creation_start: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_creation_start
+    ),
+    range_creation_end: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_creation_end
+    ),
+    range_modification_start: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_modification_start
+    ),
+    range_modification_end: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_modification_end
+    ),
+    features: list[SearchOptions] = fastapi_query(
+        SearchParamDefaults.search_features,
         default=[
             SearchOptions.PARAGRAPH,
             SearchOptions.VECTOR,
         ],
     ),
-    reload: bool = Query(default=True),
-    debug: bool = Query(False),
-    highlight: bool = Query(default=False),
-    show: List[ResourceProperties] = Query([ResourceProperties.BASIC]),
-    field_type_filter: List[FieldTypeName] = Query(
-        list(FieldTypeName), alias="field_type"
-    ),
-    extracted: List[ExtractedDataTypeName] = Query(list(ExtractedDataTypeName)),
-    shards: List[str] = Query([]),
-    with_duplicates: bool = Query(default=False),
-    with_status: Optional[ResourceProcessingStatus] = Query(default=None),
-    with_synonyms: bool = Query(default=False),
+    debug: bool = fastapi_query(SearchParamDefaults.debug),
+    highlight: bool = fastapi_query(SearchParamDefaults.highlight),
+    show: list[ResourceProperties] = fastapi_query(SearchParamDefaults.show),
+    field_type_filter: list[FieldTypeName] = fastapi_query(
+        SearchParamDefaults.field_type_filter, alias="field_type"
+    ),
+    extracted: list[ExtractedDataTypeName] = fastapi_query(
+        SearchParamDefaults.extracted
+    ),
+    with_duplicates: bool = fastapi_query(SearchParamDefaults.with_duplicates),
+    with_synonyms: bool = fastapi_query(SearchParamDefaults.with_synonyms),
+    autofilter: bool = fastapi_query(SearchParamDefaults.autofilter),
+    security_groups: list[str] = fastapi_query(SearchParamDefaults.security_groups),
     x_ndb_client: NucliaDBClientType = Header(NucliaDBClientType.API),
     x_nucliadb_user: str = Header(""),
     x_forwarded_for: str = Header(""),
 ) -> Union[KnowledgeboxFindResults, HTTPClientError]:
     try:
+        security = None
+        if len(security_groups) > 0:
+            security = RequestSecurity(groups=security_groups)
         item = FindRequest(
             query=query,
-            advanced_query=advanced_query,
             fields=fields,
             filters=filters,
-            faceted=faceted,
-            sort=(
-                SortOptions(field=sort_field, limit=sort_limit, order=sort_order)
-                if sort_field is not None
-                else None
-            ),
             page_number=page_number,
             page_size=page_size,
-            min_score=min_score,
+            min_score=min_score_from_query_params(
+                min_score_bm25, min_score_semantic, min_score
+            ),
             range_creation_end=range_creation_end,
             range_creation_start=range_creation_start,
             range_modification_end=range_modification_end,
             range_modification_start=range_modification_start,
             features=features,
-            reload=reload,
             debug=debug,
             highlight=highlight,
             show=show,
             field_type_filter=field_type_filter,
             extracted=extracted,
-            shards=shards,
             with_duplicates=with_duplicates,
-            with_status=with_status,
             with_synonyms=with_synonyms,
+            autofilter=autofilter,
+            security=security,
         )
     except ValidationError as exc:
         detail = json.loads(exc.json())
         return HTTPClientError(status_code=422, detail=detail)
-    try:
-        return await find(
-            response, kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
-        )
-    except LimitsExceededError as exc:
-        return HTTPClientError(status_code=exc.status_code, detail=exc.detail)
+
+    return await _find_endpoint(
+        response, kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
+    )
 
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/find",
     status_code=200,
     name="Find Knowledge Box",
     description="Find on a Knowledge Box",
@@ -166,100 +178,42 @@
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
 async def find_post_knowledgebox(
     request: Request,
     response: Response,
     kbid: str,
-    item: FindRequest = Body(examples=FIND_EXAMPLES),
+    item: FindRequest = Body(openapi_examples=FIND_EXAMPLES),
     x_ndb_client: NucliaDBClientType = Header(NucliaDBClientType.API),
     x_nucliadb_user: str = Header(""),
     x_forwarded_for: str = Header(""),
 ) -> Union[KnowledgeboxFindResults, HTTPClientError]:
-    try:
-        return await find(
-            response, kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
-        )
-    except LimitsExceededError as exc:
-        return HTTPClientError(status_code=exc.status_code, detail=exc.detail)
+    return await _find_endpoint(
+        response, kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
+    )
 
 
-async def find(
+async def _find_endpoint(
     response: Response,
     kbid: str,
     item: FindRequest,
     x_ndb_client: NucliaDBClientType,
     x_nucliadb_user: str,
     x_forwarded_for: str,
-    do_audit: bool = True,
-) -> KnowledgeboxFindResults:
-    audit = get_audit()
-    start_time = time()
-
-    sort_options = parse_sort_options(item)
-
-    if item.query == "" and (item.vector is None or len(item.vector) == 0):
-        # If query is not defined we force to not return vector results
-        if SearchOptions.VECTOR in item.features:
-            item.features.remove(SearchOptions.VECTOR)
-
-    # We need to query all nodes
-    processed_query = pre_process_query(item.query)
-    pb_query, incomplete_results = await global_query_to_pb(
-        kbid,
-        features=item.features,
-        query=processed_query,
-        advanced_query=item.advanced_query,
-        filters=item.filters,
-        faceted=item.faceted,
-        sort=sort_options,
-        page_number=item.page_number,
-        page_size=item.page_size,
-        range_creation_start=item.range_creation_start,
-        range_creation_end=item.range_creation_end,
-        range_modification_start=item.range_modification_start,
-        range_modification_end=item.range_modification_end,
-        fields=item.fields,
-        reload=item.reload,
-        user_vector=item.vector,
-        vectorset=item.vectorset,
-        with_duplicates=item.with_duplicates,
-        with_status=item.with_status,
-        with_synonyms=item.with_synonyms,
-    )
-
-    results, incomplete_results, queried_nodes, queried_shards = await node_query(
-        kbid, Method.SEARCH, pb_query, item.shards
-    )
-
-    # We need to merge
-    search_results = await find_merge_results(
-        results,
-        count=item.page_size,
-        page=item.page_number,
-        kbid=kbid,
-        show=item.show,
-        field_type_filter=item.field_type_filter,
-        extracted=item.extracted,
-        sort=sort_options,
-        requested_relations=pb_query.relation_subgraph,
-        min_score=item.min_score,
-        highlight=item.highlight,
-    )
-
-    response.status_code = 206 if incomplete_results else 200
-
-    if audit is not None and do_audit:
-        await audit.search(
-            kbid,
-            x_nucliadb_user,
-            x_ndb_client.to_proto(),
-            x_forwarded_for,
-            pb_query,
-            time() - start_time,
-            len(search_results.resources),
+) -> Union[KnowledgeboxFindResults, HTTPClientError]:
+    try:
+        results, incomplete, _ = await find(
+            kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
+        )
+        response.status_code = 206 if incomplete else 200
+        return results
+    except KnowledgeBoxNotFound:
+        return HTTPClientError(status_code=404, detail="Knowledge Box not found")
+    except LimitsExceededError as exc:
+        return HTTPClientError(status_code=exc.status_code, detail=exc.detail)
+    except InvalidQueryError as exc:
+        return HTTPClientError(status_code=412, detail=str(exc))
+    except predict.ProxiedPredictAPIError as err:
+        return HTTPClientError(
+            status_code=err.status,
+            detail=err.detail,
         )
-    if item.debug:
-        search_results.nodes = queried_nodes
-
-    search_results.shards = queried_shards
-    return search_results
```

## nucliadb/search/api/v1/knowledgebox.py

```diff
@@ -14,55 +14,59 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
-import json
-from typing import List, Optional
+from typing import Optional
 
-from fastapi import HTTPException, Query, Request
+from fastapi import HTTPException, Request
 from fastapi_versioning import version
 from grpc import StatusCode as GrpcStatusCode
-from grpc.aio import AioRpcError  # type: ignore
+from grpc.aio import AioRpcError
 from nucliadb_protos.noderesources_pb2 import Shard
 from nucliadb_protos.writer_pb2 import ShardObject as PBShardObject
 from nucliadb_protos.writer_pb2 import Shards
 
-from nucliadb.ingest.orm.resource import KB_RESOURCE_SLUG_BASE
-from nucliadb.ingest.txn_utils import abort_transaction
+from nucliadb.common import datamanagers
+from nucliadb.common.cluster.exceptions import ShardsNotFound
+from nucliadb.common.cluster.manager import choose_node
+from nucliadb.common.cluster.utils import get_shard_manager
 from nucliadb.search import logger
 from nucliadb.search.api.v1.router import KB_PREFIX, api
+from nucliadb.search.api.v1.utils import fastapi_query
 from nucliadb.search.search.shards import get_shard
 from nucliadb.search.settings import settings
-from nucliadb.search.utilities import get_driver, get_nodes
 from nucliadb_models.resource import NucliaDBRoles
-from nucliadb_models.search import KnowledgeboxCounters, KnowledgeboxShards
+from nucliadb_models.search import (
+    KnowledgeboxCounters,
+    KnowledgeboxShards,
+    SearchParamDefaults,
+)
 from nucliadb_telemetry import errors
 from nucliadb_utils.authentication import requires, requires_one
-from nucliadb_utils.cache import KB_COUNTER_CACHE
-from nucliadb_utils.exceptions import ShardsNotFound
-from nucliadb_utils.utilities import get_cache
+
+AVG_PARAGRAPH_SIZE_BYTES = 10_000
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/shards",
     status_code=200,
     description="Show shards from a knowledgebox",
     response_model=KnowledgeboxShards,
     include_in_schema=False,
     tags=["Knowledge Boxes"],
 )
 @requires(NucliaDBRoles.MANAGER)
 @version(1)
 async def knowledgebox_shards(request: Request, kbid: str) -> KnowledgeboxShards:
-    nodemanager = get_nodes()
+    shard_manager = get_shard_manager()
     try:
-        shards: Shards = await nodemanager.get_shards_by_kbid_inner(kbid)
+        shards: Shards = await shard_manager.get_shards_by_kbid_inner(kbid)
     except ShardsNotFound:
         raise HTTPException(
             status_code=404,
             detail="The knowledgebox or its shards configuration is missing",
         )
     return KnowledgeboxShards.from_message(shards)
 
@@ -76,72 +80,59 @@
     response_model_exclude_unset=True,
 )
 @requires_one([NucliaDBRoles.READER, NucliaDBRoles.MANAGER])
 @version(1)
 async def knowledgebox_counters(
     request: Request,
     kbid: str,
-    vectorset: str = Query(None),
-    debug: bool = Query(False),
+    vectorset: str = fastapi_query(SearchParamDefaults.vectorset),
+    debug: bool = fastapi_query(SearchParamDefaults.debug),
 ) -> KnowledgeboxCounters:
-    cache = await get_cache()
-
-    if cache is not None:
-        cached_counters = await cache.get(KB_COUNTER_CACHE.format(kbid=kbid))
-
-        if cached_counters is not None:
-            cached_counters_obj = json.loads(cached_counters)
-            # In case shards get cached, we don't want it to be retrieved if we are not debugging
-            if not debug:
-                cached_counters_obj.pop("shards", None)
-            return KnowledgeboxCounters.parse_obj(cached_counters_obj)
-
-    nodemanager = get_nodes()
+    shard_manager = get_shard_manager()
 
     try:
-        shard_groups: List[PBShardObject] = await nodemanager.get_shards_by_kbid(kbid)
+        shard_groups: list[PBShardObject] = await shard_manager.get_shards_by_kbid(kbid)
     except ShardsNotFound:
         raise HTTPException(
             status_code=404,
             detail="The knowledgebox or its shards configuration is missing",
         )
 
     ops = []
     queried_shards = []
     for shard_object in shard_groups:
         try:
-            node, shard_id, node_id = nodemanager.choose_node(shard_object)
+            node, shard_id = choose_node(shard_object)
         except KeyError:
             raise HTTPException(
                 status_code=500,
                 detail="Couldn't retrieve counters right now, node not found",
             )
         else:
             if shard_id is not None:
                 # At least one node is alive for this shard group
                 # let's add it ot the query list if has a valid value
                 ops.append(get_shard(node, shard_id, vectorset=vectorset))
                 queried_shards.append(shard_id)
 
     if not ops:
-        await abort_transaction()
         logger.info(f"No node found for any of this resources shards {kbid}")
         raise HTTPException(
             status_code=500,
             detail=f"No node found for any of this resources shards {kbid}",
         )
 
     try:
-        results: Optional[List[Shard]] = await asyncio.wait_for(  # type: ignore
+        results: Optional[list[Shard]] = await asyncio.wait_for(  # type: ignore
             asyncio.gather(*ops, return_exceptions=True),  # type: ignore
             timeout=settings.search_timeout,
         )
     except asyncio.TimeoutError as exc:
+        logger.exception("Timeout querying shards")
         errors.capture_exception(exc)
-        await abort_transaction()
         raise HTTPException(status_code=503, detail=f"Data query took too long")
     except AioRpcError as exc:
         if exc.code() is GrpcStatusCode.UNAVAILABLE:
             raise HTTPException(status_code=503, detail=f"Search backend not available")
         else:
             raise exc
 
@@ -150,47 +141,54 @@
 
     field_count = 0
     paragraph_count = 0
     sentence_count = 0
 
     for shard in results:
         if isinstance(shard, Exception):
+            logger.error("Error getting shard info", exc_info=shard)
             errors.capture_exception(shard)
-            await abort_transaction()
             raise HTTPException(
                 status_code=500, detail=f"Error while geting shard data"
             )
 
-        field_count += shard.resources
+        field_count += shard.fields
         paragraph_count += shard.paragraphs
         sentence_count += shard.sentences
 
-    # Get counters from maindb
-    driver = await get_driver()
-    txn = await driver.begin()
-
-    try:
-        resource_count = 0
-        async for _ in txn.keys(
-            match=KB_RESOURCE_SLUG_BASE.format(kbid=kbid), count=-1
-        ):
-            resource_count += 1
-    except Exception as exc:
-        errors.capture_exception(exc)
-        raise HTTPException(
-            status_code=500, detail="Couldn't retrieve counters right now"
-        )
-    finally:
-        await txn.abort()
+    async with datamanagers.with_transaction() as txn:
+        try:
+            if len(shard_groups) <= 1:
+                # for smaller kbs, this is faster and more up to date
+                resource_count = (
+                    await datamanagers.resources.calculate_number_of_resources(
+                        txn, kbid=kbid
+                    )
+                )
+            else:
+                resource_count = await datamanagers.resources.get_number_of_resources(
+                    txn, kbid=kbid
+                )
+                if resource_count == -1:
+                    # WARNING: standalone, this value will never be cached
+                    resource_count = (
+                        await datamanagers.resources.calculate_number_of_resources(
+                            txn, kbid=kbid
+                        )
+                    )
+        except Exception as exc:
+            errors.capture_exception(exc)
+            raise HTTPException(
+                status_code=500, detail="Couldn't retrieve counters right now"
+            )
 
     counters = KnowledgeboxCounters(
         resources=resource_count,
         paragraphs=paragraph_count,
         fields=field_count,
         sentences=sentence_count,
+        index_size=paragraph_count * AVG_PARAGRAPH_SIZE_BYTES,
     )
 
     if debug:
         counters.shards = queried_shards
-    if cache is not None:
-        await cache.set(KB_COUNTER_CACHE.format(kbid=kbid), counters.json())
     return counters
```

## nucliadb/search/api/v1/router.py

```diff
@@ -20,8 +20,7 @@
 from fastapi.routing import APIRouter
 
 api = APIRouter()
 
 KB_PREFIX = "kb"
 KBS_PREFIX = "kbs"
 RESOURCE_PREFIX = "resource"
-RSLUG_PREFIX = "slug"
```

## nucliadb/search/api/v1/search.py

```diff
@@ -13,317 +13,470 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import json
 from datetime import datetime
 from time import time
-from typing import List, Optional, Union
+from typing import Optional, Union
 
 from fastapi import Body, Header, Query, Request, Response
+from fastapi.openapi.models import Example
 from fastapi_versioning import version
+from pydantic.error_wrappers import ValidationError
 
-from nucliadb.ingest.txn_utils import abort_transaction
+from nucliadb.common.datamanagers.exceptions import KnowledgeBoxNotFound
 from nucliadb.models.responses import HTTPClientError
+from nucliadb.search import predict
 from nucliadb.search.api.v1.router import KB_PREFIX, api
-from nucliadb.search.requesters.utils import Method, node_query
+from nucliadb.search.api.v1.utils import fastapi_query
+from nucliadb.search.requesters.utils import Method, debug_nodes_info, node_query
+from nucliadb.search.search.exceptions import InvalidQueryError
 from nucliadb.search.search.merge import merge_results
-from nucliadb.search.search.query import global_query_to_pb, pre_process_query
-from nucliadb.search.search.utils import parse_sort_options
+from nucliadb.search.search.query import QueryParser
+from nucliadb.search.search.utils import (
+    min_score_from_payload,
+    min_score_from_query_params,
+    should_disable_vector_search,
+)
 from nucliadb_models.common import FieldTypeName
 from nucliadb_models.metadata import ResourceProcessingStatus
 from nucliadb_models.resource import ExtractedDataTypeName, NucliaDBRoles
 from nucliadb_models.search import (
+    CatalogRequest,
     KnowledgeboxSearchResults,
+    MinScore,
     NucliaDBClientType,
     ResourceProperties,
     SearchOptions,
+    SearchParamDefaults,
     SearchRequest,
     SortField,
     SortOptions,
     SortOrder,
 )
+from nucliadb_models.security import RequestSecurity
 from nucliadb_utils.authentication import requires
 from nucliadb_utils.exceptions import LimitsExceededError
 from nucliadb_utils.utilities import get_audit
 
 SEARCH_EXAMPLES = {
-    "filtering_by_icon": {
-        "summary": "Search for pdf documents where the text 'Noam Chomsky' appears",
-        "description": "For a complete list of filters, visit: https://github.com/nuclia/nucliadb/blob/main/docs/internal/SEARCH.md#filters-and-facets",  # noqa
-        "value": {
+    "filtering_by_icon": Example(
+        summary="Search for pdf documents where the text 'Noam Chomsky' appears",
+        description="For a complete list of filters, visit: https://github.com/nuclia/nucliadb/blob/main/docs/internal/SEARCH.md#filters-and-facets",  # noqa
+        value={
             "query": "Noam Chomsky",
-            "filters": ["/n/i/application/pdf"],
+            "filters": ["/icon/application/pdf"],
             "features": [SearchOptions.DOCUMENT],
         },
-    },
-    "get_language_counts": {
-        "summary": "Get the number of documents for each language",
-        "description": "For a complete list of facets, visit: https://github.com/nuclia/nucliadb/blob/main/docs/internal/SEARCH.md#filters-and-facets",  # noqa
-        "value": {
+    ),
+    "get_language_counts": Example(
+        summary="Get the number of documents for each language",
+        description="For a complete list of facets, visit: https://github.com/nuclia/nucliadb/blob/main/docs/internal/SEARCH.md#filters-and-facets",  # noqa
+        value={
             "page_size": 0,
             "faceted": ["/s/p"],
             "features": [SearchOptions.DOCUMENT],
         },
-    },
+    ),
 }
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/search",
     status_code=200,
     name="Search Knowledge Box",
-    description="Search on a Knowledge Box",
+    description="Search on a Knowledge Box and retrieve separate results for documents, paragraphs, and sentences. Usually, it is better to use `find`",  # noqa: E501
     response_model=KnowledgeboxSearchResults,
     response_model_exclude_unset=True,
     tags=["Search"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
 async def search_knowledgebox(
     request: Request,
     response: Response,
     kbid: str,
-    query: str = Query(default=""),
-    advanced_query: Optional[str] = Query(default=None),
-    fields: List[str] = Query(default=[]),
-    filters: List[str] = Query(default=[]),
-    faceted: List[str] = Query(default=[]),
-    sort_field: Optional[SortField] = Query(default=None),
-    sort_limit: Optional[int] = Query(default=None, gt=0),
-    sort_order: SortOrder = Query(default=SortOrder.DESC),
-    page_number: int = Query(default=0),
-    page_size: int = Query(default=20),
-    min_score: float = Query(default=0.70),
-    range_creation_start: Optional[datetime] = Query(default=None),
-    range_creation_end: Optional[datetime] = Query(default=None),
-    range_modification_start: Optional[datetime] = Query(default=None),
-    range_modification_end: Optional[datetime] = Query(default=None),
-    features: List[SearchOptions] = Query(
+    query: str = fastapi_query(SearchParamDefaults.query),
+    fields: list[str] = fastapi_query(SearchParamDefaults.fields),
+    filters: list[str] = fastapi_query(SearchParamDefaults.filters),
+    faceted: list[str] = fastapi_query(SearchParamDefaults.faceted),
+    sort_field: SortField = fastapi_query(SearchParamDefaults.sort_field),
+    sort_limit: Optional[int] = fastapi_query(SearchParamDefaults.sort_limit),
+    sort_order: SortOrder = fastapi_query(SearchParamDefaults.sort_order),
+    page_number: int = fastapi_query(SearchParamDefaults.page_number),
+    page_size: int = fastapi_query(SearchParamDefaults.page_size),
+    min_score: Optional[float] = Query(
+        default=None,
+        description="Minimum similarity score to filter vector index results. If not specified, the default minimum score of the semantic model associated to the Knowledge Box will be used. Check out the documentation for more information on how to use this parameter: https://docs.nuclia.dev/docs/docs/using/search/#minimum-score",  # noqa: E501
+        deprecated=True,
+    ),
+    min_score_semantic: Optional[float] = Query(
+        default=None,
+        description="Minimum semantic similarity score to filter vector index results. If not specified, the default minimum score of the semantic model associated to the Knowledge Box will be used. Check out the documentation for more information on how to use this parameter: https://docs.nuclia.dev/docs/docs/using/search/#minimum-score",  # noqa: E501
+    ),
+    min_score_bm25: float = Query(
+        default=0,
+        description="Minimum bm25 score to filter paragraph and document index results",
+        ge=0,
+    ),
+    range_creation_start: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_creation_start
+    ),
+    range_creation_end: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_creation_end
+    ),
+    range_modification_start: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_modification_start
+    ),
+    range_modification_end: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_modification_end
+    ),
+    features: list[SearchOptions] = fastapi_query(
+        SearchParamDefaults.search_features,
         default=[
             SearchOptions.PARAGRAPH,
             SearchOptions.DOCUMENT,
             SearchOptions.VECTOR,
-        ]
+        ],
+    ),
+    debug: bool = fastapi_query(SearchParamDefaults.debug),
+    highlight: bool = fastapi_query(SearchParamDefaults.highlight),
+    show: list[ResourceProperties] = fastapi_query(SearchParamDefaults.show),
+    field_type_filter: list[FieldTypeName] = fastapi_query(
+        SearchParamDefaults.field_type_filter, alias="field_type"
     ),
-    reload: bool = Query(default=True),
-    debug: bool = Query(False),
-    highlight: bool = Query(default=False),
-    show: List[ResourceProperties] = Query([ResourceProperties.BASIC]),
-    field_type_filter: List[FieldTypeName] = Query(
-        list(FieldTypeName), alias="field_type"
-    ),
-    extracted: List[ExtractedDataTypeName] = Query(list(ExtractedDataTypeName)),
-    shards: List[str] = Query([]),
-    with_duplicates: bool = Query(default=False),
-    with_status: Optional[ResourceProcessingStatus] = Query(default=None),
-    with_synonyms: bool = Query(default=False),
+    extracted: list[ExtractedDataTypeName] = fastapi_query(
+        SearchParamDefaults.extracted
+    ),
+    shards: list[str] = fastapi_query(SearchParamDefaults.shards),
+    with_duplicates: bool = fastapi_query(SearchParamDefaults.with_duplicates),
+    with_synonyms: bool = fastapi_query(SearchParamDefaults.with_synonyms),
+    autofilter: bool = fastapi_query(SearchParamDefaults.autofilter),
+    security_groups: list[str] = fastapi_query(SearchParamDefaults.security_groups),
     x_ndb_client: NucliaDBClientType = Header(NucliaDBClientType.API),
     x_nucliadb_user: str = Header(""),
     x_forwarded_for: str = Header(""),
 ) -> Union[KnowledgeboxSearchResults, HTTPClientError]:
-    item = SearchRequest(
-        query=query,
-        advanced_query=advanced_query,
-        fields=fields,
-        filters=filters,
-        faceted=faceted,
-        sort=(
-            SortOptions(field=sort_field, limit=sort_limit, order=sort_order)
-            if sort_field is not None
-            else None
-        ),
-        page_number=page_number,
-        page_size=page_size,
-        min_score=min_score,
-        range_creation_end=range_creation_end,
-        range_creation_start=range_creation_start,
-        range_modification_end=range_modification_end,
-        range_modification_start=range_modification_start,
-        features=features,
-        reload=reload,
-        debug=debug,
-        highlight=highlight,
-        show=show,
-        field_type_filter=field_type_filter,
-        extracted=extracted,
-        shards=shards,
-        with_duplicates=with_duplicates,
-        with_status=with_status,
-        with_synonyms=with_synonyms,
-    )
     try:
-        return await search(
-            response, kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
+        security = None
+        if len(security_groups) > 0:
+            security = RequestSecurity(groups=security_groups)
+        item = SearchRequest(
+            query=query,
+            fields=fields,
+            filters=filters,
+            faceted=faceted,
+            sort=(
+                SortOptions(field=sort_field, limit=sort_limit, order=sort_order)
+                if sort_field is not None
+                else None
+            ),
+            page_number=page_number,
+            page_size=page_size,
+            min_score=min_score_from_query_params(
+                min_score_bm25, min_score_semantic, min_score
+            ),
+            range_creation_end=range_creation_end,
+            range_creation_start=range_creation_start,
+            range_modification_end=range_modification_end,
+            range_modification_start=range_modification_start,
+            features=features,
+            debug=debug,
+            highlight=highlight,
+            show=show,
+            field_type_filter=field_type_filter,
+            extracted=extracted,
+            shards=shards,
+            with_duplicates=with_duplicates,
+            with_synonyms=with_synonyms,
+            autofilter=autofilter,
+            security=security,
         )
-    except LimitsExceededError as exc:
-        return HTTPClientError(status_code=exc.status_code, detail=exc.detail)
+    except ValidationError as exc:
+        detail = json.loads(exc.json())
+        return HTTPClientError(status_code=422, detail=detail)
+    return await _search_endpoint(
+        response, kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
+    )
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/catalog",
     status_code=200,
     name="List resources of a Knowledge Box",
     description="List resources of a Knowledge Box",
     response_model=KnowledgeboxSearchResults,
     response_model_exclude_unset=True,
     tags=["Search"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
-async def catalog(
+async def catalog_get(
     request: Request,
     response: Response,
     kbid: str,
-    query: str = Query(default=""),
-    filters: List[str] = Query(default=[]),
-    faceted: List[str] = Query(default=[]),
-    sort_field: Optional[SortField] = Query(default=None),
-    sort_limit: int = Query(default=None, gt=0),
-    sort_order: SortOrder = Query(default=SortOrder.DESC),
-    page_number: int = Query(default=0),
-    page_size: int = Query(default=20),
-    shards: List[str] = Query([]),
-    with_status: Optional[ResourceProcessingStatus] = Query(default=None),
-    x_ndb_client: NucliaDBClientType = Header(NucliaDBClientType.API),
-    x_nucliadb_user: str = Header(""),
-    x_forwarded_for: str = Header(""),
-) -> KnowledgeboxSearchResults:
-    sort = None
-    if sort_field:
-        sort = SortOptions(field=sort_field, limit=sort_limit, order=sort_order)
-    item = SearchRequest(
+    query: str = fastapi_query(SearchParamDefaults.query),
+    filters: list[str] = fastapi_query(SearchParamDefaults.filters),
+    faceted: list[str] = fastapi_query(SearchParamDefaults.faceted),
+    sort_field: SortField = fastapi_query(SearchParamDefaults.sort_field),
+    sort_limit: Optional[int] = fastapi_query(SearchParamDefaults.sort_limit),
+    sort_order: SortOrder = fastapi_query(SearchParamDefaults.sort_order),
+    page_number: int = fastapi_query(SearchParamDefaults.page_number),
+    page_size: int = fastapi_query(SearchParamDefaults.page_size),
+    shards: list[str] = fastapi_query(SearchParamDefaults.shards),
+    with_status: Optional[ResourceProcessingStatus] = fastapi_query(
+        SearchParamDefaults.with_status
+    ),
+    debug: bool = fastapi_query(SearchParamDefaults.debug),
+) -> Union[KnowledgeboxSearchResults, HTTPClientError]:
+    item = CatalogRequest(
         query=query,
-        fields=["a/title"],
-        faceted=faceted,
         filters=filters,
-        sort=sort,
+        faceted=faceted,
         page_number=page_number,
         page_size=page_size,
-        features=[SearchOptions.DOCUMENT],
-        show=[ResourceProperties.BASIC],
         shards=shards,
+        debug=debug,
         with_status=with_status,
     )
-    return await search(
-        response,
-        kbid,
-        item,
-        x_ndb_client,
-        x_nucliadb_user,
-        x_forwarded_for,
-        do_audit=False,
-    )
+    if sort_field:
+        item.sort = SortOptions(field=sort_field, limit=sort_limit, order=sort_order)
+    return await catalog(kbid, item)
+
+
+@api.post(
+    f"/{KB_PREFIX}/{{kbid}}/catalog",
+    status_code=200,
+    name="List resources of a Knowledge Box",
+    description="List resources of a Knowledge Box",
+    response_model=KnowledgeboxSearchResults,
+    response_model_exclude_unset=True,
+    tags=["Search"],
+)
+@requires(NucliaDBRoles.READER)
+@version(1)
+async def catalog_post(
+    request: Request,
+    kbid: str,
+    item: CatalogRequest,
+) -> Union[KnowledgeboxSearchResults, HTTPClientError]:
+    return await catalog(kbid, item)
+
+
+async def catalog(
+    kbid: str,
+    item: CatalogRequest,
+):
+    """
+    Catalog endpoint is a simplified version of the search endpoint, it only
+    returns bm25 results on titles and it does not support vector search.
+    It is useful for listing resources in a knowledge box.
+    """
+    try:
+        sort = item.sort
+        if item.sort is None:
+            # By default we sort by creation date (most recent first)
+            sort = SortOptions(
+                field=SortField.CREATED,
+                order=SortOrder.DESC,
+                limit=None,
+            )
+
+        query_parser = QueryParser(
+            kbid=kbid,
+            features=[SearchOptions.DOCUMENT],
+            query=item.query,
+            filters=item.filters,
+            faceted=item.faceted,
+            sort=sort,
+            page_number=item.page_number,
+            page_size=item.page_size,
+            min_score=MinScore(bm25=0, semantic=0),
+            fields=["a/title"],
+            with_status=item.with_status,
+        )
+        pb_query, _, _ = await query_parser.parse()
+
+        (results, _, queried_nodes) = await node_query(
+            kbid,
+            Method.SEARCH,
+            pb_query,
+            target_shard_replicas=item.shards,
+            # Catalog should not go to read replicas because we want it to be
+            # consistent and most up to date results
+            use_read_replica_nodes=False,
+        )
+
+        # We need to merge
+        search_results = await merge_results(
+            results,
+            count=item.page_size,
+            page=item.page_number,
+            kbid=kbid,
+            show=[ResourceProperties.BASIC],
+            field_type_filter=[],
+            extracted=[],
+            sort=sort,
+            requested_relations=pb_query.relation_subgraph,
+            min_score=query_parser.min_score,
+            highlight=False,
+        )
+        # We don't need sentences, paragraphs or relations on the catalog
+        # response, so we set to None so that fastapi doesn't include them
+        # in the response payload
+        search_results.sentences = None
+        search_results.paragraphs = None
+        search_results.relations = None
+        if item.debug:
+            search_results.nodes = debug_nodes_info(queried_nodes)
+        queried_shards = [shard_id for _, shard_id in queried_nodes]
+        search_results.shards = queried_shards
+        return search_results
+    except InvalidQueryError as exc:
+        return HTTPClientError(status_code=412, detail=str(exc))
+    except KnowledgeBoxNotFound:
+        return HTTPClientError(status_code=404, detail="Knowledge Box not found")
+    except LimitsExceededError as exc:
+        return HTTPClientError(status_code=exc.status_code, detail=exc.detail)
 
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/search",
     status_code=200,
     name="Search Knowledge Box",
-    description="Search on a Knowledge Box",
+    description="Search on a Knowledge Box and retrieve separate results for documents, paragraphs, and sentences. Usually, it is better to use `find`",  # noqa: E501
     response_model=KnowledgeboxSearchResults,
     response_model_exclude_unset=True,
     tags=["Search"],
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
 async def search_post_knowledgebox(
     request: Request,
     response: Response,
     kbid: str,
-    item: SearchRequest = Body(examples=SEARCH_EXAMPLES),
+    item: SearchRequest = Body(openapi_examples=SEARCH_EXAMPLES),
     x_ndb_client: NucliaDBClientType = Header(NucliaDBClientType.API),
     x_nucliadb_user: str = Header(""),
     x_forwarded_for: str = Header(""),
 ) -> Union[KnowledgeboxSearchResults, HTTPClientError]:
+    return await _search_endpoint(
+        response, kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
+    )
+
+
+async def _search_endpoint(
+    response: Response,
+    kbid: str,
+    item: SearchRequest,
+    x_ndb_client: NucliaDBClientType,
+    x_nucliadb_user: str,
+    x_forwarded_for: str,
+    **kwargs,
+) -> Union[KnowledgeboxSearchResults, HTTPClientError]:
+    # All endpoint logic should be here
     try:
-        return await search(
-            response, kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for
+        results, incomplete = await search(
+            kbid, item, x_ndb_client, x_nucliadb_user, x_forwarded_for, **kwargs
         )
+        response.status_code = 206 if incomplete else 200
+        return results
+    except KnowledgeBoxNotFound:
+        return HTTPClientError(status_code=404, detail="Knowledge Box not found")
     except LimitsExceededError as exc:
         return HTTPClientError(status_code=exc.status_code, detail=exc.detail)
+    except InvalidQueryError as exc:
+        return HTTPClientError(status_code=412, detail=str(exc))
+    except predict.ProxiedPredictAPIError as err:
+        return HTTPClientError(
+            status_code=err.status,
+            detail=err.detail,
+        )
 
 
 async def search(
-    response: Response,
     kbid: str,
     item: SearchRequest,
     x_ndb_client: NucliaDBClientType,
     x_nucliadb_user: str,
     x_forwarded_for: str,
     do_audit: bool = True,
-) -> KnowledgeboxSearchResults:
+    with_status: Optional[ResourceProcessingStatus] = None,
+) -> tuple[KnowledgeboxSearchResults, bool]:
     audit = get_audit()
     start_time = time()
 
-    sort_options = parse_sort_options(item)
+    item.min_score = min_score_from_payload(item.min_score)
 
-    if item.query == "" and (item.vector is None or len(item.vector) == 0):
-        # If query is not defined we force to not return vector results
-        if SearchOptions.VECTOR in item.features:
+    if SearchOptions.VECTOR in item.features:
+        if should_disable_vector_search(item):
             item.features.remove(SearchOptions.VECTOR)
 
     # We need to query all nodes
-    processed_query = pre_process_query(item.query)
-    pb_query, incomplete_results = await global_query_to_pb(
-        kbid,
+    query_parser = QueryParser(
+        kbid=kbid,
         features=item.features,
-        query=processed_query,
-        advanced_query=item.advanced_query,
+        query=item.query,
         filters=item.filters,
         faceted=item.faceted,
-        sort=sort_options,
+        sort=item.sort,
         page_number=item.page_number,
         page_size=item.page_size,
+        min_score=item.min_score,
         range_creation_start=item.range_creation_start,
         range_creation_end=item.range_creation_end,
         range_modification_start=item.range_modification_start,
         range_modification_end=item.range_modification_end,
         fields=item.fields,
-        reload=item.reload,
         user_vector=item.vector,
         vectorset=item.vectorset,
         with_duplicates=item.with_duplicates,
-        with_status=item.with_status,
+        with_status=with_status,
         with_synonyms=item.with_synonyms,
+        autofilter=item.autofilter,
+        security=item.security,
+        rephrase=item.rephrase,
     )
+    pb_query, incomplete_results, autofilters = await query_parser.parse()
 
-    results, query_incomplete_results, queried_nodes, queried_shards = await node_query(
-        kbid, Method.SEARCH, pb_query, item.shards
+    results, query_incomplete_results, queried_nodes = await node_query(
+        kbid, Method.SEARCH, pb_query, target_shard_replicas=item.shards
     )
+
+    incomplete_results = incomplete_results or query_incomplete_results
+
     # We need to merge
     search_results = await merge_results(
         results,
         count=item.page_size,
         page=item.page_number,
         kbid=kbid,
         show=item.show,
         field_type_filter=item.field_type_filter,
         extracted=item.extracted,
-        sort=sort_options,
+        sort=query_parser.sort,
         requested_relations=pb_query.relation_subgraph,
-        min_score=item.min_score,
+        min_score=query_parser.min_score,
         highlight=item.highlight,
     )
-    await abort_transaction()
-
-    response.status_code = (
-        206 if incomplete_results or query_incomplete_results else 200
-    )
 
     if audit is not None and do_audit:
         await audit.search(
             kbid,
             x_nucliadb_user,
             x_ndb_client.to_proto(),
             x_forwarded_for,
             pb_query,
             time() - start_time,
             len(search_results.resources),
         )
     if item.debug:
-        search_results.nodes = queried_nodes
+        search_results.nodes = debug_nodes_info(queried_nodes)
 
+    queried_shards = [shard_id for _, shard_id in queried_nodes]
     search_results.shards = queried_shards
-    return search_results
+    search_results.autofilters = autofilters
+    return search_results, incomplete_results
```

## nucliadb/search/api/v1/suggest.py

```diff
@@ -15,30 +15,33 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from datetime import datetime
 from time import time
-from typing import List, Optional
+from typing import Optional, Union
 
-from fastapi import Header, Query, Request, Response
+from fastapi import Header, Request, Response
 from fastapi_versioning import version
 
-from nucliadb.ingest.txn_utils import abort_transaction
+from nucliadb.models.responses import HTTPClientError
 from nucliadb.search.api.v1.router import KB_PREFIX, api
+from nucliadb.search.api.v1.utils import fastapi_query
 from nucliadb.search.requesters.utils import Method, node_query
+from nucliadb.search.search.exceptions import InvalidQueryError
 from nucliadb.search.search.merge import merge_suggest_results
 from nucliadb.search.search.query import suggest_query_to_pb
 from nucliadb_models.common import FieldTypeName
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_models.search import (
     KnowledgeboxSuggestResults,
     NucliaDBClientType,
     ResourceProperties,
+    SearchParamDefaults,
     SuggestOptions,
 )
 from nucliadb_utils.authentication import requires
 from nucliadb_utils.utilities import get_audit
 
 
 @api.get(
@@ -51,71 +54,120 @@
 )
 @requires(NucliaDBRoles.READER)
 @version(1)
 async def suggest_knowledgebox(
     request: Request,
     response: Response,
     kbid: str,
-    query: str,
-    fields: List[str] = Query(default=[]),
-    filters: List[str] = Query(default=[]),
-    faceted: List[str] = Query(default=[]),
-    range_creation_start: Optional[datetime] = Query(default=None),
-    range_creation_end: Optional[datetime] = Query(default=None),
-    range_modification_start: Optional[datetime] = Query(default=None),
-    range_modification_end: Optional[datetime] = Query(default=None),
-    features: List[SuggestOptions] = Query(
-        default=[
-            SuggestOptions.PARAGRAPH,
-            SuggestOptions.ENTITIES,
-            SuggestOptions.INTENT,
-        ]
-    ),
-    show: List[ResourceProperties] = Query([ResourceProperties.BASIC]),
-    field_type_filter: List[FieldTypeName] = Query(
-        list(FieldTypeName), alias="field_type"
+    query: str = fastapi_query(SearchParamDefaults.suggest_query),
+    fields: list[str] = fastapi_query(SearchParamDefaults.fields),
+    filters: list[str] = fastapi_query(SearchParamDefaults.filters),
+    faceted: list[str] = fastapi_query(SearchParamDefaults.faceted),
+    range_creation_start: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_creation_start
+    ),
+    range_creation_end: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_creation_end
+    ),
+    range_modification_start: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_modification_start
+    ),
+    range_modification_end: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_modification_end
+    ),
+    features: list[SuggestOptions] = fastapi_query(
+        SearchParamDefaults.suggest_features
+    ),
+    show: list[ResourceProperties] = fastapi_query(SearchParamDefaults.show),
+    field_type_filter: list[FieldTypeName] = fastapi_query(
+        SearchParamDefaults.field_type_filter, alias="field_type"
     ),
     x_ndb_client: NucliaDBClientType = Header(NucliaDBClientType.API),
     x_nucliadb_user: str = Header(""),
     x_forwarded_for: str = Header(""),
-    debug: bool = Query(False),
-    highlight: bool = Query(False),
+    debug: bool = fastapi_query(SearchParamDefaults.debug),
+    highlight: bool = fastapi_query(SearchParamDefaults.highlight),
+) -> Union[KnowledgeboxSuggestResults, HTTPClientError]:
+    try:
+        return await suggest(
+            response,
+            kbid,
+            query,
+            fields,
+            filters,
+            faceted,
+            range_creation_start,
+            range_creation_end,
+            range_modification_start,
+            range_modification_end,
+            features,
+            show,
+            field_type_filter,
+            x_ndb_client,
+            x_nucliadb_user,
+            x_forwarded_for,
+            debug,
+            highlight,
+        )
+    except InvalidQueryError as exc:
+        return HTTPClientError(status_code=412, detail=str(exc))
+
+
+async def suggest(
+    response,
+    kbid: str,
+    query: str,
+    fields: list[str],
+    filters: list[str],
+    faceted: list[str],
+    range_creation_start: Optional[datetime],
+    range_creation_end: Optional[datetime],
+    range_modification_start: Optional[datetime],
+    range_modification_end: Optional[datetime],
+    features: list[SuggestOptions],
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    x_ndb_client: NucliaDBClientType,
+    x_nucliadb_user: str,
+    x_forwarded_for: str,
+    debug: bool,
+    highlight: bool,
 ) -> KnowledgeboxSuggestResults:
     # We need the nodes/shards that are connected to the KB
     audit = get_audit()
     start_time = time()
 
     # We need to query all nodes
-    pb_query = await suggest_query_to_pb(
+    pb_query = suggest_query_to_pb(
         features,
         query,
         fields,
         filters,
         faceted,
         range_creation_start,
         range_creation_end,
         range_modification_start,
         range_modification_end,
     )
-    results, incomplete_results, queried_nodes, queried_shards = await node_query(
-        kbid, Method.SUGGEST, pb_query, []
+    results, incomplete_results, queried_nodes = await node_query(
+        kbid, Method.SUGGEST, pb_query
     )
 
     # We need to merge
-
     search_results = await merge_suggest_results(
         results,
         kbid=kbid,
         show=show,
         field_type_filter=field_type_filter,
         highlight=highlight,
     )
-    await abort_transaction()
 
     response.status_code = 206 if incomplete_results else 200
+
+    queried_shards = [shard_id for _, shard_id in queried_nodes]
     if debug and queried_shards:
         search_results.shards = queried_shards
 
     if audit is not None:
         await audit.suggest(
             kbid,
             x_nucliadb_user,
```

## nucliadb/search/requesters/utils.py

```diff
@@ -14,46 +14,49 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 import asyncio
+import json
 from enum import Enum
-from typing import Any, List, Optional, Tuple, TypeVar, Union, overload
+from typing import Any, Optional, TypeVar, Union, overload
 
-import backoff
 from fastapi import HTTPException
+from google.protobuf.json_format import MessageToDict
 from grpc import StatusCode as GrpcStatusCode
-from grpc.aio import AioRpcError  # type: ignore
+from grpc.aio import AioRpcError
 from nucliadb_protos.nodereader_pb2 import (
     ParagraphSearchRequest,
     ParagraphSearchResponse,
     RelationSearchRequest,
     RelationSearchResponse,
     SearchRequest,
     SearchResponse,
     SuggestRequest,
     SuggestResponse,
 )
 from nucliadb_protos.writer_pb2 import ShardObject as PBShardObject
 
-from nucliadb.ingest.orm.node import Node
-from nucliadb.ingest.txn_utils import abort_transaction
+from nucliadb.common.cluster import manager as cluster_manager
+from nucliadb.common.cluster.base import AbstractIndexNode
+from nucliadb.common.cluster.exceptions import ShardsNotFound
+from nucliadb.common.cluster.utils import get_shard_manager
 from nucliadb.search import logger
 from nucliadb.search.search.shards import (
     query_paragraph_shard,
     query_shard,
     relations_shard,
     suggest_shard,
 )
 from nucliadb.search.settings import settings
-from nucliadb.search.utilities import get_nodes
 from nucliadb_telemetry import errors
-from nucliadb_utils.exceptions import ShardsNotFound
+from nucliadb_utils import const
+from nucliadb_utils.utilities import has_feature
 
 
 class Method(Enum):
     SEARCH = 1
     PARAGRAPH = 2
     SUGGEST = 3
     RELATIONS = 4
@@ -80,164 +83,194 @@
 
 
 @overload  # type: ignore
 async def node_query(
     kbid: str,
     method: Method,
     pb_query: SuggestRequest,
-    shards: Optional[List[str]] = None,
-) -> Tuple[List[SuggestResponse], bool, List[Tuple[str, str, str]], List[str]]:
-    ...
+    target_shard_replicas: Optional[list[str]] = None,
+    use_read_replica_nodes: bool = True,
+) -> tuple[list[SuggestResponse], bool, list[tuple[AbstractIndexNode, str]]]: ...
 
 
 @overload
 async def node_query(
     kbid: str,
     method: Method,
     pb_query: ParagraphSearchRequest,
-    shards: Optional[List[str]] = None,
-) -> Tuple[List[ParagraphSearchResponse], bool, List[Tuple[str, str, str]], List[str]]:
-    ...
+    target_shard_replicas: Optional[list[str]] = None,
+    use_read_replica_nodes: bool = True,
+) -> tuple[
+    list[ParagraphSearchResponse], bool, list[tuple[AbstractIndexNode, str]]
+]: ...
 
 
 @overload
 async def node_query(
     kbid: str,
     method: Method,
     pb_query: SearchRequest,
-    shards: Optional[List[str]] = None,
-) -> Tuple[List[SearchResponse], bool, List[Tuple[str, str, str]], List[str]]:
-    ...
+    target_shard_replicas: Optional[list[str]] = None,
+    use_read_replica_nodes: bool = True,
+) -> tuple[list[SearchResponse], bool, list[tuple[AbstractIndexNode, str]]]: ...
 
 
 @overload
 async def node_query(
     kbid: str,
     method: Method,
     pb_query: RelationSearchRequest,
-    shards: Optional[List[str]] = None,
-) -> Tuple[List[RelationSearchResponse], bool, List[Tuple[str, str, str]], List[str]]:
-    ...
+    target_shard_replicas: Optional[list[str]] = None,
+    use_read_replica_nodes: bool = True,
+) -> tuple[list[RelationSearchResponse], bool, list[tuple[AbstractIndexNode, str]]]: ...
 
 
-class RetriableNodeQueryException(Exception):
-    pass
-
-
-# overload does not play nice with backoff
-@backoff.on_exception(backoff.expo, (RetriableNodeQueryException,), max_tries=3)  # type: ignore
 async def node_query(
     kbid: str,
     method: Method,
     pb_query: REQUEST_TYPE,
-    shards: Optional[List[str]] = None,
-) -> Tuple[List[T], bool, List[Tuple[str, str, str]], List[str]]:
-    nodemanager = get_nodes()
+    target_shard_replicas: Optional[list[str]] = None,
+    use_read_replica_nodes: bool = True,
+) -> tuple[list[T], bool, list[tuple[AbstractIndexNode, str]]]:
+    use_read_replica_nodes = use_read_replica_nodes and has_feature(
+        const.Features.READ_REPLICA_SEARCHES, context={"kbid": kbid}
+    )
 
+    shard_manager = get_shard_manager()
     try:
-        shard_groups: List[PBShardObject] = await nodemanager.get_shards_by_kbid(kbid)
+        shard_groups: list[PBShardObject] = await shard_manager.get_shards_by_kbid(kbid)
     except ShardsNotFound:
         raise HTTPException(
             status_code=404,
             detail="The knowledgebox or its shards configuration is missing",
         )
 
     ops = []
-    queried_shards = []
     queried_nodes = []
     incomplete_results = False
-    used_nodes = []
 
     for shard_obj in shard_groups:
         try:
-            node, shard_id, node_id = nodemanager.choose_node(shard_obj, shards)
+            node, shard_id = cluster_manager.choose_node(
+                shard_obj,
+                use_read_replica_nodes=use_read_replica_nodes,
+                target_shard_replicas=target_shard_replicas,
+            )
         except KeyError:
             incomplete_results = True
         else:
             if shard_id is not None:
                 # At least one node is alive for this shard group
                 # let's add it ot the query list if has a valid value
                 func = METHODS[method]
                 ops.append(func(node, shard_id, pb_query))  # type: ignore
-                queried_nodes.append((node.label, shard_id, node_id))
-                queried_shards.append(shard_id)
-                used_nodes.append(node)
+                queried_nodes.append((node, shard_id))
 
     if not ops:
-        await abort_transaction()
-        logger.info(f"No node found for any of this resources shards {kbid}")
+        logger.warning(f"No node found for any of this resources shards {kbid}")
         raise HTTPException(
             status_code=512,
             detail=f"No node found for any of this resources shards {kbid}",
         )
 
     try:
         results = await asyncio.wait_for(  # type: ignore
             asyncio.gather(*ops, return_exceptions=True),  # type: ignore
             timeout=settings.search_timeout,
         )
-    except asyncio.TimeoutError as exc:
+    except asyncio.TimeoutError as exc:  # pragma: no cover
+        logger.warning(
+            "Timeout while querying nodes",
+            extra={"nodes": debug_nodes_info(queried_nodes)},
+        )
         results = [exc]
 
-    error = validate_node_query_results(results or [], used_nodes)
+    error = validate_node_query_results(results or [])
     if error is not None:
-        await abort_transaction()
+        query_dict = MessageToDict(pb_query)
+        query_dict.pop("vector", None)
+        logger.error(
+            "Error while querying nodes",
+            extra={
+                "kbid": kbid,
+                "query": json.dumps(query_dict),
+            },
+        )
+        if (
+            error.status_code >= 500
+            and use_read_replica_nodes
+            and any([node.is_read_replica() for node, _ in queried_nodes])
+        ):
+            # We had an error querying a secondary node, instead of raising an
+            # error directly, retry query to primaries and hope it works
+            logger.warning(
+                "Query to read replica failed. Trying again with primary",
+                extra={"nodes": debug_nodes_info(queried_nodes)},
+            )
+
+            results, incomplete_results, primary_queried_nodes = await node_query(  # type: ignore
+                kbid,
+                method,
+                pb_query,
+                target_shard_replicas,
+                use_read_replica_nodes=False,
+            )
+            queried_nodes.extend(primary_queried_nodes)
+            return results, incomplete_results, queried_nodes
+
         raise error
 
-    return results, incomplete_results, queried_nodes, queried_shards
+    return results, incomplete_results, queried_nodes
 
 
-def validate_node_query_results(
-    results: list[Any], used_nodes: list[Node]
-) -> Optional[HTTPException]:
+def validate_node_query_results(results: list[Any]) -> Optional[HTTPException]:
     """
     Validate the results of a node query and return an exception if any error is found
 
     Handling of exception is responsibility of caller.
     """
     if results is None or len(results) == 0:
         return HTTPException(
             status_code=500, detail=f"Error while executing shard queries. No results."
         )
 
-    for i, result in enumerate(results):
+    for result in results:
         if isinstance(result, Exception):
             status_code = 500
             reason = "Error while querying shard data."
             if isinstance(result, AioRpcError):
-                if result.code() is GrpcStatusCode.UNAVAILABLE:
-                    if len(results) == len(used_nodes):
-                        # only reset connection of detected failure
-                        logger.warning(
-                            "GRPC connection failure detected, resetting connection"
-                        )
-                        used_nodes[i].reset_connections()
-                    else:
-                        logger.warning(
-                            "GRPC connection failure detected with incomplete results, resetting all connections"
-                        )
-                        # for some reason result set isn't the same, reset all connections
-                        for node in used_nodes:
-                            node.reset_connections()
-                    # Enable retries on errors here
-                    # XXX this is somewhat a workaround and we should consider
-                    # a better GRPC interface to work with everywhere longer term
-                    # that would enable some automatically retry handling for us
-                    # and stale connection handling but right now, all
-                    # our node allows us to directly interact with stub/connections
-                    raise RetriableNodeQueryException()
-                elif result.code() is GrpcStatusCode.INTERNAL:
+                if result.code() is GrpcStatusCode.INTERNAL:
                     # handle node response errors
-                    if "AllButQueryForbidden" in result.details():
+                    details = result.details() or "gRPC error without details"
+                    if "AllButQueryForbidden" in details:
                         status_code = 412
-                        reason = result.details().split(":")[-1].strip().strip("'")
+                        reason = details.split(":")[-1].strip().strip("'")
+                    else:
+                        reason = details
+                        logger.exception(f"Unhandled node error", exc_info=result)
                 else:
                     logger.error(
                         f"Unhandled GRPC error while querying shard data: {result.debug_error_string()}"
                     )
             else:
                 errors.capture_exception(result)
                 logger.exception("Error while querying shard data", exc_info=result)
 
             return HTTPException(status_code=status_code, detail=reason)
 
     return None
+
+
+def debug_nodes_info(
+    nodes: list[tuple[AbstractIndexNode, str]]
+) -> list[dict[str, str]]:
+    details: list[dict[str, str]] = []
+    for node, shard_id in nodes:
+        info = {
+            "id": node.id,
+            "shard_id": shard_id,
+            "address": node.address,
+        }
+        if node.primary_id:
+            info["primary_id"] = node.primary_id
+        details.append(info)
+    return details
```

## nucliadb/search/search/cache.py

```diff
@@ -15,54 +15,63 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 import asyncio
 from contextvars import ContextVar
-from typing import Dict, Optional
+from typing import Optional
 
 from lru import LRU  # type: ignore
 
+from nucliadb.common.maindb.driver import Transaction
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox as KnowledgeBoxORM
 from nucliadb.ingest.orm.resource import Resource as ResourceORM
-from nucliadb.ingest.txn_utils import get_transaction
+from nucliadb.middleware.transaction import get_read_only_transaction
 from nucliadb.search import SERVICE_NAME
+from nucliadb_telemetry import metrics
 from nucliadb_utils.utilities import get_storage
 
-rcache: ContextVar[Optional[Dict[str, ResourceORM]]] = ContextVar(
+rcache: ContextVar[Optional[dict[str, ResourceORM]]] = ContextVar(
     "rcache", default=None
 )
 
 
-RESOURCE_LOCKS: Dict[str, asyncio.Lock] = LRU(1000)
+RESOURCE_LOCKS: dict[str, asyncio.Lock] = LRU(1000)  # type: ignore
+RESOURCE_CACHE_OPS = metrics.Counter("nucliadb_resource_cache_ops", labels={"type": ""})
 
 
-def get_resource_cache(clear: bool = False) -> Dict[str, ResourceORM]:
-    value: Optional[Dict[str, ResourceORM]] = rcache.get()
+def get_resource_cache(clear: bool = False) -> dict[str, ResourceORM]:
+    value: Optional[dict[str, ResourceORM]] = rcache.get()
     if value is None or clear:
         value = {}
         rcache.set(value)
     return value
 
 
-async def get_resource_from_cache(kbid: str, uuid: str) -> Optional[ResourceORM]:
+async def get_resource_from_cache(
+    kbid: str, uuid: str, txn: Optional[Transaction] = None
+) -> Optional[ResourceORM]:
     orm_resource: Optional[ResourceORM] = None
 
     resource_cache = get_resource_cache()
 
     if uuid not in RESOURCE_LOCKS:
         RESOURCE_LOCKS[uuid] = asyncio.Lock()
 
     async with RESOURCE_LOCKS[uuid]:
         if uuid not in resource_cache:
-            transaction = await get_transaction()
+            RESOURCE_CACHE_OPS.inc({"type": "miss"})
+            if txn is None:
+                txn = await get_read_only_transaction()
             storage = await get_storage(service_name=SERVICE_NAME)
-            kb = KnowledgeBoxORM(transaction, storage, kbid)
+            kb = KnowledgeBoxORM(txn, storage, kbid)
             orm_resource = await kb.get(uuid)
+        else:
+            RESOURCE_CACHE_OPS.inc({"type": "hit"})
 
         if orm_resource is not None:
             resource_cache[uuid] = orm_resource
         else:
             orm_resource = resource_cache.get(uuid)
 
     return orm_resource
```

## nucliadb/search/search/fetch.py

```diff
@@ -14,44 +14,47 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from contextvars import ContextVar
-from typing import Dict, List, Optional, Tuple
+from typing import Optional
 
 from nucliadb_protos.nodereader_pb2 import DocumentResult, ParagraphResult
 from nucliadb_protos.resources_pb2 import Paragraph
 
 from nucliadb.ingest.orm.resource import KB_REVERSE
 from nucliadb.ingest.orm.resource import Resource as ResourceORM
-from nucliadb.ingest.serialize import serialize
+from nucliadb.ingest.serialize import managed_serialize
+from nucliadb.middleware.transaction import get_read_only_transaction
 from nucliadb.search import SERVICE_NAME, logger
 from nucliadb_models.common import FieldTypeName
 from nucliadb_models.resource import ExtractedDataTypeName, Resource
 from nucliadb_models.search import ResourceProperties
 
 from .cache import get_resource_from_cache
 
-rcache: ContextVar[Optional[Dict[str, ResourceORM]]] = ContextVar(
+rcache: ContextVar[Optional[dict[str, ResourceORM]]] = ContextVar(
     "rcache", default=None
 )
 
 
 async def fetch_resources(
-    resources: List[str],
+    resources: list[str],
     kbid: str,
-    show: List[ResourceProperties],
-    field_type_filter: List[FieldTypeName],
-    extracted: List[ExtractedDataTypeName],
-) -> Dict[str, Resource]:
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    extracted: list[ExtractedDataTypeName],
+) -> dict[str, Resource]:
     result = {}
+    txn = await get_read_only_transaction()
     for resource in resources:
-        serialization = await serialize(
+        serialization = await managed_serialize(
+            txn,
             kbid,
             resource,
             show,
             field_type_filter=field_type_filter,
             extracted=extracted,
             service_name=SERVICE_NAME,
         )
@@ -73,38 +76,38 @@
             metadata = field_metadata.split_metadata[result.split]
             paragraph = metadata.paragraphs[result.index]
         elif len(field_metadata.metadata.paragraphs) > result.index:
             paragraph = field_metadata.metadata.paragraphs[result.index]
     return paragraph
 
 
-async def get_labels_resource(result: DocumentResult, kbid: str) -> List[str]:
+async def get_labels_resource(result: DocumentResult, kbid: str) -> list[str]:
     orm_resource = await get_resource_from_cache(kbid, result.uuid)
 
     if orm_resource is None:
         logger.error(f"{result.uuid} does not exist on DB")
         return []
 
-    labels: List[str] = []
+    labels: list[str] = []
     basic = await orm_resource.get_basic()
     if basic is not None:
         for classification in basic.usermetadata.classifications:
             labels.append(f"{classification.labelset}/{classification.label}")
 
     return labels
 
 
-async def get_labels_paragraph(result: ParagraphResult, kbid: str) -> List[str]:
+async def get_labels_paragraph(result: ParagraphResult, kbid: str) -> list[str]:
     orm_resource = await get_resource_from_cache(kbid, result.uuid)
 
     if orm_resource is None:
         logger.error(f"{result.uuid} does not exist on DB")
         return []
 
-    labels: List[str] = []
+    labels: list[str] = []
     basic = await orm_resource.get_basic()
     if basic is not None:
         for classification in basic.usermetadata.classifications:
             labels.append(f"{classification.labelset}/{classification.label}")
 
     _, field_type, field = result.field.split("/")
     field_type_int = KB_REVERSE[field_type]
@@ -123,15 +126,15 @@
                 labels.append(f"{classification.labelset}/{classification.label}")
 
     return labels
 
 
 async def get_seconds_paragraph(
     result: ParagraphResult, kbid: str
-) -> Optional[Tuple[List[int], List[int]]]:
+) -> Optional[tuple[list[int], list[int]]]:
     orm_resource = await get_resource_from_cache(kbid, result.uuid)
 
     if orm_resource is None:
         logger.error(f"{result.uuid} does not exist on DB")
         return None
 
     paragraph = await get_paragraph_from_resource(
```

## nucliadb/search/search/find_merge.py

```diff
@@ -14,116 +14,126 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
-from typing import Any, Dict, Iterator, List, Optional, Tuple, cast
+from typing import Any, Iterator, Optional, cast
 
 from nucliadb_protos.nodereader_pb2 import (
     DocumentScored,
     EntitiesSubgraphRequest,
     ParagraphResult,
     SearchResponse,
 )
 
-from nucliadb.ingest.serialize import serialize
-from nucliadb.ingest.txn_utils import abort_transaction, get_transaction
-from nucliadb.search import SERVICE_NAME
+from nucliadb.common.maindb.driver import Transaction
+from nucliadb.ingest.serialize import managed_serialize
+from nucliadb.middleware.transaction import get_read_only_transaction
+from nucliadb.search import SERVICE_NAME, logger
 from nucliadb.search.search.cache import get_resource_cache
 from nucliadb.search.search.merge import merge_relations_results
 from nucliadb_models.common import FieldTypeName
 from nucliadb_models.resource import ExtractedDataTypeName
 from nucliadb_models.search import (
     SCORE_TYPE,
     FindField,
     FindParagraph,
     FindResource,
     KnowledgeboxFindResults,
+    MinScore,
     ResourceProperties,
-    SortOptions,
     TempFindParagraph,
     TextPosition,
 )
 from nucliadb_telemetry import metrics
 
+from . import paragraphs
 from .metrics import merge_observer
-from .paragraphs import get_paragraph_text
 
 FIND_FETCH_OPS_DISTRIBUTION = metrics.Histogram(
     "nucliadb_find_fetch_operations",
     buckets=[1, 5, 10, 20, 30, 40, 50, 60, 80, 100, 200],
 )
 
 
+def _round(x: float) -> float:
+    return round(x, ndigits=3)
+
+
+@merge_observer.wrap({"type": "set_text_value"})
 async def set_text_value(
     kbid: str,
     result_paragraph: TempFindParagraph,
     max_operations: asyncio.Semaphore,
     highlight: bool = False,
-    ematches: Optional[List[str]] = None,
+    ematches: Optional[list[str]] = None,
+    extracted_text_cache: Optional[paragraphs.ExtractedTextCache] = None,
 ):
-    # TODO: Improve
-    await max_operations.acquire()
-    try:
+    async with max_operations:
         assert result_paragraph.paragraph
         assert result_paragraph.paragraph.position
-        result_paragraph.paragraph.text = await get_paragraph_text(
+        result_paragraph.paragraph.text = await paragraphs.get_paragraph_text(
             kbid=kbid,
             rid=result_paragraph.rid,
             field=result_paragraph.field,
             start=result_paragraph.paragraph.position.start,
             end=result_paragraph.paragraph.position.end,
-            split=None,  # TODO
+            split=result_paragraph.split,
             highlight=highlight,
             ematches=ematches,
             matches=[],  # TODO
+            extracted_text_cache=extracted_text_cache,
         )
-    finally:
-        max_operations.release()
 
 
+@merge_observer.wrap({"type": "set_resource_metadada_value"})
 async def set_resource_metadata_value(
+    txn: Transaction,
     kbid: str,
     resource: str,
-    show: List[ResourceProperties],
-    field_type_filter: List[FieldTypeName],
-    extracted: List[ExtractedDataTypeName],
-    find_resources: Dict[str, FindResource],
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    extracted: list[ExtractedDataTypeName],
+    find_resources: dict[str, FindResource],
     max_operations: asyncio.Semaphore,
 ):
-    await max_operations.acquire()
-
-    try:
-        serialized_resource = await serialize(
+    async with max_operations:
+        serialized_resource = await managed_serialize(
+            txn,
             kbid,
             resource,
             show,
             field_type_filter=field_type_filter,
             extracted=extracted,
             service_name=SERVICE_NAME,
         )
         if serialized_resource is not None:
             find_resources[resource].updated_from(serialized_resource)
-    finally:
-        max_operations.release()
+        else:
+            logger.warning(f"Resource {resource} not found in {kbid}")
+            find_resources.pop(resource, None)
 
 
 class Orderer:
     def __init__(self):
         self.boosted_items = []
         self.items = []
 
     def add(self, key: Any):
         self.items.append(key)
 
     def add_boosted(self, key: Any):
         self.boosted_items.append(key)
 
+    def sorted_by_score(self) -> Iterator[Any]:
+        for key in sorted(self.items, key=lambda value: value[3], reverse=True):
+            yield key
+
     def sorted_by_insertion(self) -> Iterator[Any]:
         returned = set()
         for key in self.boosted_items:
             if key in returned:
                 continue
             returned.add(key)
             yield key
@@ -131,163 +141,204 @@
         for key in self.items:
             if key in returned:
                 continue
             returned.add(key)
             yield key
 
 
+@merge_observer.wrap({"type": "fetch_find_metadata"})
 async def fetch_find_metadata(
-    find_resources: Dict[str, FindResource],
-    result_paragraphs: List[TempFindParagraph],
+    find_resources: dict[str, FindResource],
+    best_matches: list[str],
+    result_paragraphs: list[TempFindParagraph],
     kbid: str,
-    show: List[ResourceProperties],
-    field_type_filter: List[FieldTypeName],
-    extracted: List[ExtractedDataTypeName],
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    extracted: list[ExtractedDataTypeName],
     highlight: bool = False,
-    ematches: Optional[List[str]] = None,
+    ematches: Optional[list[str]] = None,
 ):
+    txn = await get_read_only_transaction()
     resources = set()
     operations = []
     max_operations = asyncio.Semaphore(50)
-
     orderer = Orderer()
-
+    etcache = paragraphs.ExtractedTextCache()
     for result_paragraph in result_paragraphs:
         if result_paragraph.paragraph is not None:
             find_resource = find_resources.setdefault(
                 result_paragraph.rid, FindResource(id=result_paragraph.id, fields={})
             )
             find_field = find_resource.fields.setdefault(
                 result_paragraph.field, FindField(paragraphs={})
             )
 
             if result_paragraph.paragraph.id in find_field.paragraphs:
                 # Its a multiple match, push the score
-                find_field.paragraphs[result_paragraph.paragraph.id].score = 25
-                find_field.paragraphs[
-                    result_paragraph.paragraph.id
-                ].score_type = SCORE_TYPE.BOTH
-                orderer.add_boosted(
-                    (
-                        result_paragraph.rid,
-                        result_paragraph.field,
-                        result_paragraph.paragraph.id,
+                # find_field.paragraphs[result_paragraph.paragraph.id].score = 25
+                if (
+                    find_field.paragraphs[result_paragraph.paragraph.id].score
+                    < result_paragraph.paragraph.score
+                ):
+                    # Use Vector score if there are both
+                    find_field.paragraphs[result_paragraph.paragraph.id].score = (
+                        result_paragraph.paragraph.score * 2
                     )
+                    orderer.add(
+                        (
+                            result_paragraph.rid,
+                            result_paragraph.field,
+                            result_paragraph.paragraph.id,
+                            result_paragraph.paragraph.score,
+                        )
+                    )
+                find_field.paragraphs[result_paragraph.paragraph.id].score_type = (
+                    SCORE_TYPE.BOTH
                 )
+
             else:
-                find_field.paragraphs[
-                    result_paragraph.paragraph.id
-                ] = result_paragraph.paragraph
+                find_field.paragraphs[result_paragraph.paragraph.id] = (
+                    result_paragraph.paragraph
+                )
                 orderer.add(
                     (
                         result_paragraph.rid,
                         result_paragraph.field,
                         result_paragraph.paragraph.id,
+                        result_paragraph.paragraph.score,
                     )
                 )
 
             operations.append(
-                set_text_value(
-                    kbid=kbid,
-                    result_paragraph=result_paragraph,
-                    highlight=highlight,
-                    ematches=ematches,
-                    max_operations=max_operations,
+                asyncio.create_task(
+                    set_text_value(
+                        kbid=kbid,
+                        result_paragraph=result_paragraph,
+                        highlight=highlight,
+                        ematches=ematches,
+                        max_operations=max_operations,
+                        extracted_text_cache=etcache,
+                    )
                 )
             )
             resources.add(result_paragraph.rid)
+    etcache.clear()
 
-    for order, (rid, field_id, paragraph_id) in enumerate(
-        orderer.sorted_by_insertion()
-    ):
+    for order, (rid, field_id, paragraph_id, _) in enumerate(orderer.sorted_by_score()):
         find_resources[rid].fields[field_id].paragraphs[paragraph_id].order = order
+        best_matches.append(paragraph_id)
 
     for resource in resources:
         operations.append(
-            set_resource_metadata_value(
-                kbid=kbid,
-                resource=resource,
-                show=show,
-                field_type_filter=field_type_filter,
-                extracted=extracted,
-                find_resources=find_resources,
-                max_operations=max_operations,
+            asyncio.create_task(
+                set_resource_metadata_value(
+                    txn,
+                    kbid=kbid,
+                    resource=resource,
+                    show=show,
+                    field_type_filter=field_type_filter,
+                    extracted=extracted,
+                    find_resources=find_resources,
+                    max_operations=max_operations,
+                )
             )
         )
 
     FIND_FETCH_OPS_DISTRIBUTION.observe(len(operations))
     if len(operations) > 0:
-        await asyncio.wait(operations)  # type: ignore
+        done, _ = await asyncio.wait(operations)  # type: ignore
+        for task in done:
+            if task.exception() is not None:  # pragma: no cover
+                logger.error("Error fetching find metadata", exc_info=task.exception())
 
 
-async def merge_paragraphs_vectors(
-    paragraphs_shards: List[List[ParagraphResult]],
-    vectors_shards: List[List[DocumentScored]],
+@merge_observer.wrap({"type": "merge_paragraphs_vectors"})
+def merge_paragraphs_vectors(
+    paragraphs_shards: list[list[ParagraphResult]],
+    vectors_shards: list[list[DocumentScored]],
     count: int,
     page: int,
     min_score: float,
-) -> Tuple[List[TempFindParagraph], bool]:
-    merged_paragrahs: List[TempFindParagraph] = []
+) -> tuple[list[TempFindParagraph], bool]:
+    merged_paragrahs: list[TempFindParagraph] = []
 
     # We assume that paragraphs_shards and vectors_shards are already ordered
     for paragraphs_shard in paragraphs_shards:
         for paragraph in paragraphs_shard:
+            fuzzy_result = len(paragraph.matches) > 0
             merged_paragrahs.append(
                 TempFindParagraph(
                     paragraph_index=paragraph,
                     field=paragraph.field,
                     rid=paragraph.uuid,
                     score=paragraph.score.bm25,
                     start=paragraph.start,
+                    split=paragraph.split,
                     end=paragraph.end,
                     id=paragraph.paragraph,
+                    fuzzy_result=fuzzy_result,
+                    page_with_visual=paragraph.metadata.page_with_visual,
+                    reference=paragraph.metadata.representation.file,
+                    is_a_table=paragraph.metadata.representation.is_a_table,
                 )
             )
 
     # merged_paragrahs.sort(key=lambda r: r.score, reverse=True)
 
     nextpos = 1
     for vectors_shard in vectors_shards:
         for vector in vectors_shard:
-            if vector.score >= min_score:
-                doc_id_split = vector.doc_id.id.split("/")
-                if len(doc_id_split) == 5:
-                    rid, field_type, field, index, position = doc_id_split
-                    paragraph_id = f"{rid}/{field_type}/{field}/{position}"
-                elif len(doc_id_split) == 6:
-                    rid, field_type, field, split, index, position = doc_id_split
-                    paragraph_id = f"{rid}/{field_type}/{field}/{split}/{position}"
-                start, end = position.split("-")
-                merged_paragrahs.insert(
-                    nextpos,
-                    TempFindParagraph(
-                        vector_index=vector,
-                        rid=rid,
-                        field=f"/{field_type}/{field}",
-                        score=vector.score,
-                        start=int(start),
-                        end=int(end),
-                        id=paragraph_id,
-                    ),
+            if vector.score < min_score:
+                logger.warning(
+                    f"Skipping low score vector: {vector.doc_id.id}. This should not happen"
                 )
-                nextpos += 3
+                continue
+            doc_id_split = vector.doc_id.id.split("/")
+            split = None
+            if len(doc_id_split) == 5:
+                rid, field_type, field, index, position = doc_id_split
+                paragraph_id = f"{rid}/{field_type}/{field}/{position}"
+            elif len(doc_id_split) == 6:
+                rid, field_type, field, split, index, position = doc_id_split
+                paragraph_id = f"{rid}/{field_type}/{field}/{split}/{position}"
+            else:
+                logger.warning(f"Skipping invalid doc_id: {vector.doc_id.id}")
+                continue
+            start, end = position.split("-")
+            merged_paragrahs.insert(
+                nextpos,
+                TempFindParagraph(
+                    vector_index=vector,
+                    rid=rid,
+                    field=f"/{field_type}/{field}",
+                    score=vector.score,
+                    start=int(start),
+                    end=int(end),
+                    split=split,
+                    id=paragraph_id,
+                ),
+            )
+            nextpos += 3
 
     # merged_paragrahs.sort(key=lambda r: r.score, reverse=True)
     init_position = count * page
     end_position = init_position + count
     next_page = len(merged_paragrahs) > end_position
     merged_paragrahs = merged_paragrahs[init_position:end_position]
 
     for merged_paragraph in merged_paragrahs:
         if merged_paragraph.vector_index is not None:
             merged_paragraph.paragraph = FindParagraph(
-                score=vector.score,
+                score=merged_paragraph.vector_index.score,
                 score_type=SCORE_TYPE.VECTOR,
                 text="",
                 labels=[],  # TODO: Get labels from index
+                page_with_visual=merged_paragraph.vector_index.metadata.page_with_visual,
+                reference=merged_paragraph.vector_index.metadata.representation.file,
+                is_a_table=merged_paragraph.vector_index.metadata.representation.is_a_table,
                 position=TextPosition(
                     page_number=merged_paragraph.vector_index.metadata.position.page_number,
                     index=merged_paragraph.vector_index.metadata.position.index,
                     start=merged_paragraph.start,
                     end=merged_paragraph.end,
                     start_seconds=[
                         x
@@ -295,21 +346,26 @@
                     ],
                     end_seconds=[
                         x
                         for x in merged_paragraph.vector_index.metadata.position.end_seconds
                     ],
                 ),
                 id=merged_paragraph.id,
+                # Vector searches don't have fuzziness
+                fuzzy_result=False,
             )
-        if merged_paragraph.paragraph_index is not None:
+        elif merged_paragraph.paragraph_index is not None:
             merged_paragraph.paragraph = FindParagraph(
                 score=merged_paragraph.paragraph_index.score.bm25,
                 score_type=SCORE_TYPE.BM25,
                 text="",
                 labels=[x for x in merged_paragraph.paragraph_index.labels],
+                page_with_visual=merged_paragraph.paragraph_index.metadata.page_with_visual,
+                reference=merged_paragraph.paragraph_index.metadata.representation.file,
+                is_a_table=merged_paragraph.paragraph_index.metadata.representation.is_a_table,
                 position=TextPosition(
                     page_number=merged_paragraph.paragraph_index.metadata.position.page_number,
                     index=merged_paragraph.paragraph_index.metadata.position.index,
                     start=merged_paragraph.start,
                     end=merged_paragraph.end,
                     start_seconds=[
                         x
@@ -317,88 +373,91 @@
                     ],
                     end_seconds=[
                         x
                         for x in merged_paragraph.paragraph_index.metadata.position.end_seconds
                     ],
                 ),
                 id=merged_paragraph.id,
+                fuzzy_result=merged_paragraph.fuzzy_result,
             )
     return merged_paragrahs, next_page
 
 
 @merge_observer.wrap({"type": "find_merge"})
 async def find_merge_results(
-    search_responses: List[SearchResponse],
+    search_responses: list[SearchResponse],
     count: int,
     page: int,
     kbid: str,
-    show: List[ResourceProperties],
-    field_type_filter: List[FieldTypeName],
-    extracted: List[ExtractedDataTypeName],
-    sort: SortOptions,
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    extracted: list[ExtractedDataTypeName],
     requested_relations: EntitiesSubgraphRequest,
-    min_score: float = 0.85,
+    min_score_bm25: float,
+    min_score_semantic: float,
     highlight: bool = False,
 ) -> KnowledgeboxFindResults:
     # force getting transaction on current asyncio task
     # so all sub tasks will use the same transaction
     # this is contextvar magic that is probably not ideal
-    await get_transaction()
+    await get_read_only_transaction()
 
-    paragraphs: List[List[ParagraphResult]] = []
-    vectors: List[List[DocumentScored]] = []
+    paragraphs: list[list[ParagraphResult]] = []
+    vectors: list[list[DocumentScored]] = []
     relations = []
 
-    # facets_counter = Counter()
     next_page = True
-    ematches: List[str] = []
+    ematches: list[str] = []
     real_query = ""
     total_paragraphs = 0
     for response in search_responses:
         # Iterate over answers from different logic shards
 
-        # Merge facets
-        # TODO
-        # facets_counter.update(response.paragraph.facets)
         ematches.extend(response.paragraph.ematches)
         real_query = response.paragraph.query
         next_page = next_page and response.paragraph.next_page
         total_paragraphs += response.paragraph.total
 
-        paragraphs.append(cast(List[ParagraphResult], response.paragraph.results))
-        vectors.append(cast(List[DocumentScored], response.vector.documents))
+        paragraphs.append(cast(list[ParagraphResult], response.paragraph.results))
+        vectors.append(cast(list[DocumentScored], response.vector.documents))
 
         relations.append(response.relation)
 
-    get_resource_cache(clear=True)
+    rcache = get_resource_cache(clear=True)
 
-    result_paragraphs, merged_next_page = await merge_paragraphs_vectors(
-        paragraphs, vectors, count, page, min_score
-    )
-    next_page = next_page or merged_next_page
-
-    api_results = KnowledgeboxFindResults(
-        resources={},
-        facets={},
-        query=real_query,
-        total=total_paragraphs,
-        page_number=page,
-        page_size=count,
-        next_page=next_page,
-    )
-
-    await fetch_find_metadata(
-        api_results.resources,
-        result_paragraphs,
-        kbid,
-        show,
-        field_type_filter,
-        extracted,
-        highlight,
-        ematches,
-    )
-    api_results.relations = await merge_relations_results(
-        relations, requested_relations
-    )
+    try:
+        result_paragraphs, merged_next_page = merge_paragraphs_vectors(
+            paragraphs, vectors, count, page, min_score_semantic
+        )
+        next_page = next_page or merged_next_page
+
+        api_results = KnowledgeboxFindResults(
+            resources={},
+            query=real_query,
+            total=total_paragraphs,
+            page_number=page,
+            page_size=count,
+            next_page=next_page,
+            min_score=MinScore(
+                bm25=_round(min_score_bm25), semantic=_round(min_score_semantic)
+            ),
+            best_matches=[],
+        )
+
+        await fetch_find_metadata(
+            api_results.resources,
+            api_results.best_matches,
+            result_paragraphs,
+            kbid,
+            show,
+            field_type_filter,
+            extracted,
+            highlight,
+            ematches,
+        )
+        api_results.relations = await merge_relations_results(
+            relations, requested_relations
+        )
 
-    await abort_transaction()
-    return api_results
+        return api_results
+    finally:
+        rcache.clear()
```

## nucliadb/search/search/merge.py

```diff
@@ -13,131 +13,136 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import asyncio
 import datetime
 import math
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import Any, Optional, Set, Union
 
 from nucliadb_protos.nodereader_pb2 import (
     DocumentResult,
     DocumentScored,
     DocumentSearchResponse,
     EntitiesSubgraphRequest,
     ParagraphResult,
     ParagraphSearchResponse,
     RelationSearchResponse,
     SearchResponse,
     SuggestResponse,
     VectorSearchResponse,
 )
 
-from nucliadb.search import logger
 from nucliadb.search.search.fetch import (
     fetch_resources,
     get_labels_paragraph,
     get_labels_resource,
     get_seconds_paragraph,
 )
 from nucliadb_models.common import FieldTypeName
+from nucliadb_models.labels import translate_system_to_alias_label
 from nucliadb_models.metadata import RelationTypePbMap
 from nucliadb_models.resource import ExtractedDataTypeName
 from nucliadb_models.search import (
     DirectionalRelation,
     EntitySubgraph,
     KnowledgeboxSearchResults,
     KnowledgeboxSuggestResults,
+    MinScore,
     Paragraph,
     Paragraphs,
     RelatedEntities,
+    RelatedEntity,
     RelationDirection,
     RelationNodeTypeMap,
     Relations,
     ResourceProperties,
     ResourceResult,
     Resources,
     ResourceSearchResults,
     Sentence,
     Sentences,
     SortField,
     SortOptions,
     SortOrder,
     TextPosition,
 )
-from nucliadb_telemetry import errors
 
 from .cache import get_resource_cache, get_resource_from_cache
 from .metrics import merge_observer
-from .paragraphs import get_paragraph_text, get_text_sentence
+from .paragraphs import ExtractedTextCache, get_paragraph_text, get_text_sentence
 
-Bm25Score = Tuple[int, int]
+Bm25Score = tuple[float, float]
 TimestampScore = datetime.datetime
 TitleScore = str
 Score = Union[Bm25Score, TimestampScore, TitleScore]
 
 
-def sort_results_by_score(results: Union[List[ParagraphResult], List[DocumentResult]]):
+def sort_results_by_score(results: Union[list[ParagraphResult], list[DocumentResult]]):
     results.sort(key=lambda x: (x.score.bm25, x.score.booster), reverse=True)
 
 
 async def text_score(
     item: Union[DocumentResult, ParagraphResult],
     sort_field: SortField,
     kbid: str,
 ) -> Optional[Score]:
     """Returns the score for given `item` and `sort_field`. If the resource is being
     deleted, it might appear on search results but not in maindb. In this
     specific case, return None.
 
     """
-    score: Any = None
+    if sort_field == SortField.SCORE:
+        return (item.score.bm25, item.score.booster)
 
+    score: Any = None
     resource = await get_resource_from_cache(kbid, item.uuid)
     if resource is None:
         return score
     basic = await resource.get_basic()
     if basic is None:
         return score
 
-    if sort_field == SortField.SCORE:
-        score = (item.score.bm25, item.score.booster)
-    elif sort_field == SortField.CREATED:
+    if sort_field == SortField.CREATED:
         score = basic.created.ToDatetime()
     elif sort_field == SortField.MODIFIED:
         score = basic.modified.ToDatetime()
     elif sort_field == SortField.TITLE:
         score = basic.title
 
     return score
 
 
 async def merge_documents_results(
-    document_responses: List[DocumentSearchResponse],
-    resources: List[str],
+    document_responses: list[DocumentSearchResponse],
+    resources: list[str],
     count: int,
     page: int,
     kbid: str,
     sort: SortOptions,
+    min_score: float,
 ) -> Resources:
-    raw_resource_list: List[Tuple[DocumentResult, Score]] = []
-    facets: Dict[str, Any] = {}
+    raw_resource_list: list[tuple[DocumentResult, Score]] = []
+    facets: dict[str, Any] = {}
     query = None
     total = 0
     next_page = False
     for document_response in document_responses:
         if query is None:
             query = document_response.query
         if document_response.facets:
             for key, value in document_response.facets.items():
+                key = translate_system_to_alias_label(key)
                 for facetresult in value.facetresults:
-                    facets.setdefault(key, {}).setdefault(facetresult.tag, 0)
-                    facets[key][facetresult.tag] += facetresult.total
+                    facet_label = translate_system_to_alias_label(facetresult.tag)
+                    facets.setdefault(key, {}).setdefault(facet_label, 0)
+                    facets[key][facet_label] += facetresult.total
 
         if document_response.next_page:
             next_page = True
         for result in document_response.results:
             score = await text_score(result, sort.field, kbid)
             if score is not None:
                 raw_resource_list.append((result, score))
@@ -148,15 +153,15 @@
     skip = page * count
     end = skip + count
     length = len(raw_resource_list)
 
     if length > end:
         next_page = True
 
-    result_resource_list: List[ResourceResult] = []
+    result_resource_list: list[ResourceResult] = []
     for result, _ in raw_resource_list[min(skip, length) : min(end, length)]:
         # /f/file
 
         labels = await get_labels_resource(result, kbid)
         _, field_type, field = result.field.split("/")
 
         result_resource_list.append(
@@ -175,129 +180,145 @@
         facets=facets,
         results=result_resource_list,
         query=query,
         total=total,
         page_number=page,
         page_size=count,
         next_page=next_page,
+        min_score=min_score,
     )
 
 
 async def merge_suggest_paragraph_results(
-    suggest_responses: List[SuggestResponse],
+    suggest_responses: list[SuggestResponse],
     kbid: str,
     highlight: bool,
 ):
-    raw_paragraph_list: List[ParagraphResult] = []
+    raw_paragraph_list: list[ParagraphResult] = []
     query = None
     ematches = None
     for suggest_response in suggest_responses:
         if query is None:
             query = suggest_response.query
         if ematches is None:
             ematches = suggest_response.ematches
         for result in suggest_response.results:
             raw_paragraph_list.append(result)
 
     if len(suggest_responses) > 1:
         sort_results_by_score(raw_paragraph_list)
 
-    result_paragraph_list: List[Paragraph] = []
-    for result in raw_paragraph_list[:10]:
-        _, field_type, field = result.field.split("/")
-        text = await get_paragraph_text(
-            kbid=kbid,
-            rid=result.uuid,
-            field=result.field,
-            start=result.start,
-            end=result.end,
-            split=result.split,
-            highlight=highlight,
-            ematches=ematches,  # type: ignore
-            matches=result.matches,  # type: ignore
-        )
-        labels = await get_labels_paragraph(result, kbid)
-        new_paragraph = Paragraph(
-            score=result.score.bm25,
-            rid=result.uuid,
-            field_type=field_type,
-            field=field,
-            text=text,
-            labels=labels,
-            position=TextPosition(
-                index=result.metadata.position.index,
-                start=result.metadata.position.start,
-                end=result.metadata.position.end,
-                page_number=result.metadata.position.page_number,
-            ),
-        )
-        if len(result.metadata.position.start_seconds) or len(
-            result.metadata.position.end_seconds
-        ):
-            new_paragraph.start_seconds = list(result.metadata.position.start_seconds)
-            new_paragraph.end_seconds = list(result.metadata.position.end_seconds)
-        else:
-            # TODO: Remove once we are sure all data has been migrated!
-            seconds_positions = await get_seconds_paragraph(result, kbid)
-            if seconds_positions is not None:
-                new_paragraph.start_seconds = seconds_positions[0]
-                new_paragraph.end_seconds = seconds_positions[1]
-        result_paragraph_list.append(new_paragraph)
-
-    return Paragraphs(results=result_paragraph_list, query=query)
+    rcache = get_resource_cache(clear=True)
+    etcache = ExtractedTextCache()
+    try:
+        result_paragraph_list: list[Paragraph] = []
+        for result in raw_paragraph_list[:10]:
+            _, field_type, field = result.field.split("/")
+            text = await get_paragraph_text(
+                kbid=kbid,
+                rid=result.uuid,
+                field=result.field,
+                start=result.start,
+                end=result.end,
+                split=result.split,
+                highlight=highlight,
+                ematches=ematches,  # type: ignore
+                matches=result.matches,  # type: ignore
+                extracted_text_cache=etcache,
+            )
+            labels = await get_labels_paragraph(result, kbid)
+            new_paragraph = Paragraph(
+                score=result.score.bm25,
+                rid=result.uuid,
+                field_type=field_type,
+                field=field,
+                text=text,
+                labels=labels,
+                position=TextPosition(
+                    index=result.metadata.position.index,
+                    start=result.metadata.position.start,
+                    end=result.metadata.position.end,
+                    page_number=result.metadata.position.page_number,
+                ),
+            )
+            if len(result.metadata.position.start_seconds) or len(
+                result.metadata.position.end_seconds
+            ):
+                new_paragraph.start_seconds = list(
+                    result.metadata.position.start_seconds
+                )
+                new_paragraph.end_seconds = list(result.metadata.position.end_seconds)
+            else:
+                # TODO: Remove once we are sure all data has been migrated!
+                seconds_positions = await get_seconds_paragraph(result, kbid)
+                if seconds_positions is not None:
+                    new_paragraph.start_seconds = seconds_positions[0]
+                    new_paragraph.end_seconds = seconds_positions[1]
+            result_paragraph_list.append(new_paragraph)
+        return Paragraphs(results=result_paragraph_list, query=query, min_score=0)
+    finally:
+        etcache.clear()
+        rcache.clear()
 
 
 async def merge_vectors_results(
-    vector_responses: List[VectorSearchResponse],
-    resources: List[str],
+    vector_responses: list[VectorSearchResponse],
+    resources: list[str],
     kbid: str,
     count: int,
     page: int,
-    min_score: float = 0.70,
+    min_score: Optional[float] = None,
 ):
-    facets: Dict[str, Any] = {}
-    raw_vectors_list: List[DocumentScored] = []
+    facets: dict[str, Any] = {}
+    raw_vectors_list: list[DocumentScored] = []
 
     for vector_response in vector_responses:
         for document in vector_response.documents:
-            if document.score < min_score:
+            if min_score is not None and document.score < min_score:
                 continue
             if math.isnan(document.score):
                 continue
             raw_vectors_list.append(document)
 
     if len(vector_responses) > 1:
         raw_vectors_list.sort(key=lambda x: x.score, reverse=True)
 
     skip = page * count
     end_element = skip + count
     length = len(raw_vectors_list)
 
-    result_sentence_list: List[Sentence] = []
+    result_sentence_list: list[Sentence] = []
     for result in raw_vectors_list[min(skip, length) : min(end_element, length)]:
         id_count = result.doc_id.id.count("/")
         if id_count == 4:
             rid, field_type, field, index, position = result.doc_id.id.split("/")
             subfield = None
         elif id_count == 5:
             (
                 rid,
                 field_type,
                 field,
                 subfield,
                 index,
                 position,
             ) = result.doc_id.id.split("/")
-        start, end = position.split("-")
-        start_int = int(start)
-        end_int = int(end)
-        try:
-            index_int = int(index)
-        except ValueError:
-            index_int = -1
+        if result.metadata.HasField("position"):
+            start_int = result.metadata.position.start
+            end_int = result.metadata.position.end
+            index_int = result.metadata.position.index
+        else:
+            # bbb pull position from key for old results that were
+            # not properly filling metadata
+            start, end = position.split("-")
+            start_int = int(start)
+            end_int = int(end)
+            try:
+                index_int = int(index)
+            except ValueError:
+                index_int = -1
         text = await get_text_sentence(
             rid, field_type, field, kbid, index_int, start_int, end_int, subfield
         )
         result_sentence_list.append(
             Sentence(
                 score=result.score,
                 rid=rid,
@@ -308,44 +329,51 @@
                 position=TextPosition(start=start_int, end=end_int, index=index_int),
             )
         )
         if rid not in resources:
             resources.append(rid)
 
     return Sentences(
-        results=result_sentence_list, facets=facets, page_number=page, page_size=count
+        results=result_sentence_list,
+        facets=facets,
+        page_number=page,
+        page_size=count,
+        min_score=round(min_score or 0, ndigits=3),
     )
 
 
 async def merge_paragraph_results(
-    paragraph_responses: List[ParagraphSearchResponse],
-    resources: List[str],
+    paragraph_responses: list[ParagraphSearchResponse],
+    resources: list[str],
     kbid: str,
     count: int,
     page: int,
     highlight: bool,
     sort: SortOptions,
+    min_score: float,
 ):
-    raw_paragraph_list: List[Tuple[ParagraphResult, Score]] = []
-    facets: Dict[str, Any] = {}
+    raw_paragraph_list: list[tuple[ParagraphResult, Score]] = []
+    facets: dict[str, Any] = {}
     query = None
     next_page = False
-    ematches: Optional[List[str]] = None
+    ematches: Optional[list[str]] = None
     total = 0
     for paragraph_response in paragraph_responses:
         if ematches is None:
             ematches = paragraph_response.ematches  # type: ignore
         if query is None:
             query = paragraph_response.query
 
         if paragraph_response.facets:
             for key, value in paragraph_response.facets.items():
+                key = translate_system_to_alias_label(key)
                 for facetresult in value.facetresults:
-                    facets.setdefault(key, {}).setdefault(facetresult.tag, 0)
-                    facets[key][facetresult.tag] += facetresult.total
+                    facet_label = translate_system_to_alias_label(facetresult.tag)
+                    facets.setdefault(key, {}).setdefault(facet_label, 0)
+                    facets[key][facet_label] += facetresult.total
         if paragraph_response.next_page:
             next_page = True
         for result in paragraph_response.results:
             score = await text_score(result, sort.field, kbid)
             if score is not None:
                 raw_paragraph_list.append((result, score))
         total += paragraph_response.total
@@ -355,75 +383,95 @@
     skip = page * count
     end = skip + count
     length = len(raw_paragraph_list)
 
     if length > end:
         next_page = True
 
-    result_paragraph_list: List[Paragraph] = []
-    for result, _ in raw_paragraph_list[min(skip, length) : min(end, length)]:
-        _, field_type, field = result.field.split("/")
-        text = await get_paragraph_text(
-            kbid=kbid,
-            rid=result.uuid,
-            field=result.field,
-            start=result.start,
-            end=result.end,
-            split=result.split,
-            highlight=highlight,
-            ematches=ematches,
-            matches=result.matches,  # type: ignore
-        )
-        labels = await get_labels_paragraph(result, kbid)
-        new_paragraph = Paragraph(
-            score=result.score.bm25,
-            rid=result.uuid,
-            field_type=field_type,
-            field=field,
-            text=text,
-            labels=labels,
-            position=TextPosition(
-                index=result.metadata.position.index,
-                start=result.metadata.position.start,
-                end=result.metadata.position.end,
-                page_number=result.metadata.position.page_number,
-            ),
+    result_paragraph_list: list[Paragraph] = []
+    etcache = ExtractedTextCache()
+    try:
+        for result, _ in raw_paragraph_list[min(skip, length) : min(end, length)]:
+            _, field_type, field = result.field.split("/")
+            text = await get_paragraph_text(
+                kbid=kbid,
+                rid=result.uuid,
+                field=result.field,
+                start=result.start,
+                end=result.end,
+                split=result.split,
+                highlight=highlight,
+                ematches=ematches,
+                matches=result.matches,  # type: ignore
+                extracted_text_cache=etcache,
+            )
+            labels = await get_labels_paragraph(result, kbid)
+            fuzzy_result = len(result.matches) > 0
+            new_paragraph = Paragraph(
+                score=result.score.bm25,
+                rid=result.uuid,
+                field_type=field_type,
+                field=field,
+                text=text,
+                labels=labels,
+                position=TextPosition(
+                    index=result.metadata.position.index,
+                    start=result.metadata.position.start,
+                    end=result.metadata.position.end,
+                    page_number=result.metadata.position.page_number,
+                ),
+                fuzzy_result=fuzzy_result,
+            )
+            if len(result.metadata.position.start_seconds) or len(
+                result.metadata.position.end_seconds
+            ):
+                new_paragraph.start_seconds = list(
+                    result.metadata.position.start_seconds
+                )
+                new_paragraph.end_seconds = list(result.metadata.position.end_seconds)
+            else:
+                # TODO: Remove once we are sure all data has been migrated!
+                seconds_positions = await get_seconds_paragraph(result, kbid)
+                if seconds_positions is not None:
+                    new_paragraph.start_seconds = seconds_positions[0]
+                    new_paragraph.end_seconds = seconds_positions[1]
+
+            result_paragraph_list.append(new_paragraph)
+            if new_paragraph.rid not in resources:
+                resources.append(new_paragraph.rid)
+        return Paragraphs(
+            results=result_paragraph_list,
+            facets=facets,
+            query=query,
+            total=total,
+            page_number=page,
+            page_size=count,
+            next_page=next_page,
+            min_score=min_score,
         )
-        if len(result.metadata.position.start_seconds) or len(
-            result.metadata.position.end_seconds
-        ):
-            new_paragraph.start_seconds = list(result.metadata.position.start_seconds)
-            new_paragraph.end_seconds = list(result.metadata.position.end_seconds)
-        else:
-            # TODO: Remove once we are sure all data has been migrated!
-            seconds_positions = await get_seconds_paragraph(result, kbid)
-            if seconds_positions is not None:
-                new_paragraph.start_seconds = seconds_positions[0]
-                new_paragraph.end_seconds = seconds_positions[1]
-
-        result_paragraph_list.append(new_paragraph)
-        if new_paragraph.rid not in resources:
-            resources.append(new_paragraph.rid)
+    finally:
+        etcache.clear()
 
-    return Paragraphs(
-        results=result_paragraph_list,
-        facets=facets,
-        query=query,
-        total=total,
-        page_number=page,
-        page_size=count,
-        next_page=next_page,
+
+@merge_observer.wrap({"type": "merge_relations"})
+async def merge_relations_results(
+    relations_responses: list[RelationSearchResponse],
+    query: EntitiesSubgraphRequest,
+) -> Relations:
+    loop = asyncio.get_event_loop()
+    return await loop.run_in_executor(
+        None, _merge_relations_results, relations_responses, query
     )
 
 
-async def merge_relations_results(
-    relations_responses: List[RelationSearchResponse],
+def _merge_relations_results(
+    relations_responses: list[RelationSearchResponse],
     query: EntitiesSubgraphRequest,
 ) -> Relations:
-    relations = Relations(entities={}, graph=[])
+    relations = Relations(entities={})
 
     for entry_point in query.entry_points:
         relations.entities[entry_point.value] = EntitySubgraph(related_to=[])
 
     for relation_response in relations_responses:
         for relation in relation_response.subgraph.relations:
             origin = relation.source
@@ -447,38 +495,30 @@
                         entity=origin.value,
                         entity_type=RelationNodeTypeMap[origin.ntype],
                         relation=relation_type,
                         relation_label=relation_label,
                         direction=RelationDirection.IN,
                     )
                 )
-            else:
-                error_msg = "Relation search is returning an edge unrelated with queried entities"
-                logger.error(error_msg)
-                with errors.push_scope() as scope:
-                    scope.set_extra("relations_responses", relations_responses)
-                    scope.set_extra("query", query)
-                    scope.set_extra("relation", relation)
-                    errors.capture_message(error_msg, "error")
 
     return relations
 
 
 @merge_observer.wrap({"type": "merge"})
 async def merge_results(
-    search_responses: List[SearchResponse],
+    search_responses: list[SearchResponse],
     count: int,
     page: int,
     kbid: str,
-    show: List[ResourceProperties],
-    field_type_filter: List[FieldTypeName],
-    extracted: List[ExtractedDataTypeName],
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    extracted: list[ExtractedDataTypeName],
     sort: SortOptions,
     requested_relations: EntitiesSubgraphRequest,
-    min_score: float = 0.85,
+    min_score: MinScore,
     highlight: bool = False,
 ) -> KnowledgeboxSearchResults:
     paragraphs = []
     documents = []
     vectors = []
     relations = []
 
@@ -486,95 +526,106 @@
         paragraphs.append(response.paragraph)
         documents.append(response.document)
         vectors.append(response.vector)
         relations.append(response.relation)
 
     api_results = KnowledgeboxSearchResults()
 
-    get_resource_cache(clear=True)
-
-    resources: List[str] = list()
-    api_results.fulltext = await merge_documents_results(
-        documents, resources, count, page, kbid, sort
-    )
+    rcache = get_resource_cache(clear=True)
+    try:
+        resources: list[str] = list()
+        api_results.fulltext = await merge_documents_results(
+            documents, resources, count, page, kbid, sort, min_score=min_score.bm25
+        )
 
-    api_results.paragraphs = await merge_paragraph_results(
-        paragraphs,
-        resources,
-        kbid,
-        count,
-        page,
-        highlight,
-        sort,
-    )
+        api_results.paragraphs = await merge_paragraph_results(
+            paragraphs,
+            resources,
+            kbid,
+            count,
+            page,
+            highlight,
+            sort,
+            min_score=min_score.bm25,
+        )
 
-    api_results.sentences = await merge_vectors_results(
-        vectors, resources, kbid, count, page, min_score=min_score
-    )
+        api_results.sentences = await merge_vectors_results(
+            vectors, resources, kbid, count, page, min_score=min_score.semantic
+        )
 
-    api_results.relations = await merge_relations_results(
-        relations, requested_relations
-    )
+        api_results.relations = await merge_relations_results(
+            relations, requested_relations
+        )
 
-    api_results.resources = await fetch_resources(
-        resources, kbid, show, field_type_filter, extracted
-    )
-    return api_results
+        api_results.resources = await fetch_resources(
+            resources, kbid, show, field_type_filter, extracted
+        )
+        return api_results
+    finally:
+        rcache.clear()
 
 
 async def merge_paragraphs_results(
-    paragraph_responses: List[ParagraphSearchResponse],
+    paragraph_responses: list[ParagraphSearchResponse],
     count: int,
     page: int,
     kbid: str,
-    show: List[ResourceProperties],
-    field_type_filter: List[FieldTypeName],
-    extracted: List[ExtractedDataTypeName],
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
+    extracted: list[ExtractedDataTypeName],
     highlight_split: bool,
+    min_score: float,
 ) -> ResourceSearchResults:
     paragraphs = []
     for result in paragraph_responses:
         paragraphs.append(result)
 
     api_results = ResourceSearchResults()
 
-    resources: List[str] = list()
-    api_results.paragraphs = await merge_paragraph_results(
-        paragraphs,
-        resources,
-        kbid,
-        count,
-        page,
-        highlight=highlight_split,
-        sort=SortOptions(
-            field=SortField.SCORE,
-            order=SortOrder.DESC,
-            limit=None,
-        ),
-    )
-    return api_results
+    rcache = get_resource_cache(clear=True)
+    try:
+        resources: list[str] = list()
+        api_results.paragraphs = await merge_paragraph_results(
+            paragraphs,
+            resources,
+            kbid,
+            count,
+            page,
+            highlight=highlight_split,
+            sort=SortOptions(
+                field=SortField.SCORE,
+                order=SortOrder.DESC,
+                limit=None,
+            ),
+            min_score=min_score,
+        )
+        return api_results
+    finally:
+        rcache.clear()
 
 
 async def merge_suggest_entities_results(
-    suggest_responses: List[SuggestResponse],
+    suggest_responses: list[SuggestResponse],
 ) -> RelatedEntities:
-    merge = RelatedEntities(entities=[], total=0)
-
+    unique_entities: Set[RelatedEntity] = set()
     for response in suggest_responses:
-        merge.entities.extend(response.entities.entities)
-        merge.total += response.entities.total
+        response_entities = (
+            RelatedEntity(family=e.subtype, value=e.value)
+            for e in response.entity_results.nodes
+        )
+        unique_entities.update(response_entities)
 
-    return merge
+    return RelatedEntities(entities=list(unique_entities), total=len(unique_entities))
 
 
 async def merge_suggest_results(
-    suggest_responses: List[SuggestResponse],
+    suggest_responses: list[SuggestResponse],
     kbid: str,
-    show: List[ResourceProperties],
-    field_type_filter: List[FieldTypeName],
+    show: list[ResourceProperties],
+    field_type_filter: list[FieldTypeName],
     highlight: bool = False,
 ) -> KnowledgeboxSuggestResults:
     api_results = KnowledgeboxSuggestResults()
 
     api_results.paragraphs = await merge_suggest_paragraph_results(
         suggest_responses, kbid, highlight=highlight
     )
```

## nucliadb/search/search/metrics.py

```diff
@@ -16,7 +16,11 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from nucliadb_telemetry import metrics
 
 merge_observer = metrics.Observer("merge_results", labels={"type": ""})
+node_features = metrics.Counter("nucliadb_node_features", labels={"type": ""})
+query_parse_dependency_observer = metrics.Observer(
+    "query_parse_dependency", labels={"type": ""}
+)
```

## nucliadb/search/search/paragraphs.py

```diff
@@ -19,34 +19,26 @@
 
 import asyncio
 import logging
 import re
 import string
 from typing import Optional
 
-from redis import asyncio as aioredis
-from redis.asyncio.client import Redis
+from nucliadb_protos.utils_pb2 import ExtractedText
 
 from nucliadb.ingest.fields.base import Field
 from nucliadb.ingest.orm.resource import KB_REVERSE
 from nucliadb.ingest.orm.resource import Resource as ResourceORM
-from nucliadb.search.settings import settings
 from nucliadb_telemetry import metrics
-from nucliadb_utils.utilities import get_utility, set_utility
 
 from .cache import get_resource_from_cache
 
 logger = logging.getLogger(__name__)
 PRE_WORD = string.punctuation + " "
 
-CACHE_OPS = metrics.Counter("nucliadb_paragraph_cache_ops", labels={"type": "miss"})
-CACHE_HIT_DISTRIBUTION = metrics.Histogram(
-    "nucliadb_paragraph_cache_dist",
-    buckets=[1, 2, 4, 8, 16, 32, 64, 128, 512, 1024, metrics.INF],
-)
 GET_PARAGRAPH_LATENCY = metrics.Observer(
     "nucliadb_get_paragraph",
     buckets=[
         0.001,
         0.005,
         0.01,
         0.025,
@@ -59,109 +51,88 @@
         1.0,
         2.5,
         metrics.INF,
     ],
     labels={"type": "full"},
 )
 
-_PARAGRAPHS_CACHE_UTIL = "paragraphs_cache"
 
+EXTRACTED_CACHE_OPS = metrics.Counter(
+    "nucliadb_extracted_text_cache_ops", labels={"type": ""}
+)
 
-class ParagraphsCache:
-    """
-    Skeleton of paragraph cache.
 
-    For now, it will be used for us to track hits/misses on a potential
-    paragraph cache implementation.
+class ExtractedTextCache:
+    """
+    Used to cache extracted text from a resource in memory during
+    the process of search results serialization.
     """
-
-    consumer_task: Optional[asyncio.Task] = None
-    redis: Redis
 
     def __init__(self):
-        self.queue = asyncio.Queue()
+        self.locks = {}
+        self.values = {}
+
+    def get_value(self, key: str) -> Optional[ExtractedText]:
+        return self.values.get(key)
+
+    def get_lock(self, key: str) -> asyncio.Lock:
+        return self.locks.setdefault(key, asyncio.Lock())
 
-    async def initialize(self) -> None:
-        if (
-            settings.search_cache_redis_host is None
-            or settings.search_cache_redis_port is None
-        ):
-            # Cache is not configured, ignore
-            return
-        self.consumer_task = asyncio.create_task(self.queue_consumer())
-        self.redis = aioredis.from_url(
-            f"redis://{settings.search_cache_redis_host}:{settings.search_cache_redis_port}"
-        )
-
-    async def finalize(self) -> None:
-        if self.consumer_task is None:
-            return
-        self.consumer_task.cancel()
-        await self.redis.close(close_connection_pool=True)
-
-    async def queue_consumer(self) -> None:
-        while True:
-            try:
-                key = await self.queue.get()
-                key = key + "_hits"
-                val = await self.redis.get(key)
-                if val is None:
-                    CACHE_OPS.inc({"type": "miss"})
-                else:
-                    CACHE_OPS.inc({"type": "hit"})
-                    CACHE_HIT_DISTRIBUTION.observe(int(val))
-                await self.redis.incr(key, 1)
-                await self.redis.expire(key, 60 * 60)
-                self.queue.task_done()
-            except (
-                asyncio.CancelledError,
-                asyncio.TimeoutError,
-                RuntimeError,
-            ):
-                return
-            except Exception:  # pragma: no cover
-                logger.exception("Error in queue consumer, retrying...")
-                await asyncio.sleep(1)
-
-    async def get(
-        self,
-        *,
-        kbid: str,
-        rid: str,
-        field: str,
-        field_id: str,
-        start: int,
-        end: int,
-        split: Optional[str],
-    ) -> Optional[str]:
-        if self.consumer_task is None:
-            return None
-        key = f"{kbid}/{rid}/{field}/{field_id}::{start}-{end}:{split or ''}"
-        self.queue.put_nowait(key)
-        return None
-
-
-async def initialize_cache() -> None:
-    paragraphs_cache = ParagraphsCache()
-    await paragraphs_cache.initialize()
-    set_utility(_PARAGRAPHS_CACHE_UTIL, paragraphs_cache)
+    def set_value(self, key: str, value: ExtractedText) -> None:
+        self.values[key] = value
+
+    def clear(self):
+        self.values.clear()
+        self.locks.clear()
+
+
+async def get_field_extracted_text(
+    field: Field, cache: Optional[ExtractedTextCache] = None
+) -> Optional[ExtractedText]:
+    if cache is None:
+        return await field.get_extracted_text()
+
+    key = f"{field.kbid}/{field.uuid}/{field.id}"
+    extracted_text = cache.get_value(key)
+    if extracted_text is not None:
+        EXTRACTED_CACHE_OPS.inc({"type": "hit"})
+        return extracted_text
+
+    async with cache.get_lock(key):
+        # Check again in case another task already fetched it
+        extracted_text = cache.get_value(key)
+        if extracted_text is not None:
+            EXTRACTED_CACHE_OPS.inc({"type": "hit"})
+            return extracted_text
+
+        EXTRACTED_CACHE_OPS.inc({"type": "miss"})
+        extracted_text = await field.get_extracted_text()
+        if extracted_text is not None:
+            # Only cache if we actually have extracted text
+            cache.set_value(key, extracted_text)
+        return extracted_text
 
 
 @GET_PARAGRAPH_LATENCY.wrap({"type": "full"})
 async def get_paragraph_from_full_text(
-    *, field: Field, start: int, end: int, split: Optional[str] = None
+    *,
+    field: Field,
+    start: int,
+    end: int,
+    split: Optional[str] = None,
+    extracted_text_cache: Optional[ExtractedTextCache] = None,
 ) -> str:
     """
     Pull paragraph from full text stored in database.
 
     This requires downloading the full text and then slicing it.
     """
-    extracted_text = await field.get_extracted_text()
+    extracted_text = await get_field_extracted_text(field, cache=extracted_text_cache)
     if extracted_text is None:
-        logger.warning(f"{field} extracted_text does not exist on DB")
+        logger.warning(f"{field} extracted_text does not exist on DB yet")
         return ""
 
     if split not in (None, ""):
         text = extracted_text.split_text[split]  # type: ignore
         return text[start:end]
     else:
         return extracted_text.text[start:end]
@@ -177,44 +148,32 @@
     split: Optional[str] = None,
     highlight: bool = False,
     ematches: Optional[list[str]] = None,
     matches: Optional[list[str]] = None,
     orm_resource: Optional[
         ResourceORM
     ] = None,  # allow passing in orm_resource to avoid extra DB calls or txn issues
+    extracted_text_cache: Optional[ExtractedTextCache] = None,
 ) -> str:
     if orm_resource is None:
         orm_resource = await get_resource_from_cache(kbid, rid)
         if orm_resource is None:
             logger.error(f"{kbid}/{rid}:{field} does not exist on DB")
             return ""
 
     _, field_type, field = field.split("/")
     field_type_int = KB_REVERSE[field_type]
     field_obj = await orm_resource.get_field(field, field_type_int, load=False)
 
-    paragraphs_cache: ParagraphsCache = get_utility(_PARAGRAPHS_CACHE_UTIL)
-    cache_val = (
-        paragraphs_cache is not None
-        and await paragraphs_cache.get(
-            kbid=kbid,
-            rid=rid,
-            field=field,
-            field_id=field_obj.id,
-            start=start,
-            end=end,
-            split=split,
-        )
-        or None
-    )
-    if cache_val is not None:
-        return cache_val
-
     text = await get_paragraph_from_full_text(
-        field=field_obj, start=start, end=end, split=split
+        field=field_obj,
+        start=start,
+        end=end,
+        split=split,
+        extracted_text_cache=extracted_text_cache,
     )
 
     if highlight:
         text = highlight_paragraph(text, words=matches, ematches=ematches)
     return text
```

## nucliadb/search/search/query.py

```diff
@@ -13,278 +13,782 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-import re
+import asyncio
+import json
 from datetime import datetime
-from typing import List, Optional, Tuple
+from typing import Any, Awaitable, Optional, Union
 
-from fastapi import HTTPException
-from nucliadb_protos.nodereader_pb2 import (
-    ParagraphSearchRequest,
-    SearchRequest,
-    SuggestRequest,
-)
+from async_lru import alru_cache
 from nucliadb_protos.noderesources_pb2 import Resource
 
+from nucliadb.common import datamanagers
+from nucliadb.ingest.orm.synonyms import Synonyms
+from nucliadb.middleware.transaction import get_read_only_transaction
 from nucliadb.search import logger
-from nucliadb.search.predict import PredictVectorMissing, SendToPredictError
-from nucliadb.search.search.synonyms import apply_synonyms_to_request
+from nucliadb.search.predict import (
+    PredictVectorMissing,
+    SendToPredictError,
+    convert_relations,
+)
+from nucliadb.search.search.filters import (
+    convert_to_node_filters,
+    flat_filter_labels,
+    has_classification_label_filters,
+    split_labels_by_type,
+    translate_label,
+    translate_label_filters,
+)
+from nucliadb.search.search.metrics import (
+    node_features,
+    query_parse_dependency_observer,
+)
 from nucliadb.search.utilities import get_predict
+from nucliadb_models.labels import translate_system_to_alias_label
 from nucliadb_models.metadata import ResourceProcessingStatus
 from nucliadb_models.search import (
+    Filter,
+    MinScore,
+    QueryInfo,
     SearchOptions,
+    SentenceSearch,
+    SortField,
     SortFieldMap,
     SortOptions,
     SortOrder,
     SortOrderMap,
     SuggestOptions,
+    TokenSearch,
 )
+from nucliadb_models.security import RequestSecurity
+from nucliadb_protos import knowledgebox_pb2, nodereader_pb2, utils_pb2
+from nucliadb_utils import const
+from nucliadb_utils.utilities import has_feature
+
+from .exceptions import InvalidQueryError
+
+INDEX_SORTABLE_FIELDS = [
+    SortField.CREATED,
+    SortField.MODIFIED,
+]
+
+
+class QueryParser:
+    """
+    Queries are getting more and more complex and different phases of the query
+    depending on different data.
+
+    This class is an encapsulation of the different phases of the query and allow
+    some stateful interaction with a query and different depenedencies during
+    query parsing.
+    """
+
+    _min_score_task: Optional[asyncio.Task] = None
+    _query_information_task: Optional[asyncio.Task] = None
+    _convert_vectors_task: Optional[asyncio.Task] = None
+    _detected_entities_task: Optional[asyncio.Task] = None
+    _entities_meta_cache_task: Optional[asyncio.Task] = None
+    _deleted_entities_groups_task: Optional[asyncio.Task] = None
+    _synonyms_task: Optional[asyncio.Task] = None
+    _get_classification_labels_task: Optional[asyncio.Task] = None
+
+    def __init__(
+        self,
+        *,
+        kbid: str,
+        features: list[SearchOptions],
+        query: str,
+        filters: Union[list[str], list[Filter]],
+        page_number: int,
+        page_size: int,
+        min_score: MinScore,
+        faceted: Optional[list[str]] = None,
+        sort: Optional[SortOptions] = None,
+        range_creation_start: Optional[datetime] = None,
+        range_creation_end: Optional[datetime] = None,
+        range_modification_start: Optional[datetime] = None,
+        range_modification_end: Optional[datetime] = None,
+        fields: Optional[list[str]] = None,
+        user_vector: Optional[list[float]] = None,
+        vectorset: Optional[str] = None,
+        with_duplicates: bool = False,
+        with_status: Optional[ResourceProcessingStatus] = None,
+        with_synonyms: bool = False,
+        autofilter: bool = False,
+        key_filters: Optional[list[str]] = None,
+        security: Optional[RequestSecurity] = None,
+        generative_model: Optional[str] = None,
+        rephrase: Optional[bool] = False,
+    ):
+        self.kbid = kbid
+        self.features = features
+        self.query = query
+        self.filters: dict[str, Any] = convert_to_node_filters(filters)
+        self.flat_filter_labels: list[str] = []
+        self.faceted = faceted or []
+        self.page_number = page_number
+        self.page_size = page_size
+        self.min_score = min_score
+        self.sort = sort
+        self.range_creation_start = range_creation_start
+        self.range_creation_end = range_creation_end
+        self.range_modification_start = range_modification_start
+        self.range_modification_end = range_modification_end
+        self.fields = fields or []
+        self.user_vector = user_vector
+        self.vectorset = vectorset
+        self.with_duplicates = with_duplicates
+        self.with_status = with_status
+        self.with_synonyms = with_synonyms
+        self.autofilter = autofilter
+        self.key_filters = key_filters
+        self.security = security
+        self.generative_model = generative_model
+        self.rephrase = rephrase
+        self.query_endpoint_enabled = has_feature(
+            const.Features.PREDICT_QUERY_ENDPOINT,
+            default=False,
+            context={"kbid": self.kbid},
+        )
+
+        if len(self.filters) > 0:
+            self.filters = translate_label_filters(self.filters)
+            self.flat_filter_labels = flat_filter_labels(self.filters)
+
+    def _get_default_semantic_min_score(self) -> Awaitable[float]:
+        if self._min_score_task is None:  # pragma: no cover
+            self._min_score_task = asyncio.create_task(
+                get_default_semantic_min_score(self.kbid)
+            )
+        return self._min_score_task
 
-REMOVABLE_CHARS = re.compile(r"\¿|\?|\!|\¡|\,|\;|\.|\:")
+    def _get_converted_vectors(self) -> Awaitable[list[float]]:
+        if self._convert_vectors_task is None:  # pragma: no cover
+            self._convert_vectors_task = asyncio.create_task(
+                convert_vectors(self.kbid, self.query)
+            )
+        return self._convert_vectors_task
+
+    def _get_query_information(self) -> Awaitable[QueryInfo]:
+        if self.query_endpoint_enabled is False:
+            # XXX Can be removed once query endpoint is fully enabled
+            async def static_query():
+                return QueryInfo(
+                    visual_llm=False,
+                    max_context=300_000,
+                    entities=TokenSearch(tokens=[], time=0.0),
+                    sentence=SentenceSearch(data=[], time=0.0),
+                    query=self.query,
+                )
+
+            return static_query()
+        if self._query_information_task is None:  # pragma: no cover
+            self._query_information_task = asyncio.create_task(
+                query_information(
+                    self.kbid, self.query, self.generative_model, self.rephrase
+                )
+            )
+        return self._query_information_task
+
+    def _get_detected_entities(self) -> Awaitable[list[utils_pb2.RelationNode]]:
+        if self._detected_entities_task is None:  # pragma: no cover
+            self._detected_entities_task = asyncio.create_task(
+                detect_entities(self.kbid, self.query)
+            )
+        return self._detected_entities_task
+
+    def _get_entities_meta_cache(
+        self,
+    ) -> Awaitable[datamanagers.entities.EntitiesMetaCache]:
+        if self._entities_meta_cache_task is None:
+            self._entities_meta_cache_task = asyncio.create_task(
+                get_entities_meta_cache(self.kbid)
+            )
+        return self._entities_meta_cache_task
+
+    def _get_deleted_entity_groups(self) -> Awaitable[list[str]]:
+        if self._deleted_entities_groups_task is None:
+            self._deleted_entities_groups_task = asyncio.create_task(
+                get_deleted_entity_groups(self.kbid)
+            )
+        return self._deleted_entities_groups_task
+
+    def _get_synomyns(self) -> Awaitable[Optional[knowledgebox_pb2.Synonyms]]:
+        if self._synonyms_task is None:
+            self._synonyms_task = asyncio.create_task(get_kb_synonyms(self.kbid))
+        return self._synonyms_task
+
+    def _get_classification_labels(self) -> Awaitable[knowledgebox_pb2.Labels]:
+        if self._get_classification_labels_task is None:
+            self._get_classification_labels_task = asyncio.create_task(
+                get_classification_labels(self.kbid)
+            )
+        return self._get_classification_labels_task
+
+    async def _schedule_dependency_tasks(self) -> None:
+        """
+        This will schedule concurrent tasks for different data that needs to be pulled
+        for the sake of the query being performed
+        """
+        if len(self.filters) > 0 and has_classification_label_filters(
+            self.flat_filter_labels
+        ):
+            asyncio.ensure_future(self._get_classification_labels())
+        if self.min_score.semantic is None:
+            asyncio.ensure_future(self._get_default_semantic_min_score())
+
+        if SearchOptions.VECTOR in self.features and self.user_vector is None:
+            if self.query_endpoint_enabled:
+                asyncio.ensure_future(self._get_query_information())
+            else:
+                asyncio.ensure_future(self._get_converted_vectors())
+
+        if (SearchOptions.RELATIONS in self.features or self.autofilter) and len(
+            self.query
+        ) > 0:
+            if (
+                not self.query_endpoint_enabled
+                or SearchOptions.VECTOR not in self.features
+                or self.user_vector is not None
+            ):
+                self.query_endpoint_enabled = False
+                asyncio.ensure_future(self._get_detected_entities())
+            asyncio.ensure_future(self._get_entities_meta_cache())
+            asyncio.ensure_future(self._get_deleted_entity_groups())
+        if self.with_synonyms and self.query:
+            asyncio.ensure_future(self._get_synomyns())
+
+    async def parse(self) -> tuple[nodereader_pb2.SearchRequest, bool, list[str]]:
+        """
+        :return: (request, incomplete, autofilters)
+            where:
+                - request: protobuf nodereader_pb2.SearchRequest object
+                - incomplete: If the query is incomplete (missing vectors)
+                - autofilters: The autofilters that were applied
+        """
+        request = nodereader_pb2.SearchRequest()
+        request.body = self.query
+        request.with_duplicates = self.with_duplicates
+
+        await self._schedule_dependency_tasks()
+
+        await self.parse_filters(request)
+        self.parse_document_search(request)
+        self.parse_paragraph_search(request)
+        incomplete = await self.parse_vector_search(request)
+        autofilters = await self.parse_relation_search(request)
+        await self.parse_synonyms(request)
+
+        self.parse_sorting(request)
+        await self.parse_min_score(request)
+
+        return request, incomplete, autofilters
+
+    async def parse_filters(self, request: nodereader_pb2.SearchRequest) -> None:
+        if len(self.filters) > 0:
+            field_labels = self.flat_filter_labels
+            paragraph_labels: list[str] = []
+            if has_classification_label_filters(self.flat_filter_labels):
+                classification_labels = await self._get_classification_labels()
+                field_labels, paragraph_labels = split_labels_by_type(
+                    self.flat_filter_labels, classification_labels
+                )
+                check_supported_filters(self.filters, paragraph_labels)
+
+            request.filter.field_labels.extend(field_labels)
+            request.filter.paragraph_labels.extend(paragraph_labels)
+            request.filter.expression = json.dumps(self.filters)
+
+        request.faceted.labels.extend(
+            [translate_label(facet) for facet in self.faceted]
+        )
+        request.fields.extend(self.fields)
+
+        if self.security is not None and len(self.security.groups) > 0:
+            security_pb = utils_pb2.Security()
+            for group_id in self.security.groups:
+                if group_id not in security_pb.access_groups:
+                    security_pb.access_groups.append(group_id)
+            request.security.CopyFrom(security_pb)
+
+        if self.key_filters is not None and len(self.key_filters) > 0:
+            request.key_filters.extend(self.key_filters)
+            node_features.inc({"type": "key_filters"})
+
+        if self.with_status is not None:
+            request.with_status = PROCESSING_STATUS_TO_PB_MAP[self.with_status]
+
+        if self.range_creation_start is not None:
+            request.timestamps.from_created.FromDatetime(self.range_creation_start)
+
+        if self.range_creation_end is not None:
+            request.timestamps.to_created.FromDatetime(self.range_creation_end)
+
+        if self.range_modification_start is not None:
+            request.timestamps.from_modified.FromDatetime(self.range_modification_start)
+
+        if self.range_modification_end is not None:
+            request.timestamps.to_modified.FromDatetime(self.range_modification_end)
+
+    def parse_sorting(self, request: nodereader_pb2.SearchRequest) -> None:
+        if len(self.query) == 0:
+            if self.sort is None:
+                self.sort = SortOptions(
+                    field=SortField.CREATED,
+                    order=SortOrder.DESC,
+                    limit=None,
+                )
+            elif self.sort.field not in INDEX_SORTABLE_FIELDS:
+                raise InvalidQueryError(
+                    "sort_field",
+                    f"Empty query can only be sorted by '{SortField.CREATED}' or"
+                    f" '{SortField.MODIFIED}' and sort limit won't be applied",
+                )
+        else:
+            if self.sort is None:
+                self.sort = SortOptions(
+                    field=SortField.SCORE,
+                    order=SortOrder.DESC,
+                    limit=None,
+                )
+            elif (
+                self.sort.field not in INDEX_SORTABLE_FIELDS and self.sort.limit is None
+            ):
+                raise InvalidQueryError(
+                    "sort_field",
+                    f"Sort by '{self.sort.field}' requires setting a sort limit",
+                )
+
+        # We need to ask for all and cut later
+        request.page_number = 0
+        if self.sort and self.sort.limit is not None:
+            # As the index can't sort, we have to do it when merging. To
+            # have consistent results, we must limit them
+            request.result_per_page = self.sort.limit
+        else:
+            request.result_per_page = self.page_number * self.page_size + self.page_size
+
+        sort_field = SortFieldMap[self.sort.field] if self.sort else None
+        if sort_field is not None:
+            request.order.sort_by = sort_field
+            request.order.type = SortOrderMap[self.sort.order]  # type: ignore
+
+    async def parse_min_score(self, request: nodereader_pb2.SearchRequest) -> None:
+        if self.min_score.semantic is None:
+            self.min_score.semantic = await self._get_default_semantic_min_score()
+        request.min_score_semantic = self.min_score.semantic
+        request.min_score_bm25 = self.min_score.bm25
+
+    def parse_document_search(self, request: nodereader_pb2.SearchRequest) -> None:
+        if SearchOptions.DOCUMENT in self.features:
+            request.document = True
+            node_features.inc({"type": "documents"})
+
+    def parse_paragraph_search(self, request: nodereader_pb2.SearchRequest) -> None:
+        if SearchOptions.PARAGRAPH in self.features:
+            request.paragraph = True
+            node_features.inc({"type": "paragraphs"})
+
+    async def parse_vector_search(self, request: nodereader_pb2.SearchRequest) -> bool:
+        if SearchOptions.VECTOR not in self.features:
+            return False
+
+        node_features.inc({"type": "vectors"})
+
+        incomplete = False
+        if self.vectorset is not None:
+            request.vectorset = self.vectorset
+            node_features.inc({"type": "vectorset"})
+
+        if self.user_vector is None:
+            if self.query_endpoint_enabled:
+                try:
+                    query_info = await self._get_query_information()
+                    if query_info and query_info.sentence:
+                        request.vector.extend(query_info.sentence.data)
+                    else:
+                        incomplete = True
+                except SendToPredictError as err:
+                    logger.warning(
+                        f"Errors on predict api trying to embedd query: {err}"
+                    )
+                    incomplete = True
+                except PredictVectorMissing:
+                    logger.warning("Predict api returned an empty vector")
+                    incomplete = True
+            else:
+                try:
+                    request.vector.extend(await self._get_converted_vectors())
+                except SendToPredictError as err:
+                    logger.warning(
+                        f"Errors on predict api trying to embedd query: {err}"
+                    )
+                    incomplete = True
+                except PredictVectorMissing:
+                    logger.warning("Predict api returned an empty vector")
+                    incomplete = True
+        else:
+            request.vector.extend(self.user_vector)
+        return incomplete
+
+    async def parse_relation_search(
+        self, request: nodereader_pb2.SearchRequest
+    ) -> list[str]:
+        autofilters = []
+        relations_search = SearchOptions.RELATIONS in self.features
+        if relations_search or self.autofilter:
+            if not self.query_endpoint_enabled:
+                detected_entities = await self._get_detected_entities()
+            else:
+                query_info_result = await self._get_query_information()
+                if query_info_result.entities:
+                    detected_entities = convert_relations(
+                        query_info_result.entities.dict()
+                    )
+                else:
+                    detected_entities = []
+            meta_cache = await self._get_entities_meta_cache()
+            detected_entities = expand_entities(meta_cache, detected_entities)
+            if relations_search:
+                request.relation_subgraph.entry_points.extend(detected_entities)
+                request.relation_subgraph.depth = 1
+                request.relation_subgraph.deleted_groups.extend(
+                    await self._get_deleted_entity_groups()
+                )
+                for group_id, deleted_entities in meta_cache.deleted_entities.items():
+                    request.relation_subgraph.deleted_entities.append(
+                        nodereader_pb2.EntitiesSubgraphRequest.DeletedEntities(
+                            node_subtype=group_id, node_values=deleted_entities
+                        )
+                    )
+                node_features.inc({"type": "relations"})
+            if self.autofilter:
+                entity_filters = parse_entities_to_filters(request, detected_entities)
+                autofilters.extend(
+                    [translate_system_to_alias_label(e) for e in entity_filters]
+                )
+        return autofilters
+
+    async def parse_synonyms(self, request: nodereader_pb2.SearchRequest) -> None:
+        if not self.with_synonyms:
+            return
+
+        if (
+            SearchOptions.VECTOR in self.features
+            or SearchOptions.RELATIONS in self.features
+        ):
+            raise InvalidQueryError(
+                "synonyms",
+                "Search with custom synonyms is only supported on paragraph and document search",
+            )
 
+        if not self.query:
+            # Nothing to do
+            return
+
+        synonyms = await self._get_synomyns()
+        if synonyms is None:
+            # No synonyms found
+            return
+
+        synonyms_found: list[str] = []
+        advanced_query = []
+        for term in self.query.split(" "):
+            advanced_query.append(term)
+            term_synonyms = synonyms.terms.get(term)
+            if term_synonyms is None or len(term_synonyms.synonyms) == 0:
+                # No synonyms found for this term
+                continue
+            synonyms_found.extend(term_synonyms.synonyms)
+
+        if len(synonyms_found):
+            request.advanced_query = " OR ".join(advanced_query + synonyms_found)
+            request.ClearField("body")
+
+    async def get_visual_llm_enabled(self) -> bool:
+        return (await self._get_query_information()).visual_llm
+
+    async def get_max_context(self) -> int:
+        # Multiple by 3 is to have a good margin and guess
+        # between characters and tokens. This will be fully properly
+        # cut at the NUA API.
+        return (await self._get_query_information()).max_context * 3
 
-async def global_query_to_pb(
+
+async def paragraph_query_to_pb(
     kbid: str,
-    features: List[SearchOptions],
+    features: list[SearchOptions],
+    rid: str,
     query: str,
-    filters: List[str],
-    faceted: List[str],
+    fields: list[str],
+    filters: list[str],
+    faceted: list[str],
     page_number: int,
     page_size: int,
-    sort: SortOptions,
-    advanced_query: Optional[str] = None,
     range_creation_start: Optional[datetime] = None,
     range_creation_end: Optional[datetime] = None,
     range_modification_start: Optional[datetime] = None,
     range_modification_end: Optional[datetime] = None,
-    fields: Optional[List[str]] = None,
-    reload: bool = False,
-    user_vector: Optional[List[float]] = None,
-    vectorset: Optional[str] = None,
+    sort: Optional[str] = None,
+    sort_ord: str = SortOrder.DESC.value,
     with_duplicates: bool = False,
-    with_status: Optional[ResourceProcessingStatus] = None,
-    with_synonyms: bool = False,
-) -> Tuple[SearchRequest, bool]:
-    fields = fields or []
-
-    request = SearchRequest()
-    request.reload = reload
+) -> nodereader_pb2.ParagraphSearchRequest:
+    request = nodereader_pb2.ParagraphSearchRequest()
     request.with_duplicates = with_duplicates
 
-    if with_status is not None:
-        request.with_status = PROCESSING_STATUS_TO_PB_MAP[with_status]
-
     # We need to ask for all and cut later
     request.page_number = 0
-    if sort.limit is not None:
-        # As the index can't sort, we have to do it when merging. To
-        # have consistent results, we must limit them
-        request.result_per_page = sort.limit
-    else:
-        request.result_per_page = page_number * page_size + page_size
+    request.result_per_page = page_number * page_size + page_size
 
     if range_creation_start is not None:
         request.timestamps.from_created.FromDatetime(range_creation_start)
 
     if range_creation_end is not None:
         request.timestamps.to_created.FromDatetime(range_creation_end)
 
     if range_modification_start is not None:
         request.timestamps.from_modified.FromDatetime(range_modification_start)
 
     if range_modification_end is not None:
         request.timestamps.to_modified.FromDatetime(range_modification_end)
 
-    if SearchOptions.DOCUMENT in features or SearchOptions.PARAGRAPH in features:
+    if SearchOptions.PARAGRAPH in features:
+        request.uuid = rid
         request.body = query
-        if advanced_query is not None:
-            request.advanced_query = advanced_query
-        request.filter.tags.extend(filters)
-        request.faceted.tags.extend(faceted)
-
-        sort_field = SortFieldMap[sort.field]
-        if sort_field is not None:
-            request.order.sort_by = sort_field
-            request.order.type = SortOrderMap[sort.order]  # type: ignore
+        if len(filters) > 0:
+            field_labels = filters
+            paragraph_labels: list[str] = []
+            if has_classification_label_filters(filters):
+                classification_labels = await get_classification_labels(kbid)
+                field_labels, paragraph_labels = split_labels_by_type(
+                    filters, classification_labels
+                )
+            request.filter.field_labels.extend(field_labels)
+            request.filter.paragraph_labels.extend(paragraph_labels)
 
+        request.faceted.labels.extend([translate_label(facet) for facet in faceted])
+        if sort:
+            request.order.field = sort
+            request.order.type = sort_ord  # type: ignore
         request.fields.extend(fields)
 
-    request.document = SearchOptions.DOCUMENT in features
-    request.paragraph = SearchOptions.PARAGRAPH in features
-
-    incomplete = False
-    if SearchOptions.VECTOR in features:
-        incomplete = await _parse_vectors(
-            request, kbid, query, user_vector=user_vector, vectorset=vectorset
-        )
-
-    if SearchOptions.RELATIONS in features:
-        await _parse_entities(request, kbid, query)
+    return request
 
-    if with_synonyms:
-        if advanced_query:
-            raise HTTPException(
-                status_code=422,
-                detail="Search with custom synonyms is not compatible with providing advanced search",
-            )
-        if SearchOptions.VECTOR in features or SearchOptions.RELATIONS in features:
-            raise HTTPException(
-                status_code=422,
-                detail="Search with custom synonyms is only supported on paragraph and document search",
-            )
-        await apply_synonyms_to_request(request, kbid)
 
-    return request, incomplete
+@query_parse_dependency_observer.wrap({"type": "convert_vectors"})
+async def convert_vectors(kbid: str, query: str) -> list[float]:
+    predict = get_predict()
+    return await predict.convert_sentence_to_vector(kbid, query)
 
 
-async def _parse_vectors(
-    request: SearchRequest,
+@query_parse_dependency_observer.wrap({"type": "query_information"})
+async def query_information(
     kbid: str,
     query: str,
-    user_vector: Optional[List[float]],
-    vectorset: Optional[str],
-) -> bool:
-    incomplete = False
-    if vectorset is not None:
-        request.vectorset = vectorset
-    if user_vector is None:
-        predict = get_predict()
-        try:
-            predict_vector = await predict.convert_sentence_to_vector(kbid, query)
-            request.vector.extend(predict_vector)
-        except SendToPredictError as err:
-            logger.warning(f"Errors on predict api trying to embedd query: {err}")
-            incomplete = True
-        except PredictVectorMissing:
-            logger.warning("Predict api returned an empty vector")
-            incomplete = True
-    else:
-        request.vector.extend(user_vector)
-    return incomplete
+    generative_model: Optional[str] = None,
+    rephrase: bool = False,
+) -> QueryInfo:
+    predict = get_predict()
+    return await predict.query(kbid, query, generative_model, rephrase)
 
 
-async def _parse_entities(request: SearchRequest, kbid: str, query: str):
+@query_parse_dependency_observer.wrap({"type": "detect_entities"})
+async def detect_entities(kbid: str, query: str) -> list[utils_pb2.RelationNode]:
     predict = get_predict()
     try:
-        detected_entities = await predict.detect_entities(kbid, query)
-        request.relation_subgraph.entry_points.extend(detected_entities)
-        request.relation_subgraph.depth = 1
+        return await predict.detect_entities(kbid, query)
     except SendToPredictError as ex:
         logger.warning(f"Errors on predict api detecting entities: {ex}")
+        return []
 
 
-async def suggest_query_to_pb(
-    features: List[SuggestOptions],
-    query: str,
-    fields: List[str],
-    filters: List[str],
-    faceted: List[str],
-    range_creation_start: Optional[datetime] = None,
-    range_creation_end: Optional[datetime] = None,
-    range_modification_start: Optional[datetime] = None,
-    range_modification_end: Optional[datetime] = None,
-) -> SuggestRequest:
-    request = SuggestRequest()
-    if SuggestOptions.PARAGRAPH in features:
-        request.body = query
-        request.filter.tags.extend(filters)
-        request.fields.extend(fields)
+def expand_entities(
+    meta_cache: datamanagers.entities.EntitiesMetaCache,
+    detected_entities: list[utils_pb2.RelationNode],
+) -> list[utils_pb2.RelationNode]:
+    """
+    Iterate through duplicated entities in a kb.
+
+    The algorithm first makes it so we can look up duplicates by source and
+    by the referenced entity and expands from both directions.
+    """
+    result_entities = {entity.value: entity for entity in detected_entities}
+    duplicated_entities = meta_cache.duplicate_entities
+    duplicated_entities_by_value = meta_cache.duplicate_entities_by_value
 
-    if range_creation_start is not None:
-        request.timestamps.from_created.FromDatetime(range_creation_start)
-    if range_creation_end is not None:
-        request.timestamps.to_created.FromDatetime(range_creation_end)
-    if range_modification_start is not None:
-        request.timestamps.from_modified.FromDatetime(range_modification_start)
-    if range_modification_end is not None:
-        request.timestamps.to_modified.FromDatetime(range_modification_end)
+    for entity in detected_entities[:]:
+        if entity.subtype not in duplicated_entities:
+            continue
 
-    return request
+        if entity.value in duplicated_entities[entity.subtype]:
+            for duplicate in duplicated_entities[entity.subtype][entity.value]:
+                result_entities[duplicate] = utils_pb2.RelationNode(
+                    ntype=utils_pb2.RelationNode.NodeType.ENTITY,
+                    subtype=entity.subtype,
+                    value=duplicate,
+                )
+
+        if entity.value in duplicated_entities_by_value[entity.subtype]:
+            source_duplicate = duplicated_entities_by_value[entity.subtype][
+                entity.value
+            ]
+            result_entities[source_duplicate] = utils_pb2.RelationNode(
+                ntype=utils_pb2.RelationNode.NodeType.ENTITY,
+                subtype=entity.subtype,
+                value=source_duplicate,
+            )
 
+            if source_duplicate in duplicated_entities[entity.subtype]:
+                for duplicate in duplicated_entities[entity.subtype][source_duplicate]:
+                    if duplicate == entity.value:
+                        continue
+                    result_entities[duplicate] = utils_pb2.RelationNode(
+                        ntype=utils_pb2.RelationNode.NodeType.ENTITY,
+                        subtype=entity.subtype,
+                        value=duplicate,
+                    )
+
+    return list(result_entities.values())
+
+
+def parse_entities_to_filters(
+    request: nodereader_pb2.SearchRequest,
+    detected_entities: list[utils_pb2.RelationNode],
+) -> list[str]:
+    added_filters = []
+    for entity_filter in [
+        f"/e/{entity.subtype}/{entity.value}"
+        for entity in detected_entities
+        if entity.ntype == utils_pb2.RelationNode.NodeType.ENTITY
+    ]:
+        if entity_filter not in request.filter.field_labels:
+            request.filter.field_labels.append(entity_filter)
+            added_filters.append(entity_filter)
+
+    # We need to expand the filter expression with the automatically detected entities.
+    if len(added_filters) > 0:
+        # So far, autofilters feature will only yield 'and' expressions with the detected entities.
+        # More complex autofilters can be added here if we leverage the query endpoint.
+        expanded_expression = {"and": [{"literal": entity} for entity in added_filters]}
+        if request.filter.expression:
+            expression = json.loads(request.filter.expression)
+            expanded_expression["and"].append(expression)
+        request.filter.expression = json.dumps(expanded_expression)
+    return added_filters
 
-async def paragraph_query_to_pb(
-    features: List[SearchOptions],
-    rid: str,
+
+def suggest_query_to_pb(
+    features: list[SuggestOptions],
     query: str,
-    fields: List[str],
-    filters: List[str],
-    faceted: List[str],
-    page_number: int,
-    page_size: int,
+    fields: list[str],
+    filters: list[str],
+    faceted: list[str],
     range_creation_start: Optional[datetime] = None,
     range_creation_end: Optional[datetime] = None,
     range_modification_start: Optional[datetime] = None,
     range_modification_end: Optional[datetime] = None,
-    sort: Optional[str] = None,
-    sort_ord: str = SortOrder.DESC.value,
-    reload: bool = False,
-    with_duplicates: bool = False,
-) -> ParagraphSearchRequest:
-    request = ParagraphSearchRequest()
-    request.reload = reload
-    request.with_duplicates = with_duplicates
+) -> nodereader_pb2.SuggestRequest:
+    request = nodereader_pb2.SuggestRequest()
 
-    # We need to ask for all and cut later
-    request.page_number = 0
-    request.result_per_page = page_number * page_size + page_size
+    request.body = query
+    if SuggestOptions.ENTITIES in features:
+        request.features.append(nodereader_pb2.SuggestFeatures.ENTITIES)
+
+    if SuggestOptions.PARAGRAPH in features:
+        request.features.append(nodereader_pb2.SuggestFeatures.PARAGRAPHS)
+        filters = [translate_label(fltr) for fltr in filters]
+        request.filter.field_labels.extend(filters)
+        request.fields.extend(fields)
 
     if range_creation_start is not None:
         request.timestamps.from_created.FromDatetime(range_creation_start)
-
     if range_creation_end is not None:
         request.timestamps.to_created.FromDatetime(range_creation_end)
-
     if range_modification_start is not None:
         request.timestamps.from_modified.FromDatetime(range_modification_start)
-
     if range_modification_end is not None:
         request.timestamps.to_modified.FromDatetime(range_modification_end)
 
-    if SearchOptions.PARAGRAPH in features:
-        request.uuid = rid
-        request.body = query
-        request.filter.tags.extend(filters)
-        request.faceted.tags.extend(faceted)
-        if sort:
-            request.order.field = sort
-            request.order.type = sort_ord  # type: ignore
-        request.fields.extend(fields)
-
     return request
 
 
 PROCESSING_STATUS_TO_PB_MAP = {
     ResourceProcessingStatus.PENDING: Resource.ResourceStatus.PENDING,
     ResourceProcessingStatus.PROCESSED: Resource.ResourceStatus.PROCESSED,
     ResourceProcessingStatus.ERROR: Resource.ResourceStatus.ERROR,
+    ResourceProcessingStatus.EMPTY: Resource.ResourceStatus.EMPTY,
+    ResourceProcessingStatus.BLOCKED: Resource.ResourceStatus.BLOCKED,
+    ResourceProcessingStatus.EXPIRED: Resource.ResourceStatus.EXPIRED,
 }
 
 
-def pre_process_query(user_query: str) -> str:
-    # NOTE: if this logic grows in the future, consider using a Strategy pattern.
-    user_terms = user_query.split()
-    result = []
-    in_quote = False
-    for term in user_terms:
-        term = term.strip()
-        if in_quote:
-            result.append(term)
-            continue
-
-        if term.startswith('"'):
-            in_quote = True
-            result.append(term)
-            continue
-
-        if term.endswith('"'):
-            in_quote = False
+@query_parse_dependency_observer.wrap({"type": "min_score"})
+async def get_kb_model_default_min_score(kbid: str) -> Optional[float]:
+    txn = await get_read_only_transaction()
+    model = await datamanagers.kb.get_model_metadata(txn, kbid=kbid)
+    if model.HasField("default_min_score"):
+        return model.default_min_score
+    else:
+        return None
 
-        term = REMOVABLE_CHARS.sub("", term)
-        term = term.strip()
-        if len(term):
-            result.append(term)
 
-    return " ".join(result)
+@alru_cache(maxsize=None)
+async def get_default_semantic_min_score(kbid: str) -> float:
+    fallback = 0.7
+    model_min_score = await get_kb_model_default_min_score(kbid)
+    if model_min_score is not None:
+        return model_min_score
+    return fallback
+
+
+@query_parse_dependency_observer.wrap({"type": "synonyms"})
+async def get_kb_synonyms(kbid: str) -> Optional[knowledgebox_pb2.Synonyms]:
+    txn = await get_read_only_transaction()
+    return await Synonyms(txn, kbid).get()
+
+
+@query_parse_dependency_observer.wrap({"type": "entities_meta_cache"})
+async def get_entities_meta_cache(kbid: str) -> datamanagers.entities.EntitiesMetaCache:
+    txn = await get_read_only_transaction()
+    return await datamanagers.entities.get_entities_meta_cache(txn, kbid=kbid)
+
+
+@query_parse_dependency_observer.wrap({"type": "deleted_entities_groups"})
+async def get_deleted_entity_groups(kbid: str) -> list[str]:
+    txn = await get_read_only_transaction()
+    return list(
+        (await datamanagers.entities.get_deleted_groups(txn, kbid=kbid)).entities_groups
+    )
+
+
+@query_parse_dependency_observer.wrap({"type": "classification_labels"})
+async def get_classification_labels(kbid: str) -> knowledgebox_pb2.Labels:
+    txn = await get_read_only_transaction()
+    return await datamanagers.labels.get_labels(txn, kbid=kbid)
+
+
+def check_supported_filters(filters: dict[str, Any], paragraph_labels: list[str]):
+    """
+    Check if the provided filters are supported:
+    Paragraph labels can only be used with simple 'and' expressions (not nested).
+    """
+    if len(paragraph_labels) == 0:
+        return
+    if "literal" in filters:
+        return
+    if "and" not in filters:
+        # Paragraph labels can only be used with 'and' filter
+        raise InvalidQueryError(
+            "filters",
+            "Paragraph labels can only be used with 'all' filter",
+        )
+    for term in filters["and"]:
+        # Nested expressions are not allowed with paragraph labels
+        if "literal" not in term:
+            raise InvalidQueryError(
+                "filters",
+                "Paragraph labels can only be used with 'all' filter",
+            )
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## nucliadb/search/search/shards.py

```diff
@@ -13,14 +13,15 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import asyncio
 from typing import Optional
 
 from nucliadb_protos.nodereader_pb2 import (
     GetShardRequest,
     ParagraphSearchRequest,
     ParagraphSearchResponse,
     RelationSearchRequest,
@@ -28,52 +29,68 @@
     SearchRequest,
     SearchResponse,
     SuggestRequest,
     SuggestResponse,
 )
 from nucliadb_protos.noderesources_pb2 import Shard
 
-from nucliadb.ingest.orm.node import Node
+from nucliadb.common.cluster.base import AbstractIndexNode
 from nucliadb_telemetry import metrics
 
-node_observer = metrics.Observer("node_client", labels={"type": ""})
+node_observer = metrics.Observer(
+    "node_client",
+    labels={"type": "", "node_id": ""},
+    error_mappings={
+        "timeout": asyncio.CancelledError,
+    },
+)
 
 
-async def query_shard(node: Node, shard: str, query: SearchRequest) -> SearchResponse:
-    query.shard = shard
-    with node_observer({"type": "search"}):
-        return await node.reader.Search(query)  # type: ignore
+async def query_shard(
+    node: AbstractIndexNode, shard: str, query: SearchRequest
+) -> SearchResponse:
+    req = SearchRequest()
+    req.CopyFrom(query)
+    req.shard = shard
+    with node_observer({"type": "search", "node_id": node.id}):
+        return await node.reader.Search(req)  # type: ignore
 
 
 async def get_shard(
-    node: Node, shard_id: str, vectorset: Optional[str] = None
+    node: AbstractIndexNode, shard_id: str, vectorset: Optional[str] = None
 ) -> Shard:
     req = GetShardRequest()
     req.shard_id.id = shard_id
     if vectorset is not None:
         req.vectorset = vectorset
-    with node_observer({"type": "get_shard"}):
+    with node_observer({"type": "get_shard", "node_id": node.id}):
         return await node.reader.GetShard(req)  # type: ignore
 
 
 async def query_paragraph_shard(
-    node: Node, shard: str, query: ParagraphSearchRequest
+    node: AbstractIndexNode, shard: str, query: ParagraphSearchRequest
 ) -> ParagraphSearchResponse:
-    query.id = shard
-    with node_observer({"type": "paragraph_search"}):
-        return await node.reader.ParagraphSearch(query)  # type: ignore
+    req = ParagraphSearchRequest()
+    req.CopyFrom(query)
+    req.id = shard
+    with node_observer({"type": "paragraph_search", "node_id": node.id}):
+        return await node.reader.ParagraphSearch(req)  # type: ignore
 
 
 async def suggest_shard(
-    node: Node, shard: str, query: SuggestRequest
+    node: AbstractIndexNode, shard: str, query: SuggestRequest
 ) -> SuggestResponse:
-    query.shard = shard
-    with node_observer({"type": "suggest"}):
-        return await node.reader.Suggest(query)  # type: ignore
+    req = SuggestRequest()
+    req.CopyFrom(query)
+    req.shard = shard
+    with node_observer({"type": "suggest", "node_id": node.id}):
+        return await node.reader.Suggest(req)  # type: ignore
 
 
 async def relations_shard(
-    node: Node, shard: str, query: RelationSearchRequest
+    node: AbstractIndexNode, shard: str, query: RelationSearchRequest
 ) -> RelationSearchResponse:
-    query.shard_id = shard
-    with node_observer({"type": "relation_search"}):
-        return await node.reader.RelationSearch(query)  # type: ignore
+    req = RelationSearchRequest()
+    req.CopyFrom(query)
+    req.shard_id = shard
+    with node_observer({"type": "relation_search", "node_id": node.id}):
+        return await node.reader.RelationSearch(req)  # type: ignore
```

## nucliadb/search/search/utils.py

```diff
@@ -13,57 +13,62 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from fastapi import HTTPException
+from typing import Optional, Union
 
-from nucliadb_models.search import SearchRequest, SortField, SortOptions, SortOrder
+from nucliadb_models.search import BaseSearchRequest, MinScore
 
-INDEX_SORTABLE_FIELDS = [
-    SortField.CREATED,
-    SortField.MODIFIED,
-]
-
-
-def parse_sort_options(item: SearchRequest) -> SortOptions:
-    if is_empty_query(item):
-        if item.sort is None:
-            sort_options = SortOptions(
-                field=SortField.CREATED,
-                order=SortOrder.DESC,
-                limit=None,
-            )
-        elif item.sort.field not in INDEX_SORTABLE_FIELDS:
-            raise HTTPException(
-                status_code=422,
-                detail=(
-                    f"Empty query can only be sorted by '{SortField.CREATED}' or"
-                    f" '{SortField.MODIFIED}' and sort limit won't be applied"
-                ),
-            )
-        else:
-            sort_options = item.sort
-    else:
-        if item.sort is None:
-            sort_options = SortOptions(
-                field=SortField.SCORE,
-                order=SortOrder.DESC,
-                limit=None,
-            )
-        elif item.sort.field not in INDEX_SORTABLE_FIELDS and item.sort.limit is None:
-            raise HTTPException(
-                status_code=422,
-                detail=f"Sort by '{item.sort.field}' requires setting a sort limit",
-            )
-        else:
-            sort_options = item.sort
-
-    return sort_options
-
-
-def is_empty_query(request: SearchRequest) -> bool:
-    return len(request.query) == 0 and (
-        request.advanced_query is None or len(request.advanced_query) == 0
+
+def is_empty_query(request: BaseSearchRequest) -> bool:
+    return len(request.query) == 0
+
+
+def has_user_vectors(request: BaseSearchRequest) -> bool:
+    return request.vector is not None and len(request.vector) > 0
+
+
+def is_exact_match_only_query(request: BaseSearchRequest) -> bool:
+    """
+    '"something"' -> True
+    'foo "something" else' -> False
+    """
+    query = request.query.strip()
+    return len(query) > 0 and query[0] == '"' and query[-1] == '"'
+
+
+def should_disable_vector_search(request: BaseSearchRequest) -> bool:
+    if has_user_vectors(request):
+        return False
+
+    if is_exact_match_only_query(request):
+        return True
+
+    if is_empty_query(request):
+        return True
+
+    return False
+
+
+def min_score_from_query_params(
+    min_score_bm25: float,
+    min_score_semantic: Optional[float],
+    deprecated_min_score: Optional[float],
+) -> MinScore:
+    # Keep backward compatibility with the deprecated min_score parameter
+    semantic = (
+        deprecated_min_score if min_score_semantic is None else min_score_semantic
     )
+    return MinScore(bm25=min_score_bm25, semantic=semantic)
+
+
+def min_score_from_payload(min_score: Optional[Union[float, MinScore]]) -> MinScore:
+    # Keep backward compatibility with the deprecated
+    # min_score payload parameter being a float
+    if min_score is None:
+        return MinScore(bm25=0, semantic=None)
+    elif isinstance(min_score, float):
+        return MinScore(bm25=0, semantic=min_score)
+    return min_score
```

## nucliadb/search/tests/conftest.py

```diff
@@ -15,16 +15,19 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 pytest_plugins = [
     "pytest_docker_fixtures",
-    "nucliadb.ingest.tests.fixtures",
+    "nucliadb.tests.fixtures",
+    "nucliadb.tests.tikv",
+    "nucliadb.ingest.tests.fixtures",  # should be refactored out
     "nucliadb.search.tests.node",
     "nucliadb.search.tests.fixtures",
+    "nucliadb_utils.tests.conftest",
     "nucliadb_utils.tests.gcs",
     "nucliadb_utils.tests.s3",
     "nucliadb_utils.tests.nats",
     "nucliadb_utils.tests.asyncbenchmark",
     "nucliadb_utils.tests.indexing",
 ]
```

## nucliadb/search/tests/fixtures.py

```diff
@@ -15,110 +15,85 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 import asyncio
 from enum import Enum
-from os.path import dirname
-from typing import Dict, List, Optional
+from typing import Optional
 
 import pytest
 from httpx import AsyncClient
 from nucliadb_protos.nodereader_pb2 import GetShardRequest
 from nucliadb_protos.noderesources_pb2 import Shard
 from redis import asyncio as aioredis
-from starlette.routing import Mount
 
+from nucliadb.common.cluster.manager import KBShardManager, get_index_node
+from nucliadb.common.maindb.utils import get_driver
 from nucliadb.ingest.cache import clear_ingest_cache
-from nucliadb.ingest.orm import NODES
-from nucliadb.ingest.orm.node import Node
 from nucliadb.ingest.tests.fixtures import broker_resource
-from nucliadb.ingest.utils import get_driver
 from nucliadb.search import API_PREFIX
 from nucliadb_utils.tests import free_port
 from nucliadb_utils.utilities import clear_global_cache
 
 
 @pytest.fixture(scope="function")
-def test_settings_search(gcs, redis, node, maindb_driver):  # type: ignore
+def test_settings_search(storage, natsd, node, maindb_driver):  # type: ignore
     from nucliadb.ingest.settings import settings as ingest_settings
     from nucliadb_utils.cache.settings import settings as cache_settings
     from nucliadb_utils.settings import (
-        FileBackendConfig,
         nuclia_settings,
         nucliadb_settings,
         running_settings,
-        storage_settings,
     )
-    from nucliadb_utils.storages.settings import settings as extended_storage_settings
 
-    storage_settings.gcs_endpoint_url = gcs
-    storage_settings.file_backend = FileBackendConfig.GCS
-    storage_settings.gcs_bucket = "test_{kbid}"
-
-    extended_storage_settings.gcs_indexing_bucket = "indexing"
-    extended_storage_settings.gcs_deadletter_bucket = "deadletter"
-
-    url = f"redis://{redis[0]}:{redis[1]}"
-    cache_settings.cache_pubsub_driver = "redis"
-    cache_settings.cache_pubsub_channel = "pubsub-nuclia"
-    cache_settings.cache_pubsub_redis_url = url
+    cache_settings.cache_pubsub_nats_url = [natsd]
 
     running_settings.debug = False
 
     ingest_settings.disable_pull_worker = True
 
     ingest_settings.nuclia_partitions = 1
 
     nuclia_settings.dummy_processing = True
     nuclia_settings.dummy_predict = True
+    nuclia_settings.dummy_learning_services = True
 
     ingest_settings.grpc_port = free_port()
 
     nucliadb_settings.nucliadb_ingest = f"localhost:{ingest_settings.grpc_port}"
 
-    extended_storage_settings.local_testing_files = f"{dirname(__file__)}"
-
 
 @pytest.mark.asyncio
 @pytest.fixture(scope="function")
 async def search_api(test_settings_search, transaction_utility, redis):  # type: ignore
-    from nucliadb.ingest.orm import NODES
+    from nucliadb.common.cluster import manager
     from nucliadb.search.app import application
 
-    async def handler(req, exc):  # type: ignore
-        raise exc
-
     driver = aioredis.from_url(f"redis://{redis[0]}:{redis[1]}")
     await driver.flushall()
 
-    # Little hack to raise exeptions from VersionedFastApi
-    for route in application.routes:
-        if isinstance(route, Mount):
-            route.app.middleware_stack.handler = handler  # type: ignore
-
     await application.router.startup()
 
     # Make sure is clean
     await asyncio.sleep(1)
     count = 0
-    while len(NODES) < 2:
+    while len(manager.INDEX_NODES) < 2:
         print("awaiting cluster nodes - search fixtures.py")
         await asyncio.sleep(1)
         if count == 40:
             raise Exception("No cluster")
         count += 1
 
     def make_client_fixture(
-        roles: Optional[List[Enum]] = None,
+        roles: Optional[list[Enum]] = None,
         user: str = "",
         version: str = "1",
         root: bool = False,
-        extra_headers: Optional[Dict[str, str]] = None,
+        extra_headers: Optional[dict[str, str]] = None,
     ) -> AsyncClient:
         roles = roles or []
         client_base_url = "http://test"
 
         if root is False:
             client_base_url = f"{client_base_url}/{API_PREFIX}/v{version}"
 
@@ -139,95 +114,83 @@
     await application.router.shutdown()
     # Make sure nodes can sync
     await asyncio.sleep(1)
     await driver.flushall()
     await driver.close(close_connection_pool=True)
     clear_ingest_cache()
     clear_global_cache()
-    for node in NODES.values():
-        node._reader = None
-        node._writer = None
-        node._sidecar = None
+    manager.INDEX_NODES.clear()
 
 
 @pytest.fixture(scope="function")
 async def test_search_resource(
     indexing_utility_registered,
     processor,
     knowledgebox_ingest,
 ):
     """
     Create a resource that has every possible bit of information
     """
     message1 = broker_resource(knowledgebox_ingest, rid="foobar", slug="foobar-slug")
-
-    return await inject_message(processor, knowledgebox_ingest, message1)
-
-
-@pytest.fixture(scope="function")
-async def test_resource_deterministic_ids(
-    indexing_utility_registered,
-    processor,
-    knowledgebox_ingest,
-):
-    rid = "foobar"
-    slug = "foobar-slug"
-    message1 = broker_resource(knowledgebox_ingest, rid=rid, slug=slug)
-    kb = await inject_message(processor, knowledgebox_ingest, message1)
-    return kb, rid, slug
+    kbid = await inject_message(processor, knowledgebox_ingest, message1)
+    resource_field_count = 3
+    await wait_for_shard(knowledgebox_ingest, resource_field_count)
+    yield kbid
 
 
 @pytest.fixture(scope="function")
 async def multiple_search_resource(
     indexing_utility_registered,
     processor,
     knowledgebox_ingest,
 ):
     """
     Create 100 resources that have every possible bit of information
     """
     n_resources = 100
+    fields_per_resource = 3
     for count in range(1, n_resources + 1):
         message = broker_resource(knowledgebox_ingest)
         await processor.process(message=message, seqid=count)
 
-    await wait_for_shard(knowledgebox_ingest, n_resources)
+    await wait_for_shard(knowledgebox_ingest, n_resources * fields_per_resource)
     return knowledgebox_ingest
 
 
 async def inject_message(
     processor, knowledgebox_ingest, message, count: int = 1
 ) -> str:
     await processor.process(message=message, seqid=count)
     await wait_for_shard(knowledgebox_ingest, count)
     return knowledgebox_ingest
 
 
 async def wait_for_shard(knowledgebox_ingest: str, count: int) -> str:
     # Make sure is indexed
-    driver = await get_driver()
+    driver = get_driver()
     txn = await driver.begin()
-    shard = await Node.get_current_active_shard(txn, knowledgebox_ingest)
+    shard_manager = KBShardManager()
+    shard = await shard_manager.get_current_active_shard(txn, knowledgebox_ingest)
     if shard is None:
         raise Exception("Could not find shard")
     await txn.abort()
 
-    checks: Dict[str, bool] = {}
-    for replica in shard.shard.replicas:
+    checks: dict[str, bool] = {}
+    for replica in shard.replicas:
         if replica.shard.id not in checks:
             checks[replica.shard.id] = False
 
     for i in range(30):
-        for replica in shard.shard.replicas:
-            node_obj = NODES.get(replica.node)
+        for replica in shard.replicas:
+            node_obj = get_index_node(replica.node)
             if node_obj is not None:
                 req = GetShardRequest()
                 req.shard_id.id = replica.shard.id
                 count_shard: Shard = await node_obj.reader.GetShard(req)  # type: ignore
-                if count_shard.resources >= count:
+                if count_shard.fields >= count:
                     checks[replica.shard.id] = True
                 else:
                     checks[replica.shard.id] = False
 
         if all(checks.values()):
             break
         await asyncio.sleep(1)
```

## nucliadb/search/tests/node.py

```diff
@@ -13,83 +13,84 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-
+import dataclasses
 import logging
+import os
 import time
+from typing import Union
 
+import backoff
 import docker  # type: ignore
 import pytest
-from grpc import insecure_channel  # type: ignore
-from grpc_health.v1 import health_pb2_grpc  # type: ignore
-from grpc_health.v1.health_pb2 import HealthCheckRequest  # type: ignore
+from grpc import insecure_channel
+from grpc_health.v1 import health_pb2_grpc
+from grpc_health.v1.health_pb2 import HealthCheckRequest
 from nucliadb_protos.nodewriter_pb2 import EmptyQuery, ShardId
 from nucliadb_protos.nodewriter_pb2_grpc import NodeWriterStub
 from pytest_docker_fixtures import images  # type: ignore
 from pytest_docker_fixtures.containers._base import BaseImage  # type: ignore
+from pytest_lazy_fixtures import lazy_fixture
 
-from nucliadb.ingest.settings import settings
-from nucliadb_utils.tests import free_port
+from nucliadb.common.cluster.settings import settings as cluster_settings
+from nucliadb_utils.tests.conftest import get_testing_storage_backend
 
 logger = logging.getLogger(__name__)
 
 images.settings["nucliadb_node_reader"] = {
-    "image": "eu.gcr.io/stashify-218417/node",
-    "version": "main",
+    "image": "europe-west4-docker.pkg.dev/nuclia-internal/nuclia/node",
+    "version": "latest",
     "env": {
         "HOST_KEY_PATH": "/data/node.key",
-        "VECTORS_DIMENSION": "768",
         "DATA_PATH": "/data",
-        "NODE_TYPE": "Io",
         "READER_LISTEN_ADDRESS": "0.0.0.0:4445",
-        "NUCLIADB_DISABLE_TELEMETRY": "True",
-        "LAZY_LOADING": "true",
+        "NUCLIADB_DISABLE_ANALYTICS": "True",
         "RUST_BACKTRACE": "full",
-        "RUST_LOG": "nucliadb_node=DEBUG,nucliadb_vectors=DEBUG,nucliadb_fields_tantivy=DEBUG,nucliadb_paragraphs_tantivy=DEBUG,nucliadb_cluster=ERROR",  # noqa
+        "RUST_LOG": "nucliadb_*=DEBUG",
     },
     "options": {
         "command": [
             "/usr/local/bin/node_reader",
         ],
-        "ports": {"4445": None},
-        "mem_limit": "2g",  # default is 1g, need to override
+        "ports": {"4445": ("0.0.0.0", 0)},
+        "publish_all_ports": False,
+        "mem_limit": "3g",  # default is 1g, need to override
+        "platform": "linux/amd64",
     },
 }
 
 images.settings["nucliadb_node_writer"] = {
-    "image": "eu.gcr.io/stashify-218417/node",
-    "version": "main",
+    "image": "europe-west4-docker.pkg.dev/nuclia-internal/nuclia/node",
+    "version": "latest",
     "env": {
         "HOST_KEY_PATH": "/data/node.key",
-        "VECTORS_DIMENSION": "768",
         "DATA_PATH": "/data",
-        "NODE_TYPE": "Io",
         "WRITER_LISTEN_ADDRESS": "0.0.0.0:4446",
-        "CHITCHAT_PORT": "4444",
-        "NUCLIADB_DISABLE_TELEMETRY": "True",
-        "SEED_NODES": "",
+        "NUCLIADB_DISABLE_ANALYTICS": "True",
         "RUST_BACKTRACE": "full",
-        "RUST_LOG": "nucliadb_node=DEBUG,nucliadb_vectors=DEBUG,nucliadb_fields_tantivy=DEBUG,nucliadb_paragraphs_tantivy=DEBUG,nucliadb_cluster=ERROR,chitchat=ERROR",  # noqa
+        "RUST_LOG": "nucliadb_*=DEBUG",
     },
     "options": {
         "command": [
             "/usr/local/bin/node_writer",
         ],
-        "ports": {"4446": None},
-        "mem_limit": "2g",  # default is 1g, need to override
+        "ports": {"4446": ("0.0.0.0", 0)},
+        "publish_all_ports": False,
+        "mem_limit": "3g",  # default is 1g, need to override
+        "platform": "linux/amd64",
     },
 }
 
 images.settings["nucliadb_node_sidecar"] = {
-    "image": "eu.gcr.io/stashify-218417/node_sidecar",
-    "version": "main",
+    "image": "europe-west4-docker.pkg.dev/nuclia-internal/nuclia/node_sidecar",
+    "version": "latest",
     "env": {
         "INDEX_JETSTREAM_SERVERS": "[]",
         "CACHE_PUBSUB_NATS_URL": "",
         "HOST_KEY_PATH": "/data/node.key",
         "DATA_PATH": "/data",
         "SIDECAR_LISTEN_ADDRESS": "0.0.0.0:4447",
         "READER_LISTEN_ADDRESS": "0.0.0.0:4445",
@@ -97,49 +98,21 @@
         "PYTHONUNBUFFERED": "1",
         "LOG_LEVEL": "DEBUG",
     },
     "options": {
         "command": [
             "node_sidecar",
         ],
-        "ports": {"4447": None},
-    },
-}
-
-images.settings["nucliadb_cluster_manager"] = {
-    "image": "eu.gcr.io/stashify-218417/cluster_manager",
-    "version": "main",
-    "network": "host",
-    "env": {
-        "LISTEN_PORT": "4444",
-        "NODE_TYPE": "Ingest",
-        "SEEDS": "0.0.0.0:4444",
-        "MONITOR_ADDR": "TO_REPLACE",
-        "RUST_LOG": "debug",
-        "RUST_BACKTRACE": "full",
-    },
-    "options": {
-        "command": [
-            "/nucliadb_cluster/cluster_manager",
-        ],
+        "ports": {"4447": ("0.0.0.0", 0)},
+        "publish_all_ports": False,
+        "platform": "linux/amd64",
     },
 }
 
 
-def get_chitchat_port(container_obj, port):
-    network = container_obj.attrs["NetworkSettings"]
-    service_port = "{0}/udp".format(port)
-    for netport in network["Ports"].keys():
-        if netport == "6543/tcp":
-            continue
-
-        if netport == service_port:
-            return network["Ports"][service_port][0]["HostPort"]
-
-
 def get_container_host(container_obj):
     return container_obj.attrs["NetworkSettings"]["IPAddress"]
 
 
 class nucliadbNodeReader(BaseImage):
     name = "nucliadb_node_reader"
     port = 4445
@@ -186,29 +159,14 @@
         try:
             result = stub.Check(pb)
             return result.status == 1
         except:  # noqa
             return False
 
 
-class nucliadbChitchatNode(BaseImage):
-    name = "nucliadb_cluster_manager"
-    port = 4444
-
-    def run(self):
-        return super(nucliadbChitchatNode, self).run()
-
-    def get_image_options(self):
-        options = super(nucliadbChitchatNode, self).get_image_options()
-        return options
-
-    def check(self):
-        return True
-
-
 class nucliadbNodeSidecar(BaseImage):
     name = "nucliadb_node_sidecar"
     port = 4447
 
     def run(self, volume):
         self._volume = volume
         self._mount = "/data"
@@ -229,94 +187,97 @@
         except:  # noqa
             return False
 
 
 nucliadb_node_1_reader = nucliadbNodeReader()
 nucliadb_node_1_writer = nucliadbNodeWriter()
 nucliadb_node_1_sidecar = nucliadbNodeSidecar()
-nucliadb_cluster_mgr = nucliadbChitchatNode()
 
 nucliadb_node_2_reader = nucliadbNodeReader()
 nucliadb_node_2_writer = nucliadbNodeWriter()
 nucliadb_node_2_sidecar = nucliadbNodeSidecar()
 
 
+@dataclasses.dataclass
+class NodeS3Storage:
+    server: str
+
+    def envs(self):
+        return {
+            "FILE_BACKEND": "s3",
+            "S3_CLIENT_ID": "",
+            "S3_CLIENT_SECRET": "",
+            "S3_BUCKET": "test",
+            "S3_INDEXING_BUCKET": "indexing",
+            "S3_DEADLETTER_BUCKET": "deadletter",
+            "S3_ENDPOINT": self.server,
+        }
+
+
+@dataclasses.dataclass
+class NodeGCSStorage:
+    server: str
+
+    def envs(self):
+        return {
+            "FILE_BACKEND": "gcs",
+            "GCS_BUCKET": "test",
+            "GCS_INDEXING_BUCKET": "indexing",
+            "GCS_DEADLETTER_BUCKET": "deadletter",
+            "GCS_ENDPOINT_URL": self.server,
+        }
+
+
+NodeStorage = Union[NodeGCSStorage, NodeS3Storage]
+
+
 class _NodeRunner:
-    def __init__(self, natsd, gcs):
+    def __init__(self, natsd, storage: NodeStorage):
         self.docker_client = docker.from_env(version=BaseImage.docker_version)
         self.natsd = natsd
-        self.gcs = gcs
-        self.data = {}
+        self.storage = storage
+        self.data = {}  # type: ignore
 
     def start(self):
         docker_platform_name = self.docker_client.api.version()["Platform"][
             "Name"
         ].upper()
-        if (
+        if "GITHUB_ACTION" not in os.environ and (
             "DESKTOP" in docker_platform_name
             # newer versions use community
             or "DOCKER ENGINE - COMMUNITY" == docker_platform_name
         ):
             # Valid when using Docker desktop
             docker_internal_host = "host.docker.internal"
         else:
             # Valid when using github actions
             docker_internal_host = "172.17.0.1"
 
         self.volume_node_1 = self.docker_client.volumes.create(driver="local")
         self.volume_node_2 = self.docker_client.volumes.create(driver="local")
 
-        settings.chitchat_binding_host = "0.0.0.0"
-        settings.chitchat_binding_port = free_port()
-        settings.chitchat_enabled = True
-
-        images.settings["nucliadb_cluster_manager"]["env"][
-            "MONITOR_ADDR"
-        ] = f"{docker_internal_host}:{settings.chitchat_binding_port}"
-        images.settings["nucliadb_cluster_manager"]["env"]["UPDATE_INTERVAL"] = "1s"
-
-        cluster_mgr_host, cluster_mgr_port = nucliadb_cluster_mgr.run()
-
-        cluster_mgr_port = get_chitchat_port(nucliadb_cluster_mgr.container_obj, 4444)
-        cluster_mgr_real_host = get_container_host(nucliadb_cluster_mgr.container_obj)
-
-        images.settings["nucliadb_node_writer"]["env"][
-            "SEED_NODES"
-        ] = f"{cluster_mgr_real_host}:4444"
         writer1_host, writer1_port = nucliadb_node_1_writer.run(self.volume_node_1)
-
         writer2_host, writer2_port = nucliadb_node_2_writer.run(self.volume_node_2)
-        reader1_host, reader1_port = nucliadb_node_1_reader.run(self.volume_node_1)
 
+        reader1_host, reader1_port = nucliadb_node_1_reader.run(self.volume_node_1)
         reader2_host, reader2_port = nucliadb_node_2_reader.run(self.volume_node_2)
 
         natsd_server = self.natsd.replace("localhost", docker_internal_host)
-        images.settings["nucliadb_node_sidecar"]["env"][
-            "INDEX_JETSTREAM_SERVERS"
-        ] = f'["{natsd_server}"]'
-        images.settings["nucliadb_node_sidecar"]["env"][
-            "CACHE_PUBSUB_NATS_URL"
-        ] = f'["{natsd_server}"]'
-        gcs_server = self.gcs.replace("localhost", docker_internal_host)
-        images.settings["nucliadb_node_sidecar"]["env"]["GCS_ENDPOINT_URL"] = gcs_server
-        images.settings["nucliadb_node_sidecar"]["env"]["GCS_BUCKET"] = "test"
-        images.settings["nucliadb_node_sidecar"]["env"]["FILE_BACKEND"] = "gcs"
-        images.settings["nucliadb_node_sidecar"]["env"][
-            "GCS_INDEXING_BUCKET"
-        ] = "indexing"
-        images.settings["nucliadb_node_sidecar"]["env"][
-            "GCS_DEADLETTER_BUCKET"
-        ] = "deadletter"
-
-        images.settings["nucliadb_node_sidecar"]["env"][
-            "READER_LISTEN_ADDRESS"
-        ] = f"{docker_internal_host}:{reader1_port}"
-        images.settings["nucliadb_node_sidecar"]["env"][
-            "WRITER_LISTEN_ADDRESS"
-        ] = f"{docker_internal_host}:{writer1_port}"
+        images.settings["nucliadb_node_sidecar"]["env"].update(
+            {
+                "INDEX_JETSTREAM_SERVERS": f'["{natsd_server}"]',
+                "CACHE_PUBSUB_NATS_URL": f'["{natsd_server}"]',
+                "READER_LISTEN_ADDRESS": f"{docker_internal_host}:{reader1_port}",
+                "WRITER_LISTEN_ADDRESS": f"{docker_internal_host}:{writer1_port}",
+            }
+        )
+        self.storage.server = self.storage.server.replace(
+            "localhost", docker_internal_host
+        )
+        images.settings["nucliadb_node_sidecar"]["env"].update(self.storage.envs())
 
         sidecar1_host, sidecar1_port = nucliadb_node_1_sidecar.run(self.volume_node_1)
 
         images.settings["nucliadb_node_sidecar"]["env"][
             "READER_LISTEN_ADDRESS"
         ] = f"{docker_internal_host}:{reader2_port}"
         images.settings["nucliadb_node_sidecar"]["env"][
@@ -332,18 +293,14 @@
             {
                 "writer1_internal_host": writer1_internal_host,
                 "writer2_internal_host": writer2_internal_host,
                 "writer1": {
                     "host": writer1_host,
                     "port": writer1_port,
                 },
-                "chitchat": {
-                    "host": cluster_mgr_host,
-                    "port": cluster_mgr_port,
-                },
                 "writer2": {
                     "host": writer2_host,
                     "port": writer2_port,
                 },
                 "reader1": {
                     "host": reader1_host,
                     "port": reader1_port,
@@ -361,103 +318,148 @@
                     "port": sidecar2_port,
                 },
             }
         )
         return self.data
 
     def stop(self):
+        container_ids = [
+            nucliadb_node_1_reader.container_obj.id,
+            nucliadb_node_1_writer.container_obj.id,
+            nucliadb_node_1_sidecar.container_obj.id,
+            nucliadb_node_2_writer.container_obj.id,
+            nucliadb_node_2_reader.container_obj.id,
+            nucliadb_node_2_sidecar.container_obj.id,
+        ]
         nucliadb_node_1_reader.stop()
         nucliadb_node_1_writer.stop()
         nucliadb_node_1_sidecar.stop()
         nucliadb_node_2_writer.stop()
         nucliadb_node_2_reader.stop()
         nucliadb_node_2_sidecar.stop()
-        nucliadb_cluster_mgr.stop()
 
-        for container in (
-            nucliadb_node_1_reader,
-            nucliadb_node_1_writer,
-            nucliadb_node_2_reader,
-            nucliadb_node_2_writer,
-            nucliadb_node_2_sidecar,
-            nucliadb_node_2_sidecar,
-            nucliadb_cluster_mgr,
-        ):
-            for i in range(5):
+        for container_id in container_ids:
+            for _ in range(5):
                 try:
-                    self.docker_client.containers.get(container.container_obj.id)  # type: ignore
+                    self.docker_client.containers.get(container_id)  # type: ignore
                 except docker.errors.NotFound:
                     break
                 time.sleep(2)
 
         self.volume_node_1.remove()
         self.volume_node_2.remove()
 
-    def restart(self):
-        self.stop()
-        return self.start()
-
     def setup_env(self):
         # reset on every test run in case something touches it
-        settings.writer_port_map = {
+        cluster_settings.writer_port_map = {
             self.data["writer1_internal_host"]: self.data["writer1"]["port"],
             self.data["writer2_internal_host"]: self.data["writer2"]["port"],
         }
-        settings.reader_port_map = {
+        cluster_settings.reader_port_map = {
             self.data["writer1_internal_host"]: self.data["reader1"]["port"],
             self.data["writer2_internal_host"]: self.data["reader2"]["port"],
         }
-        settings.sidecar_port_map = {
-            self.data["writer1_internal_host"]: self.data["sidecar1"]["port"],
-            self.data["writer2_internal_host"]: self.data["sidecar2"]["port"],
-        }
 
-        settings.node_writer_port = None  # type: ignore
-        settings.node_reader_port = None  # type: ignore
-        settings.node_sidecar_port = None  # type: ignore
+        cluster_settings.node_writer_port = None  # type: ignore
+        cluster_settings.node_reader_port = None  # type: ignore
 
+        cluster_settings.cluster_discovery_mode = "manual"
+        cluster_settings.cluster_discovery_manual_addresses = [
+            self.data["writer1_internal_host"],
+            self.data["writer2_internal_host"],
+        ]
 
-@pytest.fixture(scope="session", autouse=False)
-def _node_runner(natsd: str, gcs: str):
-    yield _NodeRunner(natsd, gcs)
 
+@pytest.fixture(scope="session")
+def gcs_node_storage(gcs):
+    return NodeGCSStorage(server=gcs)
 
-@pytest.fixture(scope="session", autouse=False)
-def _node(natsd: str, gcs: str, _node_runner: _NodeRunner):
-    yield _node_runner.start()
 
-    _node_runner.stop()
+@pytest.fixture(scope="session")
+def s3_node_storage(s3):
+    return NodeS3Storage(server=s3)
+
+
+def lazy_load_storage_backend():
+    backend = get_testing_storage_backend()
+    if backend == "gcs":
+        return [lazy_fixture.lf("gcs_node_storage")]
+    elif backend == "s3":
+        return [lazy_fixture.lf("s3_node_storage")]
+    else:
+        print(f"Unknown storage backend {backend}, using gcs")
+        return [lazy_fixture.lf("gcs_node_storage")]
+
+
+@pytest.fixture(scope="session", params=lazy_load_storage_backend())
+def node_storage(request):
+    return request.param
+
+
+@pytest.fixture(scope="session", autouse=False)
+def _node(natsd: str, node_storage):
+    nr = _NodeRunner(natsd, node_storage)
+    try:
+        cluster_info = nr.start()
+    except Exception:
+        nr.stop()
+        raise
+    nr.setup_env()
+    yield cluster_info
+    nr.stop()
 
 
 @pytest.fixture(scope="function")
-def node(_node, _node_runner: _NodeRunner, request):
+def node(_node, request):
     # clean up all shard data before each test
     channel1 = insecure_channel(
         f"{_node['writer1']['host']}:{_node['writer1']['port']}"
     )
     channel2 = insecure_channel(
         f"{_node['writer2']['host']}:{_node['writer2']['port']}"
     )
     writer1 = NodeWriterStub(channel1)
     writer2 = NodeWriterStub(channel2)
 
     logger.debug("cleaning up shards data")
+    try:
+        cleanup_node(writer1)
+        cleanup_node(writer2)
+    except Exception:
+        logger.error(
+            "Error cleaning up shards data. Maybe the node fixture could not start properly?",
+            exc_info=True,
+        )
 
-    for shard in writer1.ListShards(EmptyQuery()).ids:
-        writer1.DeleteShard(ShardId(id=shard.id))
-
-    for shard in writer2.ListShards(EmptyQuery()).ids:
-        writer2.DeleteShard(ShardId(id=shard.id))
-
-    channel1.close()
-    channel2.close()
-
-    _node_runner.setup_env()
+        client = docker.client.from_env()
+        containers_by_port = {}
+        for container in client.containers.list():
+            name = container.name
+            command = container.attrs["Config"]["Cmd"]
+            ports = container.ports
+            print(f"container {name} executing {command} is using ports: {ports}")
+
+            for internal_port in container.ports:
+                for host in container.ports[internal_port]:
+                    port = host["HostPort"]
+                    port_containers = containers_by_port.setdefault(port, [])
+                    if container not in port_containers:
+                        port_containers.append(container)
+
+        for port, containers in containers_by_port.items():
+            if len(containers) > 1:
+                names = ", ".join([container.name for container in containers])
+                print(f"ATENTION! Containers {names} share port {port}!")
+        raise
+    finally:
+        channel1.close()
+        channel2.close()
 
     yield _node
 
-    _test_failed_statuses = getattr(request.node, "_test_failed_statuses", {})
-    if len(_test_failed_statuses) > 0 and any(_test_failed_statuses.values()):
-        logger.warning(
-            "Test failed with rerun, restarting node in case it's related to a node crash"
-        )
-        _node_runner.restart()
+
+@backoff.on_exception(
+    backoff.expo, Exception, jitter=backoff.random_jitter, max_tries=5
+)
+def cleanup_node(writer: NodeWriterStub):
+    for shard in writer.ListShards(EmptyQuery()).ids:
+        writer.DeleteShard(ShardId(id=shard.id))
```

## nucliadb/search/tests/unit/test_app.py

```diff
@@ -13,38 +13,67 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
+import json
 from unittest.mock import patch
 
 import pytest
 
+from nucliadb.common.cluster.index_node import IndexNode
 from nucliadb.search import app
 
 pytestmark = pytest.mark.asyncio
 
 
 async def test_alive():
-    with patch.object(app, "NODES", {"node1": "node1"}):
+    with patch.object(app.manager, "get_index_nodes", return_value=[{"id": "node1"}]):
         resp = await app.alive(None)
         assert resp.status_code == 200
 
 
 async def test_not_alive():
-    with patch.object(app, "NODES", {}):
+    with patch.object(app.manager, "get_index_nodes", return_value=[]):
         resp = await app.alive(None)
         assert resp.status_code == 503
 
 
 async def test_ready():
-    with patch.object(app, "NODES", {"node1": "node1"}):
+    with patch.object(app.manager, "get_index_nodes", return_value=[{"id": "node1"}]):
         resp = await app.ready(None)
         assert resp.status_code == 200
 
 
 async def test_not_ready():
-    with patch.object(app, "NODES", {}):
+    with patch.object(app.manager, "get_index_nodes", return_value=[]):
         resp = await app.ready(None)
         assert resp.status_code == 503
+
+
+async def test_node_members():
+    nodes = [
+        IndexNode(
+            id="node1", address="node1", shard_count=0, available_disk=100, dummy=True
+        ),
+        IndexNode(
+            id="node2",
+            address="node2",
+            shard_count=0,
+            available_disk=50,
+            dummy=True,
+            primary_id="node1",
+        ),
+    ]
+    with patch.object(app.manager, "get_index_nodes", return_value=nodes):
+        resp = await app.node_members(None)
+        assert resp.status_code == 200
+        members = json.loads(resp.body)
+        sorted(members, key=lambda x: x["id"])
+        assert members[0]["id"] == "node1"
+        assert members[0]["primary_id"] is None
+        assert members[0]["available_disk"] == 100
+        assert members[1]["id"] == "node2"
+        assert members[1]["primary_id"] == "node1"
+        assert members[1]["available_disk"] == 50
```

## nucliadb/search/tests/unit/test_predict.py

```diff
@@ -13,78 +13,72 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import asyncio
 from unittest.mock import AsyncMock, MagicMock, Mock
 
+import aiohttp
 import pytest
+from yarl import URL
 
 from nucliadb.search.predict import (
     DummyPredictEngine,
     PredictEngine,
     PredictVectorMissing,
+    ProxiedPredictAPIError,
+    RephraseError,
+    RephraseMissingContextError,
     SendToPredictError,
+    _parse_rephrase_response,
+    get_answer_generator,
 )
+from nucliadb.tests.utils.aiohttp_session import get_mocked_session
 from nucliadb_models.search import (
+    AskDocumentModel,
     ChatModel,
     FeedbackRequest,
     FeedbackTasks,
     RephraseModel,
+    SummarizedResource,
+    SummarizedResponse,
+    SummarizeModel,
+    SummarizeResourceModel,
 )
 from nucliadb_utils.exceptions import LimitsExceededError
 
 
-def get_mocked_session(
-    http_method: str, status: int, text=None, json=None, read=None, context_manager=True
-):
-    response = Mock(status=status)
-    if text is not None:
-        response.text = AsyncMock(return_value=text)
-    if json is not None:
-        response.json = AsyncMock(return_value=json)
-    if read is not None:
-        response.read = AsyncMock(return_value=read)
-    if context_manager:
-        # For when async with self.session.post() as response: is called
-        session = Mock()
-        http_method_mock = AsyncMock(__aenter__=AsyncMock(return_value=response))
-        getattr(session, http_method.lower()).return_value = http_method_mock
-    else:
-        # For when await self.session.post() is called
-        session = AsyncMock()
-        getattr(session, http_method.lower()).return_value = response
-    return session
-
-
 @pytest.mark.asyncio
 async def test_dummy_predict_engine():
     pe = DummyPredictEngine()
     await pe.initialize()
     await pe.finalize()
     await pe.send_feedback("kbid", Mock(), "", "", "")
     assert await pe.rephrase_query("kbid", Mock())
     assert await pe.chat_query("kbid", Mock())
     assert await pe.convert_sentence_to_vector("kbid", "some sentence")
     assert await pe.detect_entities("kbid", "some sentence")
+    assert await pe.ask_document("kbid", "query", [["footext"]], "userid")
+    assert await pe.summarize("kbid", Mock(resources={}))
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize(
     "onprem,expected_url,expected_header,expected_header_value",
     [
         (
             True,
-            "{public_url}/api/v1/predict/sentence",
+            "{public_url}/api/v1/predict/sentence/kbid",
             "X-STF-NUAKEY",
             "Bearer {service_account}",
         ),
-        (False, "{cluster}/api/internal/predict/sentence", "X-STF-KBID", "{kbid}"),
+        (False, "{cluster}/api/v1/internal/predict/sentence", "X-STF-KBID", "{kbid}"),
     ],
 )
 async def test_convert_sentence_ok(
     onprem, expected_url, expected_header, expected_header_value
 ):
     service_account = "service-account"
 
@@ -102,52 +96,52 @@
 
     kbid = "kbid"
     sentence = "some sentence"
 
     assert await pe.convert_sentence_to_vector(kbid, sentence) == [0.0, 0.1]
 
     path = expected_url.format(public_url=pe.public_url, cluster=pe.cluster_url)
-    url = f"{path}?text={sentence}"
 
     headers = {
         expected_header: expected_header_value.format(
             kbid=kbid, service_account=service_account
         )
     }
     pe.session.get.assert_awaited_once_with(
-        url=url,
+        url=path,
+        params={"text": sentence},
         headers=headers,
     )
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize("onprem", [True, False])
 async def test_convert_sentence_error(onprem):
     pe = PredictEngine(
         "cluster",
         "public-{zone}",
         "service-account",
         onprem=onprem,
     )
-    pe.session = get_mocked_session("GET", 400, read="uops!", context_manager=False)
-    with pytest.raises(SendToPredictError):
+    pe.session = get_mocked_session("GET", 400, json="uops!", context_manager=False)
+    with pytest.raises(ProxiedPredictAPIError):
         await pe.convert_sentence_to_vector("kbid", "some sentence")
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize(
     "onprem,expected_url,expected_header,expected_header_value",
     [
         (
             True,
-            "{public_url}/api/v1/predict/tokens",
+            "{public_url}/api/v1/predict/tokens/kbid",
             "X-STF-NUAKEY",
             "Bearer {service_account}",
         ),
-        (False, "{cluster}/api/internal/predict/tokens", "X-STF-KBID", "{kbid}"),
+        (False, "{cluster}/api/v1/internal/predict/tokens", "X-STF-KBID", "{kbid}"),
     ],
 )
 async def test_detect_entities_ok(
     onprem, expected_url, expected_header, expected_header_value
 ):
     cluster_url = "cluster"
     public_url = "public-{zone}"
@@ -166,42 +160,41 @@
         200,
         json={"tokens": [{"text": "foo", "ner": "bar"}]},
         context_manager=False,
     )
 
     kbid = "kbid"
     sentence = "some sentence"
-
     assert len(await pe.detect_entities(kbid, sentence)) > 0
 
     path = expected_url.format(public_url=pe.public_url, cluster=pe.cluster_url)
-    url = f"{path}?text={sentence}"
 
     headers = {
         expected_header: expected_header_value.format(
             kbid=kbid, service_account=service_account
         )
     }
     pe.session.get.assert_awaited_once_with(
-        url=url,
+        url=path,
+        params={"text": sentence},
         headers=headers,
     )
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize("onprem", [True, False])
 async def test_detect_entities_error(onprem):
     pe = PredictEngine(
         "cluster",
         "public-{zone}",
         "service-account",
         onprem=onprem,
     )
-    pe.session = get_mocked_session("GET", 500, read="error", context_manager=False)
-    with pytest.raises(SendToPredictError):
+    pe.session = get_mocked_session("GET", 500, json="error", context_manager=False)
+    with pytest.raises(ProxiedPredictAPIError):
         await pe.detect_entities("kbid", "some sentence")
 
 
 @pytest.fixture(scope="function")
 def session_limits_exceeded():
     session = AsyncMock()
     resp = Mock(status=402)
@@ -226,14 +219,15 @@
                 FeedbackRequest(ident="foo", good=True, task=FeedbackTasks.CHAT),
                 "",
                 "",
                 "",
             ],
         ),
         ("rephrase_query", ["kbid", RephraseModel(question="foo", user_id="bar")]),
+        ("ask_document", ["kbid", "query", [["footext"]], "userid"]),
     ],
 )
 async def test_predict_engine_handles_limits_exceeded_error(
     session_limits_exceeded, method, args
 ):
     pe = PredictEngine(
         "cluster",
@@ -250,14 +244,16 @@
     "method,args,exception,output",
     [
         ("chat_query", ["kbid", Mock()], True, None),
         ("rephrase_query", ["kbid", Mock()], True, None),
         ("send_feedback", ["kbid", MagicMock(), "", "", ""], False, None),
         ("convert_sentence_to_vector", ["kbid", "sentence"], False, []),
         ("detect_entities", ["kbid", "sentence"], False, []),
+        ("ask_document", ["kbid", "query", [["footext"]], "userid"], True, None),
+        ("summarize", ["kbid", Mock(resources={})], True, None),
     ],
 )
 async def test_onprem_nuclia_service_account_not_configured(
     method, args, exception, output
 ):
     pe = PredictEngine(
         "cluster",
@@ -280,7 +276,309 @@
         onprem=True,
     )
     pe.session = get_mocked_session(
         "GET", 200, json={"data": []}, context_manager=False
     )
     with pytest.raises(PredictVectorMissing):
         await pe.convert_sentence_to_vector("kbid", "sentence")
+
+
+async def test_ask_document_onprem():
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        nuclia_service_account="foo",
+        zone="europe1",
+        onprem=True,
+    )
+    pe.session = get_mocked_session(
+        "POST", 200, text="The answer", context_manager=False
+    )
+
+    assert (
+        await pe.ask_document("kbid", "query", [["footext"]], "userid") == "The answer"
+    )
+
+    pe.session.post.assert_awaited_once_with(
+        url="public-europe1/api/v1/predict/ask_document/kbid",
+        json=AskDocumentModel(
+            question="query", blocks=[["footext"]], user_id="userid"
+        ).dict(),
+        headers={"X-STF-NUAKEY": "Bearer foo"},
+        timeout=None,
+    )
+
+
+async def test_ask_document_cloud():
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        zone="europe1",
+        onprem=False,
+    )
+    pe.session = get_mocked_session(
+        "POST", 200, text="The answer", context_manager=False
+    )
+
+    assert (
+        await pe.ask_document("kbid", "query", [["footext"]], "userid") == "The answer"
+    )
+
+    pe.session.post.assert_awaited_once_with(
+        url="cluster/api/v1/internal/predict/ask_document",
+        json=AskDocumentModel(
+            question="query", blocks=[["footext"]], user_id="userid"
+        ).dict(),
+        headers={"X-STF-KBID": "kbid"},
+        timeout=None,
+    )
+
+
+async def test_rephrase():
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        zone="europe1",
+        onprem=False,
+    )
+    pe.session = get_mocked_session(
+        "POST", 200, json="rephrased", context_manager=False
+    )
+
+    item = RephraseModel(
+        question="question", chat_history=[], user_id="foo", user_context=["foo"]
+    )
+    rephrased_query = await pe.rephrase_query("kbid", item)
+    # The rephrase query should not be wrapped in quotes, otherwise it will trigger an exact match query to the index
+    assert rephrased_query.strip('"') == rephrased_query
+    assert rephrased_query == "rephrased"
+
+    pe.session.post.assert_awaited_once_with(
+        url="cluster/api/v1/internal/predict/rephrase",
+        json=item.dict(),
+        headers={"X-STF-KBID": "kbid"},
+    )
+
+
+async def test_rephrase_onprem():
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        zone="europe1",
+        onprem=True,
+        nuclia_service_account="nuakey",
+    )
+    pe.session = get_mocked_session(
+        "POST", 200, json="rephrased", context_manager=False
+    )
+
+    item = RephraseModel(
+        question="question", chat_history=[], user_id="foo", user_context=["foo"]
+    )
+    rephrased_query = await pe.rephrase_query("kbid", item)
+    # The rephrase query should not be wrapped in quotes, otherwise it will trigger an exact match query to the index
+    assert rephrased_query.strip('"') == rephrased_query
+    assert rephrased_query == "rephrased"
+
+    pe.session.post.assert_awaited_once_with(
+        url="public-europe1/api/v1/predict/rephrase/kbid",
+        json=item.dict(),
+        headers={"X-STF-NUAKEY": "Bearer nuakey"},
+    )
+
+
+async def test_feedback():
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        zone="europe1",
+        onprem=False,
+    )
+    pe.session = get_mocked_session("POST", 204, json="", context_manager=False)
+
+    x_nucliadb_user = "user"
+    x_ndb_client = "client"
+    x_forwarded_for = "fwfor"
+    item = FeedbackRequest(ident="foo", good=True, task=FeedbackTasks.CHAT)
+    await pe.send_feedback("kbid", item, x_nucliadb_user, x_ndb_client, x_forwarded_for)
+
+    json_data = item.dict()
+    json_data["user_id"] = x_nucliadb_user
+    json_data["client"] = x_ndb_client
+    json_data["forwarded"] = x_forwarded_for
+
+    pe.session.post.assert_awaited_once_with(
+        url="cluster/api/v1/internal/predict/feedback",
+        json=json_data,
+        headers={"X-STF-KBID": "kbid"},
+    )
+
+
+async def test_feedback_onprem():
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        zone="europe1",
+        onprem=True,
+        nuclia_service_account="nuakey",
+    )
+
+    pe.session = get_mocked_session("POST", 204, json="", context_manager=False)
+
+    x_nucliadb_user = "user"
+    x_ndb_client = "client"
+    x_forwarded_for = "fwfor"
+    item = FeedbackRequest(ident="foo", good=True, task=FeedbackTasks.CHAT)
+    await pe.send_feedback("kbid", item, x_nucliadb_user, x_ndb_client, x_forwarded_for)
+
+    json_data = item.dict()
+    json_data["user_id"] = x_nucliadb_user
+    json_data["client"] = x_ndb_client
+    json_data["forwarded"] = x_forwarded_for
+
+    pe.session.post.assert_awaited_once_with(
+        url="public-europe1/api/v1/predict/feedback/kbid",
+        json=json_data,
+        headers={"X-STF-NUAKEY": "Bearer nuakey"},
+    )
+
+
+@pytest.mark.parametrize(
+    "content,exception",
+    [
+        ("foobar", None),
+        ("foobar0", None),
+        ("foobar-1", RephraseError),
+        ("foobar-2", RephraseMissingContextError),
+    ],
+)
+async def test_parse_rephrase_response(content, exception):
+    resp = Mock()
+    resp.json = AsyncMock(return_value=content)
+    if exception:
+        with pytest.raises(exception):
+            await _parse_rephrase_response(resp)
+    else:
+        assert await _parse_rephrase_response(resp) == content.rstrip("0")
+
+
+async def test_check_response_error():
+    response = aiohttp.ClientResponse(
+        "GET",
+        URL("http://predict:8080/api/v1/chat"),
+        writer=None,
+        continue100=Mock(),
+        timer=Mock(),
+        request_info=Mock(),
+        traces=[],
+        loop=Mock(),
+        session=Mock(),
+    )
+    response.status = 503
+    response._body = b"some error"
+    response._headers = {"Content-Type": "text/plain; charset=utf-8"}
+
+    with pytest.raises(ProxiedPredictAPIError) as ex:
+        await PredictEngine().check_response(response, expected_status=200)
+    assert ex.value.status == 503
+    assert ex.value.detail == "some error"
+
+
+async def test_summarize():
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        zone="europe1",
+        onprem=False,
+    )
+
+    summarized = SummarizedResponse(
+        resources={"r1": SummarizedResource(summary="resource summary", tokens=10)}
+    )
+    pe.session = get_mocked_session(
+        "POST", 200, json=summarized.dict(), context_manager=False
+    )
+
+    item = SummarizeModel(
+        resources={"r1": SummarizeResourceModel(fields={"f1": "field extracted text"})}
+    )
+    summarize_response = await pe.summarize("kbid", item)
+
+    assert summarize_response == summarized
+
+    pe.session.post.assert_awaited_once_with(
+        url="cluster/api/v1/internal/predict/summarize",
+        json=item.dict(),
+        headers={"X-STF-KBID": "kbid"},
+        timeout=None,
+    )
+
+
+async def test_summarize_onprem():
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        zone="europe1",
+        onprem=True,
+        nuclia_service_account="nuakey",
+    )
+
+    summarized = SummarizedResponse(
+        resources={"r1": SummarizedResource(summary="resource summary", tokens=10)}
+    )
+    pe.session = get_mocked_session(
+        "POST", 200, json=summarized.dict(), context_manager=False
+    )
+
+    item = SummarizeModel(
+        resources={"r1": SummarizeResourceModel(fields={"f1": "field extracted text"})}
+    )
+    summarize_response = await pe.summarize("kbid", item)
+
+    assert summarize_response == summarized
+
+    pe.session.post.assert_awaited_once_with(
+        url="public-europe1/api/v1/predict/summarize/kbid",
+        json=item.dict(),
+        headers={"X-STF-NUAKEY": "Bearer nuakey"},
+        timeout=None,
+    )
+
+
+async def test_get_predict_headers_onprem():
+    nua_service_account = "nua-service-account"
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        zone="europe1",
+        onprem=True,
+        nuclia_service_account=nua_service_account,
+    )
+    assert pe.get_predict_headers("kbid") == {
+        "X-STF-NUAKEY": f"Bearer {nua_service_account}"
+    }
+
+
+async def test_get_predict_headers_hosterd():
+    pe = PredictEngine(
+        "cluster",
+        "public-{zone}",
+        zone="europe1",
+        onprem=False,
+    )
+    assert pe.get_predict_headers("kbid") == {"X-STF-KBID": "kbid"}
+
+
+async def test_get_answer_generator():
+    async def _iter_chunks():
+        await asyncio.sleep(0.1)
+        # Chunk, end_of_chunk
+        yield b"foo", False
+        yield b"bar", True
+        yield b"baz", True
+
+    resp = Mock()
+    resp.content.iter_chunks = Mock(return_value=_iter_chunks())
+    get_answer_generator(resp)
+
+    answer_chunks = [chunk async for chunk in get_answer_generator(resp)]
+    assert answer_chunks == [b"foobar", b"baz"]
```

## nucliadb/search/tests/unit/test_run.py

```diff
@@ -23,15 +23,17 @@
 
 from nucliadb.search.app import application
 from nucliadb.search.run import run
 
 
 @pytest.fixture(scope="function")
 def run_fastapi_with_metrics():
-    with patch("nucliadb.search.run.run_fastapi_with_metrics") as mocked:
+    with patch("nucliadb.search.run.run_fastapi_with_metrics") as mocked, patch(
+        "nucliadb.search.run.instrument_app"
+    ):
         yield mocked
 
 
 def test_run_with_metrics(run_fastapi_with_metrics):
     run()
 
     run_fastapi_with_metrics.assert_called_once_with(application)
```

## nucliadb/search/tests/unit/search/test_paragraphs.py

```diff
@@ -13,14 +13,16 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
+import asyncio
+import random
 from unittest.mock import AsyncMock, MagicMock, patch
 
 import pytest
 from nucliadb_protos.utils_pb2 import ExtractedText
 
 from nucliadb.search.search import paragraphs
 
@@ -81,22 +83,15 @@
         mock.get_field.return_value = field
         with patch(
             "nucliadb.search.search.paragraphs.get_resource_from_cache",
             return_value=mock,
         ):
             yield mock
 
-    @pytest.fixture()
-    def pcache(self):
-        mock = AsyncMock()
-        mock.get.return_value = None
-        with patch("nucliadb.search.search.paragraphs.get_utility", return_value=mock):
-            yield mock
-
-    async def test_get_paragraph_text(self, orm_resource, pcache):
+    async def test_get_paragraph_text(self, orm_resource):
         assert (
             await paragraphs.get_paragraph_text(
                 kbid="kbid",
                 rid="rid",
                 field="/t/text",
                 start=0,
                 end=12,
@@ -105,25 +100,58 @@
                 ematches=None,
                 matches=None,
             )
             == "Hello World!"
         )
 
         orm_resource.get_field.assert_called_once_with("text", 4, load=False)
-        pcache.get.assert_called_once()
 
-    async def test_get_paragraph_text_with_pcache(self, orm_resource, pcache):
-        pcache.get.return_value = "Cached Value!"
-        assert (
-            await paragraphs.get_paragraph_text(
-                kbid="kbid",
-                rid="rid",
-                field="/t/text",
-                start=0,
-                end=12,
-                split=None,
-                highlight=True,
-                ematches=None,
-                matches=None,
-            )
-            == "Cached Value!"
-        )
+
+async def fake_get_extracted_text_from_gcloud(*args, **kwargs):
+    await asyncio.sleep(random.uniform(0, 1))
+    return ExtractedText(text=b"Hello World!")
+
+
+async def test_get_field_extracted_text_is_cached(field):
+    field.kbid = "kbid"
+    field.uuid = "rid"
+    field.id = "fid"
+    # Simulate a slow response from GCloud
+    field.get_extracted_text = AsyncMock(
+        side_effect=fake_get_extracted_text_from_gcloud
+    )
+
+    # Run 10 times in parallel to check that the cache is working
+    etcache = paragraphs.ExtractedTextCache()
+    futures = [
+        paragraphs.get_field_extracted_text(field, cache=etcache) for _ in range(10)
+    ]
+    await asyncio.gather(*futures)
+
+    field.get_extracted_text.assert_awaited_once()
+
+
+async def test_get_field_extracted_text_is_not_cached_when_none(field):
+    field.get_extracted_text = AsyncMock(return_value=None)
+
+    await paragraphs.get_field_extracted_text(field)
+    await paragraphs.get_field_extracted_text(field)
+
+    assert field.get_extracted_text.await_count == 2
+
+
+def test_extracted_text_cache():
+    etcache = paragraphs.ExtractedTextCache()
+    assert etcache.get_value("foo") is None
+
+    assert isinstance(etcache.get_lock("foo"), asyncio.Lock)
+    assert len(etcache.locks) == 1
+
+    etcache.set_value("foo", "bar")
+    assert len(etcache.values) == 1
+
+    assert etcache.get_value("foo") == "bar"
+
+    etcache.clear()
+
+    assert len(etcache.values) == 0
+    assert len(etcache.locks) == 0
```

## nucliadb/search/tests/unit/search/requesters/test_utils.py

```diff
@@ -13,90 +13,199 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
-from unittest.mock import MagicMock, Mock
+from unittest.mock import AsyncMock, Mock
 
 import pytest
 from fastapi import HTTPException
 from grpc import StatusCode
-from grpc.aio import AioRpcError  # type: ignore
+from grpc.aio import AioRpcError
 
+from nucliadb.common.cluster.base import AbstractIndexNode
 from nucliadb.search.requesters import utils
+from nucliadb_protos import nodereader_pb2, writer_pb2
+from nucliadb_utils.utilities import Utility, clean_utility, get_utility, set_utility
+
+
+@pytest.fixture
+def fake_nodes():
+    from nucliadb.common.cluster import manager
+
+    original = manager.INDEX_NODES
+    manager.INDEX_NODES.clear()
+
+    manager.add_index_node(
+        id="node-0",
+        address="nohost",
+        shard_count=0,
+        available_disk=100,
+        dummy=True,
+    )
+    manager.add_index_node(
+        id="node-replica-0",
+        address="nohost",
+        shard_count=0,
+        available_disk=100,
+        dummy=True,
+        primary_id="node-0",
+    )
+
+    yield (["node-0"], ["node-replica-0"])
+
+    manager.INDEX_NODES = original
+
+
+@pytest.fixture
+def shard_manager():
+    original = get_utility(Utility.SHARD_MANAGER)
+
+    manager = AsyncMock()
+    manager.get_shards_by_kbid = AsyncMock(
+        return_value=[
+            writer_pb2.ShardObject(
+                shard="shard-id",
+                replicas=[
+                    writer_pb2.ShardReplica(
+                        shard=writer_pb2.ShardCreated(id="shard-id"), node="node-0"
+                    )
+                ],
+            )
+        ]
+    )
+
+    set_utility(Utility.SHARD_MANAGER, manager)
+
+    yield manager
+
+    if original is None:
+        clean_utility(Utility.SHARD_MANAGER)
+    else:
+        set_utility(Utility.SHARD_MANAGER, original)
+
+
+@pytest.fixture()
+def search_methods():
+    def fake_search(
+        node: AbstractIndexNode, shard: str, query: nodereader_pb2.SearchRequest
+    ):
+        if node.is_read_replica():
+            raise Exception()
+        return nodereader_pb2.SearchResponse()
+
+    original = utils.METHODS
+    utils.METHODS = {
+        utils.Method.SEARCH: AsyncMock(side_effect=fake_search),
+        utils.Method.PARAGRAPH: AsyncMock(),
+    }
+
+    yield utils.METHODS
+
+    utils.METHODS = original
+
+
+@pytest.mark.asyncio
+async def test_node_query_retries_primary_if_secondary_fails(
+    fake_nodes,
+    shard_manager,
+    search_methods,
+):
+    """Setting up a node and a faulty replica, validate primary is queried if
+    secondary fails.
+
+    """
+    pb_query = nodereader_pb2.SearchRequest(shard="shard-id", body="question")
+    results, incomplete_results, queried_nodes = await utils.node_query(
+        kbid="my-kbid",
+        method=utils.Method.SEARCH,
+        pb_query=pb_query,
+        use_read_replica_nodes=True,
+    )
+    # secondary fails, primary is called
+    assert search_methods[utils.Method.SEARCH].await_count == 2
+    assert len(queried_nodes) == 2
+    assert queried_nodes[0][0].is_read_replica()
+    assert not queried_nodes[1][0].is_read_replica()
+
+    results, incomplete_results, queried_nodes = await utils.node_query(
+        kbid="my-kbid",
+        method=utils.Method.PARAGRAPH,
+        pb_query=Mock(),
+        use_read_replica_nodes=True,
+    )
+    # secondary succeeds, no fallback call to primary
+    assert search_methods[utils.Method.PARAGRAPH].await_count == 1
+    assert len(queried_nodes) == 1
+    assert queried_nodes[0][0].is_read_replica()
+
+
+def test_debug_nodes_info(fake_nodes: tuple[list[str], list[str]]):
+    from nucliadb.common.cluster import manager
+
+    primary = manager.get_index_node(fake_nodes[0][0])
+    assert primary is not None
+    secondary = manager.get_index_node(fake_nodes[1][0])
+    assert secondary is not None
+
+    info = utils.debug_nodes_info([(primary, "shard-a"), (secondary, "shard-b")])
+    assert len(info) == 2
+
+    primary_keys = ["id", "shard_id", "address"]
+    secondary_keys = primary_keys + ["primary_id"]
+
+    for key in primary_keys:
+        assert key in info[0]
+
+    for key in secondary_keys:
+        assert key in info[1]
 
 
 def test_validate_node_query_results():
-    assert utils.validate_node_query_results([Mock()], []) is None
+    assert utils.validate_node_query_results([Mock()]) is None
 
 
 def test_validate_node_query_results_no_results():
-    assert isinstance(utils.validate_node_query_results([], []), HTTPException)
-    assert isinstance(utils.validate_node_query_results(None, []), HTTPException)
+    assert isinstance(utils.validate_node_query_results([]), HTTPException)
+    assert isinstance(utils.validate_node_query_results(None), HTTPException)
 
 
 def test_validate_node_query_results_unhandled_error():
-    error = utils.validate_node_query_results([Exception()], [])
+    error = utils.validate_node_query_results([Exception()])
     assert isinstance(error, HTTPException)
 
 
-def test_validate_node_query_results_unavailable_reset_conns():
-    # if result len match used nodes len, just reset the connection
-    # from the used node
-    node = MagicMock()
-    with pytest.raises(utils.RetriableNodeQueryException):
-        utils.validate_node_query_results(
-            [
-                AioRpcError(
-                    code=StatusCode.UNAVAILABLE,
-                    initial_metadata=Mock(),
-                    trailing_metadata=Mock(),
-                    details="",
-                    debug_error_string="",
-                )
-            ],
-            [node],
-        )
-
-    node.reset_connections.assert_called_once()
-
-
-def test_validate_node_query_results_unavailable_reset_all_node_conns():
-    node1 = MagicMock()
-    node2 = MagicMock()
-    with pytest.raises(utils.RetriableNodeQueryException):
-        utils.validate_node_query_results(
-            [
-                AioRpcError(
-                    code=StatusCode.UNAVAILABLE,
-                    initial_metadata=Mock(),
-                    trailing_metadata=Mock(),
-                    details="",
-                    debug_error_string="",
-                )
-            ],
-            [node1, node2],
-        )
-
-    node1.reset_connections.assert_called_once()
-    node2.reset_connections.assert_called_once()
-
-
 def test_validate_node_query_results_invalid_query():
     result = utils.validate_node_query_results(
         [
             AioRpcError(
                 code=StatusCode.INTERNAL,
                 initial_metadata=Mock(),
                 trailing_metadata=Mock(),
                 details="An invalid argument was passed: 'Query is invalid. AllButQueryForbidden'",
                 debug_error_string="",
             )
-        ],
-        [],
+        ]
     )
 
     assert isinstance(result, HTTPException)
     assert result.status_code == 412
     assert result.detail == "Query is invalid. AllButQueryForbidden"
+
+
+def test_validate_node_query_results_internal_unhandled():
+    result = utils.validate_node_query_results(
+        [
+            AioRpcError(
+                code=StatusCode.INTERNAL,
+                initial_metadata=Mock(),
+                trailing_metadata=Mock(),
+                details="There is something wrong with your query, my friend!",
+                debug_error_string="This query is simply wrong",
+            )
+        ]
+    )
+    assert isinstance(result, HTTPException)
+    assert result.status_code == 500
+    assert result.detail == "There is something wrong with your query, my friend!"
```

## nucliadb/tests/conftest.py

```diff
@@ -14,12 +14,19 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 pytest_plugins = [
+    "pytest_docker_fixtures",
     "nucliadb.tests.fixtures",
     "nucliadb.tests.knowledgeboxes",
     "nucliadb_utils.tests.asyncbenchmark",
     "nucliadb_utils.tests.nats",
+    "nucliadb.tests.fixtures",
+    "nucliadb.tests.tikv",
+    "nucliadb.search.tests.node",
+    "nucliadb.ingest.tests.fixtures",
+    "nucliadb_utils.tests.conftest",
+    "nucliadb_utils.tests.gcs",
 ]
```

## nucliadb/tests/fixtures.py

```diff
@@ -13,99 +13,177 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import logging
 import os
 import tempfile
+from os.path import dirname
+from typing import AsyncIterator
 from unittest.mock import Mock
 
+import asyncpg
 import pytest
+import tikv_client  # type: ignore
 from grpc import aio
 from httpx import AsyncClient
 from nucliadb_protos.train_pb2_grpc import TrainStub
 from nucliadb_protos.utils_pb2 import Relation, RelationNode
 from nucliadb_protos.writer_pb2 import BrokerMessage
 from nucliadb_protos.writer_pb2_grpc import WriterStub
+from pytest_lazy_fixtures import lazy_fixture
+from redis import asyncio as aioredis
 
-from nucliadb.config import config_nucliadb
-from nucliadb.run import run_async_nucliadb
-from nucliadb.settings import Settings
+from nucliadb.common.cluster import manager as cluster_manager
+from nucliadb.common.maindb.driver import Driver
+from nucliadb.common.maindb.exceptions import UnsetUtility
+from nucliadb.common.maindb.local import LocalDriver
+from nucliadb.common.maindb.pg import PGDriver
+from nucliadb.common.maindb.redis import RedisDriver
+from nucliadb.common.maindb.tikv import TiKVDriver
+from nucliadb.common.maindb.utils import get_driver
+from nucliadb.ingest.settings import DriverConfig, DriverSettings
+from nucliadb.ingest.settings import settings as ingest_settings
+from nucliadb.standalone.config import config_nucliadb
+from nucliadb.standalone.run import run_async_nucliadb
+from nucliadb.standalone.settings import Settings
 from nucliadb.tests.utils import inject_message
 from nucliadb.writer import API_PREFIX
+from nucliadb_telemetry.logs import setup_logging
+from nucliadb_telemetry.settings import (
+    LogFormatType,
+    LogLevel,
+    LogOutputType,
+    LogSettings,
+)
+from nucliadb_utils.storages.settings import settings as storage_settings
 from nucliadb_utils.tests import free_port
 from nucliadb_utils.utilities import (
     Utility,
     clean_utility,
     clear_global_cache,
     get_utility,
     set_utility,
 )
 
+logger = logging.getLogger(__name__)
+
 
 @pytest.fixture(scope="function")
 async def dummy_processing():
     from nucliadb_utils.settings import nuclia_settings
 
     nuclia_settings.dummy_processing = True
 
 
-@pytest.fixture(scope="function")
-def telemetry_disabled():
-    os.environ["NUCLIADB_DISABLE_TELEMETRY"] = "True"
+@pytest.fixture(scope="function", autouse=True)
+def analytics_disabled():
+    os.environ["NUCLIADB_DISABLE_ANALYTICS"] = "True"
     yield
-    os.environ.pop("NUCLIADB_DISABLE_TELEMETRY")
+    os.environ.pop("NUCLIADB_DISABLE_ANALYTICS")
 
 
 def reset_config():
+    from nucliadb.common.cluster import settings as cluster_settings
     from nucliadb.ingest import settings as ingest_settings
-    from nucliadb.ingest.orm import NODE_CLUSTER
     from nucliadb.train import settings as train_settings
     from nucliadb.writer import settings as writer_settings
     from nucliadb_utils import settings as utils_settings
     from nucliadb_utils.cache import settings as cache_settings
 
-    ingest_settings.settings.parse_obj(ingest_settings.Settings())
-    train_settings.settings.parse_obj(train_settings.Settings())
-    writer_settings.settings.parse_obj(writer_settings.Settings())
-    cache_settings.settings.parse_obj(cache_settings.Settings())
-
-    utils_settings.audit_settings.parse_obj(utils_settings.AuditSettings())
-    utils_settings.indexing_settings.parse_obj(utils_settings.IndexingSettings())
-    utils_settings.transaction_settings.parse_obj(utils_settings.TransactionSettings())
-    utils_settings.nucliadb_settings.parse_obj(utils_settings.NucliaDBSettings())
-    utils_settings.nuclia_settings.parse_obj(utils_settings.NucliaSettings())
-    utils_settings.storage_settings.parse_obj(utils_settings.StorageSettings())
-
-    NODE_CLUSTER.local_node = None
+    all_settings = [
+        cluster_settings.settings,
+        ingest_settings.settings,
+        train_settings.settings,
+        writer_settings.settings,
+        cache_settings.settings,
+        utils_settings.audit_settings,
+        utils_settings.http_settings,
+        utils_settings.indexing_settings,
+        utils_settings.nuclia_settings,
+        utils_settings.nucliadb_settings,
+        utils_settings.storage_settings,
+        utils_settings.transaction_settings,
+    ]
+    for settings in all_settings:
+        defaults = type(settings)()
+        for attr, _value in settings:
+            default_value = getattr(defaults, attr)
+            setattr(settings, attr, default_value)
+
+    from nucliadb.common.cluster import manager
+
+    manager.INDEX_NODES.clear()
 
 
 @pytest.fixture(scope="function")
-async def nucliadb(dummy_processing, telemetry_disabled):
-    with tempfile.TemporaryDirectory() as tmpdir:
-        settings = Settings(
-            driver="local",
-            file_backend="local",
-            local_files=f"{tmpdir}/blob",
-            driver_local_url=f"{tmpdir}/main",
-            data_path=f"{tmpdir}/node",
-            http_port=free_port(),
-            ingest_grpc_port=free_port(),
-            train_grpc_port=free_port(),
+def tmpdir():
+    try:
+        with tempfile.TemporaryDirectory() as tmpdir:
+            yield tmpdir
+    except OSError:
+        # Python error on tempfile when tearing down the fixture.
+        # Solved in version 3.11
+        pass
+
+
+@pytest.fixture(scope="function")
+async def nucliadb(
+    dummy_processing, analytics_disabled, driver_settings, tmpdir, learning_config
+):
+    from nucliadb.common.cluster import manager
+
+    manager.INDEX_NODES.clear()
+
+    # we need to force DATA_PATH updates to run every test on the proper
+    # temporary directory
+    data_path = f"{tmpdir}/node"
+    local_files = f"{tmpdir}/blob"
+    os.environ["DATA_PATH"] = data_path
+
+    settings = Settings(
+        file_backend="local",
+        local_files=local_files,
+        data_path=data_path,
+        http_port=free_port(),
+        ingest_grpc_port=free_port(),
+        train_grpc_port=free_port(),
+        standalone_node_port=free_port(),
+        log_format_type=LogFormatType.PLAIN,
+        log_output_type=LogOutputType.FILE,
+        **driver_settings.dict(),
+    )
+
+    config_nucliadb(settings)
+
+    # Make sure tests don't write logs outside of the tmpdir
+    os.environ["ERROR_LOG"] = f"{tmpdir}/logs/error.log"
+    os.environ["ACCESS_LOG"] = f"{tmpdir}/logs/access.log"
+    os.environ["INFO_LOG"] = f"{tmpdir}/logs/info.log"
+
+    setup_logging(
+        settings=LogSettings(
+            log_output_type=LogOutputType.FILE,
+            log_format_type=LogFormatType.PLAIN,
+            debug=False,
+            log_level=LogLevel.WARNING,
         )
-        config_nucliadb(settings)
-        server = await run_async_nucliadb(settings)
+    )
+    server = await run_async_nucliadb(settings)
+
+    yield settings
 
-        yield settings
+    await maybe_cleanup_maindb()
 
-        reset_config()
-        clear_global_cache()
-        await server.shutdown()
+    reset_config()
+    clear_global_cache()
+    await server.shutdown()
 
 
 @pytest.fixture(scope="function")
 async def nucliadb_reader(nucliadb: Settings):
     async with AsyncClient(
         headers={"X-NUCLIADB-ROLES": "READER"},
         base_url=f"http://localhost:{nucliadb.http_port}/{API_PREFIX}/v1",
@@ -128,42 +206,45 @@
         headers={"X-NUCLIADB-ROLES": "MANAGER"},
         base_url=f"http://localhost:{nucliadb.http_port}/{API_PREFIX}/v1",
     ) as client:
         yield client
 
 
 @pytest.fixture(scope="function")
-async def knowledgebox(nucliadb_manager: AsyncClient):
-    resp = await nucliadb_manager.post("/kbs", json={"slug": "knowledgebox"})
+async def knowledgebox(nucliadb_manager: AsyncClient, request):
+    resp = await nucliadb_manager.post(
+        "/kbs", json={"slug": "knowledgebox", "release_channel": request.param}
+    )
     assert resp.status_code == 201
     uuid = resp.json().get("uuid")
+
     yield uuid
+
     resp = await nucliadb_manager.delete(f"/kb/{uuid}")
     assert resp.status_code == 200
 
 
 @pytest.fixture(scope="function")
 async def nucliadb_grpc(nucliadb: Settings):
-    stub = WriterStub(aio.insecure_channel(f"localhost:{nucliadb.ingest_grpc_port}"))
+    stub = WriterStub(aio.insecure_channel(f"localhost:{nucliadb.ingest_grpc_port}"))  # type: ignore
     return stub
 
 
 @pytest.fixture(scope="function")
 async def nucliadb_train(nucliadb: Settings):
-    stub = TrainStub(aio.insecure_channel(f"localhost:{nucliadb.train_grpc_port}"))
+    stub = TrainStub(aio.insecure_channel(f"localhost:{nucliadb.train_grpc_port}"))  # type: ignore
     return stub
 
 
 @pytest.fixture(scope="function")
 async def knowledge_graph(
     nucliadb_writer: AsyncClient, nucliadb_grpc: WriterStub, knowledgebox
 ):
     resp = await nucliadb_writer.post(
         f"/kb/{knowledgebox}/resources",
-        headers={"X-SYNCHRONOUS": "True"},
         json={
             "title": "Knowledge graph",
             "slug": "knowledgegraph",
             "summary": "Test knowledge graph",
         },
     )
     assert resp.status_code == 201
@@ -197,18 +278,21 @@
         "Joan Antoni": RelationNode(
             value="Joan Antoni", ntype=RelationNode.NodeType.ENTITY, subtype=""
         ),
         "Joker": RelationNode(
             value="Joker", ntype=RelationNode.NodeType.ENTITY, subtype=""
         ),
         "Newton": RelationNode(
-            value="Newton", ntype=RelationNode.NodeType.ENTITY, subtype=""
+            value="Newton", ntype=RelationNode.NodeType.ENTITY, subtype="science"
+        ),
+        "Isaac Newsome": RelationNode(
+            value="Isaac Newsome", ntype=RelationNode.NodeType.ENTITY, subtype="science"
         ),
         "Physics": RelationNode(
-            value="Physics", ntype=RelationNode.NodeType.ENTITY, subtype=""
+            value="Physics", ntype=RelationNode.NodeType.ENTITY, subtype="science"
         ),
         "Poetry": RelationNode(
             value="Poetry", ntype=RelationNode.NodeType.ENTITY, subtype=""
         ),
         "Swallow": RelationNode(
             value="Swallow", ntype=RelationNode.NodeType.ENTITY, subtype=""
         ),
@@ -255,14 +339,26 @@
             relation=Relation.RelationType.ENTITY,
             source=nodes["Newton"],
             to=nodes["Gravity"],
             relation_label="formulate",
         ),
         Relation(
             relation=Relation.RelationType.ENTITY,
+            source=nodes["Isaac Newsome"],
+            to=nodes["Physics"],
+            relation_label="study",
+        ),
+        Relation(
+            relation=Relation.RelationType.ENTITY,
+            source=nodes["Isaac Newsome"],
+            to=nodes["Gravity"],
+            relation_label="formulate",
+        ),
+        Relation(
+            relation=Relation.RelationType.ENTITY,
             source=nodes["Eric"],
             to=nodes["Cat"],
             relation_label="like",
         ),
         Relation(
             relation=Relation.RelationType.ENTITY,
             source=nodes["Eric"],
@@ -321,14 +417,52 @@
 
     bm = BrokerMessage()
     bm.uuid = rid
     bm.kbid = knowledgebox
     bm.relations.extend(edges)
     await inject_message(nucliadb_grpc, bm)
 
+    resp = await nucliadb_writer.post(
+        f"/kb/{knowledgebox}/entitiesgroups",
+        json={
+            "title": "scientist",
+            "color": "",
+            "entities": {
+                "Isaac": {"value": "Isaac"},
+                "Isaac Newton": {"value": "Isaac Newton", "represents": ["Newton"]},
+                "Isaac Newsome": {"value": "Isaac Newsome"},
+            },
+            "custom": True,
+            "group": "scientist",
+        },
+    )
+    assert resp.status_code == 200, resp.content
+    resp = await nucliadb_writer.patch(
+        f"/kb/{knowledgebox}/entitiesgroup/scientist",
+        json={"add": {}, "update": {}, "delete": ["Isaac Newsome"]},
+    )
+    assert resp.status_code == 200, resp.content
+    resp = await nucliadb_writer.post(
+        f"/kb/{knowledgebox}/entitiesgroups",
+        json={
+            "title": "poet",
+            "color": "",
+            "entities": {
+                "Becquer": {
+                    "value": "Becquer",
+                    "represents": ["Gustavo Adolfo Bécquer"],
+                },
+                "Gustavo Adolfo Bécquer": {"value": "Gustavo Adolfo Bécquer"},
+            },
+            "custom": True,
+            "group": "poet",
+        },
+    )
+    assert resp.status_code == 200, resp.content
+
     return (nodes, edges)
 
 
 @pytest.fixture(scope="function")
 async def stream_audit(natsd: str):
     from nucliadb_utils.audit.stream import StreamAuditStorage
     from nucliadb_utils.settings import audit_settings
@@ -352,7 +486,250 @@
 
     yield mock
 
     if predict is None:
         clean_utility(Utility.PREDICT)
     else:
         set_utility(Utility.PREDICT, predict)
+
+
+@pytest.fixture(scope="function")
+def metrics_registry():
+    import prometheus_client.registry  # type: ignore
+
+    for collector in prometheus_client.registry.REGISTRY._names_to_collectors.values():
+        if not hasattr(collector, "_metrics"):
+            continue
+        collector._metrics.clear()
+    yield prometheus_client.registry.REGISTRY
+
+
+@pytest.fixture(scope="function")
+async def redis_config(redis):
+    ingest_settings.driver_redis_url = f"redis://{redis[0]}:{redis[1]}"
+    default_driver = ingest_settings.driver
+
+    ingest_settings.driver = "redis"
+
+    storage_settings.local_testing_files = f"{dirname(__file__)}"
+    driver = aioredis.from_url(f"redis://{redis[0]}:{redis[1]}")
+    await driver.flushall()
+
+    yield ingest_settings.driver_redis_url
+
+    ingest_settings.driver_redis_url = None
+    ingest_settings.driver = default_driver
+    await driver.flushall()
+    await driver.close(close_connection_pool=True)
+
+    pubsub = get_utility(Utility.PUBSUB)
+    if pubsub is not None:
+        await pubsub.finalize()
+
+
+@pytest.fixture(scope="function")
+def local_driver_settings(tmpdir):
+    return DriverSettings(
+        driver=DriverConfig.LOCAL,
+        driver_local_url=f"{tmpdir}/main",
+    )
+
+
+@pytest.fixture(scope="function")
+async def local_driver(local_driver_settings) -> AsyncIterator[Driver]:
+    path = local_driver_settings.driver_local_url
+    ingest_settings.driver = DriverConfig.LOCAL
+    ingest_settings.driver_local_url = path
+
+    driver: Driver = LocalDriver(url=path)
+    await driver.initialize()
+
+    yield driver
+
+    await driver.finalize()
+
+    ingest_settings.driver_local_url = None
+    clean_utility(Utility.MAINDB_DRIVER)
+
+
+@pytest.fixture(scope="function")
+def tikv_driver_settings(tikvd):
+    if os.environ.get("TESTING_TIKV_LOCAL", None):
+        url = "localhost:2379"
+    else:
+        url = f"{tikvd[0]}:{tikvd[2]}"
+
+    # before using tikv, clear the db
+    # delete here instead of `tikv_driver` fixture because
+    # these settings are used in tests that the driver fixture
+    # is not used
+    client = tikv_client.TransactionClient.connect([url])
+    txn = client.begin(pessimistic=False)
+    for key in txn.scan_keys(start=b"", end=None, limit=99999):
+        txn.delete(key)
+    txn.commit()
+
+    return DriverSettings(driver=DriverConfig.TIKV, driver_tikv_url=[url])
+
+
+@pytest.fixture(scope="function")
+async def tikv_driver(tikv_driver_settings) -> AsyncIterator[Driver]:
+    url = tikv_driver_settings.driver_tikv_url
+    ingest_settings.driver = DriverConfig.TIKV
+    ingest_settings.driver_tikv_url = url
+
+    driver: Driver = TiKVDriver(url=url)
+    await driver.initialize()
+
+    yield driver
+
+    await driver.finalize()
+    ingest_settings.driver_tikv_url = None
+    clean_utility(Utility.MAINDB_DRIVER)
+
+
+@pytest.fixture(scope="function")
+def redis_driver_settings(redis):
+    return DriverSettings(
+        driver=DriverConfig.REDIS,
+        driver_redis_url=f"redis://{redis[0]}:{redis[1]}",
+    )
+
+
+@pytest.fixture(scope="function")
+async def redis_driver(redis_driver_settings) -> AsyncIterator[RedisDriver]:
+    url = redis_driver_settings.driver_redis_url
+    ingest_settings.driver = DriverConfig.REDIS
+    ingest_settings.driver_redis_url = url
+
+    driver = RedisDriver(url=url)
+    await driver.initialize()
+
+    assert driver.redis is not None
+    await driver.redis.flushall()
+    logging.info(f"Redis driver ready at {url}")
+
+    set_utility(Utility.MAINDB_DRIVER, driver)
+
+    yield driver
+
+    await driver.finalize()
+    ingest_settings.driver_redis_url = None
+    clean_utility(Utility.MAINDB_DRIVER)
+
+
+@pytest.fixture(scope="function")
+def pg_driver_settings(pg):
+    url = f"postgresql://postgres:postgres@{pg[0]}:{pg[1]}/postgres"
+    return DriverSettings(
+        driver=DriverConfig.PG,
+        driver_pg_url=url,
+    )
+
+
+@pytest.fixture(scope="function")
+async def pg_driver(pg_driver_settings):
+    url = pg_driver_settings.driver_pg_url
+    ingest_settings.driver = DriverConfig.PG
+    ingest_settings.driver_pg_url = url
+
+    conn = await asyncpg.connect(url)
+    await conn.execute(
+        """
+DROP table IF EXISTS resources;
+"""
+    )
+    await conn.close()
+    driver = PGDriver(url=url)
+    await driver.initialize()
+
+    yield driver
+
+    await driver.finalize()
+    ingest_settings.driver_pg_url = None
+
+
+def driver_settings_lazy_fixtures(default_drivers="local"):
+    driver_types = os.environ.get("TESTING_MAINDB_DRIVERS", default_drivers)
+    return [
+        lazy_fixture.lf(f"{driver_type}_driver_settings")
+        for driver_type in driver_types.split(",")
+    ]
+
+
+@pytest.fixture(scope="function", params=driver_settings_lazy_fixtures())
+def driver_settings(request):
+    """
+    Allows dynamically loading the driver fixtures via env vars.
+
+    MAINDB_DRIVER=redis,local pytest nucliadb/nucliadb/tests/
+
+    Any test using the nucliadb fixture will be run twice, once with redis driver and once with local driver.
+    """
+    yield request.param
+
+
+def driver_lazy_fixtures(default_drivers: str = "redis"):
+    """
+    Allows running tests using maindb_driver for each supported driver type via env vars.
+
+    MAINDB_DRIVER=redis,local pytest nucliadb/nucliadb/ingest/tests/
+
+    Any test using the maindb_driver fixture will be run twice, once with redis_driver and once with local_driver.
+    """
+    driver_types = os.environ.get("TESTING_MAINDB_DRIVERS", default_drivers)
+    return [
+        lazy_fixture.lf(f"{driver_type}_driver")
+        for driver_type in driver_types.split(",")
+    ]
+
+
+@pytest.fixture(
+    scope="function",
+    params=driver_lazy_fixtures(),
+)
+async def maindb_driver(request):
+    driver = request.param
+    set_utility(Utility.MAINDB_DRIVER, driver)
+
+    yield driver
+
+    await cleanup_maindb(driver)
+    clean_utility(Utility.MAINDB_DRIVER)
+
+
+async def maybe_cleanup_maindb():
+    try:
+        driver = get_driver()
+    except UnsetUtility:
+        pass
+    else:
+        try:
+            await cleanup_maindb(driver)
+        except Exception:
+            logger.error("Could not cleanup maindb on test teardown")
+            pass
+
+
+async def cleanup_maindb(driver: Driver):
+    if not driver.initialized:
+        return
+    async with driver.transaction() as txn:
+        all_keys = [k async for k in txn.keys("", count=-1)]
+        for key in all_keys:
+            await txn.delete(key)
+        await txn.commit()
+
+
+@pytest.fixture(scope="function")
+async def txn(maindb_driver):
+    txn = await maindb_driver.begin()
+    yield txn
+    await txn.abort()
+
+
+@pytest.fixture(scope="function")
+async def shard_manager(storage, maindb_driver):
+    mng = cluster_manager.KBShardManager()
+    set_utility(Utility.SHARD_MANAGER, mng)
+    yield mng
+    clean_utility(Utility.SHARD_MANAGER)
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## nucliadb/tests/benchmarks/test_search.py

```diff
@@ -20,15 +20,17 @@
 import time
 from unittest.mock import AsyncMock, Mock
 
 import pytest
 from httpx import AsyncClient
 from nucliadb_protos.writer_pb2_grpc import WriterStub
 
-from nucliadb.tests.integration.test_search import broker_resource_with_classifications
+from nucliadb.tests.integration.search.test_search import (
+    broker_resource_with_classifications,
+)
 from nucliadb.tests.utils import inject_message
 from nucliadb_utils.tests.asyncbenchmark import AsyncBenchmarkFixture
 from nucliadb_utils.utilities import Utility, set_utility
 
 
 @pytest.mark.benchmark(
     group="search",
@@ -36,14 +38,15 @@
     max_time=0.5,
     min_rounds=5,
     timer=time.time,
     disable_gc=True,
     warmup=False,
 )
 @pytest.mark.asyncio
+@pytest.mark.parametrize("knowledgebox", ["STABLE", "EXPERIMENTAL"], indirect=True)
 async def test_search_returns_labels(
     nucliadb_reader: AsyncClient,
     nucliadb_writer: AsyncClient,
     nucliadb_grpc: WriterStub,
     knowledgebox,
     asyncbenchmark: AsyncBenchmarkFixture,
 ):
@@ -63,14 +66,15 @@
     max_time=0.5,
     min_rounds=5,
     timer=time.time,
     disable_gc=True,
     warmup=False,
 )
 @pytest.mark.asyncio
+@pytest.mark.parametrize("knowledgebox", ["STABLE", "EXPERIMENTAL"], indirect=True)
 async def test_search_relations(
     nucliadb_reader: AsyncClient,
     nucliadb_writer: AsyncClient,
     nucliadb_grpc: WriterStub,
     knowledgebox,
     knowledge_graph,
     asyncbenchmark: AsyncBenchmarkFixture,
```

## nucliadb/tests/knowledgeboxes/philosophy_books.py

```diff
@@ -188,15 +188,14 @@
     resp = await nucliadb_manager.post("/kbs", json={"slug": "philosophy-books"})
     assert resp.status_code == 201
     kbid = resp.json().get("uuid")
 
     for payload in payloads:
         resp = await nucliadb_writer.post(
             f"/kb/{kbid}/resources",
-            headers={"X-Synchronous": "true"},
             json=payload,
         )
         assert resp.status_code == 201
 
     yield kbid
 
     resp = await nucliadb_manager.delete(f"/kb/{kbid}")
```

## nucliadb/tests/knowledgeboxes/ten_dummy_resources.py

```diff
@@ -43,15 +43,14 @@
     resp = await nucliadb_manager.post("/kbs", json={"slug": "ten-dummy-resources"})
     assert resp.status_code == 201
     kbid = resp.json().get("uuid")
 
     for payload in payloads:
         resp = await nucliadb_writer.post(
             f"/kb/{kbid}/resources",
-            headers={"X-Synchronous": "true"},
             json=payload,
         )
         assert resp.status_code == 201
 
         await asyncio.sleep(1)
 
     resp = await nucliadb_reader.get(
@@ -87,15 +86,14 @@
     resp = await nucliadb_manager.post("/kbs", json={"slug": "ten-dummy-resources"})
     assert resp.status_code == 201
     kbid = resp.json().get("uuid")
 
     for payload in payloads:
         resp = await nucliadb_writer.post(
             f"/kb/{kbid}/resources",
-            headers={"X-Synchronous": "true"},
             json=payload,
         )
         assert resp.status_code == 201
 
     resp = await nucliadb_reader.get(
         f"/kb/{kbid}/resources",
     )
```

## nucliadb/train/app.py

```diff
@@ -22,36 +22,48 @@
 from fastapi.responses import JSONResponse
 from starlette.middleware import Middleware
 from starlette.middleware.authentication import AuthenticationMiddleware
 from starlette.middleware.cors import CORSMiddleware
 from starlette.requests import ClientDisconnect, Request
 from starlette.responses import HTMLResponse
 
+from nucliadb.middleware.transaction import ReadOnlyTransactionMiddleware
 from nucliadb.train import API_PREFIX
 from nucliadb.train.api.v1.router import api
 from nucliadb.train.lifecycle import finalize, initialize
 from nucliadb_telemetry import errors
-from nucliadb_utils.authentication import STFAuthenticationBackend
+from nucliadb_utils import const
+from nucliadb_utils.authentication import NucliaCloudAuthenticationBackend
 from nucliadb_utils.fastapi.openapi import extend_openapi
 from nucliadb_utils.fastapi.versioning import VersionedFastAPI
 from nucliadb_utils.settings import http_settings, running_settings
+from nucliadb_utils.utilities import has_feature
 
-middleware = [
-    Middleware(
-        CORSMiddleware,
-        allow_origins=http_settings.cors_origins,
-        allow_methods=["*"],
-        allow_headers=["*"],
-    ),
-    Middleware(
-        AuthenticationMiddleware,
-        backend=STFAuthenticationBackend(),
-    ),
-]
+middleware = []
 
+if has_feature(const.Features.CORS_MIDDLEWARE, default=False):
+    middleware.append(
+        Middleware(
+            CORSMiddleware,
+            allow_origins=http_settings.cors_origins,
+            allow_methods=["*"],
+            # Authorization will be exluded from * in the future, (CORS non-wildcard request-header).
+            # Browsers already showing deprecation notices, so it needs to be specified explicitly
+            allow_headers=["*", "Authorization"],
+        )
+    )
+
+middleware.extend(
+    [
+        Middleware(
+            AuthenticationMiddleware, backend=NucliaCloudAuthenticationBackend()
+        ),
+        Middleware(ReadOnlyTransactionMiddleware),
+    ]
+)
 
 errors.setup_error_handling(pkg_resources.get_distribution("nucliadb").version)
 
 
 on_startup = [initialize]
 on_shutdown = [finalize]
```

## nucliadb/train/generator.py

```diff
@@ -14,87 +14,87 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
+from typing import AsyncIterator, Optional
 
 from fastapi import HTTPException
 from nucliadb_protos.dataset_pb2 import TaskType, TrainSet
 
 from nucliadb.train.generators.field_classifier import (
-    generate_field_classification_payloads,
+    field_classification_batch_generator,
+)
+from nucliadb.train.generators.image_classifier import (
+    image_classification_batch_generator,
 )
 from nucliadb.train.generators.paragraph_classifier import (
-    generate_paragraph_classification_payloads,
+    paragraph_classification_batch_generator,
+)
+from nucliadb.train.generators.paragraph_streaming import (
+    paragraph_streaming_batch_generator,
+)
+from nucliadb.train.generators.question_answer_streaming import (
+    question_answer_batch_generator,
 )
 from nucliadb.train.generators.sentence_classifier import (
-    generate_sentence_classification_payloads,
+    sentence_classification_batch_generator,
 )
 from nucliadb.train.generators.token_classifier import (
-    generate_token_classification_payloads,
+    token_classification_batch_generator,
 )
-from nucliadb.train.generators.utils import get_transaction
-from nucliadb.train.utils import get_nodes_manager
+from nucliadb.train.types import TrainBatch
+from nucliadb.train.utils import get_shard_manager
 
 
 async def generate_train_data(kbid: str, shard: str, trainset: TrainSet):
     # Get the data structure to generate data
-    node_manager = get_nodes_manager()
-    node, shard_replica_id = await node_manager.get_reader(kbid, shard)
+    shard_manager = get_shard_manager()
+    node, shard_replica_id = await shard_manager.get_reader(kbid, shard)
 
     if trainset.batch_size == 0:
         trainset.batch_size = 50
 
-    if trainset.type == TaskType.PARAGRAPH_CLASSIFICATION:
-        if len(trainset.filter.labels) != 1:
-            raise HTTPException(
-                status_code=422,
-                detail="Paragraph Classification should be of 1 labelset",
-            )
-
-        async for paragraph_data in generate_paragraph_classification_payloads(
-            kbid, trainset, node, shard_replica_id
-        ):
-            payload = paragraph_data.SerializeToString()
-            yield len(payload).to_bytes(4, byteorder="big", signed=False)
-            yield payload
+    batch_generator: Optional[AsyncIterator[TrainBatch]] = None
 
     if trainset.type == TaskType.FIELD_CLASSIFICATION:
-        if len(trainset.filter.labels) != 1:
-            raise HTTPException(
-                status_code=422,
-                detail="Field Classification should be of 1 labelset",
-            )
-
-        async for field_data in generate_field_classification_payloads(
-            kbid, trainset, node, shard_replica_id
-        ):
-            payload = field_data.SerializeToString()
-            yield len(payload).to_bytes(4, byteorder="big", signed=False)
-            yield payload
-
-    if trainset.type == TaskType.TOKEN_CLASSIFICATION:
-        async for token_data in generate_token_classification_payloads(
-            kbid, trainset, node, shard_replica_id
-        ):
-            payload = token_data.SerializeToString()
-            yield len(payload).to_bytes(4, byteorder="big", signed=False)
-            yield payload
-
-    if trainset.type == TaskType.SENTENCE_CLASSIFICATION:
-        if len(trainset.filter.labels) == 0:
-            raise HTTPException(
-                status_code=422,
-                detail="Sentence Classification should be at least of 1 labelset",
-            )
-
-        async for sentence_data in generate_sentence_classification_payloads(
-            kbid, trainset, node, shard_replica_id
-        ):
-            payload = sentence_data.SerializeToString()
-            yield len(payload).to_bytes(4, byteorder="big", signed=False)
-            yield payload
+        batch_generator = field_classification_batch_generator(
+            kbid, trainset, node, shard_replica_id
+        )
+    elif trainset.type == TaskType.IMAGE_CLASSIFICATION:
+        batch_generator = image_classification_batch_generator(
+            kbid, trainset, node, shard_replica_id
+        )
+    elif trainset.type == TaskType.PARAGRAPH_CLASSIFICATION:
+        batch_generator = paragraph_classification_batch_generator(
+            kbid, trainset, node, shard_replica_id
+        )
+    elif trainset.type == TaskType.TOKEN_CLASSIFICATION:
+        batch_generator = token_classification_batch_generator(
+            kbid, trainset, node, shard_replica_id
+        )
+    elif trainset.type == TaskType.SENTENCE_CLASSIFICATION:
+        batch_generator = sentence_classification_batch_generator(
+            kbid, trainset, node, shard_replica_id
+        )
+    elif trainset.type == TaskType.PARAGRAPH_STREAMING:
+        batch_generator = paragraph_streaming_batch_generator(
+            kbid, trainset, node, shard_replica_id
+        )
+
+    elif trainset.type == TaskType.QUESTION_ANSWER_STREAMING:
+        batch_generator = question_answer_batch_generator(
+            kbid, trainset, node, shard_replica_id
+        )
 
-    txn = await get_transaction()
-    await txn.abort()
+    if batch_generator is None:
+        raise HTTPException(
+            status_code=400,
+            detail=f"Invalid train type '{TaskType.Name(trainset.type)}'",
+        )
+
+    async for item in batch_generator:
+        payload = item.SerializeToString()
+        yield len(payload).to_bytes(4, byteorder="big", signed=False)
+        yield payload
```

## nucliadb/train/lifecycle.py

```diff
@@ -14,34 +14,37 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
-from nucliadb.ingest.chitchat import start_chitchat, stop_chitchat
+from nucliadb.common.cluster.discovery.utils import (
+    setup_cluster_discovery,
+    teardown_cluster_discovery,
+)
 from nucliadb.train import SERVICE_NAME
 from nucliadb.train.utils import (
-    start_nodes_manager,
+    start_shard_manager,
     start_train_grpc,
-    stop_nodes_manager,
+    stop_shard_manager,
     stop_train_grpc,
 )
 from nucliadb_telemetry.utils import clean_telemetry, setup_telemetry
 from nucliadb_utils.utilities import start_audit_utility, stop_audit_utility
 
 
 async def initialize() -> None:
     await setup_telemetry(SERVICE_NAME)
 
-    await start_chitchat(SERVICE_NAME)
-    await start_nodes_manager()
+    await setup_cluster_discovery()
+    await start_shard_manager()
     await start_train_grpc(SERVICE_NAME)
     await start_audit_utility(SERVICE_NAME)
 
 
 async def finalize() -> None:
     await stop_audit_utility()
     await stop_train_grpc()
-    await stop_nodes_manager()
-    await stop_chitchat()
+    await stop_shard_manager()
+    await teardown_cluster_discovery()
     await clean_telemetry(SERVICE_NAME)
```

## nucliadb/train/nodes.py

```diff
@@ -13,130 +13,61 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-import random
-from typing import AsyncIterator, List, Optional, Tuple
+from typing import AsyncIterator, Optional
 
 from nucliadb_protos.train_pb2 import (
     GetFieldsRequest,
     GetParagraphsRequest,
     GetResourcesRequest,
     GetSentencesRequest,
     TrainField,
     TrainParagraph,
     TrainResource,
     TrainSentence,
 )
 from nucliadb_protos.writer_pb2 import ShardObject
-from nucliadb_protos.writer_pb2 import Shards as PBShards
 
-from nucliadb.ingest.maindb.driver import Driver, Transaction
-from nucliadb.ingest.orm import NODE_CLUSTER, NODES
+from nucliadb.common import datamanagers
+from nucliadb.common.cluster import manager
+from nucliadb.common.cluster.base import AbstractIndexNode
+from nucliadb.common.maindb.driver import Driver, Transaction
 from nucliadb.ingest.orm.entities import EntitiesManager
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
-from nucliadb.ingest.orm.node import Node
 from nucliadb.ingest.orm.resource import KB_RESOURCE_SLUG_BASE
-from nucliadb_utils.exceptions import ShardsNotFound
-from nucliadb_utils.keys import KB_SHARDS
 from nucliadb_utils.storages.storage import Storage
 
 
-class TrainNodesManager:
+class TrainShardManager(manager.KBShardManager):
     def __init__(self, driver: Driver, storage: Storage):
+        super().__init__()
         self.driver = driver
         self.storage = storage
 
-    async def get_reader(self, kbid: str, shard: str) -> Tuple[Node, str]:
+    async def get_reader(self, kbid: str, shard: str) -> tuple[AbstractIndexNode, str]:
         shards = await self.get_shards_by_kbid_inner(kbid)
         try:
             shard_object: ShardObject = next(
                 filter(lambda x: x.shard == shard, shards.shards)
             )
         except StopIteration:
             raise KeyError("Shard not found")
 
-        if NODE_CLUSTER.local_node:
-            return (
-                NODE_CLUSTER.get_local_node(),
-                shard_object.replicas[0].shard.id,
-            )
-        nodes = [x for x in range(len(shard_object.replicas))]
-        random.shuffle(nodes)
-        node_obj = None
-        shard_id = None
-        for node in nodes:
-            node_id = shard_object.replicas[node].node
-            if node_id in NODES:
-                node_obj = NODES[node_id]
-                shard_id = shard_object.replicas[node].shard.id
-                break
-
-        if node_obj is None or node_id is None or shard_id is None:
-            raise KeyError("Could not find a node to query")
-
+        node_obj, shard_id = manager.choose_node(shard_object)
         return node_obj, shard_id
 
-    async def get_shards_by_kbid_inner(self, kbid: str) -> PBShards:
-        key = KB_SHARDS.format(kbid=kbid)
-        txn = await self.driver.begin()
-        payload = await txn.get(key)
-        await txn.abort()
-        if payload is None:
-            # could be None because /shards doesn't exist, or beacause the whole KB does not exist.
-            # In any case, this should not happen
-            raise ShardsNotFound(kbid)
-
-        pb = PBShards()
-        pb.ParseFromString(payload)
-        return pb
-
-    async def get_shards_by_kbid(self, kbid: str) -> List[ShardObject]:
-        shards = await self.get_shards_by_kbid_inner(kbid)
-        return [x for x in shards.shards]
-
-    def choose_node(
-        self, shard: ShardObject, shards: Optional[List[str]] = None
-    ) -> Tuple[Node, Optional[str], str]:
-        shards = shards or []
-
-        if NODE_CLUSTER.local_node:
-            return (
-                NODE_CLUSTER.get_local_node(),
-                shard.replicas[0].shard.id,
-                shard.replicas[0].node,
-            )
-        nodes = [x for x in range(len(shard.replicas))]
-        random.shuffle(nodes)
-        node_obj = None
-        shard_id = None
-        for node in nodes:
-            node_id = shard.replicas[node].node
-            if node_id in NODES:
-                node_obj = NODES[node_id]
-                shard_id = shard.replicas[node].shard.id
-                if len(shards) > 0 and shard_id not in shards:
-                    node_obj = None
-                    shard_id = None
-                else:
-                    break
-
-        if node_obj is None or node_id is None:
-            raise KeyError("Could not find a node to query")
-
-        return node_obj, shard_id, node_id
-
     async def get_kb_obj(self, txn: Transaction, kbid: str) -> Optional[KnowledgeBox]:
         if kbid is None:
             return None
 
-        if not (await KnowledgeBox.exist_kb(txn, kbid)):
+        if not (await datamanagers.kb.exists_kb(txn, kbid=kbid)):
             return None
 
         kbobj = KnowledgeBox(txn, self.storage, kbid)
         return kbobj
 
     async def get_kb_entities_manager(
         self, txn: Transaction, kbid: str
@@ -147,68 +78,67 @@
 
         manager = EntitiesManager(kbobj, txn)
         return manager
 
     async def kb_sentences(
         self, request: GetSentencesRequest
     ) -> AsyncIterator[TrainSentence]:
-        txn = await self.driver.begin()
-        kb = KnowledgeBox(txn, self.storage, request.kb.uuid)
-        if request.uuid != "":
-            # Filter by uuid
-            resource = await kb.get(request.uuid)
-            if resource:
-                async for sentence in resource.iterate_sentences(request.metadata):
-                    yield sentence
-        else:
-            async for resource in kb.iterate_resources():
-                async for sentence in resource.iterate_sentences(request.metadata):
-                    yield sentence
-        await txn.abort()
+        async with self.driver.transaction() as txn:
+            kb = KnowledgeBox(txn, self.storage, request.kb.uuid)
+            if request.uuid != "":
+                # Filter by uuid
+                resource = await kb.get(request.uuid)
+                if resource:
+                    async for sentence in resource.iterate_sentences(request.metadata):
+                        yield sentence
+            else:
+                async for resource in kb.iterate_resources():
+                    async for sentence in resource.iterate_sentences(request.metadata):
+                        yield sentence
 
     async def kb_paragraphs(
         self, request: GetParagraphsRequest
     ) -> AsyncIterator[TrainParagraph]:
-        txn = await self.driver.begin()
-        kb = KnowledgeBox(txn, self.storage, request.kb.uuid)
-        if request.uuid != "":
-            # Filter by uuid
-            resource = await kb.get(request.uuid)
-            if resource:
-                async for paragraph in resource.iterate_paragraphs(request.metadata):
-                    yield paragraph
-        else:
-            async for resource in kb.iterate_resources():
-                async for paragraph in resource.iterate_paragraphs(request.metadata):
-                    yield paragraph
-        await txn.abort()
+        async with self.driver.transaction() as txn:
+            kb = KnowledgeBox(txn, self.storage, request.kb.uuid)
+            if request.uuid != "":
+                # Filter by uuid
+                resource = await kb.get(request.uuid)
+                if resource:
+                    async for paragraph in resource.iterate_paragraphs(
+                        request.metadata
+                    ):
+                        yield paragraph
+            else:
+                async for resource in kb.iterate_resources():
+                    async for paragraph in resource.iterate_paragraphs(
+                        request.metadata
+                    ):
+                        yield paragraph
 
     async def kb_fields(self, request: GetFieldsRequest) -> AsyncIterator[TrainField]:
-        txn = await self.driver.begin()
-        kb = KnowledgeBox(txn, self.storage, request.kb.uuid)
-        if request.uuid != "":
-            # Filter by uuid
-            resource = await kb.get(request.uuid)
-            if resource:
-                async for field in resource.iterate_fields(request.metadata):
-                    yield field
-        else:
-            async for resource in kb.iterate_resources():
-                async for field in resource.iterate_fields(request.metadata):
-                    yield field
-        await txn.abort()
+        async with self.driver.transaction() as txn:
+            kb = KnowledgeBox(txn, self.storage, request.kb.uuid)
+            if request.uuid != "":
+                # Filter by uuid
+                resource = await kb.get(request.uuid)
+                if resource:
+                    async for field in resource.iterate_fields(request.metadata):
+                        yield field
+            else:
+                async for resource in kb.iterate_resources():
+                    async for field in resource.iterate_fields(request.metadata):
+                        yield field
 
     async def kb_resources(
         self, request: GetResourcesRequest
     ) -> AsyncIterator[TrainResource]:
-        txn = await self.driver.begin()
-        kb = KnowledgeBox(txn, self.storage, request.kb.uuid)
-        base = KB_RESOURCE_SLUG_BASE.format(kbid=request.kb.uuid)
-        async for key in txn.keys(match=base, count=-1):
-            # Fetch and Add wanted item
-            rid = await txn.get(key)
-            if rid is not None:
-                resource = await kb.get(rid.decode())
-                if resource is not None:
-                    yield await resource.generate_train_resource(request.metadata)
-
-        await txn.abort()
+        async with self.driver.transaction() as txn:
+            kb = KnowledgeBox(txn, self.storage, request.kb.uuid)
+            base = KB_RESOURCE_SLUG_BASE.format(kbid=request.kb.uuid)
+            async for key in txn.keys(match=base, count=-1):
+                # Fetch and Add wanted item
+                rid = await txn.get(key)
+                if rid is not None:
+                    resource = await kb.get(rid.decode())
+                    if resource is not None:
+                        yield await resource.generate_train_resource(request.metadata)
```

## nucliadb/train/run.py

```diff
@@ -29,10 +29,11 @@
     setup_logging()
 
     instrument_app(
         application,
         tracer_provider=get_telemetry(SERVICE_NAME),
         excluded_urls=["/"],
         metrics=True,
+        trace_id_on_responses=True,
     )
 
     run_fastapi_with_metrics(application)
```

## nucliadb/train/servicer.py

```diff
@@ -36,25 +36,29 @@
     GetEntitiesRequest,
     GetEntitiesResponse,
     GetLabelsRequest,
     GetLabelsResponse,
 )
 
 from nucliadb.train.settings import settings
-from nucliadb.train.utils import get_nodes_manager
+from nucliadb.train.utils import get_shard_manager
 from nucliadb_protos import train_pb2_grpc
 from nucliadb_telemetry import errors
 
 
 class TrainServicer(train_pb2_grpc.TrainServicer):
     async def initialize(self):
-        self.proc = get_nodes_manager()
+        self.session = aiohttp.ClientSession()
+        self.proc = get_shard_manager()  # XXX this is odd use here
 
     async def finalize(self):
-        pass
+        try:
+            await self.session.close()
+        except Exception:
+            pass
 
     async def GetSentences(self, request: GetSentencesRequest, context=None):
         async for sentence in self.proc.kb_sentences(request):
             yield sentence
 
     async def GetParagraphs(self, request: GetParagraphsRequest, context=None):
         async for paragraph in self.proc.kb_paragraphs(request):
@@ -68,58 +72,55 @@
         async for resource in self.proc.kb_resources(request):
             yield resource
 
     async def GetInfo(self, request: GetInfoRequest, context=None):  # type: ignore
         result = TrainInfo()
         url = settings.internal_counter_api.format(kbid=request.kb.uuid)
         headers = {"X-NUCLIADB-ROLES": "READER"}
-        async with aiohttp.ClientSession() as sess:
-            async with sess.get(url, headers=headers) as resp:
-                data = await resp.json()
+        resp = await self.session.get(url, headers=headers)
+        resp.raise_for_status()
+        data = await resp.json()
         result.resources = data["resources"]
         result.paragraphs = data["paragraphs"]
         result.fields = data["fields"]
         result.sentences = data["sentences"]
         return result
 
     async def GetEntities(  # type: ignore
         self, request: GetEntitiesRequest, context=None
     ) -> GetEntitiesResponse:
         kbid = request.kb.uuid
         response = GetEntitiesResponse()
-        txn = await self.proc.driver.begin()
-
-        entities_manager = await self.proc.get_kb_entities_manager(txn, kbid)
-        if entities_manager is None:
-            await txn.abort()
-            response.status = GetEntitiesResponse.Status.NOTFOUND
-            return response
-
-        try:
-            await entities_manager.get_entities(response)
-        except Exception as e:
-            errors.capture_exception(e)
-            traceback.print_exc()
-            response.status = GetEntitiesResponse.Status.ERROR
-        else:
-            response.kb.uuid = kbid
-            response.status = GetEntitiesResponse.Status.OK
-
-        await txn.abort()
+        async with self.proc.driver.transaction() as txn:
+            entities_manager = await self.proc.get_kb_entities_manager(txn, kbid)
+            if entities_manager is None:
+                await txn.abort()
+                response.status = GetEntitiesResponse.Status.NOTFOUND
+                return response
+
+            try:
+                await entities_manager.get_entities(response)
+            except Exception as e:
+                errors.capture_exception(e)
+                traceback.print_exc()
+                response.status = GetEntitiesResponse.Status.ERROR
+            else:
+                response.kb.uuid = kbid
+                response.status = GetEntitiesResponse.Status.OK
         return response
 
     async def GetOntology(  # type: ignore
         self, request: GetLabelsRequest, context=None
     ) -> GetLabelsResponse:
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb.uuid)
-        labels: Optional[Labels] = None
-        if kbobj is not None:
-            labels = await kbobj.get_labels()
-        await txn.abort()
+        async with self.proc.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb.uuid)
+            labels: Optional[Labels] = None
+            if kbobj is not None:
+                labels = await kbobj.get_labels()
+
         response = GetLabelsResponse()
         if kbobj is None:
             response.status = GetLabelsResponse.Status.NOTFOUND
         else:
             response.kb.uuid = kbobj.kbid
             if labels is not None:
                 response.labels.CopyFrom(labels)
```

## nucliadb/train/upload.py

```diff
@@ -16,15 +16,15 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import argparse
 import asyncio
 from asyncio import tasks
-from typing import Callable, List
+from typing import Callable
 
 import pkg_resources
 
 from nucliadb.train.uploader import start_upload
 from nucliadb_telemetry import errors
 from nucliadb_telemetry.logs import setup_logging
 from nucliadb_utils.settings import running_settings
@@ -36,19 +36,19 @@
     parser.add_argument(
         "-r", "--request", dest="request", help="Request UUID", required=True
     )
 
     parser.add_argument("-k", "--kb", dest="kb", help="Knowledge Box", required=True)
 
 
-async def main() -> List[Callable]:
+async def main() -> list[Callable]:
     parser = arg_parse()
 
     await start_upload(parser.request, parser.kb)
-    finalizers: List[Callable] = []
+    finalizers: list[Callable] = []
 
     return finalizers
 
 
 def _cancel_all_tasks(loop):
     to_cancel = tasks.all_tasks(loop)
     if not to_cancel:
@@ -77,15 +77,15 @@
 
     errors.setup_error_handling(pkg_resources.get_distribution("nucliadb").version)
 
     if asyncio._get_running_loop() is not None:
         raise RuntimeError("cannot be called from a running event loop")
 
     loop = asyncio.new_event_loop()
-    finalizers: List[Callable] = []
+    finalizers: list[Callable] = []
     try:
         asyncio.set_event_loop(loop)
         if running_settings.debug is not None:
             loop.set_debug(running_settings.debug)
         finalizers.extend(loop.run_until_complete(main()))
 
         loop.run_forever()
```

## nucliadb/train/uploader.py

```diff
@@ -31,32 +31,31 @@
 from nucliadb_protos.writer_pb2 import (
     GetEntitiesRequest,
     GetEntitiesResponse,
     GetLabelsRequest,
     GetLabelsResponse,
 )
 
+from nucliadb.common.maindb.utils import setup_driver
 from nucliadb.ingest.orm.entities import EntitiesManager
 from nucliadb.ingest.orm.processor import Processor
-from nucliadb.ingest.utils import get_driver
 from nucliadb.train import SERVICE_NAME
 from nucliadb.train.models import RequestData
 from nucliadb.train.settings import settings
-from nucliadb_utils.utilities import get_cache, get_storage
+from nucliadb_utils.utilities import get_pubsub, get_storage
 
 
 class UploadServicer:
     async def initialize(self):
         storage = await get_storage(service_name=SERVICE_NAME)
-        driver = await get_driver()
-        cache = await get_cache()
-        self.proc = Processor(driver=driver, storage=storage, cache=cache)
+        driver = await setup_driver()
+        pubsub = await get_pubsub()
+        self.proc = Processor(driver=driver, storage=storage, pubsub=pubsub)
 
-    async def finalize(self):
-        ...
+    async def finalize(self): ...
 
     async def GetSentences(self, request: GetSentencesRequest, context=None):
         async for sentence in self.proc.kb_sentences(request):
             yield sentence
 
     async def GetParagraphs(self, request: GetParagraphsRequest, context=None):
         async for paragraph in self.proc.kb_paragraphs(request):
@@ -71,38 +70,36 @@
             yield resource
 
     async def GetEntities(  # type: ignore
         self, request: GetEntitiesRequest, context=None
     ) -> GetEntitiesResponse:
         kbid = request.kb.uuid
         response = GetEntitiesResponse()
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb)
+        async with self.proc.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb)
 
-        if kbobj is None:
-            await txn.abort()
-            response.status = GetEntitiesResponse.Status.NOTFOUND
-            return response
-
-        entities_manager = EntitiesManager(kbobj, txn)
-        await entities_manager.get_entities(response)
-        response.kb.uuid = kbid
-        response.status = GetEntitiesResponse.Status.OK
-        await txn.abort()
+            if kbobj is None:
+                response.status = GetEntitiesResponse.Status.NOTFOUND
+                return response
+
+            entities_manager = EntitiesManager(kbobj, txn)
+            await entities_manager.get_entities(response)
+            response.kb.uuid = kbid
+            response.status = GetEntitiesResponse.Status.OK
         return response
 
     async def GetOntology(  # type: ignore
         self, request: GetLabelsRequest, context=None
     ) -> GetLabelsResponse:
-        txn = await self.proc.driver.begin()
-        kbobj = await self.proc.get_kb_obj(txn, request.kb)
-        labels: Optional[Labels] = None
-        if kbobj is not None:
-            labels = await kbobj.get_labels()
-        await txn.abort()
+        async with self.proc.driver.transaction() as txn:
+            kbobj = await self.proc.get_kb_obj(txn, request.kb)
+            labels: Optional[Labels] = None
+            if kbobj is not None:
+                labels = await kbobj.get_labels()
+
         response = GetLabelsResponse()
         if kbobj is None:
             response.status = GetLabelsResponse.Status.NOTFOUND
         else:
             response.kb.uuid = kbobj.kbid
             if labels is not None:
                 response.labels.CopyFrom(labels)
```

## nucliadb/train/utils.py

```diff
@@ -15,19 +15,19 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from typing import Optional
 
-from grpc import aio  # type: ignore
+from grpc import aio
 from grpc_health.v1 import health, health_pb2_grpc
 
-from nucliadb.ingest.utils import get_driver
-from nucliadb.train.nodes import TrainNodesManager  # type: ignore
+from nucliadb.common.maindb.utils import setup_driver, teardown_driver
+from nucliadb.train.nodes import TrainShardManager  # type: ignore
 from nucliadb.train.settings import settings
 from nucliadb_protos import train_pb2_grpc
 from nucliadb_telemetry.utils import setup_telemetry
 from nucliadb_utils.grpc import get_traced_grpc_server
 from nucliadb_utils.utilities import (
     Utility,
     clean_utility,
@@ -38,15 +38,15 @@
 
 
 async def start_train_grpc(service_name: Optional[str] = None):
     actual_service = get_utility(Utility.TRAIN)
     if actual_service is not None:
         return
 
-    aio.init_grpc_aio()
+    aio.init_grpc_aio()  # type: ignore
 
     await setup_telemetry(service_name or "train")
     server = get_traced_grpc_server(service_name or "train")
 
     from nucliadb.train.servicer import TrainServicer
 
     servicer = TrainServicer()
@@ -68,23 +68,30 @@
         clean_utility(Utility.TRAIN_SERVER)
     if get_utility(Utility.TRAIN):
         util = get_utility(Utility.TRAIN)
         await util.finalize()
         clean_utility(Utility.TRAIN)
 
 
-async def start_nodes_manager():
-    driver = await get_driver()
+_TRAIN_SM_NAME = "train_shard_manager"
+
+
+async def start_shard_manager():
+    """
+    XXX this is weird but too much to untangle right now
+    """
+    driver = await setup_driver()
     storage = await get_storage()
-    set_utility(Utility.NODES, TrainNodesManager(driver=driver, storage=storage))
+    set_utility(_TRAIN_SM_NAME, TrainShardManager(driver=driver, storage=storage))
 
 
-async def stop_nodes_manager():
-    if get_utility(Utility.NODES):
-        clean_utility(Utility.NODES)
+async def stop_shard_manager():
+    if get_utility(_TRAIN_SM_NAME):
+        clean_utility(_TRAIN_SM_NAME)
+    await teardown_driver()
 
 
-def get_nodes_manager() -> TrainNodesManager:
-    util = get_utility(Utility.NODES)
+def get_shard_manager() -> TrainShardManager:
+    util = get_utility(_TRAIN_SM_NAME)
     if util is None:
         raise AttributeError("No Node Manager defined")
     return util
```

## nucliadb/train/api/utils.py

```diff
@@ -19,20 +19,20 @@
 #
 
 
 from typing import Optional
 
 from nucliadb_protos.dataset_pb2 import TrainSet
 
-from nucliadb.train.utils import get_nodes_manager
+from nucliadb.train.utils import get_shard_manager
 
 
 async def get_kb_partitions(kbid: str, prefix: Optional[str] = None):
-    nodes_manager = get_nodes_manager()
-    shards = await nodes_manager.get_shards_by_kbid_inner(kbid=kbid)
+    shard_manager = get_shard_manager()
+    shards = await shard_manager.get_shards_by_kbid_inner(kbid=kbid)
     valid_shards = []
     if prefix is None:
         prefix = ""
     for shard in shards.shards:
         if shard.shard.startswith(prefix):
             valid_shards.append(shard.shard)
     return valid_shards
```

## nucliadb/train/api/v1/check.py

```diff
@@ -17,18 +17,18 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
 from fastapi import Request
 from fastapi_versioning import version  # type: ignore
 
-from nucliadb.train.api.models import TrainSetPartitions
 from nucliadb.train.api.utils import get_kb_partitions
 from nucliadb.train.api.v1.router import KB_PREFIX, api
 from nucliadb_models.resource import NucliaDBRoles
+from nucliadb_models.trainset import TrainSetPartitions
 from nucliadb_utils.authentication import requires_one
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/check/labeler/{{labelset}}",
     tags=["Train"],
     status_code=200,
```

## nucliadb/train/api/v1/trainset.py

```diff
@@ -19,35 +19,45 @@
 #
 
 from typing import Optional
 
 from fastapi import Request
 from fastapi_versioning import version  # type: ignore
 
-from nucliadb.train.api.models import TrainSetPartitions
 from nucliadb.train.api.utils import get_kb_partitions
 from nucliadb.train.api.v1.router import KB_PREFIX, api
 from nucliadb_models.resource import NucliaDBRoles
+from nucliadb_models.trainset import TrainSetPartitions
 from nucliadb_utils.authentication import requires_one
 
 
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/trainset",
     tags=["Train"],
     status_code=200,
     name="Return Train call",
     response_model=TrainSetPartitions,
 )
+@requires_one([NucliaDBRoles.READER])
+@version(1)
+async def get_partitions_all(request: Request, kbid: str) -> TrainSetPartitions:
+    return await get_partitions(kbid)
+
+
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/trainset/{{prefix}}",
     tags=["Train"],
     status_code=200,
     name="Return Train call",
     response_model=TrainSetPartitions,
 )
 @requires_one([NucliaDBRoles.READER])
 @version(1)
-async def get_partitions(
-    request: Request, kbid: str, prefix: Optional[str] = None
+async def get_partitions_prefix(
+    request: Request, kbid: str, prefix: str
 ) -> TrainSetPartitions:
+    return await get_partitions(kbid, prefix=prefix)
+
+
+async def get_partitions(kbid: str, prefix: Optional[str] = None) -> TrainSetPartitions:
     all_keys = await get_kb_partitions(kbid, prefix)
     return TrainSetPartitions(partitions=all_keys)
```

## nucliadb/train/generators/field_classifier.py

```diff
@@ -14,71 +14,64 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
-from typing import AsyncIterator
+from typing import AsyncGenerator
 
+from fastapi import HTTPException
 from nucliadb_protos.dataset_pb2 import (
     FieldClassificationBatch,
     Label,
     TextLabel,
     TrainSet,
 )
 from nucliadb_protos.nodereader_pb2 import StreamRequest
 
-from nucliadb.ingest.orm.node import Node
+from nucliadb.common.cluster.base import AbstractIndexNode
 from nucliadb.ingest.orm.resource import KB_REVERSE
 from nucliadb.train import logger
-from nucliadb.train.generators.utils import get_resource_from_cache
+from nucliadb.train.generators.utils import batchify, get_resource_from_cache_or_db
 
 
-async def get_field_text(kbid: str, rid: str, field: str, field_type: str) -> str:
-    orm_resource = await get_resource_from_cache(kbid, rid)
-
-    if orm_resource is None:
-        logger.error(f"{rid} does not exist on DB")
-        return ""
-
-    field_type_int = KB_REVERSE[field_type]
-    field_obj = await orm_resource.get_field(field, field_type_int, load=False)
-    extracted_text = await field_obj.get_extracted_text()
-    if extracted_text is None:
-        logger.warning(
-            f"{rid} {field} {field_type_int} extracted_text does not exist on DB"
+def field_classification_batch_generator(
+    kbid: str,
+    trainset: TrainSet,
+    node: AbstractIndexNode,
+    shard_replica_id: str,
+) -> AsyncGenerator[FieldClassificationBatch, None]:
+    if len(trainset.filter.labels) != 1:
+        raise HTTPException(
+            status_code=422,
+            detail="Paragraph Classification should be of 1 labelset",
         )
-        return ""
 
-    text = ""
-    for _, split in extracted_text.split_text.items():
-        text += split
-        text += " "
-    text += extracted_text.text
-
-    return text
+    generator = generate_field_classification_payloads(
+        kbid, trainset, node, shard_replica_id
+    )
+    batch_generator = batchify(generator, trainset.batch_size, FieldClassificationBatch)
+    return batch_generator
 
 
 async def generate_field_classification_payloads(
     kbid: str,
     trainset: TrainSet,
-    node: Node,
+    node: AbstractIndexNode,
     shard_replica_id: str,
-) -> AsyncIterator[FieldClassificationBatch]:
+) -> AsyncGenerator[TextLabel, None]:
     labelset = f"/l/{trainset.filter.labels[0]}"
 
     # Query how many resources has each label
     request = StreamRequest()
     request.shard_id.id = shard_replica_id
-    request.filter.tags.append(labelset)
-    request.reload = True
+    request.filter.labels.append(labelset)
     total = 0
 
-    batch = FieldClassificationBatch()
     async for document_item in node.stream_get_fields(request):
         text_labels = []
         for label in document_item.labels:
             if label.startswith(labelset):
                 text_labels.append(label)
 
         field_id = f"{document_item.uuid}{document_item.field}"
@@ -87,15 +80,34 @@
         tl = TextLabel()
         rid, field_type, field = field_id.split("/")
         tl.text = await get_field_text(kbid, rid, field, field_type)
 
         for label in text_labels:
             _, _, labelset_title, label_title = label.split("/")
             tl.labels.append(Label(labelset=labelset_title, label=label_title))
-        batch.data.append(tl)
 
-        if len(batch.data) == trainset.batch_size:
-            yield batch
-            batch = FieldClassificationBatch()
+        yield tl
 
-    if len(batch.data):
-        yield batch
+
+async def get_field_text(kbid: str, rid: str, field: str, field_type: str) -> str:
+    orm_resource = await get_resource_from_cache_or_db(kbid, rid)
+
+    if orm_resource is None:
+        logger.error(f"{rid} does not exist on DB")
+        return ""
+
+    field_type_int = KB_REVERSE[field_type]
+    field_obj = await orm_resource.get_field(field, field_type_int, load=False)
+    extracted_text = await field_obj.get_extracted_text()
+    if extracted_text is None:
+        logger.warning(
+            f"{rid} {field} {field_type_int} extracted_text does not exist on DB"
+        )
+        return ""
+
+    text = ""
+    for _, split in extracted_text.split_text.items():
+        text += split
+        text += " "
+    text += extracted_text.text
+
+    return text
```

## nucliadb/train/generators/paragraph_classifier.py

```diff
@@ -14,95 +14,71 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
-from typing import AsyncIterator
+from typing import AsyncGenerator
 
+from fastapi import HTTPException
 from nucliadb_protos.dataset_pb2 import (
     Label,
     ParagraphClassificationBatch,
     TextLabel,
     TrainSet,
 )
 from nucliadb_protos.nodereader_pb2 import StreamRequest
 
-from nucliadb.ingest.orm.node import Node
-from nucliadb.ingest.orm.resource import KB_REVERSE
-from nucliadb.train import logger
-from nucliadb.train.generators.utils import get_resource_from_cache
-
-
-async def get_paragraph(kbid: str, result: str) -> str:
-    if result.count("/") == 5:
-        rid, field_type, field, split_str, start_end = result.split("/")
-        split = int(split_str)
-        start_str, end_str = start_end.split("-")
-    else:
-        rid, field_type, field, start_end = result.split("/")
-        split = None
-        start_str, end_str = start_end.split("-")
-    start = int(start_str)
-    end = int(end_str)
-
-    orm_resource = await get_resource_from_cache(kbid, rid)
-
-    if orm_resource is None:
-        logger.error(f"{rid} does not exist on DB")
-        return ""
-
-    field_type_int = KB_REVERSE[field_type]
-    field_obj = await orm_resource.get_field(field, field_type_int, load=False)
-    extracted_text = await field_obj.get_extracted_text()
-    if extracted_text is None:
-        logger.warning(
-            f"{rid} {field} {field_type_int} extracted_text does not exist on DB"
-        )
-        return ""
+from nucliadb.common.cluster.base import AbstractIndexNode
+from nucliadb.train.generators.utils import batchify, get_paragraph
+
 
-    if split is not None:
-        text = extracted_text.split_text[split]
-        splitted_text = text[start:end]
-    else:
-        splitted_text = extracted_text.text[start:end]
+def paragraph_classification_batch_generator(
+    kbid: str,
+    trainset: TrainSet,
+    node: AbstractIndexNode,
+    shard_replica_id: str,
+) -> AsyncGenerator[ParagraphClassificationBatch, None]:
+    if len(trainset.filter.labels) != 1:
+        raise HTTPException(
+            status_code=422,
+            detail="Paragraph Classification should be of 1 labelset",
+        )
 
-    return splitted_text
+    generator = generate_paragraph_classification_payloads(
+        kbid, trainset, node, shard_replica_id
+    )
+    batch_generator = batchify(
+        generator, trainset.batch_size, ParagraphClassificationBatch
+    )
+    return batch_generator
 
 
 async def generate_paragraph_classification_payloads(
     kbid: str,
     trainset: TrainSet,
-    node: Node,
+    node: AbstractIndexNode,
     shard_replica_id: str,
-) -> AsyncIterator[ParagraphClassificationBatch]:
+) -> AsyncGenerator[TextLabel, None]:
     labelset = f"/l/{trainset.filter.labels[0]}"
 
     # Query how many paragraphs has each label
     request = StreamRequest()
     request.shard_id.id = shard_replica_id
-    request.filter.tags.append(labelset)
-    request.reload = True
-    batch = ParagraphClassificationBatch()
+    request.filter.labels.append(labelset)
 
     async for paragraph_item in node.stream_get_paragraphs(request):
         text_labels = []
         for label in paragraph_item.labels:
             if label.startswith(labelset):
                 text_labels.append(label)
 
         tl = TextLabel()
         paragraph_text = await get_paragraph(kbid, paragraph_item.id)
 
         tl.text = paragraph_text
         for label in text_labels:
-            _, _, label_abelset, label_title = label.split("/")
-            tl.labels.append(Label(labelset=label_abelset, label=label_title))
-        batch.data.append(tl)
-
-        if len(batch.data) == trainset.batch_size:
-            yield batch
-            batch = ParagraphClassificationBatch()
+            _, _, label_labelset, label_title = label.split("/")
+            tl.labels.append(Label(labelset=label_labelset, label=label_title))
 
-    if len(batch.data):
-        yield batch
+        yield tl
```

## nucliadb/train/generators/sentence_classifier.py

```diff
@@ -14,39 +14,98 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
-from typing import AsyncIterator, List
+from typing import AsyncGenerator
 
+from fastapi import HTTPException
 from nucliadb_protos.dataset_pb2 import (
     Label,
     MultipleTextSameLabels,
     SentenceClassificationBatch,
     TrainSet,
 )
 from nucliadb_protos.nodereader_pb2 import StreamRequest
 
-from nucliadb.ingest.orm.node import Node
+from nucliadb.common.cluster.base import AbstractIndexNode
 from nucliadb.ingest.orm.resource import KB_REVERSE
 from nucliadb.train import logger
-from nucliadb.train.generators.utils import get_resource_from_cache
+from nucliadb.train.generators.utils import batchify, get_resource_from_cache_or_db
 
 
-async def get_sentences(kbid: str, result: str) -> List[str]:
+def sentence_classification_batch_generator(
+    kbid: str,
+    trainset: TrainSet,
+    node: AbstractIndexNode,
+    shard_replica_id: str,
+) -> AsyncGenerator[SentenceClassificationBatch, None]:
+    if len(trainset.filter.labels) == 0:
+        raise HTTPException(
+            status_code=422,
+            detail="Sentence Classification should be at least of 1 labelset",
+        )
+
+    generator = generate_sentence_classification_payloads(
+        kbid, trainset, node, shard_replica_id
+    )
+    batch_generator = batchify(
+        generator, trainset.batch_size, SentenceClassificationBatch
+    )
+    return batch_generator
+
+
+async def generate_sentence_classification_payloads(
+    kbid: str,
+    trainset: TrainSet,
+    node: AbstractIndexNode,
+    shard_replica_id: str,
+) -> AsyncGenerator[MultipleTextSameLabels, None]:
+    labelsets = []
+    # Query how many paragraphs has each label
+    request = StreamRequest()
+    request.shard_id.id = shard_replica_id
+    for label in trainset.filter.labels:
+        labelset = f"/l/{trainset.filter.labels[0]}"
+        labelsets.append(labelset)
+        request.filter.labels.append(labelset)
+
+    async for paragraph_item in node.stream_get_paragraphs(request):
+        text_labels: list[str] = []
+        for label in paragraph_item.labels:
+            for labelset in labelsets:
+                if label.startswith(labelset):
+                    text_labels.append(label)
+
+        tl = MultipleTextSameLabels()
+        sentences_text = await get_sentences(kbid, paragraph_item.id)
+
+        if len(sentences_text) == 0:
+            continue
+        for sentence_text in sentences_text:
+            tl.text.append(sentence_text)
+        if len(tl.text):
+            for label in text_labels:
+                _, _, labelset, label_title = label.split("/")
+                tl.labels.append(Label(labelset=labelset, label=label_title))
+
+        yield tl
+
+
+async def get_sentences(kbid: str, result: str) -> list[str]:
     if result.count("/") == 4:
         rid, field_type, field, split_str, _ = result.split("/")
         split = int(split_str)
     else:
         rid, field_type, field, _ = result.split("/")
         split = None
 
-    orm_resource = await get_resource_from_cache(kbid, rid)
+    orm_resource = await get_resource_from_cache_or_db(kbid, rid)
 
     if orm_resource is None:
         logger.error(f"{rid} does not exist on DB")
         return []
 
     field_type_int = KB_REVERSE[field_type]
     field_obj = await orm_resource.get_field(field, field_type_int, load=False)
@@ -79,54 +138,7 @@
             else:
                 key = paragraph.key
             if key == result:
                 for sentence in paragraph.sentences:
                     splitted_text = text[sentence.start : sentence.end]
                     splitted_texts.append(splitted_text)
     return splitted_texts
-
-
-async def generate_sentence_classification_payloads(
-    kbid: str,
-    trainset: TrainSet,
-    node: Node,
-    shard_replica_id: str,
-) -> AsyncIterator[SentenceClassificationBatch]:
-    labelsets = []
-    # Query how many paragraphs has each label
-    request = StreamRequest()
-    request.shard_id.id = shard_replica_id
-    for label in trainset.filter.labels:
-        labelset = f"/l/{trainset.filter.labels[0]}"
-        labelsets.append(labelset)
-        request.filter.tags.append(labelset)
-    request.reload = True
-    batch = SentenceClassificationBatch()
-
-    async for paragraph_item in node.stream_get_paragraphs(request):
-        text_labels: List[str] = []
-        for label in paragraph_item.labels:
-            for labelset in labelsets:
-                if label.startswith(labelset):
-                    text_labels.append(label)
-
-        tl = MultipleTextSameLabels()
-        sentences_text = await get_sentences(kbid, paragraph_item.id)
-
-        if len(sentences_text) == 0:
-            continue
-        for sentence_text in sentences_text:
-            tl.text.append(sentence_text)
-        if len(tl.text):
-            for label in text_labels:
-                _, _, labelset, label_title = label.split("/")
-                tl.labels.append(Label(labelset=labelset, label=label_title))
-        batch.data.append(tl)
-
-        if len(batch.data) % 10 == 0:
-            print(len(batch.data))
-        if len(batch.data) == trainset.batch_size:
-            yield batch
-            batch = SentenceClassificationBatch()
-
-    if len(batch.data):
-        yield batch
```

## nucliadb/train/generators/token_classifier.py

```diff
@@ -15,61 +15,111 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
 from collections import OrderedDict
-from typing import AsyncIterator, Dict, List, Tuple, cast
+from typing import AsyncGenerator, cast
 
 from nucliadb_protos.dataset_pb2 import (
     TokenClassificationBatch,
     TokensClassification,
     TrainSet,
 )
 from nucliadb_protos.nodereader_pb2 import StreamFilter, StreamRequest
 
-from nucliadb.ingest.orm.node import Node
+from nucliadb.common.cluster.base import AbstractIndexNode
 from nucliadb.ingest.orm.resource import KB_REVERSE
 from nucliadb.train import logger
-from nucliadb.train.generators.utils import get_resource_from_cache
+from nucliadb.train.generators.utils import batchify, get_resource_from_cache_or_db
 
-NERS_DICT = Dict[str, Dict[str, List[Tuple[int, int]]]]
-POSITION_DICT = OrderedDict[Tuple[int, int], Tuple[str, str]]
+NERS_DICT = dict[str, dict[str, list[tuple[int, int]]]]
+POSITION_DICT = OrderedDict[tuple[int, int], tuple[str, str]]
 MAIN = "__main__"
 
 
+def token_classification_batch_generator(
+    kbid: str,
+    trainset: TrainSet,
+    node: AbstractIndexNode,
+    shard_replica_id: str,
+) -> AsyncGenerator[TokenClassificationBatch, None]:
+    generator = generate_token_classification_payloads(
+        kbid, trainset, node, shard_replica_id
+    )
+    batch_generator = batchify(generator, trainset.batch_size, TokenClassificationBatch)
+    return batch_generator
+
+
+async def generate_token_classification_payloads(
+    kbid: str,
+    trainset: TrainSet,
+    node: AbstractIndexNode,
+    shard_replica_id: str,
+) -> AsyncGenerator[TokensClassification, None]:
+    request = StreamRequest()
+    request.shard_id.id = shard_replica_id
+    for entitygroup in trainset.filter.labels:
+        request.filter.labels.append(f"/e/{entitygroup}")
+        request.filter.conjunction = StreamFilter.Conjunction.OR
+    async for field_item in node.stream_get_fields(request):
+        _, field_type, field = field_item.field.split("/")
+        (
+            split_text,
+            ordered_positions,
+            split_paragaphs,
+        ) = await get_field_text(
+            kbid,
+            field_item.uuid,
+            field,
+            field_type,
+            cast(list[str], trainset.filter.labels),
+        )
+        for split, text in split_text.items():
+            ners: POSITION_DICT = ordered_positions.get(split, OrderedDict())
+            paragraphs = split_paragaphs.get(split, [])
+
+            for segments in process_entities(text, ners, paragraphs):
+                tc = TokensClassification()
+                for segment in segments:
+                    tc.token.append(segment[0])
+                    tc.label.append(segment[1])
+
+                yield tc
+
+
 async def get_field_text(
-    kbid: str, rid: str, field: str, field_type: str, valid_entity_groups: List[str]
-) -> Tuple[Dict[str, str], Dict[str, POSITION_DICT], Dict[str, List[Tuple[int, int]]]]:
-    orm_resource = await get_resource_from_cache(kbid, rid)
+    kbid: str, rid: str, field: str, field_type: str, valid_entity_groups: list[str]
+) -> tuple[dict[str, str], dict[str, POSITION_DICT], dict[str, list[tuple[int, int]]]]:
+    orm_resource = await get_resource_from_cache_or_db(kbid, rid)
 
     if orm_resource is None:
         logger.error(f"{rid} does not exist on DB")
         return {}, {}, {}
 
     field_type_int = KB_REVERSE[field_type]
     field_obj = await orm_resource.get_field(field, field_type_int, load=False)
     extracted_text = await field_obj.get_extracted_text()
     if extracted_text is None:
         logger.warning(
             f"{rid} {field} {field_type_int} extracted_text does not exist on DB"
         )
         return {}, {}, {}
 
-    split_text: Dict[str, str] = extracted_text.split_text
+    split_text: dict[str, str] = extracted_text.split_text
     split_text[MAIN] = extracted_text.text
 
-    split_ners: Dict[
-        str, NERS_DICT
-    ] = {}  # Dict of entity group , with entity and list of positions in field
+    split_ners: dict[str, NERS_DICT] = (
+        {}
+    )  # Dict of entity group , with entity and list of positions in field
     split_ners[MAIN] = {}
 
     basic_data = await orm_resource.get_basic()
-    invalid_tokens_split: Dict[str, List[Tuple[str, str, int, int]]] = {}
+    invalid_tokens_split: dict[str, list[tuple[str, str, int, int]]] = {}
     # Check user definition of entities
     if basic_data is not None:
         for userfieldmetadata in basic_data.fieldmetadata:
             if (
                 userfieldmetadata.field.field == field
                 and userfieldmetadata.field.field_type == field_type_int
             ):
@@ -135,27 +185,27 @@
                             (token.start, token.end)
                         )
                         if len(split_ners[split][token.klass][token.token]) == 0:
                             del split_ners[split][token.klass][token.token]
                         if len(split_ners[split][token.klass]) == 0:
                             del split_ners[split][token.klass]
 
-    ordered_positions: Dict[str, POSITION_DICT] = {}
+    ordered_positions: dict[str, POSITION_DICT] = {}
     for split, ners in split_ners.items():
-        split_positions: Dict[Tuple[int, int], Tuple[str, str]] = {}
+        split_positions: dict[tuple[int, int], tuple[str, str]] = {}
         for entity_group, entities in ners.items():
             for entity, positions in entities.items():
                 for position in positions:
                     split_positions[position] = (entity_group, entity)
 
         ordered_positions[split] = OrderedDict(
             sorted(split_positions.items(), key=lambda x: x[0])
         )
 
-    split_paragraphs: Dict[str, List[Tuple[int, int]]] = {}
+    split_paragraphs: dict[str, list[tuple[int, int]]] = {}
     if field_metadata is not None:
         split_paragraphs[MAIN] = sorted(
             [(p.start, p.end) for p in field_metadata.metadata.paragraphs],
             key=lambda x: x[0],
         )
         for split, metadata in field_metadata.split_metadata.items():
             split_paragraphs[split] = sorted(
@@ -195,61 +245,17 @@
         field_text = second_part
 
     for part in field_text.split():
         segments.append((part, "O"))
     return segments
 
 
-def process_entities(text: str, ners: POSITION_DICT, paragraphs: List[Tuple[int, int]]):
+def process_entities(text: str, ners: POSITION_DICT, paragraphs: list[tuple[int, int]]):
     if len(paragraphs) > 0:
         for paragraph in paragraphs:
             segments = compute_segments(
                 text[paragraph[0] : paragraph[1]], ners, paragraph[0], paragraph[1]
             )
             yield segments
     else:
         segments = compute_segments(text, ners, 0, len(text))
         yield segments
-
-
-async def generate_token_classification_payloads(
-    kbid: str,
-    trainset: TrainSet,
-    node: Node,
-    shard_replica_id: str,
-) -> AsyncIterator[TokenClassificationBatch]:
-    request = StreamRequest()
-    request.shard_id.id = shard_replica_id
-    for entitygroup in trainset.filter.labels:
-        request.filter.tags.append(f"/e/{entitygroup}")
-        request.filter.conjunction = StreamFilter.Conjunction.OR
-    request.reload = True
-    batch = TokenClassificationBatch()
-    async for field_item in node.stream_get_fields(request):
-        _, field_type, field = field_item.field.split("/")
-        (
-            split_text,
-            ordered_positions,
-            split_paragaphs,
-        ) = await get_field_text(
-            kbid,
-            field_item.uuid,
-            field,
-            field_type,
-            cast(List[str], trainset.filter.labels),
-        )
-        for split, text in split_text.items():
-            ners: POSITION_DICT = ordered_positions.get(split, OrderedDict())
-            paragraphs = split_paragaphs.get(split, [])
-
-            for segments in process_entities(text, ners, paragraphs):
-                tc = TokensClassification()
-                for segment in segments:
-                    tc.token.append(segment[0])
-                    tc.label.append(segment[1])
-                batch.data.append(tc)
-                if len(batch.data) == trainset.batch_size:
-                    yield batch
-                    batch = TokenClassificationBatch()
-
-    if len(batch.data):
-        yield batch
```

## nucliadb/train/generators/utils.py

```diff
@@ -15,41 +15,96 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
 from contextvars import ContextVar
-from typing import Dict, Optional
+from typing import Any, AsyncIterator, Optional
 
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox as KnowledgeBoxORM
+from nucliadb.ingest.orm.resource import KB_REVERSE
 from nucliadb.ingest.orm.resource import Resource as ResourceORM
-from nucliadb.ingest.txn_utils import get_transaction
-from nucliadb.train import SERVICE_NAME
+from nucliadb.middleware.transaction import get_read_only_transaction
+from nucliadb.train import SERVICE_NAME, logger
+from nucliadb.train.types import TrainBatchType
 from nucliadb_utils.utilities import get_storage
 
-rcache: ContextVar[Optional[Dict[str, ResourceORM]]] = ContextVar(
+rcache: ContextVar[Optional[dict[str, ResourceORM]]] = ContextVar(
     "rcache", default=None
 )
 
 
-def get_resource_cache(clear: bool = False) -> Dict[str, ResourceORM]:
-    value: Optional[Dict[str, ResourceORM]] = rcache.get()
+def get_resource_cache(clear: bool = False) -> dict[str, ResourceORM]:
+    value: Optional[dict[str, ResourceORM]] = rcache.get()
     if value is None or clear:
         value = {}
         rcache.set(value)
     return value
 
 
-async def get_resource_from_cache(kbid: str, uuid: str) -> Optional[ResourceORM]:
+async def get_resource_from_cache_or_db(kbid: str, uuid: str) -> Optional[ResourceORM]:
     resouce_cache = get_resource_cache()
     orm_resource: Optional[ResourceORM] = None
     if uuid not in resouce_cache:
-        transaction = await get_transaction()
+        transaction = await get_read_only_transaction()
         storage = await get_storage(service_name=SERVICE_NAME)
         kb = KnowledgeBoxORM(transaction, storage, kbid)
         orm_resource = await kb.get(uuid)
         if orm_resource is not None:
             resouce_cache[uuid] = orm_resource
     else:
         orm_resource = resouce_cache.get(uuid)
     return orm_resource
+
+
+async def get_paragraph(kbid: str, paragraph_id: str) -> str:
+    if paragraph_id.count("/") == 5:
+        rid, field_type, field, split_str, start_end = paragraph_id.split("/")
+        split = int(split_str)
+        start_str, end_str = start_end.split("-")
+    else:
+        rid, field_type, field, start_end = paragraph_id.split("/")
+        split = None
+        start_str, end_str = start_end.split("-")
+    start = int(start_str)
+    end = int(end_str)
+
+    orm_resource = await get_resource_from_cache_or_db(kbid, rid)
+
+    if orm_resource is None:
+        logger.error(f"{rid} does not exist on DB")
+        return ""
+
+    field_type_int = KB_REVERSE[field_type]
+    field_obj = await orm_resource.get_field(field, field_type_int, load=False)
+    extracted_text = await field_obj.get_extracted_text()
+    if extracted_text is None:
+        logger.warning(
+            f"{rid} {field} {field_type_int} extracted_text does not exist on DB"
+        )
+        return ""
+
+    if split is not None:
+        text = extracted_text.split_text[split]
+        splitted_text = text[start:end]
+    else:
+        splitted_text = extracted_text.text[start:end]
+
+    return splitted_text
+
+
+async def batchify(
+    producer: AsyncIterator[Any], size: int, batch_klass: TrainBatchType
+):
+    # NOTE: we are supposing all protobuffers have a data field
+    batch = []
+    async for item in producer:
+        batch.append(item)
+        if len(batch) == size:
+            batch_pb = batch_klass(data=batch)
+            yield batch_pb
+            batch = []
+
+    if len(batch):
+        batch_pb = batch_klass(data=batch)
+        yield batch_pb
```

## nucliadb/train/tests/conftest.py

```diff
@@ -19,10 +19,11 @@
 #
 pytest_plugins = [
     "pytest_docker_fixtures",
     "nucliadb.ingest.tests.fixtures",
     "nucliadb.tests.fixtures",
     "nucliadb.train.tests.fixtures",
     "nucliadb_utils.tests.nats",
+    "nucliadb_utils.tests.conftest",
     "nucliadb_utils.tests.gcs",
     "nucliadb_utils.tests.s3",
 ]
```

## nucliadb/train/tests/fixtures.py

```diff
@@ -30,40 +30,98 @@
     FieldComputedMetadataWrapper,
     FieldID,
     FieldType,
     Paragraph,
     Position,
     Sentence,
 )
-from nucliadb_protos.writer_pb2 import BrokerMessage
+from nucliadb_protos.writer_pb2 import (
+    BrokerMessage,
+    SetEntitiesRequest,
+    SetLabelsRequest,
+)
+from nucliadb_protos.writer_pb2_grpc import WriterStub
 
 from nucliadb.ingest.orm.entities import EntitiesManager
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
 from nucliadb.ingest.orm.processor import Processor
 from nucliadb.ingest.orm.resource import KB_RESOURCE_SLUG_BASE
-from nucliadb.settings import Settings
-from nucliadb.train.utils import start_nodes_manager, stop_nodes_manager
+from nucliadb.standalone.settings import Settings
+from nucliadb.train.utils import start_shard_manager, stop_shard_manager
 from nucliadb_utils.tests import free_port
-from nucliadb_utils.utilities import (
-    Utility,
-    clear_global_cache,
-    get_storage,
-    set_utility,
-)
+from nucliadb_utils.utilities import clear_global_cache, get_storage
 
 
 @pytest.fixture(scope="function")
 async def train_rest_api(nucliadb: Settings):  # type: ignore
     async with aiohttp.ClientSession(
         headers={"X-NUCLIADB-ROLES": "READER"},
         base_url=f"http://localhost:{nucliadb.http_port}",
     ) as client:
         yield client
 
 
+@pytest.fixture(scope="function")
+async def writer_rest_api(nucliadb: Settings):  # type: ignore
+    async with aiohttp.ClientSession(
+        headers={"X-NUCLIADB-ROLES": "WRITER"},
+        base_url=f"http://localhost:{nucliadb.http_port}",
+    ) as client:
+        yield client
+
+
+@pytest.fixture(scope="function")
+async def knowledgebox_with_labels(nucliadb_grpc: WriterStub, knowledgebox: str):
+    slr = SetLabelsRequest()
+    slr.kb.uuid = knowledgebox
+    slr.id = "labelset_paragraphs"
+    slr.labelset.kind.append(LabelSet.LabelSetKind.PARAGRAPHS)
+    l1 = Label(title="label_machine")
+    l2 = Label(title="label_user")
+    slr.labelset.labels.append(l1)
+    slr.labelset.labels.append(l2)
+    await nucliadb_grpc.SetLabels(slr)  # type: ignore
+
+    slr = SetLabelsRequest()
+    slr.kb.uuid = knowledgebox
+    slr.id = "labelset_resources"
+    slr.labelset.kind.append(LabelSet.LabelSetKind.RESOURCES)
+    l1 = Label(title="label_machine")
+    l2 = Label(title="label_user")
+    slr.labelset.labels.append(l1)
+    slr.labelset.labels.append(l2)
+    await nucliadb_grpc.SetLabels(slr)  # type: ignore
+
+    yield knowledgebox
+
+
+@pytest.fixture(scope="function")
+async def knowledgebox_with_entities(nucliadb_grpc: WriterStub, knowledgebox: str):
+    ser = SetEntitiesRequest()
+    ser.kb.uuid = knowledgebox
+    ser.group = "PERSON"
+    ser.entities.title = "PERSON"
+    ser.entities.entities["Ramon"].value = "Ramon"
+    ser.entities.entities["Eudald Camprubi"].value = "Eudald Camprubi"
+    ser.entities.entities["Carmen Iniesta"].value = "Carmen Iniesta"
+    ser.entities.entities["el Super Fran"].value = "el Super Fran"
+    await nucliadb_grpc.SetEntities(ser)  # type: ignore
+
+    ser = SetEntitiesRequest()
+    ser.kb.uuid = knowledgebox
+    ser.group = "ORG"
+    ser.entities.title = "ORG"
+    ser.entities.entities["Nuclia"].value = "Nuclia"
+    ser.entities.entities["Debian"].value = "Debian"
+    ser.entities.entities["Generalitat de Catalunya"].value = "Generalitat de Catalunya"
+    await nucliadb_grpc.SetEntities(ser)  # type: ignore
+
+    yield knowledgebox
+
+
 def broker_simple_resource(knowledgebox: str, number: int) -> BrokerMessage:
     rid = str(uuid.uuid4())
     message1: BrokerMessage = BrokerMessage(
         kbid=knowledgebox,
         uuid=rid,
         slug=str(number),
         type=BrokerMessage.AUTOCOMMIT,
@@ -75,17 +133,17 @@
     message1.basic.summary = "Summary of document"
     message1.basic.thumbnail = "doc"
     message1.basic.layout = "default"
     message1.basic.metadata.useful = True
     message1.basic.metadata.language = "es"
     message1.basic.created.FromDatetime(datetime.utcnow())
     message1.basic.modified.FromDatetime(datetime.utcnow())
-    message1.texts[
-        "field1"
-    ].body = "My lovely field with some information from Barcelona. This will be the good field. \n\n And then we will go Manresa."  # noqa
+    message1.texts["field1"].body = (
+        "My lovely field with some information from Barcelona. This will be the good field. \n\n And then we will go Manresa."  # noqa
+    )
     message1.source = BrokerMessage.MessageSource.WRITER
     return message1
 
 
 def broker_processed_resource(knowledgebox, number, rid) -> BrokerMessage:
     message2: BrokerMessage = BrokerMessage(
         kbid=knowledgebox,
@@ -189,17 +247,17 @@
 
         message = broker_processed_resource(knowledgebox_ingest, i, message.uuid)
         await processor.process(message=message, seqid=-1, transaction_check=False)
         # Give processed data some time to reach the node
 
     from time import time
 
-    from nucliadb.ingest.utils import get_driver
+    from nucliadb.common.maindb.utils import get_driver
 
-    driver = await get_driver()
+    driver = get_driver()
 
     t0 = time()
 
     while time() - t0 < 30:  # wait max 30 seconds for it
         txn = await driver.begin()
         count = 0
         async for key in txn.keys(
@@ -251,31 +309,30 @@
     old_gcs_bucket = storage_settings.gcs_bucket
     old_grpc_port = settings.grpc_port
 
     storage_settings.gcs_endpoint_url = gcs
     storage_settings.file_backend = FileBackendConfig.GCS
     storage_settings.gcs_bucket = "test_{kbid}"
     settings.grpc_port = free_port()
-    set_utility(Utility.CACHE, cache)
     yield
     storage_settings.file_backend = old_file_backend
     storage_settings.gcs_endpoint_url = old_gcs_endpoint_url
     storage_settings.gcs_bucket = old_gcs_bucket
     settings.grpc_port = old_grpc_port
 
 
 @pytest.fixture(scope="function")
-async def train_api(test_settings_train: None, local_files, event_loop):  # type: ignore
+async def train_api(test_settings_train: None, local_files):  # type: ignore
     from nucliadb.train.utils import start_train_grpc, stop_train_grpc
 
-    await start_nodes_manager()
+    await start_shard_manager()
     await start_train_grpc("testing_train")
     yield
     await stop_train_grpc()
-    await stop_nodes_manager()
+    await stop_shard_manager()
 
 
 @pytest.fixture(scope="function")
 async def train_client(train_api):  # type: ignore
     from nucliadb_protos.train_pb2_grpc import TrainStub
 
     from nucliadb.train.settings import settings
```

## nucliadb/train/tests/test_field_classification.py

```diff
@@ -14,235 +14,109 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 import asyncio
-from typing import AsyncIterator
 
 import aiohttp
 import pytest
 from nucliadb_protos.dataset_pb2 import FieldClassificationBatch, TaskType, TrainSet
 from nucliadb_protos.knowledgebox_pb2 import Label, LabelSet
-from nucliadb_protos.resources_pb2 import (
-    Metadata,
-    ParagraphAnnotation,
-    UserFieldMetadata,
-)
-from nucliadb_protos.writer_pb2 import BrokerMessage, SetLabelsRequest
+from nucliadb_protos.writer_pb2 import SetLabelsRequest
 from nucliadb_protos.writer_pb2_grpc import WriterStub
 
 from nucliadb.tests.utils import inject_message
+from nucliadb.tests.utils.broker_messages import BrokerMessageBuilder
 from nucliadb.train import API_PREFIX
 from nucliadb.train.api.v1.router import KB_PREFIX
-
-
-async def get_field_classification_batch_from_response(
-    response: aiohttp.ClientResponse,
-) -> AsyncIterator[FieldClassificationBatch]:
-    while True:
-        header = await response.content.read(4)
-        if header == b"":
-            break
-        payload_size = int.from_bytes(header, byteorder="big", signed=False)
-        payload = await response.content.read(payload_size)
-        pcb = FieldClassificationBatch()
-        pcb.ParseFromString(payload)
-        assert pcb.data
-        yield pcb
-
-
-def broker_resource(knowledgebox: str) -> BrokerMessage:
-    import uuid
-    from datetime import datetime
-
-    from nucliadb_protos import resources_pb2 as rpb
-
-    rid = str(uuid.uuid4())
-    slug = f"{rid}slug1"
-
-    bm: BrokerMessage = BrokerMessage(
-        kbid=knowledgebox,
-        uuid=rid,
-        slug=slug,
-        type=BrokerMessage.AUTOCOMMIT,
-    )
-
-    bm.basic.icon = "text/plain"
-    bm.basic.title = "Title Resource"
-    bm.basic.summary = "Summary of document"
-    bm.basic.thumbnail = "doc"
-    bm.basic.layout = "default"
-    bm.basic.metadata.useful = True
-    bm.basic.metadata.status = Metadata.Status.PROCESSED
-    bm.basic.metadata.language = "es"
-    bm.basic.created.FromDatetime(datetime.now())
-    bm.basic.modified.FromDatetime(datetime.now())
-    bm.origin.source = rpb.Origin.Source.WEB
-
-    c4 = rpb.Classification()
-    c4.label = "label_user"
-    c4.labelset = "labelset_resources"
-
-    bm.basic.usermetadata.classifications.append(c4)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "My own text Ramon. This is great to be here. \n Where is my beer? Do you want to go shooping? This is a test!"  # noqa
-    etw.field.field = "file"
-    etw.field.field_type = rpb.FieldType.FILE
-    bm.extracted_text.append(etw)
-
-    c3 = rpb.Classification()
-    c3.label = "label_user"
-    c3.labelset = "labelset_paragraphs"
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/47-64"  # Designed to be the RID at indexing time
-    ufm = UserFieldMetadata()
-    ufm.paragraphs.append(pa)
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/0-45"  # Designed to be the RID at indexing time
-    ufm.paragraphs.append(pa)
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/65-93"  # Designed to be the RID at indexing time
-    ufm.paragraphs.append(pa)
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/94-109"  # Designed to be the RID at indexing time
-    ufm.paragraphs.append(pa)
-
-    ufm.field.field = "file"
-    ufm.field.field_type = rpb.FieldType.FILE
-    bm.basic.fieldmetadata.append(ufm)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "Summary of document"
-    etw.field.field = "summary"
-    etw.field.field_type = rpb.FieldType.GENERIC
-    bm.extracted_text.append(etw)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "Title Resource"
-    etw.field.field = "title"
-    etw.field.field_type = rpb.FieldType.GENERIC
-    bm.extracted_text.append(etw)
-
-    bm.files["file"].added.FromDatetime(datetime.now())
-    bm.files["file"].file.source = rpb.CloudFile.Source.EXTERNAL
-
-    c1 = rpb.Classification()
-    c1.label = "label_machine"
-    c1.labelset = "labelset_paragraphs"
-
-    c2 = rpb.Classification()
-    c2.label = "label_machine"
-    c2.labelset = "labelset_resources"
-
-    fcm = rpb.FieldComputedMetadataWrapper()
-    fcm.field.field = "file"
-    fcm.field.field_type = rpb.FieldType.FILE
-    p1 = rpb.Paragraph(
-        start=0,
-        end=45,
-    )
-    p1.classifications.append(c1)
-    p2 = rpb.Paragraph(
-        start=47,
-        end=64,
-    )
-    p2.classifications.append(c1)
-
-    p3 = rpb.Paragraph(
-        start=65,
-        end=93,
-    )
-    p3.classifications.append(c1)
-
-    p4 = rpb.Paragraph(
-        start=94,
-        end=109,
-    )
-    p4.classifications.append(c1)
-
-    fcm.metadata.metadata.paragraphs.append(p1)
-    fcm.metadata.metadata.paragraphs.append(p2)
-    fcm.metadata.metadata.paragraphs.append(p3)
-    fcm.metadata.metadata.paragraphs.append(p4)
-    fcm.metadata.metadata.last_index.FromDatetime(datetime.now())
-    fcm.metadata.metadata.last_understanding.FromDatetime(datetime.now())
-    fcm.metadata.metadata.last_extract.FromDatetime(datetime.now())
-    fcm.metadata.metadata.ner["Ramon"] = "PERSON"
-
-    fcm.metadata.metadata.classifications.append(c2)
-    bm.field_metadata.append(fcm)
-
-    ev = rpb.ExtractedVectorsWrapper()
-    ev.field.field = "file"
-    ev.field.field_type = rpb.FieldType.FILE
-
-    bm.source = BrokerMessage.MessageSource.WRITER
-    return bm
-
-
-async def inject_resource_with_field_labels(knowledgebox, writer):
-    bm = broker_resource(knowledgebox)
-    await inject_message(writer, bm)
-    return bm.uuid
+from nucliadb.train.tests.utils import get_batches_from_train_response_stream
 
 
 @pytest.mark.asyncio
+@pytest.mark.parametrize("knowledgebox", ["STABLE", "EXPERIMENTAL"], indirect=True)
 async def test_generator_field_classification(
-    train_rest_api: aiohttp.ClientSession, knowledgebox: str, nucliadb_grpc: WriterStub
+    train_rest_api: aiohttp.ClientSession,
+    knowledgebox_with_labels: str,
 ):
+    kbid = knowledgebox_with_labels
+
+    async with train_rest_api.get(
+        f"/{API_PREFIX}/v1/{KB_PREFIX}/{kbid}/trainset"
+    ) as partitions:
+        assert partitions.status == 200
+        data = await partitions.json()
+        assert len(data["partitions"]) == 1
+        partition_id = data["partitions"][0]
+
+    trainset = TrainSet()
+    trainset.type = TaskType.FIELD_CLASSIFICATION
+    trainset.batch_size = 2
+
+    tests = [
+        (["labelset_resources"], 2, 4),
+        # 2 fields
+        (["labelset_resources/label_user"], 1, 2),
+        # unused label
+        (["labelset_resources/label_alien"], 0, 0),
+        # non existent
+        (["nonexistent_labelset"], 0, 0),
+    ]
+
+    for labels, expected_batches, expected_total in tests:
+        trainset.filter.ClearField("labels")
+        trainset.filter.labels.extend(labels)  # type: ignore
+
+        async with train_rest_api.post(
+            f"/{API_PREFIX}/v1/{KB_PREFIX}/{kbid}/trainset/{partition_id}",
+            data=trainset.SerializeToString(),
+        ) as response:
+            assert response.status == 200
+            batches = []
+            total = 0
+            async for batch in get_batches_from_train_response_stream(
+                response, FieldClassificationBatch
+            ):
+                batches.append(batch)
+                total += len(batch.data)
+            assert len(batches) == expected_batches
+            assert total == expected_total
+
+
+@pytest.fixture(scope="function")
+@pytest.mark.asyncio
+async def knowledgebox_with_labels(nucliadb_grpc: WriterStub, knowledgebox: str):
     slr = SetLabelsRequest()
     slr.kb.uuid = knowledgebox
     slr.id = "labelset_paragraphs"
     slr.labelset.kind.append(LabelSet.LabelSetKind.PARAGRAPHS)
-    l1 = Label(title="label_machine")
-    l2 = Label(title="label_user")
-    slr.labelset.labels.append(l1)
-    slr.labelset.labels.append(l2)
+    slr.labelset.labels.append(Label(title="label_machine"))
+    slr.labelset.labels.append(Label(title="label_user"))
+    slr.labelset.labels.append(Label(title="label_alien"))
     await nucliadb_grpc.SetLabels(slr)  # type: ignore
 
     slr = SetLabelsRequest()
     slr.kb.uuid = knowledgebox
     slr.id = "labelset_resources"
     slr.labelset.kind.append(LabelSet.LabelSetKind.RESOURCES)
-    l1 = Label(title="label_machine")
-    l2 = Label(title="label_user")
-    slr.labelset.labels.append(l1)
-    slr.labelset.labels.append(l2)
+    slr.labelset.labels.append(Label(title="label_machine"))
+    slr.labelset.labels.append(Label(title="label_user"))
+    slr.labelset.labels.append(Label(title="label_alien"))
     await nucliadb_grpc.SetLabels(slr)  # type: ignore
 
-    await inject_resource_with_field_labels(knowledgebox, nucliadb_grpc)
-    await asyncio.sleep(0.1)
-    async with train_rest_api.get(
-        f"/{API_PREFIX}/v1/{KB_PREFIX}/{knowledgebox}/trainset"
-    ) as partitions:
-        assert partitions.status == 200
-        data = await partitions.json()
-        assert len(data["partitions"]) == 1
-        partition_id = data["partitions"][0]
+    bmb = BrokerMessageBuilder(kbid=knowledgebox)
+    bmb.with_title("First resource")
+    bmb.with_summary("First summary")
+    bmb.with_resource_labels("labelset_resources", ["label_user"])
+    bm = bmb.build()
+    await inject_message(nucliadb_grpc, bm)
+
+    bmb = BrokerMessageBuilder(kbid=knowledgebox)
+    bmb.with_title("Second resource")
+    bmb.with_summary("Second summary")
+    bmb.with_resource_labels("labelset_resources", ["label_machine"])
+    bm = bmb.build()
+    await inject_message(nucliadb_grpc, bm)
 
-    trainset = TrainSet()
-    trainset.type = TaskType.FIELD_CLASSIFICATION
-    trainset.batch_size = 2
-    trainset.filter.labels.append("labelset_resources")
-    async with train_rest_api.post(
-        f"/{API_PREFIX}/v1/{KB_PREFIX}/{knowledgebox}/trainset/{partition_id}",
-        data=trainset.SerializeToString(),
-    ) as response:
-        assert response.status == 200
-        batches = []
-        total = 0
-        async for batch in get_field_classification_batch_from_response(response):
-            batches.append(batch)
-            total += len(batch.data)
-        assert len(batches) == 2
-        assert total == 3
+    await asyncio.sleep(0.1)
+    yield knowledgebox
```

## nucliadb/train/tests/test_get_info.py

```diff
@@ -13,21 +13,29 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
+import sys
+
 import pytest
 from aioresponses import aioresponses
 from nucliadb_protos.train_pb2 import GetInfoRequest, TrainInfo
 from nucliadb_protos.train_pb2_grpc import TrainStub
 
+VERSION = sys.version_info
+PY_GEQ_3_11 = VERSION.major > 3 or VERSION.major == 3 and VERSION.minor >= 11
+
 
 @pytest.mark.asyncio
+@pytest.mark.skipif(
+    PY_GEQ_3_11, reason="aioresponses not compatible with python 3.11 yet"
+)
 async def test_get_info(
     train_client: TrainStub, knowledgebox_ingest: str, test_pagination_resources
 ) -> None:
     req = GetInfoRequest()
     req.kb.uuid = knowledgebox_ingest
 
     with aioresponses() as m:
```

## nucliadb/train/tests/test_get_ontology_count.py

```diff
@@ -13,21 +13,29 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import sys
+
 import pytest
 from aioresponses import aioresponses
 from nucliadb_protos.train_pb2 import GetLabelsetsCountRequest, LabelsetsCount
 from nucliadb_protos.train_pb2_grpc import TrainStub
 
+VERSION = sys.version_info
+PY_GEQ_3_11 = VERSION.major > 3 or VERSION.major == 3 and VERSION.minor >= 11
+
 
 @pytest.mark.asyncio
+@pytest.mark.skipif(
+    PY_GEQ_3_11, reason="aioresponses not compatible with python 3.11 yet"
+)
 async def test_get_ontology_count(
     train_client: TrainStub, knowledgebox_ingest: str, test_pagination_resources
 ) -> None:
     req = GetLabelsetsCountRequest()
     req.kb.uuid = knowledgebox_ingest
 
     with aioresponses() as m:
```

## nucliadb/train/tests/test_list_sentences.py

```diff
@@ -37,14 +37,15 @@
     async for _ in train_client.GetSentences(req):  # type: ignore
         count += 1
 
     assert count == 40
 
 
 @pytest.mark.asyncio
+@pytest.mark.parametrize("knowledgebox", ["STABLE", "EXPERIMENTAL"], indirect=True)
 async def test_list_sentences_shows_ners_with_positions(
     train_client: TrainStub, knowledgebox: str, test_pagination_resources
 ) -> None:
     req = GetSentencesRequest()
     req.kb.uuid = knowledgebox
     req.metadata.entities = True
     async for sentence in train_client.GetSentences(req):  # type: ignore
```

## nucliadb/train/tests/test_paragraph_classification.py

```diff
@@ -14,234 +14,110 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 import asyncio
-from typing import AsyncIterator
+import uuid
 
 import aiohttp
 import pytest
 from nucliadb_protos.dataset_pb2 import ParagraphClassificationBatch, TaskType, TrainSet
-from nucliadb_protos.knowledgebox_pb2 import Label, LabelSet
-from nucliadb_protos.resources_pb2 import (
-    Metadata,
-    ParagraphAnnotation,
-    UserFieldMetadata,
-)
-from nucliadb_protos.writer_pb2 import BrokerMessage, SetLabelsRequest
+from nucliadb_protos.writer_pb2 import BrokerMessage
 from nucliadb_protos.writer_pb2_grpc import WriterStub
 
 from nucliadb.tests.utils import inject_message
+from nucliadb.tests.utils.broker_messages import BrokerMessageBuilder, FieldBuilder
 from nucliadb.train import API_PREFIX
 from nucliadb.train.api.v1.router import KB_PREFIX
-
-
-async def get_paragraph_classification_batch_from_response(
-    response: aiohttp.ClientResponse,
-) -> AsyncIterator[ParagraphClassificationBatch]:
-    while True:
-        header = await response.content.read(4)
-        if header == b"":
-            break
-        payload_size = int.from_bytes(header, byteorder="big", signed=False)
-        payload = await response.content.read(payload_size)
-        pcb = ParagraphClassificationBatch()
-        pcb.ParseFromString(payload)
-        assert pcb.data
-        yield pcb
-
-
-def broker_resource(knowledgebox: str) -> BrokerMessage:
-    import uuid
-    from datetime import datetime
-
-    from nucliadb_protos import resources_pb2 as rpb
-
-    rid = str(uuid.uuid4())
-    slug = f"{rid}slug1"
-
-    bm: BrokerMessage = BrokerMessage(
-        kbid=knowledgebox,
-        uuid=rid,
-        slug=slug,
-        type=BrokerMessage.AUTOCOMMIT,
-    )
-
-    bm.basic.icon = "text/plain"
-    bm.basic.title = "Title Resource"
-    bm.basic.summary = "Summary of document"
-    bm.basic.thumbnail = "doc"
-    bm.basic.layout = "default"
-    bm.basic.metadata.useful = True
-    bm.basic.metadata.status = Metadata.Status.PROCESSED
-    bm.basic.metadata.language = "es"
-    bm.basic.created.FromDatetime(datetime.now())
-    bm.basic.modified.FromDatetime(datetime.now())
-    bm.origin.source = rpb.Origin.Source.WEB
-
-    c4 = rpb.Classification()
-    c4.label = "label_user"
-    c4.labelset = "labelset_resources"
-
-    bm.basic.usermetadata.classifications.append(c4)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "My own text Ramon. This is great to be here. \n Where is my beer? Do you want to go shooping? This is a test!"  # noqa
-    etw.field.field = "file"
-    etw.field.field_type = rpb.FieldType.FILE
-    bm.extracted_text.append(etw)
-
-    c3 = rpb.Classification()
-    c3.label = "label_user"
-    c3.labelset = "labelset_paragraphs"
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/47-64"  # Designed to be the RID at indexing time
-    ufm = UserFieldMetadata()
-    ufm.paragraphs.append(pa)
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/0-45"  # Designed to be the RID at indexing time
-    ufm.paragraphs.append(pa)
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/65-93"  # Designed to be the RID at indexing time
-    ufm.paragraphs.append(pa)
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/93-109"  # Designed to be the RID at indexing time
-    ufm.paragraphs.append(pa)
-
-    ufm.field.field = "file"
-    ufm.field.field_type = rpb.FieldType.FILE
-    bm.basic.fieldmetadata.append(ufm)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "Summary of document"
-    etw.field.field = "summary"
-    etw.field.field_type = rpb.FieldType.GENERIC
-    bm.extracted_text.append(etw)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "Title Resource"
-    etw.field.field = "title"
-    etw.field.field_type = rpb.FieldType.GENERIC
-    bm.extracted_text.append(etw)
-
-    bm.files["file"].added.FromDatetime(datetime.now())
-    bm.files["file"].file.source = rpb.CloudFile.Source.EXTERNAL
-
-    c1 = rpb.Classification()
-    c1.label = "label_machine"
-    c1.labelset = "labelset_paragraphs"
-
-    c2 = rpb.Classification()
-    c2.label = "label_machine"
-    c2.labelset = "labelset_resources"
-
-    fcm = rpb.FieldComputedMetadataWrapper()
-    fcm.field.field = "file"
-    fcm.field.field_type = rpb.FieldType.FILE
-    p1 = rpb.Paragraph(
-        start=0,
-        end=45,
-    )
-    p1.classifications.append(c1)
-    p2 = rpb.Paragraph(
-        start=47,
-        end=64,
-    )
-    p2.classifications.append(c1)
-
-    p3 = rpb.Paragraph(
-        start=65,
-        end=93,
-    )
-    p3.classifications.append(c1)
-
-    p4 = rpb.Paragraph(
-        start=93,
-        end=109,
-    )
-    p4.classifications.append(c1)
-
-    fcm.metadata.metadata.paragraphs.append(p1)
-    fcm.metadata.metadata.paragraphs.append(p2)
-    fcm.metadata.metadata.paragraphs.append(p3)
-    fcm.metadata.metadata.paragraphs.append(p4)
-    fcm.metadata.metadata.last_index.FromDatetime(datetime.now())
-    fcm.metadata.metadata.last_understanding.FromDatetime(datetime.now())
-    fcm.metadata.metadata.last_extract.FromDatetime(datetime.now())
-    fcm.metadata.metadata.ner["Ramon"] = "PERSON"
-
-    fcm.metadata.metadata.classifications.append(c2)
-    bm.field_metadata.append(fcm)
-
-    ev = rpb.ExtractedVectorsWrapper()
-    ev.field.field = "file"
-    ev.field.field_type = rpb.FieldType.FILE
-
-    bm.source = BrokerMessage.MessageSource.WRITER
-    return bm
-
-
-async def inject_resource_with_paragraph_labels(knowledgebox, writer):
-    bm = broker_resource(knowledgebox)
-    await inject_message(writer, bm)
-    return bm.uuid
+from nucliadb.train.tests.utils import get_batches_from_train_response_stream
+from nucliadb_protos import resources_pb2 as rpb
 
 
 @pytest.mark.asyncio
+@pytest.mark.parametrize("knowledgebox", ["STABLE", "EXPERIMENTAL"], indirect=True)
 async def test_generator_paragraph_classification(
-    train_rest_api: aiohttp.ClientSession, knowledgebox: str, nucliadb_grpc: WriterStub
+    train_rest_api: aiohttp.ClientSession,
+    nucliadb_grpc: WriterStub,
+    knowledgebox_with_labels: str,
 ):
-    slr = SetLabelsRequest()
-    slr.kb.uuid = knowledgebox
-    slr.id = "labelset_paragraphs"
-    slr.labelset.kind.append(LabelSet.LabelSetKind.PARAGRAPHS)
-    l1 = Label(title="label_machine")
-    l2 = Label(title="label_user")
-    slr.labelset.labels.append(l1)
-    slr.labelset.labels.append(l2)
-    await nucliadb_grpc.SetLabels(slr)  # type: ignore
-
-    slr = SetLabelsRequest()
-    slr.kb.uuid = knowledgebox
-    slr.id = "labelset_resources"
-    slr.labelset.kind.append(LabelSet.LabelSetKind.RESOURCES)
-    l1 = Label(title="label_machine")
-    l2 = Label(title="label_user")
-    slr.labelset.labels.append(l1)
-    slr.labelset.labels.append(l2)
-    await nucliadb_grpc.SetLabels(slr)  # type: ignore
+    kbid = knowledgebox_with_labels
+
+    await inject_resource_with_paragraph_classification(kbid, nucliadb_grpc)
 
-    await inject_resource_with_paragraph_labels(knowledgebox, nucliadb_grpc)
-    await asyncio.sleep(0.1)
     async with train_rest_api.get(
-        f"/{API_PREFIX}/v1/{KB_PREFIX}/{knowledgebox}/trainset"
+        f"/{API_PREFIX}/v1/{KB_PREFIX}/{kbid}/trainset"
     ) as partitions:
         assert partitions.status == 200
         data = await partitions.json()
         assert len(data["partitions"]) == 1
         partition_id = data["partitions"][0]
 
     trainset = TrainSet()
     trainset.type = TaskType.PARAGRAPH_CLASSIFICATION
     trainset.batch_size = 2
     trainset.filter.labels.append("labelset_paragraphs")
 
     async with train_rest_api.post(
-        f"/{API_PREFIX}/v1/{KB_PREFIX}/{knowledgebox}/trainset/{partition_id}",
+        f"/{API_PREFIX}/v1/{KB_PREFIX}/{kbid}/trainset/{partition_id}",
         data=trainset.SerializeToString(),
     ) as response:
         assert response.status == 200
         batches = []
-        async for batch in get_paragraph_classification_batch_from_response(response):
+        async for batch in get_batches_from_train_response_stream(
+            response, ParagraphClassificationBatch
+        ):
             batches.append(batch)
             assert len(batch.data) == 2
         assert len(batches) == 2
+
+
+async def inject_resource_with_paragraph_classification(knowledgebox, writer):
+    bm = broker_resource(knowledgebox)
+    await inject_message(writer, bm)
+    await asyncio.sleep(0.1)
+    return bm.uuid
+
+
+def broker_resource(knowledgebox: str) -> BrokerMessage:
+    rid = str(uuid.uuid4())
+    bmb = BrokerMessageBuilder(kbid=knowledgebox, rid=rid)
+    bmb.with_title("Title Resource")
+    bmb.with_summary("Summary of document")
+    bmb.with_resource_labels("labelset_resources", ["label_user"])
+
+    file_field = FieldBuilder("file", rpb.FieldType.FILE)
+    file_field.with_extracted_text(
+        "My own text Ramon. This is great to be here. \n Where is my beer? Do you want to go shooping? This is a test!"  # noqa
+    )
+
+    labelset = "labelset_paragraphs"
+    labels = ["label_user"]
+    file_field.with_user_paragraph_labels(f"{rid}/f/file/0-45", labelset, labels)
+    file_field.with_user_paragraph_labels(f"{rid}/f/file/47-64", labelset, labels)
+    file_field.with_user_paragraph_labels(f"{rid}/f/file/65-93", labelset, labels)
+    file_field.with_user_paragraph_labels(f"{rid}/f/file/93-109", labelset, labels)
+
+    classification = rpb.Classification(
+        labelset="labelset_paragraphs", label="label_machine"
+    )
+    file_field.with_extracted_paragraph_metadata(
+        rpb.Paragraph(start=0, end=45, classifications=[classification])
+    )
+    file_field.with_extracted_paragraph_metadata(
+        rpb.Paragraph(start=47, end=64, classifications=[classification])
+    )
+    file_field.with_extracted_paragraph_metadata(
+        rpb.Paragraph(start=65, end=93, classifications=[classification])
+    )
+    file_field.with_extracted_paragraph_metadata(
+        rpb.Paragraph(start=93, end=109, classifications=[classification])
+    )
+
+    file_field.with_extracted_labels("labelset_resources", ["label_machine"])
+
+    bmb.add_field_builder(file_field)
+
+    bm = bmb.build()
+
+    return bm
```

## nucliadb/train/tests/test_sentence_classification.py

```diff
@@ -14,241 +14,130 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 import asyncio
-from typing import AsyncIterator
+import uuid
 
 import aiohttp
 import pytest
 from nucliadb_protos.dataset_pb2 import SentenceClassificationBatch, TaskType, TrainSet
-from nucliadb_protos.knowledgebox_pb2 import Label, LabelSet
-from nucliadb_protos.resources_pb2 import (
-    Metadata,
-    ParagraphAnnotation,
-    UserFieldMetadata,
-)
-from nucliadb_protos.writer_pb2 import BrokerMessage, SetLabelsRequest
+from nucliadb_protos.writer_pb2 import BrokerMessage
 from nucliadb_protos.writer_pb2_grpc import WriterStub
 
 from nucliadb.tests.utils import inject_message
+from nucliadb.tests.utils.broker_messages import BrokerMessageBuilder, FieldBuilder
 from nucliadb.train import API_PREFIX
 from nucliadb.train.api.v1.router import KB_PREFIX
-
-
-async def get_sentence_classification_batch_from_response(
-    response: aiohttp.ClientResponse,
-) -> AsyncIterator[SentenceClassificationBatch]:
-    while True:
-        header = await response.content.read(4)
-        if header == b"":
-            break
-        payload_size = int.from_bytes(header, byteorder="big", signed=False)
-        payload = await response.content.read(payload_size)
-        pcb = SentenceClassificationBatch()
-        pcb.ParseFromString(payload)
-        assert pcb.data
-        yield pcb
-
-
-def broker_resource(knowledgebox: str) -> BrokerMessage:
-    import uuid
-    from datetime import datetime
-
-    from nucliadb_protos import resources_pb2 as rpb
-
-    rid = str(uuid.uuid4())
-    slug = f"{rid}slug1"
-
-    bm: BrokerMessage = BrokerMessage(
-        kbid=knowledgebox,
-        uuid=rid,
-        slug=slug,
-        type=BrokerMessage.AUTOCOMMIT,
-    )
-
-    bm.basic.icon = "text/plain"
-    bm.basic.title = "Title Resource"
-    bm.basic.summary = "Summary of document"
-    bm.basic.thumbnail = "doc"
-    bm.basic.layout = "default"
-    bm.basic.metadata.useful = True
-    bm.basic.metadata.status = Metadata.Status.PROCESSED
-    bm.basic.metadata.language = "es"
-    bm.basic.created.FromDatetime(datetime.now())
-    bm.basic.modified.FromDatetime(datetime.now())
-    bm.origin.source = rpb.Origin.Source.WEB
-
-    c4 = rpb.Classification()
-    c4.label = "label_user"
-    c4.labelset = "labelset_resources"
-
-    bm.basic.usermetadata.classifications.append(c4)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "My own text Ramon. This is great to be here. \n Where is my beer? Do you want to go shooping? This is a test!"  # noqa
-    etw.field.field = "file"
-    etw.field.field_type = rpb.FieldType.FILE
-    bm.extracted_text.append(etw)
-
-    c3 = rpb.Classification()
-    c3.label = "label_user"
-    c3.labelset = "labelset_paragraphs"
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/47-64"  # Designed to be the RID at indexing time
-    ufm = UserFieldMetadata()
-    ufm.paragraphs.append(pa)
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/0-45"  # Designed to be the RID at indexing time
-    ufm.paragraphs.append(pa)
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/65-93"  # Designed to be the RID at indexing time
-    ufm.paragraphs.append(pa)
-
-    pa = ParagraphAnnotation()
-    pa.classifications.append(c3)
-    pa.key = "N_RID/f/file/94-109"  # Designed to be the RID at indexing time
-    ufm.paragraphs.append(pa)
-
-    ufm.field.field = "file"
-    ufm.field.field_type = rpb.FieldType.FILE
-    bm.basic.fieldmetadata.append(ufm)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "Summary of document"
-    etw.field.field = "summary"
-    etw.field.field_type = rpb.FieldType.GENERIC
-    bm.extracted_text.append(etw)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "Title Resource"
-    etw.field.field = "title"
-    etw.field.field_type = rpb.FieldType.GENERIC
-    bm.extracted_text.append(etw)
-
-    bm.files["file"].added.FromDatetime(datetime.now())
-    bm.files["file"].file.source = rpb.CloudFile.Source.EXTERNAL
-
-    c1 = rpb.Classification()
-    c1.label = "label_machine"
-    c1.labelset = "labelset_paragraphs"
-
-    c2 = rpb.Classification()
-    c2.label = "label_machine"
-    c2.labelset = "labelset_resources"
-
-    fcm = rpb.FieldComputedMetadataWrapper()
-    fcm.field.field = "file"
-    fcm.field.field_type = rpb.FieldType.FILE
-    p1 = rpb.Paragraph(
-        start=0,
-        end=45,
-    )
-    p1.sentences.append(rpb.Sentence(start=0, end=45))
-    p1.classifications.append(c1)
-    p2 = rpb.Paragraph(
-        start=47,
-        end=64,
-    )
-
-    p2.sentences.append(rpb.Sentence(start=47, end=64))
-    p2.classifications.append(c1)
-
-    p3 = rpb.Paragraph(
-        start=65,
-        end=93,
-    )
-
-    p3.sentences.append(rpb.Sentence(start=65, end=93))
-    p3.classifications.append(c1)
-
-    p4 = rpb.Paragraph(
-        start=94,
-        end=109,
-    )
-
-    p4.sentences.append(rpb.Sentence(start=94, end=109))
-    p4.classifications.append(c1)
-
-    fcm.metadata.metadata.paragraphs.append(p1)
-    fcm.metadata.metadata.paragraphs.append(p2)
-    fcm.metadata.metadata.paragraphs.append(p3)
-    fcm.metadata.metadata.paragraphs.append(p4)
-    fcm.metadata.metadata.last_index.FromDatetime(datetime.now())
-    fcm.metadata.metadata.last_understanding.FromDatetime(datetime.now())
-    fcm.metadata.metadata.last_extract.FromDatetime(datetime.now())
-    fcm.metadata.metadata.ner["Ramon"] = "PERSON"
-
-    fcm.metadata.metadata.classifications.append(c2)
-    bm.field_metadata.append(fcm)
-
-    ev = rpb.ExtractedVectorsWrapper()
-    ev.field.field = "file"
-    ev.field.field_type = rpb.FieldType.FILE
-
-    bm.source = BrokerMessage.MessageSource.WRITER
-    return bm
-
-
-async def inject_resource_with_paragraph_labels(knowledgebox, writer):
-    bm = broker_resource(knowledgebox)
-    await inject_message(writer, bm)
-    return bm.uuid
+from nucliadb.train.tests.utils import get_batches_from_train_response_stream
+from nucliadb_protos import resources_pb2 as rpb
 
 
 @pytest.mark.asyncio
+@pytest.mark.parametrize("knowledgebox", ["STABLE", "EXPERIMENTAL"], indirect=True)
 async def test_generator_sentence_classification(
-    train_rest_api: aiohttp.ClientSession, knowledgebox: str, nucliadb_grpc: WriterStub
+    train_rest_api: aiohttp.ClientSession,
+    nucliadb_grpc: WriterStub,
+    knowledgebox_with_labels: str,
 ):
-    slr = SetLabelsRequest()
-    slr.kb.uuid = knowledgebox
-    slr.id = "labelset_paragraphs"
-    slr.labelset.kind.append(LabelSet.LabelSetKind.PARAGRAPHS)
-    l1 = Label(title="label_machine")
-    l2 = Label(title="label_user")
-    slr.labelset.labels.append(l1)
-    slr.labelset.labels.append(l2)
-    await nucliadb_grpc.SetLabels(slr)  # type: ignore
-
-    slr = SetLabelsRequest()
-    slr.kb.uuid = knowledgebox
-    slr.id = "labelset_resources"
-    slr.labelset.kind.append(LabelSet.LabelSetKind.RESOURCES)
-    l1 = Label(title="label_machine")
-    l2 = Label(title="label_user")
-    slr.labelset.labels.append(l1)
-    slr.labelset.labels.append(l2)
-    await nucliadb_grpc.SetLabels(slr)  # type: ignore
+    kbid = knowledgebox_with_labels
+
+    await inject_resource_with_sentence_classification(kbid, nucliadb_grpc)
 
-    await inject_resource_with_paragraph_labels(knowledgebox, nucliadb_grpc)
-    await asyncio.sleep(0.1)
     async with train_rest_api.get(
-        f"/{API_PREFIX}/v1/{KB_PREFIX}/{knowledgebox}/trainset"
+        f"/{API_PREFIX}/v1/{KB_PREFIX}/{kbid}/trainset"
     ) as partitions:
         assert partitions.status == 200
         data = await partitions.json()
         assert len(data["partitions"]) == 1
         partition_id = data["partitions"][0]
 
     trainset = TrainSet()
     trainset.type = TaskType.SENTENCE_CLASSIFICATION
     trainset.batch_size = 2
     trainset.filter.labels.append("labelset_paragraphs")
 
     async with train_rest_api.post(
-        f"/{API_PREFIX}/v1/{KB_PREFIX}/{knowledgebox}/trainset/{partition_id}",
+        f"/{API_PREFIX}/v1/{KB_PREFIX}/{kbid}/trainset/{partition_id}",
         data=trainset.SerializeToString(),
     ) as response:
         assert response.status == 200
         batches = []
-        async for batch in get_sentence_classification_batch_from_response(response):
+        async for batch in get_batches_from_train_response_stream(
+            response, SentenceClassificationBatch
+        ):
             batches.append(batch)
             assert len(batch.data) == 2
         assert len(batches) == 2
+
+
+async def inject_resource_with_sentence_classification(knowledgebox, writer):
+    bm = broker_resource(knowledgebox)
+    await inject_message(writer, bm)
+    await asyncio.sleep(0.1)
+    return bm.uuid
+
+
+def broker_resource(knowledgebox: str) -> BrokerMessage:
+    rid = str(uuid.uuid4())
+    bmb = BrokerMessageBuilder(kbid=knowledgebox, rid=rid)
+    bmb.with_title("Title Resource")
+    bmb.with_summary("Summary of document")
+    bmb.with_resource_labels("labelset_resources", ["label_user"])
+
+    file_field = FieldBuilder("file", rpb.FieldType.FILE)
+    file_field.with_extracted_text(
+        "My own text Ramon. This is great to be here. \n Where is my beer? Do you want to go shooping? This is a test!"  # noqa
+    )
+
+    labelset = "labelset_paragraphs"
+    labels = ["label_user"]
+    file_field.with_user_paragraph_labels(f"{rid}/f/file/0-45", labelset, labels)
+    file_field.with_user_paragraph_labels(f"{rid}/f/file/47-64", labelset, labels)
+    file_field.with_user_paragraph_labels(f"{rid}/f/file/65-93", labelset, labels)
+    file_field.with_user_paragraph_labels(f"{rid}/f/file/93-109", labelset, labels)
+
+    classification = rpb.Classification(
+        labelset="labelset_paragraphs", label="label_machine"
+    )
+    file_field.with_extracted_paragraph_metadata(
+        rpb.Paragraph(
+            start=0,
+            end=45,
+            classifications=[classification],
+            sentences=[rpb.Sentence(start=0, end=45)],
+        )
+    )
+    file_field.with_extracted_paragraph_metadata(
+        rpb.Paragraph(
+            start=47,
+            end=64,
+            classifications=[classification],
+            sentences=[rpb.Sentence(start=47, end=64)],
+        )
+    )
+    file_field.with_extracted_paragraph_metadata(
+        rpb.Paragraph(
+            start=65,
+            end=93,
+            classifications=[classification],
+            sentences=[rpb.Sentence(start=65, end=93)],
+        )
+    )
+    file_field.with_extracted_paragraph_metadata(
+        rpb.Paragraph(
+            start=93,
+            end=109,
+            classifications=[classification],
+            sentences=[rpb.Sentence(start=94, end=109)],
+        )
+    )
+
+    file_field.with_extracted_labels("labelset_resources", ["label_machine"])
+
+    bmb.add_field_builder(file_field)
+
+    bm = bmb.build()
+
+    return bm
```

## nucliadb/train/tests/test_token_classification.py

```diff
@@ -14,247 +14,63 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 import asyncio
-from collections import OrderedDict
-from typing import AsyncIterator, List
 
 import aiohttp
 import pytest
 from nucliadb_protos.dataset_pb2 import TaskType, TokenClassificationBatch, TrainSet
-from nucliadb_protos.resources_pb2 import (
-    Metadata,
-    Position,
-    TokenSplit,
-    UserFieldMetadata,
-)
-from nucliadb_protos.writer_pb2 import BrokerMessage, SetEntitiesRequest
+from nucliadb_protos.resources_pb2 import Position
+from nucliadb_protos.writer_pb2 import BrokerMessage
 from nucliadb_protos.writer_pb2_grpc import WriterStub
 
 from nucliadb.tests.utils import inject_message
+from nucliadb.tests.utils.broker_messages import BrokerMessageBuilder, FieldBuilder
 from nucliadb.train import API_PREFIX
 from nucliadb.train.api.v1.router import KB_PREFIX
-from nucliadb.train.generators.token_classifier import process_entities
-
-
-async def get_token_classification_batch_from_response(
-    response: aiohttp.ClientResponse,
-) -> AsyncIterator[TokenClassificationBatch]:
-    while True:
-        header = await response.content.read(4)
-        if header in [b"", None]:
-            break
-        payload_size = int.from_bytes(header, byteorder="big", signed=False)
-        payload = await response.content.read(payload_size)
-        pcb = TokenClassificationBatch()
-        pcb.ParseFromString(payload)
-        assert pcb.data
-        yield pcb
-
-
-def broker_resource(knowledgebox: str) -> BrokerMessage:
-    import uuid
-    from datetime import datetime
-
-    from nucliadb_protos import resources_pb2 as rpb
-
-    rid = str(uuid.uuid4())
-    slug = f"{rid}slug1"
-
-    bm: BrokerMessage = BrokerMessage(
-        kbid=knowledgebox,
-        uuid=rid,
-        slug=slug,
-        type=BrokerMessage.AUTOCOMMIT,
-    )
-
-    bm.basic.icon = "text/plain"
-    bm.basic.title = "This is a bird, its a plane, no, its el Super Fran"
-    bm.basic.summary = "Summary of Nuclia using Debian"
-    bm.basic.thumbnail = "doc"
-    bm.basic.layout = "default"
-    bm.basic.metadata.useful = True
-    bm.basic.metadata.status = Metadata.Status.PROCESSED
-    bm.basic.metadata.language = "es"
-    bm.basic.created.FromDatetime(datetime.now())
-    bm.basic.modified.FromDatetime(datetime.now())
-    bm.origin.source = rpb.Origin.Source.WEB
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "My own text Ramon. This is great to be at Nuclia. \n Where is the Generalitat de Catalunya? Eudald Camprubi, do you want to go shooping? This is a test Carmen Iniesta!"  # noqa
-    etw.field.field = "file"
-    etw.field.field_type = rpb.FieldType.FILE
-    bm.extracted_text.append(etw)
-
-    ufm = UserFieldMetadata()
-
-    ts = TokenSplit()
-    ts.token = "Ramon"
-    ts.klass = "PERSON"
-    ts.start = 12
-    ts.end = 17
-    ufm.token.append(ts)
-
-    ts = TokenSplit()
-    ts.token = "Nuclia"
-    ts.klass = "ORG"
-    ts.start = 42
-    ts.end = 48
-    ufm.token.append(ts)
-
-    ts = TokenSplit()
-    ts.token = "Generalitat de Catalunya"
-    ts.klass = "ORG"
-    ts.start = 65
-    ts.end = 89
-    ufm.token.append(ts)
-
-    ts = TokenSplit()
-    ts.token = "Eudald Camprubi"
-    ts.klass = "PERSON"
-    ts.start = 91
-    ts.end = 106
-    ufm.token.append(ts)
-
-    ts = TokenSplit()
-    ts.token = "Carmen Iniesta"
-    ts.klass = "PERSON"
-    ts.start = 151
-    ts.end = 165
-    ufm.token.append(ts)
-
-    ufm.field.field = "file"
-    ufm.field.field_type = rpb.FieldType.FILE
-    bm.basic.fieldmetadata.append(ufm)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "Summary of Nuclia using Debian"
-    etw.field.field = "summary"
-    etw.field.field_type = rpb.FieldType.GENERIC
-    bm.extracted_text.append(etw)
-
-    etw = rpb.ExtractedTextWrapper()
-    etw.body.text = "This is a bird, its a plane, no, its el Super Fran"
-    etw.field.field = "title"
-    etw.field.field_type = rpb.FieldType.GENERIC
-    bm.extracted_text.append(etw)
-
-    bm.files["file"].added.FromDatetime(datetime.now())
-    bm.files["file"].file.source = rpb.CloudFile.Source.EXTERNAL
-
-    fcm = rpb.FieldComputedMetadataWrapper()
-    fcm.field.field = "file"
-    fcm.field.field_type = rpb.FieldType.FILE
-    p1 = rpb.Paragraph(
-        start=0,
-        end=49,
-    )
-    p2 = rpb.Paragraph(
-        start=50,
-        end=90,
-    )
-
-    p3 = rpb.Paragraph(
-        start=91,
-        end=135,
-    )
-
-    p4 = rpb.Paragraph(
-        start=136,
-        end=166,
-    )
-
-    fcm.metadata.metadata.paragraphs.append(p1)
-    fcm.metadata.metadata.paragraphs.append(p2)
-    fcm.metadata.metadata.paragraphs.append(p3)
-    fcm.metadata.metadata.paragraphs.append(p4)
-    fcm.metadata.metadata.last_index.FromDatetime(datetime.now())
-    fcm.metadata.metadata.last_understanding.FromDatetime(datetime.now())
-    fcm.metadata.metadata.last_extract.FromDatetime(datetime.now())
-
-    bm.field_metadata.append(fcm)
-
-    fcm = rpb.FieldComputedMetadataWrapper()
-    fcm.field.field = "title"
-    fcm.field.field_type = rpb.FieldType.GENERIC
-    fcm.metadata.metadata.positions["PERSON/el Super Fran"].entity = "el Super Fran"
-    pos = Position(start=37, end=50)
-    fcm.metadata.metadata.positions["PERSON/el Super Fran"].position.append(pos)
-    bm.field_metadata.append(fcm)
-
-    fcm = rpb.FieldComputedMetadataWrapper()
-    fcm.field.field = "summary"
-    fcm.field.field_type = rpb.FieldType.GENERIC
-    fcm.metadata.metadata.positions["ORG/Nuclia"].entity = "Nuclia"
-    pos = Position(start=11, end=17)
-    fcm.metadata.metadata.positions["ORG/Nuclia"].position.append(pos)
-    fcm.metadata.metadata.positions["ORG/Debian"].entity = "Debian"
-    pos = Position(start=24, end=30)
-    fcm.metadata.metadata.positions["ORG/Debian"].position.append(pos)
-    bm.field_metadata.append(fcm)
-
-    bm.source = BrokerMessage.MessageSource.WRITER
-    return bm
-
-
-async def inject_resource_with_token_classification(knowledgebox, writer):
-    bm = broker_resource(knowledgebox)
-    await inject_message(writer, bm)
-    return bm.uuid
+from nucliadb.train.tests.utils import get_batches_from_train_response_stream
+from nucliadb_protos import resources_pb2 as rpb
 
 
 @pytest.mark.asyncio
+@pytest.mark.parametrize("knowledgebox", ["STABLE", "EXPERIMENTAL"], indirect=True)
 async def test_generator_token_classification(
-    train_rest_api: aiohttp.ClientSession, knowledgebox: str, nucliadb_grpc: WriterStub
+    train_rest_api: aiohttp.ClientSession,
+    knowledgebox_with_entities: str,
+    nucliadb_grpc: WriterStub,
 ):
-    # Create Entities
-    ser = SetEntitiesRequest()
-    ser.kb.uuid = knowledgebox
-    ser.group = "PERSON"
-    ser.entities.title = "PERSON"
-    ser.entities.entities["Ramon"].value = "Ramon"
-    ser.entities.entities["Eudald Camprubi"].value = "Eudald Camprubi"
-    ser.entities.entities["Carmen Iniesta"].value = "Carmen Iniesta"
-    ser.entities.entities["el Super Fran"].value = "el Super Fran"
-    await nucliadb_grpc.SetEntities(ser)  # type: ignore
-
-    ser = SetEntitiesRequest()
-    ser.kb.uuid = knowledgebox
-    ser.group = "ORG"
-    ser.entities.title = "ORG"
-    ser.entities.entities["Nuclia"].value = "Nuclia"
-    ser.entities.entities["Debian"].value = "Debian"
-    ser.entities.entities["Generalitat de Catalunya"].value = "Generalitat de Catalunya"
-    await nucliadb_grpc.SetEntities(ser)  # type: ignore
+    kbid = knowledgebox_with_entities
+
+    await inject_resource_with_token_classification(kbid, nucliadb_grpc)
 
-    await inject_resource_with_token_classification(knowledgebox, nucliadb_grpc)
-    await asyncio.sleep(0.1)
     async with train_rest_api.get(
-        f"/{API_PREFIX}/v1/{KB_PREFIX}/{knowledgebox}/trainset"
+        f"/{API_PREFIX}/v1/{KB_PREFIX}/{kbid}/trainset"
     ) as partitions:
         assert partitions.status == 200
         data = await partitions.json()
         assert len(data["partitions"]) == 1
         partition_id = data["partitions"][0]
 
     trainset = TrainSet()
     trainset.type = TaskType.TOKEN_CLASSIFICATION
     trainset.batch_size = 2
     trainset.filter.labels.append("PERSON")
     trainset.filter.labels.append("ORG")
     async with train_rest_api.post(
-        f"/{API_PREFIX}/v1/{KB_PREFIX}/{knowledgebox}/trainset/{partition_id}",
+        f"/{API_PREFIX}/v1/{KB_PREFIX}/{kbid}/trainset/{partition_id}",
         data=trainset.SerializeToString(),
     ) as response:
         assert response.status == 200
-        batches: List[TokenClassificationBatch] = []
-        async for batch in get_token_classification_batch_from_response(response):
+        batches: list[TokenClassificationBatch] = []
+        async for batch in get_batches_from_train_response_stream(
+            response, TokenClassificationBatch
+        ):
             batches.append(batch)
 
     for batch in batches:
         if batch.data[0].token == "Eudald":
             assert batch.data[0].label == "B-PERSON"
             assert batch.data[1].label == "I-PERSON"
             assert batch.data[2].label == "O"
@@ -269,32 +85,52 @@
             assert batch.data[2].label == "B-ORG"
             assert batch.data[4].label == "B-ORG"
         if batch.data[0].token == "My":
             assert batch.data[3].label == "B-PERSON"
             assert batch.data[12].label == "B-ORG"
 
 
-def test_process_entities():
-    split_text = {"__main__": "This is a bird, its a plane, no, its el Super Fran"}
-    split_ners = {"__main__": OrderedDict([((37, 50), ("PERSON", "el Super Fran"))])}
-    split_paragaphs = {"__main__": []}
-    entities = list(
-        process_entities(
-            split_text["__main__"], split_ners["__main__"], split_paragaphs["__main__"]
-        )
+async def inject_resource_with_token_classification(knowledgebox, writer):
+    bm = broker_resource(knowledgebox)
+    await inject_message(writer, bm)
+    await asyncio.sleep(0.1)
+    return bm.uuid
+
+
+def broker_resource(knowledgebox: str) -> BrokerMessage:
+    bmb = BrokerMessageBuilder(kbid=knowledgebox)
+
+    bmb.with_title("This is a bird, its a plane, no, its el Super Fran")
+    title_field = bmb.field_builder("title", rpb.FieldType.GENERIC)
+    title_field.with_extracted_entity(
+        "PERSON", "el Super Fran", positions=[Position(start=37, end=50)]
+    )
+
+    bmb.with_summary("Summary of Nuclia using Debian")
+    summary_field = bmb.field_builder("summary", rpb.FieldType.GENERIC)
+    summary_field.with_extracted_entity(
+        "ORG", "Nuclia", positions=[Position(start=11, end=17)]
+    )
+    summary_field.with_extracted_entity(
+        "ORG", "Debian", positions=[Position(start=24, end=30)]
+    )
+
+    file_field = FieldBuilder("file", rpb.FieldType.FILE)
+    file_field.with_extracted_text(
+        "My own text Ramon. This is great to be at Nuclia. \n Where is the Generalitat de Catalunya? Eudald Camprubi, do you want to go shooping? This is a test Carmen Iniesta!"  # noqa
     )
-    assert entities == [
-        [
-            ("This", "O"),
-            ("is", "O"),
-            ("a", "O"),
-            ("bird,", "O"),
-            ("its", "O"),
-            ("a", "O"),
-            ("plane,", "O"),
-            ("no,", "O"),
-            ("its", "O"),
-            ("el", "B-PERSON"),
-            ("Super", "I-PERSON"),
-            ("Fran", "I-PERSON"),
-        ]
-    ]
+    file_field.with_extracted_paragraph_metadata(rpb.Paragraph(start=0, end=49))
+    file_field.with_extracted_paragraph_metadata(rpb.Paragraph(start=50, end=90))
+    file_field.with_extracted_paragraph_metadata(rpb.Paragraph(start=91, end=135))
+    file_field.with_extracted_paragraph_metadata(rpb.Paragraph(start=136, end=166))
+
+    file_field.with_user_entity("PERSON", "Ramon", start=12, end=17)
+    file_field.with_user_entity("ORG", "Nuclia", start=42, end=48)
+    file_field.with_user_entity("ORG", "Generalitat de Catalunya", start=65, end=89)
+    file_field.with_user_entity("PERSON", "Eudald", start=91, end=106)
+    file_field.with_user_entity("PERSON", "Carmen Iniesta", start=151, end=165)
+
+    bmb.add_field_builder(file_field)
+
+    bm = bmb.build()
+
+    return bm
```

## nucliadb/writer/app.py

```diff
@@ -13,44 +13,55 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+
+import functools
+
 import pkg_resources
 from fastapi import FastAPI
 from fastapi.responses import JSONResponse
 from starlette.middleware import Middleware
 from starlette.middleware.authentication import AuthenticationMiddleware
 from starlette.middleware.cors import CORSMiddleware
 from starlette.requests import ClientDisconnect, Request
 from starlette.responses import HTMLResponse
 
+from nucliadb.common.context.fastapi import get_app_context, set_app_context
 from nucliadb.writer import API_PREFIX
 from nucliadb.writer.api.v1.router import api as api_v1
 from nucliadb.writer.lifecycle import finalize, initialize
 from nucliadb_telemetry import errors
-from nucliadb_utils.authentication import STFAuthenticationBackend
+from nucliadb_utils import const
+from nucliadb_utils.authentication import NucliaCloudAuthenticationBackend
 from nucliadb_utils.fastapi.openapi import extend_openapi
 from nucliadb_utils.fastapi.versioning import VersionedFastAPI
 from nucliadb_utils.settings import http_settings, running_settings
+from nucliadb_utils.utilities import has_feature
+
+middleware = []
+
+if has_feature(const.Features.CORS_MIDDLEWARE, default=False):
+    middleware.append(
+        Middleware(
+            CORSMiddleware,
+            allow_origins=http_settings.cors_origins,
+            allow_methods=["*"],
+            # Authorization will be exluded from * in the future, (CORS non-wildcard request-header).
+            # Browsers already showing deprecation notices, so it needs to be specified explicitly
+            allow_headers=["*", "Authorization"],
+        )
+    )
 
-middleware = [
-    Middleware(
-        CORSMiddleware,
-        allow_origins=http_settings.cors_origins,
-        allow_methods=["*"],
-        allow_headers=["*"],
-    ),
-    Middleware(
-        AuthenticationMiddleware,
-        backend=STFAuthenticationBackend(),
-    ),
-]
+middleware.extend(
+    [Middleware(AuthenticationMiddleware, backend=NucliaCloudAuthenticationBackend())]
+)
 
 
 errors.setup_error_handling(pkg_resources.get_distribution("nucliadb").version)
 
 on_startup = [initialize]
 on_shutdown = [finalize]
 
@@ -77,29 +88,45 @@
     on_shutdown=on_shutdown,
     exception_handlers={
         Exception: global_exception_handler,
         ClientDisconnect: client_disconnect_handler,
     },
 )
 
-base_app = FastAPI(title="NucliaDB Writer API", **fastapi_settings)  # type: ignore
 
-base_app.include_router(api_v1)
+def create_application() -> FastAPI:
+    base_app = FastAPI(title="NucliaDB Writer API", **fastapi_settings)  # type: ignore
 
-extend_openapi(base_app)
+    base_app.include_router(api_v1)
 
-application = VersionedFastAPI(
-    base_app,
-    version_format="{major}",
-    prefix_format=f"/{API_PREFIX}/v{{major}}",
-    default_version=(1, 0),
-    enable_latest=False,
-    kwargs=fastapi_settings,
-)
+    extend_openapi(base_app)
 
+    application = VersionedFastAPI(
+        base_app,
+        version_format="{major}",
+        prefix_format=f"/{API_PREFIX}/v{{major}}",
+        default_version=(1, 0),
+        enable_latest=False,
+        kwargs=fastapi_settings,
+    )
 
-async def homepage(request):
-    return HTMLResponse("NucliaDB Writer Service")
+    async def homepage(request):
+        return HTMLResponse("NucliaDB Writer Service")
 
+    # Use raw starlette routes to avoid unnecessary overhead
+    application.add_route("/", homepage)
 
-# Use raw starlette routes to avoid unnecessary overhead
-application.add_route("/", homepage)
+    set_app_context(application)
+    maybe_configure_back_pressure(application)
+    return application
+
+
+def maybe_configure_back_pressure(application: FastAPI):
+    from nucliadb.writer.back_pressure import start_materializer, stop_materializer
+    from nucliadb.writer.settings import back_pressure_settings
+    from nucliadb_utils.settings import is_onprem_nucliadb
+
+    if back_pressure_settings.enabled and not is_onprem_nucliadb():
+        context = get_app_context(application)
+        start_materializer_with_context = functools.partial(start_materializer, context)
+        application.add_event_handler("startup", start_materializer_with_context)
+        application.add_event_handler("shutdown", stop_materializer)
```

## nucliadb/writer/lifecycle.py

```diff
@@ -20,39 +20,31 @@
 from nucliadb.ingest.processing import start_processing_engine
 from nucliadb.ingest.utils import start_ingest, stop_ingest
 from nucliadb.writer import SERVICE_NAME
 from nucliadb.writer.tus import finalize as storage_finalize
 from nucliadb.writer.tus import initialize as storage_initialize
 from nucliadb.writer.utilities import get_processing
 from nucliadb_telemetry.utils import clean_telemetry, setup_telemetry
-from nucliadb_utils.partition import PartitionUtility
-from nucliadb_utils.settings import nuclia_settings
 from nucliadb_utils.utilities import (
-    Utility,
     finalize_utilities,
-    set_utility,
+    start_partitioning_utility,
     start_transaction_utility,
     stop_transaction_utility,
 )
 
 
 async def initialize():
     await setup_telemetry(SERVICE_NAME)
 
     await start_ingest(SERVICE_NAME)
 
     await start_processing_engine()
 
-    set_utility(
-        Utility.PARTITION,
-        PartitionUtility(
-            partitions=nuclia_settings.nuclia_partitions,
-            seed=nuclia_settings.nuclia_hash_seed,
-        ),
-    )
+    start_partitioning_utility()
+
     await start_transaction_utility(SERVICE_NAME)
     await storage_initialize()
 
 
 async def finalize():
     await stop_transaction_utility()
```

## nucliadb/writer/openapi.py

```diff
@@ -14,60 +14,15 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
-import datetime
-import json
-import sys
 
-from fastapi.openapi.utils import get_openapi
-from starlette.routing import Mount
-
-from nucliadb.writer import API_PREFIX
-
-
-def is_versioned_route(route):
-    return isinstance(route, Mount) and route.path.startswith(f"/{API_PREFIX}/v")
-
-
-def extract_openapi(application, version, commit_id):
-    app = [
-        route.app
-        for route in application.routes
-        if is_versioned_route(route) and route.app.version == version
-    ][0]
-    document = get_openapi(
-        title=app.title,
-        version=app.version,
-        openapi_version=app.openapi_version,
-        description=app.description,
-        terms_of_service=app.terms_of_service,
-        contact=app.contact,
-        license_info=app.license_info,
-        routes=app.routes,
-        tags=app.openapi_tags,
-        servers=app.servers,
-    )
-
-    document["x-metadata"] = {
-        "nucliadb_writer": {
-            "commit": commit_id,
-            "last_updated": datetime.datetime.utcnow().isoformat(),
-        }
-    }
-    return document
+from nucliadb import openapi
 
 
 def command_extract_openapi():
-    from nucliadb.writer.app import application
-
-    openapi_json_path = sys.argv[1]
-    api_version = sys.argv[2]
-    commit_id = sys.argv[3]
+    from nucliadb.writer.app import create_application
 
-    json.dump(
-        extract_openapi(application, api_version, commit_id),
-        open(openapi_json_path, "w"),
-    )
+    openapi.command_extract_openapi(create_application(), "nucliadb_writer")
```

## nucliadb/writer/run.py

```diff
@@ -15,24 +15,26 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 
 from nucliadb.writer import SERVICE_NAME
-from nucliadb.writer.app import application
+from nucliadb.writer.app import create_application
 from nucliadb_telemetry.fastapi import instrument_app
 from nucliadb_telemetry.logs import setup_logging
 from nucliadb_telemetry.utils import get_telemetry
 from nucliadb_utils.fastapi.run import run_fastapi_with_metrics
 
 
 def run():
     setup_logging()
+    application = create_application()
     instrument_app(
         application,
         tracer_provider=get_telemetry(SERVICE_NAME),
         excluded_urls=["/"],
         metrics=True,
+        trace_id_on_responses=True,
     )
 
     run_fastapi_with_metrics(application)
```

## nucliadb/writer/settings.py

```diff
@@ -15,17 +15,59 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from typing import Optional
 
-from pydantic import BaseSettings
+from pydantic import BaseSettings, Field
 
 
 class Settings(BaseSettings):
     dm_enabled: bool = True
     dm_redis_host: Optional[str] = None
     dm_redis_port: Optional[int] = None
 
 
+class BackPressureSettings(BaseSettings):
+    enabled: bool = Field(
+        default=False,
+        description="Enable or disable back pressure.",
+        env=["back_pressure_enabled"],
+    )
+    indexing_rate: float = Field(
+        default=4,
+        description="Estimation of the indexing rate in messages per second. This is used to calculate the try again in time",  # noqa
+    )
+    ingest_rate: float = Field(
+        default=4,
+        description="Estimation of the ingest processed consumer rate in messages per second. This is used to calculate the try again in time",  # noqa
+    )
+    processing_rate: float = Field(
+        default=1,
+        description="Estimation of the processing rate in messages per second. This is used to calculate the try again in time",  # noqa
+    )
+    max_indexing_pending: int = Field(
+        default=100,
+        description="Max number of messages pending to index in a node queue before rate limiting writes. Set to 0 to disable indexing back pressure checks",  # noqa
+    )
+    max_ingest_pending: int = Field(
+        default=1_000,
+        description="Max number of messages pending to be ingested by processed consumers before rate limiting writes. Set to 0 to disable ingest back pressure checks",  # noqa
+    )
+    max_processing_pending: int = Field(
+        default=1000,
+        description="Max number of messages pending to process per Knowledge Box before rate limiting writes. Set to 0 to disable processing back pressure checks",  # noqa
+    )
+    indexing_check_interval: int = Field(
+        default=30,
+        description="Interval in seconds to check the indexing pending messages",
+    )
+    ingest_check_interval: int = Field(
+        default=30,
+        description="Interval in seconds to check the ingest pending messages",
+    )
+
+
 settings = Settings()
+
+back_pressure_settings = BackPressureSettings()
```

## nucliadb/writer/api/v1/__init__.py

```diff
@@ -13,13 +13,15 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+from . import export_import  # noqa
 from . import field  # noqa
 from . import knowledgebox  # noqa
+from . import learning_config  # noqa
 from . import resource  # noqa
 from . import services  # noqa
 from . import upload  # noqa
 from .router import api  # noqa
```

## nucliadb/writer/api/v1/field.py

```diff
@@ -13,66 +13,62 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import TYPE_CHECKING, Dict, List, Optional, Tuple
+from typing import TYPE_CHECKING, Optional
 
 from fastapi import HTTPException, Response
-from fastapi.params import Header
 from fastapi_versioning import version  # type: ignore
 from nucliadb_protos.resources_pb2 import FieldID, FieldType, Metadata
 from nucliadb_protos.writer_pb2 import BrokerMessage
 from starlette.requests import Request
 
 import nucliadb_models as models
+from nucliadb.common.maindb.utils import get_driver
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
 from nucliadb.ingest.processing import PushPayload, Source
-from nucliadb.ingest.utils import get_driver
 from nucliadb.writer import SERVICE_NAME
+from nucliadb.writer.api.constants import (
+    SKIP_STORE_DEFAULT,
+    X_FILE_PASSWORD,
+    X_NUCLIADB_USER,
+)
 from nucliadb.writer.api.v1.resource import get_rid_from_params_or_raise_error
 from nucliadb.writer.api.v1.router import KB_PREFIX, RESOURCE_PREFIX, RSLUG_PREFIX, api
+from nucliadb.writer.back_pressure import maybe_back_pressure
 from nucliadb.writer.resource.audit import parse_audit
-from nucliadb.writer.resource.basic import set_processing_info
 from nucliadb.writer.resource.field import (
     extract_file_field,
     parse_conversation_field,
     parse_datetime_field,
     parse_file_field,
     parse_keywordset_field,
     parse_layout_field,
     parse_link_field,
     parse_text_field,
 )
 from nucliadb.writer.utilities import get_processing
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_models.writer import ResourceFieldAdded, ResourceUpdated
-from nucliadb_telemetry.utils import set_info_on_span
 from nucliadb_utils.authentication import requires
 from nucliadb_utils.exceptions import LimitsExceededError, SendToProcessError
+from nucliadb_utils.transaction import TransactionCommitTimeoutError
 from nucliadb_utils.utilities import (
     get_partitioning,
     get_storage,
     get_transaction_utility,
 )
 
 if TYPE_CHECKING:  # pragma: no cover
-    SKIP_STORE_DEFAULT = False
-    FIELD_TYPE_NAME_TO_FIELD_TYPE_MAP: Dict[models.FieldTypeName, FieldType.V]
-    SYNC_CALL = False
-    X_NUCLIADB_USER = ""
-    X_FILE_PASSWORD = None
+    FIELD_TYPE_NAME_TO_FIELD_TYPE_MAP: dict[models.FieldTypeName, FieldType.V]
 else:
-    SKIP_STORE_DEFAULT = Header(False)
-    FIELD_TYPE_NAME_TO_FIELD_TYPE_MAP: Dict[models.FieldTypeName, int]
-    SYNC_CALL = Header(False)
-    X_NUCLIADB_USER = Header("")
-    X_FILE_PASSWORD = Header(None)
+    FIELD_TYPE_NAME_TO_FIELD_TYPE_MAP: dict[models.FieldTypeName, int]
 
 
 FIELD_TYPE_NAME_TO_FIELD_TYPE_MAP = {
     models.FieldTypeName.FILE: FieldType.FILE,
     models.FieldTypeName.LINK: FieldType.LINK,
     models.FieldTypeName.DATETIME: FieldType.DATETIME,
     models.FieldTypeName.KEYWORDSET: FieldType.KEYWORDSET,
@@ -81,15 +77,15 @@
     # models.FieldTypeName.GENERIC: FieldType.GENERIC,
     models.FieldTypeName.CONVERSATION: FieldType.CONVERSATION,
 }
 
 
 def prepare_field_put(
     kbid: str, rid: str, request: Request
-) -> Tuple[BrokerMessage, PushPayload, int]:
+) -> tuple[BrokerMessage, PushPayload, int]:
     partitioning = get_partitioning()
     partition = partitioning.generate_partition(kbid, rid)
 
     writer = BrokerMessage()
     toprocess = PushPayload(
         uuid=rid,
         kbid=kbid,
@@ -99,351 +95,548 @@
 
     writer.kbid = kbid
     writer.uuid = rid
     toprocess.kbid = kbid
     toprocess.uuid = rid
     toprocess.source = Source.HTTP
 
-    set_info_on_span({"nuclia.rid": rid, "nuclia.kbid": kbid})
-
     parse_audit(writer.audit, request)
     return writer, toprocess, partition
 
 
 async def finish_field_put(
     writer: BrokerMessage,
     toprocess: PushPayload,
     partition: int,
-    wait_on_commit: bool,
-) -> int:
+) -> Optional[int]:
     # Create processing message
     transaction = get_transaction_utility()
     processing = get_processing()
-
-    processing_info = await processing.send_to_process(toprocess, partition)
-
-    writer.source = BrokerMessage.MessageSource.WRITER
-    set_processing_info(writer, processing_info)
-    await transaction.commit(writer, partition, wait=wait_on_commit)
-
-    return processing_info.seqid
+    try:
+        writer.source = BrokerMessage.MessageSource.WRITER
+        await transaction.commit(writer, partition, wait=True)
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
+        )
+    try:
+        processing_info = await processing.send_to_process(toprocess, partition)
+        return processing_info.seqid
+    except LimitsExceededError as exc:
+        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
+    except SendToProcessError:
+        raise HTTPException(
+            status_code=500,
+            detail="Error while sending to process. Try calling /reprocess",
+        )
 
 
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/text/{{field_id}}",
     status_code=201,
     name="Add resource text field (by slug)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def add_resource_field_text_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    field_payload: models.TextField,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_text(
+        request,
+        kbid,
+        field_id,
+        field_payload,
+        rslug=rslug,
+    )
+
+
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/text/{{field_id}}",
     status_code=201,
     name="Add resource text field (by id)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def add_resource_field_text(
+async def add_resource_field_text_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    field_payload: models.TextField,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_text(
+        request,
+        kbid,
+        field_id,
+        field_payload,
+        rid=rid,
+    )
+
+
+async def _add_resource_field_text(
     request: Request,
     kbid: str,
     field_id: str,
     field_payload: models.TextField,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ) -> ResourceFieldAdded:
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     writer, toprocess, partition = prepare_field_put(kbid, rid, request)
     parse_text_field(field_id, field_payload, writer, toprocess)
-    try:
-        seqid = await finish_field_put(writer, toprocess, partition, x_synchronous)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
+    seqid = await finish_field_put(writer, toprocess, partition)
     return ResourceFieldAdded(seqid=seqid)
 
 
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/link/{{field_id}}",
     status_code=201,
     name="Add resource link field (by slug)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def add_resource_field_link_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    field_payload: models.LinkField,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_link(
+        request, kbid, field_id, field_payload, rslug=rslug
+    )
+
+
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/link/{{field_id}}",
     status_code=201,
     name="Add resource link field (by id)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def add_resource_field_link(
+async def add_resource_field_link_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    field_payload: models.LinkField,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_link(
+        request, kbid, field_id, field_payload, rid=rid
+    )
+
+
+async def _add_resource_field_link(
     request: Request,
     kbid: str,
     field_id: str,
     field_payload: models.LinkField,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ) -> ResourceFieldAdded:
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     writer, toprocess, partition = prepare_field_put(kbid, rid, request)
     parse_link_field(field_id, field_payload, writer, toprocess)
-    try:
-        seqid = await finish_field_put(writer, toprocess, partition, x_synchronous)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
+    seqid = await finish_field_put(writer, toprocess, partition)
     return ResourceFieldAdded(seqid=seqid)
 
 
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/keywordset/{{field_id}}",
     status_code=201,
     name="Add resource keywordset field (by slug)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def add_resource_field_keywordset_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    field_payload: models.FieldKeywordset,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_keywordset(
+        request, kbid, field_id, field_payload, rslug=rslug
+    )
+
+
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/keywordset/{{field_id}}",
     status_code=201,
     name="Add resource keywordset field (by id)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def add_resource_field_keywordset(
+async def add_resource_field_keywordset_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    field_payload: models.FieldKeywordset,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_keywordset(
+        request, kbid, field_id, field_payload, rid=rid
+    )
+
+
+async def _add_resource_field_keywordset(
     request: Request,
     kbid: str,
     field_id: str,
     field_payload: models.FieldKeywordset,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ) -> ResourceFieldAdded:
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     writer, toprocess, partition = prepare_field_put(kbid, rid, request)
     parse_keywordset_field(field_id, field_payload, writer, toprocess)
-    try:
-        seqid = await finish_field_put(writer, toprocess, partition, x_synchronous)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
+    seqid = await finish_field_put(writer, toprocess, partition)
     return ResourceFieldAdded(seqid=seqid)
 
 
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/datetime/{{field_id}}",
     status_code=201,
     name="Add resource datetime field (by slug)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def add_resource_field_datetime_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    field_payload: models.FieldDatetime,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_datetime(
+        request, kbid, field_id, field_payload, rslug=rslug
+    )
+
+
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/datetime/{{field_id}}",
     status_code=201,
     name="Add resource datetime field (by id)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def add_resource_field_datetime(
+async def add_resource_field_datetime_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    field_payload: models.FieldDatetime,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_datetime(
+        request, kbid, field_id, field_payload, rid=rid
+    )
+
+
+async def _add_resource_field_datetime(
     request: Request,
     kbid: str,
     field_id: str,
     field_payload: models.FieldDatetime,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ) -> ResourceFieldAdded:
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     writer, toprocess, partition = prepare_field_put(kbid, rid, request)
     parse_datetime_field(field_id, field_payload, writer, toprocess)
-    try:
-        seqid = await finish_field_put(writer, toprocess, partition, x_synchronous)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
+    seqid = await finish_field_put(writer, toprocess, partition)
     return ResourceFieldAdded(seqid=seqid)
 
 
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/layout/{{field_id}}",
     status_code=201,
     name="Add resource layout field (by slug)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def add_resource_field_layout_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    field_payload: models.InputLayoutField,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_layout(
+        request,
+        kbid,
+        field_id,
+        field_payload,
+        rslug=rslug,
+    )
+
+
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/layout/{{field_id}}",
     status_code=201,
     name="Add resource layout field (by id)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def add_resource_field_layout(
+async def add_resource_field_layout_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    field_payload: models.InputLayoutField,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_layout(
+        request, kbid, field_id, field_payload, rid=rid
+    )
+
+
+async def _add_resource_field_layout(
     request: Request,
     kbid: str,
     field_id: str,
     field_payload: models.InputLayoutField,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ) -> ResourceFieldAdded:
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     writer, toprocess, partition = prepare_field_put(kbid, rid, request)
     await parse_layout_field(field_id, field_payload, writer, toprocess, kbid, rid)
-    try:
-        seqid = await finish_field_put(writer, toprocess, partition, x_synchronous)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
+    seqid = await finish_field_put(writer, toprocess, partition)
     return ResourceFieldAdded(seqid=seqid)
 
 
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/conversation/{{field_id}}",
     status_code=201,
     name="Add resource conversation field (by slug)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def add_resource_field_conversation_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    field_payload: models.InputConversationField,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_conversation(
+        request, kbid, field_id, field_payload, rslug=rslug
+    )
+
+
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/conversation/{{field_id}}",
     status_code=201,
     name="Add resource conversation field (by id)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def add_resource_field_conversation(
+async def add_resource_field_conversation_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    field_payload: models.InputConversationField,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_conversation(
+        request, kbid, field_id, field_payload, rid=rid
+    )
+
+
+async def _add_resource_field_conversation(
     request: Request,
     kbid: str,
     field_id: str,
     field_payload: models.InputConversationField,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ) -> ResourceFieldAdded:
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     writer, toprocess, partition = prepare_field_put(kbid, rid, request)
     await parse_conversation_field(
         field_id, field_payload, writer, toprocess, kbid, rid
     )
-    try:
-        seqid = await finish_field_put(writer, toprocess, partition, x_synchronous)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
+    seqid = await finish_field_put(writer, toprocess, partition)
     return ResourceFieldAdded(seqid=seqid)
 
 
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/file/{{field_id}}",
     status_code=201,
     name="Add resource file field (by slug)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def add_resource_field_file_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    field_payload: models.FileField,
+    x_skip_store: bool = SKIP_STORE_DEFAULT,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_file(
+        request,
+        kbid,
+        field_id,
+        field_payload,
+        x_skip_store,
+        rslug=rslug,
+    )
+
+
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/file/{{field_id}}",
     status_code=201,
     name="Add resource file field (by id)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def add_resource_field_file(
+async def add_resource_field_file_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    field_payload: models.FileField,
+    x_skip_store: bool = SKIP_STORE_DEFAULT,
+) -> ResourceFieldAdded:
+    return await _add_resource_field_file(
+        request, kbid, field_id, field_payload, x_skip_store, rid=rid
+    )
+
+
+async def _add_resource_field_file(
     request: Request,
     kbid: str,
     field_id: str,
     field_payload: models.FileField,
+    x_skip_store: bool,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_skip_store: bool = SKIP_STORE_DEFAULT,
-    x_synchronous: bool = SYNC_CALL,
 ) -> ResourceFieldAdded:
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     writer, toprocess, partition = prepare_field_put(kbid, rid, request)
     await parse_file_field(
         field_id, field_payload, writer, toprocess, kbid, rid, skip_store=x_skip_store
     )
-
-    try:
-        seqid = await finish_field_put(
-            writer, toprocess, partition, wait_on_commit=x_synchronous
-        )
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
+    seqid = await finish_field_put(writer, toprocess, partition)
     return ResourceFieldAdded(seqid=seqid)
 
 
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/conversation/{{field_id}}/messages",
     status_code=200,
     name="Append messages to conversation field (by slug)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def append_messages_to_conversation_field_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    messages: list[models.InputMessage],
+) -> ResourceFieldAdded:
+    return await _append_messages_to_conversation_field(
+        request, kbid, field_id, messages, rslug=rslug
+    )
+
+
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/conversation/{{field_id}}/messages",
     status_code=200,
     name="Append messages to conversation field (by id)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def append_messages_to_conversation_field(
+async def append_messages_to_conversation_field_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    messages: list[models.InputMessage],
+) -> ResourceFieldAdded:
+    return await _append_messages_to_conversation_field(
+        request, kbid, field_id, messages, rid=rid
+    )
+
+
+async def _append_messages_to_conversation_field(
     request: Request,
     kbid: str,
     field_id: str,
-    messages: List[models.InputMessage],
+    messages: list[models.InputMessage],
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ) -> ResourceFieldAdded:
     transaction = get_transaction_utility()
     processing = get_processing()
     partitioning = get_partitioning()
 
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     partition = partitioning.generate_partition(kbid, rid)
 
     writer = BrokerMessage()
     toprocess = PushPayload(
         uuid=rid,
         kbid=kbid,
         partition=partition,
@@ -452,68 +645,100 @@
 
     writer.kbid = kbid
     writer.uuid = rid
     toprocess.kbid = kbid
     toprocess.uuid = rid
     toprocess.source = Source.HTTP
 
-    set_info_on_span({"nuclia.kbid": kbid, "nuclia.rid": rid})
-
     parse_audit(writer.audit, request)
 
     field = models.InputConversationField()
     field.messages.extend(messages)
 
     await parse_conversation_field(field_id, field, writer, toprocess, kbid, rid)
 
+    writer.source = BrokerMessage.MessageSource.WRITER
+    try:
+        await transaction.commit(writer, partition, wait=True)
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
+        )
     try:
         processing_info = await processing.send_to_process(toprocess, partition)
     except LimitsExceededError as exc:
         raise HTTPException(status_code=exc.status_code, detail=exc.detail)
     except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
-    writer.source = BrokerMessage.MessageSource.WRITER
-    set_processing_info(writer, processing_info)
-    await transaction.commit(writer, partition, wait=x_synchronous)
+        raise HTTPException(
+            status_code=500,
+            detail="Error while sending to process. Try calling /reprocess",
+        )
 
     return ResourceFieldAdded(seqid=processing_info.seqid)
 
 
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/layout/{{field_id}}/blocks",
     status_code=200,
     name="Append blocks to layout field (by slug)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def append_blocks_to_layout_field_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_id: str,
+    blocks: dict[str, models.InputBlock],
+) -> ResourceFieldAdded:
+    return await _append_blocks_to_layout_field(
+        request, kbid, field_id, blocks, rslug=rslug
+    )
+
+
 @api.put(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/layout/{{field_id}}/blocks",
     status_code=200,
     name="Append blocks to layout field (by id)",
     response_model=ResourceFieldAdded,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def append_blocks_to_layout_field(
+async def append_blocks_to_layout_field_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_id: str,
+    blocks: dict[str, models.InputBlock],
+) -> ResourceFieldAdded:
+    return await _append_blocks_to_layout_field(
+        request, kbid, field_id, blocks, rid=rid
+    )
+
+
+async def _append_blocks_to_layout_field(
     request: Request,
     kbid: str,
     field_id: str,
-    blocks: Dict[str, models.InputBlock],
+    blocks: dict[str, models.InputBlock],
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ) -> ResourceFieldAdded:
     transaction = get_transaction_utility()
     processing = get_processing()
     partitioning = get_partitioning()
 
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     partition = partitioning.generate_partition(kbid, rid)
 
     writer = BrokerMessage()
     toprocess = PushPayload(
         uuid=rid,
         kbid=kbid,
         partition=partition,
@@ -522,60 +747,91 @@
 
     writer.kbid = kbid
     writer.uuid = rid
     toprocess.kbid = kbid
     toprocess.uuid = rid
     toprocess.source = Source.HTTP
 
-    set_info_on_span({"nuclia.kbid": kbid, "nuclia.rid": rid})
-
     parse_audit(writer.audit, request)
 
     field = models.InputLayoutField(body=models.InputLayoutContent())
     field.body.blocks.update(blocks)
     await parse_layout_field(field_id, field, writer, toprocess, kbid, rid)
 
+    writer.source = BrokerMessage.MessageSource.WRITER
+    try:
+        await transaction.commit(writer, partition, wait=True)
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
+        )
     try:
         processing_info = await processing.send_to_process(toprocess, partition)
     except LimitsExceededError as exc:
         raise HTTPException(status_code=exc.status_code, detail=exc.detail)
     except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
-    writer.source = BrokerMessage.MessageSource.WRITER
-    set_processing_info(writer, processing_info)
-    await transaction.commit(writer, partition, wait=x_synchronous)
-
+        raise HTTPException(
+            status_code=500,
+            detail="Error while sending to process. Try calling /reprocess",
+        )
     return ResourceFieldAdded(seqid=processing_info.seqid)
 
 
 @api.delete(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/{{field_type}}/{{field_id}}",
     status_code=204,
     name="Delete Resource field (by slug)",
     response_model_exclude_unset=True,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def delete_resource_field_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field_type: models.FieldTypeName,
+    field_id: str,
+):
+    return await _delete_resource_field(
+        request,
+        kbid,
+        field_type,
+        field_id,
+        rslug=rslug,
+    )
+
+
 @api.delete(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/{{field_type}}/{{field_id}}",
     status_code=204,
     name="Delete Resource field (by id)",
     response_model_exclude_unset=True,
     tags=["Resource fields"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def delete_resource_field(
+async def delete_resource_field_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field_type: models.FieldTypeName,
+    field_id: str,
+):
+    return await _delete_resource_field(request, kbid, field_type, field_id, rid=rid)
+
+
+async def _delete_resource_field(
     request: Request,
     kbid: str,
     field_type: models.FieldTypeName,
     field_id: str,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ):
     transaction = get_transaction_utility()
     partitioning = get_partitioning()
 
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
     partition = partitioning.generate_partition(kbid, rid)
@@ -584,39 +840,47 @@
     writer.kbid = kbid
     writer.uuid = rid
 
     pb_field_id = FieldID()
     pb_field_id.field_type = FIELD_TYPE_NAME_TO_FIELD_TYPE_MAP[field_type]
     pb_field_id.field = field_id
 
-    set_info_on_span({"nuclia.kbid": kbid, "nuclia.rid": rid})
-
     writer.delete_fields.append(pb_field_id)
     parse_audit(writer.audit, request)
 
-    await transaction.commit(writer, partition, wait=x_synchronous)
+    try:
+        await transaction.commit(writer, partition, wait=True)
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
+        )
 
     return Response(status_code=204)
 
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/file/{{field_id}}/reprocess",
     status_code=202,
     name="Reprocess file field (by id)",
     response_model=models.writer.ResourceUpdated,
     tags=["Resource fields"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
 async def reprocess_file_field(
     request: Request,
     kbid: str,
     rid: str,
     field_id: str,
     x_nucliadb_user: str = X_NUCLIADB_USER,
     x_file_password: Optional[str] = X_FILE_PASSWORD,
 ) -> ResourceUpdated:
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     transaction = get_transaction_utility()
     processing = get_processing()
     partitioning = get_partitioning()
 
     partition = partitioning.generate_partition(kbid, rid)
 
     toprocess = PushPayload(
@@ -626,47 +890,55 @@
         userid=x_nucliadb_user,
     )
 
     toprocess.kbid = kbid
     toprocess.uuid = rid
     toprocess.source = Source.HTTP
 
-    set_info_on_span({"nuclia.rid": rid, "nuclia.kbid": kbid})
-
     storage = await get_storage(service_name=SERVICE_NAME)
-    driver = await get_driver()
+    driver = get_driver()
 
     async with driver.transaction() as txn:
         kb = KnowledgeBox(txn, storage, kbid)
 
         resource = await kb.get(rid)
         if resource is None:
             raise HTTPException(status_code=404, detail="Resource does not exist")
 
+        if resource.basic is not None:
+            toprocess.title = resource.basic.title
+
         try:
             await extract_file_field(
                 field_id,
                 resource=resource,
                 toprocess=toprocess,
                 password=x_file_password,
             )
         except KeyError:
             raise HTTPException(status_code=404, detail="Field does not exist")
 
-    # Send current resource to reprocess.
-    try:
-        processing_info = await processing.send_to_process(toprocess, partition)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
     writer = BrokerMessage()
     writer.kbid = kbid
     writer.uuid = rid
     writer.source = BrokerMessage.MessageSource.WRITER
     writer.basic.metadata.useful = True
     writer.basic.metadata.status = Metadata.Status.PENDING
-    set_processing_info(writer, processing_info)
-    await transaction.commit(writer, partition, wait=False)
+    try:
+        await transaction.commit(writer, partition, wait=False)
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
+        )
+    # Send current resource to reprocess.
+    try:
+        processing_info = await processing.send_to_process(toprocess, partition)
+    except LimitsExceededError as exc:
+        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
+    except SendToProcessError:
+        raise HTTPException(
+            status_code=500,
+            detail="Error while sending to process. Try calling /reprocess",
+        )
 
     return ResourceUpdated(seqid=processing_info.seqid)
```

## nucliadb/writer/api/v1/knowledgebox.py

```diff
@@ -13,35 +13,38 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import asyncio
+import json
+
 from fastapi import HTTPException, Response
-from fastapi_versioning import version  # type: ignore
+from fastapi_versioning import version
 from nucliadb_protos.knowledgebox_pb2 import (
     DeleteKnowledgeBoxResponse,
     KnowledgeBoxID,
     KnowledgeBoxNew,
     KnowledgeBoxResponseStatus,
     KnowledgeBoxUpdate,
     NewKnowledgeBoxResponse,
     UpdateKnowledgeBoxResponse,
 )
 from starlette.requests import Request
 
 from nucliadb.writer.api.v1.router import KB_PREFIX, KBS_PREFIX, api
+from nucliadb.writer.utilities import get_processing
 from nucliadb_models.resource import (
     KnowledgeBoxConfig,
     KnowledgeBoxObj,
     KnowledgeBoxObjID,
     NucliaDBRoles,
 )
-from nucliadb_telemetry.utils import set_info_on_span
 from nucliadb_utils.authentication import requires
 from nucliadb_utils.utilities import get_ingest
 
 
 @api.post(
     f"/{KBS_PREFIX}",
     status_code=201,
@@ -51,41 +54,43 @@
     openapi_extra={"x-hidden-operation": True},
 )
 @requires(NucliaDBRoles.MANAGER)
 @version(1)
 async def create_kb(request: Request, item: KnowledgeBoxConfig):
     ingest = get_ingest()
     requestpb = KnowledgeBoxNew()
-    if item.slug:
-        requestpb.slug = item.slug
-
-    if item.title:
-        requestpb.config.title = item.title
-    if item.description:
-        requestpb.config.description = item.description
-    if item.similarity:
-        requestpb.similarity = item.similarity.to_pb()
-
-    requestpb.config.disable_vectors = item.disable_vectors
-
-    requestpb.config.enabled_filters.extend(item.enabled_filters)
-    requestpb.config.enabled_insights.extend(item.enabled_insights)
+    requestpb = parse_create_kb_request(item)
     kbobj: NewKnowledgeBoxResponse = await ingest.NewKnowledgeBox(requestpb)  # type: ignore
     if item.slug != "":
         slug = item.slug
     else:
         slug = kbobj.uuid  # type: ignore
     if kbobj.status == KnowledgeBoxResponseStatus.OK:
         return KnowledgeBoxObj(uuid=kbobj.uuid, slug=slug)
     elif kbobj.status == KnowledgeBoxResponseStatus.CONFLICT:
         raise HTTPException(status_code=419, detail="Knowledge box already exists")
     elif kbobj.status == KnowledgeBoxResponseStatus.ERROR:
         raise HTTPException(status_code=500, detail="Error on creating knowledge box")
 
 
+def parse_create_kb_request(item: KnowledgeBoxConfig) -> KnowledgeBoxNew:
+    requestpb = KnowledgeBoxNew()
+    if item.slug:
+        requestpb.slug = item.slug
+    if item.title:
+        requestpb.config.title = item.title
+    if item.description:
+        requestpb.config.description = item.description
+    if item.release_channel:
+        requestpb.release_channel = item.release_channel.to_pb()
+    if item.learning_configuration:
+        requestpb.learning_config = json.dumps(item.learning_configuration)
+    return requestpb
+
+
 @api.patch(
     f"/{KB_PREFIX}/{{kbid}}",
     status_code=200,
     name="Update Knowledge Box",
     response_model=KnowledgeBoxObjID,
     tags=["Knowledge Boxes"],
     openapi_extra={"x-hidden-operation": True},
@@ -93,30 +98,18 @@
 @requires(NucliaDBRoles.MANAGER)
 @version(1)
 async def update_kb(request: Request, kbid: str, item: KnowledgeBoxConfig):
     ingest = get_ingest()
     pbrequest = KnowledgeBoxUpdate(uuid=kbid)
     if item.slug is not None:
         pbrequest.slug = item.slug
-
-    for filter_option in item.enabled_filters:
-        pbrequest.config.enabled_filters.append(filter_option)
-    for insight_option in item.enabled_insights:
-        pbrequest.config.enabled_insights.append(insight_option)
-
     if item.title:
         pbrequest.config.title = item.title
-
-    pbrequest.config.disable_vectors = item.disable_vectors
-
     if item.description:
         pbrequest.config.description = item.description
-
-    set_info_on_span({"nuclia.kbid": kbid})
-
     kbobj: UpdateKnowledgeBoxResponse = await ingest.UpdateKnowledgeBox(pbrequest)  # type: ignore
     if kbobj.status == KnowledgeBoxResponseStatus.OK:
         return KnowledgeBoxObjID(uuid=kbobj.uuid)
     elif kbobj.status == KnowledgeBoxResponseStatus.NOTFOUND:
         raise HTTPException(status_code=404, detail="Knowledge box does not exist")
     elif kbobj.status == KnowledgeBoxResponseStatus.ERROR:
         raise HTTPException(status_code=500, detail="Error on creating knowledge box")
@@ -131,20 +124,21 @@
     openapi_extra={"x-hidden-operation": True},
 )
 @requires(NucliaDBRoles.MANAGER)
 @version(1)
 async def delete_kb(request: Request, kbid: str):
     ingest = get_ingest()
 
-    set_info_on_span({"nuclia.kbid": kbid})
-
     kbobj: DeleteKnowledgeBoxResponse = await ingest.DeleteKnowledgeBox(  # type: ignore
         KnowledgeBoxID(uuid=kbid)
     )
     if kbobj.status == KnowledgeBoxResponseStatus.OK:
         return KnowledgeBoxObj(uuid=kbid)
     elif kbobj.status == KnowledgeBoxResponseStatus.NOTFOUND:
         raise HTTPException(status_code=404, detail="Knowledge Box does not exists")
     elif kbobj.status == KnowledgeBoxResponseStatus.ERROR:
         raise HTTPException(status_code=500, detail="Error on deleting knowledge box")
 
+    processing = get_processing()
+    asyncio.create_task(processing.delete_from_processing(kbid=kbid))
+
     return Response(status_code=204)
```

## nucliadb/writer/api/v1/resource.py

```diff
@@ -13,89 +13,82 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import asyncio
+import contextlib
 from time import time
-from typing import TYPE_CHECKING, Optional
+from typing import Optional
 from uuid import uuid4
 
 from fastapi import HTTPException, Query, Response
-from fastapi.params import Header
-from fastapi_versioning import version  # type: ignore
+from fastapi_versioning import version
 from grpc import StatusCode as GrpcStatusCode
-from grpc.aio import AioRpcError  # type: ignore
+from grpc.aio import AioRpcError
 from nucliadb_protos.resources_pb2 import Metadata
 from nucliadb_protos.writer_pb2 import (
     BrokerMessage,
     IndexResource,
     ResourceFieldExistsResponse,
     ResourceFieldId,
     ResourceIdRequest,
     ResourceIdResponse,
 )
 from starlette.requests import Request
 
+from nucliadb.common import datamanagers
+from nucliadb.common.context.fastapi import get_app_context
+from nucliadb.common.maindb.driver import Driver
+from nucliadb.common.maindb.exceptions import ConflictError, NotFoundError
+from nucliadb.common.maindb.utils import get_driver
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
-from nucliadb.ingest.processing import PushPayload, Source
-from nucliadb.ingest.utils import get_driver
-from nucliadb.writer import SERVICE_NAME
+from nucliadb.ingest.processing import ProcessingInfo, PushPayload, Source
+from nucliadb.writer import SERVICE_NAME, logger
+from nucliadb.writer.api.constants import SKIP_STORE_DEFAULT, X_NUCLIADB_USER
 from nucliadb.writer.api.v1.router import (
     KB_PREFIX,
     RESOURCE_PREFIX,
     RESOURCES_PREFIX,
     RSLUG_PREFIX,
     api,
 )
+from nucliadb.writer.back_pressure import maybe_back_pressure
 from nucliadb.writer.exceptions import IngestNotAvailable
 from nucliadb.writer.resource.audit import parse_audit
 from nucliadb.writer.resource.basic import (
     parse_basic,
     parse_basic_modify,
-    set_processing_info,
     set_status,
     set_status_modify,
 )
 from nucliadb.writer.resource.field import extract_fields, parse_fields
-from nucliadb.writer.resource.origin import parse_origin
+from nucliadb.writer.resource.origin import parse_extra, parse_origin
 from nucliadb.writer.resource.slug import resource_slug_exists
-from nucliadb.writer.resource.vectors import (
-    create_vectorset,
-    get_vectorsets,
-    parse_vectors,
-)
 from nucliadb.writer.utilities import get_processing
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_models.writer import (
     CreateResourcePayload,
     ResourceCreated,
     ResourceUpdated,
     UpdateResourcePayload,
 )
-from nucliadb_telemetry.utils import set_info_on_span
+from nucliadb_telemetry.errors import capture_exception
 from nucliadb_utils.authentication import requires
 from nucliadb_utils.exceptions import LimitsExceededError, SendToProcessError
+from nucliadb_utils.transaction import TransactionCommitTimeoutError
 from nucliadb_utils.utilities import (
     get_ingest,
     get_partitioning,
     get_storage,
     get_transaction_utility,
 )
 
-if TYPE_CHECKING:  # pragma: no cover
-    SKIP_STORE_DEFAULT = False
-    SYNC_CALL = False
-    X_NUCLIADB_USER = ""
-else:
-    SKIP_STORE_DEFAULT = Header(False)
-    SYNC_CALL = Header(False)
-    X_NUCLIADB_USER = Header("")
-
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCES_PREFIX}",
     status_code=201,
     name="Create Resource",
     description="Create a new Resource in a Knowledge Box",
     response_model=ResourceCreated,
@@ -105,18 +98,18 @@
 @requires(NucliaDBRoles.WRITER)
 @version(1)
 async def create_resource(
     request: Request,
     item: CreateResourcePayload,
     kbid: str,
     x_skip_store: bool = SKIP_STORE_DEFAULT,
-    x_synchronous: bool = SYNC_CALL,
 ):
+    await maybe_back_pressure(request, kbid)
+
     transaction = get_transaction_utility()
-    processing = get_processing()
     partitioning = get_partitioning()
 
     # Create resource message
     uuid = uuid4().hex
     partition = partitioning.generate_partition(kbid, uuid)
 
     writer = BrokerMessage()
@@ -129,118 +122,161 @@
     )
 
     writer.kbid = kbid
     toprocess.kbid = kbid
     writer.uuid = uuid
     toprocess.uuid = uuid
     toprocess.source = Source.HTTP
+    toprocess.title = item.title
 
     if item.slug:
         if await resource_slug_exists(kbid, item.slug):
-            raise HTTPException(status_code=409, detail="Resource slug already exists")
+            raise HTTPException(
+                status_code=409, detail=f"Resource slug {item.slug} already exists"
+            )
         writer.slug = item.slug
         toprocess.slug = item.slug
 
     parse_audit(writer.audit, request)
     parse_basic(writer, item, toprocess)
 
     if item.origin is not None:
         parse_origin(writer.origin, item.origin)
+    if item.extra is not None:
+        parse_extra(writer.extra, item.extra)
 
     await parse_fields(
         writer=writer,
         item=item,
         toprocess=toprocess,
         kbid=kbid,
         uuid=uuid,
         x_skip_store=x_skip_store,
     )
 
-    if item.uservectors:
-        vectorsets = await get_vectorsets(kbid)
-        if vectorsets and len(vectorsets.vectorsets):
-            parse_vectors(writer, item.uservectors, vectorsets)
-        else:
-            for vector in item.uservectors:
-                if vector.vectors is not None:
-                    for vectorset, uservector in vector.vectors.items():
-                        if len(uservector) == 0:
-                            raise HTTPException(
-                                status_code=412,
-                                detail=str("Vectorset without vector not allowed"),
-                            )
-                        first_vector = list(uservector.values())[0]
-                        await create_vectorset(
-                            kbid, vectorset, len(first_vector.vector)
-                        )
-            vectorsets = await get_vectorsets(kbid)
-            if vectorsets is None or len(vectorsets.vectorsets) == 0:
-                raise HTTPException(
-                    status_code=412,
-                    detail=str("Vectorset was not able to be created"),
-                )
-            parse_vectors(writer, item.uservectors, vectorsets)
-
     set_status(writer.basic, item)
 
-    set_info_on_span({"nuclia.rid": uuid, "nuclia.kbid": kbid})
-
-    try:
-        processing_info = await processing.send_to_process(toprocess, partition)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
     writer.source = BrokerMessage.MessageSource.WRITER
-    set_processing_info(writer, processing_info)
-    if x_synchronous:
+    try:
         t0 = time()
-    await transaction.commit(writer, partition, wait=x_synchronous)
-
-    if x_synchronous:
-        return ResourceCreated(
-            seqid=processing_info.seqid, uuid=uuid, elapsed=time() - t0
+        await transaction.commit(writer, partition, wait=True)
+        txn_time = time() - t0
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
         )
-    else:
-        return ResourceCreated(seqid=processing_info.seqid, uuid=uuid)
+
+    seqid = await maybe_send_to_process(toprocess, partition)
+
+    return ResourceCreated(seqid=seqid, uuid=uuid, elapsed=txn_time)
 
 
 @api.patch(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}",
     status_code=200,
     name="Modify Resource (by slug)",
     response_model=ResourceUpdated,
     tags=["Resources"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def modify_resource_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    item: UpdateResourcePayload,
+    x_skip_store: bool = SKIP_STORE_DEFAULT,
+    x_nucliadb_user: str = X_NUCLIADB_USER,
+):
+    return await modify_resource_endpoint(
+        request,
+        item,
+        kbid,
+        path_rslug=rslug,
+        x_skip_store=x_skip_store,
+        x_nucliadb_user=x_nucliadb_user,
+    )
+
+
 @api.patch(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}",
     status_code=200,
     name="Modify Resource (by id)",
     response_model=ResourceUpdated,
     tags=["Resources"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def modify_resource(
+async def modify_resource_rid_prefix(
     request: Request,
-    item: UpdateResourcePayload,
     kbid: str,
-    rid: Optional[str] = None,
-    rslug: Optional[str] = None,
+    rid: str,
+    item: UpdateResourcePayload,
     x_skip_store: bool = SKIP_STORE_DEFAULT,
-    x_synchronous: bool = SYNC_CALL,
     x_nucliadb_user: str = X_NUCLIADB_USER,
 ):
+    return await modify_resource_endpoint(
+        request,
+        item,
+        kbid,
+        path_rid=rid,
+        x_skip_store=x_skip_store,
+        x_nucliadb_user=x_nucliadb_user,
+    )
+
+
+async def modify_resource_endpoint(
+    request: Request,
+    item: UpdateResourcePayload,
+    kbid: str,
+    x_skip_store: bool,
+    x_nucliadb_user: str,
+    path_rid: Optional[str] = None,
+    path_rslug: Optional[str] = None,
+):
+    resource_uuid = await get_rid_from_params_or_raise_error(kbid, path_rid, path_rslug)
+
+    await maybe_back_pressure(request, kbid, resource_uuid=resource_uuid)
+
+    if item.slug is None:
+        return await modify_resource(
+            request,
+            item,
+            kbid,
+            x_skip_store=x_skip_store,
+            x_nucliadb_user=x_nucliadb_user,
+            rid=resource_uuid,
+        )
+
+    async with safe_update_resource_slug(
+        request, kbid, rid=resource_uuid, new_slug=item.slug
+    ):
+        return await modify_resource(
+            request,
+            item,
+            kbid,
+            x_skip_store=x_skip_store,
+            x_nucliadb_user=x_nucliadb_user,
+            rid=resource_uuid,
+        )
+
+
+async def modify_resource(
+    request: Request,
+    item: UpdateResourcePayload,
+    kbid: str,
+    x_skip_store: bool,
+    x_nucliadb_user: str,
+    *,
+    rid: str,
+):
     transaction = get_transaction_utility()
-    processing = get_processing()
     partitioning = get_partitioning()
 
-    rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
-
     partition = partitioning.generate_partition(kbid, rid)
 
     writer = BrokerMessage()
     toprocess = PushPayload(
         uuid=rid,
         kbid=kbid,
         partition=partition,
@@ -250,206 +286,312 @@
 
     writer.kbid = kbid
     writer.uuid = rid
     toprocess.kbid = kbid
     toprocess.uuid = rid
     toprocess.source = Source.HTTP
 
-    set_info_on_span({"nuclia.rid": rid, "nuclia.kbid": kbid})
-
     parse_basic_modify(writer, item, toprocess)
     parse_audit(writer.audit, request)
     if item.origin is not None:
         parse_origin(writer.origin, item.origin)
+    if item.extra is not None:
+        parse_extra(writer.extra, item.extra)
 
     await parse_fields(
         writer=writer,
         item=item,
         toprocess=toprocess,
         kbid=kbid,
         uuid=rid,
         x_skip_store=x_skip_store,
     )
-    if item.uservectors:
-        vectorsets = await get_vectorsets(kbid)
-        if vectorsets:
-            parse_vectors(writer, item.uservectors, vectorsets)
-        else:
-            raise HTTPException(status_code=412, detail=str("No vectorsets found"))
-
     set_status_modify(writer.basic, item)
-    try:
-        processing_info = await processing.send_to_process(toprocess, partition)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
+
+    toprocess.title = writer.basic.title
 
     writer.source = BrokerMessage.MessageSource.WRITER
-    set_processing_info(writer, processing_info)
 
-    await transaction.commit(writer, partition, wait=x_synchronous)
+    maybe_mark_reindex(writer, item)
 
-    return ResourceUpdated(seqid=processing_info.seqid)
+    try:
+        await transaction.commit(writer, partition, wait=True)
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
+        )
+
+    seqid = await maybe_send_to_process(toprocess, partition)
+
+    return ResourceUpdated(seqid=seqid)
+
+
+@contextlib.asynccontextmanager
+async def safe_update_resource_slug(
+    request,
+    kbid: str,
+    *,
+    rid: str,
+    new_slug: str,
+):
+    driver = get_app_context(request.app).kv_driver
+    try:
+        old_slug = await update_resource_slug(driver, kbid, rid=rid, new_slug=new_slug)
+    except NotFoundError:
+        raise HTTPException(status_code=404, detail=f"Resource does not exist: {rid}")
+    except ConflictError:
+        raise HTTPException(status_code=409, detail=f"Slug already exists: {new_slug}")
+    try:
+        yield
+    except Exception:
+        # Rollback slug update if something goes wrong
+        try:
+            await update_resource_slug(driver, kbid, rid=rid, new_slug=old_slug)
+        except Exception as ex:
+            capture_exception(ex)
+            logger.exception("Error while rolling back slug update")
+        raise
+
+
+async def update_resource_slug(
+    driver: Driver,
+    kbid: str,
+    *,
+    rid: str,
+    new_slug: str,
+):
+    async with driver.transaction() as txn:
+        old_slug = await datamanagers.resources.modify_slug(
+            txn, kbid=kbid, rid=rid, new_slug=new_slug
+        )
+        await txn.commit()
+        return old_slug
 
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/reprocess",
     status_code=202,
     name="Reprocess resource (by slug)",
     response_model=ResourceUpdated,
     tags=["Resources"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def reprocess_resource_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    x_nucliadb_user: str = X_NUCLIADB_USER,
+):
+    return await _reprocess_resource(
+        request, kbid, rslug=rslug, x_nucliadb_user=x_nucliadb_user
+    )
+
+
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/reprocess",
     status_code=202,
     name="Reprocess resource (by id)",
     response_model=ResourceUpdated,
     tags=["Resources"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def reprocess_resource(
+async def reprocess_resource_rid_prefix(
     request: Request,
     kbid: str,
+    rid: str,
+    x_nucliadb_user: str = X_NUCLIADB_USER,
+):
+    return await _reprocess_resource(
+        request, kbid, rid=rid, x_nucliadb_user=x_nucliadb_user
+    )
+
+
+async def _reprocess_resource(
+    request: Request,
+    kbid: str,
+    x_nucliadb_user: str,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_nucliadb_user: str = X_NUCLIADB_USER,
 ):
     transaction = get_transaction_utility()
-    processing = get_processing()
     partitioning = get_partitioning()
 
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     partition = partitioning.generate_partition(kbid, rid)
 
     toprocess = PushPayload(
         uuid=rid,
         kbid=kbid,
         partition=partition,
         userid=x_nucliadb_user,
     )
 
     toprocess.kbid = kbid
     toprocess.uuid = rid
     toprocess.source = Source.HTTP
 
-    set_info_on_span({"nuclia.rid": rid, "nuclia.kbid": kbid})
-
     storage = await get_storage(service_name=SERVICE_NAME)
-    driver = await get_driver()
-
-    txn = await driver.begin()
-    kb = KnowledgeBox(txn, storage, kbid)
-
-    resource = await kb.get(rid)
-    if resource is None:
-        raise HTTPException(status_code=404, detail="Resource does not exist")
+    driver = get_driver()
 
-    await extract_fields(resource=resource, toprocess=toprocess)
+    async with driver.transaction() as txn:
+        kb = KnowledgeBox(txn, storage, kbid)
 
-    if txn.open:
-        await txn.abort()
+        resource = await kb.get(rid)
+        if resource is None:
+            raise HTTPException(status_code=404, detail="Resource does not exist")
 
-    # Send current resource to reprocess.
-
-    try:
-        processing_info = await processing.send_to_process(toprocess, partition)
-    except LimitsExceededError as exc:
-        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
-    except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
+        await extract_fields(resource=resource, toprocess=toprocess)
 
     writer = BrokerMessage()
     writer.kbid = kbid
     writer.uuid = rid
     writer.source = BrokerMessage.MessageSource.WRITER
     writer.basic.metadata.useful = True
     writer.basic.metadata.status = Metadata.Status.PENDING
-    set_processing_info(writer, processing_info)
-    await transaction.commit(writer, partition, wait=False)
+    try:
+        await transaction.commit(writer, partition, wait=False)
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
+        )
+
+    processing_info = await send_to_process(toprocess, partition)
 
     return ResourceUpdated(seqid=processing_info.seqid)
 
 
 @api.delete(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}",
     status_code=204,
     name="Delete Resource (by slug)",
     tags=["Resources"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def delete_resource_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+):
+    return await _delete_resource(request, kbid, rslug=rslug)
+
+
 @api.delete(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}",
     status_code=204,
     name="Delete Resource (by id)",
     tags=["Resources"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def delete_resource(
+async def delete_resource_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+):
+    return await _delete_resource(request, kbid, rid=rid)
+
+
+async def _delete_resource(
     request: Request,
     kbid: str,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    x_synchronous: bool = SYNC_CALL,
 ):
     transaction = get_transaction_utility()
     partitioning = get_partitioning()
 
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
-    set_info_on_span({"nuclia.kbid": kbid, "nucliadb.rid": rid})
-
     partition = partitioning.generate_partition(kbid, rid)
     writer = BrokerMessage()
 
     writer.kbid = kbid
     writer.uuid = rid
     writer.type = BrokerMessage.MessageType.DELETE
 
     parse_audit(writer.audit, request)
 
     # Create processing message
-    await transaction.commit(writer, partition, wait=x_synchronous)
+    try:
+        await transaction.commit(writer, partition, wait=True)
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
+        )
+
+    processing = get_processing()
+    asyncio.create_task(processing.delete_from_processing(kbid=kbid, resource_id=rid))
 
     return Response(status_code=204)
 
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/reindex",
     status_code=204,
     name="Reindex Resource (by slug)",
     tags=["Resources"],
 )
+@requires(NucliaDBRoles.WRITER)
+@version(1)
+async def reindex_resource_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    reindex_vectors: bool = Query(False),
+):
+    return await _reindex_resource(
+        request, kbid, rslug=rslug, reindex_vectors=reindex_vectors
+    )
+
+
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/reindex",
     status_code=204,
     name="Reindex Resource (by id)",
     tags=["Resources"],
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
-async def reindex_resource(
+async def reindex_resource_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    reindex_vectors: bool = Query(False),
+):
+    return await _reindex_resource(
+        request, kbid, rid=rid, reindex_vectors=reindex_vectors
+    )
+
+
+async def _reindex_resource(
     request: Request,
     kbid: str,
+    reindex_vectors: bool,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
-    reindex_vectors: bool = Query(False),
 ):
     rid = await get_rid_from_params_or_raise_error(kbid, rid, rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=rid)
+
     ingest = get_ingest()
     index_req = IndexResource()
     index_req.kbid = kbid
     index_req.rid = rid
     index_req.reindex_vectors = reindex_vectors
 
-    set_info_on_span({"nuclia.rid": rid, "nuclia.kbid": kbid})
-
     await ingest.ReIndex(index_req)  # type: ignore
     return Response(status_code=200)
 
 
 async def get_resource_uuid_from_slug(kbid: str, slug: str) -> str:
     ingest = get_ingest()
     pbrequest = ResourceIdRequest()
@@ -493,7 +635,68 @@
         raise ValueError("Either rid or slug must be set")
 
     rid = await get_resource_uuid_from_slug(kbid, slug)
     if not rid:
         raise HTTPException(status_code=404, detail="Resource does not exist")
 
     return rid
+
+
+def maybe_mark_reindex(message: BrokerMessage, item: UpdateResourcePayload):
+    if needs_resource_reindex(item):
+        message.reindex = True
+
+
+def needs_resource_reindex(item: UpdateResourcePayload) -> bool:
+    # Some metadata need to be applied as tags to all fields of
+    # a resource and that means this message should force reindexing everything.
+    # XXX This is not ideal. Long term, we should handle it differently
+    # so this is not required
+    return item.usermetadata is not None or (
+        item.origin is not None
+        and (
+            item.origin.created is not None
+            or item.origin.modified is not None
+            or item.origin.metadata is not None
+        )
+    )
+
+
+async def maybe_send_to_process(toprocess: PushPayload, partition) -> Optional[int]:
+    if not needs_reprocess(toprocess):
+        return None
+
+    processing_info = await send_to_process(toprocess, partition)
+    return processing_info.seqid
+
+
+async def send_to_process(toprocess: PushPayload, partition) -> ProcessingInfo:
+    try:
+        processing = get_processing()
+        processing_info = await processing.send_to_process(toprocess, partition)
+        return processing_info
+    except LimitsExceededError as exc:
+        raise HTTPException(status_code=exc.status_code, detail=exc.detail)
+    except SendToProcessError:
+        raise HTTPException(
+            status_code=500,
+            detail="Error while sending to process. Try calling /reprocess",
+        )
+
+
+def needs_reprocess(processing_payload: PushPayload) -> bool:
+    """
+    Processing only pays attention to when there are fields to process,
+    so sometimes we can skip sending to processing, for instance if a
+    resource/paragraph is being annotated.
+    """
+    for field in (
+        "genericfield",
+        "filefield",
+        "linkfield",
+        "textfield",
+        "layoutfield",
+        "conversationfield",
+    ):
+        if len(getattr(processing_payload, field)) > 0:
+            return True
+    return False
```

## nucliadb/writer/api/v1/services.py

```diff
@@ -25,83 +25,39 @@
 from nucliadb_protos.writer_pb2 import (
     DelEntitiesRequest,
     DelLabelsRequest,
     DelVectorSetRequest,
     NewEntitiesGroupRequest,
     NewEntitiesGroupResponse,
     OpStatusWriter,
-    SetEntitiesRequest,
     SetLabelsRequest,
     SetSynonymsRequest,
     UpdateEntitiesGroupRequest,
     UpdateEntitiesGroupResponse,
 )
 from starlette.requests import Request
 
 from nucliadb.models.responses import (
     HTTPConflict,
     HTTPInternalServerError,
     HTTPNotFound,
 )
 from nucliadb.writer.api.v1.router import KB_PREFIX, api
-from nucliadb.writer.resource.vectors import create_vectorset  # type: ignore
+from nucliadb.writer.vectors import create_vectorset
 from nucliadb_models.entities import (
     CreateEntitiesGroupPayload,
-    EntitiesGroup,
     UpdateEntitiesGroupPayload,
 )
 from nucliadb_models.labels import LabelSet
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_models.synonyms import KnowledgeBoxSynonyms
 from nucliadb_models.vectors import VectorSet
-from nucliadb_telemetry.utils import set_info_on_span
+from nucliadb_utils import const
 from nucliadb_utils.authentication import requires
-from nucliadb_utils.utilities import get_ingest
-
-
-@api.post(
-    f"/{KB_PREFIX}/{{kbid}}/entitiesgroup/{{group}}",
-    status_code=200,
-    name="Set Knowledge Box Entities",
-    tags=["Knowledge Box Services"],
-    openapi_extra={"x-operation_order": 1},
-    deprecated=True,
-)
-@requires(NucliaDBRoles.WRITER)
-@version(1)
-async def set_entities(request: Request, kbid: str, group: str, item: EntitiesGroup):
-    ingest = get_ingest()
-    pbrequest: SetEntitiesRequest = SetEntitiesRequest()
-    pbrequest.kb.uuid = kbid
-    pbrequest.group = group
-    if item.title:
-        pbrequest.entities.title = item.title
-    if item.color:
-        pbrequest.entities.color = item.color
-
-    pbrequest.entities.custom = item.custom
-
-    for key, entity in item.entities.items():
-        entitypb = pbrequest.entities.entities[key]
-        entitypb.value = entity.value
-        entitypb.merged = entity.merged
-        entitypb.deleted = False
-        entitypb.represents.extend(entity.represents)
-
-    set_info_on_span({"nuclia.kbid": kbid})
-
-    status: OpStatusWriter = await ingest.SetEntities(pbrequest)  # type: ignore
-    if status.status == OpStatusWriter.Status.OK:
-        return None
-    elif status.status == OpStatusWriter.Status.NOTFOUND:
-        raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
-    elif status.status == OpStatusWriter.Status.ERROR:
-        raise HTTPException(
-            status_code=500, detail="Error on settings entities on a Knowledge box"
-        )
+from nucliadb_utils.utilities import get_ingest, has_feature
 
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/entitiesgroups",
     status_code=200,
     name="Create Knowledge Box Entities Group",
     tags=["Knowledge Box Services"],
@@ -126,16 +82,14 @@
     for key, entity in item.entities.items():
         entitypb = pbrequest.entities.entities[key]
         entitypb.value = entity.value
         entitypb.merged = entity.merged
         entitypb.deleted = False
         entitypb.represents.extend(entity.represents)
 
-    set_info_on_span({"nuclia.kbid": kbid})
-
     status: NewEntitiesGroupResponse = await ingest.NewEntitiesGroup(pbrequest)  # type: ignore
     if status.status == NewEntitiesGroupResponse.Status.OK:
         return
     elif status.status == NewEntitiesGroupResponse.Status.KB_NOT_FOUND:
         return HTTPNotFound(detail="Knowledge Box does not exist")
     elif status.status == NewEntitiesGroupResponse.Status.ALREADY_EXISTS:
         return HTTPConflict(
@@ -177,16 +131,14 @@
         entitypb = pbrequest.update[name]
         entitypb.value = entity.value
         entitypb.merged = entity.merged
         entitypb.represents.extend(entity.represents)
 
     pbrequest.delete.extend(item.delete)
 
-    set_info_on_span({"nuclia.kbid": kbid})
-
     status: UpdateEntitiesGroupResponse = await ingest.UpdateEntitiesGroup(pbrequest)  # type: ignore
     if status.status == UpdateEntitiesGroupResponse.Status.OK:
         return
     elif status.status == UpdateEntitiesGroupResponse.Status.KB_NOT_FOUND:
         return HTTPNotFound(detail="Knowledge Box does not exist")
     elif status.status == UpdateEntitiesGroupResponse.Status.ENTITIES_GROUP_NOT_FOUND:
         return HTTPNotFound(detail="Entities group does not exist")
@@ -207,16 +159,14 @@
 @version(1)
 async def delete_entities(request: Request, kbid: str, group: str):
     ingest = get_ingest()
     pbrequest: DelEntitiesRequest = DelEntitiesRequest()
     pbrequest.kb.uuid = kbid
     pbrequest.group = group
 
-    set_info_on_span({"nuclia.kbid": kbid})
-
     status: OpStatusWriter = await ingest.DelEntities(pbrequest)  # type: ignore
     if status.status == OpStatusWriter.Status.OK:
         return None
     elif status.status == OpStatusWriter.Status.NOTFOUND:
         raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
     elif status.status == OpStatusWriter.Status.ERROR:
         raise HTTPException(
@@ -236,16 +186,14 @@
 @requires(NucliaDBRoles.WRITER)
 @version(1)
 async def set_labels(request: Request, kbid: str, labelset: str, item: LabelSet):
     ingest = get_ingest()
     pbrequest: SetLabelsRequest = SetLabelsRequest(id=labelset)
     pbrequest.kb.uuid = kbid
 
-    set_info_on_span({"nuclia.kbid": kbid})
-
     if item.title:
         pbrequest.labelset.title = item.title
 
     if item.color:
         pbrequest.labelset.color = item.color
 
     pbrequest.labelset.multiple = item.multiple
@@ -284,15 +232,14 @@
 @requires(NucliaDBRoles.WRITER)
 @version(1)
 async def delete_labels(request: Request, kbid: str, labelset: str):
     ingest = get_ingest()
     pbrequest: DelLabelsRequest = DelLabelsRequest()
     pbrequest.kb.uuid = kbid
     pbrequest.id = labelset
-    set_info_on_span({"nuclia.kbid": kbid})
     status: OpStatusWriter = await ingest.DelLabels(pbrequest)  # type: ignore
     if status.status == OpStatusWriter.Status.OK:
         return None
     elif status.status == OpStatusWriter.Status.NOTFOUND:
         raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
     elif status.status == OpStatusWriter.Status.ERROR:
         raise HTTPException(
@@ -307,32 +254,41 @@
     name="Set Knowledge Box VectorSet",
     tags=["Knowledge Box Services"],
     openapi_extra={"x-operation_order": 1},
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
 async def set_vectorset(request: Request, kbid: str, vectorset: str, item: VectorSet):
+    if not has_feature(const.Features.VECTORSETS_V2, context={"kbid": kbid}):
+        raise HTTPException(
+            status_code=404,
+            detail="Vectorsets API is not yet implemented",
+        )
     await create_vectorset(kbid, vectorset, item.dimension, similarity=item.similarity)
 
 
 @api.delete(
     f"/{KB_PREFIX}/{{kbid}}/vectorset/{{vectorset}}",
     status_code=200,
     name="Delete Knowledge Box VectorSet",
     tags=["Knowledge Box Services"],
     openapi_extra={"x-operation_order": 3},
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
 async def delete_vectorset(request: Request, kbid: str, vectorset: str):
+    if not has_feature(const.Features.VECTORSETS_V2, context={"kbid": kbid}):
+        raise HTTPException(
+            status_code=404,
+            detail="Vectorsets API is not yet implemented",
+        )
     ingest = get_ingest()
     pbrequest: DelVectorSetRequest = DelVectorSetRequest()
     pbrequest.kb.uuid = kbid
     pbrequest.vectorset = vectorset
-    set_info_on_span({"nuclia.kbid": kbid})
     status: OpStatusWriter = await ingest.DelVectorSet(pbrequest)  # type: ignore
     if status.status == OpStatusWriter.Status.OK:
         return None
     elif status.status == OpStatusWriter.Status.NOTFOUND:
         raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
     elif status.status == OpStatusWriter.Status.ERROR:
         raise HTTPException(
@@ -347,15 +303,14 @@
     name="Set Knowledge Box Custom Synonyms",
     tags=["Knowledge Box Services"],
     openapi_extra={"x-operation_order": 1},
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
 async def set_custom_synonyms(request: Request, kbid: str, item: KnowledgeBoxSynonyms):
-    set_info_on_span({"nuclia.kbid": kbid})
     ingest = get_ingest()
     pbrequest = SetSynonymsRequest()
     pbrequest.kbid.uuid = kbid
     pbrequest.synonyms.CopyFrom(item.to_message())
     status: OpStatusWriter = await ingest.SetSynonyms(pbrequest)  # type: ignore
     if status.status == OpStatusWriter.Status.OK:
         return Response(status_code=204)
@@ -373,15 +328,14 @@
     name="Delete Knowledge Box Custom Synonyms",
     tags=["Knowledge Box Services"],
     openapi_extra={"x-operation_order": 3},
 )
 @requires(NucliaDBRoles.WRITER)
 @version(1)
 async def delete_custom_synonyms(request: Request, kbid: str):
-    set_info_on_span({"nuclia.kbid": kbid})
     ingest = get_ingest()
     pbrequest = KnowledgeBoxID(uuid=kbid)
     status: OpStatusWriter = await ingest.DelSynonyms(pbrequest)  # type: ignore
     if status.status == OpStatusWriter.Status.OK:
         return Response(status_code=204)
     elif status.status == OpStatusWriter.Status.NOTFOUND:
         raise HTTPException(status_code=404, detail="Knowledge Box does not exist")
```

## nucliadb/writer/api/v1/upload.py

```diff
@@ -20,61 +20,65 @@
 import base64
 import mimetypes
 import pickle
 import uuid
 from datetime import datetime
 from hashlib import md5
 from io import BytesIO
-from typing import List, Optional
+from typing import Optional
 
 from fastapi import HTTPException
 from fastapi.params import Header
 from fastapi.requests import Request
 from fastapi.responses import Response
 from fastapi_versioning import version  # type: ignore
 from grpc import StatusCode as GrpcStatusCode
-from grpc.aio import AioRpcError  # type: ignore
+from grpc.aio import AioRpcError
 from nucliadb_protos.resources_pb2 import FieldFile
 from nucliadb_protos.writer_pb2 import (
     BrokerMessage,
     ResourceFieldExistsResponse,
     ResourceFieldId,
 )
 from starlette.requests import Request as StarletteRequest
 
 from nucliadb.ingest.orm.utils import set_title
 from nucliadb.ingest.processing import PushPayload, Source
+from nucliadb.models.responses import HTTPClientError
 from nucliadb.writer import SERVICE_NAME
 from nucliadb.writer.api.v1.resource import get_rid_from_params_or_raise_error
+from nucliadb.writer.back_pressure import maybe_back_pressure
 from nucliadb.writer.exceptions import (
     ConflictError,
     IngestNotAvailable,
     ResourceNotFound,
 )
 from nucliadb.writer.resource.audit import parse_audit
-from nucliadb.writer.resource.basic import parse_basic, set_processing_info
+from nucliadb.writer.resource.basic import parse_basic
 from nucliadb.writer.resource.field import parse_fields
-from nucliadb.writer.resource.origin import parse_origin
+from nucliadb.writer.resource.origin import parse_extra, parse_origin
 from nucliadb.writer.tus import TUSUPLOAD, UPLOAD, get_dm, get_storage_manager
 from nucliadb.writer.tus.exceptions import (
     HTTPBadRequest,
     HTTPConflict,
     HTTPNotFound,
     HTTPPreconditionFailed,
     HTTPServiceUnavailable,
     InvalidTUSMetadata,
+    ResumableURINotAvailable,
 )
+from nucliadb.writer.tus.storage import FileStorageManager  # type: ignore
 from nucliadb.writer.tus.utils import parse_tus_metadata
 from nucliadb.writer.utilities import get_processing
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_models.writer import CreateResourcePayload, ResourceFileUploaded
-from nucliadb_telemetry.utils import set_info_on_span
 from nucliadb_utils.authentication import requires_one
 from nucliadb_utils.exceptions import LimitsExceededError, SendToProcessError
 from nucliadb_utils.storages.storage import KB_RESOURCE_FIELD
+from nucliadb_utils.transaction import TransactionCommitTimeoutError
 from nucliadb_utils.utilities import (
     get_ingest,
     get_partitioning,
     get_storage,
     get_transaction_utility,
 )
 
@@ -109,69 +113,108 @@
 @api.options(
     f"/{KB_PREFIX}/{{kbid}}/{TUSUPLOAD}",
     tags=["Knowledge Box TUS uploads"],
     name="TUS Server information",
     openapi_extra={"x-operation-order": 4},
 )
 @version(1)
-async def options_single(
+def tus_options(
     request: Request,
     kbid: str,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
     upload_id: Optional[str] = None,
     field: Optional[str] = None,
 ) -> Response:
+    return _tus_options()
+
+
+def _tus_options() -> Response:
     """
     Gather information about the Server’s current configuration such as enabled extensions, version...
     """
     resp = Response(headers=TUS_HEADERS, status_code=204)
     return resp
 
 
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/file/{{field}}/{TUSUPLOAD}",
     tags=["Resource field TUS uploads"],
     name="Create new upload on a Resource (by slug)",
     openapi_extra={"x-operation-order": 1},
 )
+@requires_one([NucliaDBRoles.WRITER])
+@version(1)
+async def tus_post_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field: str,
+    item: Optional[CreateResourcePayload] = None,
+) -> Response:
+    return await _tus_post(request, kbid, item=item, rslug=rslug, field=field)
+
+
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{path_rid}}/file/{{field}}/{TUSUPLOAD}",
     tags=["Resource field TUS uploads"],
     name="Create new upload on a Resource (by id)",
     openapi_extra={"x-operation-order": 1},
 )
+@requires_one([NucliaDBRoles.WRITER])
+@version(1)
+async def tus_post_rid_prefix(
+    request: Request,
+    kbid: str,
+    path_rid: str,
+    field: str,
+    item: Optional[CreateResourcePayload] = None,
+) -> Response:
+    return await _tus_post(request, kbid, item=item, path_rid=path_rid, field=field)
+
+
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{TUSUPLOAD}",
     tags=["Knowledge Box TUS uploads"],
     name="Create new upload on a Knowledge Box",
     openapi_extra={"x-operation-order": 1},
 )
 @requires_one([NucliaDBRoles.WRITER])
 @version(1)
-async def post(
+async def tus_post(
+    request: Request,
+    kbid: str,
+    item: Optional[CreateResourcePayload] = None,
+) -> Response:
+    return await _tus_post(request, kbid, item=item)
+
+
+# called by one the three POST above - there are defined distinctly to produce clean API doc
+async def _tus_post(
     request: Request,
     kbid: str,
     item: Optional[CreateResourcePayload] = None,
     path_rid: Optional[str] = None,
     rslug: Optional[str] = None,
     field: Optional[str] = None,
 ) -> Response:
     """
     An empty POST request is used to create a new upload resource.
     The Upload-Length header indicates the size of the entire upload in bytes.
     """
     dm = get_dm()
     storage_manager = get_storage_manager()
 
-    if rslug:
+    if rslug is not None:
         path_rid = await get_rid_from_params_or_raise_error(kbid, slug=rslug)
 
     implies_resource_creation = path_rid is None
 
+    await maybe_back_pressure(request, kbid, resource_uuid=path_rid)
+
     deferred_length = False
     if request.headers.get("upload-defer-length") == "1":
         deferred_length = True
 
     size = None
     if "upload-length" in request.headers:
         size = int(request.headers["upload-length"])
@@ -253,15 +296,18 @@
         await dm.update(
             size=size,
         )
 
     await storage_manager.start(dm, path=path, kbid=kbid)
     await dm.save()
 
-    location = f"{request['path']}/{upload_id}"
+    # Find the URL for upload, with the same parameter as this call
+    location = api.url_path_for(
+        "Upload information", upload_id=upload_id, **request.path_params
+    )
     return Response(
         status_code=201,
         headers={
             "Location": location,  # noqa
             "Tus-Resumable": "1.0.0",
             "Access-Control-Expose-Headers": "Location,Tus-Resumable",
         },
@@ -271,101 +317,184 @@
 @api.head(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/file/{{field}}/{TUSUPLOAD}/{{upload_id}}",
     tags=["Resource field TUS uploads"],
     status_code=200,
     openapi_extra={"x-operation-order": 3},
     name="Upload information",
 )
+@requires_one([NucliaDBRoles.WRITER])
+@version(1)
+async def tus_head_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field: str,
+    upload_id: str,
+) -> Response:
+    return await _tus_head(upload_id)
+
+
 @api.head(
-    f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/file/{{field}}/{TUSUPLOAD}/{{upload_id}}",
+    f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{path_rid}}/file/{{field}}/{TUSUPLOAD}/{{upload_id}}",
     tags=["Resource field TUS uploads"],
     status_code=200,
     openapi_extra={"x-operation-order": 3},
     name="Upload information",
 )
+@requires_one([NucliaDBRoles.WRITER])
+@version(1)
+async def tus_head_rid_prefix(
+    request: Request,
+    kbid: str,
+    path_rid: str,
+    field: str,
+    upload_id: str,
+) -> Response:
+    return await _tus_head(upload_id)
+
+
 @api.head(
     f"/{KB_PREFIX}/{{kbid}}/{TUSUPLOAD}/{{upload_id}}",
     tags=["Knowledge Box TUS uploads"],
     status_code=200,
     openapi_extra={"x-operation-order": 3},
     name="Upload information",
 )
 @requires_one([NucliaDBRoles.WRITER])
 @version(1)
 async def head(
     request: Request,
+    kbid: str,
+    upload_id: str,
+) -> Response:
+    return await _tus_head(upload_id)
+
+
+# called by one the three HEAD above - there are defined distinctly to produce clean API doc
+async def _tus_head(
     upload_id: str,
-    rid: Optional[str] = None,
-    rslug: Optional[str] = None,
-    field: Optional[str] = None,
 ) -> Response:
     """
     Get information about a current download (completed upload size)
     """
     dm = get_dm()
     await dm.load(upload_id)
-    head_response = {
+    tus_head_response = {
         "Upload-Offset": str(dm.offset),
         "Tus-Resumable": "1.0.0",
         "Access-Control-Expose-Headers": "Upload-Offset,Tus-Resumable,Upload-Length",
     }
     if dm.get("size"):
-        head_response["Upload-Length"] = str(dm.get("size"))
+        tus_head_response["Upload-Length"] = str(dm.get("size"))
     else:
-        head_response["Upload-Length"] = "0"
-    return Response(headers=head_response)
+        tus_head_response["Upload-Length"] = "0"
+    return Response(headers=tus_head_response)
 
 
 @api.patch(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/file/{{field}}/{TUSUPLOAD}/{{upload_id}}",
     tags=["Resource field TUS uploads"],
     status_code=200,
     name="Upload data on a Resource (by slug)",
     openapi_extra={"x-operation-order": 2},
 )
+@requires_one([NucliaDBRoles.WRITER])
+@version(1)
+async def tus_patch_rslug_prefix(
+    request: Request,
+    kbid: str,
+    rslug: str,
+    field: str,
+    upload_id: str,
+) -> Response:
+    return await tus_patch(request, kbid, upload_id, rslug=rslug, field=field)
+
+
 @api.patch(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/file/{{field}}/{TUSUPLOAD}/{{upload_id}}",
     tags=["Resource field TUS uploads"],
     status_code=200,
     name="Upload data on a Resource (by id)",
     openapi_extra={"x-operation-order": 2},
 )
+@requires_one([NucliaDBRoles.WRITER])
+@version(1)
+async def tus_patch_rid_prefix(
+    request: Request,
+    kbid: str,
+    rid: str,
+    field: str,
+    upload_id: str,
+) -> Response:
+    return await tus_patch(request, kbid, upload_id, rid=rid, field=field)
+
+
 @api.patch(
     f"/{KB_PREFIX}/{{kbid}}/{TUSUPLOAD}/{{upload_id}}",
     tags=["Knowledge Box TUS uploads"],
     status_code=200,
     name="Upload data on a Knowledge Box",
     openapi_extra={"x-operation-order": 2},
 )
 @requires_one([NucliaDBRoles.WRITER])
 @version(1)
 async def patch(
     request: Request,
     kbid: str,
     upload_id: str,
+) -> Response:
+    return await tus_patch(request, kbid, upload_id)
+
+
+async def tus_patch(
+    request: Request,
+    kbid: str,
+    upload_id: str,
+    rid: Optional[str] = None,
+    rslug: Optional[str] = None,
+    field: Optional[str] = None,
+):
+    try:
+        return await _tus_patch(
+            request,
+            kbid,
+            upload_id,
+            rid=rid,
+            rslug=rslug,
+            field=field,
+        )
+    except ResumableURINotAvailable:
+        return HTTPClientError(
+            status_code=404,
+            detail=f"Resumable URI not found for upload_id: {upload_id}",
+        )
+
+
+# called by one the three PATCH above - there are defined distinctly to produce clean API doc
+async def _tus_patch(
+    request: Request,
+    kbid: str,
+    upload_id: str,
     rid: Optional[str] = None,
     rslug: Optional[str] = None,
     field: Optional[str] = None,
-    x_synchronous: bool = Header(False),  # type: ignore
 ) -> Response:
     """
     Upload all bytes in the requests and append them in the specifyied offset
     """
-    if rslug:
+    if rslug is not None:
         rid = await get_rid_from_params_or_raise_error(kbid, slug=rslug)
 
-    if rid:
-        set_info_on_span(
-            {"nuclia.rid": rid, "nuclia.kbid": kbid, "nuclia.upload_id": upload_id}
-        )
-    else:
-        set_info_on_span({"nuclia.kbid": kbid, "nuclia.upload_id": upload_id})
-
     dm = get_dm()
     await dm.load(upload_id)
+    if not dm.metadata:
+        # If the metadata is not found for the upload id, this means
+        # that the upload was never started or it has expired
+        raise ResumableURINotAvailable()
+
     to_upload = None
     if "content-length" in request.headers:
         # header is optional, we'll be okay with unknown lengths...
         to_upload = int(request.headers["content-length"])
 
     if "upload-length" in request.headers:
         if dm.get("deferred_length"):
@@ -440,76 +569,153 @@
                 field=field,
                 rid=rid,
                 kbid=kbid,
                 path=path,
                 request=request,
                 bucket=storage_manager.storage.get_bucket_name(kbid),
                 item=creation_payload,
-                wait_on_commit=x_synchronous,
             )
         except LimitsExceededError as exc:
             raise HTTPException(status_code=exc.status_code, detail=exc.detail)
 
         headers["NDB-Seq"] = f"{seqid}"
     else:
+        check_uploaded_chunk_size(read_bytes, storage_manager)
         await dm.save()
 
     return Response(headers=headers)
 
 
+def check_uploaded_chunk_size(read_bytes: int, storage_manager: FileStorageManager):
+    if (
+        storage_manager.min_upload_size is not None
+        and read_bytes < storage_manager.min_upload_size
+    ):
+        raise HTTPPreconditionFailed(
+            detail=f"Intermediate chunks cannot be smaller than {storage_manager.min_upload_size} bytes"
+        )
+
+
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/file/{{field}}/{UPLOAD}",
     status_code=201,
     tags=["Resource fields"],
     name="Upload binary file on a Resource (by slug)",
     description="Upload a file as a field on an existing resource, if the field exists will return a conflict (419)",
 )
+@requires_one([NucliaDBRoles.WRITER])
+@version(1)
+async def upload_rslug_prefix(
+    request: StarletteRequest,
+    kbid: str,
+    rslug: str,
+    field: str,
+    x_filename: Optional[list[str]] = Header(None),  # type: ignore
+    x_password: Optional[list[str]] = Header(None),  # type: ignore
+    x_language: Optional[list[str]] = Header(None),  # type: ignore
+    x_md5: Optional[list[str]] = Header(None),  # type: ignore
+) -> ResourceFileUploaded:
+    return await _upload(
+        request,
+        kbid,
+        rslug=rslug,
+        field=field,
+        x_filename=x_filename,
+        x_password=x_password,
+        x_language=x_language,
+        x_md5=x_md5,
+    )
+
+
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{path_rid}}/file/{{field}}/{UPLOAD}",
     status_code=201,
     tags=["Resource fields"],
     name="Upload binary file on a Resource (by id)",
     description="Upload a file as a field on an existing resource, if the field exists will return a conflict (419)",
 )
+@requires_one([NucliaDBRoles.WRITER])
+@version(1)
+async def upload_rid_prefix(
+    request: StarletteRequest,
+    kbid: str,
+    path_rid: str,
+    field: str,
+    x_filename: Optional[list[str]] = Header(None),  # type: ignore
+    x_password: Optional[list[str]] = Header(None),  # type: ignore
+    x_language: Optional[list[str]] = Header(None),  # type: ignore
+    x_md5: Optional[list[str]] = Header(None),  # type: ignore
+) -> ResourceFileUploaded:
+    return await _upload(
+        request,
+        kbid,
+        path_rid=path_rid,
+        field=field,
+        x_filename=x_filename,
+        x_password=x_password,
+        x_language=x_language,
+        x_md5=x_md5,
+    )
+
+
 @api.post(
     f"/{KB_PREFIX}/{{kbid}}/{UPLOAD}",
     status_code=201,
     tags=["Knowledge Boxes"],
     name="Upload binary file on a Knowledge Box",
     description="Upload a file onto a Knowledge Box, field id will be file and rid will be autogenerated. ",
 )
 @requires_one([NucliaDBRoles.WRITER])
 @version(1)
 async def upload(
     request: StarletteRequest,
     kbid: str,
+    x_filename: Optional[list[str]] = Header(None),  # type: ignore
+    x_password: Optional[list[str]] = Header(None),  # type: ignore
+    x_language: Optional[list[str]] = Header(None),  # type: ignore
+    x_md5: Optional[list[str]] = Header(None),  # type: ignore
+) -> ResourceFileUploaded:
+    return await _upload(
+        request,
+        kbid,
+        x_filename=x_filename,
+        x_password=x_password,
+        x_language=x_language,
+        x_md5=x_md5,
+    )
+
+
+# called by one the three POST above - there are defined distinctly to produce clean API doc
+async def _upload(
+    request: StarletteRequest,
+    kbid: str,
     path_rid: Optional[str] = None,
     rslug: Optional[str] = None,
     field: Optional[str] = None,
-    x_filename: Optional[List[str]] = Header(None),  # type: ignore
-    x_password: Optional[List[str]] = Header(None),  # type: ignore
-    x_language: Optional[List[str]] = Header(None),  # type: ignore
-    x_md5: Optional[List[str]] = Header(None),  # type: ignore
-    x_synchronous: bool = Header(False),  # type: ignore
+    x_filename: Optional[list[str]] = Header(None),  # type: ignore
+    x_password: Optional[list[str]] = Header(None),  # type: ignore
+    x_language: Optional[list[str]] = Header(None),  # type: ignore
+    x_md5: Optional[list[str]] = Header(None),  # type: ignore
 ) -> ResourceFileUploaded:
-    if rslug:
+    if rslug is not None:
         path_rid = await get_rid_from_params_or_raise_error(kbid, slug=rslug)
 
+    await maybe_back_pressure(request, kbid, resource_uuid=path_rid)
+
     md5_user = x_md5[0] if x_md5 is not None and len(x_md5) > 0 else None
     try:
         path, rid, valid_field = await start_upload_field(
             kbid, path_rid, field, md5_user
         )
     except ResourceNotFound:
         raise HTTPNotFound("Resource is not found or not yet available")
     except ConflictError:
         raise HTTPConflict("A resource with the same uploaded file already exists")
     except IngestNotAvailable:
         raise HTTPServiceUnavailable("Upload not available right now, try again")
-
     dm = get_dm()
     storage_manager = get_storage_manager()
 
     implies_resource_creation = path_rid is None
 
     if implies_resource_creation:
         # When uploading a file to a new kb resource, we want to  allow multiple
@@ -581,15 +787,14 @@
             md5=x_md5[0] if x_md5 and len(x_md5) else None,
             field=valid_field,
             source=storage_manager.storage.source,
             rid=rid,
             path=path,
             request=request,
             bucket=storage_manager.storage.get_bucket_name(kbid),
-            wait_on_commit=x_synchronous,
         )
     except LimitsExceededError as exc:
         raise HTTPException(status_code=exc.status_code, detail=exc.detail)
 
     return ResourceFileUploaded(seqid=seqid, uuid=rid, field_id=valid_field)
 
 
@@ -627,19 +832,14 @@
         rid = md5
 
     if field is None and md5 is None:
         field = uuid.uuid4().hex
     elif field is None:
         field = md5
 
-    if rid and kbid and field:
-        set_info_on_span(
-            {"nuclia.rid": rid, "nuclia.kbid": kbid, "nuclia.field": field}
-        )
-
     path = KB_RESOURCE_FIELD.format(kbid=kbid, uuid=rid, field=field)
     return path, rid, field
 
 
 async def store_file_on_nuclia_db(
     size: int,
     kbid: str,
@@ -652,16 +852,15 @@
     content_type: str = "application/octet-stream",
     override_resource_title: bool = False,
     filename: Optional[str] = None,
     password: Optional[str] = None,
     language: Optional[str] = None,
     md5: Optional[str] = None,
     item: Optional[CreateResourcePayload] = None,
-    wait_on_commit: bool = False,
-) -> int:
+) -> Optional[int]:
     # File is on NucliaDB Storage at path
 
     partitioning = get_partitioning()
     transaction = get_transaction_utility()
     processing = get_processing()
     storage = await get_storage(service_name=SERVICE_NAME)
 
@@ -689,26 +888,31 @@
             toprocess.slug = item.slug
 
         toprocess.processing_options = item.processing_options
 
         parse_basic(writer, item, toprocess)
         if item.origin is not None:
             parse_origin(writer.origin, item.origin)
+        if item.extra is not None:
+            parse_extra(writer.extra, item.extra)
+
+        toprocess.title = writer.basic.title
 
         await parse_fields(
             writer=writer,
             item=item,
             toprocess=toprocess,
             kbid=kbid,
             uuid=rid,
             x_skip_store=False,
         )
 
     if override_resource_title and filename is not None:
         set_title(writer, toprocess, filename)
+
     writer.basic.icon = content_type
     writer.basic.created.FromDatetime(datetime.now())
 
     # Update resource with file
     file_field = FieldFile()
     file_field.added.FromDatetime(datetime.now())
     file_field.file.bucket_name = bucket
@@ -731,24 +935,32 @@
     # Do not store passwords on maindb
     writer.files[field].ClearField("password")
 
     toprocess.filefield[field] = await processing.convert_internal_filefield_to_str(
         file_field, storage=storage
     )
 
+    writer.source = BrokerMessage.MessageSource.WRITER
+    try:
+        await transaction.commit(writer, partition, wait=True)
+    except TransactionCommitTimeoutError:
+        raise HTTPException(
+            status_code=501,
+            detail="Inconsistent write. This resource will not be processed and may not be stored.",
+        )
+
     try:
         processing_info = await processing.send_to_process(toprocess, partition)
     except LimitsExceededError as exc:
         raise HTTPException(status_code=exc.status_code, detail=exc.detail)
     except SendToProcessError:
-        raise HTTPException(status_code=500, detail="Error while sending to process")
-
-    writer.source = BrokerMessage.MessageSource.WRITER
-    set_processing_info(writer, processing_info)
-    await transaction.commit(writer, partition, wait=wait_on_commit)
+        raise HTTPException(
+            status_code=500,
+            detail="Error while sending to process. Try calling /reprocess",
+        )
 
     return processing_info.seqid
 
 
 def maybe_b64decode(some_string: str) -> str:
     try:
         return base64.b64decode(some_string).decode()
```

## nucliadb/writer/layouts/__init__.py

```diff
@@ -13,22 +13,22 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import Any, Callable, Coroutine, Dict
+from typing import Any, Callable, Coroutine
 
 from nucliadb_protos.resources_pb2 import FieldLayout
 
 import nucliadb_models as models
 from nucliadb_utils.storages.storage import Storage
 
-VERSION: Dict[
+VERSION: dict[
     int,
     Callable[
         [models.InputLayoutField, str, str, str, Storage],
         Coroutine[Any, Any, FieldLayout],
     ],
 ] = {}
```

## nucliadb/writer/resource/basic.py

```diff
@@ -17,34 +17,40 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from datetime import datetime
 
 from fastapi import HTTPException
 from nucliadb_protos.resources_pb2 import (
+    Answers,
     Basic,
     Classification,
     ExtractedTextWrapper,
     FieldComputedMetadataWrapper,
     FieldType,
     Metadata,
+    PageSelections,
     Paragraph,
 )
 from nucliadb_protos.resources_pb2 import ParagraphAnnotation as PBParagraphAnnotation
-from nucliadb_protos.resources_pb2 import TokenSplit, UserFieldMetadata
+from nucliadb_protos.resources_pb2 import (
+    QuestionAnswerAnnotation as PBQuestionAnswerAnnotation,
+)
+from nucliadb_protos.resources_pb2 import TokenSplit, UserFieldMetadata, VisualSelection
 from nucliadb_protos.utils_pb2 import Relation, RelationNode
 from nucliadb_protos.writer_pb2 import BrokerMessage
 
 from nucliadb.ingest.orm.utils import set_title
-from nucliadb.ingest.processing import ProcessingInfo, PushPayload
+from nucliadb.ingest.processing import PushPayload
 from nucliadb_models.common import FIELD_TYPES_MAP_REVERSE
 from nucliadb_models.file import FileField
 from nucliadb_models.link import LinkField
 from nucliadb_models.metadata import (
     ParagraphAnnotation,
+    QuestionAnswerAnnotation,
     RelationNodeTypeMap,
     RelationTypeMap,
 )
 from nucliadb_models.text import TEXT_FORMAT_TO_MIMETYPE, PushTextFormat, Text
 from nucliadb_models.writer import (
     GENERIC_MIME_TYPE,
     ComingResourcePayload,
@@ -80,15 +86,22 @@
         toprocess.genericfield["summary"] = Text(
             body=item.summary, format=PushTextFormat.PLAIN
         )
     if item.thumbnail:
         bm.basic.thumbnail = item.thumbnail
     if item.layout:
         bm.basic.layout = item.layout
-    parse_icon(bm, item)
+
+    if item.metadata is not None:
+        bm.basic.metadata.metadata.update(item.metadata.metadata)
+        if item.metadata.language:
+            bm.basic.metadata.language = item.metadata.language
+        if item.metadata.languages:
+            bm.basic.metadata.languages.extend(item.metadata.languages)
+
     if item.fieldmetadata is not None:
         for fieldmetadata in item.fieldmetadata:
             userfieldmetadata = UserFieldMetadata()
             for token in fieldmetadata.token:
                 userfieldmetadata.token.append(
                     TokenSplit(
                         token=token.token,
@@ -107,30 +120,45 @@
                             labelset=classification.labelset,
                             label=classification.label,
                             cancelled_by_user=classification.cancelled_by_user,
                         )
                     )
                 userfieldmetadata.paragraphs.append(paragraphpb)
 
+            for page_selections in fieldmetadata.selections:
+                page_selections_pb = PageSelections()
+                page_selections_pb.page = page_selections.page
+                page_selections_pb.visual.extend(
+                    [
+                        VisualSelection(
+                            label=visual_selection.label,
+                            top=visual_selection.top,
+                            left=visual_selection.left,
+                            right=visual_selection.right,
+                            bottom=visual_selection.bottom,
+                            token_ids=visual_selection.token_ids,
+                        )
+                        for visual_selection in page_selections.visual
+                    ]
+                )
+                userfieldmetadata.page_selections.append(page_selections_pb)
+
+            for qa_annotation in fieldmetadata.question_answers:
+                qa_annotation_pb = build_question_answer_annotation_pb(qa_annotation)
+                userfieldmetadata.question_answers.append(qa_annotation_pb)
+
             userfieldmetadata.field.field = fieldmetadata.field.field
             userfieldmetadata.field.field_type = FIELD_TYPES_MAP_REVERSE[  # type: ignore
                 fieldmetadata.field.field_type.value
             ]
 
             bm.basic.fieldmetadata.append(userfieldmetadata)
 
     if item.usermetadata is not None:
-        # user metadata like labels need to be applied to all
-        # paragraphs in a resource and that means this message should
-        # force reindexing everything
-        # XXX This is not ideal. Long term, we should handle label
-        # indexes differently so this is not required
-        bm.reindex = True
-
-        # protobuferrs repeated fields don't support assignment
+        # protobufers repeated fields don't support assignment
         # will allways be a clean basic
         bm.basic.usermetadata.classifications.extend(
             [
                 Classification(
                     labelset=x.labelset,
                     label=x.label,
                     cancelled_by_user=x.cancelled_by_user,
@@ -169,49 +197,37 @@
             )
 
         # protobuferrs repeated fields don't support assignment so
         # in order to replace relations, we need to clear them first
         bm.basic.usermetadata.ClearField("relations")
         bm.basic.usermetadata.relations.extend(relations)
 
+    if item.security is not None:
+        unique_groups = list(set(item.security.access_groups))
+        bm.security.access_groups.extend(unique_groups)
+
 
 def parse_basic(bm: BrokerMessage, item: CreateResourcePayload, toprocess: PushPayload):
     bm.basic.created.FromDatetime(datetime.now())
 
-    if item.metadata is not None:
-        bm.basic.metadata.Clear()
-        bm.basic.metadata.metadata.update(item.metadata.metadata)
-        if item.metadata.language:
-            bm.basic.metadata.language = item.metadata.language
-        if item.metadata.languages:
-            bm.basic.metadata.languages.extend(item.metadata.languages)
-        # basic.metadata.useful = item.metadata.useful
-        # basic.metadata.status = item.metadata.status
-
     if item.title is None:
         item.title = compute_title(item, bm.uuid)
+    parse_icon_on_create(bm, item)
 
     parse_basic_modify(bm, item, toprocess)
 
 
 def set_status(basic: Basic, item: CreateResourcePayload):
     basic.metadata.status = Metadata.Status.PENDING
 
 
 def set_status_modify(basic: Basic, item: UpdateResourcePayload):
     basic.metadata.status = Metadata.Status.PENDING
 
 
-def set_processing_info(bm: BrokerMessage, processing_info: ProcessingInfo):
-    bm.basic.last_seqid = processing_info.seqid
-    if processing_info.account_seq is not None:
-        bm.basic.last_account_seq = processing_info.account_seq
-    bm.basic.queue = bm.basic.QueueType.Value(processing_info.queue.name)
-
-
 def validate_classifications(paragraph: ParagraphAnnotation):
     classifications = paragraph.classifications
     if len(classifications) == 0:
         raise HTTPException(
             status_code=422, detail="ensure classifications has at least 1 items"
         )
 
@@ -231,19 +247,43 @@
     for file_field in item.files.values():
         if file_field.file.filename:
             return file_field.file.filename
 
     return item.slug or rid
 
 
-def parse_icon(bm: BrokerMessage, item: ComingResourcePayload):
+def parse_icon_on_create(bm: BrokerMessage, item: CreateResourcePayload):
     if item.icon:
         # User input icon takes precedence
         bm.basic.icon = item.icon
         return
-    elif len(item.texts) > 0:
+
+    icon = GENERIC_MIME_TYPE
+    if len(item.texts) > 0:
         # Infer icon from text file format
         format = next(iter(item.texts.values())).format
-        bm.basic.icon = TEXT_FORMAT_TO_MIMETYPE[format]
-    else:
-        # Default
-        bm.basic.icon = GENERIC_MIME_TYPE
+        icon = TEXT_FORMAT_TO_MIMETYPE[format]
+    item.icon = icon
+    bm.basic.icon = icon
+
+
+def build_question_answer_annotation_pb(
+    qa_annotation: QuestionAnswerAnnotation,
+) -> PBQuestionAnswerAnnotation:
+    pb = PBQuestionAnswerAnnotation()
+    pb.cancelled_by_user = qa_annotation.cancelled_by_user
+    pb.question_answer.question.text = qa_annotation.question_answer.question.text
+    if qa_annotation.question_answer.question.language is not None:
+        pb.question_answer.question.language = (
+            qa_annotation.question_answer.question.language
+        )
+    pb.question_answer.question.ids_paragraphs.extend(
+        qa_annotation.question_answer.question.ids_paragraphs
+    )
+    for answer_annotation in qa_annotation.question_answer.answers:
+        answer = Answers()
+        answer.text = answer_annotation.text
+        if answer_annotation.language is not None:
+            answer.language = answer_annotation.language
+        answer.ids_paragraphs.extend(answer_annotation.ids_paragraphs)
+        pb.question_answer.answers.append(answer)
+    return pb
```

## nucliadb/writer/resource/field.py

```diff
@@ -80,17 +80,17 @@
             FieldTypeName.LINK,
         }:
             continue
 
         field_pb = await field.get_value()
 
         if field_type_name is FieldTypeName.FILE:
-            toprocess.filefield[
-                field_id
-            ] = await processing.convert_internal_filefield_to_str(field_pb, storage)
+            toprocess.filefield[field_id] = (
+                await processing.convert_internal_filefield_to_str(field_pb, storage)
+            )
 
         if field_type_name is FieldTypeName.LINK:
             parsed_link = MessageToDict(
                 field_pb,
                 preserving_proto_field_name=True,
                 including_default_value_fields=True,
             )
@@ -144,18 +144,18 @@
                         preserving_proto_field_name=True,
                         including_default_value_fields=True,
                     )
                     parsed_message["content"]["attachments"] = [
                         await processing.convert_internal_cf_to_str(cf, storage)
                         for cf in message.content.attachments
                     ]
-                    parsed_message["content"][
-                        "format"
-                    ] = resources_pb2.MessageContent.Format.Value(
-                        parsed_message["content"]["format"]
+                    parsed_message["content"]["format"] = (
+                        resources_pb2.MessageContent.Format.Value(
+                            parsed_message["content"]["format"]
+                        )
                     )
                     full_conversation.messages.append(
                         models.PushMessage(**parsed_message)
                     )
             toprocess.conversationfield[field_id] = full_conversation
 
 
@@ -310,19 +310,27 @@
         for cookie, value in link_field.cookies.items():
             writer.links[key].headers[cookie] = value
 
     if link_field.localstorage is not None:
         for local, value in link_field.localstorage.items():
             writer.links[key].localstorage[local] = value
 
+    if link_field.css_selector is not None:
+        writer.links[key].css_selector = link_field.css_selector
+
+    if link_field.xpath is not None:
+        writer.links[key].xpath = link_field.xpath
+
     toprocess.linkfield[key] = models.LinkUpload(
         link=link_field.uri,
-        headers=link_field.headers,
-        cookies=link_field.cookies,
-        localstorage=link_field.localstorage,
+        headers=link_field.headers or {},
+        cookies=link_field.cookies or {},
+        localstorage=link_field.localstorage or {},
+        css_selector=link_field.css_selector,
+        xpath=link_field.xpath,
     )
 
 
 def parse_keywordset_field(
     key: str,
     keywordset_field: models.FieldKeywordset,
     writer: BrokerMessage,
@@ -386,15 +394,15 @@
             type=block.type,
             ident=block.ident,
             payload=block.payload,
             file=await processing.convert_internal_cf_to_str(cf_conv_field, storage),
         )
 
     toprocess.layoutfield[key] = models.LayoutDiff(
-        format=lc.format, blocks=toprocess_blocks
+        format=lc.format, blocks=toprocess_blocks  # type: ignore
     )
 
 
 async def parse_conversation_field(
     key: str,
     conversation_field: models.InputConversationField,
     writer: BrokerMessage,
@@ -412,14 +420,16 @@
         if message.timestamp:
             cm.timestamp.FromDatetime(message.timestamp)
         if message.who:
             cm.who = message.who
         for to in message.to:
             cm.to.append(to)
         cm.ident = message.ident
+        if message.type_ is not None:
+            cm.type = resources_pb2.Message.MessageType.Value(message.type_.value)
 
         processing_message_content = models.PushMessageContent(
             text=message.content.text,
             format=getattr(models.PushMessageFormat, message.content.format.value),
         )
 
         cm.content.text = message.content.text
```

## nucliadb/writer/resource/origin.py

```diff
@@ -14,15 +14,16 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 from nucliadb_protos.resources_pb2 import Origin
 
-from nucliadb_models import InputOrigin
+from nucliadb_models import Extra, InputOrigin
+from nucliadb_protos import resources_pb2
 
 
 def parse_origin(origin: Origin, origin_payload: InputOrigin):
     if origin_payload.source_id:
         origin.source_id = origin_payload.source_id
     if origin_payload.url:
         origin.url = origin_payload.url
@@ -34,8 +35,16 @@
         origin.tags.extend(origin_payload.tags)
     if origin_payload.collaborators:
         origin.colaborators.extend(origin_payload.collaborators)
     if origin_payload.filename:
         origin.filename = origin_payload.filename
     if origin_payload.related:
         origin.related.extend(origin_payload.related)
+    if origin_payload.metadata:
+        origin.metadata.update(origin_payload.metadata)
+    if origin_payload.path:
+        origin.path = origin_payload.path
     origin.source = Origin.Source.API
+
+
+def parse_extra(proto: resources_pb2.Extra, payload: Extra):
+    proto.metadata.update(payload.metadata)
```

## nucliadb/writer/resource/slug.py

```diff
@@ -14,15 +14,15 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 
+from nucliadb.common.maindb.utils import get_driver
 from nucliadb.ingest.orm.knowledgebox import KnowledgeBox
-from nucliadb.ingest.utils import get_driver
 
 
 async def resource_slug_exists(kbid: str, slug: str) -> bool:
-    driver = await get_driver()
+    driver = get_driver()
     async with driver.transaction() as txn:
         return await KnowledgeBox.resource_slug_exists(txn, kbid, slug)
```

## nucliadb/writer/tests/conftest.py

```diff
@@ -17,14 +17,15 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 pytest_plugins = [
     "pytest_mock",
     "pytest_docker_fixtures",
     "nucliadb_utils.tests.nats",
-    "nucliadb.ingest.tests.fixtures",
+    "nucliadb.tests.fixtures",
+    "nucliadb.tests.tikv",
+    "nucliadb.ingest.tests.fixtures",  # should be refactored out
     "nucliadb.writer.tests.fixtures",
-    "nucliadb.ingest.tests.tikv",
+    "nucliadb_utils.tests.conftest",
     "nucliadb_utils.tests.gcs",
     "nucliadb_utils.tests.s3",
-    "nucliadb.writer.tests.tus",
 ]
```

## nucliadb/writer/tests/fixtures.py

```diff
@@ -14,49 +14,63 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from enum import Enum
-from typing import AsyncIterator, Callable, List, Optional
+from typing import AsyncIterator, Callable, Optional
+from unittest import mock
 
 import pytest
 from httpx import AsyncClient
+from pytest_lazy_fixtures import lazy_fixture
 from redis import asyncio as aioredis
 
 from nucliadb.ingest.tests.fixtures import IngestFixture
 from nucliadb.writer import API_PREFIX
 from nucliadb.writer.api.v1.router import KB_PREFIX, KBS_PREFIX
 from nucliadb.writer.settings import settings
 from nucliadb.writer.tus import clear_storage
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_utils.settings import (
     FileBackendConfig,
     nuclia_settings,
     nucliadb_settings,
     storage_settings,
 )
+from nucliadb_utils.tests.conftest import get_testing_storage_backend
+from nucliadb_utils.utilities import Utility, clean_utility, set_utility
+
+
+@pytest.fixture(scope="function")
+def disabled_back_pressure():
+    with mock.patch(
+        "nucliadb.writer.back_pressure.is_back_pressure_enabled", return_value=False
+    ) as mocked:
+        yield mocked
 
 
 @pytest.fixture(scope="function")
 async def writer_api(
+    disabled_back_pressure,
     redis,
+    storage_writer,
     grpc_servicer: IngestFixture,
-    gcs_storage_writer,
     transaction_utility,
     processing_utility,
     tus_manager,
-    event_loop,
-) -> AsyncIterator[Callable[[List[Enum], str, str], AsyncClient]]:
+) -> AsyncIterator[Callable[[list[Enum], str, str], AsyncClient]]:
     nucliadb_settings.nucliadb_ingest = grpc_servicer.host
-    from nucliadb.writer.app import application
+    from nucliadb.writer.app import create_application
+
+    application = create_application()
 
     def make_client_fixture(
-        roles: Optional[List[Enum]] = None,
+        roles: Optional[list[Enum]] = None,
         user: str = "",
         version: str = "1",
     ) -> AsyncClient:
         roles = roles or []
         client_base_url = "http://test"
         client_base_url = f"{client_base_url}/{API_PREFIX}/v{version}"
 
@@ -68,30 +82,74 @@
 
         return client
 
     driver = aioredis.from_url(f"redis://{redis[0]}:{redis[1]}")
     await driver.flushall()
 
     await application.router.startup()
+
     yield make_client_fixture
+
     await application.router.shutdown()
     clear_storage()
 
     await driver.flushall()
     await driver.close(close_connection_pool=True)
 
 
 @pytest.fixture(scope="function")
-async def gcs_storage_writer(gcs):
-    storage_settings.gcs_endpoint_url = gcs
+def gcs_storage_writer(gcs):
     storage_settings.file_backend = FileBackendConfig.GCS
+    storage_settings.gcs_endpoint_url = gcs
     storage_settings.gcs_bucket = "test_{kbid}"
 
 
 @pytest.fixture(scope="function")
+def s3_storage_writer(s3):
+    storage_settings.file_backend = FileBackendConfig.S3
+    storage_settings.s3_endpoint = s3
+    storage_settings.s3_client_id = ""
+    storage_settings.s3_client_secret = ""
+    storage_settings.s3_bucket = "test-{kbid}"
+
+
+@pytest.fixture(scope="function")
+def pg_storage_writer(pg):
+    storage_settings.file_backend = FileBackendConfig.PG
+    url = f"postgresql://postgres:postgres@{pg[0]}:{pg[1]}/postgres"
+    storage_settings.driver_pg_url = url
+
+
+def lazy_storage_writer_fixture():
+    backend = get_testing_storage_backend()
+    if backend == "gcs":
+        return [lazy_fixture.lf("gcs_storage_writer")]
+    elif backend == "s3":
+        return [lazy_fixture.lf("s3_storage_writer")]
+    elif backend == "pg":
+        return [lazy_fixture.lf("pg_storage_writer")]
+    else:
+        print(f"Unknown storage backend {backend}, using gcs")
+        return [lazy_fixture.lf("gcs_storage_writer")]
+
+
+@pytest.fixture(scope="function", params=lazy_storage_writer_fixture())
+async def storage_writer(request):
+    """
+    Generic storage fixture that allows us to run the same tests for different storage backends.
+    """
+    storage_driver = request.param
+    set_utility(Utility.STORAGE, storage_driver)
+
+    yield storage_driver
+
+    clean_utility(Utility.STORAGE)
+
+
+@pytest.fixture(scope="function")
 async def knowledgebox_writer(writer_api):
     async with writer_api(roles=[NucliaDBRoles.MANAGER]) as client:
         resp = await client.post(
             f"/{KBS_PREFIX}",
             json={
                 "slug": "kbid1",
                 "title": "My Knowledge Box",
@@ -104,15 +162,14 @@
 
 
 @pytest.fixture(scope="function")
 async def resource(redis, writer_api, knowledgebox_writer):
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_writer}/resources",
-            headers={"X-Synchronous": "true"},
             json={
                 "slug": "resource1",
                 "title": "Resource 1",
             },
         )
         assert resp.status_code == 201
         uuid = resp.json()["uuid"]
@@ -127,7 +184,8 @@
     nuclia_settings.nuclia_jwt_key = "foobarkey"
 
 
 @pytest.fixture(scope="function")
 async def tus_manager(redis):
     settings.dm_redis_host = redis[0]
     settings.dm_redis_port = redis[1]
+    yield
```

## nucliadb/writer/tests/test_fields.py

```diff
@@ -38,14 +38,16 @@
 TEST_LINK_PAYLOAD = {
     "added": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
     "headers": {},
     "cookies": {},
     "uri": "http://some-link.com",
     "language": "en",
     "localstorage": {},
+    "css_selector": "main",
+    "xpath": "my_xpath",
 }
 TEST_KEYWORDSETS_PAYLOAD = {"keywords": [{"value": "kw1"}, {"value": "kw2"}]}
 TEST_DATETIMES_PAYLOAD = {"value": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")}
 TEST_CONVERSATION_PAYLOAD = {
     "messages": [
         {
             "timestamp": datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
@@ -127,15 +129,14 @@
 
 @pytest.mark.asyncio
 async def test_resource_field_add(writer_api, knowledgebox_writer):
     knowledgebox_id = knowledgebox_writer
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={"slug": "resource1", "title": "My resource"},
         )
         assert resp.status_code == 201
         data = resp.json()
         assert "uuid" in data
         assert "seqid" in data
         rid = data["uuid"]
@@ -216,28 +217,26 @@
         data = resp.json()
         assert "seqid" in data
 
         # File field pointing to an externally hosted file
         resp = await client.put(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCE_PREFIX}/{rid}/file/externalfile",
             json=TEST_EXTERNAL_FILE_PAYLOAD,
-            headers={"X-SYNCHRONOUS": "True"},
         )
         assert resp.status_code == 201
         data = resp.json()
         assert "seqid" in data
 
 
 @pytest.mark.asyncio
 async def test_resource_field_append_extra(writer_api, knowledgebox_writer):
     knowledgebox_id = knowledgebox_writer
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={
                 "slug": "resource1",
                 "title": "My resource",
                 "layouts": {"layout1": TEST_LAYOUT_PAYLOAD},
                 "conversations": {"conv1": TEST_CONVERSATION_PAYLOAD},
             },
         )
@@ -269,15 +268,14 @@
 
 @pytest.mark.asyncio
 async def test_resource_field_delete(writer_api, knowledgebox_writer):
     knowledgebox_id = knowledgebox_writer
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={
                 "slug": "resource1",
                 "title": "My resource",
                 "texts": {"text1": TEST_TEXT_PAYLOAD},
                 "links": {"link1": TEST_LINK_PAYLOAD},
                 "files": {"file1": TEST_FILE_PAYLOAD},
                 "layouts": {"layout1": TEST_LAYOUT_PAYLOAD},
@@ -348,80 +346,73 @@
         ("layout/layout1/blocks", TEST_LAYOUT_APPEND_BLOCKS_PAYLOAD),
         ("file/file1", TEST_FILE_PAYLOAD),
     ],
 )
 async def test_sync_ops(writer_api, knowledgebox_writer, endpoint, payload):
     knowledgebox_id = knowledgebox_writer
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
-        HEADERS = {"X-SYNCHRONOUS": "True"}
         # Create a resource
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
-            headers=HEADERS,
             json={
                 "slug": "resource1",
                 "title": "My resource",
                 "layouts": {"layout1": TEST_LAYOUT_PAYLOAD},
                 "conversations": {"conv1": TEST_CONVERSATION_PAYLOAD},
             },
         )
         assert resp.status_code == 201
         data = resp.json()
         rid = data["uuid"]
 
         resource_path = f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCE_PREFIX}/{rid}"
         resp = await client.put(
             f"{resource_path}/{endpoint}",
-            headers={"X-SYNCHRONOUS": "True"},
             json=payload,
         )
         assert resp.status_code in (201, 200)
 
 
 @pytest.mark.asyncio
 async def test_external_file_field(writer_api, knowledgebox_writer):
     knowledgebox_id = knowledgebox_writer
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
             json={"slug": "resource1", "title": "My resource"},
-            headers={"X-SYNCHRONOUS": "True"},
         )
         assert resp.status_code == 201
         rid = resp.json()["uuid"]
 
         # File field pointing to an externally hosted file
         resp = await client.put(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCE_PREFIX}/{rid}/file/externalfile",
             json=TEST_EXTERNAL_FILE_PAYLOAD,
-            headers={"X-SYNCHRONOUS": "True"},
         )
         assert resp.status_code == 201
 
 
 @pytest.mark.asyncio
 async def test_file_field_validation(writer_api, knowledgebox_writer):
     knowledgebox_id = knowledgebox_writer
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
             json={"slug": "resource1", "title": "My resource"},
-            headers={"X-SYNCHRONOUS": "True"},
         )
         assert resp.status_code == 201
         rid = resp.json()["uuid"]
 
         # Remove a required key from the payload
         payload = deepcopy(TEST_FILE_PAYLOAD)
         payload["file"].pop("md5")
 
         resp = await client.put(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCE_PREFIX}/{rid}/file/file1",
             json=payload,
-            headers={"X-SYNCHRONOUS": "True"},
         )
         assert resp.status_code == 201
 
 
 @pytest.mark.parametrize(
     "method,endpoint,payload",
     [
@@ -446,15 +437,14 @@
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         slug = "my-resource"
         field_id = "myfield"
         field_type = "text"
 
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_ingest}/{RESOURCES_PREFIX}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={"slug": slug},
         )
         assert resp.status_code == 201
 
         extra_params = {}
         if payload is not None:
             extra_params["json"] = payload
```

## nucliadb/writer/tests/test_files.py

```diff
@@ -15,35 +15,36 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
 import base64
+import io
 import os
-from typing import Callable, List
+from typing import Callable
 
 import pytest
 from httpx import AsyncClient
 from nucliadb_protos.resources_pb2 import FieldType
 from nucliadb_protos.writer_pb2 import BrokerMessage, ResourceFieldId
 
-from nucliadb.writer.api.v1.router import KB_PREFIX, RSLUG_PREFIX
+from nucliadb.writer.api.v1.router import KB_PREFIX, RESOURCE_PREFIX, RSLUG_PREFIX
 from nucliadb.writer.api.v1.upload import maybe_b64decode
-from nucliadb.writer.tus import TUSUPLOAD, UPLOAD
+from nucliadb.writer.tus import TUSUPLOAD, UPLOAD, get_storage_manager
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_utils import const
 from nucliadb_utils.utilities import get_ingest, get_storage, get_transaction_utility
 
 ASSETS_PATH = os.path.dirname(__file__) + "/assets"
 
 
 @pytest.mark.asyncio
 async def test_knowledgebox_file_tus_options(
-    writer_api: Callable[[List[NucliaDBRoles]], AsyncClient], knowledgebox_writer: str
+    writer_api: Callable[[list[NucliaDBRoles]], AsyncClient], knowledgebox_writer: str
 ):
     client: AsyncClient
     async with writer_api([NucliaDBRoles.WRITER]) as client:
         resp = await client.options(
             f"/{KB_PREFIX}/{knowledgebox_writer}/resource/xxx/file/xxx/{TUSUPLOAD}/xxx"
         )
         assert resp.status_code == 204
@@ -89,40 +90,42 @@
                 "upload-defer-length": "1",
             },
         )
         assert resp.status_code == 201
         url = resp.headers["location"]
 
         offset = 0
-        with open(f"{ASSETS_PATH}/image001.jpg", "rb") as f:
-            data = f.read(10000)
-            while data != b"":
-                resp = await client.head(
-                    url,
-                )
-
-                assert resp.headers["Upload-Length"] == f"0"
-                assert resp.headers["Upload-Offset"] == f"{offset}"
-
-                headers = {
-                    "upload-offset": f"{offset}",
-                    "content-length": f"{len(data)}",
-                }
-                if len(data) < 10000:
-                    headers["upload-length"] = f"{offset + len(data)}"
-
-                resp = await client.patch(
-                    url,
-                    data=data,
-                    headers=headers,
-                )
-                offset += len(data)
-                data = f.read(10000)
 
-        assert resp.headers["Tus-Upload-Finished"] == "1"
+        # We upload a file that spans across more than one chunk
+        min_chunk_size = get_storage_manager().min_upload_size
+        raw_bytes = b"x" * min_chunk_size + b"y" * 500
+        io_bytes = io.BytesIO(raw_bytes)
+        data = io_bytes.read(min_chunk_size)
+        while data != b"":
+            resp = await client.head(url)
+            assert resp.headers["Upload-Length"] == f"0"
+            assert resp.headers["Upload-Offset"] == f"{offset}"
+
+            headers = {
+                "upload-offset": f"{offset}",
+                "content-length": f"{len(data)}",
+            }
+            is_last_chunk = len(data) < min_chunk_size
+            if is_last_chunk:
+                headers["upload-length"] = f"{offset + len(data)}"
+
+            resp = await client.patch(
+                url,
+                content=data,
+                headers=headers,
+            )
+            offset += len(data)
+            data = io_bytes.read(min_chunk_size)
+
+    assert resp.headers["Tus-Upload-Finished"] == "1"
 
     transaction = get_transaction_utility()
 
     sub = await transaction.js.pull_subscribe(
         const.Streams.INGEST.subject.format(partition="1"), "auto"
     )
     msgs = await sub.fetch(1)
@@ -134,24 +137,24 @@
     path = resp.headers["ndb-field"]
     field = path.split("/")[-1]
     rid = path.split("/")[-3]
     assert writer.uuid == rid
     assert writer.basic.icon == "image/jpg"
     assert writer.basic.title == "image.jpg"
     assert writer.files[field].language == "ca"
-    assert writer.files[field].file.size == 30472
+    assert writer.files[field].file.size == len(raw_bytes)
     assert writer.files[field].file.filename == "image.jpg"
     assert writer.files[field].file.md5 == "7af0916dba8b70e29d99e72941923529"
 
     storage = await get_storage()
     data = await storage.downloadbytes(
         bucket=writer.files[field].file.bucket_name,
         key=writer.files[field].file.uri,
     )
-    assert len(data.read()) == 30472
+    assert len(data.read()) == len(raw_bytes)
     await asyncio.sleep(1)
 
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_writer}/{TUSUPLOAD}",
             headers={
                 "tus-resumable": "1.0.0",
@@ -161,15 +164,16 @@
             },
         )
         assert resp.status_code == 409
 
 
 @pytest.mark.asyncio
 async def test_knowledgebox_file_upload_root(
-    writer_api: Callable[[List[NucliaDBRoles]], AsyncClient], knowledgebox_writer: str
+    writer_api: Callable[[list[NucliaDBRoles]], AsyncClient],
+    knowledgebox_writer: str,
 ):
     async with writer_api([NucliaDBRoles.WRITER]) as client:
         with open(f"{ASSETS_PATH}/image001.jpg", "rb") as f:
             resp = await client.post(
                 f"/{KB_PREFIX}/{knowledgebox_writer}/{UPLOAD}",
                 content=f.read(),
                 headers={
@@ -216,15 +220,16 @@
                 },
             )
             assert resp.status_code == 409
 
 
 @pytest.mark.asyncio
 async def test_knowledgebox_file_upload_root_headers(
-    writer_api: Callable[[List[NucliaDBRoles]], AsyncClient], knowledgebox_writer: str
+    writer_api: Callable[[list[NucliaDBRoles]], AsyncClient],
+    knowledgebox_writer: str,
 ):
     async with writer_api([NucliaDBRoles.WRITER]) as client:
         filename = base64.b64encode(b"image.jpg").decode()
         with open(f"{ASSETS_PATH}/image001.jpg", "rb") as f:
             resp = await client.post(
                 f"/{KB_PREFIX}/{knowledgebox_writer}/{UPLOAD}",
                 content=f.read(),
@@ -295,39 +300,41 @@
                 "upload-defer-length": "1",
             },
         )
         assert resp.status_code == 201
         url = resp.headers["location"]
 
         offset = 0
-        with open(f"{ASSETS_PATH}/image001.jpg", "rb") as f:
-            data = f.read(10000)
-            while data != b"":
-                resp = await client.head(
-                    url,
-                )
-
-                assert resp.headers["Upload-Length"] == f"0"
-                assert resp.headers["Upload-Offset"] == f"{offset}"
-
-                headers = {
-                    "upload-offset": f"{offset}",
-                    "content-length": f"{len(data)}",
-                }
-                if len(data) < 10000:
-                    headers["upload-length"] = f"{offset + len(data)}"
-
-                resp = await client.patch(
-                    url,
-                    data=data,
-                    headers=headers,
-                )
-                assert resp.status_code == 200
-                offset += len(data)
-                data = f.read(10000)
+        # We upload a file that spans across more than one chunk
+        min_chunk_size = get_storage_manager().min_upload_size
+        raw_bytes = b"x" * min_chunk_size + b"y" * 500
+        io_bytes = io.BytesIO(raw_bytes)
+        data = io_bytes.read(min_chunk_size)
+        while data != b"":
+            resp = await client.head(url)
+
+            assert resp.headers["Upload-Length"] == f"0"
+            assert resp.headers["Upload-Offset"] == f"{offset}"
+
+            headers = {
+                "upload-offset": f"{offset}",
+                "content-length": f"{len(data)}",
+            }
+            is_last_chunk = len(data) < min_chunk_size
+            if is_last_chunk:
+                headers["upload-length"] = f"{offset + len(data)}"
+
+            resp = await client.patch(
+                url,
+                content=data,
+                headers=headers,
+            )
+            assert resp.status_code == 200
+            offset += len(data)
+            data = io_bytes.read(min_chunk_size)
 
         assert resp.headers["Tus-Upload-Finished"] == "1"
 
     transaction = get_transaction_utility()
 
     sub = await transaction.js.pull_subscribe(
         const.Streams.INGEST.subject.format(partition="1"), "auto"
@@ -341,37 +348,37 @@
     path = resp.headers["ndb-field"]
     field = path.split("/")[-1]
     rid = path.split("/")[-3]
     assert writer.uuid == rid
     assert writer.basic.icon == "image/jpg"
     assert writer.basic.title == ""
     assert writer.files[field].language == "ca"
-    assert writer.files[field].file.size == 30472
+    assert writer.files[field].file.size == len(raw_bytes)
     assert writer.files[field].file.filename == "image.jpg"
     assert writer.files[field].file.md5 == "7af0916dba8b70e29d99e72941923529"
 
     storage = await get_storage()
     data = await storage.downloadbytes(
         bucket=writer.files[field].file.bucket_name,
         key=writer.files[field].file.uri,
     )
-    assert len(data.read()) == 30472
+    assert len(data.read()) == len(raw_bytes)
 
 
 @pytest.mark.asyncio
 async def test_knowledgebox_file_upload_field_headers(
     writer_api, knowledgebox_writer, resource
 ):
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         filename = "image.jpg"
         encoded_filename = base64.b64encode(filename.encode()).decode()
         with open(f"{ASSETS_PATH}/image001.jpg", "rb") as f:
             resp = await client.post(
                 f"/{KB_PREFIX}/{knowledgebox_writer}/resource/{resource}/file/field1/{UPLOAD}",
-                data=f.read(),
+                content=f.read(),
                 headers={
                     "X-FILENAME": encoded_filename,
                     "X-LANGUAGE": "ca",
                     "X-MD5": "7af0916dba8b70e29d99e72941923529",
                     "content-type": "image/jpg",
                 },
             )
@@ -410,21 +417,20 @@
     writer_api, knowledgebox_writer, resource
 ):
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         filename = "image.jpg"
         with open(f"{ASSETS_PATH}/image001.jpg", "rb") as f:
             resp = await client.post(
                 f"/{KB_PREFIX}/{knowledgebox_writer}/resource/{resource}/file/field1/{UPLOAD}",
-                data=f.read(),
+                content=f.read(),
                 headers={
                     "X-FILENAME": filename,
                     "X-LANGUAGE": "ca",
                     "X-MD5": "7af0916dba8b70e29d99e72941923529",
                     "content-type": "image/jpg",
-                    "X-SYNCHRONOUS": "True",
                 },
             )
             assert resp.status_code == 201
 
         ingest = get_ingest()
         pbrequest = ResourceFieldId()
         pbrequest.kbid = knowledgebox_writer
@@ -465,39 +471,40 @@
         assert resp.status_code == 201
         url = resp.headers["location"]
 
         # Check that we are using the slug for the whole file upload
         assert f"{RSLUG_PREFIX}/{rslug}" in url
 
         offset = 0
-        with open(f"{ASSETS_PATH}/image001.jpg", "rb") as f:
-            data = f.read(10000)
-            while data != b"":
-                resp = await client.head(
-                    url,
-                )
-
-                assert resp.headers["Upload-Length"] == f"0"
-                assert resp.headers["Upload-Offset"] == f"{offset}"
-
-                headers = {
-                    "upload-offset": f"{offset}",
-                    "content-length": f"{len(data)}",
-                }
-                if len(data) < 10000:
-                    headers["upload-length"] = f"{offset + len(data)}"
-
-                resp = await client.patch(
-                    url,
-                    data=data,
-                    headers=headers,
-                )
-                assert resp.status_code == 200
-                offset += len(data)
-                data = f.read(10000)
+        min_chunk_size = get_storage_manager().min_upload_size
+        raw_bytes = b"x" * min_chunk_size + b"y" * 500
+        io_bytes = io.BytesIO(raw_bytes)
+        data = io_bytes.read(min_chunk_size)
+        while data != b"":
+            resp = await client.head(url)
+
+            assert resp.headers["Upload-Length"] == f"0"
+            assert resp.headers["Upload-Offset"] == f"{offset}"
+
+            headers = {
+                "upload-offset": f"{offset}",
+                "content-length": f"{len(data)}",
+            }
+            is_last_chunk = len(data) < min_chunk_size
+            if is_last_chunk:
+                headers["upload-length"] = f"{offset + len(data)}"
+
+            resp = await client.patch(
+                url,
+                content=data,
+                headers=headers,
+            )
+            assert resp.status_code == 200
+            offset += len(data)
+            data = io_bytes.read(min_chunk_size)
 
         assert resp.headers["Tus-Upload-Finished"] == "1"
 
     transaction = get_transaction_utility()
 
     sub = await transaction.js.pull_subscribe(
         const.Streams.INGEST.subject.format(partition="1"), "auto"
@@ -511,24 +518,65 @@
     path = resp.headers["ndb-field"]
     field = path.split("/")[-1]
     rid = path.split("/")[-3]
     assert writer.uuid == rid
     assert writer.basic.icon == "image/jpg"
     assert writer.basic.title == ""
     assert writer.files[field].language == "ca"
-    assert writer.files[field].file.size == 30472
+    assert writer.files[field].file.size == len(raw_bytes)
     assert writer.files[field].file.filename == "image.jpg"
     assert writer.files[field].file.md5 == "7af0916dba8b70e29d99e72941923529"
 
     storage = await get_storage()
     data = await storage.downloadbytes(
         bucket=writer.files[field].file.bucket_name,
         key=writer.files[field].file.uri,
     )
-    assert len(data.read()) == 30472
+    assert len(data.read()) == len(raw_bytes)
+
+
+@pytest.mark.asyncio
+async def test_file_tus_upload_urls_field_by_resource_id(
+    writer_api, knowledgebox_writer, resource
+):
+    kb = knowledgebox_writer
+
+    async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
+        language = base64.b64encode(b"ca").decode()
+        filename = base64.b64encode(b"image.jpg").decode()
+        md5 = base64.b64encode(b"7af0916dba8b70e29d99e72941923529").decode()
+        headers = {
+            "tus-resumable": "1.0.0",
+            "upload-metadata": f"filename {filename},language {language},md5 {md5}",
+            "content-type": "image/jpg",
+            "upload-defer-length": "1",
+        }
+
+        resp = await client.post(
+            f"/{KB_PREFIX}/{kb}/resource/idonotexist/file/field1/{TUSUPLOAD}",
+            headers=headers,
+        )
+        assert resp.status_code == 404
+
+        resp = await client.post(
+            f"/{KB_PREFIX}/{kb}/resource/{resource}/file/field1/{TUSUPLOAD}",
+            headers=headers,
+        )
+        assert resp.status_code == 201
+        url = resp.headers["location"]
+
+        # Check that we are using the resource for the whole file upload
+        assert f"{RESOURCE_PREFIX}/{resource}" in url
+
+        # Make sure the returned URL works
+        resp = await client.head(url)
+        assert resp.status_code == 200
+
+        assert resp.headers["Upload-Length"] == "0"
+        assert resp.headers["Upload-Offset"] == "0"
 
 
 @pytest.mark.asyncio
 async def test_multiple_tus_file_upload_tries(
     writer_api, knowledgebox_writer, resource
 ):
     kb = knowledgebox_writer
@@ -548,15 +596,15 @@
         assert resp.status_code == 201
         url = resp.headers["location"]
 
         # Check that we are using the slug for the whole file upload
         assert f"{RSLUG_PREFIX}/{rslug}" in url
         resp = await client.patch(
             url,
-            data=b"x" * 10000,
+            content=b"x" * 10000,
             headers={
                 "upload-offset": "0",
                 "content-length": "10000",
                 "upload-length": "10000",
             },
         )
         assert resp.status_code == 200
@@ -571,15 +619,15 @@
         assert resp.status_code == 201
         url = resp.headers["location"]
 
         # Check that we are using the slug for the whole file upload
         assert f"{RSLUG_PREFIX}/{rslug}" in url
         resp = await client.patch(
             url,
-            data=b"x" * 10000,
+            content=b"x" * 10000,
             headers={
                 "upload-offset": "0",
                 "content-length": "10000",
                 "upload-length": "10000",
             },
         )
         assert resp.status_code == 200
@@ -591,28 +639,25 @@
 async def test_file_upload_by_slug(writer_api, knowledgebox_writer):
     kb = knowledgebox_writer
     rslug = "myslug"
 
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{kb}/resources",
-            headers={
-                "X-Synchronous": "True",
-            },
             json={
                 "slug": rslug,
             },
         )
         assert str(resp.status_code).startswith("2")
 
         filename = "image.jpg"
         with open(f"{ASSETS_PATH}/image001.jpg", "rb") as f:
             resp = await client.post(
                 f"/{KB_PREFIX}/{kb}/{RSLUG_PREFIX}/{rslug}/file/file1/{UPLOAD}",
-                data=f.read(),
+                content=f.read(),
                 headers={
                     "X-FILENAME": filename,
                     "content-type": "image/jpg",
                     "X-MD5": "7af0916dba8b70e29d99e72941923529",
                 },
             )
             assert resp.status_code == 201
@@ -646,7 +691,49 @@
 
 
 def test_maybe_b64decode():
     something = "something"
     something_encoded = base64.b64encode(something.encode())
     assert maybe_b64decode(something_encoded) == something
     assert maybe_b64decode(something) == something
+
+
+@pytest.mark.asyncio
+async def test_tus_validates_intermediate_chunks_length(
+    writer_api, knowledgebox_writer
+):
+    async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
+        language = base64.b64encode(b"ca").decode()
+        filename = base64.b64encode(b"image.jpg").decode()
+        md5 = base64.b64encode(b"7af0916dba8b70e29d99e72941923529").decode()
+        resp = await client.post(
+            f"/{KB_PREFIX}/{knowledgebox_writer}/{TUSUPLOAD}",
+            headers={
+                "tus-resumable": "1.0.0",
+                "upload-metadata": f"filename {filename},language {language},md5 {md5}",
+                "content-type": "image/jpg",
+                "upload-defer-length": "1",
+            },
+        )
+        assert resp.status_code == 201
+        url = resp.headers["location"]
+        # We upload a chunk that is smaller than the minimum chunk size
+        min_chunk_size = get_storage_manager().min_upload_size
+        raw_bytes = b"x" * min_chunk_size + b"y" * 500
+        io_bytes = io.BytesIO(raw_bytes)
+        chunk = io_bytes.read(min_chunk_size - 10)
+
+        resp = await client.head(url)
+
+        headers = {
+            "upload-offset": f"0",
+            "content-length": f"{len(chunk)}",
+        }
+        resp = await client.patch(
+            url,
+            content=chunk,
+            headers=headers,
+        )
+        assert resp.status_code == 412
+        assert resp.json()["detail"].startswith(
+            "Intermediate chunks cannot be smaller than"
+        )
```

## nucliadb/writer/tests/test_knowledgebox.py

```diff
@@ -28,26 +28,22 @@
     async with writer_api(roles=[NucliaDBRoles.MANAGER]) as client:
         resp = await client.post(
             f"/{KBS_PREFIX}",
             json={
                 "slug": "kbid1",
                 "title": "My Knowledge Box",
                 "description": "My lovely knowledgebox",
-                "enabled_filters": ["filter1", "filter2"],
-                "enabled_insights": ["insight1", "insight2"],
-                "disable_vectors": True,
             },
         )
         assert resp.status_code == 201
         data = resp.json()
         assert data["slug"] == "kbid1"
         kbid = data["uuid"]
 
         resp = await client.patch(
             f"/{KB_PREFIX}/{kbid}",
             json={
                 "slug": "kbid2",
                 "description": "My lovely knowledgebox2",
-                "disable_vectors": True,
             },
         )
         assert resp.status_code == 200
```

## nucliadb/writer/tests/test_reprocess_file_field.py

```diff
@@ -13,15 +13,15 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import AsyncIterator, Tuple
+from typing import AsyncIterator
 from unittest.mock import AsyncMock
 
 import pytest
 from nucliadb_protos.writer_pb2 import ResourceFieldId
 
 from nucliadb.ingest.processing import ProcessingInfo
 from nucliadb.writer.api.v1.router import KB_PREFIX, RESOURCE_PREFIX, RESOURCES_PREFIX
@@ -44,22 +44,21 @@
     yield processing
 
 
 @pytest.fixture(scope="function")
 @pytest.mark.asyncio
 async def file_field(
     writer_api, knowledgebox_writer: str
-) -> AsyncIterator[Tuple[str, str, str]]:
+) -> AsyncIterator[tuple[str, str, str]]:
     kbid = knowledgebox_writer
     field_id = "myfile"
 
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{kbid}/{RESOURCES_PREFIX}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={
                 "slug": "resource",
                 "title": "My resource",
                 "files": {
                     field_id: {
                         "language": "en",
                         "password": "xxxxxx",
@@ -103,15 +102,15 @@
             f"/{KB_PREFIX}/{kbid}/{RESOURCE_PREFIX}/{rid}/file/{field_id}/reprocess",
         )
         assert resp.status_code == 404
 
 
 @pytest.mark.asyncio
 async def test_reprocess_file_field_with_password(
-    writer_api, file_field: Tuple[str, str, str], processing_mock
+    writer_api, file_field: tuple[str, str, str], processing_mock
 ):
     kbid, rid, field_id = file_field
     password = "secret-password"
 
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{kbid}/{RESOURCE_PREFIX}/{rid}/file/{field_id}/reprocess",
@@ -122,15 +121,15 @@
         assert resp.status_code == 202
 
     assert processing_mock.send_to_process.await_count == 1
 
 
 @pytest.mark.asyncio
 async def test_reprocess_file_field_without_password(
-    writer_api, file_field: Tuple[str, str, str], processing_mock
+    writer_api, file_field: tuple[str, str, str], processing_mock
 ):
     kbid, rid, field_id = file_field
 
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{kbid}/{RESOURCE_PREFIX}/{rid}/file/{field_id}/reprocess",
         )
```

## nucliadb/writer/tests/test_resources.py

```diff
@@ -14,22 +14,24 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from datetime import datetime
-from typing import Any, Callable, Dict, List, Optional
+from typing import Any, Callable, Optional
 from unittest.mock import AsyncMock  # type: ignore
 
 import pytest
 from httpx import AsyncClient
 from nucliadb_protos.writer_pb2 import ResourceFieldId
 
 import nucliadb_models
+from nucliadb.common.maindb.local import LocalDriver
+from nucliadb.common.maindb.redis import RedisDriver
 from nucliadb.ingest.orm.resource import Resource
 from nucliadb.ingest.processing import PushPayload
 from nucliadb.writer.api.v1.router import (
     KB_PREFIX,
     RESOURCE_PREFIX,
     RESOURCES_PREFIX,
     RSLUG_PREFIX,
@@ -45,84 +47,36 @@
     TEST_TEXT_PAYLOAD,
 )
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_utils.utilities import get_ingest
 
 
 @pytest.mark.asyncio
+@pytest.mark.xfail(reason="New vectorset api is not implemented yet")
 async def test_resource_crud_min(
-    writer_api: Callable[[List[str]], AsyncClient], knowledgebox_writer: str
+    writer_api: Callable[[list[str]], AsyncClient], knowledgebox_writer: str
 ):
     knowledgebox_id = knowledgebox_writer
     async with writer_api([NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_id}/vectorset/base",
             json={"dimension": 3, "similarity": "dot"},
         )
         assert resp.status_code == 200
-        # Test create resource
-        resp = await client.post(
-            f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
-            json={
-                "uservectors": [
-                    {
-                        "vectors": {
-                            "base": {
-                                "vector1": {
-                                    "vector": [4.0, 2.0, 3.0],
-                                    "positions": [0, 0],
-                                }
-                            }
-                        },
-                        "field": {"field_type": "file", "field": "field1"},
-                    }
-                ]
-            },
-        )
-        assert resp.status_code == 201
-
-
-@pytest.mark.asyncio
-async def test_resource_crud_min_no_vectorset(
-    writer_api: Callable[[List[str]], AsyncClient], knowledgebox_writer: str
-):
-    knowledgebox_id = knowledgebox_writer
-    async with writer_api([NucliaDBRoles.WRITER]) as client:
-        # Test create resource
-        resp = await client.post(
-            f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
-            json={
-                "uservectors": [
-                    {
-                        "vectors": {
-                            "base": {
-                                "vector1": {
-                                    "vector": [4.0, 2.0, 3.0],
-                                    "positions": [0, 0],
-                                }
-                            }
-                        },
-                        "field": {"field_type": "file", "field": "field1"},
-                    }
-                ]
-            },
-        )
-        assert resp.status_code == 201
 
 
 @pytest.mark.asyncio
 async def test_resource_crud(
-    writer_api: Callable[[List[str]], AsyncClient], knowledgebox_writer: str
+    writer_api: Callable[[list[str]], AsyncClient], knowledgebox_writer: str
 ):
     knowledgebox_id = knowledgebox_writer
     async with writer_api([NucliaDBRoles.WRITER]) as client:
         # Test create resource
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={
                 "slug": "resource1",
                 "title": "My resource",
                 "summary": "Some summary",
                 "icon": "image/png",
                 "layout": "layout",
                 "metadata": {
@@ -203,22 +157,21 @@
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCE_PREFIX}/{rid}",
         )
         assert resp.status_code == 204
 
 
 @pytest.mark.asyncio
 async def test_resource_crud_sync(
-    writer_api: Callable[[List[str]], AsyncClient], knowledgebox_writer: str
+    writer_api: Callable[[list[str]], AsyncClient], knowledgebox_writer: str
 ):
     knowledgebox_id = knowledgebox_writer
     async with writer_api([NucliaDBRoles.WRITER]) as client:
         # Test create resource
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCES_PREFIX}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={
                 "slug": "resource1",
                 "title": "My resource",
                 "summary": "Some summary",
                 "icon": "image/png",
                 "layout": "layout",
                 "metadata": {
@@ -288,49 +241,53 @@
 
         res = await ingest.ResourceFieldExists(pbrequest)  # type: ignore
         assert res.found
 
         # Test update resource
         resp = await client.patch(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCE_PREFIX}/{rid}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={},
         )
         assert resp.status_code == 200
 
         # Test delete resource
 
         resp = await client.delete(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCE_PREFIX}/resource1",
-            headers={"X-SYNCHRONOUS": "True"},
         )
 
         assert resp.status_code == 404
 
         resp = await client.delete(
             f"/{KB_PREFIX}/{knowledgebox_id}/{RESOURCE_PREFIX}/{rid}",
-            headers={"X-SYNCHRONOUS": "True"},
         )
         assert resp.status_code == 204
 
         res = await ingest.ResourceFieldExists(pbrequest)  # type: ignore
         assert not res.found
 
 
 @pytest.mark.asyncio
 async def test_reprocess_resource(
-    writer_api: Callable[..., AsyncClient], test_resource: Resource, mocker
+    writer_api: Callable[..., AsyncClient],
+    test_resource: Resource,
+    mocker,
+    maindb_driver,
 ) -> None:
+    if isinstance(maindb_driver, (LocalDriver, RedisDriver)):
+        pytest.skip("Keys might not be ordered correctly in this driver")
+
     rsc = test_resource
     kbid = rsc.kb.kbid
     rid = rsc.uuid
 
     from nucliadb.writer.utilities import get_processing
 
     processing = get_processing()
+    processing.values.clear()  # type: ignore
 
     original = processing.send_to_process
     mocker.patch.object(processing, "send_to_process", AsyncMock(side_effect=original))
 
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         resp = await client.post(
             f"/{KB_PREFIX}/{kbid}/resource/{rid}/reprocess",
@@ -340,56 +297,58 @@
         assert processing.send_to_process.call_count == 1  # type: ignore
         payload = processing.send_to_process.call_args[0][0]  # type: ignore
         assert isinstance(payload, PushPayload)
         assert payload.uuid == rid
         assert payload.kbid == kbid
 
         assert isinstance(payload.filefield.get("file1"), str)
-        assert payload.filefield["file1"] == "DUMMYJWT"
+        assert payload.filefield["file1"] == "convert_internal_filefield_to_str,0"
         assert isinstance(payload.linkfield.get("link1"), nucliadb_models.LinkUpload)
         assert isinstance(payload.textfield.get("text1"), nucliadb_models.Text)
         assert isinstance(
             payload.layoutfield.get("layout1"), nucliadb_models.LayoutDiff
         )
-        assert payload.layoutfield["layout1"].blocks["field1"].file == "DUMMYJWT"
+        assert (
+            payload.layoutfield["layout1"].blocks["field1"].file
+            == "convert_internal_cf_to_str,2"
+        )
         assert isinstance(
             payload.conversationfield.get("conv1"), nucliadb_models.PushConversation
         )
         assert (
             payload.conversationfield["conv1"].messages[33].content.attachments[0]
-            == "DUMMYJWT"
+            == "convert_internal_cf_to_str,0"
         )
         assert (
             payload.conversationfield["conv1"].messages[33].content.attachments[1]
-            == "DUMMYJWT"
+            == "convert_internal_cf_to_str,1"
         )
 
 
 @pytest.mark.asyncio
 @pytest.mark.parametrize(
     "method,endpoint,payload",
     [
         ["patch", "/{KB_PREFIX}/{kb}/{RSLUG_PREFIX}/{slug}", {}],
         ["post", "/{KB_PREFIX}/{kb}/{RSLUG_PREFIX}/{slug}/reprocess", None],
         ["post", "/{KB_PREFIX}/{kb}/{RSLUG_PREFIX}/{slug}/reindex", None],
         ["delete", "/{KB_PREFIX}/{kb}/{RSLUG_PREFIX}/{slug}", None],
     ],
 )
 async def test_resource_endpoints_by_slug(
-    writer_api: Callable[[List[str]], AsyncClient],
+    writer_api: Callable[[list[str]], AsyncClient],
     knowledgebox_ingest: str,
     method: str,
     endpoint: str,
-    payload: Optional[Dict[Any, Any]],
+    payload: Optional[dict[Any, Any]],
 ):
     async with writer_api([NucliaDBRoles.WRITER]) as client:
         slug = "my-resource"
         resp = await client.post(
             f"/{KB_PREFIX}/{knowledgebox_ingest}/{RESOURCES_PREFIX}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={
                 "slug": slug,
                 "texts": {"text1": {"body": "test1", "format": "PLAIN"}},
             },
         )
         assert resp.status_code == 201
 
@@ -464,15 +423,14 @@
 @pytest.mark.asyncio
 async def test_paragraph_annotations(writer_api, knowledgebox_writer):
     kbid = knowledgebox_writer
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
         # Must have at least one classification
         resp = await client.post(
             f"/{KB_PREFIX}/{kbid}/resources",
-            headers={"X-SYNCHRONOUS": "True"},
             json={
                 "texts": {"text1": TEST_TEXT_PAYLOAD},
                 "fieldmetadata": [
                     {
                         "paragraphs": [
                             {
                                 "key": "paragraph1",
@@ -488,15 +446,14 @@
         body = resp.json()
         assert body["detail"] == "ensure classifications has at least 1 items"
 
         classification = {"label": "label", "labelset": "ls"}
 
         resp = await client.post(
             f"/{KB_PREFIX}/{kbid}/resources",
-            headers={"X-SYNCHRONOUS": "True"},
             json={
                 "texts": {"text1": TEST_TEXT_PAYLOAD},
                 "fieldmetadata": [
                     {
                         "paragraphs": [
                             {
                                 "key": "paragraph1",
@@ -510,15 +467,14 @@
         )
         assert resp.status_code == 201
         rid = resp.json()["uuid"]
 
         # Classifications need to be unique
         resp = await client.patch(
             f"/{KB_PREFIX}/{kbid}/resource/{rid}",
-            headers={"X-SYNCHRONOUS": "True"},
             json={
                 "fieldmetadata": [
                     {
                         "paragraphs": [
                             {
                                 "key": "paragraph1",
                                 "classifications": [classification, classification],
```

## nucliadb/writer/tests/test_service.py

```diff
@@ -16,15 +16,15 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import pytest
 
 from nucliadb.writer.api.v1.router import KB_PREFIX, KBS_PREFIX
-from nucliadb_models.entities import EntitiesGroup, Entity
+from nucliadb_models.entities import CreateEntitiesGroupPayload, Entity
 from nucliadb_models.labels import Label, LabelSet
 from nucliadb_models.resource import NucliaDBRoles
 from nucliadb_protos import knowledgebox_pb2, writer_pb2
 from nucliadb_utils.utilities import get_ingest
 
 
 @pytest.mark.asyncio
@@ -39,83 +39,79 @@
         )
         assert resp.status_code == 201
         data = resp.json()
         assert data["slug"] == "kbid1"
         kbid = data["uuid"]
 
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
-        eg = EntitiesGroup()
-        eg.title = "My group"
-        eg.color = "#0000000"
-        eg.entities["ent1"] = Entity(value="asd", merged=False)
-        eg.entities["ent2"] = Entity(value="asd", merged=False)
-        eg.entities["ent3"] = Entity(value="asd", merged=False)
+        eg = CreateEntitiesGroupPayload(
+            group="0",
+            title="My group",
+            color="#0000000",
+            entities={
+                "ent1": Entity(value="asd", merged=False),
+                "ent2": Entity(value="asd", merged=False),
+                "ent3": Entity(value="asd", merged=False),
+            },
+        )
 
-        resp = await client.post(f"/{KB_PREFIX}/{kbid}/entitiesgroup/0", json=eg.dict())
+        resp = await client.post(f"/{KB_PREFIX}/{kbid}/entitiesgroups", json=eg.dict())
         assert resp.status_code == 200
 
         ingest = get_ingest()
         result = await ingest.GetEntities(
             writer_pb2.GetEntitiesRequest(kb=knowledgebox_pb2.KnowledgeBoxID(uuid=kbid))
         )
         assert set(result.groups.keys()) == {"0"}
         assert result.groups["0"].title == eg.title
         assert result.groups["0"].color == eg.color
-        assert set(result.groups["0"].entities.keys()) == {"ent3", "ent1", "ent2"}
+        assert set(result.groups["0"].entities.keys()) == {"ent1", "ent2", "ent3"}
         assert result.groups["0"].entities["ent1"].value == "asd"
 
-        resp = await client.post(f"/{KB_PREFIX}/{kbid}/entitiesgroup/1", json=eg.dict())
+        eg.group = "1"
+        resp = await client.post(f"/{KB_PREFIX}/{kbid}/entitiesgroups", json=eg.dict())
         assert resp.status_code == 200
         result = await ingest.GetEntities(
             writer_pb2.GetEntitiesRequest(kb=knowledgebox_pb2.KnowledgeBoxID(uuid=kbid))
         )
         assert set(result.groups.keys()) == {"0", "1"}
 
 
 @pytest.mark.asyncio
-async def test_entities_custom_field(writer_api, entities_manager_mock):
+async def test_entities_custom_field_for_user_defined_groups(
+    writer_api, entities_manager_mock
+):
     """
     Test description:
 
     - Create an entity group and check that the default value for the `custom`
-      field is False
-
-    - Create another entity group and set `custom` to True. Check that on a
-      subsequent get, the value is correct.
+      field is True
     """
     async with writer_api(roles=[NucliaDBRoles.MANAGER]) as client:
         resp = await client.post(
             f"/{KBS_PREFIX}",
             json={
                 "slug": "kbid1",
                 "title": "My Knowledge Box",
             },
         )
         assert resp.status_code == 201
         data = resp.json()
         kbid = data["uuid"]
 
     async with writer_api(roles=[NucliaDBRoles.WRITER]) as client:
-        eg = EntitiesGroup()
-        custom_eg = EntitiesGroup(custom=True)
-
-        resp = await client.post(f"/{KB_PREFIX}/{kbid}/entitiesgroup/0", json=eg.dict())
-        assert resp.status_code == 200
-
-        resp = await client.post(
-            f"/{KB_PREFIX}/{kbid}/entitiesgroup/1", json=custom_eg.dict()
-        )
+        eg = CreateEntitiesGroupPayload(group="0")
+        resp = await client.post(f"/{KB_PREFIX}/{kbid}/entitiesgroups", json=eg.dict())
         assert resp.status_code == 200
 
         ingest = get_ingest()
         result = await ingest.GetEntities(
             writer_pb2.GetEntitiesRequest(kb=knowledgebox_pb2.KnowledgeBoxID(uuid=kbid))
         )
-        assert result.groups["0"].custom is False
-        assert result.groups["1"].custom is True
+        assert result.groups["0"].custom is True
 
 
 @pytest.mark.asyncio
 async def test_service_lifecycle_labels(writer_api):
     async with writer_api(roles=[NucliaDBRoles.MANAGER]) as client:
         resp = await client.post(
             f"/{KBS_PREFIX}",
```

## nucliadb/writer/tests/test_tus.py

```diff
@@ -13,41 +13,134 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import tempfile
 import uuid
-from typing import Dict
 
+import asyncpg
 import pytest
 
 from nucliadb.writer.settings import settings
 from nucliadb.writer.tus import get_dm
 from nucliadb.writer.tus.exceptions import CloudFileNotFound
 from nucliadb.writer.tus.gcs import GCloudBlobStore, GCloudFileStorageManager
 from nucliadb.writer.tus.local import LocalBlobStore, LocalFileStorageManager
+from nucliadb.writer.tus.pg import PGBlobStore, PGFileStorageManager
 from nucliadb.writer.tus.s3 import S3BlobStore, S3FileStorageManager
 from nucliadb.writer.tus.storage import BlobStore, FileStorageManager
+from nucliadb_utils.storages.pg import PostgresStorage
 from nucliadb_utils.storages.storage import KB_RESOURCE_FIELD
 
 
+@pytest.fixture(scope="function")
+async def s3_storage_tus(s3):
+    storage = S3BlobStore()
+    await storage.initialize(
+        client_id="",
+        client_secret="",
+        max_pool_connections=2,
+        endpoint_url=s3,
+        verify_ssl=False,
+        ssl=False,
+        region_name=None,
+        bucket="test_{kbid}",
+        bucket_tags={"testTag": "test"},
+    )
+    yield storage
+    await storage.finalize()
+
+
+@pytest.fixture(scope="function")
+async def gcs_storage_tus(gcs):
+    storage = GCloudBlobStore()
+    await storage.initialize(
+        json_credentials=None,
+        bucket="test_{kbid}",
+        location="location",
+        project="project",
+        bucket_labels={},
+        object_base_url=gcs,
+    )
+    yield storage
+    await storage.finalize()
+
+
+@pytest.fixture(scope="function")
+async def local_storage_tus():
+    folder = tempfile.TemporaryDirectory()
+    storage = LocalBlobStore(local_testing_files=folder.name)
+    await storage.initialize()
+    yield storage
+    await storage.finalize()
+    folder.cleanup()
+
+
+@pytest.fixture(scope="function")
+async def pg_storage_tus(pg):
+    dsn = f"postgresql://postgres:postgres@{pg[0]}:{pg[1]}/postgres"
+    conn = await asyncpg.connect(dsn)
+    await conn.execute(
+        """
+DROP table IF EXISTS kb_files;
+DROP table IF EXISTS kb_files_fileparts;
+"""
+    )
+    await conn.close()
+    fstorage = PostgresStorage(dsn)  # set everything up
+    await fstorage.initialize()
+    await fstorage.finalize()
+
+    storage = PGBlobStore(dsn)
+    await storage.initialize()
+    yield storage
+    await storage.finalize()
+
+
+async def clean_dm():
+    from nucliadb.writer.tus import REDIS_FILE_DATA_MANAGER_FACTORY
+
+    if REDIS_FILE_DATA_MANAGER_FACTORY is not None:
+        await REDIS_FILE_DATA_MANAGER_FACTORY.finalize()
+        REDIS_FILE_DATA_MANAGER_FACTORY = None
+
+
+@pytest.fixture(scope="function")
+async def redis_dm(redis):
+    prev = settings.dm_enabled
+
+    settings.dm_enabled = True
+    settings.dm_redis_host = redis[0]
+    settings.dm_redis_port = redis[1]
+
+    dm = get_dm()
+
+    yield dm
+
+    await clean_dm()
+
+    settings.dm_enabled = prev
+
+
 @pytest.mark.asyncio
-async def test_s3_driver(s3_storage_tus: S3BlobStore):
-    settings.dm_enabled = False
+async def test_pg_driver(redis_dm, pg_storage_tus: PGBlobStore):
+    await storage_test(pg_storage_tus, PGFileStorageManager(pg_storage_tus))
+
+
+@pytest.mark.asyncio
+async def test_s3_driver(redis_dm, s3_storage_tus: S3BlobStore):
     await storage_test(s3_storage_tus, S3FileStorageManager(s3_storage_tus))
-    settings.dm_enabled = True
 
 
 @pytest.mark.asyncio
-async def test_gcs_driver(gcs_storage_tus: GCloudBlobStore):
-    settings.dm_enabled = False
+async def test_gcs_driver(redis_dm, gcs_storage_tus: GCloudBlobStore):
     await storage_test(gcs_storage_tus, GCloudFileStorageManager(gcs_storage_tus))
-    settings.dm_enabled = True
 
 
 @pytest.mark.asyncio
 async def test_local_driver(local_storage_tus: LocalBlobStore):
     settings.dm_enabled = False
     await storage_test(local_storage_tus, LocalFileStorageManager(local_storage_tus))
     settings.dm_enabled = True
@@ -55,26 +148,29 @@
 
 async def storage_test(storage: BlobStore, file_storage_manager: FileStorageManager):
     example = b"mytestinfo"
     field = "myfield"
     rid = "myrid"
     kbid = "mykb_tus_test"
 
-    metadata: Dict[str, str] = {}
+    metadata: dict[str, str] = {}
     bucket_name = storage.get_bucket_name(kbid)
     assert bucket_name in [
         "test_mykb_tus_test",
         "test-mykb-tus-test",
         "ndb_mykb_tus_test",
+        "mykb_tus_test",
     ]
 
-    assert await storage.check_exists(bucket_name) is False
+    if not isinstance(storage, PGBlobStore):
+        # this is silly, but we don't need this for pg
+        assert await storage.check_exists(bucket_name) is False
 
-    exists = await storage.create_bucket(bucket_name)
-    assert exists is False
+        exists = await storage.create_bucket(bucket_name)
+        assert exists is False
 
     upload_id = uuid.uuid4().hex
     dm = get_dm()
     await dm.load(upload_id)
     await dm.start({})
     await dm.update(
         upload_file_id=f"{upload_id}",
```

## nucliadb/writer/tus/__init__.py

```diff
@@ -17,18 +17,19 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from dataclasses import dataclass
 from typing import Optional
 
 from nucliadb.writer.settings import settings as writer_settings
-from nucliadb.writer.tus.dm import FileDataMangaer, RedisFileDataManagerFactory
+from nucliadb.writer.tus.dm import FileDataManager, RedisFileDataManagerFactory
 from nucliadb.writer.tus.exceptions import ManagerNotAvailable
 from nucliadb.writer.tus.gcs import GCloudBlobStore, GCloudFileStorageManager
 from nucliadb.writer.tus.local import LocalBlobStore, LocalFileStorageManager
+from nucliadb.writer.tus.pg import PGBlobStore, PGFileStorageManager
 from nucliadb.writer.tus.s3 import S3BlobStore, S3FileStorageManager
 from nucliadb.writer.tus.storage import BlobStore, FileStorageManager
 from nucliadb_utils.exceptions import ConfigurationError
 from nucliadb_utils.settings import FileBackendConfig, storage_settings
 
 TUSUPLOAD = "tusupload"
 UPLOAD = "upload"
@@ -42,15 +43,14 @@
 
 DRIVER: Optional[TusStorageDriver] = None
 REDIS_FILE_DATA_MANAGER_FACTORY: Optional[RedisFileDataManagerFactory] = None
 
 
 async def initialize():
     global DRIVER
-
     if storage_settings.file_backend == FileBackendConfig.GCS:
         storage_backend = GCloudBlobStore()
 
         await storage_backend.initialize(
             json_credentials=storage_settings.gcs_base64_creds,
             bucket=storage_settings.gcs_bucket,
             location=storage_settings.gcs_location,
@@ -71,14 +71,15 @@
             client_secret=storage_settings.s3_client_secret,
             ssl=storage_settings.s3_ssl,
             verify_ssl=storage_settings.s3_verify_ssl,
             max_pool_connections=storage_settings.s3_max_pool_connections,
             endpoint_url=storage_settings.s3_endpoint,
             region_name=storage_settings.s3_region_name,
             bucket=storage_settings.s3_bucket,
+            bucket_tags=storage_settings.s3_bucket_tags,
         )
 
         storage_manager = S3FileStorageManager(storage_backend)
 
         DRIVER = TusStorageDriver(backend=storage_backend, manager=storage_manager)
 
     elif storage_settings.file_backend == FileBackendConfig.LOCAL:
@@ -86,14 +87,23 @@
 
         await storage_backend.initialize()
 
         storage_manager = LocalFileStorageManager(storage_backend)
 
         DRIVER = TusStorageDriver(backend=storage_backend, manager=storage_manager)
 
+    elif storage_settings.file_backend == FileBackendConfig.PG:
+        storage_backend = PGBlobStore(storage_settings.driver_pg_url)
+
+        await storage_backend.initialize()
+
+        storage_manager = PGFileStorageManager(storage_backend)
+
+        DRIVER = TusStorageDriver(backend=storage_backend, manager=storage_manager)
+
     elif storage_settings.file_backend == FileBackendConfig.NOT_SET:
         raise ConfigurationError("FILE_BACKEND env variable not configured")
 
 
 async def finalize():
     global DRIVER
     global REDIS_FILE_DATA_MANAGER_FACTORY
@@ -103,24 +113,24 @@
         DRIVER = None
 
     if REDIS_FILE_DATA_MANAGER_FACTORY is not None:
         await REDIS_FILE_DATA_MANAGER_FACTORY.finalize()
         REDIS_FILE_DATA_MANAGER_FACTORY = None
 
 
-def get_dm() -> FileDataMangaer:  # type: ignore
+def get_dm() -> FileDataManager:  # type: ignore
     if writer_settings.dm_enabled:
         global REDIS_FILE_DATA_MANAGER_FACTORY
         if REDIS_FILE_DATA_MANAGER_FACTORY is None:
             REDIS_FILE_DATA_MANAGER_FACTORY = RedisFileDataManagerFactory(
                 f"redis://{writer_settings.dm_redis_host}:{writer_settings.dm_redis_port}"
             )
-        dm_driver: FileDataMangaer = REDIS_FILE_DATA_MANAGER_FACTORY()
+        dm_driver: FileDataManager = REDIS_FILE_DATA_MANAGER_FACTORY()
     else:
-        dm_driver = FileDataMangaer()
+        dm_driver = FileDataManager()
     return dm_driver
 
 
 def get_storage_manager() -> FileStorageManager:
     global DRIVER
 
     if DRIVER is None:
```

## nucliadb/writer/tus/dm.py

```diff
@@ -14,32 +14,34 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import time
-from typing import Any, Dict, Optional
+from typing import Any, Optional
 
 import orjson
 from redis import asyncio as aioredis
 from starlette.requests import Request
 
+from nucliadb.writer import logger
+
 from .exceptions import HTTPPreconditionFailed
 
 
 class NoRedisConfigured(Exception):
     pass
 
 
-DATA: Dict[str, Any] = {}
+DATA: dict[str, Any] = {}
 
 
-class FileDataMangaer:
-    _data: Optional[Dict[str, Any]] = None
+class FileDataManager:
+    _data: Optional[dict[str, Any]] = None
     _loaded = False
     key = None
     _ttl = 60 * 50 * 5  # 5 minutes should be plenty of time between activity
 
     async def load(self, key):
         # preload data
         self.key = key
@@ -94,16 +96,22 @@
         return self._data.get("metadata", {})
 
     @property
     def content_type(self):
         return self.metadata.get("content_type")
 
     @property
-    def size(self):
-        return self._data.get("size", 0)
+    def size(self) -> int:
+        default = 0
+        if self._data is None:
+            return default
+        size = self._data.get("size")
+        if size is None:
+            return default
+        return int(size)
 
     @property
     def offset(self):
         return self._data.get("offset", 0)
 
     @property
     def filename(self):
@@ -123,18 +131,22 @@
     def __init__(self, redis_url: str):
         self.redis = aioredis.from_url(redis_url)
 
     def __call__(self):
         return RedisFileDataManager(self.redis)
 
     async def finalize(self):
-        await self.redis.close(close_connection_pool=True)
+        try:
+            await self.redis.close(close_connection_pool=True)
+        except Exception:
+            logger.warning("Error closing redis connection", exc_info=True)
+            pass
 
 
-class RedisFileDataManager(FileDataMangaer):
+class RedisFileDataManager(FileDataManager):
     def __init__(self, redis: aioredis.Redis):
         self.redis = redis
 
     async def load(self, key):
         # preload data
         self.key = key
         if self._data is None:
```

## nucliadb/writer/tus/gcs.py

```diff
@@ -19,53 +19,60 @@
 #
 from __future__ import annotations
 
 import asyncio
 import base64
 import json
 import os
+import socket
 import tempfile
 import uuid
 from concurrent.futures import ThreadPoolExecutor
 from copy import deepcopy
 from datetime import datetime
-from typing import AsyncIterator, Dict, Optional
+from typing import AsyncIterator, Optional
 from urllib.parse import quote_plus
 
 import aiohttp
-import backoff  # type: ignore
+import backoff
 from nucliadb_protos.resources_pb2 import CloudFile
 from oauth2client.service_account import ServiceAccountCredentials  # type: ignore
 
 from nucliadb.writer import logger
-from nucliadb.writer.tus.dm import FileDataMangaer
+from nucliadb.writer.tus.dm import FileDataManager
 from nucliadb.writer.tus.exceptions import (
     CloudFileNotFound,
     HTTPBadRequest,
     HTTPNotFound,
     HTTPPreconditionFailed,
     ResumableURINotAvailable,
 )
 from nucliadb.writer.tus.storage import BlobStore, FileStorageManager
 from nucliadb.writer.tus.utils import to_str
+from nucliadb_utils.storages.gcs import CHUNK_SIZE, MIN_UPLOAD_SIZE
 
 
 class GoogleCloudException(Exception):
     pass
 
 
 SCOPES = ["https://www.googleapis.com/auth/devstorage.read_write"]
 MAX_RETRIES = 5
 
 
 RETRIABLE_EXCEPTIONS = (
     GoogleCloudException,
     aiohttp.client_exceptions.ClientPayloadError,
+    aiohttp.client_exceptions.ClientConnectorError,
+    aiohttp.client_exceptions.ClientConnectionError,
+    aiohttp.client_exceptions.ClientOSError,
+    aiohttp.client_exceptions.ServerConnectionError,
+    aiohttp.client_exceptions.ServerDisconnectedError,
+    socket.gaierror,
 )
-CHUNK_SIZE = 524288
 
 
 class GCloudBlobStore(BlobStore):
     session: Optional[aiohttp.ClientSession] = None
     loop = None
     upload_url: str
     object_base_url: str
@@ -105,15 +112,14 @@
         self.location = location
         self.project = project
         self.bucket_labels = bucket_labels
         self.object_base_url = object_base_url + "/storage/v1/b"
         self.upload_url = (
             object_base_url + "/upload/storage/v1/b/{bucket}/o?uploadType=resumable"
         )  # noqa
-        self.session = aiohttp.ClientSession()
 
         self._credentials = None
 
         if json_credentials is not None:
             self.json_credentials_file = os.path.join(
                 tempfile.mkdtemp(), "gcs_credentials.json"
             )
@@ -165,17 +171,20 @@
                 assert resp.status == 200
         return found
 
 
 class GCloudFileStorageManager(FileStorageManager):
     storage: GCloudBlobStore
     chunk_size = CHUNK_SIZE
+    min_upload_size = MIN_UPLOAD_SIZE
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=4)
-    async def start(self, dm: FileDataMangaer, path: str, kbid: str):
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=4
+    )
+    async def start(self, dm: FileDataManager, path: str, kbid: str):
         """Init an upload.
 
         _uload_file_id : temporal url to image beeing uploaded
         _resumable_uri : uri to resumable upload
         _uri : finished uploaded image
         """
         if self.storage.session is None:
@@ -228,15 +237,17 @@
                 raise GoogleCloudException(text)
             resumable_uri = call.headers["Location"]
 
         await dm.update(
             resumable_uri=resumable_uri, upload_file_id=upload_file_id, path=path
         )
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=4)
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=4
+    )
     async def delete_upload(self, uri, kbid):
         bucket = self.storage.get_bucket_name(kbid)
 
         if uri is not None:
             url = "{}/{}/o/{}".format(
                 self.storage.object_base_url,
                 bucket,
@@ -260,19 +271,22 @@
                             exc_info=True,
                         )
                     else:
                         raise GoogleCloudException(f"{resp.status}: {json.dumps(data)}")
         else:
             raise AttributeError("No valid uri")
 
-    async def _append(self, dm: FileDataMangaer, data, offset):
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=4
+    )
+    async def _append(self, dm: FileDataManager, data, offset):
         if self.storage.session is None:
             raise AttributeError()
         if dm.size:
-            size = dm.size
+            size = str(dm.size)
         else:
             # assuming size will come eventually
             size = "*"
         content_range = "bytes {init}-{chunk}/{total}".format(
             init=offset, chunk=offset + len(data) - 1, total=size
         )
         resumable_uri = dm.get("resumable_uri")
@@ -291,18 +305,18 @@
                 "Content-Type": content_type,
                 "Content-Range": content_range,
             },
             data=data,
         ) as call:
             text = await call.text()  # noqa
             if call.status not in [200, 201, 308]:
-                logger.error(text)
+                raise GoogleCloudException(f"{call.status}: {text}")
             return call
 
-    async def append(self, dm: FileDataMangaer, iterable, offset) -> int:
+    async def append(self, dm: FileDataManager, iterable, offset) -> int:
         count = 0
 
         async for chunk in iterable:
             resp = await self._append(dm, chunk, offset)
             size = len(chunk)
             count += size
             offset += len(chunk)
@@ -324,15 +338,18 @@
                 raise HTTPBadRequest(detail=await resp.text())
 
             elif resp.status in [200, 201]:
                 # file manager will double check offsets and sizes match
                 break
         return count
 
-    async def finish(self, dm: FileDataMangaer):
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=4
+    )
+    async def finish(self, dm: FileDataManager):
         if dm.size == 0:
             if self.storage.session is None:
                 raise AttributeError()
             # If there is been no size finish the upload
             content_range = "bytes {init}-{chunk}/{total}".format(
                 init=dm.offset, chunk=dm.offset, total=dm.offset
             )
@@ -343,21 +360,21 @@
                     "Content-Length": "0",
                     "Content-Range": content_range,
                 },
                 data="",
             ) as call:
                 text = await call.text()  # noqa
                 if call.status not in [200, 201, 308]:
-                    logger.error(text)
+                    raise GoogleCloudException(f"{call.status}: {text}")
                 return call
         path = dm.get("path")
         await dm.finish()
         return path
 
-    async def iter_data(self, uri, kbid: str, headers: Optional[Dict[str, str]] = None):
+    async def iter_data(self, uri, kbid: str, headers: Optional[dict[str, str]] = None):
         if self.storage.session is None:
             raise AttributeError()
         if headers is None:
             headers = {}
 
         url = "{}/{}/o/{}".format(
             self.storage.object_base_url,
```

## nucliadb/writer/tus/local.py

```diff
@@ -23,45 +23,49 @@
 import os
 import uuid
 from typing import AsyncIterator
 
 import aiofiles
 from nucliadb_protos.resources_pb2 import CloudFile
 
-from nucliadb.writer.tus.dm import FileDataMangaer
+from nucliadb.writer.tus.dm import FileDataManager
 from nucliadb.writer.tus.exceptions import CloudFileNotFound
 from nucliadb.writer.tus.storage import BlobStore, FileStorageManager
 from nucliadb_utils.storages import CHUNK_SIZE
 
 
 class LocalFileStorageManager(FileStorageManager):
     _handler = None
     storage: LocalBlobStore
     chunk_size = CHUNK_SIZE
+    min_chunk_size = None
 
     def metadata_key(self, uri: str) -> str:
         return f"{uri}.metadata"
 
     def get_file_path(self, bucket: str, key: str):
         bucket_path = self.storage.get_bucket_path(bucket)
         return f"{bucket_path}/{key}"
 
-    async def start(self, dm: FileDataMangaer, path: str, kbid: str):
+    async def start(self, dm: FileDataManager, path: str, kbid: str):
         bucket = self.storage.get_bucket_name(kbid)
         upload_file_id = dm.get("upload_file_id", str(uuid.uuid4()))
         init_url = self.get_file_path(bucket, upload_file_id)
         metadata_init_url = self.metadata_key(init_url)
         metadata = {
             "FILENAME": dm.filename,
             "CONTENT_TYPE": dm.content_type,
             "SIZE": dm.size,
         }
         async with aiofiles.open(metadata_init_url, "w+") as resp:
             await resp.write(json.dumps(metadata))
 
+        async with aiofiles.open(init_url, "wb+") as aio_fi:
+            await aio_fi.write(b"")
+
         await dm.update(upload_file_id=upload_file_id, path=path, bucket=bucket)
 
     async def iter_data(self, uri, kbid: str, headers=None):
         bucket = self.storage.get_bucket_name(kbid)
         file_path = self.get_file_path(bucket, uri)
         async with aiofiles.open(file_path) as resp:
             data = await resp.read(CHUNK_SIZE)
@@ -88,33 +92,30 @@
                         data = data[:new_end]
                     yield data
                     count += len(data)
                     data = await resp.read(CHUNK_SIZE)
         except FileNotFoundError:
             raise CloudFileNotFound()
 
-    async def _append(self, data, offset, aiofi):
-        await aiofi.seek(offset)
-        await aiofi.write(data)
-        await aiofi.flush()
-
-    async def append(self, dm: FileDataMangaer, iterable, offset) -> int:
+    async def append(self, dm: FileDataManager, iterable, offset) -> int:
         count = 0
         bucket = dm.get("bucket")
         upload_file_id = dm.get("upload_file_id")
         init_url = self.get_file_path(bucket, upload_file_id)
-        async with aiofiles.open(init_url, "wb") as aiofi:
+        async with aiofiles.open(init_url, "rb+") as aio_fi:
+            await aio_fi.seek(offset)
             async for chunk in iterable:
-                await self._append(chunk, offset, aiofi)
+                await aio_fi.write(chunk)
                 size = len(chunk)
                 count += size
                 offset += size
+            await aio_fi.flush()
         return count
 
-    async def finish(self, dm: FileDataMangaer):
+    async def finish(self, dm: FileDataManager):
         # Move from old to new
         bucket = dm.get("bucket")
 
         upload_file_id = dm.get("upload_file_id")
         from_url = self.get_file_path(bucket, upload_file_id)
 
         path = dm.get("path")
```

## nucliadb/writer/tus/s3.py

```diff
@@ -17,104 +17,125 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
 import uuid
 from contextlib import AsyncExitStack
-from typing import AsyncIterator, Dict, Optional
+from typing import AsyncIterator, Optional
 
 import aiobotocore  # type: ignore
 import aiohttp
 import backoff  # type: ignore
 import botocore  # type: ignore
 from aiobotocore.session import AioSession  # type: ignore
 from nucliadb_protos.resources_pb2 import CloudFile
 
 from nucliadb.writer import logger
-from nucliadb.writer.tus.dm import FileDataMangaer
-from nucliadb.writer.tus.exceptions import CloudFileNotFound
+from nucliadb.writer.tus.dm import FileDataManager
+from nucliadb.writer.tus.exceptions import CloudFileNotFound, ResumableURINotAvailable
 from nucliadb.writer.tus.storage import BlobStore, FileStorageManager
+from nucliadb_utils.storages.s3 import (
+    CHUNK_SIZE,
+    MIN_UPLOAD_SIZE,
+    bucket_exists,
+    create_bucket,
+)
 
 RETRIABLE_EXCEPTIONS = (
     botocore.exceptions.ClientError,
     aiohttp.client_exceptions.ClientPayloadError,
     botocore.exceptions.BotoCoreError,
 )
-CHUNK_SIZE = 5 * 1024 * 1024
 
 
 class S3FileStorageManager(FileStorageManager):
     storage: S3BlobStore
     chunk_size = CHUNK_SIZE
+    min_upload_size = MIN_UPLOAD_SIZE
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=3)
-    async def _abort_multipart(self, dm: FileDataMangaer):
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=3
+    )
+    async def _abort_multipart(self, dm: FileDataManager):
         try:
             mpu = dm.get("mpu")
             upload_file_id = dm.get("upload_file_id")
             await self.storage._s3aioclient.abort_multipart_upload(
                 Bucket=self.storage.bucket, Key=upload_file_id, UploadId=mpu["UploadId"]
             )
         except Exception:
             logger.warning("Could not abort multipart upload", exc_info=True)
 
-    async def start(self, dm: FileDataMangaer, path: str, kbid: str):
+    async def start(self, dm: FileDataManager, path: str, kbid: str):
         bucket = self.storage.get_bucket_name(kbid)
         upload_file_id = dm.get("upload_file_id", str(uuid.uuid4()))
         if dm.get("mpu") is not None:
             await self._abort_multipart(dm)
 
         await dm.update(
             path=path,
             upload_file_id=upload_file_id,
             multipart={"Parts": []},
             block=1,
             mpu=await self._create_multipart(path, bucket),
             bucket=bucket,
         )
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=3)
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=3
+    )
     async def _create_multipart(self, path, bucket):
         return await self.storage._s3aioclient.create_multipart_upload(
             Bucket=bucket, Key=path
         )
 
-    async def append(self, dm: FileDataMangaer, iterable, offset) -> int:
+    async def append(self, dm: FileDataManager, iterable, offset) -> int:
         size = 0
         async for chunk in iterable:
             # It seems that starlette stream() finishes with an emtpy chunk of data
             size += len(chunk)
             part = await self._upload_part(dm, chunk)
-
             multipart = dm.get("multipart")
             multipart["Parts"].append(
                 {"PartNumber": dm.get("block"), "ETag": part["ETag"]}
             )
             await dm.update(multipart=multipart, block=dm.get("block") + 1)
 
         return size
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=3)
-    async def _upload_part(self, dm: FileDataMangaer, data):
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=3
+    )
+    async def _upload_part(self, dm: FileDataManager, data):
+        mpu = dm.get("mpu")
+        if mpu is None:
+            # If we don't have an ongoing multipart upload for the current upload_id
+            # we need to abort the request.
+            raise ResumableURINotAvailable()
+
         return await self.storage._s3aioclient.upload_part(
             Bucket=dm.get("bucket"),
             Key=dm.get("path"),
             PartNumber=dm.get("block"),
             UploadId=dm.get("mpu")["UploadId"],
             Body=data,
         )
 
-    async def finish(self, dm: FileDataMangaer):
+    async def finish(self, dm: FileDataManager):
+        path = dm.get("path")
         if dm.get("mpu") is not None:
             await self._complete_multipart_upload(dm)
         await dm.finish()
+        return path
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=3)
-    async def _complete_multipart_upload(self, dm: FileDataMangaer):
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=3
+    )
+    async def _complete_multipart_upload(self, dm: FileDataManager):
         # if blocks is 0, it means the file is of zero length so we need to
         # trick it to finish a multiple part with no data.
         if dm.get("block") == 1:
             part = await self._upload_part(dm, b"")
             multipart = dm.get("multipart")
             multipart["Parts"].append(
                 {"PartNumber": dm.get("block"), "ETag": part["ETag"]}
@@ -123,23 +144,25 @@
         await self.storage._s3aioclient.complete_multipart_upload(
             Bucket=dm.get("bucket"),
             Key=dm.get("path"),
             UploadId=dm.get("mpu")["UploadId"],
             MultipartUpload=dm.get("multipart"),
         )
 
-    @backoff.on_exception(backoff.expo, RETRIABLE_EXCEPTIONS, max_tries=3)
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=3
+    )
     async def _download(self, uri: str, kbid: str, **kwargs):
         bucket = self.storage.get_bucket_name(kbid)
         return await self.storage._s3aioclient.get_object(
             Bucket=bucket, Key=uri, **kwargs
         )
 
     async def iter_data(
-        self, uri: str, kbid: str, headers: Optional[Dict[str, str]] = None
+        self, uri: str, kbid: str, headers: Optional[dict[str, str]] = None
     ):
         if headers is None:
             headers = {}
         try:
             downloader = await self._download(uri, kbid, **headers)
         except self.storage._s3aioclient.exceptions.NoSuchKey:
             raise CloudFileNotFound()
@@ -174,36 +197,29 @@
                 logger.warning("Error deleting object", exc_info=True)
         else:
             raise AttributeError("No valid uri")
 
 
 class S3BlobStore(BlobStore):
     async def check_exists(self, bucket_name: str) -> bool:
-        exists = True
-        try:
-            res = await self._s3aioclient.head_bucket(Bucket=bucket_name)
-            if res["ResponseMetadata"]["HTTPStatusCode"] == 404:
-                exists = False
-        except botocore.exceptions.ClientError as e:
-            error_code = int(e.response["Error"]["Code"])
-            if error_code == 404:
-                exists = False
-        return exists
+        return await bucket_exists(self._s3aioclient, bucket_name)
 
     def get_bucket_name(self, kbid: str) -> str:
         bucket_name = super().get_bucket_name(kbid)
         if bucket_name is not None:
             return bucket_name.replace("_", "-")
         else:
             return bucket_name
 
     async def create_bucket(self, bucket):
         exists = await self.check_exists(bucket)
         if not exists:
-            await self._s3aioclient.create_bucket(Bucket=bucket)
+            await create_bucket(
+                self._s3aioclient, bucket, self.bucket_tags, self.region_name
+            )
         return exists
 
     async def finalize(self):
         await self._exit_stack.__aexit__(None, None, None)
 
     async def initialize(
         self,
@@ -211,17 +227,20 @@
         client_secret,
         ssl,
         verify_ssl,
         max_pool_connections,
         endpoint_url,
         region_name,
         bucket,
+        bucket_tags: Optional[dict[str, str]] = None,
     ):
         self.bucket = bucket
-        self.source = CloudFile.Source.GCS
+        self.bucket_tags = bucket_tags
+        self.source = CloudFile.Source.S3
+        self.region_name = region_name
 
         self._exit_stack = AsyncExitStack()
 
         self.opts = dict(
             aws_secret_access_key=client_secret,
             aws_access_key_id=client_id,
             endpoint_url=endpoint_url,
```

## nucliadb/writer/tus/storage.py

```diff
@@ -15,25 +15,25 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
-from typing import AsyncIterator, Dict, Optional
+from typing import AsyncIterator, Optional
 
 from lru import LRU  # type: ignore
 from nucliadb_protos.resources_pb2 import CloudFile
 from starlette.responses import StreamingResponse
 
 from nucliadb.writer import logger
-from nucliadb.writer.tus.dm import FileDataMangaer
+from nucliadb.writer.tus.dm import FileDataManager
 from nucliadb.writer.tus.exceptions import HTTPRangeNotSatisfiable
 
-CACHED_BUCKETS = LRU(50)
+CACHED_BUCKETS = LRU(50)  # type: ignore
 
 
 class BlobStore:
     bucket: str
     source: CloudFile.Source.V
 
     async def initialize(self, *args, **kwargs):
@@ -50,35 +50,36 @@
 
     def get_bucket_name(self, kbid: str) -> str:
         return self.bucket.format(kbid=kbid)
 
 
 class FileStorageManager:
     chunk_size: int
+    min_upload_size: Optional[int] = None
 
     def __init__(self, storage):
         self.storage = storage
 
     def read_range(
         self, uri: str, kbid: str, start: int, end: int
     ) -> AsyncIterator[bytes]:
         raise NotImplementedError()
 
     def iter_data(
-        self, uri: str, kbid: str, headers: Optional[Dict[str, str]] = None
+        self, uri: str, kbid: str, headers: Optional[dict[str, str]] = None
     ) -> AsyncIterator[bytes]:
         raise NotImplementedError()
 
-    async def start(self, dm: FileDataMangaer, path: str, kbid: str):
+    async def start(self, dm: FileDataManager, path: str, kbid: str):
         raise NotImplementedError()
 
-    async def append(self, dm: FileDataMangaer, iterable, offset) -> int:
+    async def append(self, dm: FileDataManager, iterable, offset) -> int:
         raise NotImplementedError()
 
-    async def finish(self, dm: FileDataMangaer):
+    async def finish(self, dm: FileDataManager):
         raise NotImplementedError()
 
     async def delete_upload(self, uri, kbid):
         raise NotImplementedError()
 
     async def full_download(self, content_length, content_type, upload_id):
         return StreamingResponse(
```

## nucliadb/writer/tus/utils.py

```diff
@@ -16,30 +16,29 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import base64
 import binascii
 import re
-from typing import Dict
 
 from nucliadb.writer.tus.exceptions import InvalidTUSMetadata
 
 
 def to_str(value):
     if isinstance(value, bytes):
         value = value.decode("utf-8")
     return value
 
 
 match_ascii = re.compile(r"^[ -~]+$")
 match_b64 = re.compile(r"[^-A-Za-z0-9+/=]|=[^=]|={3,}$")
 
 
-def parse_tus_metadata(header: str) -> Dict:
+def parse_tus_metadata(header: str) -> dict:
     """
     https://tus.io/protocols/resumable-upload.html#upload-metadata
     """
     metadata = {}
 
     # Be kind, ignore trailing commas
     for pair in header.rstrip(",").split(","):
```

## Comparing `nucliadb/app.py` & `nucliadb/ingest/orm/metrics.py`

 * *Files 16% similar despite different names*

```diff
@@ -13,14 +13,15 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from nucliadb.config import config_nucliadb
-from nucliadb.settings import Settings
+from nucliadb.ingest.orm.exceptions import KnowledgeBoxConflict
+from nucliadb_telemetry import metrics
 
-nucliadb_args = Settings()
-config_nucliadb(nucliadb_args)
-
-from nucliadb.one.app import application  # noqa
+processor_observer = metrics.Observer(
+    "nucliadb_ingest_processor",
+    labels={"type": ""},
+    error_mappings={"kb_conflict": KnowledgeBoxConflict},
+)
```

## Comparing `nucliadb/config.py` & `nucliadb/standalone/config.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,15 +17,16 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 
 import logging
 import os
 
-from nucliadb.settings import Settings
+from nucliadb.common.cluster.settings import StandaloneNodeRole
+from nucliadb.standalone.settings import Settings, StandaloneDiscoveryMode
 
 logger = logging.getLogger(__name__)
 
 
 def config_standalone_driver(nucliadb_args: Settings):
     from nucliadb.ingest.settings import DriverConfig, DriverSettings
     from nucliadb.ingest.settings import settings as ingest_settings
@@ -67,65 +68,68 @@
     ):
         os.makedirs(ingest_settings.driver_local_url, exist_ok=True)
 
     # need to force inject this to env var
     if "DATA_PATH" not in os.environ:
         os.environ["DATA_PATH"] = nucliadb_args.data_path
 
-    if not os.path.isdir(nucliadb_args.data_path):
-        os.makedirs(nucliadb_args.data_path, exist_ok=True)
-
 
 def config_nucliadb(nucliadb_args: Settings):
     """
     Standalone nucliadb configuration forces us to
     use some specific settings.
     """
 
-    from nucliadb.ingest.orm import NODE_CLUSTER
-    from nucliadb.ingest.orm.local_node import LocalNode
+    from nucliadb.common.cluster.settings import ClusterDiscoveryMode
+    from nucliadb.common.cluster.settings import settings as cluster_settings
     from nucliadb.ingest.settings import settings as ingest_settings
     from nucliadb.train.settings import settings as train_settings
     from nucliadb.writer.settings import settings as writer_settings
-    from nucliadb_utils.cache.settings import settings as cache_settings
     from nucliadb_utils.settings import (
         audit_settings,
         http_settings,
-        indexing_settings,
         nuclia_settings,
         nucliadb_settings,
         transaction_settings,
     )
 
-    ingest_settings.chitchat_enabled = False
+    cluster_settings.standalone_mode = True
+    cluster_settings.data_path = nucliadb_args.data_path
+    cluster_settings.standalone_node_port = nucliadb_args.standalone_node_port
+    cluster_settings.standalone_node_role = nucliadb_args.standalone_node_role
+
+    if nucliadb_args.cluster_discovery_mode == StandaloneDiscoveryMode.DEFAULT:
+        # default for standalone is single node
+        cluster_settings.cluster_discovery_mode = ClusterDiscoveryMode.SINGLE_NODE
+        cluster_settings.node_replicas = 1
+
     ingest_settings.nuclia_partitions = 1
-    ingest_settings.total_replicas = 1
     ingest_settings.replica_number = 0
     ingest_settings.partitions = ["1"]
     nuclia_settings.onprem = True
     http_settings.cors_origins = ["*"]
     nucliadb_settings.nucliadb_ingest = None
     transaction_settings.transaction_local = True
     audit_settings.audit_driver = "basic"
-    indexing_settings.index_local = True
-    cache_settings.cache_enabled = False
     writer_settings.dm_enabled = False
 
     train_settings.grpc_port = nucliadb_args.train_grpc_port
     ingest_settings.grpc_port = nucliadb_args.ingest_grpc_port
 
     config_standalone_driver(nucliadb_args)
 
     if nucliadb_args.nua_api_key:
         nuclia_settings.nuclia_service_account = nucliadb_args.nua_api_key
+        if nucliadb_args.standalone_node_role == StandaloneNodeRole.INDEX:
+            ingest_settings.disable_pull_worker = True
+        else:
+            ingest_settings.disable_pull_worker = False
     else:
         ingest_settings.disable_pull_worker = True
         nuclia_settings.dummy_processing = True
         nuclia_settings.dummy_predict = True
+        nuclia_settings.dummy_learning_services = True
 
     if nucliadb_args.zone is not None:
         nuclia_settings.nuclia_zone = nucliadb_args.zone
     elif os.environ.get("NUA_ZONE"):
         nuclia_settings.nuclia_zone = os.environ.get("NUA_ZONE", "dev")
-
-    local_node = LocalNode()
-    NODE_CLUSTER.local_node = local_node
```

## Comparing `nucliadb/purge.py` & `nucliadb/standalone/purge.py`

 * *Files 11% similar despite different names*

```diff
@@ -15,30 +15,26 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 # Standalone purge command
 import asyncio
-import os
 
 import pydantic_argparse
 
-from nucliadb.config import config_nucliadb
-from nucliadb.settings import Settings
+from nucliadb.standalone.config import config_nucliadb
+from nucliadb.standalone.settings import Settings
 
 
 def purge():
-    from nucliadb.ingest.purge import main
+    from nucliadb.purge import main
 
-    if os.environ.get("NUCLIADB_ENV"):
-        nucliadb_args = Settings()
-    else:
-        parser = pydantic_argparse.ArgumentParser(
-            model=Settings,
-            prog="NucliaDB",
-            description="NucliaDB Starting script",
-        )
-        nucliadb_args = parser.parse_typed_args()
+    parser = pydantic_argparse.ArgumentParser(
+        model=Settings,
+        prog="NucliaDB",
+        description="NucliaDB Starting script",
+    )
+    nucliadb_args = parser.parse_typed_args()
 
     config_nucliadb(nucliadb_args)
     asyncio.run(main())
```

## Comparing `nucliadb/settings.py` & `nucliadb/tests/unit/test_field_ids.py`

 * *Files 26% similar despite different names*

```diff
@@ -13,38 +13,37 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from enum import Enum
-from typing import Optional
+import pytest
+from pydantic import BaseModel, ValidationError
 
-import pydantic
+from nucliadb_models.utils import FieldIdString
 
-from nucliadb.ingest.settings import DriverSettings
-from nucliadb_utils.settings import StorageSettings
 
+class DummyFieldIdModel(BaseModel):
+    field_id: FieldIdString
 
-class LogLevel(str, Enum):
-    INFO = "INFO"
-    ERROR = "ERROR"
-    DEBUG = "DEBUG"
-
-
-class Settings(DriverSettings, StorageSettings):
-    # be consistent here with DATA_PATH env var
-    data_path: str = pydantic.Field(
-        "./data/node", description="Path to node index files"
-    )
-
-    # all settings here are mapped in to other env var settings used
-    # in the app. These are helper settings to make things easier to
-    # use with standalone app vs cluster app.
-    nua_api_key: Optional[str] = pydantic.Field(
-        description="Nuclia Understanding API Key"
-    )
-    zone: Optional[str] = pydantic.Field(description="Nuclia Understanding API Zone ID")
-    http_port: int = pydantic.Field(8080, description="HTTP Port")
-    ingest_grpc_port: int = pydantic.Field(8030, description="Ingest GRPC Port int")
-    train_grpc_port: int = pydantic.Field(8031, description="Train GRPC Port int")
+
+def test_field_ids():
+    """Test some examples of valid fields and exhaustively test invalid
+    fields.
+
+    """
+    valid_field_ids = [
+        "foo",
+        "foo_bar",
+        "foo-bar_123",
+    ]
+    for valid in valid_field_ids:
+        DummyFieldIdModel(field_id=valid)
+
+    invalid_field_ids = [
+        "",
+        "foo/bar",
+    ]
+    for invalid in invalid_field_ids:
+        with pytest.raises(ValidationError):
+            DummyFieldIdModel(field_id=invalid)
```

## Comparing `nucliadb/http_clients/__init__.py` & `migrations/__init__.py`

 * *Files identical despite different names*

## Comparing `nucliadb/http_clients/exceptions.py` & `nucliadb/common/http_clients/exceptions.py`

 * *Files identical despite different names*

## Comparing `nucliadb/ingest/purge.py` & `nucliadb/purge/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -14,87 +14,95 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
+from typing import AsyncGenerator
 
 import pkg_resources
 
+from nucliadb.common.cluster.exceptions import NodeError, ShardNotFound
+from nucliadb.common.cluster.utils import setup_cluster, teardown_cluster
+from nucliadb.common.maindb.driver import Driver
+from nucliadb.common.maindb.utils import setup_driver, teardown_driver
 from nucliadb.ingest import SERVICE_NAME, logger
-from nucliadb.ingest.maindb.driver import Driver
-from nucliadb.ingest.orm.exceptions import NodeError, ShardNotFound
 from nucliadb.ingest.orm.knowledgebox import (
     KB_TO_DELETE,
     KB_TO_DELETE_BASE,
     KB_TO_DELETE_STORAGE_BASE,
     KnowledgeBox,
 )
-from nucliadb.ingest.utils import get_driver
 from nucliadb_telemetry import errors
 from nucliadb_telemetry.logs import setup_logging
 from nucliadb_utils.storages.storage import Storage
 from nucliadb_utils.utilities import get_storage
 
 
+async def _iter_keys(driver: Driver, match: str) -> AsyncGenerator[str, None]:
+    async with driver.transaction(read_only=True) as keys_txn:
+        async for key in keys_txn.keys(match=match, count=-1):
+            yield key
+
+
 async def purge_kb(driver: Driver):
     logger.info("START PURGING KB")
-    async for key in driver.keys(match=KB_TO_DELETE_BASE, count=-1):
+    async for key in _iter_keys(driver, KB_TO_DELETE_BASE):
         logger.info(f"Purging kb {key}")
         try:
             kbid = key.split("/")[2]
         except Exception:
-            logger.info(
+            logger.warning(
                 f"  X Skipping purge {key}, wrong key format, expected {KB_TO_DELETE_BASE}"
             )
             continue
 
         try:
             await KnowledgeBox.purge(driver, kbid)
             logger.info(f"  √ Successfully Purged {kbid}")
         except ShardNotFound as exc:
             errors.capture_exception(exc)
-            logger.info(
+            logger.error(
                 f"  X At least one shard was unavailable while purging {kbid}, skipping"
             )
             continue
         except NodeError as exc:
             errors.capture_exception(exc)
-            logger.info(
+            logger.error(
                 f"  X At least one node was unavailable while purging {kbid}, skipping"
             )
             continue
 
         except Exception as exc:
             errors.capture_exception(exc)
-            logger.info(
+            logger.error(
                 f"  X ERROR while executing KnowledgeBox.purge of {kbid}, skipping: {exc.__class__.__name__} {exc}"
             )
             continue
 
         # Now delete the tikv delete mark
         try:
             txn = await driver.begin()
             key_to_purge = KB_TO_DELETE.format(kbid=kbid)
             await txn.delete(key_to_purge)
             await txn.commit()
             logger.info(f"  √ Deleted {key_to_purge}")
         except Exception as exc:
             errors.capture_exception(exc)
-            logger.info(f"  X Error while deleting key {key_to_purge}")
+            logger.error(f"  X Error while deleting key {key_to_purge}")
             await txn.abort()
     logger.info("END PURGING KB")
 
 
 async def purge_kb_storage(driver: Driver, storage: Storage):
     # Last iteration deleted all kbs, and set their storages marked to be deleted also in tikv
     # Here we'll delete those storage buckets
     logger.info("START PURGING KB STORAGE")
-    async for key in driver.keys(match=KB_TO_DELETE_STORAGE_BASE, count=-1):
+    async for key in _iter_keys(driver, KB_TO_DELETE_STORAGE_BASE):
         logger.info(f"Purging storage {key}")
         try:
             kbid = key.split("/")[2]
         except Exception:
             logger.info(
                 f"  X Skipping purge {key}, wrong key format, expected {KB_TO_DELETE_STORAGE_BASE}"
             )
@@ -103,21 +111,24 @@
         deleted, conflict = await storage.delete_kb(kbid)
 
         delete_marker = False
         if conflict:
             logger.info(
                 f"  . Nothing was deleted for {key}, (Bucket not yet empty), will try next time"
             )
+            # Just in case something failed while setting a lifecycle policy to
+            # remove all elements from the bucket, reschedule it
+            await storage.schedule_delete_kb(kbid)
         elif not deleted:
             logger.info(
                 f"  ! Expected bucket for {key} was not found, will delete marker"
             )
             delete_marker = True
         elif deleted:
-            logger.info(f"  √ Bucket successfully deleted")
+            logger.info("  √ Bucket successfully deleted")
             delete_marker = True
 
         if delete_marker:
             try:
                 txn = await driver.begin()
                 await txn.delete(key)
                 logger.info(f"  √ Deleted storage deletion marker {key}")
@@ -128,24 +139,30 @@
             else:
                 await txn.commit()
 
     logger.info("FINISH PURGING KB STORAGE")
 
 
 async def main():
-    # Clean up all kb marked to delete
-    driver = await get_driver()
+    """
+    This script will purge all knowledge boxes marked to be deleted in maindb.
+    """
+    await setup_cluster()
+    driver = await setup_driver()
     storage = await get_storage(
         gcs_scopes=["https://www.googleapis.com/auth/devstorage.full_control"],
         service_name=SERVICE_NAME,
     )
-
-    await purge_kb(driver)
-    await purge_kb_storage(driver, storage)
-    await storage.finalize()
+    try:
+        await purge_kb(driver)
+        await purge_kb_storage(driver, storage)
+    finally:
+        await storage.finalize()
+        await teardown_driver()
+        await teardown_cluster()
 
 
 def run() -> int:  # pragma: no cover
     setup_logging()
 
     errors.setup_error_handling(pkg_resources.get_distribution("nucliadb").version)
```

## Comparing `nucliadb/ingest/maindb/__init__.py` & `nucliadb/common/__init__.py`

 * *Files identical despite different names*

## Comparing `nucliadb/ingest/maindb/driver.py` & `nucliadb/common/maindb/driver.py`

 * *Files 19% similar despite different names*

```diff
@@ -15,32 +15,33 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
+import asyncio
 from contextlib import asynccontextmanager
-from typing import AsyncGenerator, List, Optional
+from typing import AsyncGenerator, Optional
 
 DEFAULT_SCAN_LIMIT = 10
-DEFAULT_BATCH_SCAN_LIMIT = 100
+DEFAULT_BATCH_SCAN_LIMIT = 500
 
 
 class Transaction:
     driver: Driver
     open: bool
 
     async def abort(self):
         raise NotImplementedError()
 
     async def commit(self):
         raise NotImplementedError()
 
-    async def batch_get(self, keys: List[str]):
+    async def batch_get(self, keys: list[str]) -> list[Optional[bytes]]:
         raise NotImplementedError()
 
     async def get(self, key: str) -> Optional[bytes]:
         raise NotImplementedError()
 
     async def set(self, key: str, value: bytes):
         raise NotImplementedError()
@@ -49,39 +50,60 @@
         raise NotImplementedError()
 
     def keys(
         self, match: str, count: int = DEFAULT_SCAN_LIMIT, include_start: bool = True
     ) -> AsyncGenerator[str, None]:
         raise NotImplementedError()
 
+    async def count(self, match: str) -> int:
+        raise NotImplementedError()
+
 
 class Driver:
     initialized = False
+    _abort_tasks: list[asyncio.Task] = []
 
     async def initialize(self):
         raise NotImplementedError()
 
     async def finalize(self):
-        raise NotImplementedError()
-
-    async def begin(self) -> Transaction:
-        raise NotImplementedError()
+        while len(self._abort_tasks) > 0:
+            task = self._abort_tasks.pop()
+            if not task.done():
+                try:
+                    await task
+                except Exception:
+                    pass
 
-    async def keys(
-        self, match: str, count: int = DEFAULT_SCAN_LIMIT, include_start: bool = True
-    ) -> AsyncGenerator[str, None]:
+    async def begin(self, read_only: bool = False) -> Transaction:
         raise NotImplementedError()
-        # mypy is funny in some cases and wants a yield to detect the type properly
-        yield
 
     @asynccontextmanager
-    async def transaction(self) -> AsyncGenerator[Transaction, None]:
+    async def transaction(
+        self, wait_for_abort: bool = True, read_only: bool = False
+    ) -> AsyncGenerator[Transaction, None]:
         """
-        Use to make sure transaction is always aborted
+        Use to make sure transaction is always aborted.
+
+        :param wait_for_abort: If True, wait for abort to finish before returning.
+                               If False, abort is done in background (unless there
+                               is an error)
         """
         txn: Optional[Transaction] = None
+        error: bool = False
         try:
-            txn = await self.begin()
+            txn = await self.begin(read_only=read_only)
             yield txn
+        except Exception:
+            error = True
+            raise
         finally:
             if txn is not None and txn.open:
-                await txn.abort()
+                if error or wait_for_abort:
+                    await txn.abort()
+                else:
+                    self._async_abort(txn)
+
+    def _async_abort(self, txn: Transaction):
+        task = asyncio.create_task(txn.abort())
+        task.add_done_callback(lambda task: self._abort_tasks.remove(task))
+        self._abort_tasks.append(task)
```

## Comparing `nucliadb/ingest/maindb/keys.py` & `nucliadb/tasks/logger.py`

 * *Files 17% similar despite different names*

```diff
@@ -13,11 +13,11 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
+import logging
 
-KB_ENTITIES = "/kbs/{kbid}/entities"
-KB_ENTITIES_GROUP = "/kbs/{kbid}/entities/{id}"
-KB_DELETED_ENTITIES_GROUPS = "/kbs/{kbid}/deletedentities"
+SERVICE_NAME = "nucliadb.tasks"
+logger = logging.getLogger(SERVICE_NAME)
```

## Comparing `nucliadb/ingest/maindb/local.py` & `nucliadb/common/maindb/local.py`

 * *Files 8% similar despite different names*

```diff
@@ -15,17 +15,17 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import glob
 import os
-from typing import Dict, List, Optional
+from typing import Optional
 
-from nucliadb.ingest.maindb.driver import (
+from nucliadb.common.maindb.driver import (
     DEFAULT_BATCH_SCAN_LIMIT,
     DEFAULT_SCAN_LIMIT,
     Driver,
     Transaction,
 )
 
 try:
@@ -33,17 +33,17 @@
 
     FILES = True
 except ImportError:  # pragma: no cover
     FILES = False
 
 
 class LocalTransaction(Transaction):
-    modified_keys: Dict[str, bytes]
-    visited_keys: Dict[str, bytes]
-    deleted_keys: List[str]
+    modified_keys: dict[str, bytes]
+    visited_keys: dict[str, bytes]
+    deleted_keys: list[str]
 
     def __init__(self, url: str, driver: Driver):
         self.url = url
         self.open = True
         self.driver = driver
         self.modified_keys = {}
         self.visited_keys = {}
@@ -101,33 +101,31 @@
         for key in self.deleted_keys:
             await self.remove(key)
             not_to_check.append(count)
             count += 1
         self.clean()
         self.open = False
 
-    async def batch_get(self, keys: List[str]):
-        results = []
+    async def batch_get(self, keys: list[str]) -> list[Optional[bytes]]:
+        results: list[Optional[bytes]] = []
         for key in keys:
-            if key in self.deleted_keys:
-                raise KeyError(f"Not found {key}")
+            obj = await self.get(key)
+            if obj:
+                results.append(obj)
+            else:
+                results.append(None)
 
+        for idx, key in enumerate(keys):
+            if key in self.deleted_keys:
+                results[idx] = None
             if key in self.modified_keys:
-                results.append(self.modified_keys[key])
-                keys.remove(key)
-
+                results[idx] = self.modified_keys[key]
             if key in self.visited_keys:
-                results.append(self.visited_keys[key])
-                keys.remove(key)
+                results[idx] = self.visited_keys[key]
 
-        if len(keys) > 0:
-            for key in keys:
-                obj = await self.get(key)
-                if obj:
-                    results.append(obj)
         return results
 
     async def get(self, key: str) -> Optional[bytes]:
         if key in self.deleted_keys:
             raise KeyError(f"Not found {key}")
 
         if key in self.modified_keys:
@@ -191,14 +189,20 @@
             real_count += 1
             prev_key = real_key
         if prev_key is None:
             for new_key in self.modified_keys.keys():
                 if match in new_key:
                     yield new_key.replace(self.url, "")
 
+    async def count(self, match: str) -> int:
+        value = 0
+        async for _ in self.keys(match, count=-1):
+            value += 1
+        return value
+
 
 class LocalDriver(Driver):
     url: str
 
     def __init__(self, url: str):
         self.url = os.path.abspath(url.rstrip("/"))
 
@@ -206,25 +210,11 @@
         if self.initialized is False and os.path.exists(self.url) is False:
             os.makedirs(self.url, exist_ok=True)
         self.initialized = True
 
     async def finalize(self):
         pass
 
-    async def begin(self) -> LocalTransaction:
+    async def begin(self, read_only: bool = False) -> LocalTransaction:
         if self.url is None:
             raise AttributeError("Invalid url")
         return LocalTransaction(self.url, self)
-
-    async def keys(
-        self, match: str, count: int = DEFAULT_SCAN_LIMIT, include_start: bool = True
-    ):
-        path = f"{self.url}/{match}"
-        actual_count = 0
-        for str_key in glob.glob(path + "**", recursive=True):
-            if os.path.isdir(str_key) or not os.path.exists(str_key):
-                continue
-
-            yield str_key.replace(self.url, "")
-            if actual_count >= count:
-                break
-            actual_count += 1
```

## Comparing `nucliadb/ingest/maindb/pg.py` & `nucliadb/common/maindb/pg.py`

 * *Files 22% similar despite different names*

```diff
@@ -16,83 +16,113 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
 import asyncio
-from typing import Any, AsyncGenerator, List, Optional
+from typing import Any, AsyncGenerator, Optional, Union
 
 import asyncpg
+import backoff
 
-from nucliadb.ingest.maindb.driver import DEFAULT_SCAN_LIMIT, Driver, Transaction
+from nucliadb.common.maindb.driver import DEFAULT_SCAN_LIMIT, Driver, Transaction
+
+RETRIABLE_EXCEPTIONS = (
+    asyncpg.CannotConnectNowError,
+    OSError,
+    ConnectionResetError,
+)
 
 CREATE_TABLE = """
 CREATE TABLE IF NOT EXISTS resources (
     key TEXT PRIMARY KEY,
     value BYTEA
 );
 """
 
 
 class DataLayer:
-    def __init__(self, connection: asyncpg.Connection):
+    def __init__(self, connection: Union[asyncpg.Connection, asyncpg.Pool]):
         self.connection = connection
+        self.lock = asyncio.Lock()
 
     async def get(self, key: str) -> Optional[bytes]:
-        return await self.connection.fetchval(
-            "SELECT value FROM resources WHERE key = $1", key
-        )
+        async with self.lock:
+            return await self.connection.fetchval(
+                "SELECT value FROM resources WHERE key = $1", key
+            )
 
     async def set(self, key: str, value: bytes) -> None:
-        await self.connection.execute(
-            """
+        async with self.lock:
+            await self.connection.execute(
+                """
 INSERT INTO resources (key, value)
 VALUES ($1, $2)
 ON CONFLICT (key)
 DO UPDATE SET value = EXCLUDED.value
 """,
-            key,
-            value,
-        )
+                key,
+                value,
+            )
 
     async def delete(self, key: str) -> None:
-        await self.connection.execute("DELETE FROM resources WHERE key = $1", key)
+        async with self.lock:
+            await self.connection.execute("DELETE FROM resources WHERE key = $1", key)
 
-    async def batch_get(self, keys: List[str]) -> List[bytes]:
-        records = {
-            record["key"]: record["value"]
-            for record in await self.connection.fetch(
-                "SELECT key, value FROM resources WHERE key = ANY($1)", keys
-            )
-        }
+    async def batch_get(self, keys: list[str]) -> list[Optional[bytes]]:
+        async with self.lock:
+            records = {
+                record["key"]: record["value"]
+                for record in await self.connection.fetch(
+                    "SELECT key, value FROM resources WHERE key = ANY($1)", keys
+                )
+            }
         # get sorted by keys
-        return [records[key] for key in keys]
+        return [records.get(key) for key in keys]
 
     async def scan_keys(
         self,
         prefix: str,
         limit: int = DEFAULT_SCAN_LIMIT,
         include_start: bool = True,
     ) -> AsyncGenerator[str, None]:
-        query = "SELECT key FROM resources WHERE key LIKE $1"
+        query = """SELECT key FROM resources
+WHERE key LIKE $1
+ORDER BY key
+"""
         args: list[Any] = [prefix + "%"]
         if limit > 0:
             query += " LIMIT $2"
             args.append(limit)
-        async for record in self.connection.cursor(query, *args):
-            if not include_start and record["key"] == prefix:
-                continue
-            yield record["key"]
+        async with self.lock:
+            async for record in self.connection.cursor(query, *args):
+                if not include_start and record["key"] == prefix:
+                    continue
+                yield record["key"]
+
+    async def count(self, match: str) -> int:
+        async with self.lock:
+            results = await self.connection.fetch(
+                "SELECT count(*) FROM resources WHERE key LIKE $1", match + "%"
+            )
+        return results[0]["count"]
 
 
 class PGTransaction(Transaction):
     driver: PGDriver
 
-    def __init__(self, connection: asyncpg.Connection, txn: Any, driver: PGDriver):
+    def __init__(
+        self,
+        pool: asyncpg.Pool,
+        connection: asyncpg.Connection,
+        txn: Any,
+        driver: PGDriver,
+    ):
+        self.pool = pool
         self.connection = connection
         self.data_layer = DataLayer(connection)
         self.txn = txn
         self.driver = driver
         self.open = True
         self._lock = asyncio.Lock()
 
@@ -112,64 +142,124 @@
             except Exception:
                 await self.txn.rollback()
                 raise
             finally:
                 self.open = False
                 await self.connection.close()
 
-    async def batch_get(self, keys: List[str]):
+    async def batch_get(self, keys: list[str]):
         return await self.data_layer.batch_get(keys)
 
     async def get(self, key: str) -> Optional[bytes]:
         return await self.data_layer.get(key)
 
     async def set(self, key: str, value: bytes):
         await self.data_layer.set(key, value)
 
     async def delete(self, key: str):
         await self.data_layer.delete(key)
 
-    def keys(
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=2
+    )
+    async def keys(
+        self,
+        match: str,
+        count: int = DEFAULT_SCAN_LIMIT,
+        include_start: bool = True,
+    ):
+        async with self.pool.acquire() as conn, conn.transaction():
+            # all txn implementations implement this API outside of the current txn
+            dl = DataLayer(conn)
+            async for key in dl.scan_keys(match, count, include_start=include_start):
+                yield key
+
+    async def count(self, match: str) -> int:
+        return await self.data_layer.count(match)
+
+
+class ReadOnlyPGTransaction(Transaction):
+    driver: PGDriver
+
+    def __init__(self, pool: asyncpg.Pool, driver: PGDriver):
+        self.pool = pool
+        self.driver = driver
+        self.open = True
+
+    async def abort(self):
+        # This is a no-op because we don't have a transaction to abort on read-only transactions.
+        ...
+
+    async def commit(self):
+        raise Exception("Cannot commit transaction in read only mode")
+
+    async def batch_get(self, keys: list[str]):
+        return await DataLayer(self.pool).batch_get(keys)
+
+    async def get(self, key: str) -> Optional[bytes]:
+        return await DataLayer(self.pool).get(key)
+
+    async def set(self, key: str, value: bytes):
+        raise Exception("Cannot set in read only transaction")
+
+    async def delete(self, key: str):
+        raise Exception("Cannot delete in read only transaction")
+
+    async def keys(
         self,
         match: str,
         count: int = DEFAULT_SCAN_LIMIT,
         include_start: bool = True,
     ):
-        return self.data_layer.scan_keys(match, count, include_start=include_start)
+        async with self.pool.acquire() as conn, conn.transaction():
+            # all txn implementations implement this API outside of the current txn
+            dl = DataLayer(conn)
+            async for key in dl.scan_keys(match, count, include_start=include_start):
+                yield key
+
+    async def count(self, match: str) -> int:
+        return await DataLayer(self.pool).count(match)
 
 
 class PGDriver(Driver):
     pool: asyncpg.Pool
 
-    def __init__(self, url: str):
+    def __init__(self, url: str, connection_pool_max_size: int = 10):
         self.url = url
+        self.connection_pool_max_size = connection_pool_max_size
         self._lock = asyncio.Lock()
 
     async def initialize(self):
         async with self._lock:
             if self.initialized is False:
-                self.pool = await asyncpg.create_pool(self.url)
+                self.pool = await asyncpg.create_pool(
+                    self.url,
+                    max_size=self.connection_pool_max_size,
+                )
 
                 # check if table exists
-                async with self.pool.acquire() as conn:
-                    await conn.execute(CREATE_TABLE)
+                try:
+                    async with self.pool.acquire() as conn:
+                        await conn.execute(CREATE_TABLE)
+                except asyncpg.exceptions.UniqueViolationError:  # pragma: no cover
+                    pass
 
             self.initialized = True
 
     async def finalize(self):
         async with self._lock:
             await self.pool.close()
             self.initialized = False
 
-    async def begin(self) -> PGTransaction:
-        conn = await self.pool.acquire()
-        txn = conn.transaction()
-        await txn.start()
-        return PGTransaction(conn, txn, driver=self)
-
-    async def keys(
-        self, match: str, count: int = DEFAULT_SCAN_LIMIT, include_start: bool = True
-    ):
-        async with self.pool.acquire() as conn, conn.transaction():
-            dl = DataLayer(conn)
-            async for key in dl.scan_keys(match, count, include_start):
-                yield key
+    @backoff.on_exception(
+        backoff.expo, RETRIABLE_EXCEPTIONS, jitter=backoff.random_jitter, max_tries=3
+    )
+    async def begin(
+        self, read_only: bool = False
+    ) -> Union[PGTransaction, ReadOnlyPGTransaction]:
+        if read_only:
+            return ReadOnlyPGTransaction(self.pool, driver=self)
+        else:
+            conn: asyncpg.Connection = await self.pool.acquire()
+            txn = conn.transaction()
+            await txn.start()
+            return PGTransaction(self.pool, conn, txn, driver=self)
```

## Comparing `nucliadb/ingest/maindb/redis.py` & `nucliadb/common/maindb/redis.py`

 * *Files 13% similar despite different names*

```diff
@@ -13,17 +13,17 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import Any, Dict, List, Optional
+from typing import Any, Optional
 
-from nucliadb.ingest.maindb.driver import (
+from nucliadb.common.maindb.driver import (
     DEFAULT_BATCH_SCAN_LIMIT,
     DEFAULT_SCAN_LIMIT,
     Driver,
     Transaction,
 )
 
 try:
@@ -31,17 +31,17 @@
 
     REDIS = True
 except ImportError:  # pragma: no cover
     REDIS = False
 
 
 class RedisTransaction(Transaction):
-    modified_keys: Dict[str, bytes]
-    visited_keys: Dict[str, bytes]
-    deleted_keys: List[str]
+    modified_keys: dict[str, bytes]
+    visited_keys: dict[str, bytes]
+    deleted_keys: list[str]
 
     def __init__(self, redis: Any, driver: Driver):
         self.redis = redis
         self.driver = driver
         self.modified_keys = {}
         self.visited_keys = {}
         self.deleted_keys = []
@@ -76,32 +76,29 @@
         for index, ok in enumerate(oks):
             # We do no check deleted if its already deleted
             if index not in not_to_check:
                 assert ok
         self.clean()
         self.open = False
 
-    async def batch_get(self, keys: List[str]):
-        results = []
-        for key in keys:
-            if key in self.deleted_keys:
-                raise KeyError(f"Not found {key}")
+    async def batch_get(self, keys: list[str]) -> list[Optional[bytes]]:
+        if len(keys) == 0:
+            return []
 
-            if key in self.modified_keys:
-                results.append(self.modified_keys[key])
-                keys.remove(key)
+        bytes_keys: list[bytes] = [x.encode() for x in keys]
+        results = await self.redis.mget(bytes_keys)
 
+        for idx, key in enumerate(keys):
+            if key in self.deleted_keys:
+                results[idx] = None
+            if key in self.modified_keys:
+                results[idx] = self.modified_keys[key]
             if key in self.visited_keys:
-                results.append(self.visited_keys[key])
-                keys.remove(key)
+                results[idx] = self.visited_keys[key]
 
-        if len(keys) > 0:
-            bytes_keys: List[bytes] = [x.encode() for x in keys]
-            objs = await self.redis.mget(bytes_keys)
-            results.extend(objs)
         return results
 
     async def get(self, key: str) -> Optional[bytes]:
         if key in self.deleted_keys:
             raise KeyError(f"Not found {key}")
 
         if key in self.modified_keys:
@@ -159,14 +156,24 @@
                 yield str_key
                 prev_key = str_key
         if prev_key is None:
             for new_key in self.modified_keys.keys():
                 if match in new_key:
                     yield new_key
 
+    async def count(self, match: str) -> int:
+        """
+        This is not efficient but it works and redis is mostly for experiments
+        and should not be used for production environments
+        """
+        value = 0
+        async for _ in self.keys(match, count=-1):
+            value += 1
+        return value
+
 
 class RedisDriver(Driver):
     redis = None
     url = None
 
     def __init__(self, url: str):
         if REDIS is False:
@@ -179,18 +186,9 @@
         self.initialized = True
 
     async def finalize(self):
         if self.initialized is True:
             await self.redis.close()
             self.initialized = False
 
-    async def begin(self) -> RedisTransaction:
+    async def begin(self, read_only: bool = False) -> RedisTransaction:
         return RedisTransaction(self.redis, driver=self)
-
-    async def keys(
-        self, match: str, count: int = DEFAULT_SCAN_LIMIT, include_start: bool = True
-    ):
-        if self.redis is None:
-            raise AttributeError()
-        async with self.redis.client() as conn:
-            async for key in conn.scan_iter(match=match.encode() + b"*", count=count):
-                yield key.decode()
```

## Comparing `nucliadb/ingest/orm/abc.py` & `nucliadb/common/cluster/base.py`

 * *Files 27% similar despite different names*

```diff
@@ -14,195 +14,130 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 from abc import ABCMeta, abstractmethod
-from typing import Any, AsyncIterator, List, Optional
+from typing import AsyncIterator, Optional
 
-from nucliadb_protos.nodereader_pb2 import GetShardRequest  # type: ignore
-from nucliadb_protos.nodereader_pb2 import DocumentItem, ParagraphItem, StreamRequest
 from nucliadb_protos.nodereader_pb2_grpc import NodeReaderStub
-from nucliadb_protos.noderesources_pb2 import EmptyQuery
-from nucliadb_protos.noderesources_pb2 import Resource as PBBrainResource
-from nucliadb_protos.noderesources_pb2 import Shard as NodeResourcesShard
-from nucliadb_protos.noderesources_pb2 import (
-    ShardCleaned,
-    ShardCreated,
-    ShardId,
-    VectorSetID,
-    VectorSetList,
-)
 from nucliadb_protos.nodewriter_pb2 import (
     NewShardRequest,
     NewVectorSetRequest,
     OpStatus,
 )
 from nucliadb_protos.nodewriter_pb2_grpc import NodeWriterStub
-from nucliadb_protos.utils_pb2 import VectorSimilarity
-from nucliadb_protos.writer_pb2 import ShardObject as PBShard
-from nucliadb_protos.writer_pb2 import Shards as PBShards
-from pydantic import BaseModel
-
-from nucliadb.ingest.maindb.driver import Transaction
-from nucliadb_utils.keys import KB_SHARDS
 
+from nucliadb_protos import nodereader_pb2, noderesources_pb2, utils_pb2
 
-class ShardCounter(BaseModel):
-    shard: str
-    fields: int
-    paragraphs: int
 
+class AbstractIndexNode(metaclass=ABCMeta):
+    label: str = "index-node"
 
-class AbstractShard(metaclass=ABCMeta):
-    @abstractmethod
     def __init__(
-        self, sharduuid: str, shard: PBShard, node: Optional[Any] = None
-    ):  # pragma: no cover
-        pass
-
-    @abstractmethod
-    async def delete_resource(
-        self, uuid: str, txid: int, partition: str, kb: str
-    ):  # pragma: no cover
-        pass
-
-    @abstractmethod
-    async def add_resource(
         self,
-        resource: PBBrainResource,
-        txid: int,
-        partition: str,
-        kb: str,
-        reindex_id: Optional[str] = None,
-    ) -> None:  # pragma: no cover
-        pass
+        *,
+        id: str,
+        address: str,
+        shard_count: int,
+        available_disk: int,
+        dummy: bool = False,
+        primary_id: Optional[str] = None,
+    ):
+        self.id = id
+        self.address = address
+        self.shard_count = shard_count
+        self.available_disk = available_disk
+        self.dummy = dummy
+        self.primary_id = primary_id
+
+    def __str__(self):
+        if self.primary_id is None:
+            return f"{self.__class__.__name__}({self.id}, {self.address})"
+        else:
+            return f"{self.__class__.__name__}({self.id}, {self.address}, primary_id={self.primary_id})"
 
+    def __repr__(self):
+        return self.__str__()
 
-class AbstractNode(metaclass=ABCMeta):
-    label: str
+    def is_read_replica(self) -> bool:
+        return self.primary_id is not None
 
     @property
     @abstractmethod
     def reader(self) -> NodeReaderStub:  # pragma: no cover
         pass
 
     @property
     @abstractmethod
     def writer(self) -> NodeWriterStub:  # pragma: no cover
         pass
 
-    @classmethod
-    @abstractmethod
-    def create_shard_klass(
-        cls, shard_id: str, pbshard: PBShard
-    ) -> AbstractShard:  # pragma: no cover
-        pass
-
-    @classmethod
-    @abstractmethod
-    async def create_shard_by_kbid(
-        cls, txn: Transaction, kbid: str, similarity: VectorSimilarity.ValueType
-    ) -> AbstractShard:  # pragma: no cover
-        """
-        Create a new shard for a knowledge box and assign the current active
-        shard for new resources to be indexed into.
-        """
-        pass
-
-    @classmethod
-    @abstractmethod
-    async def get_current_active_shard(
-        cls, txn: Transaction, kbid: str
-    ) -> Optional[AbstractShard]:  # pragma: no cover
-        """
-        Shard that is currently receiving new resources
-        """
-        pass
-
     async def stream_get_fields(
-        self, stream_request: StreamRequest
-    ) -> AsyncIterator[DocumentItem]:
+        self, stream_request: nodereader_pb2.StreamRequest
+    ) -> AsyncIterator[nodereader_pb2.DocumentItem]:
         async for idandfacets in self.reader.Documents(stream_request):  # type: ignore
             yield idandfacets
 
     async def stream_get_paragraphs(
-        self, stream_request: StreamRequest
-    ) -> AsyncIterator[ParagraphItem]:
+        self, stream_request: nodereader_pb2.StreamRequest
+    ) -> AsyncIterator[nodereader_pb2.ParagraphItem]:
         async for idandfacets in self.reader.Paragraphs(stream_request):  # type: ignore
             yield idandfacets
 
-    @classmethod
-    async def get_all_shards(cls, txn: Transaction, kbid: str) -> Optional[PBShards]:
-        key = KB_SHARDS.format(kbid=kbid)
-        kb_shards_bytes: Optional[bytes] = await txn.get(key)
-        if kb_shards_bytes is not None:
-            kb_shards = PBShards()
-            kb_shards.ParseFromString(kb_shards_bytes)
-            return kb_shards
-        else:
-            return None
-
-    async def get_reader_shard(
+    async def get_shard(
         self, shard_id: str, vectorset: Optional[str] = None
-    ) -> NodeResourcesShard:
-        req = GetShardRequest()
+    ) -> noderesources_pb2.Shard:
+        req = nodereader_pb2.GetShardRequest()
         req.shard_id.id = shard_id
         if vectorset is not None:
             req.vectorset = vectorset
         return await self.reader.GetShard(req)  # type: ignore
 
-    async def get_shard(self, id: str) -> ShardId:
-        req = ShardId(id=id)
-        resp = await self.writer.GetShard(req)  # type: ignore
-        return resp
-
     async def new_shard(
         self,
         kbid: str,
-        similarity: VectorSimilarity.ValueType,
-    ) -> ShardCreated:
-        req = NewShardRequest(kbid=kbid, similarity=similarity)
+        similarity: utils_pb2.VectorSimilarity.ValueType,
+        release_channel: utils_pb2.ReleaseChannel.ValueType,
+    ) -> noderesources_pb2.ShardCreated:
+        req = NewShardRequest(
+            kbid=kbid, similarity=similarity, release_channel=release_channel
+        )
+
         resp = await self.writer.NewShard(req)  # type: ignore
         return resp
 
+    async def list_shards(self) -> list[str]:
+        shards = await self.writer.ListShards(noderesources_pb2.EmptyQuery())  # type: ignore
+        return [shard.id for shard in shards.ids]
+
     async def delete_shard(self, id: str) -> str:
-        req = ShardId(id=id)
-        resp: ShardId = await self.writer.DeleteShard(req)  # type: ignore
+        req = noderesources_pb2.ShardId(id=id)
+        resp: noderesources_pb2.ShardId = await self.writer.DeleteShard(req)  # type: ignore
         return resp.id
 
-    async def list_shards(self) -> List[str]:
-        req = EmptyQuery()
-        resp = await self.writer.ListShards(req)  # type: ignore
-        return resp.shards
-
-    async def clean_and_upgrade_shard(self, id: str) -> ShardCleaned:
-        req = ShardId(id=id)
-        resp = await self.writer.CleanAndUpgradeShard(req)  # type: ignore
-        return resp
-
     async def del_vectorset(self, shard_id: str, vectorset: str) -> OpStatus:
-        req = VectorSetID()
+        req = noderesources_pb2.VectorSetID()
         req.shard.id = shard_id
         req.vectorset = vectorset
         resp = await self.writer.RemoveVectorSet(req)  # type: ignore
         return resp
 
     async def set_vectorset(
         self,
         shard_id: str,
         vectorset: str,
-        similarity: VectorSimilarity.ValueType = VectorSimilarity.COSINE,
+        similarity: utils_pb2.VectorSimilarity.ValueType = utils_pb2.VectorSimilarity.COSINE,
     ) -> OpStatus:
         req = NewVectorSetRequest()
         req.id.shard.id = shard_id
         req.id.vectorset = vectorset
         req.similarity = similarity
         resp = await self.writer.AddVectorSet(req)  # type: ignore
         return resp
 
-    async def get_vectorset(self, shard_id: str) -> VectorSetList:
-        req = ShardId()
+    async def get_vectorset(self, shard_id: str) -> noderesources_pb2.VectorSetList:
+        req = noderesources_pb2.ShardId()
         req.id = shard_id
         resp = await self.writer.ListVectorSets(req)  # type: ignore
         return resp
```

## Comparing `nucliadb/ingest/orm/grpc_node_binding.py` & `nucliadb/common/cluster/standalone/grpc_node_binding.py`

 * *Files 9% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from __future__ import annotations
 
 import asyncio
 import logging
+import os
 import threading
 from concurrent.futures import ThreadPoolExecutor
 from typing import AsyncIterator
 
 from nucliadb_protos.nodereader_pb2 import (
     DocumentItem,
     EdgeList,
@@ -35,69 +36,83 @@
     RelationSearchRequest,
     RelationSearchResponse,
     SearchRequest,
     SearchResponse,
     StreamRequest,
     SuggestRequest,
     SuggestResponse,
-    TypeList,
 )
-from nucliadb_protos.noderesources_pb2 import EmptyQuery, Resource, ResourceID
+from nucliadb_protos.noderesources_pb2 import (
+    EmptyQuery,
+    EmptyResponse,
+    Resource,
+    ResourceID,
+)
 from nucliadb_protos.noderesources_pb2 import Shard as NodeResourcesShard
 from nucliadb_protos.noderesources_pb2 import (
-    ShardCleaned,
     ShardCreated,
     ShardId,
     ShardIds,
     ShardMetadata,
     VectorSetID,
     VectorSetList,
 )
-from nucliadb_protos.nodewriter_pb2 import OpStatus, SetGraph
+from nucliadb_protos.nodewriter_pb2 import OpStatus
 
-from nucliadb.ingest.settings import settings
+from ..settings import settings
 
 logger = logging.getLogger(__name__)
+
+try:
+    from nucliadb_node_binding import IndexNodeException  # type: ignore
+except ImportError:  # pragma: no cover
+    logger.warning("Import error while importing IndexNodeException")
+    IndexNodeException = Exception
+
 try:
     from nucliadb_node_binding import NodeReader  # type: ignore
     from nucliadb_node_binding import NodeWriter  # type: ignore
 except ImportError:  # pragma: no cover
     NodeReader = None
     NodeWriter = None
 
 
-class LocalReaderWrapper:
+class StandaloneReaderWrapper:
     reader: NodeReader
 
     def __init__(self):
-        self.reader = NodeReader.new()
+        if NodeReader is None:
+            raise ImportError(
+                "NucliaDB index node bindings are not installed (reader not found)"
+            )
+        self.reader = NodeReader()
         self.executor = ThreadPoolExecutor(settings.local_reader_threads)
 
     async def Search(
         self, request: SearchRequest, retry: bool = False
     ) -> SearchResponse:
         try:
             loop = asyncio.get_running_loop()
             result = await loop.run_in_executor(
                 self.executor, self.reader.search, request.SerializeToString()
             )
             pb_bytes = bytes(result)
             pb = SearchResponse()
             pb.ParseFromString(pb_bytes)
             return pb
-        except TypeError as exc:
+        except IndexNodeException as exc:
             if "IO error" not in str(exc):
                 # ignore any other error
                 raise
 
             # try some mitigations...
-            logger.error(f"TypeError in Search: {request}", exc_info=True)
+            logger.error(f"IndexNodeException in Search: {request}", exc_info=True)
             if not retry:
                 # reinit?
-                self.reader = NodeReader.new()
+                self.reader = NodeReader()
                 return await self.Search(request, retry=True)
             else:
                 raise
 
     async def ParagraphSearch(
         self, request: ParagraphSearchRequest
     ) -> ParagraphSearchResponse:
@@ -162,15 +177,15 @@
                 element = generator.next()
                 while element is not None:
                     pb_bytes = bytes(element)
                     pb = DocumentItem()
                     pb.ParseFromString(pb_bytes)
                     asyncio.run_coroutine_threadsafe(q.put(pb), loop).result()
                     element = generator.next()
-            except TypeError:
+            except StopIteration:
                 # this is the end
                 pass
             except Exception as e:
                 exception = e
             finally:
                 asyncio.run_coroutine_threadsafe(q.put(_END), loop).result()
 
@@ -201,15 +216,15 @@
                 element = generator.next()
                 while element is not None:
                     pb_bytes = bytes(element)
                     pb = ParagraphItem()
                     pb.ParseFromString(pb_bytes)
                     asyncio.run_coroutine_threadsafe(q.put(pb), loop).result()
                     element = generator.next()
-            except TypeError:
+            except StopIteration:
                 # this is the end
                 pass
             except Exception as e:
                 exception = e
             finally:
                 asyncio.run_coroutine_threadsafe(q.put(_END), loop).result()
 
@@ -230,42 +245,52 @@
             self.executor, self.reader.relation_edges, request.SerializeToString()
         )
         pb_bytes = bytes(result)
         edge_list = EdgeList()
         edge_list.ParseFromString(pb_bytes)
         return edge_list
 
-    async def RelationTypes(self, request: ShardId):
+
+async def Search(self, request: SearchRequest, retry: bool = False) -> SearchResponse:
+    try:
         loop = asyncio.get_running_loop()
         result = await loop.run_in_executor(
-            self.executor, self.reader.relation_types, request.SerializeToString()
+            self.executor, self.reader.search, request.SerializeToString()
         )
         pb_bytes = bytes(result)
-        type_list = TypeList()
-        type_list.ParseFromString(pb_bytes)
-        return type_list
+        pb = SearchResponse()
+        pb.ParseFromString(pb_bytes)
+        return pb
+    except IndexNodeException as exc:
+        if "IO error" not in str(exc):
+            # ignore any other error
+            raise
+
+        # try some mitigations...
+        logger.error(f"IndexNodeException in Search: {request}", exc_info=True)
+        if not retry:
+            # reinit?
+            self.reader = NodeReader()
+            return await self.Search(request, retry=True)
+        else:
+            raise
 
 
-class LocalWriterWrapper:
+class StandaloneWriterWrapper:
     writer: NodeWriter
 
     def __init__(self):
-        self.writer = NodeWriter.new()
+        os.makedirs(settings.data_path, exist_ok=True)
+        if NodeWriter is None:
+            raise ImportError(
+                "NucliaDB index node bindings are not installed (writer not found)"
+            )
+        self.writer = NodeWriter()
         self.executor = ThreadPoolExecutor(settings.local_writer_threads)
 
-    async def GetShard(self, request: ShardId) -> ShardId:
-        loop = asyncio.get_running_loop()
-        result = await loop.run_in_executor(
-            self.executor, self.writer.get_shard, request.SerializeToString()
-        )
-        pb_bytes = bytes(result)
-        pb = ShardId()
-        pb.ParseFromString(pb_bytes)
-        return pb
-
     async def NewShard(self, request: ShardMetadata) -> ShardCreated:
         loop = asyncio.get_running_loop()
         resp = await loop.run_in_executor(
             self.executor, self.writer.new_shard, request.SerializeToString()
         )
         pb_bytes = bytes(resp)
         shard_created = ShardCreated()
@@ -281,33 +306,22 @@
         shard_id = ShardId()
         shard_id.ParseFromString(pb_bytes)
         return shard_id
 
     async def ListShards(self, request: EmptyQuery) -> ShardIds:
         loop = asyncio.get_running_loop()
         resp = await loop.run_in_executor(
-            self.executor, self.writer.list_shards, request.SerializeToString()
+            self.executor,
+            self.writer.list_shards,
         )
         pb_bytes = bytes(resp)
         shard_ids = ShardIds()
         shard_ids.ParseFromString(pb_bytes)
         return shard_ids
 
-    async def CleanAndUpgradeShard(self, request: ShardId) -> ShardCleaned:
-        loop = asyncio.get_running_loop()
-        resp = await loop.run_in_executor(
-            self.executor,
-            self.writer.clean_and_upgrade_shard,
-            request.SerializeToString(),
-        )
-        pb_bytes = bytes(resp)
-        resp = ShardCleaned()
-        resp.ParseFromString(pb_bytes)
-        return resp
-
     async def RemoveVectorSet(self, request: VectorSetID):
         loop = asyncio.get_running_loop()
         resp = await loop.run_in_executor(
             self.executor, self.writer.del_vectorset, request.SerializeToString()
         )
         pb_bytes = bytes(resp)
         resp = OpStatus()
@@ -350,16 +364,38 @@
             self.executor, self.writer.remove_resource, request.SerializeToString()
         )
         pb_bytes = bytes(resp)
         op_status = OpStatus()
         op_status.ParseFromString(pb_bytes)
         return op_status
 
-    async def JoinGraph(self, request: SetGraph) -> OpStatus:
+    async def GC(self, request: ShardId) -> EmptyResponse:
         loop = asyncio.get_running_loop()
         resp = await loop.run_in_executor(
-            self.executor, self.writer.join_graph, request.SerializeToString()
+            self.executor, self.writer.gc, request.SerializeToString()
         )
         pb_bytes = bytes(resp)
-        op_status = OpStatus()
+        op_status = EmptyResponse()
         op_status.ParseFromString(pb_bytes)
         return op_status
+
+
+# supported marshalled reader methods for standalone node support
+READER_METHODS = {
+    "Search": (SearchRequest, SearchResponse),
+    "ParagraphSearch": (ParagraphSearchRequest, ParagraphSearchResponse),
+    "RelationSearch": (RelationSearchRequest, RelationSearchResponse),
+    "GetShard": (GetShardRequest, NodeResourcesShard),
+    "Suggest": (SuggestRequest, SuggestResponse),
+    "RelationEdges": (ShardId, EdgeList),
+}
+WRITER_METHODS = {
+    "NewShard": (ShardMetadata, ShardCreated),
+    "DeleteShard": (ShardId, ShardId),
+    "ListShards": (EmptyQuery, ShardIds),
+    "RemoveVectorSet": (VectorSetID, OpStatus),
+    "AddVectorSet": (VectorSetID, OpStatus),
+    "ListVectorSets": (ShardId, VectorSetList),
+    "SetResource": (Resource, OpStatus),
+    "RemoveResource": (ResourceID, OpStatus),
+    "GC": (ShardId, EmptyResponse),
+}
```

## Comparing `nucliadb/ingest/orm/grpc_node_dummy.py` & `nucliadb/common/cluster/grpc_node_dummy.py`

 * *Files 22% similar despite different names*

```diff
@@ -13,123 +13,89 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from __future__ import annotations
-
-from typing import Any, Dict, List
+from typing import Any
 
 from nucliadb_protos.nodereader_pb2 import (
     EdgeList,
     RelationEdge,
     RelationSearchResponse,
-    TypeList,
 )
+from nucliadb_protos.noderesources_pb2 import EmptyResponse
 from nucliadb_protos.noderesources_pb2 import Shard as NodeResourcesShard
 from nucliadb_protos.noderesources_pb2 import (
-    ShardCleaned,
     ShardCreated,
     ShardId,
-    ShardList,
+    ShardIds,
     VectorSetList,
 )
-from nucliadb_protos.nodesidecar_pb2 import Counter
-from nucliadb_protos.nodewriter_pb2 import OpStatus, SetGraph
+from nucliadb_protos.nodewriter_pb2 import OpStatus
 from nucliadb_protos.utils_pb2 import Relation
 
 
 class DummyWriterStub:  # pragma: no cover
-    calls: Dict[str, List[Any]] = {}
-
-    async def GetShard(self, data):
-        self.calls.setdefault("GetShard", []).append(data)
-        return NodeResourcesShard(
-            shard_id="shard", resources=2, paragraphs=2, sentences=2
-        )
+    def __init__(self):
+        self.calls: dict[str, list[Any]] = {}
 
-    async def NewShard(self, data):
+    async def NewShard(self, data):  # pragma: no cover
         self.calls.setdefault("NewShard", []).append(data)
         return ShardCreated(id="shard")
 
-    async def DeleteShard(self, data):
+    async def DeleteShard(self, data):  # pragma: no cover
         self.calls.setdefault("DeleteShard", []).append(data)
         return ShardId(id="shard")
 
-    async def CleanAndUpgradeShard(self, data):
-        self.calls.setdefault("CleanAndUpgradeShard", []).append(data)
-        return ShardCleaned(
-            document_service=ShardCreated.DocumentService.DOCUMENT_V1,
-            paragraph_service=ShardCreated.ParagraphService.PARAGRAPH_V1,
-            vector_service=ShardCreated.VectorService.VECTOR_V1,
-            relation_service=ShardCreated.RelationService.RELATION_V1,
-        )
-
-    async def ListShards(self, data):
+    async def ListShards(self, data):  # pragma: no cover
         self.calls.setdefault("ListShards", []).append(data)
-        sl = ShardList()
-        sl.shards.append(NodeResourcesShard(shard_id="shard", resources=2))
-        sl.shards.append(NodeResourcesShard(shard_id="shard2", resources=4))
-        return sl
+        shards = ShardIds()
+        shards.append(ShardId(shard_id="shard"))
+        shards.append(ShardId(shard_id="shard2"))
+        return shards
 
-    async def SetResource(self, data):
+    async def SetResource(self, data):  # pragma: no cover
         self.calls.setdefault("SetResource", []).append(data)
         result = OpStatus()
-        result.count = 1
+        result.field_count = 1
         return result
 
-    async def AddVectorSet(self, data):
+    async def AddVectorSet(self, data):  # pragma: no cover
         self.calls.setdefault("AddVectorSet", []).append(data)
         result = OpStatus()
-        result.count = 1
+        result.field_count = 1
         return result
 
-    async def ListVectorSet(self, data: ShardId):
+    async def ListVectorSet(self, data: ShardId):  # pragma: no cover
         self.calls.setdefault("ListVectorSet", []).append(data)
         result = VectorSetList()
         result.shard.id = data.id
         result.vectorset.append("base")
         return result
 
-    async def JoinGraph(self, data: SetGraph):
-        self.calls.setdefault("JoinGraph", []).append(data)
-        result = OpStatus()
-        result.count = 1
-        return result
+    async def GC(self, request: ShardId) -> EmptyResponse:  # pragma: no cover
+        self.calls.setdefault("GC", []).append(request)
+        return EmptyResponse()
 
 
 class DummyReaderStub:  # pragma: no cover
-    calls: Dict[str, List[Any]] = {}
+    def __init__(self):
+        self.calls: dict[str, list[Any]] = {}
 
-    async def GetShard(self, data):
+    async def GetShard(self, data):  # pragma: no cover
         self.calls.setdefault("GetShard", []).append(data)
-        return NodeResourcesShard(
-            shard_id="shard", resources=2, paragraphs=2, sentences=2
-        )
+        return NodeResourcesShard(shard_id="shard", fields=2, paragraphs=2, sentences=2)
 
-    async def RelationSearch(self, data):
+    async def RelationSearch(self, data):  # pragma: no cover
         self.calls.setdefault("RelationSearch", []).append(data)
         result = RelationSearchResponse()
         return result
 
-    async def RelationEdges(self, data):
+    async def RelationEdges(self, data):  # pragma: no cover
         self.calls.setdefault("RelationEdges", []).append(data)
         result = EdgeList()
         result.list.append(
             RelationEdge(edge_type=Relation.RelationType.ENTITY, property="dummy")
         )
         return result
-
-    async def RelationTypes(self, data):
-        self.calls.setdefault("RelationTypes", []).append(data)
-        result = TypeList()
-        return result
-
-
-class DummySidecarStub:  # pragma: no cover
-    calls: Dict[str, List[Any]] = {}
-
-    async def GetCount(self, data):
-        self.calls.setdefault("GetCount", []).append(data)
-        return Counter(paragraphs=2, resources=2)
```

## Comparing `nucliadb/ingest/orm/labels.py` & `nucliadb/standalone/tests/fixtures.py`

 * *Files 27% similar despite different names*

```diff
@@ -13,34 +13,26 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import Dict, List
+import uuid
 
-BASE_TAGS: Dict[str, List[str]] = {
-    "t": [],  # doc tags
-    "l": [],  # doc labels
-    "n": [],  # type of element: i (Icon). s (Processing Status)
-    "e": [],  # entities e/type/entityid
-    "s": [],  # languages p (Principal) s (ALL)
-    "u": [],  # contributors s (Source) o (Origin)
-    "p": [],  # paragraph labels
-    "f": [],  # field keyword field (field/keyword)
-    "fg": [],  # field keyword (keywords) flat
-}
-
-
-def flat_resource_tags(tags_dict):
-    flat_tags = []
-    for key, values in tags_dict.items():
-        if isinstance(values, dict):
-            for prefix, subvalues in values.items():
-                for value in subvalues:
-                    flat_tags.append(f"/{key}/{prefix}/{value}")
-
-        if isinstance(values, list):
-            for value in values:
-                flat_tags.append(f"/{key}/{value}")
-    return flat_tags
+import pytest
+
+from nucliadb.search.api.v1.router import KB_PREFIX, KBS_PREFIX
+
+
+@pytest.fixture(scope="function")
+async def knowledgebox_one(nucliadb_manager):
+    kbslug = str(uuid.uuid4())
+    data = {"slug": kbslug}
+    resp = await nucliadb_manager.post(f"/{KBS_PREFIX}", json=data)
+    assert resp.status_code == 201
+    kbid = resp.json()["uuid"]
+
+    yield kbid
+
+    resp = await nucliadb_manager.delete(f"/{KB_PREFIX}/{kbid}")
+    assert resp.status_code == 200
```

## Comparing `nucliadb/ingest/tests/tikv.py` & `nucliadb/tests/tikv.py`

 * *Files identical despite different names*

## Comparing `nucliadb/ingest/tests/unit/test_purge.py` & `nucliadb/tests/unit/test_purge.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,20 +13,20 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
-from unittest.mock import AsyncMock, patch
+from unittest.mock import AsyncMock, MagicMock, patch
 
 import pytest
 
-from nucliadb.ingest import purge
-from nucliadb.ingest.orm.exceptions import NodeError, ShardNotFound
+from nucliadb import purge
+from nucliadb.common.cluster.exceptions import NodeError, ShardNotFound
 
 pytestmark = pytest.mark.asyncio
 
 
 class DataIterator:
     def __init__(self, data):
         self.data = data
@@ -41,31 +41,40 @@
 
 @pytest.fixture
 def keys():
     yield []
 
 
 @pytest.fixture
-def driver(keys):
+def txn(keys):
     mock = AsyncMock()
     mock.keys = DataIterator(keys)
     yield mock
 
 
 @pytest.fixture
+def driver(txn):
+    mock = AsyncMock()
+    cm = AsyncMock()
+    cm.__aenter__.return_value = txn
+    mock.transaction = MagicMock(return_value=cm)
+    yield mock
+
+
+@pytest.fixture
 def storage():
     mock = AsyncMock()
     mock.delete_kb.return_value = True, False
     yield mock
 
 
 @pytest.fixture(autouse=True)
 def kb():
     mock = AsyncMock()
-    with patch("nucliadb.ingest.purge.KnowledgeBox", mock):
+    with patch("nucliadb.purge.KnowledgeBox", mock):
         yield mock
 
 
 async def test_purge(kb, keys, driver):
     keys.append("/pathto/kbid")
 
     await purge.purge_kb(driver)
@@ -86,15 +95,19 @@
 
     await purge.purge_kb(driver)
 
     driver.begin.return_value.commit.assert_not_called()
     driver.begin.return_value.abort.assert_called_once()
 
 
-async def test_purge_kb_storage(keys, driver, storage):
+async def test_purge_kb_storage(
+    keys,
+    driver,
+    storage,
+):
     keys.append("/pathto/kbid")
 
     await purge.purge_kb_storage(driver, storage)
 
     driver.begin.return_value.commit.assert_called_once()
 
 
@@ -106,18 +119,20 @@
 
     await purge.purge_kb_storage(driver, storage)
 
     driver.begin.return_value.commit.assert_not_called()
 
 
 async def test_main(driver, storage):
-    with patch("nucliadb.ingest.purge.purge_kb", AsyncMock()) as purge_kb, patch(
-        "nucliadb.ingest.purge.purge_kb_storage", AsyncMock()
+    with patch("nucliadb.purge.purge_kb", AsyncMock()) as purge_kb, patch(
+        "nucliadb.purge.purge_kb_storage", AsyncMock()
     ) as purge_kb_storage, patch(
-        "nucliadb.ingest.purge.get_storage", return_value=storage
+        "nucliadb.purge.get_storage", return_value=storage
+    ), patch(
+        "nucliadb.purge.setup_driver", return_value=driver
     ), patch(
-        "nucliadb.ingest.purge.get_driver", return_value=driver
+        "nucliadb.purge.setup_cluster", return_value=driver
     ):
         await purge.main()
 
         purge_kb.assert_called_once_with(driver)
         purge_kb_storage.assert_called_once_with(driver, storage)
```

## Comparing `nucliadb/ingest/tests/unit/test_txn_utils.py` & `nucliadb/tests/unit/common/maindb/test_tikv.py`

 * *Files 25% similar despite different names*

```diff
@@ -12,35 +12,42 @@
 # This program is distributed in the hope that it will be useful,
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
 
-import asyncio
-from unittest.mock import AsyncMock, patch
+from unittest.mock import AsyncMock, MagicMock
 
 import pytest
 
-from nucliadb.ingest import txn_utils
-
-pytestmark = pytest.mark.asyncio
-
-
-@pytest.fixture(autouse=True)
-def driver():
-    mock = AsyncMock()
-    with patch("nucliadb.ingest.txn_utils.get_driver", return_value=mock):
-        yield mock
-
-
-async def test_get_transaction_auto_aborts(driver) -> None:
-    async def mytask():
-        await txn_utils.get_transaction()
+from nucliadb.common.maindb.exceptions import ConflictError
+from nucliadb.common.maindb.tikv import LeaderNotFoundError, TiKVDataLayer
 
-    await asyncio.create_task(mytask())
 
-    await asyncio.sleep(0.05)
+@pytest.mark.parametrize(
+    "tikv_exception,handled_exception",
+    [
+        (
+            Exception("gRPC error: RpcFailure: 4-DEADLINE_EXCEEDED Deadline Exceeded"),
+            TimeoutError,
+        ),
+        (Exception("Leader of region 34234 is not found"), LeaderNotFoundError),
+    ],
+)
+async def test_get_retrials(tikv_exception, handled_exception):
+    inner_txn = MagicMock(get=AsyncMock(side_effect=tikv_exception))
+    tikv_txn = TiKVDataLayer(inner_txn)
+
+    with pytest.raises(handled_exception):
+        await tikv_txn.get("key")
+
+    assert inner_txn.get.call_count == 2
+
+
+async def test_commit_raises_conflict_error():
+    inner_txn = MagicMock(commit=AsyncMock(side_effect=Exception("WriteConflict")))
+    tikv_txn = TiKVDataLayer(inner_txn)
 
-    driver.begin.return_value.abort.assert_called_once()
+    with pytest.raises(ConflictError):
+        await tikv_txn.commit()
```

## Comparing `nucliadb/ingest/tests/unit/test_utils.py` & `nucliadb/tests/unit/common/maindb/test_utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -17,65 +17,65 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 
 from unittest.mock import AsyncMock, patch
 
 import pytest
 
-from nucliadb.ingest.utils import get_driver, settings
+from nucliadb.common.maindb.utils import settings, setup_driver
 from nucliadb_utils.exceptions import ConfigurationError
-from nucliadb_utils.store import MAIN
+from nucliadb_utils.utilities import Utility, clean_utility
 
 
 @pytest.fixture(autouse=True)
 def reset_driver_utils():
-    MAIN.pop("driver", None)
+    clean_utility(Utility.MAINDB_DRIVER)
     yield
-    MAIN.pop("driver", None)
+    clean_utility(Utility.MAINDB_DRIVER)
 
 
 @pytest.mark.asyncio
-async def test_get_driver_redis():
+async def test_setup_driver_redis():
     mock = AsyncMock(initialized=False)
     with patch.object(settings, "driver", "redis"), patch.object(
         settings, "driver_redis_url", "driver_redis_url"
-    ), patch("nucliadb.ingest.utils.RedisDriver", return_value=mock):
-        assert await get_driver() == mock
+    ), patch("nucliadb.common.maindb.utils.RedisDriver", return_value=mock):
+        assert await setup_driver() == mock
         mock.initialize.assert_awaited_once()
 
 
 @pytest.mark.asyncio
-async def test_get_driver_tikv():
+async def test_setup_driver_tikv():
     mock = AsyncMock(initialized=False)
     with patch.object(settings, "driver", "tikv"), patch.object(
         settings, "driver_tikv_url", "driver_tikv_url"
-    ), patch("nucliadb.ingest.utils.TiKVDriver", return_value=mock):
-        assert await get_driver() == mock
+    ), patch("nucliadb.common.maindb.utils.TiKVDriver", return_value=mock):
+        assert await setup_driver() == mock
         mock.initialize.assert_awaited_once()
 
 
 @pytest.mark.asyncio
-async def test_get_driver_pg():
+async def test_setup_driver_pg():
     mock = AsyncMock(initialized=False)
     with patch.object(settings, "driver", "pg"), patch.object(
         settings, "driver_pg_url", "driver_pg_url"
-    ), patch("nucliadb.ingest.utils.PGDriver", return_value=mock):
-        assert await get_driver() == mock
+    ), patch("nucliadb.common.maindb.utils.PGDriver", return_value=mock):
+        assert await setup_driver() == mock
         mock.initialize.assert_awaited_once()
 
 
 @pytest.mark.asyncio
-async def test_get_driver_local():
+async def test_setup_driver_local():
     mock = AsyncMock(initialized=False)
     with patch.object(settings, "driver", "local"), patch.object(
         settings, "driver_local_url", "driver_local_url"
-    ), patch("nucliadb.ingest.utils.LocalDriver", return_value=mock):
-        assert await get_driver() == mock
+    ), patch("nucliadb.common.maindb.utils.LocalDriver", return_value=mock):
+        assert await setup_driver() == mock
         mock.initialize.assert_awaited_once()
 
 
 @pytest.mark.asyncio
-async def test_get_driver_error():
+async def test_setup_driver_error():
     with patch.object(settings, "driver", "pg"), patch.object(
         settings, "driver_pg_url", None
     ), pytest.raises(ConfigurationError):
-        await get_driver()
+        await setup_driver()
```

## Comparing `nucliadb/ingest/tests/unit/orm/test_node.py` & `nucliadb/tests/unit/common/cluster/test_kb_shard_manager.py`

 * *Files 26% similar despite different names*

```diff
@@ -13,145 +13,156 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from unittest import mock
+import asyncio
+import uuid
+from typing import Any, Optional
+from unittest.mock import MagicMock
 
 import pytest
-from nucliadb_protos.writer_pb2 import Member
-from nucliadb_protos.writer_pb2 import Shards as PBShards
 
-from nucliadb.ingest.orm import NODES, NodeClusterSmall
-from nucliadb.ingest.orm.node import (
-    READ_CONNECTIONS,
-    SIDECAR_CONNECTIONS,
-    WRITE_CONNECTIONS,
-    Node,
-)
-from nucliadb.ingest.settings import settings
-from nucliadb_models.cluster import MemberType
-from nucliadb_utils.keys import KB_SHARDS
-
-
-def test_member_type_from_str():
-    for raw_type, member_type in [
-        ("Io", MemberType.IO),
-        ("Train", MemberType.TRAIN),
-        ("Ingest", MemberType.INGEST),
-        ("Search", MemberType.SEARCH),
-    ]:
-        assert MemberType(raw_type) == member_type
-
-
-def test_member_type_pb_conversion():
-    for member_type, pb_member_type in [
-        (MemberType.IO, Member.Type.IO),
-        (MemberType.TRAIN, Member.Type.TRAIN),
-        (MemberType.INGEST, Member.Type.INGEST),
-        (MemberType.SEARCH, Member.Type.SEARCH),
-        (MemberType.UNKNOWN, Member.Type.UNKNOWN),
-    ]:
-        assert member_type.to_pb() == pb_member_type
-        assert MemberType.from_pb(pb_member_type) == member_type
-
-
-async def get_kb_shards(txn, kbid):
-    kb_shards = None
-    kb_shards_key = KB_SHARDS.format(kbid=kbid)
-    kb_shards_binary = await txn.get(kb_shards_key)
-    if kb_shards_binary:
-        kb_shards = PBShards()
-        kb_shards.ParseFromString(kb_shards_binary)
-    return kb_shards
+from nucliadb.common import datamanagers
+from nucliadb.common.cluster import manager
+from nucliadb.common.cluster.settings import settings
+from nucliadb.common.maindb.driver import Transaction
+from nucliadb_protos import knowledgebox_pb2, utils_pb2, writer_pb2
+
+
+def test_should_create_new_shard():
+    sm = manager.KBShardManager()
+    low_para_counter = {
+        "num_paragraphs": settings.max_shard_paragraphs - 1,
+    }
+    high_para_counter = {
+        "num_paragraphs": settings.max_shard_paragraphs + 1,
+    }
+    assert sm.should_create_new_shard(**low_para_counter) is False
+    assert sm.should_create_new_shard(**high_para_counter) is True
 
 
 @pytest.fixture(scope="function")
-def one_replica():
-    prev = settings.node_replicas
-    settings.node_replicas = 1
-    yield
-    settings.node_replicas = prev
-
-
-@pytest.mark.asyncio
-async def test_create_shard_by_kbid(one_replica, txn, fake_node):
-    kbid = "mykbid"
-    # Initially there is no shards object
-    assert await get_kb_shards(txn, kbid) is None
-
-    # Create a shard
-    await Node.create_shard_by_kbid(txn, kbid)
-
-    # Check that kb shards object is correct
-    kb_shards = await get_kb_shards(txn, kbid)
-    assert len(kb_shards.shards) == 1
-    assert kb_shards.actual == 0
-    assert len(kb_shards.shards[0].replicas) == 1
-    node = kb_shards.shards[0].replicas[0].node
-
-    # Create another shard
-    await Node.create_shard_by_kbid(txn, kbid)
-
-    # Check that kb shards object was updated correctly
-    kb_shards = await get_kb_shards(txn, kbid)
-    assert len(kb_shards.shards) == 2
-    assert kb_shards.actual == 1
-    assert len(kb_shards.shards[1].replicas) == 1
-    # New shard has been created in a different node
-    assert kb_shards.shards[1].replicas[0].node != node
-
-
-@pytest.mark.asyncio
-async def test_create_shard_by_kbid_insufficient_nodes(txn):
-    with pytest.raises(NodeClusterSmall):
-        await Node.create_shard_by_kbid(txn, "foo")
-
-
-@pytest.fixture(scope="function")
-async def node_errors():
-    id, node = NODES.popitem()
-    await Node.set(
-        id,
-        address="nohost:9999",
-        type=MemberType.IO,
+async def fake_node():
+    manager.INDEX_NODES.clear()
+    yield manager.add_index_node(
+        id="node-0",
+        address="nohost",
         shard_count=0,
+        available_disk=100,
         dummy=True,
     )
-    NODES[id].new_shard = mock.AsyncMock(side_effect=ValueError)
+    manager.INDEX_NODES.clear()
+
+
+async def test_standalone_node_garbage_collects(fake_node):
+    mng = manager.StandaloneKBShardManager()
+
+    mng.max_ops_before_checks = 0
+
+    await mng.add_resource(
+        writer_pb2.ShardObject(
+            shard="123",
+            replicas=[
+                writer_pb2.ShardReplica(
+                    shard=writer_pb2.ShardCreated(id="123"), node="node-0"
+                )
+            ],
+        ),
+        resource=MagicMock(),
+        txid=-1,
+        partition=0,
+        kb="kb",
+    )
 
-    yield
+    await asyncio.sleep(0.05)
+    assert len(fake_node.writer.calls["GC"]) == 1
 
-    NODES[id] = node
 
+async def test_shard_creation(fake_index_nodes: list[str], txn: Transaction):
+    """Given a cluster of index nodes, validate shard creation logic.
 
-@pytest.mark.asyncio
-async def test_create_shard_by_kbid_rolls_back(txn, fake_node, node_errors):
-    with pytest.raises(ValueError):
-        await Node.create_shard_by_kbid(txn, "foo")
+    Every logic shard should create a configured amount of indexing replicas and
+    update the information about writable shards.
+
+    """
+    index_nodes = set(fake_index_nodes)
+    kbid = f"kbid:{test_shard_creation.__name__}"
+    semantic_model = knowledgebox_pb2.SemanticModelMetadata()
+    release_channel = utils_pb2.ReleaseChannel.STABLE
+    sm = manager.KBShardManager()
+
+    shards = await datamanagers.cluster.get_kb_shards(txn, kbid=kbid)
+    assert shards is None
+
+    # create first shard
+    await sm.create_shard_by_kbid(txn, kbid, semantic_model, release_channel)
+
+    shards = await datamanagers.cluster.get_kb_shards(txn, kbid=kbid)
+    assert shards is not None
+    assert len(shards.shards) == 1
+    assert shards.shards[0].read_only is False
+    # B/c with Shards.actual
+    assert shards.actual == 0
+    assert set((replica.node for replica in shards.shards[0].replicas)) == index_nodes
+
+    # adding a second shard will mark the first as read only
+    await sm.create_shard_by_kbid(txn, kbid, semantic_model, release_channel)
+
+    shards = await datamanagers.cluster.get_kb_shards(txn, kbid=kbid)
+    assert shards is not None
+    assert len(shards.shards) == 2
+    assert shards.shards[0].read_only is True
+    assert shards.shards[1].read_only is False
+    # B/c with Shards.actual
+    assert shards.actual == 1
+    assert set((replica.node for replica in shards.shards[1].replicas)) == index_nodes
+
+    # adding a third one will be equivalent
+    await sm.create_shard_by_kbid(txn, kbid, semantic_model, release_channel)
+
+    shards = await datamanagers.cluster.get_kb_shards(txn, kbid=kbid)
+    assert shards is not None
+    assert len(shards.shards) == 3
+    assert shards.shards[0].read_only is True
+    assert shards.shards[1].read_only is True
+    assert shards.shards[2].read_only is False
+    # B/c with Shards.actual
+    assert shards.actual == 2
+    assert set((replica.node for replica in shards.shards[1].replicas)) == index_nodes
+
+
+@pytest.fixture
+def txn():
+    class MockTransaction:
+        def __init__(self):
+            self.store = {}
 
+        async def get(self, key: str) -> Optional[Any]:
+            return self.store.get(key, None)
 
-def test_reset_connection():
-    READ_CONNECTIONS.clear()
-    WRITE_CONNECTIONS.clear()
-    SIDECAR_CONNECTIONS.clear()
+        async def set(self, key: str, value: Any):
+            self.store[key] = value
 
-    node = Node("host:1234", MemberType.IO, 0, dummy=True)
-    assert node.reader is not None
-    assert node.writer is not None
-    assert node.sidecar is not None
+    yield MockTransaction()
 
-    assert len(READ_CONNECTIONS) == 1
-    assert len(WRITE_CONNECTIONS) == 1
-    assert len(SIDECAR_CONNECTIONS) == 1
 
-    node.reset_connections()
+@pytest.fixture(scope="function")
+def fake_index_nodes():
+    assert len(manager.INDEX_NODES) == 0, "Some test isn't cleaning global state!"
+
+    nodes = [f"node-{i}" for i in range(settings.node_replicas)]
+    for node_id in nodes:
+        manager.add_index_node(
+            id=node_id,
+            address=f"nohost-{str(uuid.uuid4())}:1234",
+            shard_count=0,
+            available_disk=100,
+            dummy=True,
+        )
 
-    assert len(READ_CONNECTIONS) == 0
-    assert len(WRITE_CONNECTIONS) == 0
-    assert len(SIDECAR_CONNECTIONS) == 0
+    yield nodes
 
-    assert node._reader is None
-    assert node._writer is None
-    assert node._sidecar is None
+    for node_id in nodes:
+        manager.remove_index_node(node_id)
```

## Comparing `nucliadb/ingest/tests/unit/orm/test_shard.py` & `nucliadb/migrator/exceptions.py`

 * *Files 23% similar despite different names*

```diff
@@ -13,29 +13,11 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from nucliadb_protos.writer_pb2 import ShardObject as PBShard
-from nucliadb_protos.writer_pb2 import ShardReplica as PBReplica
 
-from nucliadb.ingest.orm.shard import Shard
 
-
-def test_indexing_replicas():
-    pbshard = PBShard()
-
-    rep1 = PBReplica()
-    rep1.shard.id = "rep1"
-    rep1.node = "node1"
-    pbshard.replicas.append(rep1)
-
-    rep2 = PBReplica()
-    rep2.shard.id = "rep2"
-    rep2.node = "node2"
-    pbshard.replicas.append(rep2)
-
-    shard = Shard("shard_id", pbshard)
-
-    assert shard.indexing_replicas() == [("rep1", "node1"), ("rep2", "node2")]
+class MigrationValidationError(Exception):
+    pass
```

## Comparing `nucliadb/one/__init__.py` & `nucliadb/common/cluster/standalone/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -12,13 +12,7 @@
 # This program is distributed in the hope that it will be useful,
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-import logging
-
-logger = logging.getLogger("nucliadb.one")
-
-API_PREFIX = "api"
```

## Comparing `nucliadb/one/app.py` & `nucliadb/standalone/app.py`

 * *Files 26% similar despite different names*

```diff
@@ -16,136 +16,161 @@
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import logging
 import os
 
-import nucliadb_contributor_assets  # type: ignore
-import pkg_resources
+import nucliadb_admin_assets  # type: ignore
 from fastapi import FastAPI
-from fastapi.responses import JSONResponse
-from fastapi.routing import APIRouter
+from fastapi.responses import JSONResponse, RedirectResponse
 from fastapi.staticfiles import StaticFiles
-from fastapi_versioning import version
 from starlette.middleware import Middleware
 from starlette.middleware.authentication import AuthenticationMiddleware
 from starlette.middleware.cors import CORSMiddleware
+from starlette.requests import ClientDisconnect, Request
 from starlette.responses import HTMLResponse
 from starlette.routing import Mount
 
-from nucliadb.http_clients.processing import ProcessingHTTPClient
-from nucliadb.one.lifecycle import finalize, initialize
+from nucliadb.common.context.fastapi import set_app_context
+from nucliadb.middleware import ProcessTimeHeaderMiddleware
+from nucliadb.middleware.transaction import ReadOnlyTransactionMiddleware
 from nucliadb.reader import API_PREFIX
 from nucliadb.reader.api.v1.router import api as api_reader_v1
 from nucliadb.search.api.v1.router import api as api_search_v1
+from nucliadb.standalone.lifecycle import finalize, initialize
 from nucliadb.train.api.v1.router import api as api_train_v1
 from nucliadb.writer.api.v1.router import api as api_writer_v1
 from nucliadb_telemetry import errors
-from nucliadb_telemetry.fastapi import instrument_app, metrics_endpoint
-from nucliadb_utils.authentication import STFAuthenticationBackend
+from nucliadb_telemetry.fastapi import metrics_endpoint
 from nucliadb_utils.fastapi.openapi import extend_openapi
 from nucliadb_utils.fastapi.versioning import VersionedFastAPI
-from nucliadb_utils.settings import http_settings, nuclia_settings, running_settings
+from nucliadb_utils.settings import http_settings, running_settings
 
-logger = logging.getLogger(__name__)
-
-middleware = [
-    Middleware(
-        CORSMiddleware,
-        allow_origins=http_settings.cors_origins,
-        allow_methods=["*"],
-        allow_headers=["*"],
-    ),
-    Middleware(
-        AuthenticationMiddleware,
-        backend=STFAuthenticationBackend(),
-    ),
-]
-
-
-errors.setup_error_handling(pkg_resources.get_distribution("nucliadb").version)
-
-
-on_startup = [initialize]
-on_shutdown = [finalize]
-
-
-fastapi_settings = dict(
-    debug=running_settings.debug,
-    middleware=middleware,
-    on_startup=on_startup,
-    on_shutdown=on_shutdown,
-)
-
-
-base_app = FastAPI(title="NucliaDB API", **fastapi_settings)  # type: ignore
-
-base_app.include_router(api_writer_v1)
-base_app.include_router(api_reader_v1)
-base_app.include_router(api_search_v1)
-base_app.include_router(api_train_v1)
-
-
-standalone_api_router = APIRouter()
-
-
-@standalone_api_router.get("/config-check")
-@version(1)
-async def api_config_check():
-    valid_nua_key = False
-    nua_key_check_error = None
-    if nuclia_settings.nuclia_service_account is not None:
-        async with ProcessingHTTPClient() as processing_client:
-            try:
-                await processing_client.status()
-                valid_nua_key = True
-            except Exception as exc:
-                logger.warning(f"Error validating nua key", exc_info=exc)
-                nua_key_check_error = f"Error checking NUA key: {str(exc)}"
-    return JSONResponse(
-        {
-            "nua_api_key": {
-                "has_key": nuclia_settings.nuclia_service_account is not None,
-                "valid": valid_nua_key,
-                "error": nua_key_check_error,
-            }
-        }
-    )
+from .api_router import standalone_api_router
+from .auth import get_auth_backend
+from .settings import Settings
 
+logger = logging.getLogger(__name__)
 
-base_app.include_router(standalone_api_router)
 
-application = VersionedFastAPI(
-    base_app,
-    version_format="{major}",
-    prefix_format=f"/{API_PREFIX}/v{{major}}",
-    default_version=(1, 0),
-    enable_latest=False,
-    kwargs=fastapi_settings,
-)
+HOMEPAGE_HTML = """
+<!DOCTYPE html>
+<html lang="en">
+<head>
+    <meta charset="UTF-8">
+    <meta name="viewport" content="width=device-width, initial-scale=1.0">
+    <title>NucliaDB Standalone Server</title>
+</head>
+<body>
+    <h1>Welcome to NucliaDB Standalone Server</h1>
+    <p> The NucliaDB API is exposed at /api/v1. </p>
+    <br>
+    <h2>Quick Links</h2>
+    <ul>
+        <li><a href="/admin">Admin UI</a></li>
+        <li><a href="https://docs.nuclia.dev/docs/guides/nucliadb/deploy/basics">NucliaDB Deployment Documentation</a></li>
+        <li><a href="https://docs.nuclia.dev/docs/api">API Reference</a></li>
+        <li><a href="/api/v1/docs">API Explorer</a></li>
+        <li><a href="/metrics">Metrics</a></li>
+        <li><a href="https://docs.nuclia.dev/docs/">Nuclia Documentation</a></li>
+    </ul>
+</body>
+</html>
+"""  # noqa: E501
+
+
+def application_factory(settings: Settings) -> FastAPI:
+    middleware = [
+        Middleware(
+            CORSMiddleware,
+            allow_origins=http_settings.cors_origins,
+            allow_methods=["*"],
+            allow_headers=["*"],
+        ),
+        Middleware(
+            AuthenticationMiddleware,
+            backend=get_auth_backend(settings),
+        ),
+        Middleware(ReadOnlyTransactionMiddleware),
+    ]
+    if running_settings.debug:
+        middleware.append(Middleware(ProcessTimeHeaderMiddleware))
+
+    fastapi_settings = dict(
+        debug=running_settings.debug,
+        middleware=middleware,
+        on_startup=[initialize],
+        on_shutdown=[finalize],
+        exception_handlers={
+            Exception: global_exception_handler,
+            ClientDisconnect: client_disconnect_handler,
+        },
+    )
 
+    base_app = FastAPI(title="NucliaDB API", **fastapi_settings)  # type: ignore
+    base_app.include_router(api_writer_v1)
+    base_app.include_router(api_reader_v1)
+    base_app.include_router(api_search_v1)
+    base_app.include_router(api_train_v1)
+    base_app.include_router(standalone_api_router)
+
+    application = VersionedFastAPI(
+        base_app,
+        version_format="{major}",
+        prefix_format=f"/{API_PREFIX}/v{{major}}",
+        default_version=(1, 0),
+        enable_latest=False,
+        kwargs=fastapi_settings,
+    )
 
-for route in application.routes:
-    if isinstance(route, Mount):
-        extend_openapi(route)
+    for route in application.routes:
+        if isinstance(route, Mount):
+            extend_openapi(route)
+
+    async def homepage(request):
+        return HTMLResponse(HOMEPAGE_HTML)
+
+    # Use raw starlette routes to avoid unnecessary overhead
+    application.add_route("/", homepage)
+    application.add_route("/metrics", metrics_endpoint)
+
+    # mount admin app assets
+    application.mount(
+        "/admin",
+        StaticFiles(
+            directory=os.path.dirname(nucliadb_admin_assets.__file__), html=True
+        ),
+        name="static",
+    )
+    # redirect /contributor -> /admin
+    application.add_route("/contributor", lambda request: RedirectResponse("/admin"))
+    application.mount(
+        "/widget",
+        StaticFiles(directory=os.path.dirname(__file__) + "/static", html=True),
+        name="widget",
+    )
 
+    application.settings = settings  # type: ignore
+    for route in application.router.routes:
+        if isinstance(route, Mount):
+            route.app.settings = settings  # type: ignore
 
-async def homepage(request):
-    return HTMLResponse("NucliaDB Standalone Server")
+    # Inject application context into the fastapi app's state
+    set_app_context(application)
 
+    return application
 
-# Use raw starlette routes to avoid unnecessary overhead
-application.add_route("/", homepage)
-application.add_route("/metrics", metrics_endpoint)
 
-# mount contributor app assets
-application.mount(
-    "/contributor",
-    StaticFiles(
-        directory=os.path.dirname(nucliadb_contributor_assets.__file__), html=True
-    ),
-    name="static",
-)
+async def global_exception_handler(request: Request, exc: Exception):
+    errors.capture_exception(exc)
+    return JSONResponse(
+        status_code=500,
+        content={"detail": "Something went wrong, please contact your administrator"},
+    )
 
 
-instrument_app(application, excluded_urls=["/"], metrics=True)
+async def client_disconnect_handler(request: Request, exc: ClientDisconnect):
+    return JSONResponse(
+        status_code=200,
+        content={"detail": "Client disconnected while an operation was in course"},
+    )
```

## Comparing `nucliadb/one/lifecycle.py` & `nucliadb/standalone/lifecycle.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,14 +15,15 @@
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 import asyncio
 
+from nucliadb.common.cluster.utils import setup_cluster, teardown_cluster
 from nucliadb.ingest.app import initialize_grpc as initialize_ingest_grpc
 from nucliadb.ingest.app import initialize_pull_workers
 from nucliadb.ingest.settings import settings as ingest_settings
 from nucliadb.reader.lifecycle import finalize as finalize_reader
 from nucliadb.reader.lifecycle import initialize as initialize_reader
 from nucliadb.search.lifecycle import finalize as finalize_search
 from nucliadb.search.lifecycle import initialize as initialize_search
@@ -41,14 +42,15 @@
     else:
         finalizers = await initialize_pull_workers()
     SYNC_FINALIZERS.extend(finalizers)
     await initialize_writer()
     await initialize_reader()
     await initialize_search()
     await initialize_train()
+    await setup_cluster()
 
 
 async def finalize():
     for finalizer in SYNC_FINALIZERS:
         if asyncio.iscoroutinefunction(finalizer):
             await finalizer()
         else:
@@ -56,7 +58,8 @@
     SYNC_FINALIZERS.clear()
 
     await finalize_writer()
     await finalize_reader()
     await finalize_search()
     await finalize_train()
     await finalize_utilities()
+    await teardown_cluster()
```

## Comparing `nucliadb/one/tests/__init__.py` & `nucliadb/common/cluster/__init__.py`

 * *Files identical despite different names*

## Comparing `nucliadb/one/tests/conftest.py` & `nucliadb/tests/unit/tasks/conftest.py`

 * *Files 20% similar despite different names*

```diff
@@ -13,18 +13,30 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-pytest_plugins = [
-    "pytest_docker_fixtures",
-    "nucliadb.ingest.tests.fixtures",
-    "nucliadb.search.tests.fixtures",
-    "nucliadb.search.tests.node",
-    "nucliadb.one.tests.fixtures",
-    "nucliadb_utils.tests.gcs",
-    "nucliadb_utils.tests.nats",
-    "nucliadb_utils.tests.s3",
-    "nucliadb_utils.tests.indexing",
-]
+from unittest.mock import AsyncMock, Mock
+
+import nats
+import pytest
+
+
+@pytest.fixture(scope="function")
+def nats_manager():
+    nats_manager = Mock()
+    nats_manager.subscribe = AsyncMock()
+    js = Mock()
+    js.stream_info = AsyncMock(side_effect=nats.js.errors.NotFoundError)
+    js.add_stream = AsyncMock()
+    nats_manager.js = js
+    yield nats_manager
+
+
+@pytest.fixture(scope="function")
+def context(nats_manager):
+    context = Mock()
+    context.initialize = AsyncMock()
+    context.nats_manager = nats_manager
+    yield context
```

## Comparing `nucliadb/one/tests/test_basic.py` & `nucliadb/ingest/tests/integration/consumer/test_materializer.py`

 * *Files 26% similar despite different names*

```diff
@@ -13,58 +13,77 @@
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
-from typing import Callable
 
-import pytest
-from httpx import AsyncClient
-
-from nucliadb.search.api.v1.router import KB_PREFIX
-from nucliadb_models.resource import NucliaDBRoles
+import asyncio
 
+import pytest
 
-@pytest.mark.asyncio
-async def test_basic_patch_thumbnail_sc_2390(
-    nucliadb_api: Callable[..., AsyncClient], knowledgebox_one
-) -> None:
-    async with nucliadb_api(roles=[NucliaDBRoles.WRITER]) as client:
-        resp = await client.post(
-            f"/{KB_PREFIX}/{knowledgebox_one}/resources",
-            json={
-                "title": "Resource title",
-                "summary": "A simple summary",
-                "thumbnail": "thumbnail-on-creation",
-            },
-            headers={"X-Synchronous": "true"},
+from nucliadb.common import datamanagers
+from nucliadb.ingest.consumer import materializer
+from nucliadb.ingest.tests.fixtures import create_resource
+from nucliadb_protos import writer_pb2
+from nucliadb_utils import const
+
+pytestmark = pytest.mark.asyncio
+
+
+async def test_materialize_kb_data(
+    maindb_driver,
+    pubsub,
+    storage,
+    fake_node,
+    knowledgebox_ingest,
+):
+    count = 10
+    for _ in range(count):
+        await create_resource(
+            storage=storage,
+            driver=maindb_driver,
+            knowledgebox_ingest=knowledgebox_ingest,
         )
-        assert resp.status_code == 201
-        rid = resp.json()["uuid"]
 
-    async with nucliadb_api(roles=[NucliaDBRoles.READER]) as client:
-        resp = await client.get(
-            f"/{KB_PREFIX}/{knowledgebox_one}/resource/{rid}",
+    mz = materializer.MaterializerHandler(
+        driver=maindb_driver,
+        storage=storage,
+        pubsub=pubsub,
+        check_delay=0.05,
+    )
+    await mz.initialize()
+
+    async with datamanagers.with_transaction() as txn:
+        assert (
+            await datamanagers.resources.get_number_of_resources(
+                txn, kbid=knowledgebox_ingest
+            )
+            == -1
         )
-        assert resp.status_code == 200
-
-        resource = resp.json()
-        assert resource["thumbnail"] == "thumbnail-on-creation"
-
-    async with nucliadb_api(roles=[NucliaDBRoles.WRITER]) as client:
-        resp = await client.patch(
-            f"/{KB_PREFIX}/{knowledgebox_one}/resource/{rid}",
-            json={"thumbnail": "thumbnail-modified"},
-            headers={"X-Synchronous": "true"},
+        assert (
+            await datamanagers.resources.calculate_number_of_resources(
+                txn, kbid=knowledgebox_ingest
+            )
+            == count
         )
-        assert resp.status_code == 200
 
-    async with nucliadb_api(roles=[NucliaDBRoles.READER]) as client:
-        resp = await client.get(
-            f"/{KB_PREFIX}/{knowledgebox_one}/resource/{rid}",
+    await pubsub.publish(
+        const.PubSubChannels.RESOURCE_NOTIFY.format(kbid=knowledgebox_ingest),
+        writer_pb2.Notification(
+            kbid=knowledgebox_ingest,
+            action=writer_pb2.Notification.Action.COMMIT,
+        ).SerializeToString(),
+    )
+
+    await asyncio.sleep(0.2)
+
+    async with datamanagers.with_transaction() as txn:
+        assert (
+            await datamanagers.resources.get_number_of_resources(
+                txn, kbid=knowledgebox_ingest
+            )
+            == count
         )
-        assert resp.status_code == 200
 
-        resource = resp.json()
-        assert resource["thumbnail"] == "thumbnail-modified"
+    await mz.finalize()
```

## Comparing `nucliadb/search/api/v1/resource.py` & `nucliadb/search/api/v1/resource/search.py`

 * *Files 18% similar despite different names*

```diff
@@ -14,126 +14,131 @@
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from datetime import datetime
-from typing import List, Optional
+from typing import Optional, Union
 
-from fastapi import Header, HTTPException, Query, Request, Response
+from fastapi import Header, Request, Response
 from fastapi_versioning import version
 
-from nucliadb.ingest.serialize import get_resource_uuid_by_slug
-from nucliadb.ingest.txn_utils import abort_transaction
-from nucliadb.search import SERVICE_NAME
-from nucliadb.search.requesters.utils import Method, node_query
+from nucliadb.models.responses import HTTPClientError
+from nucliadb.search.api.v1.router import KB_PREFIX, RESOURCE_PREFIX, api
+from nucliadb.search.api.v1.utils import fastapi_query
+from nucliadb.search.requesters.utils import Method, debug_nodes_info, node_query
+from nucliadb.search.search.exceptions import InvalidQueryError
 from nucliadb.search.search.merge import merge_paragraphs_results
 from nucliadb.search.search.query import paragraph_query_to_pb
 from nucliadb_models.common import FieldTypeName
 from nucliadb_models.resource import ExtractedDataTypeName, NucliaDBRoles
 from nucliadb_models.search import (
     NucliaDBClientType,
     ResourceProperties,
     ResourceSearchResults,
     SearchOptions,
+    SearchParamDefaults,
     SortField,
     SortOrder,
 )
 from nucliadb_utils.authentication import requires_one
 
-from .router import KB_PREFIX, RESOURCE_PREFIX, RSLUG_PREFIX, api
 
-
-@api.get(
-    f"/{KB_PREFIX}/{{kbid}}/{RSLUG_PREFIX}/{{rslug}}/search",
-    status_code=200,
-    description="Search on a Resource",
-    tags=["Search"],
-    response_model_exclude_unset=True,
-)
 @api.get(
     f"/{KB_PREFIX}/{{kbid}}/{RESOURCE_PREFIX}/{{rid}}/search",
     status_code=200,
+    name="Search on Resource",
     description="Search on a Resource",
     tags=["Search"],
     response_model_exclude_unset=True,
+    response_model=ResourceSearchResults,
 )
 @requires_one([NucliaDBRoles.READER])
 @version(1)
-async def search(
+async def resource_search(
     request: Request,
     response: Response,
     kbid: str,
     query: str,
-    rid: Optional[str] = None,
-    rslug: Optional[str] = None,
-    fields: List[str] = Query(default=[]),
-    filters: List[str] = Query(default=[]),
-    faceted: List[str] = Query(default=[]),
-    sort: Optional[SortField] = Query(default=None, alias="sort_field"),
-    sort_order: SortOrder = Query(default=SortOrder.DESC),
-    page_number: int = 0,
-    page_size: int = 20,
-    range_creation_start: Optional[datetime] = None,
-    range_creation_end: Optional[datetime] = None,
-    range_modification_start: Optional[datetime] = None,
-    range_modification_end: Optional[datetime] = None,
-    reload: bool = Query(False),
-    highlight: bool = Query(False),
-    split: bool = Query(False),
-    show: List[ResourceProperties] = Query(list(ResourceProperties)),
-    field_type_filter: List[FieldTypeName] = Query(
-        list(FieldTypeName), alias="field_type"
+    rid: str,
+    fields: list[str] = fastapi_query(SearchParamDefaults.fields),
+    filters: list[str] = fastapi_query(SearchParamDefaults.filters),
+    faceted: list[str] = fastapi_query(SearchParamDefaults.faceted),
+    sort: Optional[SortField] = fastapi_query(
+        SearchParamDefaults.sort_field, alias="sort_field"
+    ),
+    sort_order: SortOrder = fastapi_query(SearchParamDefaults.sort_order),
+    page_number: int = fastapi_query(SearchParamDefaults.page_number),
+    page_size: int = fastapi_query(SearchParamDefaults.page_size),
+    range_creation_start: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_creation_start
+    ),
+    range_creation_end: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_creation_end
+    ),
+    range_modification_start: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_modification_start
+    ),
+    range_modification_end: Optional[datetime] = fastapi_query(
+        SearchParamDefaults.range_modification_end
+    ),
+    highlight: bool = fastapi_query(SearchParamDefaults.highlight),
+    show: list[ResourceProperties] = fastapi_query(
+        SearchParamDefaults.show, default=list(ResourceProperties)
+    ),
+    field_type_filter: list[FieldTypeName] = fastapi_query(
+        SearchParamDefaults.field_type_filter, alias="field_type"
+    ),
+    extracted: list[ExtractedDataTypeName] = fastapi_query(
+        SearchParamDefaults.extracted
     ),
-    extracted: List[ExtractedDataTypeName] = Query(list(ExtractedDataTypeName)),
     x_ndb_client: NucliaDBClientType = Header(NucliaDBClientType.API),
-    debug: bool = Query(False),
-    shards: List[str] = Query(default=[]),
-) -> ResourceSearchResults:
-    if not rid:
-        rid = await get_resource_uuid_by_slug(kbid, rslug, service_name=SERVICE_NAME)  # type: ignore
-        if rid is None:
-            raise HTTPException(status_code=404, detail="Resource does not exist")
-
+    debug: bool = fastapi_query(SearchParamDefaults.debug),
+    shards: list[str] = fastapi_query(SearchParamDefaults.shards),
+) -> Union[ResourceSearchResults, HTTPClientError]:
     # We need to query all nodes
-    pb_query = await paragraph_query_to_pb(
-        [SearchOptions.PARAGRAPH],
-        rid,
-        query,
-        fields,
-        filters,
-        faceted,
-        page_number,
-        page_size,
-        range_creation_start,
-        range_creation_end,
-        range_modification_start,
-        range_modification_end,
-        reload=reload,
-        sort=sort.value if sort else None,
-        sort_ord=sort_order.value,
-    )
+    try:
+        pb_query = await paragraph_query_to_pb(
+            kbid,
+            [SearchOptions.PARAGRAPH],
+            rid,
+            query,
+            fields,
+            filters,
+            faceted,
+            page_number,
+            page_size,
+            range_creation_start,
+            range_creation_end,
+            range_modification_start,
+            range_modification_end,
+            sort=sort.value if sort else None,
+            sort_ord=sort_order.value,
+        )
+    except InvalidQueryError as exc:
+        return HTTPClientError(status_code=412, detail=str(exc))
 
-    results, incomplete_results, queried_nodes, queried_shards = await node_query(
+    results, incomplete_results, queried_nodes = await node_query(
         kbid, Method.PARAGRAPH, pb_query, shards
     )
 
     # We need to merge
     search_results = await merge_paragraphs_results(
         results,
         count=page_size,
         page=page_number,
         kbid=kbid,
         show=show,
         field_type_filter=field_type_filter,
         extracted=extracted,
         highlight_split=highlight,
+        min_score=0.0,
     )
-    await abort_transaction()
 
     response.status_code = 206 if incomplete_results else 200
     if debug:
-        search_results.nodes = queried_nodes
+        search_results.nodes = debug_nodes_info(queried_nodes)
 
+    queried_shards = [shard_id for _, shard_id in queried_nodes]
     search_results.shards = queried_shards
     return search_results
```

## Comparing `nucliadb/search/tests/unit/test_openapi.py` & `nucliadb/tests/unit/test_openapi.py`

 * *Files 10% similar despite different names*

```diff
@@ -17,22 +17,22 @@
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
 #
 from unittest.mock import Mock
 
 from starlette.routing import Mount
 
+from nucliadb.openapi import extract_openapi, is_versioned_route
 from nucliadb.search.app import application
-from nucliadb.search.openapi import extract_openapi, is_versioned_route
 
 
 def get_route(path):
     return Mount(path=path, app=Mock())
 
 
 def test_is_versioned_route():
     assert is_versioned_route(get_route(path="/api/v1/search"))
     assert not is_versioned_route(get_route(path="/metrics"))
 
 
 def test_extract_openapi():
-    assert extract_openapi(application, "1", "commitid")
+    assert extract_openapi(application, "1", "commitid", "nucliadb_search")
```

## Comparing `nucliadb/static/favicon.ico` & `nucliadb/standalone/static/favicon.ico`

 * *Files identical despite different names*

## Comparing `nucliadb/static/index.html` & `nucliadb/standalone/static/index.html`

 * *Files identical despite different names*

## Comparing `nucliadb/static/logo.svg` & `nucliadb/standalone/static/logo.svg`

 * *Files identical despite different names*

## Comparing `nucliadb/train/api/models.py` & `nucliadb/common/cluster/discovery/types.py`

 * *Files 15% similar despite different names*

```diff
@@ -12,17 +12,21 @@
 # This program is distributed in the hope that it will be useful,
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-
-
-from typing import List
-
-from pydantic import BaseModel
+import time
+from dataclasses import dataclass, field
+from typing import Optional
 
 
-class TrainSetPartitions(BaseModel):
-    partitions: List[str]
+@dataclass
+class IndexNodeMetadata:
+    node_id: str
+    name: str
+    address: str
+    shard_count: int
+    available_disk: int
+    primary_id: Optional[str] = None
+    updated_at: float = field(default_factory=time.time)
```

## Comparing `nucliadb/writer/tests/tus.py` & `nucliadb/search/search/chat/images.py`

 * *Files 23% similar despite different names*

```diff
@@ -12,57 +12,49 @@
 # This program is distributed in the hope that it will be useful,
 # but WITHOUT ANY WARRANTY; without even the implied warranty of
 # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 # GNU Affero General Public License for more details.
 #
 # You should have received a copy of the GNU Affero General Public License
 # along with this program. If not, see <http://www.gnu.org/licenses/>.
-#
-import tempfile
 
-import pytest
+import base64
+
+from nucliadb.search import SERVICE_NAME
+from nucliadb_models.search import Image
+from nucliadb_utils.utilities import get_storage
 
-from nucliadb.writer.tus.gcs import GCloudBlobStore
-from nucliadb.writer.tus.local import LocalBlobStore
-from nucliadb.writer.tus.s3 import S3BlobStore
-
-
-@pytest.fixture(scope="function")
-async def s3_storage_tus(s3):
-    storage = S3BlobStore()
-    await storage.initialize(
-        client_id="",
-        client_secret="",
-        max_pool_connections=2,
-        endpoint_url=s3,
-        verify_ssl=False,
-        ssl=False,
-        region_name=None,
-        bucket="test_{kbid}",
-    )
-    yield storage
-    await storage.finalize()
 
+async def get_page_image(kbid: str, paragraph_id: str, page: int) -> Image:
+    storage = await get_storage(service_name=SERVICE_NAME)
 
-@pytest.fixture(scope="function")
-async def gcs_storage_tus(gcs):
-    storage = GCloudBlobStore()
-    await storage.initialize(
-        json_credentials=None,
-        bucket="test_{kbid}",
-        location="location",
-        project="project",
-        bucket_labels={},
-        object_base_url=gcs,
+    rid, field_type_letter, field_id, _ = paragraph_id.split("/")[:4]
+
+    sf = storage.file_extracted(
+        kbid, rid, field_type_letter, field_id, f"generated/extracted_images_{page}.png"
+    )
+    image = Image(
+        b64encoded=base64.b64encode(
+            (await sf.storage.downloadbytes(sf.bucket, sf.key)).read()
+        ).decode(),
+        content_type="image/png",
     )
-    yield storage
-    await storage.finalize()
 
+    return image
+
+
+async def get_paragraph_image(kbid: str, paragraph_id: str, reference: str) -> Image:
+    storage = await get_storage(service_name=SERVICE_NAME)
+
+    rid, field_type_letter, field_id, _ = paragraph_id.split("/")[:4]
+
+    sf = storage.file_extracted(
+        kbid, rid, field_type_letter, field_id, f"generated/{reference}"
+    )
+    image = Image(
+        b64encoded=base64.b64encode(
+            (await sf.storage.downloadbytes(sf.bucket, sf.key)).read()
+        ).decode(),
+        content_type="image/png",
+    )
 
-@pytest.fixture(scope="function")
-async def local_storage_tus():
-    folder = tempfile.TemporaryDirectory()
-    storage = LocalBlobStore(local_testing_files=folder.name)
-    await storage.initialize()
-    yield storage
-    await storage.finalize()
-    folder.cleanup()
+    return image
```

## Comparing `nucliadb-2.9.0.post267.dist-info/entry_points.txt` & `nucliadb-3.0.0.post414.dist-info/entry_points.txt`

 * *Files 16% similar despite different names*

```diff
@@ -1,17 +1,22 @@
 [console_scripts]
-nucliadb = nucliadb.run:run
+nucliadb = nucliadb.standalone.run:run
 nucliadb-dataset-upload = nucliadb.train.upload:run
 nucliadb-extract-openapi-reader = nucliadb.reader.openapi:command_extract_openapi
 nucliadb-extract-openapi-search = nucliadb.search.openapi:command_extract_openapi
 nucliadb-extract-openapi-writer = nucliadb.writer.openapi:command_extract_openapi
 nucliadb-ingest = nucliadb.ingest.app:run_consumer
 nucliadb-ingest-orm-grpc = nucliadb.ingest.app:run_orm_grpc
 nucliadb-ingest-processed-consumer = nucliadb.ingest.app:run_processed_consumer
-nucliadb-ingest-purge = nucliadb.ingest.purge:run
 nucliadb-ingest-subscriber-workers = nucliadb.ingest.app:run_subscriber_workers
-nucliadb-purge = nucliadb.purge:purge
+nucliadb-metrics-exporter = nucliadb.metrics_exporter:main
+nucliadb-migrate = nucliadb.migrator.command:main
+nucliadb-migration-runner = nucliadb.migrator.command:main_forever
+nucliadb-orphan-shards = nucliadb.purge.orphan_shards:run
+nucliadb-purge = nucliadb.purge:run
 nucliadb-reader = nucliadb.reader.run:run
+nucliadb-rebalance = nucliadb.common.cluster.rebalance:main
+nucliadb-rollover-kbid = nucliadb.common.cluster.rollover:rollover_kbid_command
 nucliadb-search = nucliadb.search.run:run
 nucliadb-train = nucliadb.train.run:run
+nucliadb-validate-migrations = nucliadb.migrator.command:validate
 nucliadb-writer = nucliadb.writer.run:run
-
```

