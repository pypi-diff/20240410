# Comparing `tmp/paxml-1.3.1-py3-none-any.whl.zip` & `tmp/paxml-1.4.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,132 +1,136 @@
-Zip file size: 430029 bytes, number of entries: 130
--rw-r--r--  2.0 unx    29105 b- defN 24-Feb-21 08:00 paxml/automl.py
--rw-r--r--  2.0 unx    17158 b- defN 24-Feb-21 08:00 paxml/automl_interfaces.py
--rw-r--r--  2.0 unx    36536 b- defN 24-Feb-21 08:00 paxml/automl_test.py
--rw-r--r--  2.0 unx     2030 b- defN 24-Feb-21 08:00 paxml/base_executor.py
--rw-r--r--  2.0 unx     7176 b- defN 24-Feb-21 08:00 paxml/base_experiment.py
--rw-r--r--  2.0 unx     3214 b- defN 24-Feb-21 08:00 paxml/base_inference_runner.py
--rw-r--r--  2.0 unx     3058 b- defN 24-Feb-21 08:00 paxml/base_inference_runner_test.py
--rw-r--r--  2.0 unx    17258 b- defN 24-Feb-21 08:00 paxml/base_metrics.py
--rw-r--r--  2.0 unx     7160 b- defN 24-Feb-21 08:00 paxml/base_metrics_test.py
--rw-r--r--  2.0 unx     1106 b- defN 24-Feb-21 08:00 paxml/base_task.py
--rw-r--r--  2.0 unx    22776 b- defN 24-Feb-21 08:00 paxml/checkpoint_creators.py
--rw-r--r--  2.0 unx    22358 b- defN 24-Feb-21 08:00 paxml/checkpoint_managers.py
--rw-r--r--  2.0 unx    36281 b- defN 24-Feb-21 08:00 paxml/checkpoint_managers_test.py
--rw-r--r--  2.0 unx     9996 b- defN 24-Feb-21 08:00 paxml/checkpoint_metadata.py
--rw-r--r--  2.0 unx     8351 b- defN 24-Feb-21 08:00 paxml/checkpoint_paths.py
--rw-r--r--  2.0 unx     1555 b- defN 24-Feb-21 08:00 paxml/checkpoint_types.py
--rw-r--r--  2.0 unx     2030 b- defN 24-Feb-21 08:00 paxml/checkpoint_version.py
--rw-r--r--  2.0 unx    30794 b- defN 24-Feb-21 08:00 paxml/checkpoints.py
--rw-r--r--  2.0 unx    13204 b- defN 24-Feb-21 08:00 paxml/checkpoints_test.py
--rw-r--r--  2.0 unx    16889 b- defN 24-Feb-21 08:00 paxml/decode_programs.py
--rw-r--r--  2.0 unx    38940 b- defN 24-Feb-21 08:00 paxml/eval_lib.py
--rw-r--r--  2.0 unx    20594 b- defN 24-Feb-21 08:00 paxml/executors.py
--rw-r--r--  2.0 unx     1659 b- defN 24-Feb-21 08:00 paxml/experiment_imports_all_test.py
--rw-r--r--  2.0 unx     5162 b- defN 24-Feb-21 08:00 paxml/experiment_imports_test_helper.py
--rw-r--r--  2.0 unx     6734 b- defN 24-Feb-21 08:00 paxml/experiment_registry.py
--rw-r--r--  2.0 unx     4771 b- defN 24-Feb-21 08:00 paxml/experiment_registry_test.py
--rw-r--r--  2.0 unx     4202 b- defN 24-Feb-21 08:00 paxml/experiment_utils.py
--rw-r--r--  2.0 unx     4496 b- defN 24-Feb-21 08:00 paxml/host_callback.py
--rw-r--r--  2.0 unx     2683 b- defN 24-Feb-21 08:00 paxml/host_callback_test.py
--rw-r--r--  2.0 unx    13232 b- defN 24-Feb-21 08:00 paxml/io_utils.py
--rw-r--r--  2.0 unx     6381 b- defN 24-Feb-21 08:00 paxml/io_utils_test.py
--rw-r--r--  2.0 unx    26711 b- defN 24-Feb-21 08:00 paxml/learners.py
--rw-r--r--  2.0 unx    45747 b- defN 24-Feb-21 08:00 paxml/learners_test.py
--rw-r--r--  2.0 unx    21958 b- defN 24-Feb-21 08:00 paxml/main.py
--rw-r--r--  2.0 unx     2207 b- defN 24-Feb-21 08:00 paxml/main_test.py
--rw-r--r--  2.0 unx     7444 b- defN 24-Feb-21 08:00 paxml/metric_utils.py
--rw-r--r--  2.0 unx    12901 b- defN 24-Feb-21 08:00 paxml/metric_utils_test.py
--rw-r--r--  2.0 unx     7770 b- defN 24-Feb-21 08:00 paxml/parameterized_experiment.py
--rw-r--r--  2.0 unx     6055 b- defN 24-Feb-21 08:00 paxml/parameterized_experiment_test.py
--rw-r--r--  2.0 unx    58148 b- defN 24-Feb-21 08:00 paxml/partitioning.py
--rw-r--r--  2.0 unx     1863 b- defN 24-Feb-21 08:00 paxml/partitioning_test.py
--rw-r--r--  2.0 unx     3130 b- defN 24-Feb-21 08:00 paxml/profiling.py
--rw-r--r--  2.0 unx    36593 b- defN 24-Feb-21 08:00 paxml/programs.py
--rw-r--r--  2.0 unx     4517 b- defN 24-Feb-21 08:00 paxml/programs_test.py
--rw-r--r--  2.0 unx    76681 b- defN 24-Feb-21 08:00 paxml/seqio_input.py
--rw-r--r--  2.0 unx    52224 b- defN 24-Feb-21 08:00 paxml/seqio_input_test.py
--rw-r--r--  2.0 unx     2830 b- defN 24-Feb-21 08:00 paxml/setup_jax.py
--rw-r--r--  2.0 unx    29994 b- defN 24-Feb-21 08:00 paxml/sgf.py
--rw-r--r--  2.0 unx    34259 b- defN 24-Feb-21 08:00 paxml/summary_utils.py
--rw-r--r--  2.0 unx    16853 b- defN 24-Feb-21 08:00 paxml/summary_utils_test.py
--rw-r--r--  2.0 unx    79608 b- defN 24-Feb-21 08:00 paxml/tasks_lib.py
--rw-r--r--  2.0 unx    54467 b- defN 24-Feb-21 08:00 paxml/tasks_lib_test.py
--rw-r--r--  2.0 unx      793 b- defN 24-Feb-21 08:00 paxml/test_helper.py
--rw-r--r--  2.0 unx     5670 b- defN 24-Feb-21 08:00 paxml/tf_data_service_lib.py
--rw-r--r--  2.0 unx    10911 b- defN 24-Feb-21 08:00 paxml/train.py
--rw-r--r--  2.0 unx     4136 b- defN 24-Feb-21 08:00 paxml/train_states.py
--rw-r--r--  2.0 unx    61465 b- defN 24-Feb-21 08:00 paxml/trainer_lib.py
--rw-r--r--  2.0 unx     7065 b- defN 24-Feb-21 08:00 paxml/trainer_lib_test.py
--rw-r--r--  2.0 unx    39966 b- defN 24-Feb-21 08:00 paxml/tuning_lib.py
--rw-r--r--  2.0 unx    34356 b- defN 24-Feb-21 08:00 paxml/tuning_lib_test.py
--rw-r--r--  2.0 unx     3864 b- defN 24-Feb-21 08:00 paxml/xla_passthrough.py
--rw-r--r--  2.0 unx     4354 b- defN 24-Feb-21 08:00 paxml/xla_passthrough_test.py
--rw-r--r--  2.0 unx     9642 b- defN 24-Feb-21 08:00 paxml/contrib/gpu/scripts_gpu/configs.py
--rw-r--r--  2.0 unx      784 b- defN 24-Feb-21 08:00 paxml/contrib/gpu/scripts_gpu/download_lambada.py
--rw-r--r--  2.0 unx      777 b- defN 24-Feb-21 08:00 paxml/contrib/gpu/scripts_gpu/download_the_pile.py
--rw-r--r--  2.0 unx     6130 b- defN 24-Feb-21 08:00 paxml/contrib/gpu/scripts_gpu/tasks.py
--rw-r--r--  2.0 unx     5411 b- defN 24-Feb-21 08:00 paxml/contrib/gpu/scripts_gpu/tfds_lambada.py
--rw-r--r--  2.0 unx     7376 b- defN 24-Feb-21 08:00 paxml/contrib/gpu/scripts_gpu/tfds_pile.py
--rw-r--r--  2.0 unx     4968 b- defN 24-Feb-21 08:00 paxml/experimental/baseline_experiment.py
--rw-r--r--  2.0 unx     5047 b- defN 24-Feb-21 08:00 paxml/experimental/baseline_experiment_test.py
--rw-r--r--  2.0 unx     1271 b- defN 24-Feb-21 08:00 paxml/experimental/nested_map_config_helper.py
--rw-r--r--  2.0 unx     1018 b- defN 24-Feb-21 08:00 paxml/experimental/nested_map_config_helper_test.py
--rw-r--r--  2.0 unx     4814 b- defN 24-Feb-21 08:00 paxml/ghostnorm/base.py
--rw-r--r--  2.0 unx     6408 b- defN 24-Feb-21 08:00 paxml/ghostnorm/embedding.py
--rw-r--r--  2.0 unx    12969 b- defN 24-Feb-21 08:00 paxml/ghostnorm/generic_wrapper.py
--rw-r--r--  2.0 unx    24887 b- defN 24-Feb-21 08:00 paxml/ghostnorm/layers_test.py
--rw-r--r--  2.0 unx     5870 b- defN 24-Feb-21 08:00 paxml/ghostnorm/linears.py
--rw-r--r--  2.0 unx      597 b- defN 24-Feb-21 08:00 paxml/tasks/lm/__init__.py
--rw-r--r--  2.0 unx    17398 b- defN 24-Feb-21 08:00 paxml/tasks/lm/input_generator.py
--rw-r--r--  2.0 unx     4089 b- defN 24-Feb-21 08:00 paxml/tasks/lm/input_generator_test.py
--rw-r--r--  2.0 unx    35722 b- defN 24-Feb-21 08:00 paxml/tasks/lm/model_params.py
--rw-r--r--  2.0 unx     6610 b- defN 24-Feb-21 08:00 paxml/tasks/lm/params/bert.py
--rw-r--r--  2.0 unx    30685 b- defN 24-Feb-21 08:00 paxml/tasks/lm/params/c4.py
--rw-r--r--  2.0 unx     7395 b- defN 24-Feb-21 08:00 paxml/tasks/lm/params/c4_multislice.py
--rw-r--r--  2.0 unx     3647 b- defN 24-Feb-21 08:00 paxml/tasks/lm/params/c4_test.py
--rw-r--r--  2.0 unx    10423 b- defN 24-Feb-21 08:00 paxml/tasks/lm/params/lm_cloud.py
--rw-r--r--  2.0 unx    17732 b- defN 24-Feb-21 08:00 paxml/tasks/lm/params/nvidia.py
--rw-r--r--  2.0 unx     3040 b- defN 24-Feb-21 08:00 paxml/tasks/lm/params/optimal_scaling.py
--rw-r--r--  2.0 unx     1124 b- defN 24-Feb-21 08:00 paxml/tasks/test/synthetic.py
--rw-r--r--  2.0 unx     5197 b- defN 24-Feb-21 08:00 paxml/tasks/vision/input_generator.py
--rw-r--r--  2.0 unx     1611 b- defN 24-Feb-21 08:00 paxml/tasks/vision/input_generator_test.py
--rw-r--r--  2.0 unx     7002 b- defN 24-Feb-21 08:00 paxml/tasks/vision/resnet_preprocessing.py
--rw-r--r--  2.0 unx     9055 b- defN 24-Feb-21 08:00 paxml/tasks/vision/params/imagenet_resnets.py
--rw-r--r--  2.0 unx     8285 b- defN 24-Feb-21 08:00 paxml/tasks/vision/params/mnist.py
--rw-r--r--  2.0 unx     1319 b- defN 24-Feb-21 08:00 paxml/tools/dump_hparams.py
--rw-r--r--  2.0 unx     7098 b- defN 24-Feb-21 08:00 paxml/tools/dump_hparams_lib.py
--rw-r--r--  2.0 unx     1590 b- defN 24-Feb-21 08:00 paxml/tools/dump_input_specs.py
--rw-r--r--  2.0 unx     3755 b- defN 24-Feb-21 08:00 paxml/tools/dump_input_specs_lib.py
--rw-r--r--  2.0 unx    11008 b- defN 24-Feb-21 08:00 paxml/tools/model_analysis.py
--rw-r--r--  2.0 unx     1000 b- defN 24-Feb-21 08:00 paxml/tools/validate_config.py
--rw-r--r--  2.0 unx     4976 b- defN 24-Feb-21 08:00 paxml/tools/validate_config_lib.py
--rw-r--r--  2.0 unx    28469 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen.py
--rw-r--r--  2.0 unx     5045 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen_external_init_checkpoint_fns.py
--rw-r--r--  2.0 unx     3738 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen_external_init_checkpoint_fns_test.py
--rw-r--r--  2.0 unx     3303 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen_highlevel_parameterization.py
--rw-r--r--  2.0 unx     2268 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen_highlevel_parameterization_test.py
--rw-r--r--  2.0 unx     1420 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen_pax_code_ir.py
--rw-r--r--  2.0 unx     4723 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen_sharding.py
--rw-r--r--  2.0 unx    31202 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen_test.py
--rw-r--r--  2.0 unx     3488 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen_tracer.py
--rw-r--r--  2.0 unx     5917 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/codegen_tracer_test.py
--rw-r--r--  2.0 unx     8857 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/config_normalization.py
--rw-r--r--  2.0 unx     5526 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/config_normalization_test.py
--rw-r--r--  2.0 unx     1741 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/convert_seqio_task_objects.py
--rw-r--r--  2.0 unx     1344 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/convert_seqio_task_objects_test.py
--rw-r--r--  2.0 unx     2437 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/graphviz_utils.py
--rw-r--r--  2.0 unx      987 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/graphviz_utils_test.py
--rw-r--r--  2.0 unx     3043 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/make_parameterized_experiment.py
--rw-r--r--  2.0 unx     2862 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/remove_sharding.py
--rw-r--r--  2.0 unx     3709 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/remove_sharding_test.py
--rw-r--r--  2.0 unx     6062 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/test_fixtures.py
--rw-r--r--  2.0 unx     2486 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/unshare_sharding.py
--rw-r--r--  2.0 unx     4813 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/unshare_sharding_test.py
--rw-r--r--  2.0 unx     1326 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/wrap_nested_maps.py
--rw-r--r--  2.0 unx     1759 b- defN 24-Feb-21 08:00 paxml/tools/fiddle/wrap_nested_maps_test.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Feb-21 08:09 paxml-1.3.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     1177 b- defN 24-Feb-21 08:09 paxml-1.3.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Feb-21 08:09 paxml-1.3.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        6 b- defN 24-Feb-21 08:09 paxml-1.3.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    11444 b- defN 24-Feb-21 08:09 paxml-1.3.1.dist-info/RECORD
-130 files, 1609760 bytes uncompressed, 412013 bytes compressed:  74.4%
+Zip file size: 440198 bytes, number of entries: 134
+-rw-r--r--  2.0 unx    29105 b- defN 24-Apr-09 21:17 paxml/automl.py
+-rw-r--r--  2.0 unx    17158 b- defN 24-Apr-09 21:17 paxml/automl_interfaces.py
+-rw-r--r--  2.0 unx    36854 b- defN 24-Apr-09 21:17 paxml/automl_test.py
+-rw-r--r--  2.0 unx     2030 b- defN 24-Apr-09 21:17 paxml/base_executor.py
+-rw-r--r--  2.0 unx     7176 b- defN 24-Apr-09 21:17 paxml/base_experiment.py
+-rw-r--r--  2.0 unx     3214 b- defN 24-Apr-09 21:17 paxml/base_inference_runner.py
+-rw-r--r--  2.0 unx     3086 b- defN 24-Apr-09 21:17 paxml/base_inference_runner_test.py
+-rw-r--r--  2.0 unx    17258 b- defN 24-Apr-09 21:17 paxml/base_metrics.py
+-rw-r--r--  2.0 unx     7160 b- defN 24-Apr-09 21:17 paxml/base_metrics_test.py
+-rw-r--r--  2.0 unx     1106 b- defN 24-Apr-09 21:17 paxml/base_task.py
+-rw-r--r--  2.0 unx    22776 b- defN 24-Apr-09 21:17 paxml/checkpoint_creators.py
+-rw-r--r--  2.0 unx    21268 b- defN 24-Apr-09 21:17 paxml/checkpoint_managers.py
+-rw-r--r--  2.0 unx    36573 b- defN 24-Apr-09 21:17 paxml/checkpoint_managers_test.py
+-rw-r--r--  2.0 unx     9996 b- defN 24-Apr-09 21:17 paxml/checkpoint_metadata.py
+-rw-r--r--  2.0 unx    11335 b- defN 24-Apr-09 21:17 paxml/checkpoint_paths.py
+-rw-r--r--  2.0 unx     1555 b- defN 24-Apr-09 21:17 paxml/checkpoint_types.py
+-rw-r--r--  2.0 unx     2030 b- defN 24-Apr-09 21:17 paxml/checkpoint_version.py
+-rw-r--r--  2.0 unx    30794 b- defN 24-Apr-09 21:17 paxml/checkpoints.py
+-rw-r--r--  2.0 unx    13204 b- defN 24-Apr-09 21:17 paxml/checkpoints_test.py
+-rw-r--r--  2.0 unx    16889 b- defN 24-Apr-09 21:17 paxml/decode_programs.py
+-rw-r--r--  2.0 unx    39112 b- defN 24-Apr-09 21:17 paxml/eval_lib.py
+-rw-r--r--  2.0 unx    20594 b- defN 24-Apr-09 21:17 paxml/executors.py
+-rw-r--r--  2.0 unx     1659 b- defN 24-Apr-09 21:17 paxml/experiment_imports_all_test.py
+-rw-r--r--  2.0 unx     5149 b- defN 24-Apr-09 21:17 paxml/experiment_imports_test_helper.py
+-rw-r--r--  2.0 unx     6734 b- defN 24-Apr-09 21:17 paxml/experiment_registry.py
+-rw-r--r--  2.0 unx     4771 b- defN 24-Apr-09 21:17 paxml/experiment_registry_test.py
+-rw-r--r--  2.0 unx     4202 b- defN 24-Apr-09 21:17 paxml/experiment_utils.py
+-rw-r--r--  2.0 unx      651 b- defN 24-Apr-09 21:17 paxml/first_result_metric_callback.py
+-rw-r--r--  2.0 unx     4496 b- defN 24-Apr-09 21:17 paxml/host_callback.py
+-rw-r--r--  2.0 unx     2683 b- defN 24-Apr-09 21:17 paxml/host_callback_test.py
+-rw-r--r--  2.0 unx    13232 b- defN 24-Apr-09 21:17 paxml/io_utils.py
+-rw-r--r--  2.0 unx     6381 b- defN 24-Apr-09 21:17 paxml/io_utils_test.py
+-rw-r--r--  2.0 unx    26711 b- defN 24-Apr-09 21:17 paxml/learners.py
+-rw-r--r--  2.0 unx    46163 b- defN 24-Apr-09 21:17 paxml/learners_test.py
+-rw-r--r--  2.0 unx    22105 b- defN 24-Apr-09 21:17 paxml/main.py
+-rw-r--r--  2.0 unx     2207 b- defN 24-Apr-09 21:17 paxml/main_test.py
+-rw-r--r--  2.0 unx     7549 b- defN 24-Apr-09 21:17 paxml/metric_utils.py
+-rw-r--r--  2.0 unx    13059 b- defN 24-Apr-09 21:17 paxml/metric_utils_test.py
+-rw-r--r--  2.0 unx     7770 b- defN 24-Apr-09 21:17 paxml/parameterized_experiment.py
+-rw-r--r--  2.0 unx     6055 b- defN 24-Apr-09 21:17 paxml/parameterized_experiment_test.py
+-rw-r--r--  2.0 unx    58214 b- defN 24-Apr-09 21:17 paxml/partitioning.py
+-rw-r--r--  2.0 unx     1863 b- defN 24-Apr-09 21:17 paxml/partitioning_test.py
+-rw-r--r--  2.0 unx     3130 b- defN 24-Apr-09 21:17 paxml/profiling.py
+-rw-r--r--  2.0 unx    37241 b- defN 24-Apr-09 21:17 paxml/programs.py
+-rw-r--r--  2.0 unx     4557 b- defN 24-Apr-09 21:17 paxml/programs_test.py
+-rw-r--r--  2.0 unx    77003 b- defN 24-Apr-09 21:17 paxml/seqio_input.py
+-rw-r--r--  2.0 unx    53698 b- defN 24-Apr-09 21:17 paxml/seqio_input_test.py
+-rw-r--r--  2.0 unx     2830 b- defN 24-Apr-09 21:17 paxml/setup_jax.py
+-rw-r--r--  2.0 unx    29994 b- defN 24-Apr-09 21:17 paxml/sgf.py
+-rw-r--r--  2.0 unx    34279 b- defN 24-Apr-09 21:17 paxml/summary_utils.py
+-rw-r--r--  2.0 unx    17018 b- defN 24-Apr-09 21:17 paxml/summary_utils_test.py
+-rw-r--r--  2.0 unx    79592 b- defN 24-Apr-09 21:17 paxml/tasks_lib.py
+-rw-r--r--  2.0 unx    54788 b- defN 24-Apr-09 21:17 paxml/tasks_lib_test.py
+-rw-r--r--  2.0 unx      793 b- defN 24-Apr-09 21:17 paxml/test_helper.py
+-rw-r--r--  2.0 unx     5670 b- defN 24-Apr-09 21:17 paxml/tf_data_service_lib.py
+-rw-r--r--  2.0 unx    11267 b- defN 24-Apr-09 21:17 paxml/train.py
+-rw-r--r--  2.0 unx     4136 b- defN 24-Apr-09 21:17 paxml/train_states.py
+-rw-r--r--  2.0 unx    61465 b- defN 24-Apr-09 21:17 paxml/trainer_lib.py
+-rw-r--r--  2.0 unx     7249 b- defN 24-Apr-09 21:17 paxml/trainer_lib_test.py
+-rw-r--r--  2.0 unx    39966 b- defN 24-Apr-09 21:17 paxml/tuning_lib.py
+-rw-r--r--  2.0 unx    34450 b- defN 24-Apr-09 21:17 paxml/tuning_lib_test.py
+-rw-r--r--  2.0 unx     3864 b- defN 24-Apr-09 21:17 paxml/xla_passthrough.py
+-rw-r--r--  2.0 unx     4354 b- defN 24-Apr-09 21:17 paxml/xla_passthrough_test.py
+-rw-r--r--  2.0 unx    11498 b- defN 24-Apr-09 21:17 paxml/contrib/gpu/scripts_gpu/configs.py
+-rw-r--r--  2.0 unx      748 b- defN 24-Apr-09 21:17 paxml/contrib/gpu/scripts_gpu/download_boolq.py
+-rw-r--r--  2.0 unx      784 b- defN 24-Apr-09 21:17 paxml/contrib/gpu/scripts_gpu/download_lambada.py
+-rw-r--r--  2.0 unx      777 b- defN 24-Apr-09 21:17 paxml/contrib/gpu/scripts_gpu/download_the_pile.py
+-rw-r--r--  2.0 unx     6384 b- defN 24-Apr-09 21:17 paxml/contrib/gpu/scripts_gpu/llama_utils.py
+-rw-r--r--  2.0 unx    13319 b- defN 24-Apr-09 21:17 paxml/contrib/gpu/scripts_gpu/saxml_layers.py
+-rw-r--r--  2.0 unx     9028 b- defN 24-Apr-09 21:17 paxml/contrib/gpu/scripts_gpu/tasks.py
+-rw-r--r--  2.0 unx     5411 b- defN 24-Apr-09 21:17 paxml/contrib/gpu/scripts_gpu/tfds_lambada.py
+-rw-r--r--  2.0 unx     7376 b- defN 24-Apr-09 21:17 paxml/contrib/gpu/scripts_gpu/tfds_pile.py
+-rw-r--r--  2.0 unx     4968 b- defN 24-Apr-09 21:17 paxml/experimental/baseline_experiment.py
+-rw-r--r--  2.0 unx     5047 b- defN 24-Apr-09 21:17 paxml/experimental/baseline_experiment_test.py
+-rw-r--r--  2.0 unx     1271 b- defN 24-Apr-09 21:17 paxml/experimental/nested_map_config_helper.py
+-rw-r--r--  2.0 unx     1018 b- defN 24-Apr-09 21:17 paxml/experimental/nested_map_config_helper_test.py
+-rw-r--r--  2.0 unx     4814 b- defN 24-Apr-09 21:17 paxml/ghostnorm/base.py
+-rw-r--r--  2.0 unx     6408 b- defN 24-Apr-09 21:17 paxml/ghostnorm/embedding.py
+-rw-r--r--  2.0 unx    12969 b- defN 24-Apr-09 21:17 paxml/ghostnorm/generic_wrapper.py
+-rw-r--r--  2.0 unx    24887 b- defN 24-Apr-09 21:17 paxml/ghostnorm/layers_test.py
+-rw-r--r--  2.0 unx     5870 b- defN 24-Apr-09 21:17 paxml/ghostnorm/linears.py
+-rw-r--r--  2.0 unx      597 b- defN 24-Apr-09 21:17 paxml/tasks/lm/__init__.py
+-rw-r--r--  2.0 unx    17398 b- defN 24-Apr-09 21:17 paxml/tasks/lm/input_generator.py
+-rw-r--r--  2.0 unx     4084 b- defN 24-Apr-09 21:17 paxml/tasks/lm/input_generator_test.py
+-rw-r--r--  2.0 unx    35790 b- defN 24-Apr-09 21:17 paxml/tasks/lm/model_params.py
+-rw-r--r--  2.0 unx     6610 b- defN 24-Apr-09 21:17 paxml/tasks/lm/params/bert.py
+-rw-r--r--  2.0 unx    30685 b- defN 24-Apr-09 21:17 paxml/tasks/lm/params/c4.py
+-rw-r--r--  2.0 unx     7395 b- defN 24-Apr-09 21:17 paxml/tasks/lm/params/c4_multislice.py
+-rw-r--r--  2.0 unx     3647 b- defN 24-Apr-09 21:17 paxml/tasks/lm/params/c4_test.py
+-rw-r--r--  2.0 unx    10423 b- defN 24-Apr-09 21:17 paxml/tasks/lm/params/lm_cloud.py
+-rw-r--r--  2.0 unx    17732 b- defN 24-Apr-09 21:17 paxml/tasks/lm/params/nvidia.py
+-rw-r--r--  2.0 unx     3040 b- defN 24-Apr-09 21:17 paxml/tasks/lm/params/optimal_scaling.py
+-rw-r--r--  2.0 unx     1124 b- defN 24-Apr-09 21:17 paxml/tasks/test/synthetic.py
+-rw-r--r--  2.0 unx     5197 b- defN 24-Apr-09 21:17 paxml/tasks/vision/input_generator.py
+-rw-r--r--  2.0 unx     1611 b- defN 24-Apr-09 21:17 paxml/tasks/vision/input_generator_test.py
+-rw-r--r--  2.0 unx     7002 b- defN 24-Apr-09 21:17 paxml/tasks/vision/resnet_preprocessing.py
+-rw-r--r--  2.0 unx     9055 b- defN 24-Apr-09 21:17 paxml/tasks/vision/params/imagenet_resnets.py
+-rw-r--r--  2.0 unx     8285 b- defN 24-Apr-09 21:17 paxml/tasks/vision/params/mnist.py
+-rw-r--r--  2.0 unx     1319 b- defN 24-Apr-09 21:17 paxml/tools/dump_hparams.py
+-rw-r--r--  2.0 unx     7098 b- defN 24-Apr-09 21:17 paxml/tools/dump_hparams_lib.py
+-rw-r--r--  2.0 unx     1590 b- defN 24-Apr-09 21:17 paxml/tools/dump_input_specs.py
+-rw-r--r--  2.0 unx     3755 b- defN 24-Apr-09 21:17 paxml/tools/dump_input_specs_lib.py
+-rw-r--r--  2.0 unx    11008 b- defN 24-Apr-09 21:17 paxml/tools/model_analysis.py
+-rw-r--r--  2.0 unx     1000 b- defN 24-Apr-09 21:17 paxml/tools/validate_config.py
+-rw-r--r--  2.0 unx     4976 b- defN 24-Apr-09 21:17 paxml/tools/validate_config_lib.py
+-rw-r--r--  2.0 unx    28469 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen.py
+-rw-r--r--  2.0 unx     5045 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen_external_init_checkpoint_fns.py
+-rw-r--r--  2.0 unx     3738 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen_external_init_checkpoint_fns_test.py
+-rw-r--r--  2.0 unx     3303 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen_highlevel_parameterization.py
+-rw-r--r--  2.0 unx     2268 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen_highlevel_parameterization_test.py
+-rw-r--r--  2.0 unx     1420 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen_pax_code_ir.py
+-rw-r--r--  2.0 unx     4723 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen_sharding.py
+-rw-r--r--  2.0 unx    31202 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen_test.py
+-rw-r--r--  2.0 unx     3488 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen_tracer.py
+-rw-r--r--  2.0 unx     5917 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/codegen_tracer_test.py
+-rw-r--r--  2.0 unx     8857 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/config_normalization.py
+-rw-r--r--  2.0 unx     5526 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/config_normalization_test.py
+-rw-r--r--  2.0 unx     1741 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/convert_seqio_task_objects.py
+-rw-r--r--  2.0 unx     1344 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/convert_seqio_task_objects_test.py
+-rw-r--r--  2.0 unx     2437 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/graphviz_utils.py
+-rw-r--r--  2.0 unx      987 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/graphviz_utils_test.py
+-rw-r--r--  2.0 unx     3043 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/make_parameterized_experiment.py
+-rw-r--r--  2.0 unx     2862 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/remove_sharding.py
+-rw-r--r--  2.0 unx     3709 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/remove_sharding_test.py
+-rw-r--r--  2.0 unx     6062 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/test_fixtures.py
+-rw-r--r--  2.0 unx     2486 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/unshare_sharding.py
+-rw-r--r--  2.0 unx     4813 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/unshare_sharding_test.py
+-rw-r--r--  2.0 unx     1326 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/wrap_nested_maps.py
+-rw-r--r--  2.0 unx     1759 b- defN 24-Apr-09 21:17 paxml/tools/fiddle/wrap_nested_maps_test.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Apr-09 21:24 paxml-1.4.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1336 b- defN 24-Apr-09 21:24 paxml-1.4.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-09 21:24 paxml-1.4.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        6 b- defN 24-Apr-09 21:24 paxml-1.4.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    11846 b- defN 24-Apr-09 21:24 paxml-1.4.0.dist-info/RECORD
+134 files, 1643431 bytes uncompressed, 421532 bytes compressed:  74.4%
```

## zipnote {}

```diff
@@ -75,14 +75,17 @@
 
 Filename: paxml/experiment_registry_test.py
 Comment: 
 
 Filename: paxml/experiment_utils.py
 Comment: 
 
+Filename: paxml/first_result_metric_callback.py
+Comment: 
+
 Filename: paxml/host_callback.py
 Comment: 
 
 Filename: paxml/host_callback_test.py
 Comment: 
 
 Filename: paxml/io_utils.py
@@ -183,20 +186,29 @@
 
 Filename: paxml/xla_passthrough_test.py
 Comment: 
 
 Filename: paxml/contrib/gpu/scripts_gpu/configs.py
 Comment: 
 
+Filename: paxml/contrib/gpu/scripts_gpu/download_boolq.py
+Comment: 
+
 Filename: paxml/contrib/gpu/scripts_gpu/download_lambada.py
 Comment: 
 
 Filename: paxml/contrib/gpu/scripts_gpu/download_the_pile.py
 Comment: 
 
+Filename: paxml/contrib/gpu/scripts_gpu/llama_utils.py
+Comment: 
+
+Filename: paxml/contrib/gpu/scripts_gpu/saxml_layers.py
+Comment: 
+
 Filename: paxml/contrib/gpu/scripts_gpu/tasks.py
 Comment: 
 
 Filename: paxml/contrib/gpu/scripts_gpu/tfds_lambada.py
 Comment: 
 
 Filename: paxml/contrib/gpu/scripts_gpu/tfds_pile.py
@@ -369,23 +381,23 @@
 
 Filename: paxml/tools/fiddle/wrap_nested_maps.py
 Comment: 
 
 Filename: paxml/tools/fiddle/wrap_nested_maps_test.py
 Comment: 
 
-Filename: paxml-1.3.1.dist-info/LICENSE
+Filename: paxml-1.4.0.dist-info/LICENSE
 Comment: 
 
-Filename: paxml-1.3.1.dist-info/METADATA
+Filename: paxml-1.4.0.dist-info/METADATA
 Comment: 
 
-Filename: paxml-1.3.1.dist-info/WHEEL
+Filename: paxml-1.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: paxml-1.3.1.dist-info/top_level.txt
+Filename: paxml-1.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: paxml-1.3.1.dist-info/RECORD
+Filename: paxml-1.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## paxml/automl_test.py

```diff
@@ -274,39 +274,45 @@
 
 
 class SearchHParamsTest(absltest.TestCase):
   """Tests for search hyperparameters."""
 
   def test_hyperparameter_tuning(self):
     p = automl.hyperparameter_tuning(automl.Metric.eval('accuracy'))
+    assert p.search_algorithm is not None
+    assert p.search_reward is not None
     # Check algorithm cls for hyperparameter tuning.
     self.assertIs(p.search_algorithm.cls, automl.Sweeping)
     self.assertIs(p.search_reward.cls, automl.SingleObjective)
     self.assertEqual(p.search_reward.metric, automl.Metric.eval('accuracy'))
     self.assertEqual(p.search_reward.goal, 'maximize')
     self.assertIsNone(p.search_reward.reward_for_nan)
     self.assertEqual(p.max_num_trials, 100)
     self.assertIsNone(p.errors_to_skip)
 
   def test_neural_architecture_search_single_objective(self):
     p = automl.neural_architecture_search(automl.Metric.eval('accuracy'))
+    assert p.search_algorithm is not None
+    assert p.search_reward is not None
     self.assertIs(p.search_algorithm.cls, automl.RegularizedEvolution)
     self.assertIs(p.search_reward.cls, automl.SingleObjective)
     self.assertEqual(p.search_reward.metric, automl.Metric.eval('accuracy'))
     self.assertIsNone(p.search_reward.reward_for_nan)
     self.assertEqual(p.max_num_trials, 10000)
     self.assertIsNone(p.errors_to_skip)
 
   def test_neural_architecture_search_multi_objective(self):
     p = automl.neural_architecture_search([
         automl.Metric.eval('accuracy'),
         automl.Metric.train_steps_per_second()
     ],
                                           150,
                                           max_num_trials=6000)
+    assert p.search_algorithm is not None
+    assert p.search_reward is not None
     self.assertIs(p.search_algorithm.cls, automl.RegularizedEvolution)
     self.assertIs(p.search_reward.cls, automl.MultiObjective)
     self.assertEqual(p.search_reward.metrics, [
         automl.Metric.eval('accuracy'),
         automl.Metric.train_steps_per_second()
     ])
     self.assertEqual(p.search_reward.aggregator_tpl.cost_objective, 150)
@@ -315,30 +321,33 @@
   def test_neural_architecture_search_multi_objective_aggregators(self):
     p = automl.neural_architecture_search([
         automl.Metric.eval('accuracy'),
         automl.Metric.train_steps_per_second()
     ],
                                           150,
                                           reward_type='tunas').search_reward
+    assert p is not None
     self.assertTrue(
         issubclass(fdl.get_callable(p.aggregator_tpl), automl.TunasAbsolute))
     p = automl.neural_architecture_search([
         automl.Metric.eval('accuracy'),
         automl.Metric.train_steps_per_second()
     ],
                                           150,
                                           reward_type='mnas_hard').search_reward
+    assert p is not None
     self.assertTrue(
         issubclass(fdl.get_callable(p.aggregator_tpl), automl.MnasHard))
     p = automl.neural_architecture_search([
         automl.Metric.eval('accuracy'),
         automl.Metric.train_steps_per_second()
     ],
                                           150,
                                           reward_type='mnas_soft').search_reward
+    assert p is not None
     self.assertTrue(
         issubclass(fdl.get_callable(p.aggregator_tpl), automl.MnasSoft))
     with self.assertRaisesRegex(ValueError, 'Unsupported reward type'):
       automl.neural_architecture_search([
           automl.Metric.eval('accuracy'),
           automl.Metric.train_steps_per_second()
       ],
```

## paxml/base_inference_runner_test.py

```diff
@@ -13,15 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for base_inference_runner."""
 
 from __future__ import annotations
 
-from typing import Any
+from typing import Any, Sequence
 
 from absl.testing import absltest
 import jax
 import numpy as np
 from paxml import base_inference_runner
 from paxml import train_states
 from praxis import base_hyperparams
@@ -68,21 +68,23 @@
     )
     infer_runner = infer_runner_p.Instantiate(model=None)
 
     serialized_outputs = infer_runner.serialize_outputs(
         # Pass dummy values to all 4 arguments of infer().
         infer_runner.infer(*([None] * 4)))
 
-    expected_outputs: list[NestedMap] = py_utils.tree_unstack(dummy_output, 0)
+    expected_outputs: Sequence[NestedMap] = py_utils.tree_unstack(
+        dummy_output, 0
+    )
     self.assertEqual(len(serialized_outputs), len(expected_outputs))
 
     features_dict = tfds.features.FeaturesDict(dummy_schema)
     for serialized, expected in zip(serialized_outputs, expected_outputs):
       output = features_dict.deserialize_example(serialized)
-      output_np = jax.tree_map(lambda x: x.numpy(), output)
+      output_np = jax.tree.map(lambda x: x.numpy(), output)
 
       for output_leaf, expected_leaf in zip(
           jax.tree_util.tree_leaves(output_np),
           jax.tree_util.tree_leaves(expected)):
         self.assertArraysEqual(output_leaf, expected_leaf)
```

## paxml/checkpoint_creators.py

```diff
@@ -355,15 +355,15 @@
           return jax.sharding.PartitionSpec(None)
         else:
           return jax.sharding.PartitionSpec()
 
       global_mesh = jax.sharding.Mesh(
           np.array(jax.devices()), axis_names=('x',)
       )
-      fully_replicated_state_specs = jax.tree_map(
+      fully_replicated_state_specs = jax.tree.map(
           _get_spec, train_state_global_shapes
       )
       restore_args = {
           'specs': fully_replicated_state_specs,
           'mesh': global_mesh,
       }
     restored_state = self.checkpoint_manager.restore(
@@ -372,21 +372,21 @@
         train_state_unpadded_shape_dtype_struct=train_state_unpadded_shape_dtype_struct,
         train_input_pipeline=train_input_pipeline,
         restore_kwargs=restore_args,
     )
     if not py_utils.pmap_use_tensorstore():
       return restored_state
     if self._checkpoint_type == CheckpointType.PERSISTENCE:
-      return jax.tree_map(
+      return jax.tree.map(
           py_utils.convert_fully_replicated_array_to_pmap_array,
           restored_state,
       )
     # model_states is jax.Array; we convert back to DA or jax.Array with
     # single device sharding for pmap.
-    return jax.tree_map(lambda x: x.addressable_data(0), restored_state)
+    return jax.tree.map(lambda x: x.addressable_data(0), restored_state)
 
   # TODO(laigd): merge this with _PmapEvalCheckpointer.get_model_states().
   def get_model_states(
       self,
       partitioner: partitioning.Partitioner,
       metadata: trainer_lib.TrainStateMetadata,
       root_prng_key: PRNGKey,
@@ -463,29 +463,29 @@
       return
 
     with py_utils.timeit() as save_period:
       if py_utils.pmap_use_tensorstore():
         logging.info(
             'Saving a ckpt at %sstep: %d', 'final ' if is_final else '', step_i
         )
-        fully_replicated_gda_train_state = jax.tree_map(
+        fully_replicated_gda_train_state = jax.tree.map(
             py_utils.convert_host_local_array_to_global_array,
             partitioned_train_state,
         )
         self._save_with_args(
             step_i,
             train_state=fully_replicated_gda_train_state,
             train_state_unpadded_shape_dtype_struct=(
                 train_state_unpadded_shape_dtype_struct
             ),
             train_input_pipeline=train_input_pipeline,
             force=is_final,
         )
       else:
-        unreplicated_train_state = jax.tree_map(
+        unreplicated_train_state = jax.tree.map(
             lambda x: x[0], partitioned_train_state
         )
         self._save_with_args(
             step_i,
             train_state=unreplicated_train_state,
             train_state_unpadded_shape_dtype_struct=(
                 train_state_unpadded_shape_dtype_struct
```

## paxml/checkpoint_managers.py

```diff
@@ -12,15 +12,14 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Module to manage checkpoint metadata and automatic checkpoint deletion."""
 
 import dataclasses
-import os
 import typing
 from typing import Any, Sequence, Union
 
 from absl import logging
 from etils import epath
 import orbax.checkpoint as ocp
 from paxml import checkpoint_metadata
@@ -96,38 +95,17 @@
         tensorstore_use_ocdbt=tensorstore_use_ocdbt,
     )
     items.update({METADATA_ITEM_NAME: metadata})
 
   return items
 
 
-def _is_legacy_flax_checkpoint(path: epath.Path) -> bool:
-  """Returns whether the checkpoint is a legacy Flax checkpoint format.
-
-  Old-format Flax checkpoint conforming to
-  'path/to/dir/checkpoints/checkpoint_100'.
-  Contrast with 'standard' old-format Flax checkpoint conforming to
-  'path/to/dir/checkpoints/checkpoint_100/checkpoint'.
-  The former is not considered a valid checkpoint by Orbax because it is not a
-  directory. It thus requires special handling.
-
-  Args:
-    path: the checkpoint path.
-
-  Returns:
-    Boolean indicating whether the path is legacy Flax checkpoint or not.
-  """
-  return checkpoint_paths.is_checkpoint_asset(path) and (
-      not checkpoint_paths.is_tmp_checkpoint_asset(path) and path.is_file()
-  )
-
-
 def _has_digit_step_subdirectory(directory) -> bool:
   """Indicates whether the checkpoints have digit-like step subdirectories."""
-  return False  # mapped to internal digit step impl.
+  return checkpoint_paths.is_tfhub_dir(directory)
 
 
 @dataclasses.dataclass
 class CheckpointManagerOptions(ocp.CheckpointManagerOptions):
   """Options for constructing OrbaxCheckpointManager.
 
   See superclass.
@@ -330,24 +308,24 @@
                   'restoring and saving future checkpoints.'
               ),
               version,
               self._version,
           )
           self._version = version
 
-    super().__init__(directory, checkpointers=checkpointers, options=options)
+    options = options or CheckpointManagerOptions()
     # Set to 1 if not provided or set to 0.
-    self._options.save_interval_steps = self._options.save_interval_steps or 1
-    self._options.step_prefix = checkpoint_paths.checkpoint_prefix(
-        self._checkpoint_type
-    )
-    self._options.step_format_fixed_length = (
-        checkpoint_paths.checkpoint_name_fixed_length(self._checkpoint_type)
+    options.save_interval_steps = options.save_interval_steps or 1
+    options.step_name_format = checkpoint_paths.PaxStepNameFormat(
+        checkpoint_type=self._checkpoint_type,
+        use_digit_step_subdirectory=self._use_digit_step_subdirectory,
     )
 
+    super().__init__(directory, checkpointers=checkpointers, options=options)
+
     if self.version < 1:
       composite_handler = typing.cast(
           ocp.CompositeCheckpointHandler, self._checkpointer._handler  # pylint: disable=protected-access
       )
       original_state_handler = composite_handler._known_handlers[  # pylint: disable=protected-access
           STATE_ITEM_NAME
       ]
@@ -355,14 +333,15 @@
           **{STATE_ITEM_NAME: original_state_handler}
       )
       is_legacy_flax_checkpoint = (
           checkpointers[STATE_ITEM_NAME].__class__.__name__
           == 'FlaxCheckpointer'
       )
       if ocp.checkpoint_manager.is_async_checkpointer(self._checkpointer):
+        assert hasattr(self._checkpointer, '_async_manager')  # Hint for pytype
         self._checkpointer = _AsyncCheckpointer(
             handler=handler,
             is_legacy_flax_checkpoint=is_legacy_flax_checkpoint,
             timeout_secs=self._checkpointer._async_manager._timeout_secs,  # pylint: disable=protected-access
         )
       else:
         self._checkpointer = _Checkpointer(
@@ -374,39 +353,36 @@
   def version(self) -> float:
     return self._version
 
   def all_steps(self, read: bool = False) -> Sequence[int]:
     steps = list(super().all_steps(read=read))
     if read:
       for path in self.directory.iterdir():
-        if _is_legacy_flax_checkpoint(path):
+        if checkpoint_paths.is_legacy_flax_checkpoint(path):
           steps.append(checkpoint_paths.get_step_from_checkpoint_asset(path))
     return steps
 
   def any_step(self) -> int | None:
     """Returns any step tracked by the checkpoint manager.
 
     Returns:
       A step (integer) or None.
     """
     any_step = ocp.utils.any_checkpoint_step(self.directory)
     if any_step is not None:
       return any_step
 
     for path in self.directory.iterdir():
-      if _is_legacy_flax_checkpoint(path):
+      if checkpoint_paths.is_legacy_flax_checkpoint(path):
         return checkpoint_paths.get_step_from_checkpoint_asset(path)
     return None
 
   def _checkpoint_name(self, step: int) -> str:
-    return checkpoint_paths.checkpoint_name(
-        step,
-        checkpoint_type=self._checkpoint_type,
-        use_digit_step_subdirectory=self._use_digit_step_subdirectory,
-    )
+    assert self._options.step_name_format is not None  # Hint for pytype
+    return self._options.step_name_format.build_name(step)
 
   def should_save(self, step: int) -> bool:
     """Indicates whether there is a need to save a checkpoint."""
     if self._use_digit_step_subdirectory:
       raise NotImplementedError(
           'Checkpoints with digit step subdirectories do not support the '
           'saving mode.'
@@ -440,27 +416,14 @@
     if self._use_digit_step_subdirectory:
       raise NotImplementedError(
           'Checkpoints with digit step subdirectories do not support the '
           'saving mode.'
       )
     return super().save(*args, **kwargs)
 
-  def _get_save_directory(
-      self,
-      step: int,
-      directory: epath.Path,
-  ) -> epath.Path:
-    """Returns the standardized path to a save directory for a single item."""
-    return checkpoint_paths.make_checkpoint_step_dir(
-        directory,
-        step,
-        checkpoint_type=self._checkpoint_type,
-        use_digit_step_subdirectory=self._use_digit_step_subdirectory,
-    )
-
 
 class OrbaxCheckpointManager:
   """Wrapper class for overridden _CheckpointManagerImpl."""
 
   def __init__(
       self,
       directory: epath.Path,
```

## paxml/checkpoint_managers_test.py

```diff
@@ -26,14 +26,15 @@
 
 from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 from etils import epath
 import jax
 from jax.experimental import multihost_utils
+import jax.numpy as jnp
 from jax.sharding import Mesh
 import numpy as np
 import optax
 import orbax.checkpoint as ocp
 from paxml import checkpoint_managers
 from paxml import checkpoint_paths
 from paxml import checkpoint_types
@@ -47,52 +48,55 @@
 
 FLAGS = flags.FLAGS
 CheckpointType = checkpoint_types.CheckpointType
 CHECKPOINT_PREFIX = checkpoint_paths.CHECKPOINT_PREFIX
 TrainState = train_states.TrainState
 
 
-@contextlib.contextmanager
-def ocdbt_checkpoint_context(use_ocdbt: bool, ts_context: Any):
-  """Use OCDBT driver within context."""
-  original_registry = list(
-      ocp.type_handlers._TYPE_REGISTRY  # pylint: disable=protected-access
-  )
-  if use_ocdbt:
-    ocp.type_handlers.register_standard_handlers_with_options(
-        use_ocdbt=use_ocdbt, ts_context=ts_context
-    )
-  try:
-    yield
-  finally:
-    ocp.type_handlers._TYPE_REGISTRY = (  # pylint: disable=protected-access
-        original_registry
-    )
+# TODO(b/330585279): remove the polyfill when releases have stabilized.
+if hasattr(ocp.test_utils, 'ocdbt_checkpoint_context'):
+  ocdbt_checkpoint_context = ocp.test_utils.ocdbt_checkpoint_context
+else:
+
+  @contextlib.contextmanager
+  def ocdbt_checkpoint_context(use_ocdbt: bool, ts_context: Any):
+    """Use OCDBT driver within context."""
+    original_registry = list(
+        ocp.type_handlers._TYPE_REGISTRY  # pylint: disable=protected-access
+    )
+    if use_ocdbt:
+      ocp.type_handlers.register_standard_handlers_with_options(
+          use_ocdbt=use_ocdbt, ts_context=ts_context
+      )
+    try:
+      yield
+    finally:
+      ocp.type_handlers._TYPE_REGISTRY = (  # pylint: disable=protected-access
+          original_registry
+      )
 
 
 def _expected_checkpoint_filenames(
     steps: list[int], checkpoint_type: CheckpointType = CheckpointType.GDA
-):
+) -> list[epath.Path]:
   """Returns checkpoint basenames corresponding to all the `steps`."""
   results = []
   for step in steps:
     if checkpoint_type == CheckpointType.FLAX:
       name = f'{CHECKPOINT_PREFIX}{step}'
     else:
       name = f'{CHECKPOINT_PREFIX}{step:08d}'
-    results.append(name)
+    results.append(epath.Path(name))
   return results
 
 
-def _actual_checkpoint_filenames(directory: str) -> list[str]:
+def _actual_checkpoint_filenames(directory: epath.Path) -> list[epath.Path]:
   return [
-      os.path.basename(v)
-      for v in tf.io.gfile.glob(
-          os.path.join(directory, f'{CHECKPOINT_PREFIX}*')
-      )
+      epath.Path(os.path.basename(v))
+      for v in tf.io.gfile.glob(str(directory / f'{CHECKPOINT_PREFIX}*'))
   ]
 
 
 def create_train_state(step: int = 0):
   mdl_vars = ocp.test_utils.setup_pytree()
   global_mesh = Mesh(np.asarray(jax.devices()), ('x',))
   axes = jax.sharding.PartitionSpec(
@@ -105,15 +109,15 @@
           mesh_axes=axes,
       ),
       mdl_vars,
   )
   opt_states = [mdl_vars]
   extra_state = [mdl_vars]
   train_state = TrainState(
-      step=step,
+      step=jnp.array(step),
       mdl_vars=mdl_vars,
       opt_states=opt_states,
       extra_state=extra_state,
   )
 
   def _create_sharded_array(x):
     return ocp.test_utils.create_sharded_array(x, global_mesh, axes)
@@ -170,15 +174,17 @@
     return d
 
 
 class CheckpointManagerTest(parameterized.TestCase):
 
   def setUp(self):
     super().setUp()
-    self.directory = self.create_tempdir(name='checkpointing_test').full_path
+    self.directory = epath.Path(
+        self.create_tempdir(name='checkpointing_test').full_path
+    )
     (
         self.global_mesh,
         self.state_specs,
         self.train_state,
         self.train_state_unpadded_shape_dtype_struct,
     ) = create_train_state()
 
@@ -307,14 +313,15 @@
         train_input_pipeline,
         train_state_unpadded_shape_dtype_struct=(
             self.train_state_unpadded_shape_dtype_struct
         ),
     )
     ocp.test_utils.print_directory(checkpoint_manager.directory)
     if use_train_input:
+      assert train_input_pipeline is not None
       train_input_pipeline.reset()
     expected = self.train_state
     if checkpoint_type == CheckpointType.FLAX:
       expected = jax.tree_util.tree_map(
           lambda x: np.asarray(x.addressable_data(0)),
           expected,
       )
@@ -762,19 +769,19 @@
           train_state_unpadded_shape_dtype_struct=(
               self.train_state_unpadded_shape_dtype_struct
           ),
       )
 
     self.assertSameElements(
         _expected_checkpoint_filenames([0, 1], checkpoint_type=checkpoint_type),
-        _actual_checkpoint_filenames(os.path.join(self.directory, 'archive')),
+        _actual_checkpoint_filenames(self.directory / 'archive'),
     )
     self.assertSameElements(
         _expected_checkpoint_filenames([2, 3], checkpoint_type=checkpoint_type),
-        _actual_checkpoint_filenames(os.path.join(self.directory)),
+        _actual_checkpoint_filenames(self.directory),
     )
     self.assertIn('archive', tf.io.gfile.listdir(self.directory))
     self.assertSameElements([2, 3], checkpoint_manager.all_steps())
 
   @parameterized.parameters((CheckpointType.GDA,), (CheckpointType.FLAX,))
   def test_reinitialize(self, checkpoint_type):
     options = checkpoint_managers.CheckpointManagerOptions(max_to_keep=2)
@@ -1026,18 +1033,15 @@
         self.train_state,
         train_input_pipeline,
         train_state_unpadded_shape_dtype_struct=(
             self.train_state_unpadded_shape_dtype_struct
         ),
     )
     if remove_checkpoint_prefix:
-      os.rename(
-          os.path.join(self.directory, 'checkpoint_00000000'),
-          os.path.join(self.directory, '0'),
-      )
+      (self.directory / 'checkpoint_00000000').rename(self.directory / '0'),
     expected = self.train_state
     train_state_global_shapes = jax.eval_shape(lambda x: x, self.train_state)
     with mock.patch.object(
         checkpoint_managers,
         '_has_digit_step_subdirectory',
         return_value=remove_checkpoint_prefix,
     ) as mock_has_digit:
```

## paxml/checkpoint_paths.py

```diff
@@ -11,16 +11,18 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Shared checkpointing utility functions."""
 
+import dataclasses
+import os
 import re
-from typing import Any
+from typing import Any, Iterator
 
 from absl import logging
 from etils import epath
 from jax.experimental import multihost_utils
 import numpy as np
 import orbax.checkpoint as ocp
 from paxml import checkpoint_types
@@ -148,24 +150,25 @@
     checkpoint_dir: The base directory from where to retrieve checkpoints.
 
   Returns:
     Path to the latest checkpoint or None if there is no checkpoint.
   """
   checkpoint_dir = epath.Path(checkpoint_dir)
   if not checkpoint_dir.exists():
-    logging.info('Checkpoint dir \'%s\' does not exist.', checkpoint_dir)
+    logging.info("Checkpoint dir '%s' does not exist.", checkpoint_dir)
     return None
   checkpoint_assets = [
       v
       for v in checkpoint_dir.iterdir()
       if is_checkpoint_asset(v) and not is_tmp_checkpoint_asset(v)
   ]
   if not checkpoint_assets:
     logging.info(
-        'No non-temporary checkpoints found in dir: \'%s\'', checkpoint_dir)
+        "No non-temporary checkpoints found in dir: '%s'", checkpoint_dir
+    )
     return None
   checkpoint_assets = sorted(
       checkpoint_assets, key=get_step_from_checkpoint_asset
   )
   return checkpoint_dir / checkpoint_assets[-1]
 
 
@@ -205,15 +208,15 @@
   Args:
     checkpoint_dir: The base directory from where to retrieve checkpoints.
 
   Returns:
     The latest checkpoint step as an integer or None if no checkpoint is found.
   """
   if not checkpoint_dir.exists():
-    logging.info('Checkpoint dir \'%s\' does not exist.', checkpoint_dir)
+    logging.info("Checkpoint dir '%s' does not exist.", checkpoint_dir)
     checkpoint_step = -1
   else:
     latest_checkpoint_path = latest_checkpoint_if_exists(checkpoint_dir)
     if latest_checkpoint_path is None:
       checkpoint_step = -1
     else:
       checkpoint_step = get_step_from_checkpoint_asset(latest_checkpoint_path)
@@ -249,14 +252,101 @@
   """
   step = retrieve_latest_checkpoint_step_if_exists(checkpoint_dir)
   if step is None:
     _raise_checkpoint_missing_error(checkpoint_dir)
   return step
 
 
+def is_tfhub_dir(directory: epath.Path) -> bool:
+  """Returns whether the given directory is a TFHub directory."""
+  return False  # mapped to internal impl.
+
+
 def _raise_checkpoint_missing_error(checkpoint_dir: epath.Path):
   """Raise checkpoint missing error with helpful message."""
   if not checkpoint_dir.exists():
     raise ValueError(f'{checkpoint_dir=!r} does not exist')
   raise ValueError(
       f'No checkpoints were found in directory {checkpoint_dir=!r}'
   )
+
+
+def is_legacy_flax_checkpoint(path: epath.Path) -> bool:
+  """Returns whether the checkpoint is a legacy Flax checkpoint format.
+
+  Old-format Flax checkpoint conforming to
+  'path/to/dir/checkpoints/checkpoint_100'.
+  Contrast with 'standard' old-format Flax checkpoint conforming to
+  'path/to/dir/checkpoints/checkpoint_100/checkpoint'.
+  The former is not considered a valid checkpoint by Orbax because it is not a
+  directory. It thus requires special handling.
+
+  Args:
+    path: the checkpoint path.
+
+  Returns:
+    Boolean indicating whether the path is legacy Flax checkpoint or not.
+  """
+  return is_checkpoint_asset(path) and (
+      not is_tmp_checkpoint_asset(path) and path.is_file()
+  )
+
+
+@dataclasses.dataclass(frozen=True)
+class PaxStepNameFormat(ocp.step.NameFormat):
+  """Name format for Pax checkpoints.
+
+  Example names:
+
+  - use_digit_step_subdirectory=T  step=7  => 7
+
+  For use_digit_step_subdirectory=F:
+  - checkpoint_type=`FLAX` step=7 => `checkpoint_7`
+
+  - checkpoint_type != `FLAX` step=1234 => `checkpoint_00001234`
+
+  Attributes:
+    checkpoint_type: Enum `CheckpointType` for the type of checkpoint format.
+    use_digit_step_subdirectory: If True, then step number itself is used as a
+      name. Otherwise, the name is built with a fixed string prefix and optional
+      zero padding.
+  """
+
+  checkpoint_type: CheckpointType = CheckpointType.UNSPECIFIED
+  use_digit_step_subdirectory: bool = False
+
+  def build_name(self, step: int) -> str:
+    """Returns `step` name using NameFormat attributes."""
+    return checkpoint_name(
+        step,
+        checkpoint_type=self.checkpoint_type,
+        use_digit_step_subdirectory=self.use_digit_step_subdirectory,
+    )
+
+  def build_metadata(
+      self, step_path: epath.Path, step: int | None = None
+  ) -> ocp.step.Metadata | None:
+    """Returns metadata for given `step_path` if it is valid or None."""
+    if is_tmp_checkpoint_asset(step_path):
+      return None
+
+    if not is_checkpoint_asset(step_path):
+      return None
+
+    if step is not None:
+      if not step_path.exists():
+        return None
+
+    step = step or get_step_from_checkpoint_asset(step_path)
+    return ocp.step.Metadata(step=step, path=step_path)
+
+  def find_metadata(
+      self, base_path: epath.PathLike, step: int
+  ) -> ocp.step.Metadata | None:
+    """Returns metadata for given `base_path` and `step` or None."""
+    step_path = ocp.step.build_step_path(base_path, self, step)
+    return self.build_metadata(step_path, step=step)
+
+  def find_all(self, base_path: epath.PathLike) -> Iterator[ocp.step.Metadata]:
+    """Returns metadata of all steps (ignores current NameFormat attributes)."""
+    step_paths = epath.Path(base_path).iterdir()
+    return ocp.step.build_step_metadatas(step_paths, self.build_metadata)
```

## paxml/checkpoints.py

```diff
@@ -353,22 +353,22 @@
     - flattened_nested_names: A flattened version of the nested names, where all
       entries corresponding to MaskedNode have been filtered out.
     - out: Either None when the input state_specs is None or the flattened
       version of the state_specs, where all entries corresponding to MaskedNode
       instances have been filtered out.
   """
   # This replaces MaskedNode instances by None values ...
-  train_state_none = jax.tree_map(
+  train_state_none = jax.tree.map(
       _masked_node_to_none,
       train_state,
       train_state,
       is_leaf=py_utils.is_optax_masked_node,
   )
   if state_specs is not None:
-    state_specs_none = jax.tree_map(
+    state_specs_none = jax.tree.map(
         _masked_node_to_none,
         train_state,
         state_specs,
         is_leaf=py_utils.is_optax_masked_node,
     )
   # ... that are filtered out when calling jax.tree_util.tree_flatten() here.
   flattened_train_state, _ = jax.tree_util.tree_flatten(train_state_none)
@@ -575,15 +575,15 @@
           'Found `None` for `state_specs` during restoration. If not restoring'
           ' using PMAP and `pmap_use_tensorstore`, this may indicate an error.'
       )
       restore_args = jax.tree_util.tree_map(
           _create_restore_args, reference_train_state
       )
     else:
-      restore_args = jax.tree_map(
+      restore_args = jax.tree.map(
           _create_sharded_restore_args,
           reference_train_state,
           reference_state_specs,
       )
     restored_train_state = super().restore(
         directory,
         item=reference_train_state,
```

## paxml/decode_programs.py

```diff
@@ -393,27 +393,27 @@
           'Finished decoding input batch %d for %s', step_num, self._name
       )
 
       if jax.process_index() == 0:
         # Copy the tensor from device memory to ram, since accumulating such
         # tensor on devices may cause HBM OOM, when
         # task_p.train.summary_accumulate_interval_steps is set.
-        weighted_scalars = jax.tree_map(np.array, weighted_scalars)
+        weighted_scalars = jax.tree.map(np.array, weighted_scalars)
         decode_metrics.store(weighted_scalars)
 
         xla_passthrough.merge_back_xla_unsupported_batch(
             per_example_out, tpu_unsupported_batch
         )
 
         # Run `process_decode_out` on CPU device as its implementation
         # is not expected to be JIT friendly. Since we keep track of
         # its outputs, we also don't want on-device allocation as
         # would eventually lead to HBM OOM.
         with jax.default_device(jax.local_devices(backend='cpu')[0]):
-          per_example_out = jax.tree_map(np.asarray, per_example_out)
+          per_example_out = jax.tree.map(np.asarray, per_example_out)
           process_weighted_scalars, processed_out, processed_metric_updates = (
               self._task.model.process_decode_out(
                   self.decode_input, per_example_out
               )
           )
         if processed_out:
           processed_out = seqio_input.maybe_update_decode_output_keys(
```

## paxml/eval_lib.py

```diff
@@ -17,14 +17,15 @@
 
 import abc
 import contextlib
 import dataclasses
 import functools
 import gc
 import itertools
+import os
 import time
 import typing
 from typing import Any, Sequence
 
 from absl import logging
 from etils import epath
 import jax
@@ -820,18 +821,24 @@
     continuous_decode: bool,
     eval_runner: _EvalRunner,
     partitioner: partitioning.Partitioner,
     decode_output_pickle: bool = True,
     checkpoint_path: epath.Path | None = None,
     enable_summary_writer: bool = True,
 ):
-  step_prefix = checkpoint_paths.checkpoint_prefix(checkpointer.checkpoint_type)
-  step_format_fixed_length = checkpoint_paths.checkpoint_name_fixed_length(
-      checkpointer.checkpoint_type
-  )
+  if checkpoint_paths.is_tfhub_dir(checkpointer.restore_checkpoint_dir):
+    step_prefix = None
+    step_format_fixed_length = None
+  else:
+    step_prefix = checkpoint_paths.checkpoint_prefix(
+        checkpointer.checkpoint_type
+    )
+    step_format_fixed_length = checkpoint_paths.checkpoint_name_fixed_length(
+        checkpointer.checkpoint_type
+    )
   # If preemption happened during evaluation, some checkpoints may be locked.
   orbax_checkpoint_utils.unlock_existing_checkpoints(
       checkpointer.restore_checkpoint_dir,
       step_prefix=step_prefix,
       step_format_fixed_length=step_format_fixed_length,
   )
   # Retrieve last step from the TrainState directly in case new checkpoints
@@ -1072,15 +1079,15 @@
       outputs = infer_pmap_step(
           replicated_model_states,
           output_seeds,
           partitioner.preprocess_inputs(input_gen, batch, None),
       )
       # Get first device's output since it's been replicated by all-gather
       outputs = py_utils.maybe_unreplicate_for_fully_replicated(outputs)
-      outputs_cpu = jax.tree_map(np.asarray, outputs)
+      outputs_cpu = jax.tree.map(np.asarray, outputs)
 
       if jax.process_index() == 0:
         serialized_outputs = task.inference_runner.serialize_outputs(
             outputs_cpu
         )
         # fire-and-forget writing
         writer.write(serialized_outputs)
```

## paxml/experiment_imports_test_helper.py

```diff
@@ -76,16 +76,17 @@
           'initialization of the training pipeline.')
       input_specs = input_specs_provider.get_input_specs()
       model: base_model.BaseModel = task.model  # pytype: disable=attribute-error
       # TODO(pax-dev): Add better/cleaner API to identify pmap vs. pjit models
       # (and check for dcn_mesh_shape too).
       if (hasattr(model, 'ici_mesh_shape') and
           model.ici_mesh_shape is not None):
-        input_specs = jax.tree_map(py_utils.get_global_input_shape_dtype,
-                                   input_specs)
+        input_specs = jax.tree.map(
+            py_utils.get_global_input_shape_dtype, input_specs
+        )
       model.abstract_init_with_metadata(input_specs)
 
   @classmethod
   def create_test_methods_for_all_registered_experiments(
       cls,
       registry,
       task_regexes=None,
```

## paxml/io_utils.py

```diff
@@ -204,15 +204,15 @@
     cast_to_ndarray: bool = True,
     write_pickle: bool = True,
 ) -> None:
   """Writes `key_value_pairs` to pkl and jsonl files."""
   filename = epath.Path(filename)
 
   if cast_to_ndarray:
-    key_value_pairs = jax.tree_map(_to_ndarray, key_value_pairs)
+    key_value_pairs = jax.tree.map(_to_ndarray, key_value_pairs)
 
   if write_pickle:
     with filename.with_suffix('.pickle').open('wb') as pkl_f:
       pickle.dump(key_value_pairs, pkl_f, protocol=pickle.HIGHEST_PROTOCOL)
 
   with filename.with_suffix('.jsonl').open('w') as jsonl_f:
     for _, v in key_value_pairs:
```

## paxml/learners.py

```diff
@@ -57,15 +57,15 @@
 
   Example client code:
 
   p =  pax_fiddle.Config(Learner).set(...)
   learner = base_hyperparams.instantiate(p)
 
   mdl_vars = ...
-  var_weight_hparams = jax.tree_map(
+  var_weight_hparams = jax.tree.map(
     lambda v: base_layer.WeightHParams(v.shape), mdl_vars)
 
   grad_tx = learner.get_grad_tx(var_weight_hparams)
   opt_states0 = grad_tx.init(mdl_vars)
 
   grads0 = ...
   grads1, opt_states1 = learner.update_states(
@@ -187,25 +187,25 @@
         self.repeat_prefix_sep,
         force_prefix_structure=self.force_repeat_prefix_structure,
     )
 
   def summarize_norms(
       self, summary_prefix: str, tensors: pytypes.NestedJTensor
   ) -> None:
-    norms = jax.tree_map(lambda x: jnp.sqrt(jnp.sum(x * x)), tensors)
+    norms = jax.tree.map(lambda x: jnp.sqrt(jnp.sum(x * x)), tensors)
     keys = py_utils.extract_prefixed_keys_from_nested_map(tensors)
 
     def add_grad_norm_summary(key, value):
       base_layer.add_global_summary(
           f'{summary_prefix}/{key}',
           value,
           SummaryType.AGGREGATE_SCALAR,
       )
 
-    jax.tree_map(add_grad_norm_summary, keys, norms)
+    jax.tree.map(add_grad_norm_summary, keys, norms)
 
   def scale_gradients(
       self,
       raw_grads: NestedMap,
       optimizer_name: str | None = None,
       clip_gradient_norm_to_value: float | None = None,
       clip_gradient_single_norm_to_value: float | None = None,
@@ -274,28 +274,28 @@
       if clip_gradient_norm_to_value:
         grad_norm: JTensor
         assert clip_gradient_single_norm_to_value == 0.0
         grad_scale = jnp.minimum(
             jnp.array(1, grad_norm.dtype),
             jnp.array(clip_gradient_norm_to_value, grad_norm.dtype) / grad_norm,
         )
-        grads = jax.tree_map(lambda g: g * grad_scale, grads)
+        grads = jax.tree.map(lambda g: g * grad_scale, grads)
       elif clip_gradient_single_norm_to_value:
         assert clip_gradient_norm_to_value == 0.0
-        grad_single_norm = jax.tree_map(
+        grad_single_norm = jax.tree.map(
             lambda x: jnp.sqrt(jnp.sum(x * x)), grads
         )
 
         def scale_gradient(grad, norm):
           return grad * jnp.minimum(
               jnp.array(1, norm.dtype),
               jnp.array(clip_gradient_single_norm_to_value, norm.dtype) / norm,
           )
 
-        grads = jax.tree_map(scale_gradient, grads, grad_single_norm)
+        grads = jax.tree.map(scale_gradient, grads, grad_single_norm)
         grad_scale = jnp.array(1.0)
       else:
         # no clipping is needed.
         grad_scale = jnp.array(1.0)
       return grads, grad_scale
 
     if self.check_valid_step:
@@ -346,15 +346,15 @@
     orig_grads = grads
     orig_var_weight_hparams = var_weight_hparams
 
     # Omit the `overwrite_with_gradient` parameters as they are unnecessary for
     # the optimizer state update. Gradient overwriting takes place at the very
     # end of the function to update the transformed gradients.
     def _filter_owg(original):
-      return jax.tree_map(
+      return jax.tree.map(
           lambda o, h: (
               py_utils.BpropMaskedNode()
               if base_layer.var_overwrite_with_gradient(h)
               else o
           ),
           original,
           orig_var_weight_hparams,
@@ -367,39 +367,39 @@
     grads, valid_step = self.scale_gradients(grads)
     transformed_grad, new_states = self.get_grad_tx(var_weight_hparams).update(
         grads, states, old_vars
     )
 
     if self.enable_skip_step_on_gradient_anomalies:
       # Set grads to 0 if the step is invalid.
-      transformed_grad = jax.tree_map(
+      transformed_grad = jax.tree.map(
           lambda x: jnp.where(valid_step, x, jnp.zeros_like(x)),
           transformed_grad,
       )
 
       def _update(updated, original):
         """Keep the old state if the step is invalid."""
         if any(py_utils.is_optax_masked_node(x) for x in (updated, original)):
           return updated
         return jnp.where(valid_step, updated, original)
 
-      new_states = jax.tree_map(
+      new_states = jax.tree.map(
           _update, new_states, states, is_leaf=py_utils.is_optax_masked_node
       )
 
     # Final applied grad norm.
     if self.grad_norm_summary:
       applied_grad_norm = _compute_norm(transformed_grad)
       base_layer.add_global_summary(
           'learning/applied_grad_norm',
           applied_grad_norm,
           SummaryType.AGGREGATE_SCALAR,
       )
 
-    transformed_grad = jax.tree_map(
+    transformed_grad = jax.tree.map(
         lambda h, g, t_g: (
             g if base_layer.var_overwrite_with_gradient(h) else t_g
         ),
         orig_var_weight_hparams,
         orig_grads,
         transformed_grad,
     )
@@ -562,15 +562,15 @@
     # Aggregate all the auxiliary optimizer masks.
     for regex in self.auxiliary_regex:
       regexp = re.compile(regex)
       prefix = py_utils.extract_prefixed_keys_from_nested_map(
           var_weight_hparams
       )
       optimizer_mask.append(
-          jax.tree_map(
+          jax.tree.map(
               lambda x, regexp=regexp: regexp.match(x) is not None, prefix
           )
       )
 
     # Create the default optimizer mask.
     def check_var_in_auxiliary_regex(*args):
       """Check if a variable is already activated by an auxiliary optimizer."""
@@ -582,16 +582,16 @@
             raise ValueError(
                 'The regex pattern of auxiliary optimizers should'
                 'be non-overlapping.'
             )
           r = True
       return r
 
-    default_mask = jax.tree_map(check_var_in_auxiliary_regex, *optimizer_mask)
-    default_mask = jax.tree_map(lambda mask: not mask, default_mask)
+    default_mask = jax.tree.map(check_var_in_auxiliary_regex, *optimizer_mask)
+    default_mask = jax.tree.map(lambda mask: not mask, default_mask)
 
     return optimizer_mask, default_mask
 
   def get_grad_tx(
       self, var_weight_hparams: NestedWeightHParams
   ) -> optimizers.GeneralGradientTransformation:
     """The gradient transformation of the MultiOptimizer learner.
@@ -646,32 +646,32 @@
 
   def scale_gradients_by_optimizer(
       self, raw_grads: NestedMap, var_weight_hparams: NestedWeightHParams
   ) -> tuple[NestedMap, JTensor]:
     optimizer_mask, default_mask = self.get_masks(var_weight_hparams)
 
     all_grads, all_valid_step = self.scale_gradients(
-        jax.tree_map(lambda x, y: x * y, raw_grads, default_mask),
+        jax.tree.map(lambda x, y: x * y, raw_grads, default_mask),
         optimizer_name='main',
     )
 
     for name, mask, optimizer in zip(
         self.auxiliary_names,
         optimizer_mask,
         self.auxiliary_optimizers,
     ):
       assert optimizer.clip_gradient_norm_to_value is not None
       assert optimizer.clip_gradient_single_norm_to_value is not None
       grads, valid_step = self.scale_gradients(
-          jax.tree_map(lambda x, y: x * y, raw_grads, mask),
+          jax.tree.map(lambda x, y: x * y, raw_grads, mask),
           optimizer_name=name,
           clip_gradient_norm_to_value=optimizer.clip_gradient_norm_to_value,
           clip_gradient_single_norm_to_value=optimizer.clip_gradient_single_norm_to_value,
       )
-      all_grads = jax.tree_map(lambda x, y: x + y, all_grads, grads)
+      all_grads = jax.tree.map(lambda x, y: x + y, all_grads, grads)
       all_valid_step = jnp.logical_and(all_valid_step, valid_step)
     return all_grads, all_valid_step
 
   def update_states(
       self,
       grads: NestedMap,
       states: optax.OptState,
@@ -695,24 +695,24 @@
       )
     else:
       grads, valid_step = self.scale_gradients(grads)
     grad_tx = self.get_grad_tx(var_weight_hparams)
     transformed_grad, new_states = grad_tx.update(grads, states, old_vars)
     if self.enable_skip_step_on_gradient_anomalies:
       # Set grads to 0 if the step is invalid.
-      transformed_grad = jax.tree_map(
+      transformed_grad = jax.tree.map(
           lambda x: jnp.where(valid_step, x, jnp.zeros_like(x)),
           transformed_grad,
       )
 
       # Keep the old state if the step is invalid.
       def _update(x, y):
         if not py_utils.is_optax_masked_node(
             x
         ) and not py_utils.is_optax_masked_node(y):
           return jnp.where(valid_step, x, y)
         return x
 
-      new_states = jax.tree_map(
+      new_states = jax.tree.map(
           _update, new_states, states, is_leaf=py_utils.is_optax_masked_node
       )
     return transformed_grad, new_states
```

## paxml/learners_test.py

```diff
@@ -11,14 +11,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for Learners."""
 
+import typing
+
 from absl import logging
 from absl.testing import absltest
 from absl.testing import parameterized
 import jax
 from jax import numpy as jnp
 import numpy as np
 import optax
@@ -112,16 +114,17 @@
     grads.ffn = jnp.asarray(
         np.random.normal(1.6, 2.0, [1, 2]).astype('float32'))
     old_vars = grads.DeepCopy()
     old_vars.lm.w = jnp.asarray(
         np.random.normal(1.2, 4.0, [1, 2]).astype('float32'))
     old_vars.ffn = jnp.asarray(
         np.random.normal(1.6, 2.0, [1, 2]).astype('float32'))
-    var_weight_hparams = jax.tree_map(
-        lambda v: base_layer.WeightHParams(v.shape), old_vars)
+    var_weight_hparams = jax.tree.map(
+        lambda v: base_layer.WeightHParams(v.shape), old_vars
+    )
 
     grad_tx = learner_instance.get_grad_tx(var_weight_hparams)
 
     opt_states_pspec = grad_tx.init_partition_spec(var_weight_hparams)
     # Due to a new optax update, chained pytrees are masked.
     opt_states_pspec = opt_states_pspec.inner_state
     logging.info('opt_states_pspec=%s', opt_states_pspec)
@@ -141,19 +144,20 @@
     # {"count": DeviceArray(0, dtype=int32)},
     # )
     opt_states = grad_tx.init(old_vars)
     logging.info('opt_states: %s', opt_states)
 
     # Similar to tf.nest.assert_same_structure(opt_states_pspec, opt_states),
     # but takes is_leaf arg to treat WeightHParams as a leaf.
-    _ = jax.tree_map(
+    _ = jax.tree.map(
         lambda x, y: True,
         opt_states_pspec,
         opt_states,
-        is_leaf=lambda x: isinstance(x, base_layer.WeightHParams))
+        is_leaf=lambda x: isinstance(x, base_layer.WeightHParams),
+    )
 
     with base_layer.JaxContext.new_context():
       transformed_grads, updated_opt_states = learner_instance.update_states(
           grads, opt_states, old_vars, var_weight_hparams)
 
     logging.info('updated_opt_states: %s', updated_opt_states)
 
@@ -164,15 +168,15 @@
     #         "ffn": DeviceArray([[0.2296397, -1.6318845]], dtype=float32),
     #         "lm": {"w": DeviceArray([[4.0486345, 0.8042676]], dtype=float32)},
     #       }
     #     ),
     #     ScaleByScheduleState(count=DeviceArray(0, dtype=int32)),
     # )
     updated_grads, updated_state = sgd.update(grads, opt_states[2])
-    updated_grads = jax.tree_map(
+    updated_grads = jax.tree.map(
         lambda g, p: g - lr * decoupled_weight_decay * p,
         updated_grads,
         old_vars,
     )
     logging.info('updated_state: %s', updated_state)
     self.assertAllClose(transformed_grads.lm.w, updated_grads['lm']['w'])
     self.assertAllClose(transformed_grads.ffn, updated_grads['ffn'])
@@ -228,15 +232,15 @@
     old_vars = grads.DeepCopy()
     old_vars.lm.w = jnp.asarray(
         np.random.normal(1.2, 4.0, [1, 2]).astype('float32')
     )
     old_vars.ffn = jnp.asarray(
         np.random.normal(1.6, 2.0, [1, 2]).astype('float32')
     )
-    var_weight_hparams = jax.tree_map(
+    var_weight_hparams = jax.tree.map(
         lambda v: base_layer.WeightHParams(
             v.shape, mesh_shape=mesh_shape, tensor_split_dims_mapping=[-1, 1]
         ),
         old_vars,
     )
 
     grad_tx = learner_instance.get_grad_tx(var_weight_hparams)
@@ -248,15 +252,15 @@
     # Apply sharding with tree_map_params instead of calling init_partition_spec
     opt_states_pspec = opt_vec.partition_params(
         grad_tx, var_weight_hparams, opt_states
     )
     logging.info('opt_states_pspec=%s', opt_states_pspec)
     # Similar to tf.nest.assert_same_structure(opt_states_pspec, opt_states),
     # but takes is_leaf arg to treat WeightHParams as a leaf.
-    jax.tree_map(
+    jax.tree.map(
         lambda x, y: True,
         opt_states_pspec,
         opt_states,
         is_leaf=lambda x: isinstance(x, base_layer.WeightHParams),
     )
 
     with base_layer.JaxContext.new_context():
@@ -339,16 +343,17 @@
         w=jnp.asarray(np.random.normal(1.4, 2.0, [4, 4]).astype('float32')))
     old_vars.lm.transformer = NestedMap(
         w=jnp.asarray(np.random.normal(1.2, 4.0, [4, 4]).astype('float32')))
     grads.lm.ffn = NestedMap(
         k=jnp.asarray(np.random.normal(1.6, 2.0, [4, 4]).astype('float32')))
     old_vars.lm.ffn = NestedMap(
         k=jnp.asarray(np.random.normal(1.3, 2.0, [4, 4]).astype('float32')))
-    var_weight_hparams = jax.tree_map(
-        lambda v: base_layer.WeightHParams(v.shape), old_vars)
+    var_weight_hparams = jax.tree.map(
+        lambda v: base_layer.WeightHParams(v.shape), old_vars
+    )
     grad_tx = learner_instance.get_grad_tx(var_weight_hparams)
     opt_states = grad_tx.init(old_vars)
     with base_layer.JaxContext.new_context():
       transformed_grads, _ = learner_instance.update_states(
           grads, opt_states, old_vars, var_weight_hparams)
 
     expected_grad1 = -lr_multiplier1 * (
@@ -483,15 +488,15 @@
     grads.lm.ffn = NestedMap(
         k=jnp.asarray(np.random.normal(1.6, 2.0, [4, 4]).astype('float32'))
     )
     old_vars.lm.ffn = NestedMap(
         k=jnp.asarray(np.random.normal(1.3, 2.0, [4, 4]).astype('float32'))
     )
 
-    var_weight_hparams = jax.tree_map(
+    var_weight_hparams = jax.tree.map(
         lambda v: base_layer.WeightHParams(
             v.shape, mesh_shape=mesh_shape, tensor_split_dims_mapping=[-1, 1]
         ),
         old_vars,
     )
     var_weight_hparams.lm.ffn = NestedMap(
         k=base_layer.WeightHParams(
@@ -518,18 +523,18 @@
     # auxiliary optimizers plus 1 (for the primary optimizer).
     self.assertLen(
         partition_spec, len(learner_instance._auxiliary_optimizer_insts) + 1
     )
     # MaskedState has inner_state representing the single optimizer state
     # and the masked states are chained for optimizer and auziliary optimizers.
     for p in partition_spec:
-      jax.tree_map(
+      jax.tree.map(
           asserts.assert_same_structure,
           partition_spec_single,
-          p.inner_state,
+          p.inner_state,  # pytype:disable=attribute-error
       )
     with base_layer.JaxContext.new_context():
       transformed_grads, _ = learner_instance.update_states(
           grads, opt_state, old_vars, var_weight_hparams
       )
 
     expected_grad1 = -lr_multiplier1 * (
@@ -603,16 +608,17 @@
         w=jnp.asarray(np.random.normal(1.4, 2.0, [4, 4]).astype('float32')))
     old_vars.lm.transformer = NestedMap(
         w=jnp.asarray(np.random.normal(1.2, 4.0, [4, 4]).astype('float32')))
     grads.lm.ffn = NestedMap(
         k=jnp.asarray(np.random.normal(1.6, 2.0, [4, 4]).astype('float32')))
     old_vars.lm.ffn = NestedMap(
         k=jnp.asarray(np.random.normal(1.3, 2.0, [4, 4]).astype('float32')))
-    var_weight_hparams = jax.tree_map(
-        lambda v: base_layer.WeightHParams(v.shape), old_vars)
+    var_weight_hparams = jax.tree.map(
+        lambda v: base_layer.WeightHParams(v.shape), old_vars
+    )
     grad_tx = learner_instance.get_grad_tx(var_weight_hparams)
     opt_states = grad_tx.init(old_vars)
     logging.info('opt_states: %s', opt_states)
 
   def test_multioptimizer_learner_value_error(self):
     learner_p = pax_fiddle.Config(learners.MultiOptimizerLearner)
     learner_p.name = 'learner'
@@ -656,16 +662,17 @@
         w=jnp.asarray(np.random.normal(1.4, 2.0, [4, 4]).astype('float32')))
     old_vars.lm.transformer = NestedMap(
         w=jnp.asarray(np.random.normal(1.2, 4.0, [4, 4]).astype('float32')))
     grads.lm.ffn = NestedMap(
         k=jnp.asarray(np.random.normal(1.6, 2.0, [4, 4]).astype('float32')))
     old_vars.lm.ffn = NestedMap(
         k=jnp.asarray(np.random.normal(1.3, 2.0, [4, 4]).astype('float32')))
-    var_weight_hparams = jax.tree_map(
-        lambda v: base_layer.WeightHParams(v.shape), old_vars)
+    var_weight_hparams = jax.tree.map(
+        lambda v: base_layer.WeightHParams(v.shape), old_vars
+    )
     with self.assertRaises(ValueError):
       learner_instance.get_grad_tx(var_weight_hparams)
 
   def test_multioptimizer_learner_sharding(self):
     learner_p = pax_fiddle.Config(learners.MultiOptimizerLearner)
     learner_p.name = 'learner'
     learner_p.loss_name = 'loss'
@@ -769,27 +776,27 @@
             getattr(p[0][2], k), getattr(partition_spec_single[0][2], k))
 
   def test_vectorized_prefix(self):
 
     def _opt_init(params):
       # Reduction over each variable. Behavior will depend on vectorization.
       logging.info(f'Init called with params {params}')
-      return jax.tree_map(jnp.sum, params)
+      return jax.tree.map(jnp.sum, params)
 
     def _opt_update(updates, state, params):
       del params
-      return jax.tree_map(lambda u, s: u + s, updates, state), state
+      return jax.tree.map(lambda u, s: u + s, updates, state), state
 
     def _init_partition_spec(var_hparams):
 
       def _init_one(p):
         assert not p.repeat_prefix
         return p
 
-      return jax.tree_map(_init_one, var_hparams)
+      return jax.tree.map(_init_one, var_hparams)
 
     class TestOptimizer(optimizers.BaseOptimizer):
 
       def _get_raw_grad_transformation(self, lr):
         return optimizers.ShardedGradientTransformation(
             init=_opt_init,
             update=_opt_update,
@@ -846,15 +853,16 @@
                      [('data', 'mdl'), None])
     logging.info(f'Prefix vectorization partition spec .. {partition_spec} ')
     state = grad_tx.init(variables)
     logging.info('Prefix vectorization state after init .. ')
     logging.info(state)
     # Computed update is 0 + state, and state is sum of each variable.
     update, state = grad_tx.update(
-        jax.tree_map(jnp.zeros_like, variables), state, variables)
+        jax.tree.map(jnp.zeros_like, variables), state, variables
+    )
     # Variables a and c are scalars excluding the prefix, so the update must be
     # equal to the initial variable values.
     self.assertAllClose(update.a, variables.a)
     self.assertAllClose(update.c, variables.c)
     # b is not vectorized, so the update equals the sum reduction of the initial
     # variable value.
     logging.info(f'Prefix vectorization a after update .. {update.a}')
@@ -863,19 +871,19 @@
     self.assertAllClose(update.b,
                         jnp.zeros_like(variables.b) + jnp.sum(variables.b))
 
   def test_vectorized_prefix_with_tree_map_params(self):
     def _opt_init(params):
       # Reduction over each variable. Behavior will depend on vectorization.
       logging.info(f'Init called with params {params}')
-      return jax.tree_map(jnp.sum, params)
+      return jax.tree.map(jnp.sum, params)
 
     def _opt_update(updates, state, params):
       del params
-      return jax.tree_map(lambda u, s: u + s, updates, state), state
+      return jax.tree.map(lambda u, s: u + s, updates, state), state
 
     learner_p = pax_fiddle.Config(
         learners.Learner,
         name='learner',
         loss_name='loss',
         grad_norm_individual_vars=True,
     )
@@ -907,39 +915,45 @@
 
     state = grad_tx.init(variables)
     logging.info(state)
     opt_states_pspec = opt_vec.partition_params(grad_tx, var_hparams, state)
 
     logging.info('opt_states_pspec=%s', opt_states_pspec)
 
-    self.assertIn(opt_vec.NO_PREFIX_KEY, opt_states_pspec.keys())
-    self.assertIn('p#2.2#tsdata,smdl.', opt_states_pspec.keys())
-    self.assertIn('p#2#i-1', opt_states_pspec.keys())
+    opt_states_pspec_dict = typing.cast(
+        NestedMap, opt_states_pspec
+    ).ToNestedDict()
+    self.assertIn(opt_vec.NO_PREFIX_KEY, opt_states_pspec_dict.keys())
+    self.assertIn('p#2.2#tsdata,smdl.', opt_states_pspec_dict.keys())
+    self.assertIn('p#2#i-1', opt_states_pspec_dict.keys())
 
     pspec_1 = opt_states_pspec['p#2#i-1']
     pspec_2 = opt_states_pspec[opt_vec.NO_PREFIX_KEY]
     pspec_3 = opt_states_pspec['p#2.2#tsdata,smdl.']
 
     opt_idx = 2
-    self.assertEqual(pspec_1[opt_idx].a.shape, ())
-    self.assertEqual(pspec_1[opt_idx].a.repeat_prefix, [2])
-    self.assertEqual(pspec_1[opt_idx].a.repeat_prefix_split_dims_mapping, [-1])
-    self.assertEqual(pspec_2[opt_idx].b.shape, (2,))
-    self.assertEmpty(pspec_2[opt_idx].b.repeat_prefix or [])
-    self.assertEqual(pspec_3[opt_idx].c.shape, ())
-    self.assertEqual(pspec_3[opt_idx].c.repeat_prefix, [2, 2])
+    pspec_1_opt_idx = typing.cast(NestedMap, pspec_1[opt_idx])
+    pspec_2_opt_idx = typing.cast(NestedMap, pspec_2[opt_idx])
+    pspec_3_opt_idx = typing.cast(NestedMap, pspec_3[opt_idx])
+    self.assertEqual(pspec_1_opt_idx.a.shape, ())
+    self.assertEqual(pspec_1_opt_idx.a.repeat_prefix, [2])
+    self.assertEqual(pspec_1_opt_idx.a.repeat_prefix_split_dims_mapping, [-1])
+    self.assertEqual(pspec_2_opt_idx.b.shape, (2,))
+    self.assertEmpty(pspec_2_opt_idx.b.repeat_prefix or [])
+    self.assertEqual(pspec_3_opt_idx.c.shape, ())
+    self.assertEqual(pspec_3_opt_idx.c.repeat_prefix, [2, 2])
     self.assertEqual(
-        pspec_3[opt_idx].c.repeat_prefix_split_dims_mapping,
+        pspec_3_opt_idx.c.repeat_prefix_split_dims_mapping,
         [('data', 'mdl'), None],
     )
 
     logging.info('Prefix vectorization state after init .. ')
     # Computed update is 0 + state, and state is sum of each variable.
     update, state = grad_tx.update(
-        jax.tree_map(jnp.zeros_like, variables), state, variables
+        jax.tree.map(jnp.zeros_like, variables), state, variables
     )
     # Variables a and c are scalars excluding the prefix, so the update must be
     # equal to the initial variable values.
     self.assertAllClose(update.a, variables.a)
     self.assertAllClose(update.c, variables.c)
     # b is not vectorized, so the update equals the sum reduction of the initial
     # variable value.
@@ -949,26 +963,26 @@
     self.assertAllClose(
         update.b, jnp.zeros_like(variables.b) + jnp.sum(variables.b)
     )
 
   def test_scale_update_by_var_norm(self):
     def _opt_init(params):
       # Reduction over each variable. Behavior will depend on vectorization.
-      return jax.tree_map(jnp.sum, params)
+      return jax.tree.map(jnp.sum, params)
 
     def _opt_update(updates, state, params):
       del params
-      return jax.tree_map(lambda u, s: u + s, updates, state), state
+      return jax.tree.map(lambda u, s: u + s, updates, state), state
 
     def _init_partition_spec(var_hparams):
       def _init_one(p):
         assert not p.repeat_prefix
         return p
 
-      return jax.tree_map(_init_one, var_hparams)
+      return jax.tree.map(_init_one, var_hparams)
 
     class TestOptimizer(optimizers.BaseOptimizer):
 
       def _get_raw_grad_transformation(self, lr):
         return optimizers.ShardedGradientTransformation(
             init=_opt_init,
             update=_opt_update,
@@ -1007,34 +1021,36 @@
     with base_layer.JaxContext.new_context():
       learner_instance.apply_gradient(variables, grads, var_hparams)
 
   def test_vectorized_prefix_with_global_summary(self):
 
     def _opt_init(params):
       # Reduction over each variable. Behavior will depend on vectorization.
-      return jax.tree_map(jnp.sum, params)
+      return jax.tree.map(jnp.sum, params)
 
     def _opt_update(updates, state, params):
       del params
 
       def _opt_update_with_global_summary(u, s):
         base_layer.add_global_summary('u', u.sum())
         base_layer.add_global_summary('s', s.sum())
         return u + s
 
-      return jax.tree_map(_opt_update_with_global_summary, updates,
-                          state), state
+      return (
+          jax.tree.map(_opt_update_with_global_summary, updates, state),
+          state,
+      )
 
     def _init_partition_spec(var_hparams):
 
       def _init_one(p):
         assert not p.repeat_prefix
         return p
 
-      return jax.tree_map(_init_one, var_hparams)
+      return jax.tree.map(_init_one, var_hparams)
 
     class TestOptimizer(optimizers.BaseOptimizer):
 
       def _get_raw_grad_transformation(self, lr):
         return optimizers.ShardedGradientTransformation(
             init=_opt_init,
             update=_opt_update,
@@ -1087,15 +1103,16 @@
       self.assertEqual(pspec_3[opt_idx].c.repeat_prefix, [2, 2])
       self.assertEqual(pspec_3[opt_idx].c.repeat_prefix_split_dims_mapping,
                        [('data', 'mdl'), None])
 
       state = grad_tx.init(variables)
       # Computed update is 0 + state, and state is sum of each variable.
       update, state = grad_tx.update(
-          jax.tree_map(jnp.zeros_like, variables), state, variables)
+          jax.tree.map(jnp.zeros_like, variables), state, variables
+      )
       # Variables a and c are scalars excluding the prefix, so the update must
       # be equal to the initial variable values.
       self.assertAllClose(update.a, variables.a)
       self.assertAllClose(update.c, variables.c)
       # b is not vectorized, so the update equals the sum reduction of the
       # initial variable value.
       self.assertAllClose(update.b,
@@ -1106,29 +1123,33 @@
           'u.[2]_scalar', 'u.[]_scalar'
       ]
       summaries = base_layer.all_global_summaries()
       with self.subTest('test_keys'):
         self.assertCountEqual(expected_summary_keys, sorted(summaries))
       self.assertEqual(summaries['s.[2, 2]_scalar'].shape, (2, 2))
       self.assertEqual(summaries['s.[2, 2]_scalar'].dtype, np.float32)
-      self.assertEqual(summaries['s.[2, 2]_scalar']._value.tolist(),
-                       [[1.0, 2.0], [3.0, 4.0]])
+      self.assertEqual(
+          np.array(summaries['s.[2, 2]_scalar']).tolist(),
+          [[1.0, 2.0], [3.0, 4.0]],
+      )
       self.assertEqual(summaries['s.[2]_scalar'].shape, (2,))
       self.assertEqual(summaries['s.[2]_scalar'].dtype, np.float32)
-      self.assertEqual(summaries['s.[2]_scalar']._value.tolist(), [1.0, 2.0])
+      self.assertEqual(np.array(summaries['s.[2]_scalar']).tolist(), [1.0, 2.0])
       self.assertEqual(summaries['s.[]_scalar'].shape, ())
       self.assertEqual(summaries['s.[]_scalar'].dtype, np.float32)
-      self.assertEqual(summaries['s.[]_scalar']._value.tolist(), 3.0)
+      self.assertEqual(np.array(summaries['s.[]_scalar']).tolist(), 3.0)
       self.assertEqual(summaries['u.[2, 2]_scalar'].shape, (2, 2))
       self.assertEqual(summaries['u.[2, 2]_scalar'].dtype, np.float32)
-      self.assertEqual(summaries['u.[2, 2]_scalar']._value.tolist(),
-                       [[0.0, 0.0], [0.0, 0.0]])
+      self.assertEqual(
+          np.array(summaries['u.[2, 2]_scalar']).tolist(),
+          [[0.0, 0.0], [0.0, 0.0]],
+      )
       self.assertEqual(summaries['u.[2]_scalar'].shape, (2,))
       self.assertEqual(summaries['u.[2]_scalar'].dtype, np.float32)
-      self.assertEqual(summaries['u.[2]_scalar']._value.tolist(), [0.0, 0.0])
+      self.assertEqual(np.array(summaries['u.[2]_scalar']).tolist(), [0.0, 0.0])
       self.assertEqual(summaries['u.[]_scalar'].shape, ())
       self.assertEqual(summaries['u.[]_scalar'].dtype, np.float32)
-      self.assertEqual(summaries['u.[]_scalar']._value.tolist(), 0.0)
+      self.assertEqual(np.array(summaries['u.[]_scalar']).tolist(), 0.0)
 
 
 if __name__ == '__main__':
   absltest.main()
```

## paxml/main.py

```diff
@@ -40,14 +40,15 @@
 from clu import platform
 from etils import epath
 from fiddle import absl_flags
 import jax
 from paxml import base_experiment
 from paxml import eval_lib
 from paxml import experiment_registry
+from paxml import first_result_metric_callback
 from paxml import setup_jax
 from paxml import tasks_lib
 from paxml import tf_data_service_lib
 from paxml import train
 from paxml import trainer_lib
 from paxml import tuning_lib
 from praxis import pax_fiddle
@@ -328,14 +329,15 @@
         enable_checkpoint_saving=enable_checkpoint_saving,
         enforce_restore_shape_check=FLAGS.enforce_restore_shape_check,
         tensorstore_use_ocdbt=FLAGS.tensorstore_use_ocdbt,
         exit_after_ondemand_checkpoint=FLAGS.exit_after_ondemand_checkpoint,
         override_num_train_steps=override_num_train_steps,
         enable_summary_writer=FLAGS.enable_summary_writer,
         async_timeout_secs=FLAGS.async_timeout_secs,
+        train_first_result_callback_fn=first_result_metric_callback.train_first_result_callback_fn,
     )
 
   elif FLAGS.mode == 'eval':
     work_unit.set_task_status(f'Eval experiment {FLAGS.exp} at'
                               f' {job_log_dir}')
     eval_lib.evaluate(
         experiment_config=experiment_config,
```

## paxml/metric_utils.py

```diff
@@ -111,15 +111,17 @@
       and len(v) == 2
       and is_scalar(v[0])
       and is_scalar(v[1])
   )
 
 
 def is_float_convertible(
-    metric_value: numbers.Number | clu_values.Value | seqio.metrics.MetricValue,
+    metric_value: (
+        numbers.Number | clu_values.Value | seqio.metrics.MetricValue | Any
+    ),
 ):
   """Returns True if a metricv value is float convertible."""
   return (
       isinstance(metric_value, numbers.Number)
       or isinstance(metric_value, clu_values.Scalar)
       or isinstance(metric_value, seqio.metrics.Scalar)
       or is_weighted_scalar(metric_value)
@@ -127,19 +129,24 @@
           isinstance(metric_value, list)
           and all(is_weighted_scalar(v) for v in metric_value)
       )
   )
 
 
 def as_float(
-    metric_value: numbers.Number
-    | clu_values.Scalar
-    | seqio.metrics.Scalar
-    | WeightedScalar
-    | Sequence[WeightedScalar],
+    metric_value: (
+        numbers.Number
+        | clu_values.Scalar
+        | seqio.metrics.Scalar
+        | WeightedScalar
+        | Sequence[WeightedScalar]
+        | float
+        | np.ndarray
+        | Any
+    ),
 ) -> float:
   """Returns the aggregated float value from heterogeneous metric value."""
   if is_weighted_scalar(metric_value):
     metric_value = [metric_value]
 
   if isinstance(metric_value, list):
     assert all(is_weighted_scalar(v) for v in metric_value), metric_value
```

## paxml/metric_utils_test.py

```diff
@@ -12,21 +12,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for metric_utils."""
 
 import os
-from typing import Any
+from typing import Any, Sequence
 # Internal unittest mock import
 
 from absl.testing import absltest
 import clu.metrics as clu_metrics
 import clu.values as clu_values
+from etils import epath
 import flax
+import jax
+import jax.numpy as jnp
 import numpy as np
 import numpy.testing as npt
 from paxml import metric_utils
 from paxml import summary_utils
 # Internal platform import
 import seqio
 
@@ -40,18 +43,18 @@
   @classmethod
   def from_model_output(cls) -> clu_metrics.Metric:
     return MockMetric()
 
   def merge(self, other: clu_metrics.Metric) -> clu_metrics.Metric:
     return MockMetric()
 
-  def compute(self) -> None:
-    return None
+  def compute(self) -> jax.Array:
+    return jnp.array([])
 
-  def compute_value(self) -> clu_values.Value:
+  def compute_value(self) -> Any:
     raise NotImplementedError('Other mock metrics should define this.')
 
 
 def _mock_clu_image(batch_size=None) -> clu_values.Image:
   base_shape = [12, 12, 3]
   if batch_size:
     base_shape = [batch_size] + base_shape
@@ -75,15 +78,15 @@
 def _mock_seqio_audio(batch_size=None) -> seqio.metrics.Audio:
   base_shape = [12, 1]  # [time, channel]
   if batch_size:
     base_shape = [batch_size] + base_shape
   return seqio.metrics.Audio(np.ones(base_shape), sample_rate=44_100)
 
 
-def _mock_clu_video(batch_size=None) -> clu_values.Summary:
+def _mock_clu_video(batch_size=None) -> list[clu_values.Summary]:
   class Metadata(object):
     message: str = 'dummy metadata'
 
   base_shape = [5, 12, 12, 3]
   video = np.array([
       str(base_shape[1]),
       str(base_shape[2]),
@@ -104,15 +107,15 @@
     @flax.struct.dataclass
     class ScalarMetric(MockMetric):
 
       def compute_value(self) -> clu_values.Scalar:
         return clu_values.Scalar(5)
 
     metrics = {'test': ScalarMetric()}
-    test_dir = self._test_dir()
+    test_dir = epath.Path(self._test_dir())
 
     metric_values = metric_utils.compute_metric_values(metrics)
     self.assertIn('test', metric_values)
     self.assertEqual(metric_values['test'].value, 5)
 
     with summary_utils.get_summary_writer(test_dir):
       summary_utils.write_clu_metric_summaries(metric_values, step_i=0)
@@ -148,15 +151,15 @@
     self.assertEqual(metric_values['test/test_7'].value.shape, (2, 12, 1))
     self.assertEqual(metric_values['test/test_7'].sample_rate, 44_100)
 
   def test_tuple_compute_metric_values(self):
     @flax.struct.dataclass
     class ScalarTupleMetric(MockMetric):
 
-      def compute_value(self) -> tuple[Any]:
+      def compute_value(self) -> tuple[clu_values.Value, clu_values.Value]:
         return (clu_values.Scalar(5), clu_values.Text('hi'))
 
     metrics = {'test': ScalarTupleMetric()}
 
     metric_values = metric_utils.compute_metric_values(metrics)
     self.assertEqual(metric_values['test/test_0'].value, 5)
     self.assertEqual(metric_values['test/test_1'].value, 'hi')
@@ -246,15 +249,15 @@
             'histogram_0': clu_values.Histogram(
                 np.array([1, 2, 3]), num_buckets=2
             ),
         }
 
     metrics = {'test': MixedDictMetric()}
     metric_values = metric_utils.compute_metric_values(metrics)
-    test_dir = self._test_dir()
+    test_dir = epath.Path(self._test_dir())
     with summary_utils.get_summary_writer(test_dir):
       summary_utils.write_clu_metric_summaries(metric_values, step_i=0)
 
   def test_write_seqio_metric_summaries(self):
     @flax.struct.dataclass
     class MixedDictMetric(MockMetric):
 
@@ -270,15 +273,15 @@
             'histogram_0': seqio.metrics.Histogram(
                 np.array([1, 2, 3]), bins=2
             ),
         }
 
     metrics = {'test': MixedDictMetric()}
     metric_values = metric_utils.compute_metric_values(metrics)
-    test_dir = self._test_dir()
+    test_dir = epath.Path(self._test_dir())
     with summary_utils.get_summary_writer(test_dir):
       summary_utils.write_seqio_metric_summaries(
           [metric_values], step=0, metric_name_prefix='test_prefix'
       )
 
   def is_float_convertible(self):
     self.assertTrue(metric_utils.is_float_convertible(0.1))
@@ -292,15 +295,15 @@
         metric_utils.is_float_convertible([(np.array([1.0]), np.array([0.1]))]))
     self.assertFalse(metric_utils.is_float_convertible('abc'))
     self.assertFalse(
         metric_utils.is_float_convertible(seqio.metrics.Text('abc')))
     self.assertFalse(metric_utils.is_float_convertible(clu_values.Text('abc')))
     self.assertFalse(
         metric_utils.is_float_convertible(
-            clu_values.Histogram(np.array([1, 2, 3]))
+            clu_values.Histogram(np.array([1, 2, 3]), num_buckets=2)
         )
     )
 
   def test_as_float(self):
     self.assertEqual(metric_utils.as_float(0.2), 0.2)
     self.assertEqual(metric_utils.as_float(np.float32(1.0)), 1.0)
     self.assertEqual(metric_utils.as_float(clu_values.Scalar(0.2)), 0.2)
```

## paxml/partitioning.py

```diff
@@ -61,14 +61,19 @@
 BaseStepFnStaticArgs = trainer_lib.BaseStepFnStaticArgs
 TrainState = train_states.TrainState
 TrainStateProvenance = train_states.TrainStateProvenance
 TrainStateMetadata = trainer_lib.TrainStateMetadata
 RunningMode = trainer_lib.RunningMode
 
 
+def _identity(x):
+  """A helper identity function, defined globally so it is JIT-compiled once."""
+  return x
+
+
 def filter_nestedmap(full_specs, partial_specs):
   """Project full_specs into partial_specs."""
   if isinstance(full_specs, dict):
     result = type(full_specs)()
     for key in partial_specs.keys():  # pytype: disable=attribute-error  # jax-ndarray
       result[key] = filter_nestedmap(full_specs[key], partial_specs[key])
     return result
@@ -117,15 +122,15 @@
 
   def _create_aval(x):
     # canonicalize_dtype is necessary to avoid errors like
     # data types are different when compiling and when being called.
     dtype = jax.dtypes.canonicalize_dtype(x.dtype)
     return core.ShapedArray(x.shape, dtype)
 
-  inputs_shape_dtype = jax.tree_map(_create_aval, inputs_shape_dtype)
+  inputs_shape_dtype = jax.tree.map(_create_aval, inputs_shape_dtype)
   compiled = step_fn.lower(
       train_state, step_key, inputs_shape_dtype, static_args
   ).compile()
   return compiled, compiled.input_shardings[0]
 
 
 def _remove_input_padding(
@@ -140,15 +145,15 @@
   if padded_global_batch_size == unpadded_global_batch_size:
     return inputs
 
   def _remove_padding(x, pspec):
     x = x[:unpadded_global_batch_size]
     return base_layer.maybe_shard(x, pspec, mesh_names)
 
-  return jax.tree_map(_remove_padding, inputs, input_partition_spec)
+  return jax.tree.map(_remove_padding, inputs, input_partition_spec)
 
 
 def _write_input_specs(
     input_specs: NestedShapeDtypeLike, job_log_dir: epath.Path | None
 ) -> None:
   """Writes input specs as JSON to a file."""
   if job_log_dir is None or jax.process_index() != 0:
@@ -594,15 +599,15 @@
       self, train_input_pipeline: base_input.BaseInput
   ) -> NestedShapeDtypeLike:
     sample_inputs = train_input_pipeline.peek_padded()
     # Reshard inputs and only keep the inputs corresponding to a given device.
     sample_inputs = self.preprocess_inputs(
         train_input_pipeline, sample_inputs, partition_specs=None
     )
-    per_device_shape_dtype = jax.tree_map(
+    per_device_shape_dtype = jax.tree.map(
         lambda x: jax.ShapeDtypeStruct(shape=x.shape[1:], dtype=x.dtype),
         sample_inputs,
     )
     _write_input_specs(per_device_shape_dtype, self._job_log_dir)
     return per_device_shape_dtype
 
   def check_input_spec(self, batch: NestedJTensor) -> None:
@@ -610,16 +615,16 @@
     # Inputs preprocessed by this partitioner are sharded along the first axis.
     # Shapes specified by users for pmap-type models use per-device batch sizes,
     # hence we slice out that dimension here.
     logging.info('Checking input spec [pmap partitioner]')
 
     # Strip out the batch dimension from the batch and from the spec.
     fn = lambda x: jax.ShapeDtypeStruct(shape=x.shape[1:], dtype=x.dtype)
-    nested_shape_dtype = jax.tree_map(fn, batch)
-    spec = jax.tree_map(fn, self.train_inputs_shape_dtype)
+    nested_shape_dtype = jax.tree.map(fn, batch)
+    spec = jax.tree.map(fn, self.train_inputs_shape_dtype)
 
     if not trees.is_subset(spec, nested_shape_dtype):
       _spec_mismatch_error(nested_shape_dtype, spec)
 
   def initialize_prng_key_and_train_state(
       self,
       root_prng_key: PRNGKey,
@@ -639,21 +644,21 @@
           metadata.input_shape_dtype,
           discard_opt_states=discard_opt_states,
           is_eval=self._init_do_eval,
           checkpoint_type=checkpoint_type,
           var_weight_hparams=metadata.var_weight_hparams,
       )
 
-    logging.info('train state shapes: %s', jax.tree_map(jnp.shape, train_state))
+    logging.info('train state shapes: %s', jax.tree.map(jnp.shape, train_state))
     replicated_train_state = trainer_lib.replicate_model_state(train_state)
     # Unreplicated model states are not needed anymore at that point.
     del train_state
     logging.info(
         'replicated train state shapes: %s',
-        jax.tree_map(jnp.shape, replicated_train_state),
+        jax.tree.map(jnp.shape, replicated_train_state),
     )
 
     # From now on, different replicas should use different random seeds.
     # Here, each process will have its unique prng key.
     # root_prng_key will be further split so that each core on a host will get
     # different key.
     root_prng_key = jax.random.fold_in(root_prng_key, jax.process_index())
@@ -782,15 +787,15 @@
     # Pjit'ed function to preprocess the prng key.
     self._broadcast_key_fn = None
 
   def _get_train_inputs_shape_dtype(
       self, train_input_pipeline: base_input.BaseInput
   ) -> NestedShapeDtypeLike:
     sample_inputs = train_input_pipeline.peek_padded()
-    global_shape_dtype = jax.tree_map(
+    global_shape_dtype = jax.tree.map(
         py_utils.get_global_input_shape_dtype, sample_inputs
     )
     perhost_inputs_shape_dtype = trees.get_shape_dtype(sample_inputs)
     _write_input_specs(perhost_inputs_shape_dtype, self._job_log_dir)
     return global_shape_dtype
 
   @property
@@ -808,16 +813,16 @@
     )
 
   def check_input_spec(self, batch: NestedJTensor) -> None:
     """Check that the first input batch matches the given input spec."""
     logging.info('Checking input spec [pjit partitioner]')
 
     fn = lambda x: jax.ShapeDtypeStruct(shape=x.shape[1:], dtype=x.dtype)
-    spec = jax.tree_map(fn, self.train_inputs_shape_dtype)
-    input_batch_spec = jax.tree_map(fn, batch)
+    spec = jax.tree.map(fn, self.train_inputs_shape_dtype)
+    input_batch_spec = jax.tree.map(fn, batch)
 
     if not trees.is_subset(spec, input_batch_spec):
       _spec_mismatch_error(input_batch_spec, spec)
 
   def initialize_prng_key_and_train_state(
       self,
       root_prng_key: PRNGKey,
@@ -850,15 +855,15 @@
               checkpoint_type=checkpoint_type,
               discard_opt_states=discard_opt_states,
               var_weight_hparams=metadata.var_weight_hparams,
           )
       )
     logging.info(
         'partitioned train state shapes (global shape): %s',
-        jax.tree_map(jnp.shape, partitioned_train_state),
+        jax.tree.map(jnp.shape, partitioned_train_state),
     )
 
     # We do not fold in jax.process_index in contrast to the pmap version and
     # use a single global key instead to rely on pjit to split for different
     # replicas.
     logging.info('root prng key: %s', root_prng_key)
     return root_prng_key, partitioned_train_state, train_state_provenance
@@ -866,17 +871,14 @@
   def preprocess_prng_key(self, prng_key: PRNGKey) -> PRNGKey:
     """Preprocess the key before using it to run the partitioned function."""
     if not self._broadcast_key_fn:
       # The prng keys are already created on device with jax.random.split. We
       # broadcast it with an identity pjit function to avoid doing it in the
       # loop where a multi-slice program could be generated.
       def _broadcast_key(k):
-        def _identity(x):
-          return x
-
         rep_sharding = jax.sharding.NamedSharding(
             self._global_mesh, jax.sharding.PartitionSpec()
         )
         return pjit.pjit(
             _identity, in_shardings=rep_sharding, out_shardings=rep_sharding
         )(k)
 
@@ -979,15 +981,15 @@
         inputs: NestedJTensor,
         static_args: BaseStepFnStaticArgs | None = None,
     ) -> tuple[TrainState | None, StepFnOutput]:
       if use_padding:
         # When there are input padding on multi-host, we use a different device
         # order in the program's input sharding. We now make sure they are
         # resharded back to the device order consistent with the global mesh.
-        inputs = jax.tree_map(
+        inputs = jax.tree.map(
             lambda x, s: base_layer.maybe_shard(x, s, self._mesh_names),
             inputs,
             input_partition_spec,
         )
         # Vars/inputs are padded at program entry/exit to avoid uneven sharding.
         # We slice the vars to remove padding before the step computation, and
         # pad them after the step computation to make user code independent of
@@ -1003,15 +1005,15 @@
         )
 
       # Reshard inputs.
       # When xla auto-sharding is enabled, it is likely that we still
       # need to keep this resharding, because the resharding is related to the
       # locations of the real data (inputs_split_mapping) and is not visible
       # from HLOs.
-      inputs = jax.tree_map(reshard_inputs_fn, inputs)
+      inputs = jax.tree.map(reshard_inputs_fn, inputs)
 
       fn_out = step_fn(
           self._jax_task,
           state,
           prng_key,
           inputs,
           model.fprop_dtype,
@@ -1037,21 +1039,21 @@
       fn_in_partition_specs: NestedPartitionSpec,
       fn_out_partition_specs: NestedPartitionSpec,
       use_pspec_on_array_inputs: bool = False,
   ):
     logging.info('step_fn fn_in_partition_specs=%s', fn_in_partition_specs)
     logging.info('step_fn fn_out_partition_specs=%s', fn_out_partition_specs)
 
-    fn_in_shardings = jax.tree_map(
+    fn_in_shardings = jax.tree.map(
         lambda p: jax.sharding.NamedSharding(self._global_mesh, p)
         if not isinstance(p, pjit.AUTO)
         else p,
         fn_in_partition_specs,
     )
-    fn_out_shardings = jax.tree_map(
+    fn_out_shardings = jax.tree.map(
         lambda p: jax.sharding.NamedSharding(self._global_mesh, p)
         if not isinstance(p, pjit.AUTO)
         else p,
         fn_out_partition_specs,
     )
     extra_kwargs = dict(in_shardings=fn_in_shardings)
     if not use_pspec_on_array_inputs:
@@ -1064,15 +1066,15 @@
         donate_argnums=() if is_eval else (0,),
         static_argnums=(3,),  # For static_args.
         **extra_kwargs,
     )
     return trainer_lib.bind_mesh(pjitted_fn, self.global_mesh)
 
   def _get_state_unpadded_shapes(self, metadata: TrainStateMetadata):
-    return jax.tree_map(jnp.shape, metadata.unpadded_global_shapes)
+    return jax.tree.map(jnp.shape, metadata.unpadded_global_shapes)
 
   def _pad_states(
       self, metadata: TrainStateMetadata, unpadded_state: TrainState
   ):
     """Pad variables to avoid uneven sharding."""
     model = self._jax_task.model
 
@@ -1304,30 +1306,30 @@
 
     logging.info(
         (
             'Lowering auto sharded function with input shapes:'
             ' train_state_metadata=%s, prng_key=%s, inputs=%s static_args=%s'
         ),
         metadata.unpadded_global_shapes,
-        jax.tree_map(jnp.shape, self._init_key),
-        jax.tree_map(jnp.shape, inputs_shape_dtype),
+        jax.tree.map(jnp.shape, self._init_key),
+        jax.tree.map(jnp.shape, inputs_shape_dtype),
         static_args,
     )
     (
         auto_sharded_step_fn,
         input_shardings,
     ) = compile_for_auto_sharding(
         partitioned_step_fn,
         metadata.unpadded_global_shapes,
         self._init_key,
         inputs_shape_dtype,
         static_args,
     )
-    new_train_state_pspec = jax.tree_map(lambda x: x.spec, input_shardings[0])
-    new_input_pspec = jax.tree_map(lambda x: x.spec, input_shardings[2])
+    new_train_state_pspec = jax.tree.map(lambda x: x.spec, input_shardings[0])
+    new_input_pspec = jax.tree.map(lambda x: x.spec, input_shardings[2])
     return auto_sharded_step_fn, new_input_pspec, new_train_state_pspec
 
   def get_train_state_metadata(
       self,
       discard_opt_states: bool = False,
       unpadded_global_batch_size: int | None = None,
   ) -> TrainStateMetadata:
```

## paxml/programs.py

```diff
@@ -13,19 +13,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """The basic program concept that encapsulates a per-step runnable."""
 
 import abc
 import collections
+import concurrent
 import contextlib
 import dataclasses
 import queue
 import time
-from typing import Any, Mapping, Sequence
+from typing import Any, Callable, Mapping, Sequence
 
 from absl import flags
 from absl import logging
 from clu import periodic_actions
 from etils import epath
 import jax
 from jax import monitoring
@@ -219,14 +220,26 @@
 
     # Other states used during training.
     self._first_step_completion_time: float = None
     self._init_duration_set = False
 
     # Used to enter context of various summary writer at .setup().
     self._exitstack = contextlib.ExitStack()
+    self._first_result_callback_pool = concurrent.futures.ThreadPoolExecutor(
+        max_workers=1, thread_name_prefix='FirstResultCallback'
+    )
+    self._train_first_result_callback_fn = lambda: None
+
+  def register_first_result_callback_fn(
+      self, train_first_result_callback_fn: Callable[[], None]
+  ) -> None:
+    self._train_first_result_callback_fn = train_first_result_callback_fn
+
+  def first_result_callback_fn(self) -> None:
+    self._train_first_result_callback_fn()
 
   @property
   def train_input(self) -> base_input.BaseInput:
     assert self._train_input
     return self._train_input
 
   @property
@@ -368,14 +381,15 @@
         '[PAX STATUS]: train_step() took %f seconds.',
         20,
         train_period.elapsed,
     )
     self._pending_train_losses.add_computation(train_outputs.loss)
     if step == self._initial_step:
       self._first_step_completion_time = time.time()
+      self._first_result_callback_pool.submit(self.first_result_callback_fn)
 
     if do_profile and step - self._initial_step < profiler_capture_step:
       self._profiler.update_step_moving_mean(train_period.elapsed)
     logging.log_first_n(
         logging.INFO, '[PAX STATUS]:  Writing summaries (attempt).', 5
     )
     steps_per_sec = self._maybe_write_summaries(
@@ -573,14 +587,15 @@
   # TODO(laigd): remove this.
   @property
   def train_unpadded_global_batch_size(self) -> int:
     return self._train_unpadded_global_batch_size
 
   def shutdown(self) -> None:
     self._pending_train_losses.wait_for_all()
+    self._first_result_callback_pool.shutdown(wait=True)
     self._train_summary_handler.close()
     if self._eval_train_summary_handler:
       self._eval_train_summary_handler.close()
     self._exitstack.close()
 
 
 class SingleTaskTrainProgram(BaseTrainProgram):
@@ -971,15 +986,15 @@
         metrics[k].append(weighted_scalars[k])
 
       # Use jax.device_get to overlap the device -> host memory transfer.
       # Make a copy on the transferred tensor since running
       # py_utils.tree_unstack() on XLA-backed numpy arrays is inefficient
       # (b/284371615).
       per_example_scores.append(
-          jax.tree_map(lambda x: x.copy(), jax.device_get(per_example_out))
+          jax.tree.map(lambda x: x.copy(), jax.device_get(per_example_out))
       )
 
       # Merge clu.metrics to update for each minibatch.
       clu_metrics = metric_utils.merge_clu_metrics(
           clu_metrics, eval_step_clu_metrics_out
       )
     if self.eval_input.reset_for_eval:
```

## paxml/programs_test.py

```diff
@@ -116,15 +116,16 @@
     lp.optimizer.lr_schedule = pax_fiddle.Config(schedules.Constant)
     cls.task = instantiate(task_p)
 
 
 class SingleTaskPjitTrainProgramTest(ProgramTestBase):
 
   def test_train_program(self):
-    inputs_shape_dtype = jax.tree_map(
+    assert self.train_input is not None
+    inputs_shape_dtype = jax.tree.map(
         lambda x: jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype),
         self.train_input.get_next(),
     )
     partitioner = partitioning.PjitPartitioner(
         init_is_eval=False, reshard_inputs=True, task=self.task
     )
     prng_key = jax.random.PRNGKey(0)
```

## paxml/seqio_input.py

```diff
@@ -123,15 +123,15 @@
   """Converts any bytes leafs to strings in a pytree."""
   def _convert_fn(leaf: Any) -> Any:
     if not isinstance(leaf, bytes):
       return leaf
 
     return leaf.decode('utf-8')
 
-  return jax.tree_map(_convert_fn, tree)
+  return jax.tree.map(_convert_fn, tree)
 
 
 def select_split(
     task: str,
     split_name: str | Callable[[str], str],
 ) -> str:
   """Returns a split name given a split selector (Callable) or str literal."""
@@ -945,14 +945,22 @@
         num_epochs=num_epochs,
         shard_info=shard_info,
         use_cached=self.use_cached,
         seed=self.input_random_seed,
         trim_output_features=self.trim_output_features,
         try_in_mem_cache=self.try_in_mem_cache,
     )
+    if (
+        isinstance(self.mixture_or_task_inst, seqio.Mixture)
+        and self.feature_converter
+        and hasattr(self.feature_converter, '_passthrough_features')
+    ):
+      kwargs.update(
+          passthrough_features=self.feature_converter._passthrough_features  #  pylint:disable=protected-access
+      )
     ds = self.mixture_or_task_inst.get_dataset(**kwargs)
     assert self.feature_converter
     ds = self.feature_converter(
         ds, task_feature_lengths=self.task_feature_lengths
     )
 
     # We want to add enumeration provenance fields *after* applying all
@@ -1360,15 +1368,15 @@
         ])
         return model_outs
 
       # When model output type is PREDICTION_WITH_AUX or
       # SCORE_WITH_INTERMEDIATES, model output is a tuple of two arrays/lists.
       if isinstance(model_outs, tuple):
         prediction_or_score, aux_value = model_outs
-        aux_value = jax.tree_map(
+        aux_value = jax.tree.map(
             np.array,
             aux_value,
             is_leaf=lambda x: isinstance(x, list),
         )
         model_outs = (
             _pad_if_inhomogeneous(prediction_or_score),
             aux_value,
```

## paxml/seqio_input_test.py

```diff
@@ -59,18 +59,30 @@
               seqio.test_utils.sentencepiece_vocab(),
               add_eos=add_eos)
           for feat in output_feature_names
       },
       metric_fns=[])
 
 
+def _register_mixture(
+    mixture_name: str,
+    task_name: str,
+) -> None:
+  """Register a dummy task."""
+  seqio.MixtureRegistry.add(
+      mixture_name,
+      [
+          (task_name, 1),
+      ],
+  )
+
+
 # A score metric function. It must two args: `targets` and `predictions`. See:
 # https://github.com/google/seqio/blob/90c76914ed13fcce53f00966b824e45fb266b973/seqio/dataset_providers.py#L817-L821
-def _dummy_metric(targets: Sequence[str],
-                  predictions: Sequence[str]) -> Mapping[str, float]:
+def _dummy_metric(targets: float, predictions: float) -> Mapping[str, float]:
   return {'accuracy': targets + predictions}
 
 
 # A score metric function. It must two args: `targets` and `scores`. See:
 # https://github.com/google/seqio/blob/90c76914ed13fcce53f00966b824e45fb266b973/seqio/dataset_providers.py#L817-L821
 def _dummy_score_metric(targets: Sequence[Any],
                         scores: Sequence[float]) -> Mapping[str, float]:
@@ -355,14 +367,52 @@
     inp = instantiate(p)
     batch = inp.get_next()
     self.assertArraysEqual(
         batch.passthrough,
         np.array([[5, 6, 1]], dtype=np.int32),
     )
 
+  def test_passthrough_features_mixture(self):
+    """test passthrough features mixture."""
+    name = 'passthrough_features'
+    mix_name = 'mix'
+    passthrough_features = {
+        'passthrough': seqio.feature_converters.FeatureConverter.FeatureSpec(
+            dtype=tf.int32
+        ),
+    }
+    x = [{
+        'inputs': [1, 2],
+        'targets': [3, 4],
+        'passthrough': [5, 6],
+    }]
+    ds = seqio.test_utils.create_default_dataset(
+        x, feature_names=['inputs', 'targets', 'passthrough']
+    )
+    _register_task(
+        name, ds, output_feature_names=['inputs', 'targets', 'passthrough']
+    )
+    _register_mixture(mix_name, name)
+    p = pax_fiddle.Config(seqio_input.SeqIOInput)
+    p.mixture_name = mix_name
+    p.split_name = 'train'
+    p.task_feature_lengths = {'inputs': 3, 'targets': 3, 'passthrough': 3}
+    p.feature_converter = seqio_input.LanguageModelFeatures(
+        pack=False, passthrough_features=passthrough_features
+    )
+    p.batch_size = 1
+    p.is_training = True
+    p.input_random_seed = 137
+    inp = instantiate(p)
+    batch = inp.get_next()
+    self.assertArraysEqual(
+        batch.passthrough,
+        np.array([[5, 6, 1]], dtype=np.int32),
+    )
+
   # TODO(b/272314337): enable after the next TF OSS release.
   def test_file_based_checkpointing(self):
     it = tf.data.Dataset.range(1).as_numpy_iterator()
     if not isinstance(it, tf.__internal__.tracking.Trackable):
       # TODO(b/272314337): enable after the next TF OSS release.
       self.skipTest('file-based iterator checkpointing is not supported')
 
@@ -1091,14 +1141,15 @@
         seed,
         seqio_input.MetricType.SCORE,
         eval_metrics_retain_task_features=True,
         feature_converter=seqio.PassThroughFeatureConverter(),
         pass_entire_feature_lengths=True,
     )
     inp: seqio_input.SeqIOInput = instantiate(score_hparams[0])
+    assert inp.task_feature_lengths is not None
     self.assertSameElements(inp.task_feature_lengths.keys(),
                             ['inputs', 'targets', 'weights', 'embeddings'])
 
     score_hparams = seqio_input.get_eval_hparams_for_seqio(
         mixture_name,
         batch_size,
         feature_lengths,
@@ -1106,14 +1157,15 @@
         seqio_input.MetricType.SCORE,
         eval_metrics_retain_task_features=False,
         feature_converter=seqio.PassThroughFeatureConverter(),
         pass_entire_feature_lengths=True,
     )
     inp: seqio_input.SeqIOInput = instantiate(score_hparams[0])
     inp.get_next()
+    assert inp.task_feature_lengths is not None
     self.assertSameElements(
         inp.task_feature_lengths.keys(),
         ['inputs', 'targets', 'weights', 'embeddings'],
     )
 
   def test_get_eval_hparams_for_seqio_scoring_keeps_lengths(self):
     feature_lengths = {'inputs': 1024, 'targets': 3, 'weights': 3}
```

## paxml/sgf.py

```diff
@@ -232,15 +232,15 @@
       # with std scaled by `sqrt(num_devices)``, we need to further scale the
       # noise_std on each device to correct this.
       noise_stddev *= clipping_bound_scaling * jnp.sqrt(clipping_bound_scaling)
 
     def _add_noise_to_array(x, prng):
       return x + noise_stddev * jax.random.normal(prng, shape=x.shape)
 
-    final_grads = jax.tree_map(_add_noise_to_array, grads, prng_tree)
+    final_grads = jax.tree.map(_add_noise_to_array, grads, prng_tree)
     return final_grads
 
   def grad_fn(
       self,
       loss_fn: Callable[..., tuple[JTensor, GradAuxInfo]],
       mdl_vars_grad: NestedJTensor,
       mdl_vars_nograd_and_inputs: tuple[NestedJTensor, NestedMap],
@@ -281,15 +281,15 @@
 
     if self.l2_norm_clip is not None:
       grads, num_clipped, grad_norm = self._clip_gradients(
           grads, clipping_bound_scaling * self.l2_norm_clip
       )
 
     if self.normalize_gradients:
-      grads = jax.tree_map(lambda x: x / self.l2_norm_clip, grads)
+      grads = jax.tree.map(lambda x: x / self.l2_norm_clip, grads)
 
     # Optimization if using this class only for clipping (e.g., with DP-MF)
     if self.noise_multiplier > 0.0:
       noise_stddev = self.noise_multiplier * (
           1.0 if self.normalize_gradients else self.l2_norm_clip
       )
       grads = self._add_noise(
@@ -364,24 +364,24 @@
       microbatch_size: int = 1,
   ) -> tuple[NestedMap, GradAuxInfo, int]:
     def _reshape_and_mean(g):
       return jnp.mean(
           jnp.reshape(g, [-1, microbatch_size, *g.shape[1:]]), axis=1
       )
 
-    grads = jax.tree_map(_reshape_and_mean, grads)
+    grads = jax.tree.map(_reshape_and_mean, grads)
     grads_flat, grads_treedef = jax.tree_util.tree_flatten(grads)
     sum_clipped, num_clipped = optax.per_example_global_norm_clip(
         grads=grads_flat, l2_norm_clip=l2_norm_clip
     )
     sum_grads = jax.tree_util.tree_unflatten(grads_treedef, sum_clipped)
 
     # Normalize gradients across all examples.
     batch_size = grads_flat[0].shape[0]
-    clipped_grads_mean = jax.tree_map(lambda x: x / batch_size, sum_grads)
+    clipped_grads_mean = jax.tree.map(lambda x: x / batch_size, sum_grads)
     frac_clipped = num_clipped / batch_size
     dp_aux_info = {'frac_clipped': frac_clipped}
 
     return clipped_grads_mean, dp_aux_info, batch_size  # pytype: disable=bad-return-type  # jax-types
 
   def _add_noise(  # pytype: disable=annotation-type-mismatch  # jax-ndarray
       self,
@@ -406,23 +406,23 @@
       # with std scaled by `sqrt(num_devices)``, we need to further scale the
       # noise_std on each device to correct this.
       noise_stddev *= clipping_bound_scaling * jnp.sqrt(clipping_bound_scaling)
 
     def _add_noise_to_array(x, prng):
       return x + noise_stddev * jax.random.normal(prng, shape=x.shape)
 
-    final_grads = jax.tree_map(_add_noise_to_array, grads, prng_tree)
+    final_grads = jax.tree.map(_add_noise_to_array, grads, prng_tree)
     return final_grads
 
   def _prepare_inputs(self, inputs):
     """Reshape inputs to prepare for vmap to find per-example gradients."""
-    return jax.tree_map(jax.tree_util.Partial(jnp.expand_dims, axis=1), inputs)
+    return jax.tree.map(jax.tree_util.Partial(jnp.expand_dims, axis=1), inputs)
 
   def process_aux_info(self, aux_info: GradAuxInfo) -> GradAuxInfo:
-    aux_info = jax.tree_map(jax.tree_util.Partial(jnp.mean, axis=0), aux_info)
+    aux_info = jax.tree.map(jax.tree_util.Partial(jnp.mean, axis=0), aux_info)
     return aux_info
 
   def grad_fn(
       self,
       loss_fn: Callable[..., Any],
       mdl_vars_grad: NestedJTensor,
       mdl_vars_nograd_and_inputs: tuple[NestedJTensor, NestedMap],
@@ -477,19 +477,19 @@
               inner_batch_size,
               -1,
               microbatch_size // microbatch_splits,
               *x.shape[2:],
           ],
       )
 
-    inputs = jax.tree_map(reshape_batch, inputs)
+    inputs = jax.tree.map(reshape_batch, inputs)
 
     def _process_inner_batch(index: int) -> Any:
       """Computes mean clipped gradient for inner batch specified by index."""
-      new_inputs = jax.tree_map(lambda x: x[:, index, ...], inputs)
+      new_inputs = jax.tree.map(lambda x: x[:, index, ...], inputs)
 
       # Compute loss and gradients.
       (values, aux), grads = grad_fn(
           mdl_vars_grad, (mdl_vars_nograd, new_inputs), inner_prng_keys[index]
       )
       clipping_bound_scaling = _clipping_bound_scaling(
           self.use_loss_weight_scaling, loss_weight=aux.loss_weight
@@ -498,15 +498,15 @@
       # Clip and aggregate gradients.
       grads, dp_aux_info, _ = self._clip_and_mean_gradients(
           grads,
           clipping_bound_scaling * self.l2_norm_clip,
           microbatch_splits,
       )
       # Aggregate values and aux.
-      values = jax.tree_map(jax.tree_util.Partial(jnp.mean, axis=0), values)
+      values = jax.tree.map(jax.tree_util.Partial(jnp.mean, axis=0), values)
       aux = self.process_aux_info(aux)
       return (
           values,
           DPGradAuxInfo(
               dp_aux_info=dp_aux_info,
               aux_info=aux.aux_info,
               loss_weight=aux.loss_weight,
@@ -515,35 +515,35 @@
       )
 
     def _loop_process_inner_batch(index: int, val: Any) -> Any:
       """Wrapper for _process_inner_batch suitable for fori_loop."""
       cur_values, cur_aux, cur_grads = val
       values, aux, grads = _process_inner_batch(index)
 
-      new_values = jax.tree_map(jnp.add, cur_values, values)
-      new_aux = jax.tree_map(jnp.add, cur_aux, aux)
-      new_grads = jax.tree_map(jnp.add, cur_grads, grads)
+      new_values = jax.tree.map(jnp.add, cur_values, values)
+      new_aux = jax.tree.map(jnp.add, cur_aux, aux)
+      new_grads = jax.tree.map(jnp.add, cur_grads, grads)
       return (new_values, new_aux, new_grads)
 
     # Loop over inner batches, summing the results together.
     # We have to do one iteration first to get the correct shape of the return
     # values.
     values, aux, grads = jax.lax.fori_loop(
         1, num_iters, _loop_process_inner_batch, _process_inner_batch(0)
     )
 
     # Normalize results by number of inner batches.
-    values, aux, grads = jax.tree_map(
+    values, aux, grads = jax.tree.map(
         jax.tree_util.Partial(jnp.multiply, 1.0 / num_iters),
         (values, aux, grads),
     )
 
     # Add noise to normalized gradients.
     if self.normalize_gradients:
-      grads = jax.tree_map(lambda x: x / self.l2_norm_clip, grads)
+      grads = jax.tree.map(lambda x: x / self.l2_norm_clip, grads)
 
     # Optimization if using this class only for clipping (e.g., with DP-MF)
     if self.noise_multiplier > 0.0:
       noise_stddev = (
           self.noise_multiplier
           / batch_size
           * (1.0 if self.normalize_gradients else self.l2_norm_clip)
@@ -564,15 +564,15 @@
     microbatch_size: The number of samples in one micro-batch. See Section 5.6
       of https://arxiv.org/pdf/2303.00654.pdf for more details.
   """
 
   microbatch_size: int = 1
 
   def _prepare_inputs(self, inputs):
-    return jax.tree_map(self._prepare_for_microbatching, inputs)
+    return jax.tree.map(self._prepare_for_microbatching, inputs)
 
   def _prepare_for_microbatching(self, tensor: JTensor) -> JTensor:
     """Reshapes tensor for vmap with microbatch size support.
 
     Args:
       tensor: the input tensor, of shape `(batch_size, ...)`, where the
         batch_size should be dividable by the microbatch_size.
@@ -627,15 +627,15 @@
       microbatch_size: int = 1,
   ) -> tuple[NestedMap, GradAuxInfo, int]:
     def _reshape_and_mean(g):
       return jnp.mean(
           jnp.reshape(g, [-1, microbatch_size, *g.shape[1:]]), axis=1
       )
 
-    grads = jax.tree_map(_reshape_and_mean, grads)
+    grads = jax.tree.map(_reshape_and_mean, grads)
     grads_flat, grads_treedef = jax.tree_flatten(grads)
     sum_grads_flat, num_clipped_flat = optax.per_example_layer_norm_clip(
         grads=grads_flat,
         global_l2_norm_clip=l2_norm_clip,
         uniform=self.use_uniform,
     )
 
@@ -653,21 +653,21 @@
     ]
     sum_layer_grad_norms = jax.tree_unflatten(
         grads_treedef, sum_layer_grad_norms
     )
 
     # Normalize gradients across all examples.
     batch_size = grads_flat[0].shape[0]
-    mean_clipped_grads = jax.tree_map(lambda x: x / batch_size, sum_grads)
-    mean_layer_grad_norms = jax.tree_map(
+    mean_clipped_grads = jax.tree.map(lambda x: x / batch_size, sum_grads)
+    mean_layer_grad_norms = jax.tree.map(
         lambda x: x / batch_size, sum_layer_grad_norms
     )
 
     # Compute frac clipped statistics across all layers
-    frac_clipped = jax.tree_map(lambda x: x / batch_size, num_clipped)
+    frac_clipped = jax.tree.map(lambda x: x / batch_size, num_clipped)
     frac_clipped_flat, _ = jax.tree_util.tree_flatten(frac_clipped)
     frac_clipped_flat = jnp.stack(frac_clipped_flat)
     mean_frac_clipped = jnp.mean(frac_clipped_flat)
     stdev_frac_clipped = jnp.std(frac_clipped_flat)
 
     dp_aux_info = {
         'frac_clipped': frac_clipped,
@@ -712,15 +712,15 @@
     grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
     batch_size = jax.tree_util.tree_flatten(mdl_vars_nograd_and_inputs[1])[0][
         0
     ].shape[0]
 
     # Pass 1: get per-example gradient norms
     scales = jnp.ones(batch_size)
-    params_with_sq_norms = jax.tree_map(
+    params_with_sq_norms = jax.tree.map(
         lambda x: ghostnorm_base.ParamWithAux(x, scales), mdl_vars_grad[PARAMS]
     )
     (_, aux), grad_with_sq_norms = grad_fn(
         {**mdl_vars_grad, PARAMS: params_with_sq_norms},
         mdl_vars_nograd_and_inputs,
         prng_key,
     )
@@ -753,23 +753,23 @@
       scales = jnp.minimum(1.0, self.l2_norm_clip / grad_norms)
       frac_clipped = jnp.mean(scales < 1.0)
       if self.normalize_gradients:
         # Scale gradients to have norm at most 1 instead of l2_norm_clip.
         scales = scales / self.l2_norm_clip
 
     # Pass 2: get average of clipped gradients
-    params_with_sq_norms = jax.tree_map(
+    params_with_sq_norms = jax.tree.map(
         lambda x: ghostnorm_base.ParamWithAux(x, scales), mdl_vars_grad[PARAMS]
     )
     (loss, aux), clipped_grads = grad_fn(
         {**mdl_vars_grad, PARAMS: params_with_sq_norms},
         mdl_vars_nograd_and_inputs,
         prng_key,
     )
-    clipped_grads[PARAMS] = jax.tree_map(
+    clipped_grads[PARAMS] = jax.tree.map(
         lambda x: x.param, clipped_grads[PARAMS], is_leaf=is_leaf
     )
 
     # Note here noise stddev is divided by num_devices because in PAX the loss
     # is scaled by global batch size when pmap is used (see above)
     if self.noise_multiplier > 0.0:
       noise_stddev = (
```

## paxml/summary_utils.py

```diff
@@ -282,15 +282,15 @@
       return 'x'.join(str(e) for e in x.shape[1:]) + maybe_dtype_str(x)
     else:
       # If var is not replicated, no need to remove the first dim.
       # Retrieves the global shape for Jax array; otherwise host-local shape.
       x_sda = x
       return 'x'.join(str(e) for e in x_sda.shape) + maybe_dtype_str(x)
 
-  out = jax.tree_map(pps, replicated_vars)
+  out = jax.tree.map(pps, replicated_vars)
   out = pretty_repr(out)
   return pretty_format_iters(out)
 
 
 def pretty_repr_provenance(
     provenance: TensorProvenance | Nested[TensorProvenance],
 ) -> str:
@@ -331,15 +331,15 @@
   """L2 Norms over pytree."""
 
   def _sq(x):
     a = jnp.maximum(jnp.amax(jnp.abs(x)), 1.0)
     x = x / a
     return jnp.array([x.size, (a**2) * jnp.sum(x**2)])
 
-  squares = jax.tree_map(_sq, tree)
+  squares = jax.tree.map(_sq, tree)
   names, squares = zip(*_yield_subtrees(squares, max_level=max_level))
   names = [sep.join(name) for name in names]
   if prefix:
     names = [prefix + sep + n for n in names]
 
   def norm_fn(tree: NestedJTensor) -> jnp.float32:
     out = jax.tree_util.tree_reduce(operator.add, tree)
@@ -412,31 +412,36 @@
     elif base_layer.get_summary_base_type(summary_type) == SummaryType.VIDEO:
       video_summaries[k] = v
 
   # Compute the mean of scalars.
   scalar_summaries = jax.lax.pmean(
       scalar_summaries, axis_name=PMAP_PARALLEL_AXIS_NAME)
   # Gather per-replica image results.
-  image_summaries = jax.tree_map(
+  image_summaries = jax.tree.map(
       lambda x: jax.lax.all_gather(x, axis_name=PMAP_PARALLEL_AXIS_NAME),
-      image_summaries)
+      image_summaries,
+  )
   max_entries = MAX_IMAGES_PER_SUMMARY
-  image_summaries = jax.tree_map(
+  image_summaries = jax.tree.map(
       lambda x: jnp.reshape(x, [-1] + list(x.shape)[-3:])[:max_entries],
-      image_summaries)
-  audio_summaries = jax.tree_map(
+      image_summaries,
+  )
+  audio_summaries = jax.tree.map(
       lambda x: jax.lax.all_gather(x, axis_name=PMAP_PARALLEL_AXIS_NAME),
-      audio_summaries)
+      audio_summaries,
+  )
   max_entries = MAX_AUDIOS_PER_SUMMARY
-  audio_summaries = jax.tree_map(
+  audio_summaries = jax.tree.map(
       lambda x: jnp.reshape(x, [-1] + list(x.shape[-2:]))[:max_entries],
-      audio_summaries)
-  video_summaries = jax.tree_map(
+      audio_summaries,
+  )
+  video_summaries = jax.tree.map(
       lambda x: jax.lax.all_gather(x, axis_name=PMAP_PARALLEL_AXIS_NAME),
-      video_summaries)
+      video_summaries,
+  )
 
   summary_tensors = summary_tensors.copy()  # pytype: disable=attribute-error  # jax-ndarray
   for summary_dict in (
       scalar_summaries,
       image_summaries,
       audio_summaries,
       video_summaries,
```

## paxml/summary_utils_test.py

```diff
@@ -61,15 +61,15 @@
   def _get_weighted_scalars_and_clu_metrics(
       self,
       use_clu_metrics_instead_of_weighted_scalars: bool,
       values_1: jnp.ndarray,
       weights_1: jnp.ndarray,
       values_2: jnp.ndarray,
       weights_2: jnp.ndarray,
-  ) -> tuple[dict[str, Any], dict[str, Any]]:
+  ) -> tuple[dict[str, Any] | None, dict[str, Any] | None]:
     clu_metrics = None
     weighted_scalars = None
     if use_clu_metrics_instead_of_weighted_scalars:
       clu_metrics = {
           'output_0': base_metrics.WeightedScalarCluMetric.create(
               weight=weights_1, value=values_1
           ),
@@ -82,30 +82,30 @@
           'output_0': (values_1, weights_1),
           'output_1': (values_2, weights_2),
       }
     return clu_metrics, weighted_scalars
 
   def _get_weighted_scalars_and_clu_metrics_1(
       self, use_clu_metrics_instead_of_weighted_scalars: bool
-  ) -> tuple[dict[str, Any], dict[str, Any]]:
+  ) -> tuple[dict[str, Any] | None, dict[str, Any] | None]:
     values_1 = jnp.array([5.0, 6.0])
     weights_1 = jnp.array([3, 2])
     values_2 = jnp.array([4.0, 7.0, 8.0])
     weights_2 = jnp.array([1, 1, 1])
     return self._get_weighted_scalars_and_clu_metrics(
         use_clu_metrics_instead_of_weighted_scalars,
         values_1,
         weights_1,
         values_2,
         weights_2,
     )
 
   def _get_weighted_scalars_and_clu_metrics_2(
       self, use_clu_metrics_instead_of_weighted_scalars: bool
-  ) -> tuple[dict[str, Any], dict[str, Any]]:
+  ) -> tuple[dict[str, Any] | None, dict[str, Any] | None]:
     values_3 = jnp.array([15.0, 16.0])
     weights_3 = jnp.array([13, 12])
     values_4 = jnp.array([14.0, 17.0, 18.0])
     weights_4 = jnp.array([11, 11, 11])
     return self._get_weighted_scalars_and_clu_metrics(
         use_clu_metrics_instead_of_weighted_scalars,
         values_3,
@@ -199,60 +199,60 @@
                   loss=loss_2,
                   weighted_scalars=weighted_scalars_2,
                   summary_tensors=summary_tensors_2,
                   steps_per_sec=steps_per_sec_2,
               )
             summary_handler.close()
     # In this test all summaries use the latest values from step 2.
-    expected_loss = jnp.mean(loss_2)
+    expected_loss = np.mean(loss_2).item()
     mock_tf_summary_scalar.assert_any_call('loss',
                                            MatcherAlmostEqual(expected_loss), 2)
     mock_tf_summary_scalar.assert_any_call('Steps/sec', steps_per_sec_2, 2)
     if use_clu_metrics_instead_of_weighted_scalars:
-      expected_metrics_output_0 = clu_metrics_2['output_0'].compute()
+      expected_metrics_output_0 = clu_metrics_2['output_0'].compute().item()
       mock_tf_summary_scalar.assert_any_call(
           'Metrics/output_0',
           MatcherAlmostEqual(expected_metrics_output_0, 1e-6),
           2,
       )
-      expected_metrics_output_1 = clu_metrics_2['output_1'].compute()
+      expected_metrics_output_1 = clu_metrics_2['output_1'].compute().item()
       mock_tf_summary_scalar.assert_any_call(
           'Metrics/output_1',
           MatcherAlmostEqual(expected_metrics_output_1, 1e-6),
           2,
       )
     else:
-      expected_metrics_output_0_weight = jnp.sum(
+      expected_metrics_output_0_weight = np.sum(
           weighted_scalars_2['output_0'][1]
-      )
+      ).item()
       expected_metrics_output_0 = (
-          jnp.sum(
+          np.sum(
               weighted_scalars_2['output_0'][0]
               * weighted_scalars_2['output_0'][1]
-          )
+          ).item()
           / expected_metrics_output_0_weight
       )
       mock_tf_summary_scalar.assert_any_call(
           'Metrics/output_0',
           MatcherAlmostEqual(expected_metrics_output_0, 1e-6),
           2,
       )
       mock_tf_summary_scalar.assert_any_call(
           'Metrics/output_0-weight',
           MatcherAlmostEqual(expected_metrics_output_0_weight),
           2,
       )
-      expected_metrics_output_1_weight = jnp.sum(
+      expected_metrics_output_1_weight = np.sum(
           weighted_scalars_2['output_1'][1]
-      )
+      ).item()
       expected_metrics_output_1 = (
-          jnp.sum(
+          np.sum(
               weighted_scalars_2['output_1'][0]
               * weighted_scalars_2['output_1'][1]
-          )
+          ).item()
           / expected_metrics_output_1_weight
       )
       mock_tf_summary_scalar.assert_any_call(
           'Metrics/output_1',
           MatcherAlmostEqual(expected_metrics_output_1, 1e-6),
           2,
       )
@@ -260,15 +260,19 @@
           'Metrics/output_1-weight',
           MatcherAlmostEqual(expected_metrics_output_1_weight),
           2,
       )
 
     mock_tf_summary_scalar.assert_any_call(
         'summary_a_scalar',
-        MatcherAlmostEqual(jnp.mean(summary_tensors_2['summary_a_scalar'])), 2)
+        MatcherAlmostEqual(
+            np.mean(summary_tensors_2['summary_a_scalar']).item()
+        ),
+        2,
+    )
     mock_tf_summary_image.assert_any_call(
         'summary_b_image/0',
         MatcherArrayAlmostEqual(np.array(summary_tensors_2['summary_b_image'])),
         2)
     mock_tf_summary_audio.assert_any_call(
         'summary_c_audio/0',
         MatcherArrayAlmostEqual(np.array(summary_tensors_2['summary_c_audio'])),
@@ -384,46 +388,46 @@
       )
       mock_tf_summary_scalar.assert_any_call(
           'Metrics/output_1',
           MatcherAlmostEqual(expected_metrics_output_1, 1e-6),
           2,
       )
     else:
-      expected_metrics_output_0_weight = jnp.sum(
+      expected_metrics_output_0_weight = np.sum(
           weighted_scalars_1['output_0'][1] + weighted_scalars_2['output_0'][1]
-      )
+      ).item()
       expected_metrics_output_0 = (
-          jnp.sum(
+          np.sum(
               weighted_scalars_1['output_0'][0]
               * weighted_scalars_1['output_0'][1]
               + weighted_scalars_2['output_0'][0]
               * weighted_scalars_2['output_0'][1]
-          )
+          ).item()
           / expected_metrics_output_0_weight
       )
       mock_tf_summary_scalar.assert_any_call(
           'Metrics/output_0',
           MatcherAlmostEqual(expected_metrics_output_0, 1e-6),
           2,
       )
       mock_tf_summary_scalar.assert_any_call(
           'Metrics/output_0-weight',
           MatcherAlmostEqual(expected_metrics_output_0_weight),
           2,
       )
-      expected_metrics_output_1_weight = jnp.sum(
+      expected_metrics_output_1_weight = np.sum(
           weighted_scalars_1['output_1'][1] + weighted_scalars_2['output_1'][1]
-      )
+      ).item()
       expected_metrics_output_1 = (
           jnp.sum(
               weighted_scalars_1['output_1'][0]
               * weighted_scalars_1['output_1'][1]
               + weighted_scalars_2['output_1'][0]
               * weighted_scalars_2['output_1'][1]
-          )
+          ).item()
           / expected_metrics_output_1_weight
       )
       mock_tf_summary_scalar.assert_any_call(
           'Metrics/output_1',
           MatcherAlmostEqual(expected_metrics_output_1, 1e-6),
           2,
       )
@@ -434,15 +438,16 @@
       )
 
     summary_a_scalars = [
         summary_tensors_1['summary_a_scalar'],
         summary_tensors_2['summary_a_scalar'],
     ]
     expected_summary_a_scalar = np.mean(
-        [np.array(s) for s in summary_a_scalars])
+        [np.array(s) for s in summary_a_scalars]
+    ).item()
     mock_tf_summary_scalar.assert_any_call(
         'summary_a_scalar', MatcherAlmostEqual(expected_summary_a_scalar), 2)
     mock_tf_summary_image.assert_any_call(
         'summary_b_image/0',
         MatcherArrayAlmostEqual(np.array(summary_tensors_1['summary_b_image'])),
         2)
     mock_tf_summary_image.assert_any_call(
```

## paxml/tasks_lib.py

```diff
@@ -166,25 +166,25 @@
     for item in model_states.opt_states[0].values():  # pytype: disable=attribute-error  # jax-ndarray
       if isinstance(item, tuple):
         for v in item:
           if isinstance(v, dict) and 'ema' in v:
             if extracted is None:
               extracted = v.ema
             else:
-              extracted = jax.tree_map(
+              extracted = jax.tree.map(
                   lambda x, y: y if py_utils.is_optax_masked_node(x) else x,
                   extracted,
                   v.ema,
                   is_leaf=py_utils.is_optax_masked_node,
               )
   if extracted is None:
     raise ValueError(
         'Could not find EMA states in `%r`.' % model_states.opt_states
     )
-  extracted = jax.tree_map(
+  extracted = jax.tree.map(
       lambda x: None if py_utils.is_optax_masked_node(x) else x,
       extracted,
       is_leaf=py_utils.is_optax_masked_node,
   )
 
   def _replace_bprop_masked(x, from_mdl_vars):
     if not py_utils.is_bprop_masked_node(x):
@@ -420,15 +420,15 @@
     return py_utils.is_optax_masked_node(x) or py_utils.is_bprop_masked_node(x)
 
   def _filter_vars_and_get_pspecs(variables, must_include_for_ema=None):
     # must_include_for_ema is a mask indicating if the non-EMA var must be
     # included to be used as the EMA of a bprop-excluded var.
     prefix = py_utils.extract_prefixed_keys_from_nested_map(
         # extract_prefixed_keys_from_nested_map doesn't work with mask nodes.
-        jax.tree_map(
+        jax.tree.map(
             lambda x: True if is_masked(x) else x, variables, is_leaf=is_masked
         ),
         key_separator='.',
     )
     flatten_prefix, treedef = jax.tree_util.tree_flatten(prefix)
     flatten_variable, _ = jax.tree_util.tree_flatten(
         variables, is_leaf=is_masked
@@ -468,28 +468,28 @@
   missing_in_ema = None
   # TODO(nanxinchen): move this to a helper function
   if load_ema_states:
     new_states = []
     new_states_pspecs = []
     vectorized = is_vectorized(ckpt_train_state)
 
-    missing_in_ema = jax.tree_map(
+    missing_in_ema = jax.tree.map(
         lambda _: True, ckpt_train_state.mdl_vars, is_leaf=is_masked
     )
 
     if not vectorized:
       for i, v in enumerate(ckpt_train_state.opt_states[0]):
         if 'ema' not in v:
           new_states.append(v)
           if train_state_pspecs is not None:
             new_states_pspecs.append(train_state_pspecs.opt_states[0][i])
         else:
           filtered_ema, ema_pspecs = _filter_vars_and_get_pspecs(v)
           # is_bprop_masked_node means matched but excluded.
-          missing_in_ema = jax.tree_map(
+          missing_in_ema = jax.tree.map(
               lambda x, y: x and py_utils.is_bprop_masked_node(y),
               missing_in_ema,
               filtered_ema['ema'],
           )
           v = (
               filtered_ema  # pytype: disable=unsupported-operands  # jax-ndarray
           )
@@ -524,15 +524,15 @@
               v = ema_pspecs if update_pspecs else filtered_vars
             return v
 
           new_states0[key] = tuple(update_for_ema(v) for v in item)  # pytype: disable=unsupported-operands  # jax-ndarray
           for v in new_states0[key]:
             if isinstance(v, dict) and 'ema' in v:
               # is_bprop_masked_node means matched but excluded.
-              missing_in_ema = jax.tree_map(
+              missing_in_ema = jax.tree.map(
                   lambda x, y: x and py_utils.is_bprop_masked_node(y),
                   missing_in_ema,
                   v['ema'],
                   is_leaf=is_masked,
               )
           if new_states_pspecs0 is not None:
             new_states_pspecs0[key] = tuple(  # pytype: disable=unsupported-operands  # jax-ndarray
@@ -679,15 +679,15 @@
 
   def _get_spec(shape):
     if shape.shape:
       return jax.sharding.PartitionSpec(None)
     else:
       return jax.sharding.PartitionSpec()
 
-  fully_replicated_state_specs = jax.tree_map(_get_spec, global_shapes)
+  fully_replicated_state_specs = jax.tree.map(_get_spec, global_shapes)
   with restore_global_mesh:
     fully_replicated_gda_model_states = checkpoints.restore_checkpoint(
         global_shapes,
         checkpoint_dir,
         global_mesh=restore_global_mesh,
         checkpoint_type=checkpoint_type,
         state_specs=fully_replicated_state_specs,
@@ -695,21 +695,21 @@
         enforce_restore_shape_check=enforce_restore_shape_check,
         tensorstore_use_ocdbt=tensorstore_use_ocdbt,
         restore_transformations=restore_transformations,
     )
   if global_mesh is not None:
     return fully_replicated_gda_model_states
   if checkpoint_type == CheckpointType.PERSISTENCE:
-    return jax.tree_map(
+    return jax.tree.map(
         py_utils.convert_fully_replicated_array_to_pmap_array,
         fully_replicated_gda_model_states,
     )
   # model_states is jax.Array; we convert back to DA or jax.Array with
   # single device sharding for pmap.
-  return jax.tree_map(
+  return jax.tree.map(
       lambda x: x.addressable_data(0), fully_replicated_gda_model_states
   )
 
 
 class CheckpointLoadingRules(NamedTuple):
   """Utility class for representing how to read the checkpoint.
 
@@ -805,21 +805,21 @@
   """Returns whether each var should be excluded for grad/optimizer."""
   # Skip variables for gradients.
   if learner.bprop_variable_inclusion:
     assert not learner.bprop_variable_exclusion
     included_for_grad = py_utils.match_variable_names(
         var_weight_hparams, learner.bprop_variable_inclusion
     )
-    excluded_for_grad = jax.tree_map(lambda x: not x, included_for_grad)
+    excluded_for_grad = jax.tree.map(lambda x: not x, included_for_grad)
   else:
     excluded_for_grad = py_utils.match_variable_names(
         var_weight_hparams, learner.bprop_variable_exclusion
     )
   if mask_all_overwrite_with_gradient:
-    excluded_for_grad = jax.tree_map(
+    excluded_for_grad = jax.tree.map(
         lambda x, e: base_layer.var_overwrite_with_gradient(x) or e,
         var_weight_hparams,
         excluded_for_grad,
     )
   if mask_all_non_trainable:
     excluded_for_grad = jax.tree_util.tree_map(
         lambda x, e: base_layer.var_not_trainable(x) or e,
@@ -831,15 +831,15 @@
 
 def get_excluded_var_mask_for_opt(
     var_weight_hparams: NestedJTensor,
     learner: learners_lib.Learner,
 ) -> NestedMap:
   """Returns whether each var should be excluded for optimizer."""
   if learner.keep_optimizer_state_for_excluded_vars:
-    return jax.tree_map(lambda _: False, var_weight_hparams)
+    return jax.tree.map(lambda _: False, var_weight_hparams)
   return get_excluded_var_mask_for_grad_or_opt(
       var_weight_hparams,
       learner,
       learner.optimizer.ema_decay == 0.0,
       True,
   )
 
@@ -854,15 +854,15 @@
   )
 
 
 def filter_vars_for_grad_or_opt(
     mdl_vars: NestedMap, excluded_for_grad: NestedMap
 ) -> NestedMap:
   """Filters out vars that should be excluded for grad or optimizer."""
-  return jax.tree_map(
+  return jax.tree.map(
       lambda v, e: py_utils.BpropMaskedNode() if e else v,
       mdl_vars,
       excluded_for_grad,
   )
 
 
 def create_state_partition_specs(
@@ -935,18 +935,19 @@
       return isinstance(x, optax.MaskedState)
 
     def _maybe_unmask_outer_masked_state(x):
       if _is_instance_masked_state(x):
         return x.inner_state
       return x
 
-    opt_var_partition_specs = jax.tree_map(
+    opt_var_partition_specs = jax.tree.map(
         _maybe_unmask_outer_masked_state,
         opt_var_partition_specs,
-        is_leaf=_is_instance_masked_state)
+        is_leaf=_is_instance_masked_state,
+    )
   return TrainState(
       step=step_partition_spec,
       mdl_vars=var_partition_specs,
       opt_states=opt_var_partition_specs,
       extra_state=(),
   )
 
@@ -1046,15 +1047,15 @@
       backward variables.
   """
 
   def _get_shape(var_param):
     shape = tuple(var_param.repeat_prefix or ()) + tuple(var_param.shape)
     return jax.ShapeDtypeStruct(shape, var_param.dtype)
 
-  var_shapes = jax.tree_map(_get_shape, var_weight_hparams)
+  var_shapes = jax.tree.map(_get_shape, var_weight_hparams)
 
   def _create_train_state_from_shape(mdl_vars):
     return create_state(
         mdl_vars, var_weight_hparams, discard_opt_states, learners
     )
 
   return jax.eval_shape(_create_train_state_from_shape, var_shapes)
@@ -1107,15 +1108,15 @@
     unpadded_shape = shape_dtype.shape
     paddings = py_utils.get_uneven_sharding_paddings(
         pspec, unpadded_shape, mesh_shape, mesh_axis_names
     )
     padded_shape = [s + p for (s, p) in zip(unpadded_shape, paddings)]
     return jax.ShapeDtypeStruct(padded_shape, shape_dtype.dtype)
 
-  padded_shapes = jax.tree_map(
+  padded_shapes = jax.tree.map(
       _maybe_pad,
       unpadded_shapes,
       model_state_partition_specs,
       is_leaf=py_utils.is_optax_masked_node,
   )
   return padded_shapes
 
@@ -1646,15 +1647,15 @@
     """
     if self.vn.vn_scale > 0.0:
       names = py_utils.extract_prefixed_keys_from_nested_map(var_weight_hparams)
       regexp = re.compile(self.vn.vn_regex)
 
       # This is the mask of variational noise
       # True: apply vn; False: without vn
-      vn_mask = jax.tree_map(lambda x: bool(regexp.match(x) is not None), names)
+      vn_mask = jax.tree.map(lambda x: bool(regexp.match(x) is not None), names)
 
       # Check if any variables match the rule
       # If vn_scale > 0.0 but none of variables match, throw error
       if not any(jax.tree_util.tree_leaves(vn_mask)):
         raise RuntimeError('Variational noise is enabled but rules don\'t '
                            'match any variables. Please disable vn by specify'
                            ' vn.vn_scale = 0. or check vn.vn_regex. One common'
@@ -1673,16 +1674,16 @@
           return params + self.vn.vn_scale * jax.random.normal(
               shape=params.shape, key=rng
           )
         else:
           return params
 
       # VN only updates trainable part and copy non-trainable
-      ret = jax.tree_map(add_vn, mdl_vars, rng_tree, vn_mask)
-      return jax.tree_map(
+      ret = jax.tree.map(add_vn, mdl_vars, rng_tree, vn_mask)
+      return jax.tree.map(
           lambda x, y: jnp.where(step >= self.vn.vn_start_step, x, y),
           ret,
           mdl_vars,
       )
     else:
       return mdl_vars
 
@@ -1776,16 +1777,17 @@
     input_specs_provider_p = rules.input_specs_provider_p
     input_specs_provider = instantiate(input_specs_provider_p)
     inputs_shape_dtype = input_specs_provider.get_input_specs()
     # TODO(pax-dev): Add better/cleaner API to identify pmap vs. pjit models
     # (and check for dcn_mesh_shape too).
     if (hasattr(ckpt_task.model, 'ici_mesh_shape') and
         ckpt_task.model.ici_mesh_shape is not None):
-      inputs_shape_dtype = jax.tree_map(py_utils.get_global_input_shape_dtype,
-                                        inputs_shape_dtype)
+      inputs_shape_dtype = jax.tree.map(
+          py_utils.get_global_input_shape_dtype, inputs_shape_dtype
+      )
     # Initialize with a dummy seed
     var_weight_hparams = ckpt_task.model.abstract_init_with_metadata(
         inputs_shape_dtype)
     ckpt_train_state = ckpt_task.create_train_state_padded_shapes(
         var_weight_hparams)
     train_state_pspecs = ckpt_task.create_train_state_partition_specs(
         var_weight_hparams)
```

## paxml/tasks_lib_test.py

```diff
@@ -14,14 +14,15 @@
 # limitations under the License.
 
 """Unit tests for tasks_lib."""
 
 from __future__ import annotations
 
 import re
+from typing import Any
 
 from absl.testing import absltest
 from absl.testing import parameterized
 import flax
 import jax
 import jax.numpy as jnp
 import numpy as np
@@ -45,14 +46,17 @@
 
 
 CheckpointType = checkpoints.CheckpointType
 NestedMap = py_utils.NestedMap
 WeightHParams = base_layer.WeightHParams
 JTensor = pytypes.JTensor
 WeightInit = base_layer.WeightInit
+Metrics = pytypes.Metrics
+WeightedScalars = pytypes.WeightedScalars
+Predictions = JTensor | NestedMap | dict[str, Any] | dict[int, Any]
 
 PMAP_PARALLEL_AXIS_NAME = base_layer.PMAP_PARALLEL_AXIS_NAME
 
 RANDOM = base_layer.RANDOM
 PARAMS = base_layer.PARAMS
 OVERWRITE_WITH_GRADIENT = (
     base_layer.WeightHParamsCollection.OVERWRITE_WITH_GRADIENT
@@ -84,16 +88,17 @@
     )
 
 
 class LMInputSpecsProvider(base_input.BaseInputSpecsProvider):
   """Class to provide input specs for model initialization."""
 
   def get_input_specs(self):
-    return jax.tree_map(lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),
-                        get_model_inputs())
+    return jax.tree.map(
+        lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype), get_model_inputs()
+    )
 
 
 class TestModel01(base_model.BaseModel):
   """Simple model for testing.
 
   Attributes:
     input_dims: Depth of the input.
@@ -111,16 +116,16 @@
   def compute_predictions(self, input_batch: NestedMap) -> JTensor:
     ret = jnp.einsum('bi,io->bo', input_batch.inputs, self.theta.var01)
     self.add_summary('debug', ret, verbosity=4)
     self.add_summary('info', ret, verbosity=3)
     return ret
 
   def compute_loss(
-      self, predictions: JTensor, input_batch: NestedMap
-  ) -> tuple[NestedMap, NestedMap]:
+      self, predictions: Predictions, input_batch: NestedMap
+  ) -> tuple[WeightedScalars | Metrics, dict[str, Any]]:
     del input_batch
     loss = jnp.sum(predictions)
     loss02 = jnp.max(jnp.abs(self.theta.var01))
     # Here loss is the main loss to back-prop into, and loss02 is an eval
     # metric.
     per_example_out = NestedMap()
     return NestedMap(
@@ -150,16 +155,16 @@
         base_layer.WeightHParams(shape=[self.input_dims, self.output_dims]),
     )
 
   def compute_predictions(self, input_batch: NestedMap) -> JTensor:
     return self.repeated_ffn(input_batch.inputs)
 
   def compute_loss(
-      self, predictions: JTensor, input_batch: NestedMap
-  ) -> tuple[NestedMap, NestedMap]:
+      self, predictions: Predictions, input_batch: NestedMap
+  ) -> tuple[WeightedScalars | Metrics, dict[str, Any]]:
     del input_batch
     loss = jnp.sum(predictions)
     per_example_out = NestedMap()
     return NestedMap(loss=(loss, jnp.array(1.0, loss.dtype))), per_example_out
 
 
 class TestModel03(base_model.BaseModel):
@@ -183,16 +188,16 @@
     )
 
   def compute_predictions(self, input_batch: NestedMap) -> JTensor:
     x = jnp.einsum('bi,io->bo', input_batch.inputs, self.theta.var01)
     return jnp.einsum('bo,oi->bi', x, self.theta.var02)
 
   def compute_loss(
-      self, predictions: JTensor, input_batch: NestedMap
-  ) -> tuple[NestedMap, NestedMap]:
+      self, predictions: Predictions, input_batch: NestedMap
+  ) -> tuple[WeightedScalars | Metrics, dict[str, Any]]:
     del input_batch
     loss = jnp.sum(predictions)
     loss02 = jnp.max(jnp.abs(self.theta.var01))
     # Here loss is the main loss to back-prop into, and loss02 is an eval
     # metric.
     per_example_out = NestedMap()
     return (
@@ -235,16 +240,16 @@
     ret = (
         jnp.einsum('bi,io->bo', input_batch.inputs, self.theta.var_normal)
         + input_batch.inputs[0][0] * self.theta.var_owg[0]
     )
     return ret
 
   def compute_loss(
-      self, predictions: JTensor, input_batch: NestedMap
-  ) -> tuple[NestedMap, NestedMap]:
+      self, predictions: Predictions, input_batch: NestedMap
+  ) -> tuple[WeightedScalars | Metrics, dict[str, Any]]:
     loss = jnp.sum(predictions)
     per_example_out = NestedMap()
     return NestedMap(loss=(loss, jnp.array(1.0, loss.dtype))), per_example_out
 
 
 class BaseTaskTest(test_utils.TestCase):
 
@@ -820,15 +825,15 @@
         ),
     }
     task = instantiate(task_p)
 
     partitioner = partitioning.create_partitioner(
         task,
         reshard_inputs=False,
-        auto_sharding_mode=True,
+        auto_sharding_mode=trainer_lib.RunningMode.DECODE,
     )
     prng_key = jax.random.PRNGKey(1)
     partitioner.setup(task, prng_key, sample_inputs)
     if load_from_cp_train_state:
       train_state = trainer_lib.initialize_replicate_model_state(
           task, prng_key, sample_inputs
       )
@@ -844,14 +849,15 @@
 
     # train_state_provenance should be None if restoring train_state
     # from an existing checkpoint i.e. in the case of preemption
     if load_from_cp_train_state:
       self.assertIsNone(train_state_provenance)
       return
 
+    assert train_state_provenance is not None
     var_provenance_serialized = flax.serialization.to_state_dict(
         train_state_provenance.mdl_vars
     )
     opt_provenance_serialized = flax.serialization.to_state_dict(
         train_state_provenance.opt_states
     )
     mdl_var01 = (
```

## paxml/train.py

```diff
@@ -13,15 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Training loop for Pax model."""
 
 import contextlib
 import typing
-from typing import Type
+from typing import Callable, Type
 
 from absl import logging
 from etils import epath
 import jax
 import jax.numpy as jnp
 from paxml import base_experiment
 from paxml import checkpoint_creators
@@ -89,14 +89,18 @@
     )
     job_log_dir.mkdir(parents=True, exist_ok=True)
 
     cls_vars_summary = experiment_utils.get_cls_vars_summary(exp_cls)
     exp_summary_fpath.write_text(cls_vars_summary)
 
 
+def _no_op() -> None:
+  return
+
+
 @py_utils.benchmark('[PAX STATUS]: ')
 def train_and_evaluate(
     experiment_config: base_experiment.BaseExperiment,
     job_log_dir: epath.PathLike,
     maybe_use_persistence_checkpointing: bool,
     eval_on_test: bool | None,
     checkpoint_todelete_subdir: str | None = None,
@@ -107,14 +111,15 @@
     enable_checkpoint_saving: bool = True,
     enforce_restore_shape_check: bool = False,
     tensorstore_use_ocdbt: bool = False,
     exit_after_ondemand_checkpoint: bool = False,
     override_num_train_steps: int | None = None,
     enable_summary_writer: bool = True,
     async_timeout_secs: int | None = None,
+    train_first_result_callback_fn: Callable[[], None] = _no_op,
 ) -> None:
   """The shared path to run the training and evaluation loop.
 
   Args:
     experiment_config: an instance of BaseExperiment for the experiment to train
       and evaluate.
     job_log_dir: The directory for the job logs.
@@ -143,14 +148,16 @@
       on-demand checkpoint due to preemption.
     override_num_train_steps: If not None, it will override the num_train_steps
       defined in the train task.
     enable_summary_writer: If false, a noop summary writer will be used by the
       program. Therefore, it will not generate summary files.
     async_timeout_secs: Timeout in seconds for asynchronous save operations.
       `None` indicates that no timeout is set.
+    train_first_result_callback_fn: An optional callable function for reporting
+      the time of the first step
   """
   jax.monitoring.record_event('/jax/pax/train_and_evaluate/beacon')
   task_p = experiment_config.task()
   task_p = typing.cast(pax_fiddle.Config[tasks_lib.SingleTask], task_p)
 
   if override_num_train_steps:
     logging.info(
@@ -233,14 +240,18 @@
     specs_provider_p.input_p = partitioner.preprocess_input_config(
         specs_provider_p.input_p
     )
 
   # Creates the train/eval/decode programs.
   logging.info('[PAX STATUS]: Initializing train program.')
   train_program = experiment_config.train_programs()[0]
+  if train_first_result_callback_fn:
+    train_program.register_first_result_callback_fn(
+        train_first_result_callback_fn
+    )
 
   logging.info('[PAX STATUS]: Initializing eval programs.')
   eval_programs = []
   if (
       eval_on_test
       and task_p.train.eval_interval_steps is not None
       and task_p.train.eval_interval_steps > 0
```

## paxml/train_states.py

```diff
@@ -117,11 +117,11 @@
   provenance = TensorProvenance()
   if checkpoint_path:
     provenance = TensorProvenance(
         checkpoint_path=checkpoint_path, checkpoint_step=step
     )
   return TrainStateProvenance(
       step=provenance,
-      mdl_vars=jax.tree_map(lambda x: provenance, train_state.mdl_vars),
-      opt_states=jax.tree_map(lambda x: provenance, train_state.opt_states),
-      extra_state=jax.tree_map(lambda x: provenance, train_state.extra_state),
+      mdl_vars=jax.tree.map(lambda x: provenance, train_state.mdl_vars),
+      opt_states=jax.tree.map(lambda x: provenance, train_state.opt_states),
+      extra_state=jax.tree.map(lambda x: provenance, train_state.extra_state),
   )
```

## paxml/trainer_lib.py

```diff
@@ -389,21 +389,21 @@
   @jax.jit
   def init_fn(init_key):
     context_p = base_layer.JaxContext.HParams(
         do_eval=is_eval_for_init,
         summary_verbosity=jax_task.summary_verbosity,
     )
     with base_layer.JaxContext.new_context(hparams=context_p):
-      inputs = jax.tree_map(jnp.zeros_like, inputs_shape_dtype)
+      inputs = jax.tree.map(jnp.zeros_like, inputs_shape_dtype)
       if model.hparams.fprop_dtype == jnp.bfloat16:
-        inputs = jax.tree_map(_maybe_to_bfloat16, inputs)
+        inputs = jax.tree.map(_maybe_to_bfloat16, inputs)
       return model.init(init_key, inputs)
 
   initial_vars = init_fn(init_key)
-  logging.info('initial_vars: %s', jax.tree_map(jnp.shape, initial_vars))
+  logging.info('initial_vars: %s', jax.tree.map(jnp.shape, initial_vars))
 
   # In case jax_task.model wraps a t5x model, let's remove the params_axes
   # variable collection.
   if 'params_axes' in initial_vars:
     del initial_vars['params_axes']
   train_state = jax_task.create_train_state(
       initial_vars, var_weight_hparams, discard_opt_states
@@ -442,15 +442,15 @@
   def _replicate(state):
     # Skip the copy if it's already replicated.
     if isinstance(state, jax.Array) and len(state.devices()) != 1:
       return state
     else:
       return jax.device_put_replicated(state, jax.local_devices())
 
-  return jax.tree_map(_replicate, model_states)
+  return jax.tree.map(_replicate, model_states)
 
 
 # TODO(laigd): remove this since it's used only by tests.
 def initialize_replicate_model_state(
     jax_task: tasks_lib.SingleTask,
     prng_key: PRNGKey,
     inputs_shape_dtype: NestedShapeDtypeLike,
@@ -825,15 +825,15 @@
       mdl_vars: NestedJTensor, inputs: NestedMap, prng_key: PRNGKey
   ) -> tuple[JTensor, sgf.GradAuxInfo]:
     """Computes loss as well as other auxiliary outputs."""
     if fprop_dtype == jnp.float32:
       pass
     elif fprop_dtype == jnp.bfloat16:
       mdl_vars = _maybe_to_bfloat16_vars(mdl_vars, var_weight_hparams)
-      inputs = jax.tree_map(_maybe_to_bfloat16, inputs)
+      inputs = jax.tree.map(_maybe_to_bfloat16, inputs)
     else:
       assert NotImplementedError(f'fprop_dtype {fprop_dtype} not supported.')
 
     with base_layer.JaxContext.new_context(hparams=context_p):
       k1, k2, k3 = jax.random.split(prng_key, 3)
       (metrics, per_example_output), updated_vars = apply_fn(
           model=jax_task.model,
@@ -932,38 +932,38 @@
       mdl_vars: NestedJTensor,
       inputs: NestedMap,
       prng_key: PRNGKey,
   ):
     with_grad = tasks_lib.filter_vars_for_grad_or_opt(
         mdl_vars, excluded_for_grad
     )
-    no_grad = jax.tree_map(
+    no_grad = jax.tree.map(
         lambda x, e: x if e else {}, mdl_vars, excluded_for_grad
     )
 
     def _loss(
         mdl_vars_grad: NestedJTensor,
         mdl_vars_nograd_and_inputs: tuple[NestedJTensor, NestedMap],
         prng_key: PRNGKey,
     ):
       mdl_vars_nograd, inputs = mdl_vars_nograd_and_inputs
-      merged_vars = jax.tree_map(
+      merged_vars = jax.tree.map(
           lambda e, x, y: y if e else x,
           excluded_for_grad,
           mdl_vars_grad,
           mdl_vars_nograd,
       )
       return loss_fn(merged_vars, inputs, prng_key)
 
     if learner.stochastic_gradient is None:
       g = jax.value_and_grad(_loss, has_aux=True, allow_int=True)
     else:
       g = functools.partial(learner.stochastic_gradient.grad_fn, _loss)
     values, grads = g(with_grad, (no_grad, inputs), prng_key)
-    grads = jax.tree_map(
+    grads = jax.tree.map(
         lambda eo, eg, m, g: jnp.zeros_like(m) if eg and not eo else g,
         excluded_for_opt,
         excluded_for_grad,
         mdl_vars,
         grads,
     )
     return values, grads
@@ -1149,30 +1149,30 @@
     mdl_vars = states.mdl_vars.copy()  # pytype: disable=attribute-error  # jax-ndarray
     if (
         expose_updated_nontrainables_to_learner
         and NON_TRAINABLE in fwd_updated_vars
     ):
       # Make updated non-trainable vars visible to EMA.
       mdl_vars[NON_TRAINABLE] = fwd_updated_vars[NON_TRAINABLE]
-    excluded_for_learner = jax.tree_map(
+    excluded_for_learner = jax.tree.map(
         lambda eo, eg: eo and eg, excluded_for_opt, excluded_for_grad
     )
     vars_with_opt = tasks_lib.filter_vars_for_grad_or_opt(
         mdl_vars, excluded_for_learner
     )
     wps_with_opt = tasks_lib.filter_vars_for_grad_or_opt(
         var_weight_hparams, excluded_for_learner
     )
     transformed_grads, new_opt_states = learner.update_states(
         grads, states.opt_states[0], vars_with_opt, wps_with_opt
     )
     vars_with_opt = learner.apply_gradient(
         vars_with_opt, transformed_grads, wps_with_opt
     )
-    mdl_vars = jax.tree_map(
+    mdl_vars = jax.tree.map(
         lambda e, old, new: old if e else new,
         excluded_for_grad,
         mdl_vars,
         vars_with_opt,
     )
 
     for collection in [NON_TRAINABLE] + NON_PAX_VAR_COLLECTION:
@@ -1184,15 +1184,15 @@
         mdl_vars[collection] = _maybe_synchronize_non_learnable_vars(
             states.mdl_vars[collection],
             fwd_updated_vars[collection],
             var_weight_hparams[collection],
         )
 
     # We may have updated non-trainable vars that have been explicitly excluded.
-    mdl_vars = jax.tree_map(
+    mdl_vars = jax.tree.map(
         lambda e, old, new: old if e else new,
         # Filter out only the explicitly masked non-trainables.
         tasks_lib.get_excluded_var_mask_for_grad_or_opt(
             var_weight_hparams, learner, mask_all_non_trainable=False
         ),
         states.mdl_vars,
         mdl_vars,
@@ -1302,15 +1302,15 @@
         do_eval=not jax_task.hparams.train.always_use_train_for_model_init,
     )
 
   if fprop_dtype == jnp.float32:
     pass
   elif fprop_dtype == jnp.bfloat16:
     mdl_vars = _maybe_to_bfloat16_vars(mdl_vars, var_weight_hparams)
-    inputs = jax.tree_map(_maybe_to_bfloat16, inputs)
+    inputs = jax.tree.map(_maybe_to_bfloat16, inputs)
   else:
     assert NotImplementedError(f'fprop_dtype {fprop_dtype} not supported.')
 
   enum_keys, inputs = py_utils.filter_by_matching_keys(
       inputs, [py_utils.PROVENANCE_PREFIX]
   )
   with base_layer.JaxContext.new_context(hparams=context_p):
@@ -1358,15 +1358,15 @@
   if fprop_dtype == jnp.bfloat16:
     (
         mean_loss,
         aggregated_scalars,
         per_example_out,
         aggregated_summaries,
         aggregated_clu_metrics,
-    ) = jax.tree_map(
+    ) = jax.tree.map(
         _maybe_to_float32,
         (
             mean_loss,
             aggregated_scalars,
             per_example_out,
             aggregated_summaries,
             aggregated_clu_metrics,
@@ -1415,15 +1415,15 @@
   prng_key = jax.random.fold_in(prng_key, states.step)  # pytype: disable=wrong-arg-types  # jax-ndarray
   mdl_vars = states.mdl_vars
 
   assert not states.opt_states
 
   if fprop_dtype == jnp.bfloat16:
     mdl_vars = _maybe_to_bfloat16_vars(mdl_vars, var_weight_hparams)
-    inputs = jax.tree_map(_maybe_to_bfloat16, inputs)
+    inputs = jax.tree.map(_maybe_to_bfloat16, inputs)
   elif fprop_dtype != jnp.float32:
     assert NotImplementedError(f'fprop_dtype {fprop_dtype} not supported.')
 
   enum_keys, inputs = py_utils.filter_by_matching_keys(
       inputs, [py_utils.PROVENANCE_PREFIX]
   )
   with base_layer.JaxContext.new_context(hparams=context_p):
@@ -1452,15 +1452,15 @@
     # merge back, if any, enum keys for eval matching
     per_example_out = outputs[1]
     if decode_method == 'decode':
       per_example_out.update(enum_keys)
 
     summary_tensors = updated_vars.get(base_layer.SUMMARIES, {})
     if summary_tensors:
-      summary_tensors = jax.tree_map(_maybe_to_float32, summary_tensors)
+      summary_tensors = jax.tree.map(_maybe_to_float32, summary_tensors)
       updated_vars[base_layer.SUMMARIES] = summary_tensors
 
     return outputs, updated_vars
 
 
 def _decode_step_for_partitioner(
     task,
@@ -1556,15 +1556,15 @@
     var_weight_hparams = model.abstract_init_with_metadata(
         global_input_shapes, do_eval=is_eval
     )
 
   train_state_partition_specs = (
       state_specs.to_eval_state() if discard_opt_states else state_specs
   )
-  train_state_unpadded_shapes = jax.tree_map(
+  train_state_unpadded_shapes = jax.tree.map(
       jnp.shape,
       jax_task.create_train_state_unpadded_shapes(
           var_weight_hparams, discard_opt_states
       ),
   )
   assert train_state_partition_specs is not None
 
@@ -1593,19 +1593,19 @@
   asserts.assert_same_structure(
       train_state_unpadded_shapes, train_state_partition_specs
   )
 
   mesh_names = model.hparams.mesh_axis_names
   prng_key_partition_spec = base_layer.to_partition_spec((None,), mesh_names)
 
-  prng_key_shardings = jax.tree_map(
+  prng_key_shardings = jax.tree.map(
       lambda p: jax.sharding.NamedSharding(global_mesh, p),
       prng_key_partition_spec,
   )
-  train_state_shardings = jax.tree_map(
+  train_state_shardings = jax.tree.map(
       lambda p: jax.sharding.NamedSharding(global_mesh, p),
       train_state_partition_specs,
   )
 
   init_fn = pjit.pjit(
       init_model_from_seed,
       in_shardings=prng_key_shardings,
@@ -1698,19 +1698,19 @@
     input_config: a Fiddle config parameterizing a BaseInput.
 
   Returns:
     A tuple consisting of the per-host input shapes and the global input shapes.
   """
   sample_inputs = instantiate(input_config).get_next_padded()
 
-  perhost_inputs_shape_dtype = jax.tree_map(
+  perhost_inputs_shape_dtype = jax.tree.map(
       lambda x: jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype),
       sample_inputs,
   )
-  global_inputs_shape_dtype = jax.tree_map(
+  global_inputs_shape_dtype = jax.tree.map(
       py_utils.get_global_input_shape_dtype, sample_inputs
   )
   return perhost_inputs_shape_dtype, global_inputs_shape_dtype
 
 
 def get_input_partition_specs(mesh_axis_names, inputs_shape_dtype):
   logging.info(
@@ -1772,12 +1772,12 @@
   logging.info(
       'Spec yielded by InputSpecProvider for model init: %s',
       pprint.pformat(train_input_specs),
   )
 
   # All pjit models specify at least the ICI mesh shape.
   if task.model.mesh_shape is not None:
-    train_input_specs = jax.tree_map(
+    train_input_specs = jax.tree.map(
         py_utils.get_global_input_shape_dtype, train_input_specs
     )
 
   return train_input_specs
```

## paxml/trainer_lib_test.py

```diff
@@ -12,14 +12,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for trainer_lib."""
 
 import itertools
+from typing import Any
 
 from absl.testing import absltest
 from absl.testing import parameterized
 from etils import epath
 import jax.numpy as jnp
 from paxml import tasks_lib
 from paxml import trainer_lib
@@ -31,14 +32,17 @@
 from praxis import py_utils
 from praxis import pytypes
 from praxis import schedules
 
 
 NestedMap = py_utils.NestedMap
 JTensor = pytypes.JTensor
+Metrics = pytypes.Metrics
+WeightedScalars = pytypes.WeightedScalars
+Predictions = JTensor | NestedMap | dict[str, Any] | dict[int, Any]
 
 
 class RunningModeTest(parameterized.TestCase):
 
   def test_unknown_mode(self):
     self.assertEqual(
         trainer_lib.RunningMode.detect(False, False, False),
@@ -92,16 +96,16 @@
   def compute_predictions(self, input_batch: NestedMap) -> JTensor:
     ret = jnp.einsum('bi,io->bo', input_batch.inputs, self.theta.weights)
     self.add_summary('debug', ret, verbosity=4)
     self.add_summary('info', ret, verbosity=3)
     return ret
 
   def compute_loss(
-      self, predictions: JTensor, input_batch: NestedMap
-  ) -> tuple[NestedMap, NestedMap]:
+      self, predictions: Predictions, input_batch: NestedMap
+  ) -> tuple[WeightedScalars | Metrics, dict[str, Any]]:
     del input_batch
     prediction_loss = jnp.sum(predictions)
     theta_loss = jnp.max(jnp.abs(self.theta.weights))
     # Here loss is the main loss to back-prop into, and loss02 is an eval
     # metric.
     per_example_out = NestedMap()
     return (
```

## paxml/tuning_lib_test.py

```diff
@@ -49,14 +49,15 @@
       if step <= global_step and (
           i == len(self.threshold) - 1 or self.threshold[i + 1][0] > global_step
       ):
         return value
     return 0.0
 
   def __call__(self, metrics_dict: dict[str, float], global_step: int) -> float:
+    assert self.metric is not None
     reward = self.metric.get_value(metrics_dict)
     if reward < self.get_threshold(global_step):
       if self.skip:
         raise automl.EarlyStoppingError(
             skip=self.skip,
             skip_reason='Trial skipped due to lower metric value.',
             step=global_step,
@@ -901,14 +902,15 @@
   def test_tune_always_aggregate_train_metrics(
       self,
       test_paxml_early_stop_always_aggregate_train_metrics,
       expected_tuning_metrics_keys,
   ):
     mock_early_stop_fn = mock.Mock()
 
+    checkpoint_path = epath.Path('/foo')
     with mock.patch.dict(
         os.environ,
         {
             'PAXML_EARLY_STOP_ALWAYS_AGGREGATE_TRAIN_METRICS': (
                 test_paxml_early_stop_always_aggregate_train_metrics
             )
         },
@@ -919,23 +921,23 @@
           train_steps_per_sec=2.5,
           train_weighted_scalars={
               'metric_0': (jax.numpy.array([0, 1]), jax.numpy.array([1, 0]))
           },
           is_last_ckpt=False,
           eval_metrics=None,
           decode_metrics=None,
-          checkpoint_path='/foo',
+          checkpoint_path=checkpoint_path,
       )
 
     mock_early_stop_fn.assert_called_once_with(
         mock.ANY,
         trainer_lib.RunningMode.TRAIN,
         10,
         False,
-        '/foo',
+        checkpoint_path,
     )
     self.assertEqual(
         set(mock_early_stop_fn.call_args.args[0].keys()),
         set(expected_tuning_metrics_keys),
     )
```

## paxml/contrib/gpu/scripts_gpu/configs.py

```diff
@@ -15,26 +15,28 @@
 
 """Configurations for GPU models."""
 
 import fiddle as fdl
 import jax.numpy as jnp
 from paxml import experiment_registry
 from paxml import tasks_lib
+from paxml.contrib.gpu.scripts_gpu.llama_utils import BaseLLaMA
+from paxml.contrib.gpu.scripts_gpu.tasks import BoolQDataset
 from paxml.contrib.gpu.scripts_gpu.tasks import LambadaDataset
 from paxml.contrib.gpu.scripts_gpu.tasks import PileUnsupervisedDataset
 from paxml.tasks.lm.params.c4 import TransformerLmSpmdAdam
 from paxml.tasks.lm.params.lm_cloud import SyntheticDataset
 from praxis import base_layer
 from praxis import layers
 from praxis import optimizers
 from praxis import pax_fiddle
 from praxis import schedules
+from praxis.contrib.gpu.scripts_gpu.models import CustomMetricsLM
 from praxis.layers import transformers
 
-
 WeightInit = base_layer.WeightInit
 
 GPT_EOS_ID = 1
 
 
 ## from https://github.com/google/paxml/commit/9b5682019806dcb058b82ec2f122aa30ed51f255
 def configure_gpt3_task(
@@ -95,14 +97,16 @@
 
   return task_p
 
 
 ## 8 node
 class GPT126MBase(TransformerLmSpmdAdam):
 
+  MODEL_CLASS = CustomMetricsLM
+
   USE_REPEATED_LAYER = False
   ICI_MESH_SHAPE = [8, 1, 1]
   DCN_MESH_SHAPE = [8, 1, 1]
   FPROP_DTYPE = jnp.bfloat16
   MAX_STEPS = 600000
 
   MAX_SEQ_LEN = 2048
@@ -144,14 +148,17 @@
 
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     self.TRAINABLE_PE_MAX_SEQ_LEN = self.MAX_SEQ_LEN
 
     task_p = super().task()
     task_p = configure_gpt3_task(self, task_p)
     task_p.train.num_train_steps = self.MAX_STEPS
+    task_p.train.compute_steps_per_sec_interval_steps = (
+        self.SUMMARY_INTERVAL_STEPS
+    )
 
     model_p = task_p.model
 
     ### compute layernorm reductions in fp32. Needed for stable training on GPUs
     stacked_p = model_p.lm_tpl.stacked_transformer_tpl
     if fdl.get_callable(stacked_p) == transformers.PipelinedTransformer:
       stacked_p = stacked_p.pipeline_stage
@@ -329,14 +336,90 @@
 class Lambada126M(GPT126MBase, LambadaDataset):
 
   ICI_MESH_SHAPE = [8, 1, 1]
 
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     task_p = super().task()
     task_p.train.always_use_train_for_model_init = False
-    task_p.model.report_strict_acc = True
+    task_p.model.eval_task = 'lambada'
     return task_p
 
 
 ### legacy aliases
 GPT5B = Pile5B
 GPT175B = Pile175B
+
+
+@experiment_registry.register
+class LLaMA7B(BaseLLaMA, BoolQDataset):
+  """7B model on a A100-40GB.
+
+  Checkpoint:
+  gs://sax-data/pax-llama/7B/checkpoint_00000000/
+
+  April 14, 2023
+  Latency = 3.619s with 128 decoded tokens. 27ms per output token
+  """
+
+  NUM_LAYERS = 32
+  VOCAB_SIZE = 32000
+  DIMS_PER_HEAD = 128
+  NUM_HEADS = 32
+  MODEL_DIMS = 4096
+  HIDDEN_DIMS = 11008
+
+  PERCORE_BATCH_SIZE = 16
+
+  ICI_MESH_SHAPE = [1, 8, 1]
+  DCN_MESH_SHAPE = [1, 1, 1]
+
+  def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
+
+    task_p = super().task()
+    task_p.train.always_use_train_for_model_init = False
+    task_p.model.apply_eval_sample_weights = True
+    task_p.model.eval_task = 'boolq'
+    task_p.model.boolq_yn_tokens = jnp.array(
+        [self.TRUE_TOKEN, self.FALSE_TOKEN]
+    )
+
+    return task_p
+
+
+@experiment_registry.register
+class LLaMA13B(LLaMA7B):
+  """13B model on a A100-40GB.
+
+  April 12, 2023
+  Latency = 5.06s with 128 decoded tokens. 38ms per output token.
+  """
+
+  NUM_LAYERS = 40
+  VOCAB_SIZE = 32000
+  DIMS_PER_HEAD = 128
+  NUM_HEADS = 40
+  MODEL_DIMS = 5120
+  HIDDEN_DIMS = 13824
+
+  PERCORE_BATCH_SIZE = 8
+
+  ICI_MESH_SHAPE = [1, 8, 1]
+  DCN_MESH_SHAPE = [1, 1, 1]
+
+
+@experiment_registry.register
+class LLaMA70B(LLaMA7B):
+  """LlaMA-2 70B model on TPUv5-16."""
+
+  NUM_LAYERS = 80
+  VOCAB_SIZE = 32000
+  DIMS_PER_HEAD = 128
+  NUM_HEADS = 64
+  MODEL_DIMS = 8192
+  HIDDEN_DIMS = 28672
+  USE_MQA = True
+  NUM_KV_HEADS = 8
+
+  PERCORE_BATCH_SIZE = 4
+
+  ICI_MESH_SHAPE = [1, 8, 1]
+  DCN_MESH_SHAPE = [1, 2, 1]
```

## paxml/contrib/gpu/scripts_gpu/tasks.py

```diff
@@ -24,14 +24,15 @@
 from paxml.contrib.gpu.scripts_gpu import tfds_pile
 from paxml.tasks.lm.params.c4 import TaskRegistry
 from praxis import base_input
 from praxis import pax_fiddle
 import seqio
 import t5.data
 from t5.data import preprocessors as t5_preprocessors
+import tensorflow as tf
 
 ### for now, make sure to set 'VOCAB_PATH' as an environment variable in your bash script
 vocab_path = os.getenv('VOCAB_PATH', None)
 assert (
     vocab_path is not None and vocab_path != ''
 ), 'Make sure to set VOCAB_PATH as an environment variable'
 vocab = t5.data.SentencePieceVocabulary(vocab_path)
@@ -75,14 +76,48 @@
         seqio.preprocessors.tokenize,
     ],
     output_features=LAMBADA_OUTPUT_FEATURES,
     metric_fns=[],
     shuffle_buffer_size=None,
 )
 
+BOOLQ_OUTPUT_FEATURES = {
+    'inputs': seqio.Feature(vocabulary=vocab, add_eos=False),
+    'targets': seqio.Feature(vocabulary=vocab, add_eos=False),
+}
+
+
+def concatenate_passage_and_question(dataset):
+  @seqio.map_over_dataset
+  def _my_fn(x):
+    inputs = x['passage'] + '\nquestion: ' + x['question'] + '?\nanswer:'
+
+    @tf.function
+    def yesno(label):
+      return tf.cond(label > 0, true_fn=lambda: 'yes', false_fn=lambda: 'no')
+
+    return {'inputs': inputs, 'targets': yesno(x['label'])}
+
+  return _my_fn(dataset)
+
+
+TaskRegistry.add_versioned_tfds_task(
+    'boolq_eval',
+    versions=['1.0.2'],
+    pinned_version='1.0.2',
+    tfds_name='super_glue/boolq',
+    tfds_data_dir=None,
+    preprocessors=[
+        concatenate_passage_and_question,
+        seqio.preprocessors.tokenize,
+    ],
+    output_features=BOOLQ_OUTPUT_FEATURES,
+    metric_fns=[],
+    shuffle_buffer_size=None,
+)
 
 class PileUnsupervisedDataset(base_experiment.BaseExperiment):
   """Used for training Baseline ULM."""
 
   PERCORE_BATCH_SIZE = 1
   MAX_SEQ_LEN = 2048
   TRAIN_INPUT_RANDOM_SEED = None
@@ -170,14 +205,71 @@
         ),
         is_training=is_training,
         input_random_seed=4321,
         batch_size=batch_size_per_process,
         num_infeed_hosts=num_infeed_hosts,
         reset_for_eval=False if is_training else True,
         shuffle=False,
+        eval_loop_num_batches=-1,
+    )
+    return p
+
+  def datasets(self) -> list[pax_fiddle.Config[base_input.BaseInput]]:
+    """Returns a list of dataset parameters."""
+    return [self._dataset_common(is_training=False)]
+
+
+class BoolQDataset(base_experiment.BaseExperiment):
+  """Used for zero-shot eval."""
+
+  PERCORE_BATCH_SIZE: int = 8
+  MAX_SEQ_LEN: int = 4096
+  BOS_ID: int = 1
+  EOS_ID: int = 2
+
+  s = seqio.SentencePieceVocabulary(vocab_path)
+  TRUE_TOKEN: int = s.encode('yes')
+  FALSE_TOKEN: int = s.encode('no')
+
+  def _dataset_common(
+      self, is_training
+  ) -> pax_fiddle.Config[base_input.BaseInput]:
+    num_local_devices = jax.local_device_count()
+    if self.PERCORE_BATCH_SIZE >= 1:
+      batch_size_per_process = int(self.PERCORE_BATCH_SIZE * num_local_devices)
+      num_infeed_hosts = jax.process_count()
+    else:
+      global_batch_size = int(
+          self.PERCORE_BATCH_SIZE * num_local_devices * jax.process_count()
+      )
+      # batch_size_per_process = num_local_devices
+      batch_size_per_process = int(self.PERCORE_BATCH_SIZE * num_local_devices)
+      num_infeed_hosts = global_batch_size // batch_size_per_process
+    p = pax_fiddle.Config(
+        seqio_input.SeqIOInput,
+        name='BoolQValidation',
+        mixture_name='boolq_eval',
+        split_name='validation',
+        ## 'targets' is only one word
+        task_feature_lengths={'targets': 64, 'inputs': self.MAX_SEQ_LEN - 64},
+        use_cached=False,
+        repeat=True if is_training else False,
+        feature_converter=seqio_input.LanguageModelFeatures(
+            pack=False,
+            use_custom_packing_ops=False,
+            weights_on_targets_only=True,
+            bos_id=self.BOS_ID,
+            eos_id=self.EOS_ID,
+        ),
+        is_training=is_training,
+        input_random_seed=4321,
+        batch_size=batch_size_per_process,
+        num_infeed_hosts=num_infeed_hosts,
+        reset_for_eval=False if is_training else True,
+        shuffle=False,
         eval_loop_num_batches=-1,
     )
     return p
 
   def datasets(self) -> list[pax_fiddle.Config[base_input.BaseInput]]:
     """Returns a list of dataset parameters."""
     return [self._dataset_common(is_training=False)]
```

## paxml/ghostnorm/generic_wrapper.py

```diff
@@ -90,15 +90,15 @@
     # computing the average of scaled (i.e. L2-norm clipped) per-example
     # gradients, this contains a scaling coefficient for each example in the
     # batch. Shape is (batch_size,).
     scales = aux
 
     # scaled gradients for parameters to achieve per-eg grad clipping
     # scaled_g: (batch_size, ..., output_dim)
-    scaled_g = jax.tree_map(
+    scaled_g = jax.tree.map(
         lambda g_: jnp.einsum('i, i... -> i...', scales, g_), g
     )
     vjp_params, *_ = vjp_fun(scaled_g)
     _, *vjp_args = vjp_fun(g)
 
     def vmappable_vjp(g_, *args_):
       _, vjp_fun = jax.vjp(fn, params, *args_)
@@ -107,21 +107,21 @@
     per_example_grad = jax.vmap(vmappable_vjp)(scaled_g, *args)
 
     # -----------------------------------------------------------------------
     # Compute per-example gradient square norms.
     # The batch_size factor is needed when the loss is *averaged* over the
     # mini-batch of examples (instead of summed over).
     batch_size = args[0].shape[0]
-    batch_scaled_per_example_grad = jax.tree_map(
+    batch_scaled_per_example_grad = jax.tree.map(
         lambda x: x * batch_size, per_example_grad
     )
-    per_example_grad_sq_norms = jax.tree_map(
+    per_example_grad_sq_norms = jax.tree.map(
         jax.vmap(lambda x: (x**2).sum()), batch_scaled_per_example_grad
     )
-    vjp_params = jax.tree_map(
+    vjp_params = jax.tree.map(
         base.ParamWithAux, vjp_params, per_example_grad_sq_norms
     )
     return vjp_params, *vjp_args
 
   f.defvjp(fwd, bwd)
 
   def f_with_kwargs(params, *args, **kwargs):
```

## paxml/ghostnorm/layers_test.py

```diff
@@ -114,28 +114,28 @@
   def _get_per_eg_grad(
       self, initial_vars, loss_fn, *inputs_args, **inputs_kwargs
   ):
     # per-example gradients with jax.vmap
     grad_fn = jax.grad(loss_fn)
     grad_fn_with_vars = functools.partial(grad_fn, initial_vars)
     per_eg_grad_fn = jax.vmap(grad_fn_with_vars)
-    vmap_inputs_args = jax.tree_map(
+    vmap_inputs_args = jax.tree.map(
         lambda x: jnp.expand_dims(x, axis=1), inputs_args
     )
-    vmap_inputs_kwargs = jax.tree_map(
+    vmap_inputs_kwargs = jax.tree.map(
         lambda x: jnp.expand_dims(x, axis=1), inputs_kwargs
     )
     per_eg_grad = per_eg_grad_fn(*vmap_inputs_args, **vmap_inputs_kwargs)
     return per_eg_grad
 
   def _get_grad_and_norms(
       self, initial_vars, loss_fn, scales, *inputs_args, **inputs_kwargs
   ):
     grad_fn = jax.grad(loss_fn)
-    params_with_sq_norms = jax.tree_map(
+    params_with_sq_norms = jax.tree.map(
         lambda x: base.ParamWithAux(x, scales), initial_vars[PARAMS]
     )
     params_with_sq_norms = {**initial_vars, PARAMS: params_with_sq_norms}
     grad_with_sq_norms = grad_fn(
         params_with_sq_norms, *inputs_args, **inputs_kwargs
     )[PARAMS]
 
@@ -238,15 +238,15 @@
     )
     if unwrap_grad_fn:
       grad_with_sq_norms = unwrap_grad_fn(grad_with_sq_norms)
 
     # test if the computed gradient matches the grad of the reference layer
     grad_fn_ref = jax.grad(loss_fn_ref)
     grad_ref = grad_fn_ref(initial_vars, *inputs_args, **inputs_kwargs)[PARAMS]
-    grad_diffs = jax.tree_map(
+    grad_diffs = jax.tree.map(
         lambda x, y: np.mean(np.abs(x - y.param)), grad_ref, grad_with_sq_norms
     )
     np.testing.assert_allclose(
         jax.tree_util.tree_flatten(grad_diffs)[0],
         0,
         atol=1e-5,
         err_msg='average gradients are different.',
@@ -295,33 +295,33 @@
     l2_clip = np.median(jax.vmap(optax.global_norm)(grads_flat))
     l2_clip = max(l2_clip, 1e-4)
 
     sum_clipped, _ = optax.per_example_global_norm_clip(
         grads=grads_flat, l2_norm_clip=l2_clip
     )
     sum_grads = jax.tree_unflatten(grads_treedef, sum_clipped)
-    expected_grads = jax.tree_map(lambda x: x / batch_size, sum_grads)
+    expected_grads = jax.tree.map(lambda x: x / batch_size, sum_grads)
 
     _, fast_per_eg_grad_norms = self._get_grad_and_norms(
         initial_vars,
         loss_fn,
         jnp.ones(batch_size),
         *inputs_args,
         **inputs_kwargs,
     )
     scales = jnp.minimum(1.0, l2_clip / fast_per_eg_grad_norms)
     grad_with_sq_norms, _ = self._get_grad_and_norms(
         initial_vars, loss_fn, scales, *inputs_args, **inputs_kwargs
     )
 
     is_leaf = lambda x: isinstance(x, base.ParamWithAux)
-    obtained_grads = jax.tree_map(
+    obtained_grads = jax.tree.map(
         lambda x: x.param, grad_with_sq_norms, is_leaf=is_leaf
     )
-    diffs = jax.tree_map(
+    diffs = jax.tree.map(
         lambda x, y: np.mean(np.abs(x - y)), expected_grads, obtained_grads
     )
 
     np.testing.assert_allclose(
         jax.tree_util.tree_flatten(diffs)[0],
         0.0,
         atol=1e-5,
@@ -498,15 +498,15 @@
 
     ref_layer = instantiate(ref_layer_tpl)
     ghostnorm_layer = instantiate(ghostnorm_layer_tpl)
     inputs, inputs_kwargs = (), {}
     if inputs_fn is not None:
       inputs = jnp.asarray(inputs_fn())
     if inputs_kwargs_fn is not None:
-      inputs_kwargs = jax.tree_map(jnp.asarray, inputs_kwargs_fn())
+      inputs_kwargs = jax.tree.map(jnp.asarray, inputs_kwargs_fn())
 
     prng_key = jax.random.key(seed=1234)
     init_key, random_key = jax.random.split(prng_key)
     ghost_initial_vars = ghostnorm_layer.init(
         {PARAMS: init_key, RANDOM: random_key}, *inputs, **inputs_kwargs
     )
     initial_vars = {PARAMS: ghost_initial_vars[PARAMS][LAYER]}
```

## paxml/tasks/lm/input_generator_test.py

```diff
@@ -70,18 +70,18 @@
   @parameterized.parameters(True, False)
   def test_sharded(self, provide_data_size):
     p = pax_fiddle.Config(input_generator.TFRecordBertInput)
     # There are 10 examples in this test data file.
     p.input_file = test_helper.test_src_dir_path('tasks/lm/testdata/tfrecords')
     p.batch_size = 4
     p.eval_data_size = 10 if provide_data_size else 0
-    sharded_inputs = [None] * 4
+    sharded_inputs = []
     for i in range(4):
       local_p = p.clone().set(infeed_host_index=i, num_infeed_hosts=4)
-      sharded_inputs[i] = instantiate(local_p)
+      sharded_inputs.append(instantiate(local_p))
 
     # This is the same as in test_full() above.
     expected_ids = np.array(
         [2003, 1996, 1996, 2049, 3748, 1007, 4862, 1996, 2004, 2002],
         dtype=np.int32)
     expected_lengths = np.array([35, 239, 55, 56, 511, 511, 161, 43, 416, 511],
                                 dtype=np.int32)
```

## paxml/tasks/lm/model_params.py

```diff
@@ -560,29 +560,31 @@
 
   # Sub-class has to specify a mesh.
   ICI_MESH_SHAPE = None
   # Default to a single slice
   DCN_MESH_SHAPE = [1, 1, 1]
   TRAINING_OPTIMIZED_SHARDING = True
 
+  MODEL_CLASS = models.LanguageModel
+
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     """Returns the task parameters."""
     if self.DIMS_PER_HEAD is not None:
       if self.NUM_HEADS is None:
         assert self.MODEL_DIMS % self.DIMS_PER_HEAD == 0
         num_heads = int(self.MODEL_DIMS / self.DIMS_PER_HEAD)
       else:
         assert self.MODEL_DIMS == self.NUM_HEADS * self.DIMS_PER_HEAD
         num_heads = self.NUM_HEADS
     else:
       assert self.NUM_HEADS is not None
       num_heads = self.NUM_HEADS
 
     task_p = pax_fiddle.Config(tasks_lib.SingleTask, name='xformer_task')
-    task_p.model = pax_fiddle.Config(models.LanguageModel, name='xformer_lm')
+    task_p.model = pax_fiddle.Config(self.MODEL_CLASS, name='xformer_lm')
     model_p = task_p.model
     model_p.lm_tpl.packed_input = self.PACKED_INPUT
     model_p.lm_tpl.model_dims = self.MODEL_DIMS
     model_p.lm_tpl.vocab_size = self.VOCAB_SIZE
 
     if self.SEPARATE_EMBEDDING:
       model_p.lm_tpl.separate_embedding_tpl = pax_fiddle.Config(
@@ -750,14 +752,16 @@
   # allgather on microbatches, but want to use 'data' on the embedding weight
   # which is outside the pipeline.
   EMB_W_DATA_DIMS = 'data'
   # Whether to do input/output streaming across stages. This is typicall useful
   # for DCN.
   STREAM_IO = False
 
+  MODEL_CLASS = models.LanguageModel
+
   def task(self) -> pax_fiddle.Config[tasks_lib.SingleTask]:
     """Returns the task parameters."""
     if self.DIMS_PER_HEAD is not None:
       if self.NUM_HEADS is None:
         assert self.MODEL_DIMS % self.DIMS_PER_HEAD == 0
         num_heads = int(self.MODEL_DIMS / self.DIMS_PER_HEAD)
       else:
@@ -771,15 +775,15 @@
     assert self.NUM_LAYERS % (self.NUM_STAGES * self.CIRCULAR_REPEAT) == 0
     assert self.NUM_MICROBATCHES is not None or self.MICROBATCH_SIZE is not None
     assert self.ICI_MESH_SHAPE is not None and len(self.ICI_MESH_SHAPE) == 4
     assert self.DCN_MESH_SHAPE is not None and len(self.DCN_MESH_SHAPE) == 4
     assert self.ICI_MESH_SHAPE[0] * self.DCN_MESH_SHAPE[0] == self.NUM_STAGES
 
     task_p = pax_fiddle.Config(tasks_lib.SingleTask, name='xformer_task')
-    task_p.model = pax_fiddle.Config(models.LanguageModel, name='xformer_lm')
+    task_p.model = pax_fiddle.Config(self.MODEL_CLASS, name='xformer_lm')
     model_p = task_p.model
     model_p.lm_tpl.packed_input = True
     model_p.lm_tpl.model_dims = self.MODEL_DIMS
     model_p.lm_tpl.vocab_size = self.VOCAB_SIZE
 
     if self.SEPARATE_EMBEDDING:
       model_p.lm_tpl.separate_embedding_tpl = pax_fiddle.Config(
```

## paxml/tools/dump_hparams_lib.py

```diff
@@ -78,15 +78,15 @@
 ) -> None:
   """Dumps post init model hparams file."""
   model = instantiate(model_param)
 
   # TODO(pax-dev): Add better/cleaner API to identify pmap vs. pjit models
   # (and check for dcn_mesh_shape too).
   if hasattr(model, 'ici_mesh_shape') and model.ici_mesh_shape is not None:
-    input_specs = jax.tree_map(
+    input_specs = jax.tree.map(
         py_utils.get_global_input_shape_dtype, input_specs
     )
 
   hyper_params = model.abstract_init_with_mdl_config(input_specs)
   params_inits = model.abstract_init_with_metadata(input_specs)
 
   with filepath.open('w') as fout:
```

## paxml/tools/model_analysis.py

```diff
@@ -151,15 +151,15 @@
     input_specs_provider = input_specs_provider_p.Instantiate()
     input_specs = input_specs_provider.get_input_specs()
     return input_specs
 
   def _generate_datum(self):
     if not self.debug_file_pattern:
       input_specs = self._extract_input_specs()
-      datum = jax.tree_map(
+      datum = jax.tree.map(
           lambda x: jnp.zeros(shape=x.shape, dtype=x.dtype), input_specs
       )
     else:
       input_p = self.exp.datasets()[0]
       assert hasattr(input_p, 'input')
 
       if hasattr(input_p.input, 'file_pattern'):
@@ -177,15 +177,15 @@
       elif hasattr(input_p.input, 'args'):
         input_p.input.args.batch = self.bs
       data = input_p.Instantiate()
       datum = data.get_next()
 
     if datum:
       print('\n==========input=========')
-      pprint.pprint(jax.tree_map(jnp.shape, datum))
+      pprint.pprint(jax.tree.map(jnp.shape, datum))
       print('=======================')
 
     return datum
 
   def _extract_model(self, datum):
     task = self._extract_task()
     model = task.model
@@ -247,15 +247,15 @@
     ):
       var_weight_hparams = model.abstract_init_with_metadata(datum)
 
     learner = task.learners[0]
     excluded_for_grad = tasks_lib.get_excluded_var_mask_for_grad(
         var_weight_hparams, learner
     )
-    included_for_grad = jax.tree_map(lambda x: not x, excluded_for_grad)
+    included_for_grad = jax.tree.map(lambda x: not x, excluded_for_grad)
     trainable_variables = py_utils.NestedMap.FromNestedDict(included_for_grad)
 
     prefixes = py_utils.extract_prefixed_keys_from_nested_map(
         trainable_variables
     )
     max_param_len = max(
         [len(prefix) for prefix in jax.tree_util.tree_leaves(prefixes)]
@@ -280,15 +280,15 @@
       else:
         trainable_type = 'others'
       params_count[trainable_type] += param_size
       leaf = param_name, param_shape, param_size, trainable_type
       params_list.append(leaf)
       return leaf
 
-    jax.tree_map(
+    jax.tree.map(
         collect_params,
         prefixes,
         trainable_variables,
         py_utils.NestedMap.FromNestedDict(model_vars),
     )
     for param_name, param_shape, param_size, trainable_type in params_list:
       output_line = output_line = (
```

## paxml/tools/validate_config_lib.py

```diff
@@ -66,15 +66,15 @@
 def _hparams_post_init(model_param, input_specs) -> None:
   """Calls post-init of model hparams."""
   model = instantiate(model_param)
 
   # TODO(pax-dev): Add better/cleaner API to identify pmap vs. pjit models
   # (and check for dcn_mesh_shape too).
   if hasattr(model, 'ici_mesh_shape') and model.ici_mesh_shape is not None:
-    input_specs = jax.tree_map(
+    input_specs = jax.tree.map(
         py_utils.get_global_input_shape_dtype, input_specs
     )
 
   _ = model.abstract_init_with_mdl_config(input_specs)
   _ = model.abstract_init_with_metadata(input_specs)
```

## Comparing `paxml-1.3.1.dist-info/LICENSE` & `paxml-1.4.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `paxml-1.3.1.dist-info/METADATA` & `paxml-1.4.0.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,39 +1,39 @@
 Metadata-Version: 2.1
 Name: paxml
-Version: 1.3.1
+Version: 1.4.0
 Summary: Framework to configure and run machine learning experiments on top of Jax.
 Home-page: https://github.com/google/paxml
 Author: PAX team
 Author-email: pax-dev@google.com
 License: Apache-2.0
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Requires-Python: >=3.10
 License-File: LICENSE
-Requires-Dist: absl-py
-Requires-Dist: clu
-Requires-Dist: etils
-Requires-Dist: flax
-Requires-Dist: graphviz
-Requires-Dist: jax
-Requires-Dist: lingvo
-Requires-Dist: numpy
-Requires-Dist: orbax-checkpoint
-Requires-Dist: praxis
+Requires-Dist: absl-py (==1.4.0)
+Requires-Dist: clu (==0.0.11)
+Requires-Dist: etils (==1.7.0)
+Requires-Dist: flax (==0.8.2)
+Requires-Dist: graphviz (==0.20.1)
+Requires-Dist: jax (==0.4.26)
+Requires-Dist: lingvo (==0.12.7)
+Requires-Dist: numpy (==1.26.4)
+Requires-Dist: orbax-checkpoint (==0.5.9)
+Requires-Dist: praxis (==1.4.0)
 Requires-Dist: protobuf (==3.19.6)
-Requires-Dist: pyglove
-Requires-Dist: seqio-nightly
-Requires-Dist: t5
+Requires-Dist: pyglove (==0.4.4)
+Requires-Dist: seqio-nightly (==0.0.17.dev20231010)
+Requires-Dist: t5 (==0.9.4)
 Requires-Dist: tensorflow-datasets (==4.8.3)
 Requires-Dist: tensorflow-metadata (==1.12.0)
 Requires-Dist: tensorflow-text (~=2.9.0)
 Requires-Dist: tensorflow (~=2.9.2)
-Requires-Dist: tensorstore
+Requires-Dist: tensorstore (==0.1.55)
 Requires-Dist: tfds-nightly (==4.8.3.dev202303280045)
 Provides-Extra: gpu
 Requires-Dist: jsonlines (==3.1.0) ; extra == 'gpu'
 Requires-Dist: pysimdjson (==5.0.2) ; extra == 'gpu'
 Requires-Dist: zstandard (==0.18.0) ; extra == 'gpu'
 
 UNKNOWN
```

## Comparing `paxml-1.3.1.dist-info/RECORD` & `paxml-1.4.0.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -1,108 +1,112 @@
 paxml/automl.py,sha256=V2el2kPP4whiIxTSGk5bMlL7tq0-X3yB1f0Edl8qSMI,29105
 paxml/automl_interfaces.py,sha256=Cgo7mSPETrlVmC-mpEYRREFPs6c6lPM6wT0LRPxBpWE,17158
-paxml/automl_test.py,sha256=Ikm1cLUIPPWXlaCQpWPbMUPz_Hy75hoAIM-Gzb2LNpg,36536
+paxml/automl_test.py,sha256=uwPGdCP-qu2Cpfs2ssxtkoG4xS3X7LGsnbHutqHCWEk,36854
 paxml/base_executor.py,sha256=-y53J8DHpDK9bexYurJ-Zsx8bhovCTcxkUetNSadKi4,2030
 paxml/base_experiment.py,sha256=wHykFf_SDSYe4P0v9dnMhCRpOyiexWgo5gJVw_tvpB4,7176
 paxml/base_inference_runner.py,sha256=wva_A7SYoOEzYI3Vxr1JQ0jdNKPiDCvHALdEbKOy5XQ,3214
-paxml/base_inference_runner_test.py,sha256=Adgmxq1eu2Nuan0pDC5ApKj3Y1f-7vmfMC2CXGfPFHg,3058
+paxml/base_inference_runner_test.py,sha256=ZqM26D0VW-YTQRf8YtsI-0q7ZJXQFsj7gNVxeJEB0Jg,3086
 paxml/base_metrics.py,sha256=EBCmzDYyOu8H2Sd9jmeuslzUp0vCJPOucJjtc5E9aRk,17258
 paxml/base_metrics_test.py,sha256=3cOSHCvb-HTVXuwECpK8chAyo0wBax4zKZXYR3mCSes,7160
 paxml/base_task.py,sha256=LpsiwBqM-MoGPc2ZfTR4IcP6yrhmTGaD4G6NEx58QfA,1106
-paxml/checkpoint_creators.py,sha256=yZ65lhqN8CE74O8qPJwPYYoPaAaeXIrVhwpr0REi7eY,22776
-paxml/checkpoint_managers.py,sha256=PsgARNojQbv6cbaax9pQRp5-lmFB50IwsqVfCeVdDY4,22358
-paxml/checkpoint_managers_test.py,sha256=ngIzMb4YvDXUSCoi5_ee7aX8c3hOBLdxr-yqcAVW-vk,36281
+paxml/checkpoint_creators.py,sha256=sFkX9fBCSAG1oETDdN0pbHkJ54HX6J-H1MsefSR5vD0,22776
+paxml/checkpoint_managers.py,sha256=8KAQ94_IHK9XRMVLFIBSMPAJQD_zlB9Ss6eEr8FYSQw,21268
+paxml/checkpoint_managers_test.py,sha256=8UV6tZwDXPLkqGVBcyFMGS46xNQ-qMg3SPpuC5NI7S4,36573
 paxml/checkpoint_metadata.py,sha256=QCF0ORIpieF-aqIzBnm1cAI_vs_b0lNG7y1vDNyvtuM,9996
-paxml/checkpoint_paths.py,sha256=wWP8uEtDCsEz6lDPiLa0IaoeNCM_UeCT07b2RYYyOxA,8351
+paxml/checkpoint_paths.py,sha256=YIYISY7LyB4ZVu7cQlyfmOstxa9brVrObke5BmT6QY4,11335
 paxml/checkpoint_types.py,sha256=g-4L9sqyWPkVWe_gnJiodfeUJpKQRV0jek6s2YBhg1Q,1555
 paxml/checkpoint_version.py,sha256=wYWgc2A0yiBEfDSQcmjqT_pzzm3Ajy2jL19tHHo_gAk,2030
-paxml/checkpoints.py,sha256=j4rgioQM0ffFuEo53UwQAn4xmjdgg6_zYKTz4Vce8hg,30794
+paxml/checkpoints.py,sha256=ZaCitimyKMn7E-ZTE-kD0jiuykDXxF7iMZNvfD7CiNI,30794
 paxml/checkpoints_test.py,sha256=rUSSIsNrMIXN_KUe1c7S36X6kNMpQc5ADCXwWpnnNUQ,13204
-paxml/decode_programs.py,sha256=HDYGK7ODez5qP5onGmE3MnIw1-utw7TKpJ5DYiF5oK4,16889
-paxml/eval_lib.py,sha256=MExp6IihrdQEwWLOzGH8R57IkrHRpu7K62UPcdePmOU,38940
+paxml/decode_programs.py,sha256=ssT_0RbbIl78T0ZOyGTTcBaoeT0E3U-S8xruw8lYOko,16889
+paxml/eval_lib.py,sha256=O1IOyN91wgqPN4xtC6EPPabit1zusoa8mc65h_bP610,39112
 paxml/executors.py,sha256=kcut8SzS33Tl2sE48jlat21dFvNECbw594waTL7y1XE,20594
 paxml/experiment_imports_all_test.py,sha256=Xhd1LIbVZICjnH4jHM-9wQxy1csNKTaCueOE_P3nSTA,1659
-paxml/experiment_imports_test_helper.py,sha256=XM9vTlJfXr7CCws4ZLfSGctk-f7QZHPoyRPE3iSWY2c,5162
+paxml/experiment_imports_test_helper.py,sha256=tWAkqHAx80IgrQ_qHTJqfeD27ZFdgrfosw7hysu9j5k,5149
 paxml/experiment_registry.py,sha256=YCDm5snnn70k0prechpDfPJkxdu9rc2B4Gw5PB0j830,6734
 paxml/experiment_registry_test.py,sha256=Hpek1PaoI-VvfYMp2UB2h_ycs5-bIPQkTMvGgm_rn84,4771
 paxml/experiment_utils.py,sha256=VCsn0Slv6AhbDnNjnifGHX8VDaYj0NtUFOVa2G9IEUg,4202
+paxml/first_result_metric_callback.py,sha256=Z3WqvdgKq8vmuGFDuqBbByAVaTn7JwXXY9m_Z8i4ZXY,651
 paxml/host_callback.py,sha256=FxHFYJb2CB_97ex55p6HVkxCOL90j8WFi_9corG7uVM,4496
 paxml/host_callback_test.py,sha256=l3GvzPb2Mmp0w-TlqaffUbIR2puQW0747ra9S-Gb89o,2683
-paxml/io_utils.py,sha256=AqIbVbWJqpdcTur_D-ziHFI8lv4qr9PVgR8asALITKs,13232
+paxml/io_utils.py,sha256=cHuET5SLiP-NhWeyWUza353Q2Jpq4cjDzse0wr6KA48,13232
 paxml/io_utils_test.py,sha256=STvq3eYOKzZemuMyTVo5VKc5XY_zVlGYpd2mYcRlEWk,6381
-paxml/learners.py,sha256=HuRViRKA6wd_y5SDILg5uqaf3AgFsMkGIt2eYsjcXu8,26711
-paxml/learners_test.py,sha256=iOxbvk7GY-nyR1abyl_FCFLzvPfWf4KhUscXESwuKP4,45747
-paxml/main.py,sha256=oWdVVJSMeBISvHBUlAEMaDCSV5ug61ztvQJ7hYERl_4,21958
+paxml/learners.py,sha256=auAn1AZyDu1-Yj-WOymlnPIhCIavee4VFDMe71mhZr0,26711
+paxml/learners_test.py,sha256=I7u60yjoNdMS11s5TqwYv50Bu2kz6kQ6zc63wZVnU5g,46163
+paxml/main.py,sha256=cgjRVuT9yTozydPK6P9JqTrPVdK_xQHsYzm5D5v7IPo,22105
 paxml/main_test.py,sha256=xjCpa7X5pMYp-LaDF_dQQ1GkJIBoAiEaP1aP1Guv-wU,2207
-paxml/metric_utils.py,sha256=eqnjR0Ws4iWM7DuGz0GV8H127KSuIIm0qExx0ujcBz8,7444
-paxml/metric_utils_test.py,sha256=JKs856ov6OTAjlzTJku3_0YHRjXb0WEBPLzyG2wgk38,12901
+paxml/metric_utils.py,sha256=PnpmlBopaS1Zv7g2mbW45ozmCC1oyBQO8DGC1NtSosM,7549
+paxml/metric_utils_test.py,sha256=rPX2TJhrsSxMkDPzkF2pUbwFe1ABbUXvG1vN9vs3rm8,13059
 paxml/parameterized_experiment.py,sha256=kZ9jn34sOFPkpZ3hnedwKOg4a3XtYt341Yy8mq3Fh4Q,7770
 paxml/parameterized_experiment_test.py,sha256=8e9i-hYEgC2HyTIhih__zZoxUunlxnp88bBu3GeITm4,6055
-paxml/partitioning.py,sha256=nA5hV9DCMkmU34CBf52WdDHDu83b1emftZDsHRTLbTE,58148
+paxml/partitioning.py,sha256=jPbq8lX_45ya8N3_UbFzy0rwoAewrKlvIJCb_PXlT2c,58214
 paxml/partitioning_test.py,sha256=ZYP9PDJYyELqvWeeM5QkZVWjDGY-UDhcDQ-g1xpV6xE,1863
 paxml/profiling.py,sha256=rVeW9arFeiaagbG_S9sDVn-RS32dDOTu4HqumTJpdik,3130
-paxml/programs.py,sha256=4CnE5vGryWPd3evOkGzqCeBjO8BAvYOfiHIiHDj5FPE,36593
-paxml/programs_test.py,sha256=WG6RVLw4WOBXRtHNTNUoDvBRiH6jHqu-MUraPuCXOYY,4517
-paxml/seqio_input.py,sha256=-bwJ8nx-MjzS33FYv9WgWU_AVtijAhVAY8w8pTSJz-0,76681
-paxml/seqio_input_test.py,sha256=5CGKs8KIFb1qPsds_PpMMHh3hTNoHtiom-Yoxpheql4,52224
+paxml/programs.py,sha256=TdzCLQ-mF-kwc9t5_R-QvuM2CHaCLjODU-L-bpIT3eU,37241
+paxml/programs_test.py,sha256=QVztTKfZrJpvIJrTYQObo5J5XDOq0mARvVAsKpDRAOw,4557
+paxml/seqio_input.py,sha256=Yb8myCrpREm1kwzFfa2SPWImFXagKNa3c8KcJE7cnWg,77003
+paxml/seqio_input_test.py,sha256=cKdNn9_Rg1Jr2LyanY2M3w2jlijAQkkC_2OKw39pHC0,53698
 paxml/setup_jax.py,sha256=e_M0U78cqKGeBAhqocGK1d9TBoEzsdOUgK7_lx1vk-U,2830
-paxml/sgf.py,sha256=6yVXJPyIGICiO6gK8f0hprO-ebDnGxs4koSbCL1CZIA,29994
-paxml/summary_utils.py,sha256=01kuJLtg6axG1KEBDYNuGFzp4kaRPYK-zvh_hLlecWs,34259
-paxml/summary_utils_test.py,sha256=Zoq3-wXhfjRwFuzHL6ZeMrIYQx3wnOWK7q3zeXi8wu8,16853
-paxml/tasks_lib.py,sha256=XUD1X3LQ9oIh7iA4s9AmrCQKPZCrB9B1o8DcQN_SWzs,79608
-paxml/tasks_lib_test.py,sha256=5TCUEZkzklzWBNw1oMzrnuf1Ib-iQREDuusCqdDz05o,54467
+paxml/sgf.py,sha256=uiAy4lA2opyRz4dF70xoy7aOWTq02lcxYuWrWM46EcM,29994
+paxml/summary_utils.py,sha256=bQForSPJeDppSbaF1RvFVbiLkfwiQzVdUY5SwAM7gKg,34279
+paxml/summary_utils_test.py,sha256=4FG_W0HjklO3RCkxAdTuLmZx3wzTt9utFc8zA-PV9HI,17018
+paxml/tasks_lib.py,sha256=arx1V-Ven8IXKFH3OtCSFnmvGYNYI7devyXLYe89rRY,79592
+paxml/tasks_lib_test.py,sha256=GsTpYdRROZWMZxwQHGiXtC_AEBCIaBB4U7AA0Art81k,54788
 paxml/test_helper.py,sha256=KBP7ihH-tgd60iKsaZdVJ7TOjGNsqXBlNAY570jDhw0,793
 paxml/tf_data_service_lib.py,sha256=8bTkh6oCiHChlZHDYCnu1CWo2SSqzC4oCdacfqooRYY,5670
-paxml/train.py,sha256=S11V9nCJ50tgaIiAfRSSJRJQVEVZTuzkcDmNz1_j9eE,10911
-paxml/train_states.py,sha256=QJA0S3ZahKKblJfQsEGfog4GM-yQwIkMELUPEKC_kWg,4136
-paxml/trainer_lib.py,sha256=63DJY835HmwJevy-8E0fppWKTyUCMpKmBOfBohFuubU,61465
-paxml/trainer_lib_test.py,sha256=Lw5Z8sT-i_p32VnGKADQFBldqJxxx7jj2iLierJ5yrg,7065
+paxml/train.py,sha256=vOWD2dG7pfI44gLUvaM8tmXlM-jdui7pAJXzY50lMBc,11267
+paxml/train_states.py,sha256=tvOR8F_byBYEc8Qyre5Xl-O8pNM3PHP2pV3ZOCQq0zs,4136
+paxml/trainer_lib.py,sha256=BXwYYIVsu32mF34aiW8zcNgY5qgG7uz7o4PsxMCp2fc,61465
+paxml/trainer_lib_test.py,sha256=70oIQQCwD6UyZEQLySuCd_jTjQTWEVDu8fpSx-fJ6gc,7249
 paxml/tuning_lib.py,sha256=N153juwR8iDR5YP-ruSEDVtERtT0LbzEHd93485tZgY,39966
-paxml/tuning_lib_test.py,sha256=2MDAA5_K52osm_d1OgBH21b8jVS5JfgQP697o2-0xLw,34356
+paxml/tuning_lib_test.py,sha256=Q32ZhpxQHTu62cbDGu7qL8nXqzg8g8L_iP_0xi0eaKg,34450
 paxml/xla_passthrough.py,sha256=2I49n5GnOxSQ2bmYJ8jkgtIgizM1attCKW0tMXbGhi8,3864
 paxml/xla_passthrough_test.py,sha256=3AMDl3ltdnhpMvxoigO-Z6GTvxZv6xBQvaCbDM2kgWU,4354
-paxml/contrib/gpu/scripts_gpu/configs.py,sha256=03s4gx1WMDYoDGNAOlUSxsyUaHTU275fgQaKPbIzToU,9642
+paxml/contrib/gpu/scripts_gpu/configs.py,sha256=MEj3SCgatwqzv66icoWrj535AtRPCh-pxTM_WzQbIlg,11498
+paxml/contrib/gpu/scripts_gpu/download_boolq.py,sha256=dvDn2UkTo9owuB3122xyoELkhHDSv0cv-yVSjqr7_oA,748
 paxml/contrib/gpu/scripts_gpu/download_lambada.py,sha256=0JhQcW3YM8BPOX-Iu4WRtr-MYzNWCj8RZQVTzzBhG9A,784
 paxml/contrib/gpu/scripts_gpu/download_the_pile.py,sha256=r7KDILgYl2P210QzZ57yx88uG6bmPa0lrFDyseCRZqo,777
-paxml/contrib/gpu/scripts_gpu/tasks.py,sha256=fEqiSBZlJR0W6mxTkitSTHEf2yNiz7Lmh8Cd5oYFTh8,6130
+paxml/contrib/gpu/scripts_gpu/llama_utils.py,sha256=fGsw5MDmbfsqlm8TLMHz2eDgwej7pPTrsW8rKXyFFJg,6384
+paxml/contrib/gpu/scripts_gpu/saxml_layers.py,sha256=Nghzlr9rCZ1I85-wurpziu9qItUPtV__lGeLqbnaCuQ,13319
+paxml/contrib/gpu/scripts_gpu/tasks.py,sha256=hGmwMmLKxfZNwORg1pzt0p84phGtmrWvRA-J6MX3wWQ,9028
 paxml/contrib/gpu/scripts_gpu/tfds_lambada.py,sha256=jwCx3fUsbMsDnV0EOVjMbw53UrqrlVLkf291IzGEU6I,5411
 paxml/contrib/gpu/scripts_gpu/tfds_pile.py,sha256=w_0J5A6k_qs8qSpKXv6UfoXyvsnkNA3qqzmVr5Bvxc4,7376
 paxml/experimental/baseline_experiment.py,sha256=nEgmwsr8ZURvtUE8PTsxm-9V3hWGNQ3jFBVY0kJlxEs,4968
 paxml/experimental/baseline_experiment_test.py,sha256=t8Qp5ImrO8wUkKfSNJOwtQn6D6P4coUHZZGCJpZEErc,5047
 paxml/experimental/nested_map_config_helper.py,sha256=qi1rqyxfU5RoXdYQdgteyu7r5vicr8A55SAu-CMvLNg,1271
 paxml/experimental/nested_map_config_helper_test.py,sha256=Bcwwpxo5KjoadHiVXw4H-Weum0S-nNqCcCGFlkhqUpY,1018
 paxml/ghostnorm/base.py,sha256=xdQsGpNc8hU3LsYmNbUc7imjGgIqEcm9RSrdcMJD0cc,4814
 paxml/ghostnorm/embedding.py,sha256=5vAxVCJ9G7VIsb6bMJAhFtrDTdY1o4EsfvzPjrIv2SM,6408
-paxml/ghostnorm/generic_wrapper.py,sha256=G1QzmZeUvLufTdDpSt5YZaNClg4NMekLSDtjPYyXfhE,12969
-paxml/ghostnorm/layers_test.py,sha256=oCuCCgmLa9Ih41LuY3gDRuriP5I0SPLOGMtfVnSuWVc,24887
+paxml/ghostnorm/generic_wrapper.py,sha256=9zxNGTc71CYE2Is0nVTgFFOUFIXmoYeC-4_RJG-pE70,12969
+paxml/ghostnorm/layers_test.py,sha256=snuZvvv_bIGC0XN5WiuKhGmHrjvfUe4mhd7rKrAnI7U,24887
 paxml/ghostnorm/linears.py,sha256=1Ll3xZhC5nOSDqQr5arF1n32QJ3r_-UgtYGBXJh1Gj8,5870
 paxml/tasks/lm/__init__.py,sha256=Kvb2PoI3YIk0TIr1ntASQoVpYeVxpEwVj0tTxsU7uxg,597
 paxml/tasks/lm/input_generator.py,sha256=Kc5zsNznbukDXTf2nXJmf3OgP41S0yDhF35JdZg-MNo,17398
-paxml/tasks/lm/input_generator_test.py,sha256=M4bAdDPKltVwn4gZOWdPadZ291c0aFU6M_UWiZcWIGM,4089
-paxml/tasks/lm/model_params.py,sha256=39rGazAcLnOQyzLGUGY2EjWw6noWqhaoHHOSgEpDU4c,35722
+paxml/tasks/lm/input_generator_test.py,sha256=Z_4n2vdVpNnjxD1pFlGppieD-yEYkbDyV_5pq5kEJ58,4084
+paxml/tasks/lm/model_params.py,sha256=yfLkbwXOVouYMsy-CKpXT69i1V3ndxCk3fIUbXvi4bI,35790
 paxml/tasks/lm/params/bert.py,sha256=fkeIqEVqNwBnO1apmZhweZXJzlhV0CjramZXAbfcTEo,6610
 paxml/tasks/lm/params/c4.py,sha256=Yqlj5eeHxrLfdFTyjYss4YFJ3HApEbWocHAzk01AN4M,30685
 paxml/tasks/lm/params/c4_multislice.py,sha256=qyCQpHl_2fwD_R92ATDoQxb5lvGJKUlFydHtJZFEoBs,7395
 paxml/tasks/lm/params/c4_test.py,sha256=lEdEP5EOPr8LIQLKPu0oJhgHdVmpbt1jX8w1wuHFXg0,3647
 paxml/tasks/lm/params/lm_cloud.py,sha256=zY31Q0e1iLLuWyshKeAhEaDSL0-UsopG1TE6Ar02TRk,10423
 paxml/tasks/lm/params/nvidia.py,sha256=F9vdf44FN9mI_frGKdrvQfXMnrId-gCk4mZQVdGBFVI,17732
 paxml/tasks/lm/params/optimal_scaling.py,sha256=KggpiEUYJybmWhVEHQTj1jQlQoGzlw9vlbKtuexeaeY,3040
 paxml/tasks/test/synthetic.py,sha256=H4oqOotmObzPseqgC7mCQd8oeQ2Exp-F8q_afsLcXbg,1124
 paxml/tasks/vision/input_generator.py,sha256=V8cvnug4RnKYq8qcWolO1s33ylO2XsTcfqMpLKu5jxE,5197
 paxml/tasks/vision/input_generator_test.py,sha256=4Tnn8YYYkI8w0zpVSxTdSUaUSLrEdzOdmhrckW1zuKo,1611
 paxml/tasks/vision/resnet_preprocessing.py,sha256=imHewQV5Xb6BAZxqi9ejPM_WtJO81okSLZdN7vUuuYs,7002
 paxml/tasks/vision/params/imagenet_resnets.py,sha256=kE39N0c4ycxxh6oTgSZvtOLqgoLaa9Oktw8rNiA45ug,9055
 paxml/tasks/vision/params/mnist.py,sha256=wHGlcz3S4ZP4vdbfKxsCeouVuojz2l2kb5fMFhtK9HI,8285
 paxml/tools/dump_hparams.py,sha256=LDM2lBQW1C3JP7MYDutt_2hshg_VuvlJGCQvSnEiCIs,1319
-paxml/tools/dump_hparams_lib.py,sha256=69lrw_TouqpmcdnCU3Vlf2lH5p7jK-wRAC6BlI0_-ow,7098
+paxml/tools/dump_hparams_lib.py,sha256=J5WAIFvxpJfLePvgf-PxM3QNDBZTOdFliYoV-unjq74,7098
 paxml/tools/dump_input_specs.py,sha256=5afJViiiDxI6HSl7M83HmvZbIcA59U4DwrvBbYXywl4,1590
 paxml/tools/dump_input_specs_lib.py,sha256=Bbv7t37ARX9o1e_Cf8eUo3JrxlcQqA3RWkdiMH1DaTQ,3755
-paxml/tools/model_analysis.py,sha256=BQo2K8Wo4ZQC9hUGa6S3K5niwGAn3zKePJ9SbZTzWyE,11008
+paxml/tools/model_analysis.py,sha256=1Ds6c-EeW2zjsPdeUN77Psh4RRUK_9zp_dki-jC142g,11008
 paxml/tools/validate_config.py,sha256=D99R5S7CKAv_sr460gCkaJGtF1cpY3yFmq-X9pjd3Rc,1000
-paxml/tools/validate_config_lib.py,sha256=nw_PRdBQHt5wx-DjSWcnmqkdHDuUkbh9HWtJxbw9q04,4976
+paxml/tools/validate_config_lib.py,sha256=3RfKhITls10w2sU4soSX-FPPpb1I6Rg-KQQ8w0Gxvks,4976
 paxml/tools/fiddle/codegen.py,sha256=Ztx_iXYx6TTYu1cENeuMoLYHY0YpDqrZIC57xv-ln3E,28469
 paxml/tools/fiddle/codegen_external_init_checkpoint_fns.py,sha256=6t1tBtHGe4S8KPKGLR4RNq2rXTxFJSP30AT-e1RD-Vg,5045
 paxml/tools/fiddle/codegen_external_init_checkpoint_fns_test.py,sha256=5tGE5VKl3ZAoyD7kJl_RvSMs_i95Lcc70T163PiZ15Q,3738
 paxml/tools/fiddle/codegen_highlevel_parameterization.py,sha256=hEg0F7xAZS12l6e2w2e2ItqDalObHbwCBEbI1EQqvqc,3303
 paxml/tools/fiddle/codegen_highlevel_parameterization_test.py,sha256=0nrhyvRmhQuGtDCWsL8riV5aear2UVjgWGWEtJA-hmI,2268
 paxml/tools/fiddle/codegen_pax_code_ir.py,sha256=5kkO9SZnpGVuXlueppkFQ1vHXpFrHT_hrOHdBXHQiFs,1420
 paxml/tools/fiddle/codegen_sharding.py,sha256=_Oz7hhPCR8weAIETt2RVcQaqw04Wwi2XSyH7Ox_7_zU,4723
@@ -119,12 +123,12 @@
 paxml/tools/fiddle/remove_sharding.py,sha256=8fBCThx8Amw2WwM2gb-IlV_VkJ-zN8q-6aURya8ld0A,2862
 paxml/tools/fiddle/remove_sharding_test.py,sha256=jC42d6mVs6u56XNgEBPVN28Ok0rXVXAW6tfs_WcP2mM,3709
 paxml/tools/fiddle/test_fixtures.py,sha256=dL4ohZmiHYZZ71qriXoMZacVknes48scWIb2Ckr0cI8,6062
 paxml/tools/fiddle/unshare_sharding.py,sha256=2bd7lCzE1fJY1GHPiedBsFxxN4JXCdBT2DzNn5edXOw,2486
 paxml/tools/fiddle/unshare_sharding_test.py,sha256=JCqBOwTJuRyyVf-5EBBRC4zt8iGxQmvJjI-nr02mCR0,4813
 paxml/tools/fiddle/wrap_nested_maps.py,sha256=KBv0Pnl_9mdb0NDcI66MGYReGamPe7jpQs1gnhWZTdU,1326
 paxml/tools/fiddle/wrap_nested_maps_test.py,sha256=4LqCdQXaacNSm61_PZXFRFe8sNtlv7ZfrImVFgN_BhU,1759
-paxml-1.3.1.dist-info/LICENSE,sha256=WNHhf_5RCaeuKWyq_K39vmp9F28LxKsB4SpomwSZ2L0,11357
-paxml-1.3.1.dist-info/METADATA,sha256=ztpqHqn1LL_LaDM9PKzt0XqR2rXK60_jjVRJxzrk1KE,1177
-paxml-1.3.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-paxml-1.3.1.dist-info/top_level.txt,sha256=hPiAZDM4XGMlFcftlHk-0rW4_hq3Nur4MaWwEiDB94c,6
-paxml-1.3.1.dist-info/RECORD,,
+paxml-1.4.0.dist-info/LICENSE,sha256=WNHhf_5RCaeuKWyq_K39vmp9F28LxKsB4SpomwSZ2L0,11357
+paxml-1.4.0.dist-info/METADATA,sha256=WMBCs1S1hJaIU2tg7eBiYhdTmO4zMb1Nr0U9P24bwro,1336
+paxml-1.4.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+paxml-1.4.0.dist-info/top_level.txt,sha256=hPiAZDM4XGMlFcftlHk-0rW4_hq3Nur4MaWwEiDB94c,6
+paxml-1.4.0.dist-info/RECORD,,
```

