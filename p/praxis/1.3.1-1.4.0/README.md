# Comparing `tmp/praxis-1.3.1-py3-none-any.whl.zip` & `tmp/praxis-1.4.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,182 +1,182 @@
-Zip file size: 761344 bytes, number of entries: 180
--rw-r--r--  2.0 unx      596 b- defN 24-Feb-21 00:22 praxis/__init__.py
--rw-r--r--  2.0 unx    22000 b- defN 24-Feb-21 00:22 praxis/asserts.py
--rw-r--r--  2.0 unx    11563 b- defN 24-Feb-21 00:22 praxis/asserts_test.py
--rw-r--r--  2.0 unx    44079 b- defN 24-Feb-21 00:22 praxis/base_hyperparams.py
--rw-r--r--  2.0 unx    27369 b- defN 24-Feb-21 00:22 praxis/base_hyperparams_test.py
--rw-r--r--  2.0 unx    44661 b- defN 24-Feb-21 00:22 praxis/base_input.py
--rw-r--r--  2.0 unx    34707 b- defN 24-Feb-21 00:22 praxis/base_input_test.py
--rw-r--r--  2.0 unx   100806 b- defN 24-Feb-21 00:22 praxis/base_layer.py
--rw-r--r--  2.0 unx    31284 b- defN 24-Feb-21 00:22 praxis/base_layer_test.py
--rw-r--r--  2.0 unx     5773 b- defN 24-Feb-21 00:22 praxis/base_model.py
--rw-r--r--  2.0 unx    19218 b- defN 24-Feb-21 00:22 praxis/beam_search.py
--rw-r--r--  2.0 unx    15704 b- defN 24-Feb-21 00:22 praxis/beam_search_test.py
--rw-r--r--  2.0 unx     6514 b- defN 24-Feb-21 00:22 praxis/decoder_hparams.py
--rw-r--r--  2.0 unx    25749 b- defN 24-Feb-21 00:22 praxis/decoder_utils.py
--rw-r--r--  2.0 unx    11628 b- defN 24-Feb-21 00:22 praxis/decoder_utils_test.py
--rw-r--r--  2.0 unx     1475 b- defN 24-Feb-21 00:22 praxis/fiddle_tags.py
--rw-r--r--  2.0 unx    11591 b- defN 24-Feb-21 00:22 praxis/flat_beam_search.py
--rw-r--r--  2.0 unx     4451 b- defN 24-Feb-21 00:22 praxis/flat_beam_search_test.py
--rw-r--r--  2.0 unx     5602 b- defN 24-Feb-21 00:22 praxis/flax_utils.py
--rw-r--r--  2.0 unx    35272 b- defN 24-Feb-21 00:22 praxis/gshard_utils.py
--rw-r--r--  2.0 unx     2600 b- defN 24-Feb-21 00:22 praxis/lazy_loader.py
--rw-r--r--  2.0 unx     1555 b- defN 24-Feb-21 00:22 praxis/lingvo_lib.py
--rw-r--r--  2.0 unx     2523 b- defN 24-Feb-21 00:22 praxis/metric_utils.py
--rw-r--r--  2.0 unx    16997 b- defN 24-Feb-21 00:22 praxis/optimizer_prefix_vectorization.py
--rw-r--r--  2.0 unx     3065 b- defN 24-Feb-21 00:22 praxis/optimizer_prefix_vectorization_test.py
--rw-r--r--  2.0 unx   113212 b- defN 24-Feb-21 00:22 praxis/optimizers.py
--rw-r--r--  2.0 unx    13883 b- defN 24-Feb-21 00:22 praxis/optimizers_test.py
--rw-r--r--  2.0 unx    37119 b- defN 24-Feb-21 00:22 praxis/pax_fiddle.py
--rw-r--r--  2.0 unx    41079 b- defN 24-Feb-21 00:22 praxis/pax_fiddle_test.py
--rw-r--r--  2.0 unx    39408 b- defN 24-Feb-21 00:22 praxis/py_utils.py
--rw-r--r--  2.0 unx    18672 b- defN 24-Feb-21 00:22 praxis/py_utils_test.py
--rw-r--r--  2.0 unx     5338 b- defN 24-Feb-21 00:22 praxis/pytypes.py
--rw-r--r--  2.0 unx     1583 b- defN 24-Feb-21 00:22 praxis/pytypes_test.py
--rw-r--r--  2.0 unx    76628 b- defN 24-Feb-21 00:22 praxis/sample_decode.py
--rw-r--r--  2.0 unx    26693 b- defN 24-Feb-21 00:22 praxis/sample_decode_test.py
--rw-r--r--  2.0 unx    22153 b- defN 24-Feb-21 00:22 praxis/schedules.py
--rw-r--r--  2.0 unx    21498 b- defN 24-Feb-21 00:22 praxis/schedules_test.py
--rw-r--r--  2.0 unx    18628 b- defN 24-Feb-21 00:22 praxis/test_utils.py
--rw-r--r--  2.0 unx    24670 b- defN 24-Feb-21 00:22 praxis/token_samplers.py
--rw-r--r--  2.0 unx    11224 b- defN 24-Feb-21 00:22 praxis/token_samplers_test.py
--rw-r--r--  2.0 unx     8428 b- defN 24-Feb-21 00:22 praxis/trees.py
--rw-r--r--  2.0 unx    12754 b- defN 24-Feb-21 00:22 praxis/trees_test.py
--rw-r--r--  2.0 unx     6959 b- defN 24-Feb-21 00:22 praxis/layers/__init__.py
--rw-r--r--  2.0 unx     5340 b- defN 24-Feb-21 00:22 praxis/layers/activations.py
--rw-r--r--  2.0 unx     1858 b- defN 24-Feb-21 00:22 praxis/layers/activations_test.py
--rw-r--r--  2.0 unx     8286 b- defN 24-Feb-21 00:22 praxis/layers/adapters.py
--rw-r--r--  2.0 unx     6878 b- defN 24-Feb-21 00:22 praxis/layers/adapters_test.py
--rw-r--r--  2.0 unx   130332 b- defN 24-Feb-21 00:22 praxis/layers/attentions.py
--rw-r--r--  2.0 unx    87739 b- defN 24-Feb-21 00:22 praxis/layers/attentions_test.py
--rw-r--r--  2.0 unx     7169 b- defN 24-Feb-21 00:22 praxis/layers/augmentations.py
--rw-r--r--  2.0 unx     8015 b- defN 24-Feb-21 00:22 praxis/layers/augmentations_test.py
--rw-r--r--  2.0 unx     1673 b- defN 24-Feb-21 00:22 praxis/layers/base_ops.py
--rw-r--r--  2.0 unx    12641 b- defN 24-Feb-21 00:22 praxis/layers/bregman.py
--rw-r--r--  2.0 unx     5181 b- defN 24-Feb-21 00:22 praxis/layers/bregman_test.py
--rw-r--r--  2.0 unx     4889 b- defN 24-Feb-21 00:22 praxis/layers/checkpoint_policy.py
--rw-r--r--  2.0 unx     2220 b- defN 24-Feb-21 00:22 praxis/layers/chunk.py
--rw-r--r--  2.0 unx     1472 b- defN 24-Feb-21 00:22 praxis/layers/chunk_test.py
--rw-r--r--  2.0 unx    19084 b- defN 24-Feb-21 00:22 praxis/layers/conformers.py
--rw-r--r--  2.0 unx    11325 b- defN 24-Feb-21 00:22 praxis/layers/conformers_test.py
--rw-r--r--  2.0 unx    34498 b- defN 24-Feb-21 00:22 praxis/layers/convolutions.py
--rw-r--r--  2.0 unx    19012 b- defN 24-Feb-21 00:22 praxis/layers/convolutions_test.py
--rw-r--r--  2.0 unx    17816 b- defN 24-Feb-21 00:22 praxis/layers/ctc_objectives.py
--rw-r--r--  2.0 unx    13351 b- defN 24-Feb-21 00:22 praxis/layers/ctc_objectives_test.py
--rw-r--r--  2.0 unx     3364 b- defN 24-Feb-21 00:22 praxis/layers/einsum.py
--rw-r--r--  2.0 unx     2143 b- defN 24-Feb-21 00:22 praxis/layers/einsum_test.py
--rw-r--r--  2.0 unx    50401 b- defN 24-Feb-21 00:22 praxis/layers/embedding_softmax.py
--rw-r--r--  2.0 unx    39419 b- defN 24-Feb-21 00:22 praxis/layers/embedding_softmax_test.py
--rw-r--r--  2.0 unx     6733 b- defN 24-Feb-21 00:22 praxis/layers/flax_adapter.py
--rw-r--r--  2.0 unx    11257 b- defN 24-Feb-21 00:22 praxis/layers/flax_adapter_test.py
--rw-r--r--  2.0 unx    17194 b- defN 24-Feb-21 00:22 praxis/layers/frnn.py
--rw-r--r--  2.0 unx    17737 b- defN 24-Feb-21 00:22 praxis/layers/frnn_test.py
--rw-r--r--  2.0 unx    13776 b- defN 24-Feb-21 00:22 praxis/layers/glam.py
--rw-r--r--  2.0 unx    12304 b- defN 24-Feb-21 00:22 praxis/layers/gpu_fast_attention.py
--rw-r--r--  2.0 unx    16941 b- defN 24-Feb-21 00:22 praxis/layers/grouped_query_attention.py
--rw-r--r--  2.0 unx     6067 b- defN 24-Feb-21 00:22 praxis/layers/grouped_query_attention_test.py
--rw-r--r--  2.0 unx    18696 b- defN 24-Feb-21 00:22 praxis/layers/linears.py
--rw-r--r--  2.0 unx    29853 b- defN 24-Feb-21 00:22 praxis/layers/linears_test.py
--rw-r--r--  2.0 unx     3826 b- defN 24-Feb-21 00:22 praxis/layers/losses.py
--rw-r--r--  2.0 unx    79712 b- defN 24-Feb-21 00:22 praxis/layers/models.py
--rw-r--r--  2.0 unx    56657 b- defN 24-Feb-21 00:22 praxis/layers/models_test.py
--rw-r--r--  2.0 unx    65534 b- defN 24-Feb-21 00:22 praxis/layers/multi_query_attention.py
--rw-r--r--  2.0 unx    28992 b- defN 24-Feb-21 00:22 praxis/layers/multi_query_attention_test.py
--rw-r--r--  2.0 unx    51858 b- defN 24-Feb-21 00:22 praxis/layers/ngrammer.py
--rw-r--r--  2.0 unx    26471 b- defN 24-Feb-21 00:22 praxis/layers/ngrammer_test.py
--rw-r--r--  2.0 unx    22231 b- defN 24-Feb-21 00:22 praxis/layers/normalizations.py
--rw-r--r--  2.0 unx    19447 b- defN 24-Feb-21 00:22 praxis/layers/normalizations_test.py
--rw-r--r--  2.0 unx    48753 b- defN 24-Feb-21 00:22 praxis/layers/pipeline.py
--rw-r--r--  2.0 unx    13518 b- defN 24-Feb-21 00:22 praxis/layers/poolings.py
--rw-r--r--  2.0 unx     9528 b- defN 24-Feb-21 00:22 praxis/layers/poolings_test.py
--rw-r--r--  2.0 unx    16337 b- defN 24-Feb-21 00:22 praxis/layers/quantizer.py
--rw-r--r--  2.0 unx     3892 b- defN 24-Feb-21 00:22 praxis/layers/quantizer_objectives.py
--rw-r--r--  2.0 unx     1525 b- defN 24-Feb-21 00:22 praxis/layers/quantizer_objectives_test.py
--rw-r--r--  2.0 unx     4351 b- defN 24-Feb-21 00:22 praxis/layers/quantizer_test.py
--rw-r--r--  2.0 unx    23959 b- defN 24-Feb-21 00:22 praxis/layers/repeats.py
--rw-r--r--  2.0 unx    16308 b- defN 24-Feb-21 00:22 praxis/layers/repeats_test.py
--rw-r--r--  2.0 unx    16250 b- defN 24-Feb-21 00:22 praxis/layers/resnets.py
--rw-r--r--  2.0 unx    17426 b- defN 24-Feb-21 00:22 praxis/layers/rnn_cell.py
--rw-r--r--  2.0 unx    13660 b- defN 24-Feb-21 00:22 praxis/layers/rnn_cell_test.py
--rw-r--r--  2.0 unx     2290 b- defN 24-Feb-21 00:22 praxis/layers/searchable.py
--rw-r--r--  2.0 unx     1910 b- defN 24-Feb-21 00:22 praxis/layers/searchable_test.py
--rw-r--r--  2.0 unx     1270 b- defN 24-Feb-21 00:22 praxis/layers/sequential.py
--rw-r--r--  2.0 unx     3167 b- defN 24-Feb-21 00:22 praxis/layers/sequential_test.py
--rw-r--r--  2.0 unx     4640 b- defN 24-Feb-21 00:22 praxis/layers/sharding.py
--rw-r--r--  2.0 unx     9571 b- defN 24-Feb-21 00:22 praxis/layers/shared_layers_test.py
--rw-r--r--  2.0 unx    10049 b- defN 24-Feb-21 00:22 praxis/layers/spectrum_augmenter.py
--rw-r--r--  2.0 unx     6714 b- defN 24-Feb-21 00:22 praxis/layers/spectrum_augmenter_test.py
--rw-r--r--  2.0 unx    13853 b- defN 24-Feb-21 00:22 praxis/layers/ssm.py
--rw-r--r--  2.0 unx     4454 b- defN 24-Feb-21 00:22 praxis/layers/ssm_test.py
--rw-r--r--  2.0 unx    26821 b- defN 24-Feb-21 00:22 praxis/layers/ssm_transformers.py
--rw-r--r--  2.0 unx     5695 b- defN 24-Feb-21 00:22 praxis/layers/ssm_transformers_test.py
--rw-r--r--  2.0 unx     1537 b- defN 24-Feb-21 00:22 praxis/layers/stats.py
--rw-r--r--  2.0 unx     1258 b- defN 24-Feb-21 00:22 praxis/layers/stats_test.py
--rw-r--r--  2.0 unx     5159 b- defN 24-Feb-21 00:22 praxis/layers/stochastics.py
--rw-r--r--  2.0 unx     4517 b- defN 24-Feb-21 00:22 praxis/layers/stochastics_test.py
--rw-r--r--  2.0 unx     7898 b- defN 24-Feb-21 00:22 praxis/layers/test_layers.py
--rw-r--r--  2.0 unx    77009 b- defN 24-Feb-21 00:22 praxis/layers/transformer_models.py
--rw-r--r--  2.0 unx     9529 b- defN 24-Feb-21 00:22 praxis/layers/transformer_models_encoder_decoder_test.py
--rw-r--r--  2.0 unx    58376 b- defN 24-Feb-21 00:22 praxis/layers/transformer_models_test.py
--rw-r--r--  2.0 unx    93733 b- defN 24-Feb-21 00:22 praxis/layers/transformers.py
--rw-r--r--  2.0 unx    66804 b- defN 24-Feb-21 00:22 praxis/layers/transformers_test.py
--rw-r--r--  2.0 unx    10068 b- defN 24-Feb-21 00:22 praxis/layers/vanillanets.py
--rw-r--r--  2.0 unx     2836 b- defN 24-Feb-21 00:22 praxis/layers/vanillanets_test.py
--rw-r--r--  2.0 unx    18433 b- defN 24-Feb-21 00:22 praxis/layers/vits.py
--rw-r--r--  2.0 unx     9661 b- defN 24-Feb-21 00:22 praxis/layers/vits_test.py
--rw-r--r--  2.0 unx     1343 b- defN 24-Feb-21 00:22 praxis/layers/chain/__init__.py
--rw-r--r--  2.0 unx     5210 b- defN 24-Feb-21 00:22 praxis/layers/chain/chain.py
--rw-r--r--  2.0 unx     8837 b- defN 24-Feb-21 00:22 praxis/layers/chain/chain_extensions.py
--rw-r--r--  2.0 unx     6887 b- defN 24-Feb-21 00:22 praxis/layers/chain/chain_test.py
--rw-r--r--  2.0 unx      710 b- defN 24-Feb-21 00:22 praxis/layers/injection/__init__.py
--rw-r--r--  2.0 unx     3718 b- defN 24-Feb-21 00:22 praxis/layers/injection/fp8_nvidia_gpu.py
--rw-r--r--  2.0 unx     3603 b- defN 24-Feb-21 00:22 praxis/layers/injection/fp8_nvidia_gpu_test.py
--rw-r--r--  2.0 unx     2041 b- defN 24-Feb-21 00:22 praxis/layers/quantization/__init__.py
--rw-r--r--  2.0 unx    31334 b- defN 24-Feb-21 00:22 praxis/layers/quantization/attentions.py
--rw-r--r--  2.0 unx    20575 b- defN 24-Feb-21 00:22 praxis/layers/quantization/attentions_test.py
--rw-r--r--  2.0 unx     3101 b- defN 24-Feb-21 00:22 praxis/layers/quantization/automl_select.py
--rw-r--r--  2.0 unx     1937 b- defN 24-Feb-21 00:22 praxis/layers/quantization/automl_select_test.py
--rw-r--r--  2.0 unx     2703 b- defN 24-Feb-21 00:22 praxis/layers/quantization/conformers.py
--rw-r--r--  2.0 unx     3174 b- defN 24-Feb-21 00:22 praxis/layers/quantization/conformers_test.py
--rw-r--r--  2.0 unx     4105 b- defN 24-Feb-21 00:22 praxis/layers/quantization/convolutions.py
--rw-r--r--  2.0 unx     3698 b- defN 24-Feb-21 00:22 praxis/layers/quantization/convolutions_test.py
--rw-r--r--  2.0 unx     3349 b- defN 24-Feb-21 00:22 praxis/layers/quantization/einsum.py
--rw-r--r--  2.0 unx     2338 b- defN 24-Feb-21 00:22 praxis/layers/quantization/einsum_test.py
--rw-r--r--  2.0 unx    23612 b- defN 24-Feb-21 00:22 praxis/layers/quantization/embedding_softmax.py
--rw-r--r--  2.0 unx    20031 b- defN 24-Feb-21 00:22 praxis/layers/quantization/embedding_softmax_test.py
--rw-r--r--  2.0 unx    13073 b- defN 24-Feb-21 00:22 praxis/layers/quantization/linears.py
--rw-r--r--  2.0 unx    26152 b- defN 24-Feb-21 00:22 praxis/layers/quantization/linears_test.py
--rw-r--r--  2.0 unx     7762 b- defN 24-Feb-21 00:22 praxis/layers/quantization/multi_query_attention.py
--rw-r--r--  2.0 unx     4889 b- defN 24-Feb-21 00:22 praxis/layers/quantization/multi_query_attention_test.py
--rw-r--r--  2.0 unx     4853 b- defN 24-Feb-21 00:22 praxis/layers/quantization/ngrammer.py
--rw-r--r--  2.0 unx    17527 b- defN 24-Feb-21 00:22 praxis/layers/quantization/ngrammer_test.py
--rw-r--r--  2.0 unx    34582 b- defN 24-Feb-21 00:22 praxis/layers/quantization/operations.py
--rw-r--r--  2.0 unx    31830 b- defN 24-Feb-21 00:22 praxis/layers/quantization/operations_test.py
--rw-r--r--  2.0 unx     5601 b- defN 24-Feb-21 00:22 praxis/layers/quantization/optimization.py
--rw-r--r--  2.0 unx     2479 b- defN 24-Feb-21 00:22 praxis/layers/quantization/optimization_test.py
--rw-r--r--  2.0 unx     5388 b- defN 24-Feb-21 00:22 praxis/layers/quantization/overflow_check.py
--rw-r--r--  2.0 unx     6063 b- defN 24-Feb-21 00:22 praxis/layers/quantization/overflow_check_test.py
--rw-r--r--  2.0 unx     7911 b- defN 24-Feb-21 00:22 praxis/layers/quantization/quantization_hparams.py
--rw-r--r--  2.0 unx     5626 b- defN 24-Feb-21 00:22 praxis/layers/quantization/quantization_test.py
--rw-r--r--  2.0 unx    30116 b- defN 24-Feb-21 00:22 praxis/layers/quantization/quantize.py
--rw-r--r--  2.0 unx    18578 b- defN 24-Feb-21 00:22 praxis/layers/quantization/quantize_test.py
--rw-r--r--  2.0 unx    29519 b- defN 24-Feb-21 00:22 praxis/layers/quantization/quantizer.py
--rw-r--r--  2.0 unx    23801 b- defN 24-Feb-21 00:22 praxis/layers/quantization/quantizer_test.py
--rw-r--r--  2.0 unx     3965 b- defN 24-Feb-21 00:22 praxis/layers/quantization/searchable.py
--rw-r--r--  2.0 unx     3786 b- defN 24-Feb-21 00:22 praxis/layers/quantization/searchable_test.py
--rw-r--r--  2.0 unx    12190 b- defN 24-Feb-21 00:22 praxis/layers/quantization/utils.py
--rw-r--r--  2.0 unx     7667 b- defN 24-Feb-21 00:22 praxis/layers/quantization/utils_test.py
--rw-r--r--  2.0 unx      644 b- defN 24-Feb-21 00:22 praxis/layers/quantization/sparsity/__init__.py
--rw-r--r--  2.0 unx    15376 b- defN 24-Feb-21 00:22 praxis/layers/quantization/sparsity/attentions_test.py
--rw-r--r--  2.0 unx    10473 b- defN 24-Feb-21 00:22 praxis/layers/quantization/sparsity/linears_test.py
--rw-r--r--  2.0 unx    19054 b- defN 24-Feb-21 00:22 praxis/layers/quantization/sparsity/sparsifier.py
--rw-r--r--  2.0 unx    42196 b- defN 24-Feb-21 00:22 praxis/layers/quantization/sparsity/sparsifier_test.py
--rw-r--r--  2.0 unx     9579 b- defN 24-Feb-21 00:22 praxis/layers/quantization/sparsity/sparsity.py
--rw-r--r--  2.0 unx     8299 b- defN 24-Feb-21 00:22 praxis/layers/quantization/sparsity/sparsity_hparams.py
--rw-r--r--  2.0 unx     5898 b- defN 24-Feb-21 00:22 praxis/layers/quantization/sparsity/sparsity_modes.py
--rw-r--r--  2.0 unx    12254 b- defN 24-Feb-21 00:22 praxis/layers/quantization/sparsity/sparsity_test.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Feb-21 00:54 praxis-1.3.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     1115 b- defN 24-Feb-21 00:54 praxis-1.3.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Feb-21 00:54 praxis-1.3.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 24-Feb-21 00:54 praxis-1.3.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    16146 b- defN 24-Feb-21 00:54 praxis-1.3.1.dist-info/RECORD
-180 files, 3237016 bytes uncompressed, 735938 bytes compressed:  77.3%
+Zip file size: 772480 bytes, number of entries: 180
+-rw-r--r--  2.0 unx      596 b- defN 24-Apr-09 17:52 praxis/__init__.py
+-rw-r--r--  2.0 unx    22000 b- defN 24-Apr-09 17:52 praxis/asserts.py
+-rw-r--r--  2.0 unx    11563 b- defN 24-Apr-09 17:52 praxis/asserts_test.py
+-rw-r--r--  2.0 unx    44079 b- defN 24-Apr-09 17:52 praxis/base_hyperparams.py
+-rw-r--r--  2.0 unx    27429 b- defN 24-Apr-09 17:52 praxis/base_hyperparams_test.py
+-rw-r--r--  2.0 unx    44661 b- defN 24-Apr-09 17:52 praxis/base_input.py
+-rw-r--r--  2.0 unx    34620 b- defN 24-Apr-09 17:52 praxis/base_input_test.py
+-rw-r--r--  2.0 unx   100935 b- defN 24-Apr-09 17:52 praxis/base_layer.py
+-rw-r--r--  2.0 unx    31284 b- defN 24-Apr-09 17:52 praxis/base_layer_test.py
+-rw-r--r--  2.0 unx     5773 b- defN 24-Apr-09 17:52 praxis/base_model.py
+-rw-r--r--  2.0 unx    19218 b- defN 24-Apr-09 17:52 praxis/beam_search.py
+-rw-r--r--  2.0 unx    15704 b- defN 24-Apr-09 17:52 praxis/beam_search_test.py
+-rw-r--r--  2.0 unx     6540 b- defN 24-Apr-09 17:52 praxis/decoder_hparams.py
+-rw-r--r--  2.0 unx    25777 b- defN 24-Apr-09 17:52 praxis/decoder_utils.py
+-rw-r--r--  2.0 unx    12772 b- defN 24-Apr-09 17:52 praxis/decoder_utils_test.py
+-rw-r--r--  2.0 unx     1475 b- defN 24-Apr-09 17:52 praxis/fiddle_tags.py
+-rw-r--r--  2.0 unx    11591 b- defN 24-Apr-09 17:52 praxis/flat_beam_search.py
+-rw-r--r--  2.0 unx     4407 b- defN 24-Apr-09 17:52 praxis/flat_beam_search_test.py
+-rw-r--r--  2.0 unx     5602 b- defN 24-Apr-09 17:52 praxis/flax_utils.py
+-rw-r--r--  2.0 unx    35272 b- defN 24-Apr-09 17:52 praxis/gshard_utils.py
+-rw-r--r--  2.0 unx     2600 b- defN 24-Apr-09 17:52 praxis/lazy_loader.py
+-rw-r--r--  2.0 unx     1555 b- defN 24-Apr-09 17:52 praxis/lingvo_lib.py
+-rw-r--r--  2.0 unx     2523 b- defN 24-Apr-09 17:52 praxis/metric_utils.py
+-rw-r--r--  2.0 unx    16997 b- defN 24-Apr-09 17:52 praxis/optimizer_prefix_vectorization.py
+-rw-r--r--  2.0 unx     3196 b- defN 24-Apr-09 17:52 praxis/optimizer_prefix_vectorization_test.py
+-rw-r--r--  2.0 unx   113212 b- defN 24-Apr-09 17:52 praxis/optimizers.py
+-rw-r--r--  2.0 unx    13883 b- defN 24-Apr-09 17:52 praxis/optimizers_test.py
+-rw-r--r--  2.0 unx    37119 b- defN 24-Apr-09 17:52 praxis/pax_fiddle.py
+-rw-r--r--  2.0 unx    41079 b- defN 24-Apr-09 17:52 praxis/pax_fiddle_test.py
+-rw-r--r--  2.0 unx    39434 b- defN 24-Apr-09 17:52 praxis/py_utils.py
+-rw-r--r--  2.0 unx    18905 b- defN 24-Apr-09 17:52 praxis/py_utils_test.py
+-rw-r--r--  2.0 unx     5338 b- defN 24-Apr-09 17:52 praxis/pytypes.py
+-rw-r--r--  2.0 unx     1583 b- defN 24-Apr-09 17:52 praxis/pytypes_test.py
+-rw-r--r--  2.0 unx    76553 b- defN 24-Apr-09 17:52 praxis/sample_decode.py
+-rw-r--r--  2.0 unx    26698 b- defN 24-Apr-09 17:52 praxis/sample_decode_test.py
+-rw-r--r--  2.0 unx    22153 b- defN 24-Apr-09 17:52 praxis/schedules.py
+-rw-r--r--  2.0 unx    21498 b- defN 24-Apr-09 17:52 praxis/schedules_test.py
+-rw-r--r--  2.0 unx    18628 b- defN 24-Apr-09 17:52 praxis/test_utils.py
+-rw-r--r--  2.0 unx    24670 b- defN 24-Apr-09 17:52 praxis/token_samplers.py
+-rw-r--r--  2.0 unx    11225 b- defN 24-Apr-09 17:52 praxis/token_samplers_test.py
+-rw-r--r--  2.0 unx     8428 b- defN 24-Apr-09 17:52 praxis/trees.py
+-rw-r--r--  2.0 unx    12764 b- defN 24-Apr-09 17:52 praxis/trees_test.py
+-rw-r--r--  2.0 unx     6959 b- defN 24-Apr-09 17:52 praxis/layers/__init__.py
+-rw-r--r--  2.0 unx     5340 b- defN 24-Apr-09 17:52 praxis/layers/activations.py
+-rw-r--r--  2.0 unx     1858 b- defN 24-Apr-09 17:52 praxis/layers/activations_test.py
+-rw-r--r--  2.0 unx     8286 b- defN 24-Apr-09 17:52 praxis/layers/adapters.py
+-rw-r--r--  2.0 unx     6878 b- defN 24-Apr-09 17:52 praxis/layers/adapters_test.py
+-rw-r--r--  2.0 unx   130505 b- defN 24-Apr-09 17:52 praxis/layers/attentions.py
+-rw-r--r--  2.0 unx    88020 b- defN 24-Apr-09 17:52 praxis/layers/attentions_test.py
+-rw-r--r--  2.0 unx     7169 b- defN 24-Apr-09 17:52 praxis/layers/augmentations.py
+-rw-r--r--  2.0 unx     8903 b- defN 24-Apr-09 17:52 praxis/layers/augmentations_test.py
+-rw-r--r--  2.0 unx     1673 b- defN 24-Apr-09 17:52 praxis/layers/base_ops.py
+-rw-r--r--  2.0 unx    12641 b- defN 24-Apr-09 17:52 praxis/layers/bregman.py
+-rw-r--r--  2.0 unx     5181 b- defN 24-Apr-09 17:52 praxis/layers/bregman_test.py
+-rw-r--r--  2.0 unx     4889 b- defN 24-Apr-09 17:52 praxis/layers/checkpoint_policy.py
+-rw-r--r--  2.0 unx     2220 b- defN 24-Apr-09 17:52 praxis/layers/chunk.py
+-rw-r--r--  2.0 unx     1472 b- defN 24-Apr-09 17:52 praxis/layers/chunk_test.py
+-rw-r--r--  2.0 unx    19084 b- defN 24-Apr-09 17:52 praxis/layers/conformers.py
+-rw-r--r--  2.0 unx    11395 b- defN 24-Apr-09 17:52 praxis/layers/conformers_test.py
+-rw-r--r--  2.0 unx    34498 b- defN 24-Apr-09 17:52 praxis/layers/convolutions.py
+-rw-r--r--  2.0 unx    19012 b- defN 24-Apr-09 17:52 praxis/layers/convolutions_test.py
+-rw-r--r--  2.0 unx    17834 b- defN 24-Apr-09 17:52 praxis/layers/ctc_objectives.py
+-rw-r--r--  2.0 unx    13365 b- defN 24-Apr-09 17:52 praxis/layers/ctc_objectives_test.py
+-rw-r--r--  2.0 unx     3364 b- defN 24-Apr-09 17:52 praxis/layers/einsum.py
+-rw-r--r--  2.0 unx     2143 b- defN 24-Apr-09 17:52 praxis/layers/einsum_test.py
+-rw-r--r--  2.0 unx    50399 b- defN 24-Apr-09 17:52 praxis/layers/embedding_softmax.py
+-rw-r--r--  2.0 unx    39419 b- defN 24-Apr-09 17:52 praxis/layers/embedding_softmax_test.py
+-rw-r--r--  2.0 unx     6733 b- defN 24-Apr-09 17:52 praxis/layers/flax_adapter.py
+-rw-r--r--  2.0 unx    11273 b- defN 24-Apr-09 17:52 praxis/layers/flax_adapter_test.py
+-rw-r--r--  2.0 unx    17194 b- defN 24-Apr-09 17:52 praxis/layers/frnn.py
+-rw-r--r--  2.0 unx    17737 b- defN 24-Apr-09 17:52 praxis/layers/frnn_test.py
+-rw-r--r--  2.0 unx    13988 b- defN 24-Apr-09 17:52 praxis/layers/glam.py
+-rw-r--r--  2.0 unx    17268 b- defN 24-Apr-09 17:52 praxis/layers/gpu_fast_attention.py
+-rw-r--r--  2.0 unx    16941 b- defN 24-Apr-09 17:52 praxis/layers/grouped_query_attention.py
+-rw-r--r--  2.0 unx     6067 b- defN 24-Apr-09 17:52 praxis/layers/grouped_query_attention_test.py
+-rw-r--r--  2.0 unx    18696 b- defN 24-Apr-09 17:52 praxis/layers/linears.py
+-rw-r--r--  2.0 unx    29853 b- defN 24-Apr-09 17:52 praxis/layers/linears_test.py
+-rw-r--r--  2.0 unx     3826 b- defN 24-Apr-09 17:52 praxis/layers/losses.py
+-rw-r--r--  2.0 unx    78696 b- defN 24-Apr-09 17:52 praxis/layers/models.py
+-rw-r--r--  2.0 unx    57477 b- defN 24-Apr-09 17:52 praxis/layers/models_test.py
+-rw-r--r--  2.0 unx    65639 b- defN 24-Apr-09 17:52 praxis/layers/multi_query_attention.py
+-rw-r--r--  2.0 unx    28992 b- defN 24-Apr-09 17:52 praxis/layers/multi_query_attention_test.py
+-rw-r--r--  2.0 unx    51858 b- defN 24-Apr-09 17:52 praxis/layers/ngrammer.py
+-rw-r--r--  2.0 unx    26513 b- defN 24-Apr-09 17:52 praxis/layers/ngrammer_test.py
+-rw-r--r--  2.0 unx    22231 b- defN 24-Apr-09 17:52 praxis/layers/normalizations.py
+-rw-r--r--  2.0 unx    19447 b- defN 24-Apr-09 17:52 praxis/layers/normalizations_test.py
+-rw-r--r--  2.0 unx    48753 b- defN 24-Apr-09 17:52 praxis/layers/pipeline.py
+-rw-r--r--  2.0 unx    13518 b- defN 24-Apr-09 17:52 praxis/layers/poolings.py
+-rw-r--r--  2.0 unx     9528 b- defN 24-Apr-09 17:52 praxis/layers/poolings_test.py
+-rw-r--r--  2.0 unx    16337 b- defN 24-Apr-09 17:52 praxis/layers/quantizer.py
+-rw-r--r--  2.0 unx     3892 b- defN 24-Apr-09 17:52 praxis/layers/quantizer_objectives.py
+-rw-r--r--  2.0 unx     1690 b- defN 24-Apr-09 17:52 praxis/layers/quantizer_objectives_test.py
+-rw-r--r--  2.0 unx     4351 b- defN 24-Apr-09 17:52 praxis/layers/quantizer_test.py
+-rw-r--r--  2.0 unx    23959 b- defN 24-Apr-09 17:52 praxis/layers/repeats.py
+-rw-r--r--  2.0 unx    16378 b- defN 24-Apr-09 17:52 praxis/layers/repeats_test.py
+-rw-r--r--  2.0 unx    16250 b- defN 24-Apr-09 17:52 praxis/layers/resnets.py
+-rw-r--r--  2.0 unx    17426 b- defN 24-Apr-09 17:52 praxis/layers/rnn_cell.py
+-rw-r--r--  2.0 unx    13660 b- defN 24-Apr-09 17:52 praxis/layers/rnn_cell_test.py
+-rw-r--r--  2.0 unx     2290 b- defN 24-Apr-09 17:52 praxis/layers/searchable.py
+-rw-r--r--  2.0 unx     1910 b- defN 24-Apr-09 17:52 praxis/layers/searchable_test.py
+-rw-r--r--  2.0 unx     1270 b- defN 24-Apr-09 17:52 praxis/layers/sequential.py
+-rw-r--r--  2.0 unx     3167 b- defN 24-Apr-09 17:52 praxis/layers/sequential_test.py
+-rw-r--r--  2.0 unx     4657 b- defN 24-Apr-09 17:52 praxis/layers/sharding.py
+-rw-r--r--  2.0 unx     9571 b- defN 24-Apr-09 17:52 praxis/layers/shared_layers_test.py
+-rw-r--r--  2.0 unx    10049 b- defN 24-Apr-09 17:52 praxis/layers/spectrum_augmenter.py
+-rw-r--r--  2.0 unx     6714 b- defN 24-Apr-09 17:52 praxis/layers/spectrum_augmenter_test.py
+-rw-r--r--  2.0 unx    13853 b- defN 24-Apr-09 17:52 praxis/layers/ssm.py
+-rw-r--r--  2.0 unx     4454 b- defN 24-Apr-09 17:52 praxis/layers/ssm_test.py
+-rw-r--r--  2.0 unx    26821 b- defN 24-Apr-09 17:52 praxis/layers/ssm_transformers.py
+-rw-r--r--  2.0 unx     5695 b- defN 24-Apr-09 17:52 praxis/layers/ssm_transformers_test.py
+-rw-r--r--  2.0 unx     1537 b- defN 24-Apr-09 17:52 praxis/layers/stats.py
+-rw-r--r--  2.0 unx     1258 b- defN 24-Apr-09 17:52 praxis/layers/stats_test.py
+-rw-r--r--  2.0 unx     5159 b- defN 24-Apr-09 17:52 praxis/layers/stochastics.py
+-rw-r--r--  2.0 unx     4517 b- defN 24-Apr-09 17:52 praxis/layers/stochastics_test.py
+-rw-r--r--  2.0 unx     7898 b- defN 24-Apr-09 17:52 praxis/layers/test_layers.py
+-rw-r--r--  2.0 unx    80038 b- defN 24-Apr-09 17:52 praxis/layers/transformer_models.py
+-rw-r--r--  2.0 unx     9529 b- defN 24-Apr-09 17:52 praxis/layers/transformer_models_encoder_decoder_test.py
+-rw-r--r--  2.0 unx    58376 b- defN 24-Apr-09 17:52 praxis/layers/transformer_models_test.py
+-rw-r--r--  2.0 unx    93733 b- defN 24-Apr-09 17:52 praxis/layers/transformers.py
+-rw-r--r--  2.0 unx    67654 b- defN 24-Apr-09 17:52 praxis/layers/transformers_test.py
+-rw-r--r--  2.0 unx    10068 b- defN 24-Apr-09 17:52 praxis/layers/vanillanets.py
+-rw-r--r--  2.0 unx     2836 b- defN 24-Apr-09 17:52 praxis/layers/vanillanets_test.py
+-rw-r--r--  2.0 unx    18433 b- defN 24-Apr-09 17:52 praxis/layers/vits.py
+-rw-r--r--  2.0 unx     9661 b- defN 24-Apr-09 17:52 praxis/layers/vits_test.py
+-rw-r--r--  2.0 unx     1343 b- defN 24-Apr-09 17:52 praxis/layers/chain/__init__.py
+-rw-r--r--  2.0 unx     5210 b- defN 24-Apr-09 17:52 praxis/layers/chain/chain.py
+-rw-r--r--  2.0 unx     8837 b- defN 24-Apr-09 17:52 praxis/layers/chain/chain_extensions.py
+-rw-r--r--  2.0 unx     6913 b- defN 24-Apr-09 17:52 praxis/layers/chain/chain_test.py
+-rw-r--r--  2.0 unx      710 b- defN 24-Apr-09 17:52 praxis/layers/injection/__init__.py
+-rw-r--r--  2.0 unx     3718 b- defN 24-Apr-09 17:52 praxis/layers/injection/fp8_nvidia_gpu.py
+-rw-r--r--  2.0 unx     3603 b- defN 24-Apr-09 17:52 praxis/layers/injection/fp8_nvidia_gpu_test.py
+-rw-r--r--  2.0 unx     2173 b- defN 24-Apr-09 17:52 praxis/layers/quantization/__init__.py
+-rw-r--r--  2.0 unx    38463 b- defN 24-Apr-09 17:52 praxis/layers/quantization/attentions.py
+-rw-r--r--  2.0 unx    23701 b- defN 24-Apr-09 17:52 praxis/layers/quantization/attentions_test.py
+-rw-r--r--  2.0 unx     3101 b- defN 24-Apr-09 17:52 praxis/layers/quantization/automl_select.py
+-rw-r--r--  2.0 unx     1937 b- defN 24-Apr-09 17:52 praxis/layers/quantization/automl_select_test.py
+-rw-r--r--  2.0 unx     2703 b- defN 24-Apr-09 17:52 praxis/layers/quantization/conformers.py
+-rw-r--r--  2.0 unx     3217 b- defN 24-Apr-09 17:52 praxis/layers/quantization/conformers_test.py
+-rw-r--r--  2.0 unx     4105 b- defN 24-Apr-09 17:52 praxis/layers/quantization/convolutions.py
+-rw-r--r--  2.0 unx     3698 b- defN 24-Apr-09 17:52 praxis/layers/quantization/convolutions_test.py
+-rw-r--r--  2.0 unx     3349 b- defN 24-Apr-09 17:52 praxis/layers/quantization/einsum.py
+-rw-r--r--  2.0 unx     2338 b- defN 24-Apr-09 17:52 praxis/layers/quantization/einsum_test.py
+-rw-r--r--  2.0 unx    23612 b- defN 24-Apr-09 17:52 praxis/layers/quantization/embedding_softmax.py
+-rw-r--r--  2.0 unx    20031 b- defN 24-Apr-09 17:52 praxis/layers/quantization/embedding_softmax_test.py
+-rw-r--r--  2.0 unx    18061 b- defN 24-Apr-09 17:52 praxis/layers/quantization/linears.py
+-rw-r--r--  2.0 unx    27071 b- defN 24-Apr-09 17:52 praxis/layers/quantization/linears_test.py
+-rw-r--r--  2.0 unx     7833 b- defN 24-Apr-09 17:52 praxis/layers/quantization/multi_query_attention.py
+-rw-r--r--  2.0 unx     4889 b- defN 24-Apr-09 17:52 praxis/layers/quantization/multi_query_attention_test.py
+-rw-r--r--  2.0 unx     4853 b- defN 24-Apr-09 17:52 praxis/layers/quantization/ngrammer.py
+-rw-r--r--  2.0 unx    17527 b- defN 24-Apr-09 17:52 praxis/layers/quantization/ngrammer_test.py
+-rw-r--r--  2.0 unx    38313 b- defN 24-Apr-09 17:52 praxis/layers/quantization/operations.py
+-rw-r--r--  2.0 unx    37067 b- defN 24-Apr-09 17:52 praxis/layers/quantization/operations_test.py
+-rw-r--r--  2.0 unx     5601 b- defN 24-Apr-09 17:52 praxis/layers/quantization/optimization.py
+-rw-r--r--  2.0 unx     2479 b- defN 24-Apr-09 17:52 praxis/layers/quantization/optimization_test.py
+-rw-r--r--  2.0 unx     5388 b- defN 24-Apr-09 17:52 praxis/layers/quantization/overflow_check.py
+-rw-r--r--  2.0 unx     6063 b- defN 24-Apr-09 17:52 praxis/layers/quantization/overflow_check_test.py
+-rw-r--r--  2.0 unx     9082 b- defN 24-Apr-09 17:52 praxis/layers/quantization/quantization_hparams.py
+-rw-r--r--  2.0 unx     5626 b- defN 24-Apr-09 17:52 praxis/layers/quantization/quantization_test.py
+-rw-r--r--  2.0 unx    40176 b- defN 24-Apr-09 17:52 praxis/layers/quantization/quantize.py
+-rw-r--r--  2.0 unx    18578 b- defN 24-Apr-09 17:52 praxis/layers/quantization/quantize_test.py
+-rw-r--r--  2.0 unx    29590 b- defN 24-Apr-09 17:52 praxis/layers/quantization/quantizer.py
+-rw-r--r--  2.0 unx    23806 b- defN 24-Apr-09 17:52 praxis/layers/quantization/quantizer_test.py
+-rw-r--r--  2.0 unx     3965 b- defN 24-Apr-09 17:52 praxis/layers/quantization/searchable.py
+-rw-r--r--  2.0 unx     3790 b- defN 24-Apr-09 17:52 praxis/layers/quantization/searchable_test.py
+-rw-r--r--  2.0 unx    18029 b- defN 24-Apr-09 17:52 praxis/layers/quantization/utils.py
+-rw-r--r--  2.0 unx    10211 b- defN 24-Apr-09 17:52 praxis/layers/quantization/utils_test.py
+-rw-r--r--  2.0 unx      644 b- defN 24-Apr-09 17:52 praxis/layers/quantization/sparsity/__init__.py
+-rw-r--r--  2.0 unx    15376 b- defN 24-Apr-09 17:52 praxis/layers/quantization/sparsity/attentions_test.py
+-rw-r--r--  2.0 unx    10473 b- defN 24-Apr-09 17:52 praxis/layers/quantization/sparsity/linears_test.py
+-rw-r--r--  2.0 unx    19155 b- defN 24-Apr-09 17:52 praxis/layers/quantization/sparsity/sparsifier.py
+-rw-r--r--  2.0 unx    44642 b- defN 24-Apr-09 17:52 praxis/layers/quantization/sparsity/sparsifier_test.py
+-rw-r--r--  2.0 unx    13598 b- defN 24-Apr-09 17:52 praxis/layers/quantization/sparsity/sparsity.py
+-rw-r--r--  2.0 unx     9407 b- defN 24-Apr-09 17:52 praxis/layers/quantization/sparsity/sparsity_hparams.py
+-rw-r--r--  2.0 unx     5898 b- defN 24-Apr-09 17:52 praxis/layers/quantization/sparsity/sparsity_modes.py
+-rw-r--r--  2.0 unx    19317 b- defN 24-Apr-09 17:52 praxis/layers/quantization/sparsity/sparsity_test.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Apr-09 18:19 praxis-1.4.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     1218 b- defN 24-Apr-09 18:19 praxis-1.4.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-09 18:19 praxis-1.4.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 24-Apr-09 18:19 praxis-1.4.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    16148 b- defN 24-Apr-09 18:19 praxis-1.4.0.dist-info/RECORD
+180 files, 3309257 bytes uncompressed, 747074 bytes compressed:  77.4%
```

## zipnote {}

```diff
@@ -519,23 +519,23 @@
 
 Filename: praxis/layers/quantization/sparsity/sparsity_modes.py
 Comment: 
 
 Filename: praxis/layers/quantization/sparsity/sparsity_test.py
 Comment: 
 
-Filename: praxis-1.3.1.dist-info/LICENSE
+Filename: praxis-1.4.0.dist-info/LICENSE
 Comment: 
 
-Filename: praxis-1.3.1.dist-info/METADATA
+Filename: praxis-1.4.0.dist-info/METADATA
 Comment: 
 
-Filename: praxis-1.3.1.dist-info/WHEEL
+Filename: praxis-1.4.0.dist-info/WHEEL
 Comment: 
 
-Filename: praxis-1.3.1.dist-info/top_level.txt
+Filename: praxis-1.4.0.dist-info/top_level.txt
 Comment: 
 
-Filename: praxis-1.3.1.dist-info/RECORD
+Filename: praxis-1.4.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## praxis/base_hyperparams_test.py

```diff
@@ -11,14 +11,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for base_hyperparams."""
 
+from __future__ import annotations
+
 import dataclasses
 import functools
 import inspect
 import pickle
 import textwrap
 from typing import Any, Callable, NamedTuple
 
@@ -234,19 +236,20 @@
     x.freeze()
     with self.assertRaises(AttributeError):
       x.d.a = 100
     x.unfreeze()
     x.d.a = 200
     self.assertEqual(200, x.d.a)
     x.freeze()
-    self.assertEqual(True, x._internal_frozen)
-    self.assertEqual(True, x.d._internal_frozen)
+    self.assertTrue(x._internal_frozen)
+    assert x.d is not None
+    self.assertTrue(x.d._internal_frozen)
     x_clone = x.clone()
     self.assertEqual(200, x_clone.d.a)
-    self.assertEqual(False, x_clone._internal_frozen)
+    self.assertFalse(x_clone._internal_frozen)
     x_clone.d.a = 300
     self.assertEqual(300, x_clone.d.a)
     # pylint: enable=protected-access
 
   def test_copy_fields(self):
     e_new = 0.123
     a_new = 123
@@ -790,19 +793,19 @@
     # ignored when traversing Flax modules.
     class LayerWithSublayer(base_layer.BaseLayer):
       sub_layer: base_layer.BaseLayer = pax_fiddle.instance_field(
           FiddleToTextTestClass
       )
 
     class WeightedLoss(NamedTuple):
-      loss: Callable[..., Any]
+      loss: Callable[..., Any] | pax_fiddle.Config
       weight: float = 1.0
 
     class Model(base_layer.BaseLayer):
-      weighted_loss: WeightedLoss = None
+      weighted_loss: Any | None = None
 
       def __call__(self, x):
         return x
 
     layer_cfg = pax_fiddle.Config(
         Model,
         weighted_loss=WeightedLoss(
```

## praxis/base_input_test.py

```diff
@@ -15,15 +15,16 @@
 
 """Tests for base_input."""
 
 import dataclasses
 import itertools
 import os
 import pickle
-from typing import Any
+import typing
+from typing import Any, Type
 from unittest import mock
 
 from absl import flags
 from absl.testing import absltest
 from absl.testing import parameterized
 import fiddle as fdl
 import jax
@@ -113,18 +114,21 @@
   def _set_state_internal(self, state: bytes):
     self._state = int(state.decode())
     self._iter = iter(self.dataset.skip(self._state))
 
 
 class LingvoInput(base_input_generator.BaseInputGeneratorFromFiles):
 
-  def _DataSourceFromFilePattern(self,
-                                 file_pattern,
-                                 input_source_weights=None,
-                                 **extra_input_kwargs):
+  def _DataSourceFromFilePattern(
+      self,
+      file_pattern,
+      input_source_weights=None,
+      input_source_id_offset=0,
+      **extra_input_kwargs,
+  ):
     p = self.params
     assert not tf.compat.v1.executing_eagerly()
     assert tf.compat.v1.executing_eagerly_outside_functions()
 
     def _process(source_id, record):
       del source_id
       num = tf.strings.to_number(record, tf.int32)
@@ -333,21 +337,21 @@
     self.assertEqual(x.data2, 1)
 
   def test_tfdata_input(self):
     p = pax_fiddle.Config(TestInput)
     p.num_infeed_hosts = 3
     p.input_random_seed = 345
     p.batch_size = 2
-    train = [None] * p.num_infeed_hosts
-    test = [None] * p.num_infeed_hosts
+    train = []
+    test = []
     for i in range(p.num_infeed_hosts):
       train_p = p.clone().set(infeed_host_index=i)
       test_p = train_p.clone().set(reset_for_eval=True)
-      train[i] = instantiate(train_p)
-      test[i] = instantiate(test_p)
+      train.append(instantiate(train_p))
+      test.append(instantiate(test_p))
 
     num_train_batches = 10
     for _ in range(num_train_batches):
       for i in range(p.num_infeed_hosts):
         batch = train[i].get_next()
         self.assertTrue(np.all(batch.data % p.num_infeed_hosts == i))
 
@@ -370,21 +374,21 @@
   # operations to verify that restorable input objects can be saved & restored
   # correctly.
   def test_tfdata_input_save_restore(self):
     p = pax_fiddle.Config(TestInputCheckpointable)
     p.num_infeed_hosts = 3
     p.input_random_seed = 345
     p.batch_size = 2
-    train = [None] * p.num_infeed_hosts
-    test = [None] * p.num_infeed_hosts
+    train = []
+    test = []
     for i in range(p.num_infeed_hosts):
       train_p = p.clone().set(infeed_host_index=i)
       test_p = train_p.clone().set(reset_for_eval=True)
-      train[i] = instantiate(train_p)
-      test[i] = instantiate(test_p)
+      train.append(instantiate(train_p))
+      test.append(instantiate(test_p))
 
     for i in range(p.num_infeed_hosts):
       train[i] = pickle.loads(pickle.dumps(train[i]))
       test[i] = pickle.loads(pickle.dumps(test[i]))
 
     num_train_batches = 10
     for _ in range(num_train_batches):
@@ -639,18 +643,17 @@
         bucket_batch_limit=[1])
     adaptor_p = pax_fiddle.Config(
         base_input.LingvoEvalAdaptor,
         input=input_p,
         batch_size=2,
         num_infeed_hosts=3,
     )
-    self.assertEqual(fdl.get_callable(adaptor_p).get_batch_size(adaptor_p), 2)
-    self.assertEqual(
-        fdl.get_callable(adaptor_p).get_global_batch_size(adaptor_p), 6
-    )
+    adaptor_cls = typing.cast(Type, fdl.get_callable(adaptor_p))
+    self.assertEqual(adaptor_cls.get_batch_size(adaptor_p), 2)
+    self.assertEqual(adaptor_cls.get_global_batch_size(adaptor_p), 6)
 
   def test_lingvo_lazy_eval_adaptor(self):
     tmp = os.path.join(FLAGS.test_tmpdir, 'lazy_eval_adaptor')
     num_data = 13
     with tf.io.TFRecordWriter(tmp) as w:
       for i in range(num_data):
         w.write(('%04d' % i).encode('utf-8'))
```

## praxis/base_layer.py

```diff
@@ -33,14 +33,15 @@
 from fiddle import daglish
 from flax import core as flax_core
 from flax import linen as nn
 from flax import struct
 import jax
 from jax import numpy as jnp
 from jax import random as jrandom
+from jax.interpreters import pxla
 import numpy as np
 from praxis import asserts
 from praxis import base_hyperparams
 from praxis import pax_fiddle
 from praxis import py_utils
 from praxis import pytypes
 
@@ -276,15 +277,15 @@
   """
 
   assert len(device_axis_names) == len(mesh_shape)
 
   def _get_spec(var_p: WeightHParams) -> jax.sharding.PartitionSpec:
     return to_partition_spec(var_p.full_split_dims_mapping, device_axis_names)
 
-  return jax.tree_map(_get_spec, var_specs)
+  return jax.tree_util.tree_map(_get_spec, var_specs)
 
 
 def transpose_one_axis(
     axis: str | None, mesh_axes_transpose: dict[str, str] | None
 ) -> str | None:
   """Remap one device mesh axis based on mesh_axes_transpose."""
   if mesh_axes_transpose is None or axis is None:
@@ -1003,15 +1004,15 @@
 
 def _is_meta(x):
   return isinstance(x, BoxedParam) or _is_internal_meta(x)
 
 
 def maybe_unbox_value(tree):
   """Return the `value` leaf component of the pytree if it is a BoxedParam."""
-  return jax.tree_map(
+  return jax.tree_util.tree_map(
       lambda bp: bp.value if _is_meta(bp) else bp, tree, is_leaf=_is_meta
   )
 
 
 def _internal_meta_to_hparams(meta: Any) -> WeightHParams:
   # Checks and converts the axis types.
   ndim = len(meta.value.shape)
@@ -1043,15 +1044,15 @@
   # Creates the WeightHParams.
   shape = meta.value.shape[len(repeat_prefix) :]
   param = WeightHParams(shape=shape)
   param.dtype = meta.value.dtype
   param.init = WeightInit.Constant(meta.value)  # TODO(laigd): do we need this?
 
   if meta.mesh is not None:
-    current_mesh = jax.experimental.maps.thread_resources.env.physical_mesh
+    current_mesh = pxla.thread_resources.env.physical_mesh
     if meta.mesh is not current_mesh:
       raise ValueError(
           f'Internal metadata uses a different mesh ({meta.mesh}) than the one'
           f' currently being used ({current_mesh})'
       )
     param.mesh_shape = meta.mesh.devices.shape
 
@@ -1088,15 +1089,15 @@
   def extract_meta(bp):
     if isinstance(bp, BoxedParam):
       return bp.meta
     elif _is_internal_meta(bp):
       return _internal_meta_to_hparams(bp)
     return WeightHParams(shape=bp.shape, dtype=bp.dtype)
 
-  return jax.tree_map(extract_meta, tree, is_leaf=_is_meta)
+  return jax.tree_util.tree_map(extract_meta, tree, is_leaf=_is_meta)
 
 
 class SummaryType(enum.Enum):
   """Types of summary tensors."""
   SCALAR = 1
   IMAGE = 2
   TEXT = 5
@@ -2008,16 +2009,20 @@
     # Disable logging to reduce logspam.
     with py_utils.logging_verbosity_level('FATAL'):
       context_p = JaxContext.HParams(
           do_eval=do_eval, self_reflect_configs=self_reflect_configs
       )
       with JaxContext.new_context(hparams=context_p):
         if self.fprop_dtype == jnp.bfloat16:
-          converted_args = jax.tree_map(_maybe_to_bfloat16_dtype, args)
-          converted_kwargs = jax.tree_map(_maybe_to_bfloat16_dtype, kwargs)
+          converted_args = jax.tree_util.tree_map(
+              _maybe_to_bfloat16_dtype, args
+          )
+          converted_kwargs = jax.tree_util.tree_map(
+              _maybe_to_bfloat16_dtype, kwargs
+          )
         else:
           converted_args = args
           converted_kwargs = kwargs
         variables_abstract = jax.eval_shape(
             init_fn, rngs, *converted_args, **converted_kwargs)
     return variables_abstract
 
@@ -2064,15 +2069,15 @@
         do_eval=do_eval,
         method=method,
         extra_mutable_list=extra_mutable_list,
         self_reflect_configs=True,
         capture_intermediates=capture_intermediates,
         **kwargs,
     )
-    hyper_params = jax.tree_map(
+    hyper_params = jax.tree_util.tree_map(
         lambda x: x.meta,
         variables_abstract[HYPER_PARAMS],
         is_leaf=lambda x: isinstance(x, WrappedHParams),
     )
     return hyper_params
 
   # Notes on Flax interoperability:
```

## praxis/decoder_hparams.py

```diff
@@ -155,7 +155,8 @@
   global_normalize: bool = False
   cf_guidance_scale: list[float] | float | None = None
   controlled_decoding: decoder_utils.ControlledDecodingHParams | None = None
   sort_samples: bool | None = True
   override_next_token_sampler_params: bool = True
   optimize_eos: bool = False
   vanilla_sample_decode: bool = False
+  early_exit: bool = True
```

## praxis/decoder_utils.py

```diff
@@ -130,19 +130,19 @@
   )(logprobs, hyp_ids)
   one_hot = jax.nn.one_hot(ids, logprobs.shape[-1], dtype=logprobs.dtype)
   output_logprobs = jnp.einsum('bkv,bkv->bk', new_logprobs, one_hot)
   return output_logprobs
 
 
 def two_stage_topk(
-    logits: jnp.ndarray,
-    hyp_scores: jnp.ndarray,
+    logits: jax.Array,
+    hyp_scores: jax.Array,
     terminal_ids: list[int],
     tokens_per_beam: int | None = None,
-) -> tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:
+) -> tuple[jax.Array, jax.Array, jax.Array, jax.Array]:
   """Two stage TopK to choose TopK values and indices from each beam.
 
   Args:
     logits: The logits of [batch_size, beam_size, vocab_size or beam_size *
       vocab_size].
     hyp_scores: The topK scores of [batch_size, beam_size].
     terminal_ids: terminal ids. In most cases this is simply eos_id.
@@ -373,34 +373,35 @@
     pad_value: Value for padding.
     batch_size: x.shape[0] in int.
 
   Returns:
     Left aligned tensor with shape [batch_size, seqlen, num_heads, head_dim].
   """
   rank = len(x.shape)
+  slice_sizes = x.shape[1:]
   if rank not in [3, 4]:
     raise ValueError(
         f'Argument `x` needs to be 3 or 4-index, but has shape: {x.shape}'
     )
-
-  out = []
+  pad_width = [[0, max_prefix_len], [0, 0]]
+  if rank == 4:
+    pad_width = pad_width + [[0, 0]]
   for i in range(batch_size):
-    pad_width = [[0, max_prefix_len], [0, 0]]
     start_indices = [max_prefix_len - prefix_lengths[i], 0]
     if rank == 4:
-      pad_width = pad_width + [[0, 0]]
       start_indices = start_indices + [0]
     padded = jnp.pad(
         x[i],
         pad_width,
         mode='constant',
         constant_values=x.dtype.type(pad_value),
     )
-    out.append(jax.lax.dynamic_slice(padded, start_indices, x.shape[1:]))
-  return jnp.stack(out)
+    padded = jax.lax.dynamic_slice(padded, start_indices, slice_sizes)
+    x = x.at[i].set(padded)
+  return x
 
 
 def concat_suffix_and_left_align(
     decoded_tensors: JTensor,
     suffix_tensors: JTensor,
     decode_end_indices: JTensor,
     prefix_lengths: JTensor,
```

## praxis/decoder_utils_test.py

```diff
@@ -15,14 +15,15 @@
 
 """Unit tests for decoder_utils."""
 
 from absl.testing import absltest
 from absl.testing import parameterized
 from jax import numpy as jnp
 import numpy as np
+from praxis import base_model
 from praxis import decoder_utils
 from praxis import pytypes
 from praxis import test_utils
 
 
 class DecoderUtilsTest(test_utils.TestCase):
 
@@ -60,15 +61,17 @@
         [2, 3, 4, 1, 15],  # Top-4 id: 4, 0, 1, 2
         [25, 4, 3, 2, 1],  # Top-4 id: 0, 1, 2, 3
         [0, 0, 0, 0, 0],  # Top-4 id: 0, 1, 2, 3
         [1, 2, 3, 4, 5],  # Top-4 id: 4, 3, 2, 1
     ]
     topk_value, topk_indices, final_topk_value, final_topk_indices = (
         decoder_utils.two_stage_topk(
-            np.array(logits, dtype=np.float32), hyp_scores, terminal_ids))
+            jnp.array(logits, dtype=np.float32), hyp_scores, terminal_ids
+        )
+    )
 
     # Compares 1st topk
     self.assertArraysEqual(topk_value,
                            np.array(target_topk_value, dtype=np.float32))
     self.assertArraysEqual(
         topk_indices,
         np.array([[4, 2, 1, 0, 0, 1, 2, 3, 0, 1, 2, 3, 4, 3, 2, 1]],
@@ -112,15 +115,16 @@
   def test_right_align_states(self):
     decode_cache = jnp.array(
         [[[[0.1, 0.1], [0.2, 0.3]], [[0.2, 0.3], [0.4, 0.5]],
           [[0, 0.1], [0.1, 0]], [[0, 0], [0, 0]], [[0, 0], [0, 0]]]],
         dtype=np.float32)
     seq_lengths = jnp.array([3])
     right_align_decode_cache = decoder_utils.right_align_state_fn(seq_lengths)(
-        decode_cache, batch_dim=0, time_dim=1)
+        decode_cache, 0, 1
+    )
     self.assertArraysEqual(
         right_align_decode_cache,
         np.array([[[[0, 0], [0, 0]], [[0, 0], [0, 0]], [[0.1, 0.1], [0.2, 0.3]],
                    [[0.2, 0.3], [0.4, 0.5]], [[0, 0.1], [0.1, 0]]]],
                  dtype=np.float32))
 
   def test_left_align_tensor(self):
@@ -213,35 +217,43 @@
 
     def _extend_step_fn(model, ids, pos):
       return ids.astype(jnp.float32) / 2
 
     expanded_extend_step_fn = decoder_utils.coerce_to_expanded_extend_step_fn(
         _extend_step_fn
     )
+    unused_model = base_model.BaseModel()
 
     self.assertArraysEqual(
         expanded_extend_step_fn(
-            None, jnp.array([[1, 2]]), jnp.array([3, 4]), pytypes.NestedMap()
+            unused_model,
+            jnp.array([[1, 2]]),
+            jnp.array([3, 4]),
+            pytypes.NestedMap(),
         ),
         jnp.array([[0.5, 1.0]]),
     )
 
   def test_coerce_to_expanded_extend_step_fn_cast(self):
 
     def _extend_step_fn(model, ids, pos, state):
       return ids.astype(jnp.float32) / 2
 
     expanded_extend_step_fn = decoder_utils.coerce_to_expanded_extend_step_fn(
         _extend_step_fn
     )
+    unused_model = base_model.BaseModel()
 
     self.assertIs(expanded_extend_step_fn, _extend_step_fn)  # merely casted
     self.assertArraysEqual(
         expanded_extend_step_fn(
-            None, jnp.array([[1, 2]]), jnp.array([3, 4]), pytypes.NestedMap()
+            unused_model,
+            jnp.array([[1, 2]]),
+            jnp.array([3, 4]),
+            pytypes.NestedMap(),
         ),
         jnp.array([[0.5, 1.0]]),
     )
 
   def test_collect_results_to_optimize_eos(self):
     result = pytypes.NestedMap(
         logprobs=jnp.array(
@@ -289,30 +301,30 @@
             [[[False, False], [True, False]], [[False, True], [False, False]]]
         ),
     )
 
   def test_find_first_stop_seq_match(self):
 
     # We only look for stop sequence matches at these indices or after!
-    first_new_decode_idx = np.asarray([
+    first_new_decode_idx = jnp.asarray([
         2,
         0,
         2,
         2,
     ])
-    sequences = np.asarray([
+    sequences = jnp.asarray([
         [2, 3, 4, 5],  # Will match first eos seq.
         [2, 3, 4, 5],  # Nearly match first eos seq, but index too small.
         [6, 7, 8, 9],  # No match.
         [4, 2, 6, 7],  # Match second eos sequence, which starts with padding.
     ])
 
     # 0 corresponds to padding. Stop sequences are left padded.
     stop_sequences = self._tile_to_bsize(
-        np.asarray([
+        jnp.asarray([
             [3, 4, 5],
             [0, 2, 6],
         ]),
         sequences.shape[0],
     )
 
     self.assertArraysEqual(
@@ -327,10 +339,39 @@
         # Second sequence doesn't match any stop sequence.
         # Third sequence doesn't match any stop sequence.
         # Fourth sequence matches second stop sequence at index 2, which is 0
         # tokens past the first new token.
         np.array([1, 2, 2, 0]),
     )
 
+  def test_left_align_kv_cache(self):
+    num_cache_slots = 3
+    sql_len = 6
+    num_kv_heads = 3
+    head_dims = 2
+
+    kv_state_shape = (
+        num_cache_slots,
+        sql_len,
+        num_kv_heads,
+        head_dims,
+    )
+    x = jnp.ones(kv_state_shape)
+    prefix_lengths = jnp.array([1, 3, 2], dtype=jnp.int32)
+    # max step is 4 which means the longest seq length is 5 at this moment.
+    # Given the sql_len is 6, the slots[:,-1,:,:] should be padded with zero.
+    max_step = 4
+    left_align_steps_arr = jnp.ones_like(prefix_lengths) * max_step
+    got = decoder_utils.left_align_kv_cache(
+        x, left_align_steps_arr, sql_len - 1, pad_value=0, batch_size=3
+    )
+    self.assertArraysEqual(
+        got[:, -1, :, :], jnp.zeros((num_cache_slots, num_kv_heads, head_dims))
+    )
+    self.assertArraysEqual(
+        got[:, 0:-1, :, :],
+        jnp.ones((num_cache_slots, sql_len - 1, num_kv_heads, head_dims)),
+    )
+
 
 if __name__ == '__main__':
   absltest.main()
```

## praxis/flat_beam_search_test.py

```diff
@@ -33,37 +33,45 @@
         beam_mask, hyp_id, time_step=None)
     self.assertArraysEqual(
         update_beam_mask,
         np.array([[[1, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0]]],
                  dtype=np.float32))
 
   def test_update_mask_without_step2(self):
-    beam_mask = np.array([[[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
-                           [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],
-                           [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
-                           [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]]],
-                         dtype=jnp.float32)
-    hyp_id = np.array([[3, 3, 0, 1]], jnp.float32)
+    beam_mask = jnp.array(
+        [[
+            [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
+            [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],
+            [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
+            [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
+        ]],
+        dtype=jnp.float32,
+    )
+    hyp_id = jnp.array([[3, 3, 0, 1]], jnp.float32)
     update_beam_mask = flat_beam_search.update_beam_mask(
         beam_mask, hyp_id, time_step=None)
     self.assertArraysEqual(
         update_beam_mask,
         np.array([[[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
                    [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
                    [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]]],
                  dtype=np.float32))
 
   def test_update_mask_with_step(self):
-    beam_mask = np.array([[[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
-                           [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],
-                           [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
-                           [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]]],
-                         dtype=jnp.float32)
-    hyp_id = np.array([[3, 3, 0, 1]], jnp.float32)
+    beam_mask = jnp.array(
+        [[
+            [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
+            [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],
+            [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
+            [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],
+        ]],
+        dtype=jnp.float32,
+    )
+    hyp_id = jnp.array([[3, 3, 0, 1]], jnp.float32)
     update_beam_mask = flat_beam_search.update_beam_mask(
         beam_mask, hyp_id, time_step=2)
     self.assertArraysEqual(
         update_beam_mask,
         np.array([[[0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0],
                    [0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0],
                    [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],
```

## praxis/optimizer_prefix_vectorization_test.py

```diff
@@ -11,14 +11,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Unit tests for prefix vectorization in optimizers."""
 
+import typing
+
 from absl import logging
 from absl.testing import absltest
 import jax
 from jax import numpy as jnp
 import optax
 from praxis import base_layer
 from praxis import optimizer_prefix_vectorization as opt_vec
@@ -68,14 +70,16 @@
     logging.info('opt_states_pspec=%s', opt_states_pspec)
     # Computed update is 0 + state, and state is sum of each variable.
     update, _ = grad_tx.update(
         jax.tree_map(jnp.zeros_like, variables), state, variables
     )
     # Variables a and c are scalars excluding the prefix, so the update must be
     # equal to the initial variable values.
+    update = typing.cast(base_layer.NestedMap, update)
+    variables = typing.cast(base_layer.NestedMap, variables)
     self.assertAllClose(update.a, variables.a)
     self.assertAllClose(update.c, variables.c)
     # b is not vectorized, so the update equals the sum reduction of the initial
     # variable value.
     self.assertAllClose(
         update.b, jnp.zeros_like(variables.b) + jnp.sum(variables.b)
     )
```

## praxis/py_utils.py

```diff
@@ -27,14 +27,15 @@
 from absl import flags
 from absl import logging
 import flax
 import jax
 from jax import lax
 from jax.experimental import mesh_utils
 from jax.experimental import multihost_utils
+from jax.interpreters import pxla
 import jax.numpy as jnp
 import numpy as np
 import optax
 from praxis import lingvo_lib
 from praxis import pytypes
 from praxis import trees
 
@@ -59,15 +60,15 @@
 current_cluster = lingvo_lib.current_cluster
 infeed_context_scope = lingvo_lib.infeed_context_scope
 InstantiableParams = pytypes.InstantiableParams
 NestedMap = pytypes.NestedMap
 HParams = pytypes.HParams
 Nested = pytypes.Nested
 
-JTensor = jnp.ndarray
+JTensor = jax.Array
 
 
 def merge_dict(dict1, dict2):
   """Merges two dictionaries and asserts keys in both have identical values."""
   for key in set(dict1) & set(dict2):
     # The values must be the same object
     if dict1[key] is not dict2[key]:
@@ -117,15 +118,15 @@
 except ValueError:
   logging.error(
       'ValueError: a serialization handler for "NestedMap" is already'
       ' registered'
   )
 
 
-@functools.partial(functools.partial, jax.tree_map)
+@functools.partial(functools.partial, jax.tree.map)
 def assert_same_shape_and_dtype(x, y):
   assert x.shape == y.shape and x.dtype == y.dtype, f'x={x}, y={y}'
 
 
 def reshard(array: jnp.ndarray) -> np.ndarray:
   """Reshards an input tensor according to the number of local devices."""
   num_devices = jax.local_device_count()
@@ -177,15 +178,15 @@
 
   Args:
     data: An array containing data.
 
   Returns:
     First shard of data.
   """
-  return jax.tree_map(_unreplicate, data)
+  return jax.tree.map(_unreplicate, data)
 
 
 def maybe_unreplicate_for_first_shard(data):
   """Unreplicate data for first shard.
 
   'data' may not be fully replicated.
 
@@ -194,15 +195,15 @@
 
   Args:
     data: An array containing data.
 
   Returns:
     First shard of data.
   """
-  return jax.tree_map(_unreplicate, data)
+  return jax.tree.map(_unreplicate, data)
 
 
 def extract_keys(n, p, key_separator, left_separator, right_separator, is_leaf):
   """Alias long function call with fixed separators."""
   return extract_prefixed_keys_from_nested_map(
       n,
       p,
@@ -378,26 +379,26 @@
       sharding is not None
   ), 'Either pspecs or sharding must be provided.'
   local_devices = global_mesh.local_devices
 
   def _put_to_devices(x):
     return put_to_devices(x, local_devices)
 
-  device_buffers = jax.tree_map(_put_to_devices, host_arrays)
+  device_buffers = jax.tree.map(_put_to_devices, host_arrays)
 
   def _jax_array(global_shape, dbs, sharding):
     return jax.make_array_from_single_device_arrays(
         global_shape.shape, sharding, dbs
     )
 
   # If sharding not provided, create it from the partition spec.
-  sharding = sharding or jax.tree_map(
+  sharding = sharding or jax.tree.map(
       lambda x: jax.sharding.NamedSharding(global_mesh, x), pspecs
   )
-  return jax.tree_map(_jax_array, global_shapes, device_buffers, sharding)
+  return jax.tree.map(_jax_array, global_shapes, device_buffers, sharding)
 
 
 def convert_fully_replicated_array_to_pmap_array(arr):
   """Converts a fully replicated Array to Array with PmapSharding.
 
   Args:
     arr: Fully replicated jax.Array.
@@ -458,22 +459,22 @@
 def set_globally_use_rbg_prng_key() -> None:
   """Must call this before any JAX computation to set RBG PRNGKey globally."""
   jax.config.update('jax_default_prng_impl', 'rbg')
 
 
 def total_num_vars(variables) -> int:
   """Returns the total number of variables of the given variable collections."""
-  param_shape_counts = jax.tree_map(lambda x: np.prod(x.shape), variables)
+  param_shape_counts = jax.tree.map(lambda x: np.prod(x.shape), variables)
   flattened_counts, _ = jax.tree_util.tree_flatten(param_shape_counts)
   return np.sum(flattened_counts)
 
 
 def global_mesh_defined() -> bool:
   """Checks if global xmap/pjit mesh resource environment is defined."""
-  maps_env = jax.experimental.maps.thread_resources.env
+  maps_env = pxla.thread_resources.env
   return maps_env.physical_mesh.devices.shape != ()  # pylint: disable=g-explicit-bool-comparison
 
 
 # This wrapped with_sharding_constraint will not throw error for eval_shape
 # outside pjit. It is also used in p5x.
 def with_sharding_constraint(
     x: JTensor, axis_resources: jax.sharding.PartitionSpec | None
@@ -524,15 +525,15 @@
     if all(p == 0 for p in paddings):
       return x
     # Annotate before pad to make sure they have the same sharding.
     # (Pad does not have the highest sharding propagation priority.)
     x = with_sharding_constraint(x, pspec)
     return jnp.pad(x, [[0, p] for p in paddings])
 
-  return jax.tree_map(
+  return jax.tree.map(
       _maybe_pad,
       xs,
       partition_specs,
       unpadded_shapes,
       is_leaf=is_optax_masked_node,
   )
 
@@ -553,15 +554,15 @@
     if x.shape == (0,):
       return x
     x = jax.lax.slice(x, [0] * x.ndim, shape)
     # Annotate after slice to make sure they have the same sharding. (Slice does
     # not have the highest sharding propagation priority.)
     return with_sharding_constraint(x, pspec)
 
-  return jax.tree_map(
+  return jax.tree.map(
       _maybe_slice, xs, partition_spec, unpadded_shapes, is_leaf=is_leaf
   )
 
 
 @contextlib.contextmanager
 def logging_verbosity_level(level: str):
   prev_level = logging.get_verbosity()
@@ -581,15 +582,15 @@
     *trees: PyTree with the same structure as `indices`.
 
   Returns:
     PyTree with the same structure with the arguments. For example, if tree
     nodes are accessible as `tree[key]`, each node in the return value is
     defined as `ret[key] = trees[indices[key]][key]`.
   """
-  return jax.tree_map(lambda idx, *arrays: arrays[idx], indices, *trees)
+  return jax.tree.map(lambda idx, *arrays: arrays[idx], indices, *trees)
 
 
 Patterns = str | re.Pattern | Iterable[re.Pattern | str]
 
 
 def match_variable_names(
     tree: NestedMap,
@@ -636,27 +637,27 @@
       variables that are not matching to `patterns` will be updated.
 
   Returns:
     An updated NestedMap
   """
   mask = match_variable_names(old_tree, patterns)  # True for update
   if invert:
-    mask = jax.tree_map(lambda x: not x, mask)
+    mask = jax.tree.map(lambda x: not x, mask)
   flat_var_prefix = jax.tree_flatten(
       extract_prefixed_keys_from_nested_map(old_tree))[0]
   flat_mask = jax.tree_flatten(mask)[0]
   assert len(flat_var_prefix) == len(flat_mask)
   for prefix, match in zip(flat_var_prefix, flat_mask):
     if match:
       logging.info('Bprop included var: %s', prefix)
   for prefix, match in zip(flat_var_prefix, flat_mask):
     if not match:
       logging.info('Bprop excluded var: %s', prefix)
 
-  indices = jax.tree_map(lambda x: 1 if x else 0, mask)
+  indices = jax.tree.map(lambda x: 1 if x else 0, mask)
   return select_nodes_by_indices(indices, old_tree, new_tree)
 
 
 def l2_normalize(
     x: JTensor, axis: int | Sequence[int] = -1, epsilon: float = 1e-12
 ) -> JTensor:
   """L2-normalize a Jax tensor along certain dimension."""
@@ -703,15 +704,15 @@
     device_mesh = mesh_utils.create_device_mesh(
         ici_mesh_shape, contiguous_submeshes=contiguous_submeshes
     )
   logging.info('device_mesh: %s', device_mesh)
   return device_mesh
 
 
-def get_large_negative_number(dtype: jnp.dtype) -> JTensor:
+def get_large_negative_number(dtype: jnp.dtype | np.dtype) -> JTensor:
   """Returns a large negative value for the given dtype."""
   # -0.7 is a float64 in Jax. Explicit cast output to target dtype.
   if jnp.issubdtype(dtype, jnp.inexact):
     dtype_max = jnp.finfo(dtype).max
   elif jnp.issubdtype(dtype, jnp.integer):
     dtype_max = jnp.iinfo(dtype).max
   else:
@@ -880,15 +881,15 @@
   if not all(
       leaf.ndim > axis and leaf.shape[axis] == axis_size for leaf in leaves):
     raise ValueError(f'all leaves must have x.ndim > {axis}'
                      f' and x.shape[{axis}] == {axis_size}')
 
   flat_pytrees = []
   for i in range(axis_size):
-    flat_pytrees.append(jax.tree_map(lambda x: x.take(i, axis), tree))  # pylint: disable=cell-var-from-loop"
+    flat_pytrees.append(jax.tree.map(lambda x: x.take(i, axis), tree))  # pylint: disable=cell-var-from-loop"
 
   return flat_pytrees
 
 
 def apply_padding(
     inputs: JTensor,
     padding: JTensor,
```

## praxis/py_utils_test.py

```diff
@@ -263,90 +263,90 @@
     for l1, l2 in zip(
         jax.tree_util.tree_leaves(tree), jax.tree_util.tree_leaves(merged_tree)
     ):
       self.assertArraysEqual(l1, l2)
 
   def test_apply_padding_zero(self):
     y = py_utils.apply_padding(
-        inputs=np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
-        padding=np.array([[0.0], [1.0], [0.0]]),
+        inputs=jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
+        padding=jnp.array([[0.0], [1.0], [0.0]]),
     )
     self.assertAllClose(y, [[1.0, 2.0], [0.0, 0.0], [5.0, 6.0]])
 
   def test_apply_padding_constant(self):
     y = py_utils.apply_padding(
-        inputs=np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
-        padding=np.array([[0.0], [1.0], [0.0]]),
-        pad_value=np.array([[1.0, 2.0], [9.0, 10.0], [5.0, 6.0]]),
+        inputs=jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
+        padding=jnp.array([[0.0], [1.0], [0.0]]),
+        pad_value=jnp.array([[1.0, 2.0], [9.0, 10.0], [5.0, 6.0]]),
     )
     self.assertAllClose(y, [[1.0, 2.0], [9.0, 10.0], [5.0, 6.0]])
 
   def test_apply_padding_zero_arithmetic(self):
     y = py_utils.apply_padding(
-        inputs=np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
-        padding=np.array([[0.0], [1.0], [0.0]]),
+        inputs=jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
+        padding=jnp.array([[0.0], [1.0], [0.0]]),
         use_select=False,
     )
     self.assertAllClose(y, [[1.0, 2.0], [0.0, 0.0], [5.0, 6.0]])
 
   def test_apply_padding_with_axis_0(self):
     y = py_utils.apply_padding(
-        inputs=np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
-        padding=np.array([[0.0], [1.0], [0.0]]),
+        inputs=jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
+        padding=jnp.array([[0.0], [1.0], [0.0]]),
         axis=0,
     )
     self.assertAllClose(y, [[1.0, 2.0], [0.0, 0.0], [5.0, 6.0]])
 
   def test_apply_padding_with_axis_0_and_one_more_dim(self):
     # inputs=[3, 2] and paddings=[3, 2, 1]
     y = py_utils.apply_padding(
-        inputs=np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
-        padding=np.expand_dims(
-            np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]]), -1
+        inputs=jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
+        padding=jnp.expand_dims(
+            jnp.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]]), -1
         ),
         axis=0,
     )
     self.assertAllClose(y, [[1.0, 2.0], [0.0, 4.0], [5.0, 0.0]])
 
   def test_pad_inputs_with_axis_0_and_one_less_dim(self):
     # inputs=[1, 2, 3] and paddings=[1, 2]
     y = py_utils.apply_padding(
-        inputs=np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]]),
-        padding=np.array([[1.0, 0.0]]),
+        inputs=jnp.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]]),
+        padding=jnp.array([[1.0, 0.0]]),
         axis=0,
     )
     self.assertAllClose(y, [[[0.0, 0.0, 0.0], [4.0, 5.0, 6.0]]])
 
   def test_apply_padding_with_axis_1_and_one_more_dim(self):
     # inputs=[3, 2] and paddings=[3, 2, 1]
     y = py_utils.apply_padding(
-        inputs=np.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
-        padding=np.expand_dims(
-            np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]]), -1
+        inputs=jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]),
+        padding=jnp.expand_dims(
+            jnp.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]]), -1
         ),
         axis=1,
     )
     self.assertAllClose(y, [[1.0, 2.0], [0.0, 4.0], [5.0, 0.0]])
 
   def test_pad_inputs_with_axis_1_and_same_rank(self):
     # inputs=[5, 1, 2, 3] and paddings=[1, 2, 1]
     batch = 5
     y = py_utils.apply_padding(
-        inputs=np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]] * batch),
-        padding=np.expand_dims(np.array([[1.0, 0.0]]), -1),
+        inputs=jnp.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]] * batch),
+        padding=jnp.expand_dims(np.array([[1.0, 0.0]]), -1),
         axis=1,
     )
     self.assertAllClose(y, [[[0.0, 0.0, 0.0], [4.0, 5.0, 6.0]]] * batch)
 
   def test_pad_inputs_with_axis_1_and_one_less_dim(self):
     # inputs=[5, 1, 2, 3] and paddings=[1, 2]
     batch = 5
     y = py_utils.apply_padding(
-        inputs=np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]] * batch),
-        padding=np.array([[1.0, 0.0]]),
+        inputs=jnp.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]] * batch),
+        padding=jnp.array([[1.0, 0.0]]),
         axis=1,
     )
     self.assertAllClose(y, [[[0.0, 0.0, 0.0], [4.0, 5.0, 6.0]]] * batch)
 
   def test_timeit(self):
     start_time = time.time()
     with py_utils.timeit() as period:
@@ -386,54 +386,58 @@
       ('_pad1', 1),
   )
   def test_pad_or_trim_to(self, pad_val):
     key = jax.random.PRNGKey(seed=123456)
     x = jax.random.normal(key, shape=(3, 3))
     shape = [4, 6]
     padded_x = py_utils.pad_or_trim_to(x, shape, pad_val=pad_val)
+    assert padded_x is not None
     self.assertEqual(padded_x.shape, (4, 6))
     self.assertAllClose(x, padded_x[:3, :3])
     sum_diff = jnp.sum(padded_x) - jnp.sum(x)
     self.assertAllClose(sum_diff, pad_val * 15.0)
 
   @parameterized.named_parameters(
       dict(testcase_name='_dim1', dst=(2, 2)),
       dict(testcase_name='_dim0', dst=(1, 3)),
       dict(testcase_name='_dim01', dst=(1, 2)),
   )
   def test_pad_or_trim_to_trim(self, dst):
     src = (2, 3)
     x = np.random.uniform(0, 1, src).astype(np.float32)
     y = py_utils.pad_or_trim_to(x, dst)
+    assert y is not None
     self.assertSequenceEqual(y.shape, dst)
     self.assertAllClose(y, x[: dst[0], : dst[1]])
 
   @parameterized.named_parameters(
       dict(testcase_name='_dim1', dst=(2, 4)),
       dict(testcase_name='_dim0', dst=(3, 3)),
       dict(testcase_name='_dim01', dst=(3, 4)),
   )
   def test_pad_or_trim_to(self, dst):
     src = (2, 3)
     pad_value = 42.0
     x = np.random.uniform(0, 1, src).astype(np.float32)
     y = py_utils.pad_or_trim_to(x, dst, pad_value)
+    assert y is not None
     self.assertSequenceEqual(y.shape, dst)
     self.assertAllClose(y[: src[0], : src[1]], x)
     self.assertAllClose(y[src[0] :], pad_value)
     self.assertAllClose(y[:, src[1] :], pad_value)
 
   @parameterized.named_parameters(
       dict(testcase_name='_pad0_trim1', src=(2, 3), dst=(3, 2)),
       dict(testcase_name='_pad1_trim0', src=(3, 2), dst=(2, 3)),
   )
   def test_pad_or_trim_to_mix(self, src, dst):
     pad_value = 42.0
     x = np.random.uniform(0, 1, src).astype(np.float32)
     y = py_utils.pad_or_trim_to(x, dst, pad_value)
+    assert y is not None
     self.assertSequenceEqual(y.shape, dst)
     self.assertAllClose(y[:2, :2], x[:2, :2])
     self.assertAllClose(y[2:], pad_value)
     self.assertAllClose(y[:, 2:], pad_value)
 
   @parameterized.named_parameters(('truncate', False), ('extend', True))
   def test_append_eos(self, extend_if_overflow):
@@ -508,42 +512,42 @@
     output = py_utils.concat_sequences_with_padding(
         input0, paddings0, input1, paddings1
     )
     output = output[0] * jnp.expand_dims(1 - output[1], -1)
     self.assertAllClose(output, expected_output)
 
   def test_concat_nested_maps(self):
-    map_0 = {
+    map_0 = py_utils.NestedMap.FromNestedDict({
         'A': jnp.zeros((8, 1, 4)),
         'B': jnp.zeros((1, 2)),
         'C': {
             'D': jnp.zeros((4, 1)),
             'E': 5,
             'F': jnp.zeros((5)),
         },
-    }
+    })
 
-    map_1 = {
+    map_1 = py_utils.NestedMap.FromNestedDict({
         'A': jnp.zeros((8, 2, 4)),
         'B': jnp.zeros((1, 3)),
         'C': {
             'D': jnp.zeros((4, 1)),
             'F': jnp.zeros(5),
         },
-    }
+    })
 
-    map_2 = {
+    map_2 = py_utils.NestedMap.FromNestedDict({
         'A': jnp.zeros((8, 3, 4)),
         'B': jnp.zeros((1, 1)),
         'C': {
             'D': jnp.zeros((4, 1)),
             'E': 5,
             'F': jnp.zeros((5)),
         },
-    }
+    })
 
     expected = {
         'A': jnp.zeros((8, 6, 4)),
         'B': jnp.zeros((1, 6)),
         'C': {
             'D': jnp.zeros((4, 3)),
             'E': 5,
```

## praxis/sample_decode.py

```diff
@@ -889,15 +889,15 @@
     def cond_func(model, val):
       """Whether the while loop should continue."""
       del model
       # We continue the decoding search iff both:
       #   (1) We have yet to exceed the max steps set by self.decoder.seqlen
       #   (2) At least one row in the batch has not terminated.
       max_steps = start_step + stop_at_decode_steps
-      length_ok = val.step < min(seq_len - 1, max_steps)
+      length_ok = val.step < jnp.minimum(seq_len - 1, max_steps)
       all_rows_done = jnp.all(val.done)
       return jnp.logical_and(length_ok, jnp.logical_not(all_rows_done))
 
     return cond_func
 
   def loop_body(model, val):
     """From ids at `step`, update output ids at `step + 1`."""
@@ -1277,18 +1277,14 @@
       # recursively merge two dictionaries.
       reinsert_collection(model, base_layer.SUMMARIES, model_summaries_copy)
 
   if result_callback is not None and result_callback.done_fn is not None:
     result_callback.done_fn()
 
   if optimize_eos:
-    if fprop_for_prefix:
-      decode_length_shift = max_prefix_len
-    else:
-      decode_length_shift = 0
     result = decoder_utils.collect_results_to_optimize_eos(
         result, decode_length_shift=max_prefix_len
     )
 
   if return_result_for_suffix_score:
     return result
 
@@ -1348,16 +1344,17 @@
   return result
 
 
 # TODO(b/249483164): Rename BaseLayerApi->BaseLayer after Fiddle migration.
 def vanilla_sample_decode(
     model: base_layer.BaseLayerApi,
     fprop_fn: decoder_utils.FPropFn,
-    extend_step_fn: decoder_utils.ExtendStepFn
-    | decoder_utils.ExpandedExtendStepFn,
+    extend_step_fn: (
+        decoder_utils.ExtendStepFn | decoder_utils.ExpandedExtendStepFn
+    ),
     transform_state_fn: decoder_utils.TransformStateFn,
     next_token_sampler: base_layer.BaseLayerApi,
     prefix_ids: JTensor,
     prefix_paddings: JTensor,
     temperature: float | JTensor = 1.0,
     gumbel_prng_key: JTensor | None = None,
     max_decode_steps: int = 0,
@@ -1597,16 +1594,17 @@
 
     return result
 
 
 # TODO(b/249483164): Rename BaseLayerApi->BaseLayer after Fiddle migration.
 def greedy_decode(
     model: base_layer.BaseLayerApi,
-    extend_step_fn: decoder_utils.ExtendStepFn
-    | decoder_utils.ExpandedExtendStepFn,
+    extend_step_fn: (
+        decoder_utils.ExtendStepFn | decoder_utils.ExpandedExtendStepFn
+    ),
     prefix_ids: JTensor,
     prefix_paddings: JTensor,
     seq_len: int,
     fprop_for_prefix: bool = False,
     fprop_fn: decoder_utils.FPropFn | None = None,
     transform_state_fn: decoder_utils.TransformStateFn | None = None,
     max_prefix_len: int | None = None,
@@ -1726,15 +1724,15 @@
   decode_state.per_sample_steps = (
       jnp.ones(shape=batch_size, dtype=jnp.int32) * start_step
   )
 
   decode_state.output_ids = output_ids
   decode_state.logprobs = jnp.ones_like(output_ids, dtype=jnp.float32)
 
-  decode_state.done = jnp.ones(shape=batch_size, dtype=jnp.bool_)
+  decode_state.done = jnp.zeros(shape=batch_size, dtype=jnp.bool_)
   decode_state.has_eos = jnp.zeros(shape=batch_size, dtype=jnp.bool_)
 
   decode_state.prefix_lengths = prefix_lengths
   decode_state.decode_lengths = jnp.ones_like(prefix_lengths) * seq_len
   decode_state.segment_pos = prefix_lengths - 1
 
   next_token_sampler = base_layer.instantiate(
```

## praxis/sample_decode_test.py

```diff
@@ -104,15 +104,15 @@
 
 class TestModelWithLogits(base_model.BaseModel):
   use_dummy_next_token_sampler: bool = True
   vocab_size: int = 0
   num_samples: int = 0
   seq_len: int = 0
   batch_size: int = 0
-  logits: jnp.ndarray = None
+  logits: jax.Array | None = None
 
   def setup(self) -> None:
     super().setup()
     assert self.logits is not None
     expected_shape = (
         self.seq_len,
         self.batch_size * self.num_samples,
```

## praxis/token_samplers_test.py

```diff
@@ -209,15 +209,15 @@
             )
         ),
     )
     self.assertArraysEqual(
         top_k_indices,
         np.array([[1, 2], [4, 0], [1, 0], [0, 2]], dtype=np.int32),
     )
-    large_neg = py_utils.get_large_negative_number(np.float32)
+    large_neg = py_utils.get_large_negative_number(jnp.float32)
     self.assertAllClose(
         top_p_logits,
         np.array(
             [[0.7, large_neg], [0.5, large_neg], [0.6, large_neg], [0.5, 0.5]],
             dtype=np.float32,
         ),
     )
```

## praxis/trees_test.py

```diff
@@ -29,16 +29,16 @@
 from praxis import base_layer
 from praxis import pytypes
 from praxis import test_utils
 from praxis import trees
 
 
 class TestPair(NamedTuple):
-  subset: pytypes.Nested
-  superset: pytypes.Nested
+  subset: pytypes.Nested[Any]
+  superset: pytypes.Nested[Any]
 
 
 class TrainState(struct.PyTreeNode):
   """Simple train state."""
 
   step: base_layer.JTensorOrPartitionSpec
   mdl_vars: base_layer.NestedJTensorOrPartitionSpec
```

## praxis/layers/attentions.py

```diff
@@ -1905,15 +1905,18 @@
           name, extend_value, time_step, time_dim=time_dim
       )
       return self._shard_blnh(extended_state)
 
     key_state_name = 'key_state'
     value_state_name = 'value_state'
     if not is_cross_attention:
-      key_state = _extend_decode_state_and_shard(key_state_name, key_proj)
+      # No need to update key_state at this point if consolidate_rope_key_state
+      # is set.
+      if not (self.use_rotary_position_emb and self.consolidate_rope_key_state):
+        key_state = _extend_decode_state_and_shard(key_state_name, key_proj)
       value_state = _extend_decode_state_and_shard(value_state_name, value_proj)
 
     # Apply depth-wise convolution as in Primer.
     # Paper: https://arxiv.org/abs/2109.08668.
     if self.dconv_qkv:
       key_state_name = 'key_post_dconv'
       value_state_name = 'value_post_dconv'
@@ -2915,15 +2918,15 @@
     return term_ac + term_bd
 
   def _atten_logits_one_step(self, query, key, step):
     t = step + 1
     s = key.shape[1]
 
     # [1, S]
-    pos = jnp.expand_dims(jnp.arange(t - 1, t - s - 1, -1), 0)
+    pos = jnp.expand_dims(t - 1 - jnp.arange(s), 0)
     sin_emb = self.pos_emb(position=pos)
     # [1, S, N, H]
     sin_emb = self.pos_proj(sin_emb)
     # [S, N, H]
     sin_emb = jnp.squeeze(sin_emb, 0)
 
     # [B, N, T, S=T]
@@ -3380,15 +3383,15 @@
     key = _padded_slice(key, time_step + 1 - l, f, 1, 0.0)
     value = _padded_slice(value, time_step + 1 - l, f, 1, 0.0)
     atten_mask = _padded_slice(
         atten_mask,
         time_step + 1 - l,
         f,
         -1,
-        py_utils.get_large_negative_number(jnp.float32),
+        py_utils.get_large_negative_number(atten_mask.dtype),
     )
 
     b, f, n, h = key.shape
     asserts.eq(f, self.left_context + self.right_context)
     base_layer.assert_has_shape(value, [b, f, n, h])
     base_layer.assert_has_shape(query, [b, n, h])
     base_layer.assert_has_shape(atten_mask, [-1, 1, f])
```

## praxis/layers/attentions_test.py

```diff
@@ -178,15 +178,15 @@
             batch_size,
         ],
     )
     padding = jnp.asarray(get_padding_from_length(length))
 
     mask = attentions.limited_context_mask(
         left_context, right_context, padding.shape[1], np.float32
-    )
+    )  # pytype: disable=wrong-arg-types
 
     # Merge the above mask with paddings:
     padding_mask = attentions.convert_paddings_to_mask(padding)
     rev_padding_mask = jnp.transpose(padding_mask, (0, 1, 3, 2))
     result = jnp.minimum(jnp.minimum(mask, padding_mask), rev_padding_mask)
 
     expect = np.zeros((batch_size, 1, max_length, max_length))
@@ -753,15 +753,15 @@
     ).astype(np.float32)
     value_vec = np.random.normal(
         size=[target_batch_size, source_max_length, mdl_dim]
     ).astype(np.float32)
     segment_ids = np.random.randint(
         0, 2, size=[target_batch_size, target_max_length]
     ).astype(np.int32)
-    atten_mask = attentions.causal_segment_mask(segment_ids, np.float32)
+    atten_mask = attentions.causal_segment_mask(segment_ids, np.float32)  # pytype: disable=wrong-arg-types
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       prng_key, init_key = jax.random.split(prng_key)
       initial_vars = layer.init(
           init_key, query_vec, key_vec, value_vec, atten_mask
       )
@@ -829,15 +829,15 @@
     key_vec = np.random.normal(
         size=[target_batch_size, source_max_length, mdl_dim]
     ).astype(np.float32)
     value_vec = np.random.normal(
         size=[target_batch_size, source_max_length, mdl_dim]
     ).astype(np.float32)
     paddings = np.zeros([target_batch_size, source_max_length], dtype=np.int32)
-    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)
+    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)  # pytype: disable=wrong-arg-types
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       prng_key, init_key = jax.random.split(prng_key)
       initial_vars = layer.init(
           init_key, query_vec, key_vec, value_vec, atten_mask
       )
@@ -1001,15 +1001,15 @@
     ).astype(np.float32)
     value_vec = np.random.normal(
         size=[target_batch_size, source_max_length, mdl_dim]
     ).astype(np.float32)
     paddings = np.zeros(
         [target_batch_size, source_max_length], dtype=np.float32
     )
-    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)
+    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)  # pytype: disable=wrong-arg-types
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       prng_key, init_key = jax.random.split(prng_key)
       initial_vars = layer.init(
           init_key, query_vec, key_vec, value_vec, atten_mask
       )
@@ -1104,15 +1104,15 @@
     ).astype(np.float32)
     value_vec = np.random.normal(
         size=[target_batch_size, source_max_length, mdl_dim]
     ).astype(np.float32)
     paddings = range(source_max_length)[-target_batch_size:]
     paddings = [[0] * l + [1] * (source_max_length - l) for l in paddings]
     paddings = np.array(paddings)
-    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)
+    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)  # pytype: disable=wrong-arg-types
     if is_full:
       atten_mask = jnp.tile(atten_mask, [1, 1, source_max_length, 1])
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       prng_key, init_key = jax.random.split(prng_key)
       initial_vars = layer.init(
@@ -1190,38 +1190,40 @@
         [
             i > p - left_context and i <= p + right_context
             for i in range(source_max_length)
         ]
         for p in padding_zone
     ]
     paddings = np.array(paddings)
-    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)
+    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)  # pytype: disable=wrong-arg-types
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       prng_key, init_key = jax.random.split(prng_key)
       initial_vars = layer.init(
           init_key, query_vec, key_vec, value_vec, atten_mask
       )
       jax_fprop_out, _ = layer.apply(
           initial_vars, query_vec, key_vec, value_vec, atten_mask
       )
 
     # Those positions are fully masked.
-    fully_masked_out = [jax_fprop_out[i, p] for i, p in enumerate(padding_zone)]
-    self.assertEqual(np.sum(np.abs(test_utils.to_np(fully_masked_out))), 0)
+    fully_masked_out = np.array(
+        [jax_fprop_out[i, p] for i, p in enumerate(padding_zone)]
+    )
+    self.assertEqual(np.sum(np.abs(fully_masked_out)), 0)
 
     # Example of positions which are not fully masked.
-    non_masked_out = [
+    non_masked_out = np.array([
         jax_fprop_out[0, padding_zone[0] + 1],
         jax_fprop_out[1, padding_zone[1] - 1],
         jax_fprop_out[1, padding_zone[2] + 1],
         jax_fprop_out[2, padding_zone[2] - 1],
-    ]
-    self.assertNotEqual(np.amin(np.abs(test_utils.to_np(non_masked_out))), 0)
+    ])
+    self.assertNotEqual(np.amin(np.abs(non_masked_out)), 0)
 
   @parameterized.parameters([
       (4, 2, 1, True, True),
       (4, 2, 1, False, True),
       (8, 3, 5, True, False),
       (8, 3, 5, False, False),
       (5, 4, 0, False, True),
@@ -1263,15 +1265,15 @@
     value_vec = np.random.normal(
         size=[target_batch_size, source_max_length, mdl_dim]
     ).astype(np.float32)
 
     paddings = range(source_max_length)[-target_batch_size:]
     paddings = [[0] * l + [1] * (source_max_length - l) for l in paddings]
     paddings = np.array(paddings)
-    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)
+    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)  # pytype: disable=wrong-arg-types
     if is_full:
       atten_mask = jnp.tile(atten_mask, [1, 1, source_max_length, 1])
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       prng_key, init_key = jax.random.split(prng_key)
       initial_vars = layer.init(
@@ -1346,15 +1348,15 @@
     value_vec = np.random.normal(
         size=[target_batch_size, source_max_length, mdl_dim]
     ).astype(np.float32)
 
     paddings = range(source_max_length)[-target_batch_size:]
     paddings = [[0] * l + [1] * (source_max_length - l) for l in paddings]
     paddings = np.array(paddings)
-    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)
+    atten_mask = attentions.convert_paddings_to_mask(paddings, np.float32)  # pytype: disable=wrong-arg-types
     if is_full:
       atten_mask = jnp.tile(atten_mask, [1, 1, source_max_length, 1])
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       prng_key, init_key = jax.random.split(prng_key)
       initial_vars = layer.init(
@@ -1393,15 +1395,15 @@
       ([[1, 2, 3, 4], [6, 7, 8, 9]], 1, 0, [[0, 0, 0, 0], [1, 2, 3, 4]]),
       ([[1, 2, 3, 4], [6, 7, 8, 9]], -1, 0, [[6, 7, 8, 9], [0, 0, 0, 0]]),
       ([[1, 2, 3, 4], [6, 7, 8, 9]], 1, 1, [[0, 1, 2, 3], [0, 6, 7, 8]]),
       ([[1, 2, 3, 4], [6, 7, 8, 9]], -1, 1, [[2, 3, 4, 0], [7, 8, 9, 0]]),
       ([1], 1, 0, [0]),
   )
   def test_shift1d(self, inputs, offset, axis, outputs):
-    inputs = np.asarray(inputs)
+    inputs = jnp.asarray(inputs)
     shift_outputs = attentions.shift_1d(inputs, offset, axis)
     self.assertArraysEqual(shift_outputs, np.asarray(outputs))
 
   @parameterized.parameters(
       ([8, 16, 32], 3, 1, 32),
       ([8, 8, 4, 34], 2, 0, [4, 34]),
       ([2, 32, 8, 16, 128], 3, 1, [8, 16, 128]),
@@ -1641,23 +1643,23 @@
     )
     test_layer_p.use_length_as_position = False
     layer_raw = instantiate(test_layer_p)
     test_layer_p.use_length_as_position = True
     layer_len = instantiate(test_layer_p)
     target_batch_size = 3
     source_max_length = 8
-    segment_ids = np.array(
+    segment_ids = jnp.array(
         [
             [0, 0, 0, 0, 0, 1, 1, 1],
             [0, 0, 0, 0, 1, 1, 1, 1],
             [0, 0, 0, 1, 1, 1, 1, 1],
         ],
         dtype=jnp.int32,
     )
-    segment_pos = np.array(
+    segment_pos = jnp.array(
         [
             [0, 1, 2, 3, 4, 0, 1, 2],
             [0, 1, 2, 3, 0, 1, 2, 3],
             [0, 1, 2, 0, 1, 2, 3, 4],
         ],
         dtype=jnp.int32,
     )
```

## praxis/layers/augmentations_test.py

```diff
@@ -54,18 +54,20 @@
           {PARAMS: prng_key, RANDOM: compute_key}, inputs, paddings
       )
       augmented_ids, augmented_pos = layer.apply(
           initial_vars, inputs, paddings, rngs={RANDOM: compute_key}
       )
     logging.info('augmented_ids: %s', augmented_ids)
     logging.info('augmented_pos: %s', augmented_pos)
-    expected_ids = np.array([0, 1, 0, 3, 4, 5, 0, 7, 8, 9])
-    expected_pos = np.array([0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])
-    self.assertAllClose(to_np(expected_ids), to_np(augmented_ids))
-    self.assertAllClose(to_np(expected_pos), to_np(augmented_pos))
+    expected_ids = np.array([0, 1, 0, 3, 4, 5, 0, 7, 8, 9], np.float32)
+    expected_pos = np.array(
+        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], np.float32
+    )
+    self.assertAllClose(expected_ids, to_np(augmented_ids))
+    self.assertAllClose(expected_pos, to_np(augmented_pos))
 
   def testMaskedLmDataAugmenterLarge(self):
     p = pax_fiddle.Config(
         augmentations.MaskedLmDataAugmenter,
         name='mlm',
         vocab_size=32000,
         mask_token_id=0,
@@ -81,220 +83,226 @@
       )
       augmented_ids, augmented_pos = layer.apply(
           initial_vars, inputs, paddings, rngs={RANDOM: compute_key}
       )
     logging.info('augmented_ids: %s', np.array_repr(augmented_ids))
     logging.info('augmented_pos: %s', np.array_repr(augmented_pos))
     np.set_printoptions(threshold=np.inf)
-    expected_ids = np.array([
-        0,
-        1,
-        2,
-        3,
-        4,
-        5,
-        6,
-        7,
-        8,
-        9,
-        10,
-        11,
-        12,
-        13,
-        14,
-        15,
-        16,
-        17,
-        18,
-        19,
-        20,
-        21,
-        22,
-        23,
-        24,
-        25,
-        0,
-        27,
-        28,
-        29,
-        30,
-        31,
-        32,
-        33,
-        34,
-        35,
-        36,
-        37,
-        38,
-        39,
-        40,
-        41,
-        42,
-        43,
-        26592,
-        45,
-        46,
-        11329,
-        48,
-        49,
-        50,
-        51,
-        52,
-        53,
-        54,
-        55,
-        56,
-        57,
-        58,
-        59,
-        60,
-        0,
-        62,
-        63,
-        64,
-        65,
-        66,
-        67,
-        68,
-        69,
-        70,
-        71,
-        19237,
-        73,
-        0,
-        0,
-        76,
-        77,
-        78,
-        79,
-        80,
-        81,
-        82,
-        83,
-        84,
-        85,
-        86,
-        87,
-        88,
-        89,
-        90,
-        91,
-        92,
-        93,
-        94,
-        95,
-        0,
-        97,
-        98,
-        99,
-    ])
-    expected_pos = np.array([
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        1.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        1.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        1.0,
-        0.0,
-        0.0,
-        1.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        1.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        1.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        1.0,
-        0.0,
-        1.0,
-        1.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        0.0,
-        1.0,
-        0.0,
-        0.0,
-        0.0,
-    ])
-    self.assertAllClose(to_np(expected_ids), to_np(augmented_ids))
-    self.assertAllClose(to_np(expected_pos), to_np(augmented_pos))
+    expected_ids = np.array(
+        [
+            0,
+            1,
+            2,
+            3,
+            4,
+            5,
+            6,
+            7,
+            8,
+            9,
+            10,
+            11,
+            12,
+            13,
+            14,
+            15,
+            16,
+            17,
+            18,
+            19,
+            20,
+            21,
+            22,
+            23,
+            24,
+            25,
+            0,
+            27,
+            28,
+            29,
+            30,
+            31,
+            32,
+            33,
+            34,
+            35,
+            36,
+            37,
+            38,
+            39,
+            40,
+            41,
+            42,
+            43,
+            26592,
+            45,
+            46,
+            11329,
+            48,
+            49,
+            50,
+            51,
+            52,
+            53,
+            54,
+            55,
+            56,
+            57,
+            58,
+            59,
+            60,
+            0,
+            62,
+            63,
+            64,
+            65,
+            66,
+            67,
+            68,
+            69,
+            70,
+            71,
+            19237,
+            73,
+            0,
+            0,
+            76,
+            77,
+            78,
+            79,
+            80,
+            81,
+            82,
+            83,
+            84,
+            85,
+            86,
+            87,
+            88,
+            89,
+            90,
+            91,
+            92,
+            93,
+            94,
+            95,
+            0,
+            97,
+            98,
+            99,
+        ],
+        np.float32,
+    )
+    expected_pos = np.array(
+        [
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            1.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            1.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            1.0,
+            0.0,
+            0.0,
+            1.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            1.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            1.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            1.0,
+            0.0,
+            1.0,
+            1.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            0.0,
+            1.0,
+            0.0,
+            0.0,
+            0.0,
+        ],
+        np.float32,
+    )
+    self.assertAllClose(expected_ids, to_np(augmented_ids))
+    self.assertAllClose(expected_pos, to_np(augmented_pos))
 
   def test_shifting(self):
     p = pax_fiddle.Config(
         augmentations.TemporalShifting,
         name='shifting',
         shift_range_ms=13.3,
         sample_rate=1000.0,
```

## praxis/layers/conformers_test.py

```diff
@@ -181,15 +181,15 @@
     paddings = get_padding_from_length(
         target_batch_size, source_max_length, length
     )
 
     # Convert paddings to atten_mask:
     atten_mask_padding = attentions.convert_paddings_to_mask(
         paddings, np.float32
-    )
+    )  # pytype: disable=wrong-arg-types
     rev_padding_mask = jnp.transpose(atten_mask_padding, (0, 1, 3, 2))
     atten_mask_padding = jnp.minimum(atten_mask_padding, rev_padding_mask)
 
     # Emulate local attention for DotProductAttentionXL.
     context_mask = attentions.limited_context_mask(
         left_context, right_context, paddings.shape[1]
     )
@@ -287,15 +287,15 @@
     paddings = get_padding_from_length(
         target_batch_size, source_max_length, length
     )
 
     # Convert paddings to atten_mask for LocalSelfAttentionXL:
     atten_mask_padding = attentions.convert_paddings_to_mask(
         paddings, np.float32
-    )
+    )  # pytype: disable=wrong-arg-types
 
     # Emulate local attention for DotProductAttentionXL.
     context_mask = attentions.limited_context_mask(
         left_context, right_context, paddings.shape[1]
     )
     rev_padding_mask = jnp.transpose(atten_mask_padding, (0, 1, 3, 2))
     dot_product_atten_mask_padding = jnp.minimum(
```

## praxis/layers/ctc_objectives.py

```diff
@@ -135,15 +135,15 @@
   # logalpha_phi is [T, B, N+1]
   # logalpha_emit is [T, B, N]
   per_seq_loss, logalpha_phi, logalpha_emit = optax.ctc_loss_with_forward_probs(
       logits,
       logitpaddings,
       labels,
       labelpaddings,
-      blank_id,
+      blank_id=blank_id,
       log_epsilon=logepsilon,
   )
 
   # Now we compute the same computation, but backwards; starting from the
   # end of the logits and end of the word, and working back toward the start.
 
   # ctc_loss works fine with logits padding at the start of the sequence, so
@@ -163,15 +163,15 @@
   # _, [T, B, N+1], [T, B, N]
   _, logbeta_reverse_phi, logbeta_reverse_emit = (
       optax.ctc_loss_with_forward_probs(
           reverse_logits,
           reverse_logitpaddings,
           reverse_labels,
           labelpaddings,
-          blank_id,
+          blank_id=blank_id,
           log_epsilon=logepsilon,
       )
   )
 
   # These results are backward twice: the time is backward and the label
   # sequence is backward. So we flip the time and label axes, then shift
   # them left (see the hello example, above).
```

## praxis/layers/ctc_objectives_test.py

```diff
@@ -67,17 +67,17 @@
     label_paddings: JTensor,
 ) -> JTensor:
   return jnp.average(
       optax.ctc_loss(logprobs, logprob_paddings, labels, label_paddings)
   )
 
 
-def lengths_to_paddings(lengths: JTensor, maxlength: int) -> JTensor:
-  indices = jnp.arange(maxlength).reshape((1,) * lengths.ndim + (maxlength,))
-  lengths = jnp.expand_dims(lengths, axis=-1)
+def lengths_to_paddings(lengths: np.ndarray, maxlength: int) -> np.ndarray:
+  indices = np.arange(maxlength).reshape((1,) * lengths.ndim + (maxlength,))
+  lengths = np.expand_dims(lengths, axis=-1)
   elem_valid = indices < lengths
   return np.logical_not(elem_valid).astype(np.float32)
 
 
 class CtcTest(test_utils.TestCase):
   """Tests for the CTC loss function.
 
@@ -121,15 +121,15 @@
   def test_against_tf_ctc_loss_with_paddings(self):
     batchsize = 8
     timesteps = 150
     labelsteps = 25
     nclasses = 400
 
     logits = np.random.randn(batchsize, timesteps, nclasses)
-    logprobs = jax.nn.log_softmax(logits)
+    logprobs = np.array(jax.nn.log_softmax(logits))
     logprob_lens = np.random.randint(25, timesteps - 3, size=(batchsize,))
     logprob_paddings = lengths_to_paddings(logprob_lens, timesteps)
 
     labels = np.random.randint(
         1, nclasses, size=(batchsize, labelsteps)
     ).astype(np.int32)
     label_lens = np.random.randint(10, labelsteps, size=(batchsize,))
```

## praxis/layers/embedding_softmax.py

```diff
@@ -1317,15 +1317,15 @@
     Returns:
       a JTensor of shape [batch, seq_length, embedding_dim] if position JTensor
       is specified, else of shape [1, seq_length, embedding_dim].
     """
     ap = self.activation_split_dims_mapping
     if position is None:
       assert seq_length is not None
-      position = jnp.arange(seq_length, dtype=jnp.float32)[jnp.newaxis, :]
+      position = jnp.arange(seq_length, dtype=jnp.int32)[jnp.newaxis, :]
     if seq_length is None:
       assert position is not None
       assert position.ndim == 2
       # Only infer 'seq_length' from 'position' for 'matmul' lookup. For 'index'
       # lookup, using full position embedding table to make sure the index is
       # not missing by keeping the 'seq_length' value as None.
       if self.lookup_style == 'matmul':
```

## praxis/layers/flax_adapter_test.py

```diff
@@ -91,15 +91,15 @@
     out1 = self.cnn_p1(
         x, use_running_average=self.use_running_average
     ) + self.cnn_p1(x / 2.0, use_running_average=self.use_running_average)
     out2 = self.cnn_p2(x, use_running_average=self.use_running_average)
     out = self.bn(out1 + out2)
     return out1, out2, out
 
-  def classify(self, x: JTensor) -> JTensor:
+  def classify(self, x: JTensor) -> tuple[JTensor, JTensor]:
     out1 = self.cnn_p1.call_method('classify', x)
     out2 = self.cnn_p2.call_method('classify', x)
     return out1, out2
 
 
 class DirectMixLayer(base_layer.BaseLayer):
   """Direct instantiation version of the mix layer above."""
```

## praxis/layers/glam.py

```diff
@@ -12,18 +12,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Helper function to config GLaM models."""
 
 import fiddle as fdl
-from praxis import base_layer
 from praxis import pax_fiddle
 from praxis.layers import activations
 from praxis.layers import attentions
+from praxis.layers import checkpoint_policy
 from praxis.layers import embedding_softmax
 from praxis.layers import normalizations
 from praxis.layers import transformer_models
 from praxis.layers import transformers
 
 LanguageModelType = transformer_models.LanguageModelType
 
@@ -207,14 +207,15 @@
     moe_load_balance_loss_weight=0.01,
     z_loss_weight=1e-4,
     combine_qkv=False,
     bidirectional=False,
     num_pipeline_stages=1,
     num_pipeline_microbatches=1,
     model_type=LanguageModelType.CAUSAL,
+    checkpoint_policy=checkpoint_policy.AutodiffCheckpointType.SAVE_NOTHING,
 ) -> pax_fiddle.Config[transformer_models.TransformerLm]:
   """Common setup for GLaM Decoder-only Transformer Model.
 
   This function sets up configs for both MoE and dense GLaM models.
   The MoE block consists of two transformer layer with the feedforward
   sublayer of the first one replaced by a MoE layer. The dense block consists
   of a transformer. The transformer layer used by GLam differs from the
@@ -259,14 +260,15 @@
     z_loss_weight: additional loss term to stablize the final softmax logit.
     combine_qkv: if combined qkv projection layer is used.
     bidirectional: Set to support bidirectional relative attention.
     num_pipeline_stages: Number of pipeline stages.
     num_pipeline_microbatches: Number of pipeline microbatches.
     model_type: Type of the Language Model. Either `CAUSAL`, `PREFIX`, or
       `BIDIRECTIONAL`.
+    checkpoint_policy: Activation remat policy when pipelining is disabled.
 
   Returns:
     A Params object to set up a StackedTransformer.
   """
   p = pax_fiddle.Config(transformer_models.TransformerLm)
   p.name = name
   p.packed_input = True
@@ -322,14 +324,15 @@
   if num_pipeline_stages == 1:
     p.stacked_transformer_tpl = pax_fiddle.Config(
         transformers.StackedTransformerRepeated,
         name='decoder',
         unroll_in_decode=True,
         block=glam_p,
         x_times=num_blocks,
+        checkpoint_policy=checkpoint_policy,
     )
   else:
     assert num_blocks % num_pipeline_stages == 0
     glam_p.num_layers = num_transformer_layers // num_pipeline_stages
     glam_p.moe_layers = list(range(0, glam_p.num_layers, 2))
     p.stacked_transformer_tpl = pax_fiddle.Config(
         transformers.PipelinedTransformer,
```

## praxis/layers/gpu_fast_attention.py

```diff
@@ -27,14 +27,15 @@
 from jax.experimental.shard_map import shard_map
 from praxis import asserts
 from praxis import base_layer
 from praxis import py_utils
 from praxis import pytypes
 from praxis.layers import attentions
 from praxis.layers import grouped_query_attention
+from praxis.layers import multi_query_attention
 from praxis.layers import normalizations
 
 # pylint: disable=g-import-not-at-top
 try:
   from jax.experimental.pallas.ops import attention
   from jax.experimental.pallas.ops import layer_norm
   from jax.experimental.pallas.ops.gpu import decode_attention
@@ -131,28 +132,29 @@
     # TODO(zhangqiaorjc): Use hparam instead of env var.
     bwd_pass_impl = os.getenv(
         'pax_flash_attention_backward_pass_impl', default='xla'
     )
 
     @functools.partial(
         shard_map,
-        mesh=self.get_mesh(),
+        mesh=self._get_mesh(),
         in_specs=(
             blnh_pspec,
             blnh_pspec,
             blnh_pspec,
         ),
         out_specs=blnh_pspec,
         check_rep=False,
     )
     def sharded_mha(q, k, v):
       return attention.mha(
           q,
           k,
           v,
+          segment_ids=None,
           causal=self.is_causal,
           backward_pass_impl=bwd_pass_impl,
       )
 
     encoded = sharded_mha(query, key, value)
     encoded = self._shard_blnh(encoded)
     # TODO(zhangqiaorjc): return probs.
@@ -192,15 +194,14 @@
           value_state_name,
           atten_mask,
           relative_bias,
           time_step,
       )
 
     assert relative_bias is None
-    assert not self.scale_logits_by_head_dims
     assert self.attention_extra_logit is None
     assert not self.zero_fully_masked
     del time_step
     key = self._shard_blnh(self.get_decode_state(key_state_name))
     value = self._shard_blnh(self.get_decode_state(value_state_name))
     k_b = key.shape[0]
     q_b = query.shape[0]
@@ -217,15 +218,15 @@
     query = self._scale_query(query)
 
     bnh_pspec = self._bnh_pspec()
     blnh_pspec = self._blnh_pspec()
 
     @functools.partial(
         shard_map,
-        mesh=self.get_mesh(),
+        mesh=self._get_mesh(),
         in_specs=(
             bnh_pspec,
             blnh_pspec,
             blnh_pspec,
         ),
         out_specs=bnh_pspec,
         check_rep=False,
@@ -288,15 +289,15 @@
     # Assume causal self-attention mask. Not supporting cross_attention.
     sh = self.activation_split_dims_mapping
     bnh_pspec = jax.sharding.PartitionSpec(sh.btnh[0], sh.btnh[2], sh.btnh[3])
     blnh_pspec = jax.sharding.PartitionSpec(sh.bskh)
 
     @functools.partial(
         shard_map,
-        mesh=self.get_mesh(),
+        mesh=self._get_mesh(),
         in_specs=(
             bnh_pspec,
             blnh_pspec,
             blnh_pspec,
         ),
         out_specs=bnh_pspec,
         check_rep=False,
@@ -309,14 +310,171 @@
           k_splits=self.flash_decoding_k_splits,
       )
 
     encoded = sharded_decode_gqa(query, key, value)
     return encoded, None  # pytype: disable=bad-return-type  # jax-ndarray
 
 
+class GpuTritonFusedMultiQueryDotProductAttention(
+    multi_query_attention.MultiQueryDotProductAttention
+):
+  """Using flash decoding for MultiQueryDotProductAttention."""
+
+  use_flash_decoding: bool = False
+  flash_decoding_k_splits: int = 16
+
+  def _get_mesh(self) -> jax.sharding.Mesh:
+    device_mesh = py_utils.create_device_mesh(
+        self.ici_mesh_shape,
+        self.dcn_mesh_shape,
+        contiguous_submeshes=self.contiguous_submeshes,
+    )
+    mesh = jax.sharding.Mesh(device_mesh, self.mesh_axis_names)
+    return mesh
+
+  def _atten_context(
+      self,
+      query: JTensor,
+      key: JTensor,
+      value: JTensor,
+      atten_mask: JTensor,
+      relative_bias: JTensor | None = None,
+  ) -> Tuple[JTensor, JTensor]:
+    """Computes atten context."""
+    b, t, n, h = query.shape
+    is_decoding = t == 1
+    if not is_decoding or not self.use_flash_decoding:
+      return super()._atten_context(
+          query, key, value, atten_mask, relative_bias
+      )
+
+    if self.atten_dropout_prob > 0.0 and not self.do_eval:
+      raise NotImplementedError
+    if self.atten_logit_cap > 0.0:
+      raise NotImplementedError
+    if relative_bias is not None:
+      raise NotImplementedError
+
+    query = self._scale_query(query)
+    query = query.reshape([b, n, h])
+
+    # Assume causal self-attention mask. Not supporting cross_attention.
+    sh = self.activation_split_dims_mapping
+    bnh_pspec = jax.sharding.PartitionSpec(sh.btnh[0], sh.btnh[2], sh.btnh[3])
+    blnh_pspec = jax.sharding.PartitionSpec(sh.bskh)
+
+    @functools.partial(
+        shard_map,
+        mesh=self._get_mesh(),
+        in_specs=(
+            bnh_pspec,
+            blnh_pspec,
+            blnh_pspec,
+        ),
+        out_specs=bnh_pspec,
+        check_rep=False,
+    )
+    def sharded_decode_gqa(q, k, v):
+      return decode_attention.gqa(
+          q,  # [batch_size, num_q_heads, head_dim]
+          k,  # [batch_size, k_seq_len, num_kv_heads, head_dim]
+          v,  # [batch_size, k_seq_len, num_kv_heads, head_dim]
+          k_splits=self.flash_decoding_k_splits,
+      )
+
+    encoded = sharded_decode_gqa(query, key, value)
+    return encoded, None  # pytype: disable=bad-return-type  # jax-ndarray
+
+  def _dot_atten_one_step(
+      self,
+      query: JTensor,
+      key_state_name: str,
+      value_state_name: str,
+      atten_mask: JTensor,
+      relative_bias: JTensor | None = None,
+      time_step: JTensor | None = None,
+  ) -> tuple[JTensor, JTensor]:
+    """Dot attention function for queries with 1 time step.
+
+    Args:
+      query: JTensor of shape [B, N, H].
+      key_state_name: Name of the decoding key state variable.
+      value_state_name: Name of the decoding value state variable.
+      atten_mask: JTensor of shape [1|B, 1, S] which is a mask that is applied
+        to prevent attention between unwanted pairs. This has already been
+        converted into large negative logits. The first dimension is allowed to
+        be of size 1, if the mask is shared by all items in the batch (e.g.,
+        only a causal mask).
+      relative_bias: Relative bias of shape [1|B, N, 1, S].
+      time_step: A scalar. The time step tensor.
+
+    Returns:
+      encoded: JTensor of shape [B, N, H].
+      probs: JTensor of shape [B, N, S].
+    """
+    del time_step
+    if not self.use_flash_decoding:
+      return super()._dot_atten_one_step(
+          query,
+          key_state_name,
+          value_state_name,
+          atten_mask,
+          relative_bias,
+      )
+
+    assert relative_bias is None
+    assert self.attention_extra_logit is None
+
+    key = self._shard_blnh(self.get_decode_state(key_state_name))
+    value = self._shard_blnh(self.get_decode_state(value_state_name))
+    k_b = key.shape[0]
+    q_b = query.shape[0]
+    assert k_b == q_b, (k_b, q_b)
+
+    # query is 3d.
+    query = self._shard_bnh(query)
+
+    b, s, n, h = key.shape
+    base_layer.assert_has_shape(value, [b, s, n, h])
+    base_layer.assert_has_shape(query, [b, n * self.num_kv_heads, h])
+    base_layer.assert_has_shape(atten_mask, [-1, 1, s])
+    asserts.in_set(atten_mask.shape[0], [b, 1])
+    query = self._scale_query(query)
+
+    blnh_pspec = base_layer.to_partition_spec(
+        self.activation_split_dims_mapping.blnh, self.mesh_axis_names
+    )
+    bnh_pspec = jax.sharding.PartitionSpec(
+        blnh_pspec[0], blnh_pspec[2], blnh_pspec[3]
+    )
+
+    @functools.partial(
+        shard_map,
+        mesh=self._get_mesh(),
+        in_specs=(
+            bnh_pspec,
+            blnh_pspec,
+            blnh_pspec,
+        ),
+        out_specs=bnh_pspec,
+        check_rep=False,
+    )
+    def sharded_decode_gqa(q, k, v):
+      return decode_attention.gqa(
+          q,
+          k,
+          v,
+          k_splits=self.flash_decoding_k_splits,
+      )
+
+    encoded = sharded_decode_gqa(query, key, value)
+    encoded = self._shard_bnh(encoded)
+    return encoded, None  # pytype: disable=bad-return-type  # jax-ndarray
+
+
 class GpuTritonFusedLayerNorm(normalizations.LayerNorm):
 
   def _ble_pspec(self):
     """Return sharding annotations to tensors of shape [b, l, e]."""
     # TODO(zhangqiaorjc): Avoid hardcode batch dim sharding..
     return base_layer.to_partition_spec(
         [('replica', 'data'), None, None], self.mesh_axis_names
```

## praxis/layers/models.py

```diff
@@ -757,14 +757,15 @@
             use_top_k_for_logprobs=decoder_params.use_top_k_for_logprobs,
             return_entropy_score=return_entropy_score,
             process_result_fn=decoder_params.process_result_fn,
             optimize_eos=decoder_params.optimize_eos,
             sample_constraint=decoder_params.sample_constraint,
             enforce_sample_constraints=enforce_sample_constraints,
             num_per_token_logprobs=num_per_token_logprobs,
+            early_exit=decoder_params.early_exit,
         )
 
     elif template_has_type(decoder_params, GreedyDecoderHParams):
       assert isinstance(decoder_params, GreedyDecoderHParams)
 
       def fprop_fn(mdl, ids, paddings):
         del ids, paddings
@@ -912,15 +913,15 @@
     return metrics, ret, out_clu_metrics
 
 
 # TODO(@jwyang): add support for sample decoding and beam search
 class LanguageModelContinuousBatching(LanguageModel):
   """Language model that uses continuous batching."""
 
-  def _last_decode_step(self, decoder_params):
+  def _last_decode_step(self, decoder_params) -> int:
     max_decode_steps = decoder_params.max_decode_steps
     if isinstance(decoder_params.max_decode_steps, int):
       max_decode_steps = [decoder_params.max_decode_steps]
     max_decode_steps = sorted(max_decode_steps)
     return max(max_decode_steps)
 
   def sample_init_decode_state(
@@ -955,32 +956,37 @@
           f'{decoder_params.seqlen}'
       )
     decode_data = self._prepare_decode_data(input_batch, decoder_params)
 
     # run prefill
     def fprop_fn(mdl, ids, paddings):
       del ids, paddings
-      mdl(
+      output = mdl(
           decode_data.fprop_input_ids,
           decode_data.fprop_input_paddings,
           segment_ids=decode_data.fprop_segment_ids,
           segment_pos=decode_data.fprop_segment_pos,
           start_time_step=decode_data.start_time_step,
           causal_attention_mask=decode_data.causal_attention_mask,
           **decode_data.extra_input_kwargs,
       )
+      return output.logits
 
-    fprop_fn(self.lm, decode_data.input_ids, decode_data.input_paddings)
+    logits = fprop_fn(
+        self.lm, decode_data.input_ids, decode_data.input_paddings
+    )
+    last_prefix_logits = logits[:, -1, :]
 
     # init prefix decode state
     prefill_decode_state = self.sample_init_decode_state(
         decode_data, decoder_params
     )
 
     batch_size = input_batch.ids.shape[0]
+    logging.info('Prefill batch_size is %s', batch_size)
     # Fetch dynamic per params from input_batch if the
     # input_batch has this information.
     last_decode_step = self._last_decode_step(decoder_params)
     temperature = getattr(decoder_params, 'temperature', 0.0)
     prefill_decode_state.temperature = getattr(
         input_batch,
         'temperature',
@@ -998,197 +1004,144 @@
     )
     prefill_decode_state.per_example_top_k = getattr(
         input_batch,
         'per_example_top_k',
         jnp.ones(shape=(batch_size,), dtype=jnp.int32),
     )
 
+    def extend_step_fn(mdl, ids, segment_pos):
+      del mdl, ids, segment_pos
+      return last_prefix_logits
+
+    last_decode_steps = self._last_decode_step(decoder_params)
+    max_prefix_len = decoder_params.seqlen - last_decode_steps
+
+    prefill_decode_state = sample_decode.sample_decoding_step(
+        model=self.lm,
+        extend_step_fn=extend_step_fn,
+        decode_state=prefill_decode_state,
+        max_prefix_len=max_prefix_len,
+        eos_id=decoder_params.eos_id,
+        decode_loop_mesh_axes_transpose=decoder_params.decode_loop_mesh_axes_transpose,
+        max_decode_steps=decoder_params.max_decode_steps,
+    )
     return prefill_decode_state
 
   def sample_insert(
       self,
       decoder_params,
       prefix_decode_state,
       prefix_decode_cache,
       decode_state,
+      prefix_slot,
       slot,
   ):
-
     # update decode_state
     decode_state.per_sample_steps = decode_state.per_sample_steps.at[slot].set(
-        prefix_decode_state.per_sample_steps[0]
+        prefix_decode_state.per_sample_steps[prefix_slot]
     )
 
     # set 0 to start decoding phase
     decode_state.done = decode_state.done.at[slot].set(0)
-    decode_state.has_eos = decode_state.has_eos.at[slot].set(
-        prefix_decode_state.has_eos[0]
-    )
 
-    decode_state.prefix_lengths = decode_state.prefix_lengths.at[slot].set(
-        prefix_decode_state.prefix_lengths[0]
-    )
-    decode_state.segment_pos = decode_state.segment_pos.at[slot].set(
-        prefix_decode_state.segment_pos[0]
-    )
-    decode_state.decode_lengths = decode_state.decode_lengths.at[slot].set(
-        prefix_decode_state.decode_lengths[0]
-    )
+    attrs = [
+        'has_eos',
+        'prefix_lengths',
+        'segment_pos',
+        'decode_lengths',
+        'output_ids',
+        'logprobs',
+        'temperature',
+        'per_example_max_decode_steps',
+        'per_example_top_p',
+        'per_example_top_k',
+    ]
 
-    decode_state.output_ids = decode_state.output_ids.at[slot].set(
-        prefix_decode_state.output_ids[0]
-    )
-    decode_state.logprobs = decode_state.logprobs.at[slot].set(
-        prefix_decode_state.logprobs[0]
-    )
-    decode_state.temperature = decode_state.temperature.at[slot].set(
-        prefix_decode_state.temperature[0]
-    )
-    decode_state.per_example_max_decode_steps = (
-        decode_state.per_example_max_decode_steps.at[slot].set(
-            prefix_decode_state.per_example_max_decode_steps[0]
+    for attr in attrs:
+      update = getattr(prefix_decode_state, attr)
+      update = update[prefix_slot]
+      state_dtype = getattr(decode_state, attr).dtype
+      if state_dtype != update.dtype:
+        logging.info(
+            '%s changed from %s to %s:', attr, state_dtype, update.dtype
         )
-    )
-    decode_state.per_example_top_p = decode_state.per_example_top_p.at[
-        slot
-    ].set(prefix_decode_state.per_example_top_p[0])
-    decode_state.per_example_top_k = decode_state.per_example_top_k.at[
-        slot
-    ].set(prefix_decode_state.per_example_top_k[0])
+        update = update.astype(state_dtype)
+
+      if update.ndim == 0:
+        update = jnp.expand_dims(update, axis=0)
+      ret = jax.lax.dynamic_update_slice_in_dim(
+          getattr(decode_state, attr), update, slot, axis=0
+      )
+      setattr(decode_state, attr, ret)
 
     # update kv_cache (need to right aligned)
     max_prefix_len = decoder_params.seqlen - decoder_params.max_decode_steps
     sequence_len = decoder_params.seqlen
 
-    right_aligned_length = sequence_len - (
-        decode_state.step - max_prefix_len + 1
-    )
+    right_aligned_length = sequence_len - (decode_state.step - max_prefix_len)
     for i in range(self.lm_tpl.stacked_transformer_tpl.num_layers):
       layer_kv_cache_key = 'x_layers_{}'.format(i)
-      per_layerprefix_decode_cache = prefix_decode_cache['decoder_cache']['lm'][
+      per_layer_prefix_decode_cache = prefix_decode_cache['decoder_cache'][
+          'lm'
+      ]['transformer'][layer_kv_cache_key]['self_attention']
+      atten_state = self.variables[base_layer.DECODE_CACHE]['lm'][
           'transformer'
       ][layer_kv_cache_key]['self_attention']
-      new_key_cache = per_layerprefix_decode_cache['key_state']
-
-      new_value_cache = per_layerprefix_decode_cache['value_state']
-
-      new_pos_emb = None
-      if 'key_post_rotary_pos_emb' in per_layerprefix_decode_cache:
-        new_pos_emb = per_layerprefix_decode_cache['key_post_rotary_pos_emb']
-
-      self.variables[base_layer.DECODE_CACHE]['lm']['transformer'][
-          layer_kv_cache_key
-      ]['self_attention']['key_state'] = (
-          self.variables[base_layer.DECODE_CACHE]['lm']['transformer'][
-              layer_kv_cache_key
-          ]['self_attention']['key_state']
-          .at[slot]
-          .set(
-              decoder_utils.right_align_tensors(
-                  new_key_cache, right_aligned_length
-              )[0]
-          )
-      )
-
-      self.variables[base_layer.DECODE_CACHE]['lm']['transformer'][
-          layer_kv_cache_key
-      ]['self_attention']['value_state'] = (
-          self.variables[base_layer.DECODE_CACHE]['lm']['transformer'][
-              layer_kv_cache_key
-          ]['self_attention']['value_state']
-          .at[slot]
-          .set(
-              decoder_utils.right_align_tensors(
-                  new_value_cache, right_aligned_length
-              )[0]
-          )
-      )
 
-      if new_pos_emb is not None:
-        self.variables[base_layer.DECODE_CACHE]['lm']['transformer'][
-            layer_kv_cache_key
-        ]['self_attention']['key_post_rotary_pos_emb'] = (
-            self.variables[base_layer.DECODE_CACHE]['lm']['transformer'][
-                layer_kv_cache_key
-            ]['self_attention']['key_post_rotary_pos_emb']
-            .at[slot]
-            .set(
-                decoder_utils.right_align_tensors(
-                    new_pos_emb, right_aligned_length
-                )[0]
-            )
+      for name in atten_state.keys():
+        new_state = per_layer_prefix_decode_cache[name][prefix_slot]
+        new_state = jnp.expand_dims(new_state, axis=0)
+        atten_state[name] = jax.lax.dynamic_update_slice_in_dim(
+            atten_state[name],
+            decoder_utils.right_align_tensors(new_state, right_aligned_length),
+            slot,
+            axis=0,
         )
+
     return decode_state
 
   def left_align_decode_state(
       self, max_prefix_len, max_decode_steps, decode_state, batch_size
   ):
     # when reach end of sequence, align all tensors to left end, reset step
     decode_state.per_sample_steps = jnp.where(
-        decode_state.done, max_prefix_len - 1, decode_state.per_sample_steps
+        decode_state.done, max_prefix_len, decode_state.per_sample_steps
     )
     left_align_steps = jnp.max(decode_state.per_sample_steps)
     left_align_steps_arr = (
         jnp.ones_like(decode_state.prefix_lengths) * left_align_steps
     )
 
     row_length = max_prefix_len + max_decode_steps
 
     transformer_kv_cache = self.variables[base_layer.DECODE_CACHE]['lm'][
         'transformer'
     ]
     for i in range(self.lm_tpl.stacked_transformer_tpl.num_layers):
       layer_kv_cache_key = 'x_layers_{}'.format(i)
       atten_kv = transformer_kv_cache[layer_kv_cache_key]['self_attention']
-      new_key_cache = atten_kv['key_state']
-      new_value_cache = atten_kv['value_state']
-
-      new_pos_emb = None
-      if 'key_post_rotary_pos_emb' in atten_kv:
-        new_pos_emb = atten_kv['key_post_rotary_pos_emb']
-        new_pos_emb = jnp.where(
-            decode_state.step < row_length - 1,
-            new_pos_emb,
-            decoder_utils.left_align_kv_cache(
-                new_pos_emb, left_align_steps_arr, row_length - 1, batch_size
-            ),
+      for name in atten_kv.keys():
+        atten_kv[name] = decoder_utils.left_align_kv_cache(
+            atten_kv[name],
+            left_align_steps_arr,
+            row_length - 1,
+            batch_size=batch_size,
         )
 
-      new_key_cache = jnp.where(
-          decode_state.step < row_length - 1,
-          new_key_cache,
-          decoder_utils.left_align_kv_cache(
-              new_key_cache, left_align_steps_arr, row_length - 1, batch_size
-          ),
-      )
-      new_value_cache = jnp.where(
-          decode_state.step < row_length - 1,
-          new_value_cache,
-          decoder_utils.left_align_kv_cache(
-              new_value_cache, left_align_steps_arr, row_length - 1, batch_size
-          ),
-      )
-
-      atten_kv['key_state'] = new_key_cache
-      atten_kv['value_state'] = new_value_cache
-      if new_pos_emb is not None:
-        atten_kv['key_post_rotary_pos_emb'] = new_pos_emb
-
     decode_state.step = jnp.where(
         decode_state.step < row_length - 1, decode_state.step, left_align_steps
     )
     self.variables[base_layer.DECODE_CACHE]['lm']['time_step'] = (
         decode_state.step[0]
     )
 
     return decode_state
 
   def sample_generate(
       self,
-      tokens: JTensor,
       decode_state: NestedMap,
       decoder_params: DecoderHParams,
       align_decode_state: bool = False,
   ) -> NestedMap:
     def extend_step_fn(mdl, ids, segment_pos):
       xent = mdl.extend_step(ids, segment_pos=segment_pos)
       return xent.logits
@@ -1202,16 +1155,14 @@
       decode_state = self.left_align_decode_state(
           max_prefix_len,
           last_decode_steps,
           decode_state,
           decoder_params.num_cache_slots,
       )
 
-    decode_state.output_ids = tokens
-
     decode_state = sample_decode.sample_decoding_step(
         model=model,
         extend_step_fn=extend_step_fn,
         decode_state=decode_state,
         max_prefix_len=max_prefix_len,
         eos_id=decoder_params.eos_id,
         decode_loop_mesh_axes_transpose=decode_mesh_transpose,
@@ -1373,68 +1324,57 @@
       # assert per_example_xent is float32, learning might be unstable in
       # bfloat16.
       assert softmax_out.per_example_xent.dtype == jnp.float32
       assert softmax_out.per_sequence_xent.dtype == jnp.float32
       assert len(softmax_out.per_sequence_xent.shape) == 1
       return -1.0 * softmax_out.per_sequence_xent
 
-    y_w_ref_log_p = per_seq_log_p(predictions.y_w_ref)
+    # Prevent backprop into reference model
+    y_w_ref_log_p = jax.lax.stop_gradient(per_seq_log_p(predictions.y_w_ref))
+    y_l_ref_log_p = jax.lax.stop_gradient(per_seq_log_p(predictions.y_l_ref))
+    # Allow backprop into policy model
     y_w_pi_log_p = per_seq_log_p(predictions.y_w_pi)
-    y_l_ref_log_p = per_seq_log_p(predictions.y_l_ref)
     y_l_pi_log_p = per_seq_log_p(predictions.y_l_pi)
 
     self.add_summary('dpo/y_w_ref_log_p', jnp.mean(y_w_ref_log_p))
     self.add_summary('dpo/y_w_pi_log_p', jnp.mean(y_w_pi_log_p))
     self.add_summary('dpo/y_l_ref_log_p', jnp.mean(y_l_ref_log_p))
     self.add_summary('dpo/y_l_pi_log_p', jnp.mean(y_l_pi_log_p))
     add_hist(self, 'dpo/y_w_ref_log_p', y_w_ref_log_p)
     add_hist(self, 'dpo/y_w_pi_log_p', y_w_pi_log_p)
     add_hist(self, 'dpo/y_l_ref_log_p', y_l_ref_log_p)
     add_hist(self, 'dpo/y_l_pi_log_p', y_l_pi_log_p)
 
-    r_hat_y_w = jax.lax.stop_gradient(
-        self.beta * (y_w_pi_log_p - y_w_ref_log_p)
-    )
-    r_hat_y_l = jax.lax.stop_gradient(
-        self.beta * (y_l_pi_log_p - y_l_ref_log_p)
-    )
+    r_hat_y_w = self.beta * (y_w_pi_log_p - y_w_ref_log_p)
+    r_hat_y_l = self.beta * (y_l_pi_log_p - y_l_ref_log_p)
 
-    # This is first equation on page #5 in the DPO paper.
-    per_example_loss = (
-        self.beta
-        * jax.nn.sigmoid(r_hat_y_l - r_hat_y_w)
-        * (y_l_pi_log_p - y_w_pi_log_p)
-    )
-    loss = jnp.mean(per_example_loss)
     # This is the dpo_loss, same as what equation 7 in the paper computes.
-    dpo_loss = jnp.mean(-1.0 * jnp.log(jax.nn.sigmoid(r_hat_y_w - r_hat_y_l)))
+    loss = -1.0 * jnp.mean(jax.nn.log_sigmoid(r_hat_y_w - r_hat_y_l))
 
     self.add_summary('dpo/r_hat_y_w', jnp.mean(r_hat_y_w))
     self.add_summary('dpo/r_hat_y_w_std', jnp.std(r_hat_y_w))
     self.add_summary('dpo/r_hat_y_l', jnp.mean(r_hat_y_l))
     self.add_summary('dpo/r_hat_y_l_std', jnp.std(r_hat_y_l))
     self.add_summary('dpo/delta_r_hat', jnp.mean(r_hat_y_w - r_hat_y_l))
     self.add_summary('dpo/delta_r_hat_std', jnp.std(r_hat_y_w - r_hat_y_l))
-    self.add_summary('dpo/dpo_loss', dpo_loss)
+    self.add_summary('dpo/dpo_loss', loss)
     self.add_summary(
         '_dpo_topline/p_correct_ranking',
         jnp.mean(jax.nn.sigmoid(r_hat_y_w - r_hat_y_l)),
     )
     add_hist(self, 'dpo/r_hat_y_w', r_hat_y_w)
     add_hist(self, 'dpo/r_hat_y_l', r_hat_y_l)
     add_hist(self, 'dpo/delta_r_hat', r_hat_y_w - r_hat_y_l)
 
     batch_size = predictions.y_l_ref.per_example_xent.shape[0]
 
     # TODO(yonghui): Add diagnostic summaries.
-    # pair_loss is what learning back-props into.
     return (
         NestedMap(
             total_loss=(loss, jnp.array(batch_size, loss.dtype)),
-            dpo_loss=(dpo_loss, jnp.array(batch_size, dpo_loss.dtype)),
         ),
         {},
     )
 
 
 class SequenceModel(base_model.BaseModel):
   """Sequence Model base task.
@@ -1483,14 +1423,17 @@
         input_paddings=input_batch.src.paddings,
         targets=input_batch.tgt.ids,
         target_paddings=input_batch.tgt.paddings,
         labels=labels,
         **packed_input_kwargs,
     )
 
+  def _prepare_guidance_decode_data(self, decode_data: NestedMap) -> NestedMap:
+    raise NotImplementedError('SequenceModel does not support guidance.')
+
   def compute_loss(self, predictions, input_batch):
     """Computes the loss and other metrics for the given predictions.
 
     Args:
       predictions: The output of `ComputePredictions`.
       input_batch: A `.NestedMap` object containing input tensors to this tower.
 
@@ -1592,26 +1535,51 @@
           fprop_input_ids,
           fprop_input_paddings,
           decoder_params,
       )
     elif template_has_type(decoder_params, SampleDecoderHParams):
       assert isinstance(decoder_params, SampleDecoderHParams)
 
+      decode_data = NestedMap(
+          input_ids=input_batch.src.ids,
+          input_paddings=input_batch.src.paddings,
+          target_ids=input_batch.tgt.ids,
+          target_paddings=input_batch.tgt.paddings,
+          prefix_lengths=input_batch.prefix_lengths
+          if 'prefix_lengths' in input_batch
+          else None,
+      )
+
+      if getattr(decoder_params, 'cf_guidance_scale', None) is not None:
+        decode_data = self._prepare_guidance_decode_data(decode_data)
+
       def fprop_fn(mdl, ids, paddings):
         del ids, paddings
-        mdl.model(
-            inputs=input_batch.src.ids,
-            input_paddings=input_batch.src.paddings,
-            targets=jax.lax.dynamic_slice_in_dim(
-                input_batch.tgt.ids, 0, 1, axis=1
-            ),
-            target_paddings=jax.lax.dynamic_slice_in_dim(
-                input_batch.tgt.paddings, 0, 1, axis=1
-            ),
-        )
+        if 'encoder_output' in decode_data:
+          mdl.model.fprop_with_encoder_output(
+              encoder_output=decode_data.encoder_output,
+              input_paddings=decode_data.input_paddings,
+              targets=jax.lax.dynamic_slice_in_dim(
+                  decode_data.target_ids, 0, 1, axis=1
+              ),
+              target_paddings=jax.lax.dynamic_slice_in_dim(
+                  decode_data.target_paddings, 0, 1, axis=1
+              ),
+          )
+        else:
+          mdl.model(
+              inputs=decode_data.input_ids,
+              input_paddings=decode_data.input_paddings,
+              targets=jax.lax.dynamic_slice_in_dim(
+                  decode_data.target_ids, 0, 1, axis=1
+              ),
+              target_paddings=jax.lax.dynamic_slice_in_dim(
+                  decode_data.target_paddings, 0, 1, axis=1
+              ),
+          )
 
       temperature = decoder_params.temperature
 
       next_token_sampler_p = decoder_params.next_token_sampler_tpl.clone()
       # TODO(b/260646361): Avoid this param propagation.
       next_token_sampler_p.top_k = decoder_params.k
       next_token_sampler_p.top_p = decoder_params.p
@@ -1620,25 +1588,23 @@
           decoder_params.top_k_recall_target
       )
       next_token_sampler_p.use_top_k_for_logprobs = (
           decoder_params.use_top_k_for_logprobs
       )
       next_token_sampler = base_layer.instantiate(next_token_sampler_p)
 
-      prefix_lengths = None
-      if 'prefix_lengths' in input_batch:
-        prefix_lengths = input_batch.prefix_lengths
+      prefix_lengths = decode_data.prefix_lengths
       result = sample_decode.sample_decode(
           self,
           extend_step_fn,
           transform_state_fn=transform_decode_state_fn,
           lazy_broadcast_prefix_fn=None,
           next_token_sampler=next_token_sampler,
-          prefix_ids=input_batch.tgt.ids,
-          prefix_paddings=input_batch.tgt.paddings,
+          prefix_ids=decode_data.target_ids,
+          prefix_paddings=decode_data.target_paddings,
           prefix_lengths=prefix_lengths,
           seq_len=decoder_params.seqlen,
           fprop_fn=fprop_fn,
           num_samples=decoder_params.num_samples,
           fprop_for_prefix=decoder_params.fprop_for_prefix,
           temperature=temperature,
           max_decode_steps=decoder_params.max_decode_steps,
@@ -1646,14 +1612,15 @@
           cf_guidance_scale=decoder_params.cf_guidance_scale,
           controlled_decoding=decoder_params.controlled_decoding,
           sort_samples=decoder_params.sort_samples,
           top_k_recall_target=decoder_params.top_k_recall_target,
           use_top_k_for_logprobs=decoder_params.use_top_k_for_logprobs,
           return_entropy_score=False,
           process_result_fn=decoder_params.process_result_fn,
+          early_exit=decoder_params.early_exit,
       )
     elif template_has_type(decoder_params, GreedyDecoderHParams):
 
       def fprop_fn(mdl, ids, paddings):
         del ids, paddings
         mdl.model(
             inputs=input_batch.src.ids,
```

## praxis/layers/models_test.py

```diff
@@ -359,16 +359,16 @@
         paddings=jnp.array([[0.0, 0.0, 1.0, 1.0, 1.0]], dtype=jnp.float32),
     )
     results = self._run_decode(p, logits, input_batch)
     self.assertArraysEqual(
         results.prefix_lengths, np.array([[2]], dtype=np.int32)
     )
 
-    expected_output_ids = np.array([[[11, 5, 1, 3, 4]]], dtype=np.int32)
-    expected_prefix_ids = np.array([[[11, 5, 0, 0, 0]]], dtype=np.int32)
+    expected_output_ids = jnp.array([[[11, 5, 1, 3, 4]]], dtype=np.int32)
+    expected_prefix_ids = jnp.array([[[11, 5, 0, 0, 0]]], dtype=np.int32)
 
     if fprop_for_prefix:
       total_len = p.seqlen + p.max_decode_steps
       expected_output_ids = py_utils.pad_or_trim_to(
           expected_output_ids, [1, 1, total_len], pad_val=0
       )
       expected_prefix_ids = py_utils.pad_or_trim_to(
@@ -417,20 +417,24 @@
                            np.array([[2]], dtype=np.int32))
 
     expected_output_ids = np.array([[[11, 5, 1, 3, 4]]], dtype=np.int32)
     expected_prefix_ids = np.array([[[11, 5, 0, 0, 0]]], dtype=np.int32)
 
     if fprop_for_prefix:
       total_len = p.seqlen + p.max_decode_steps
-      expected_output_ids = py_utils.pad_or_trim_to(
-          expected_output_ids, [1, 1, total_len], pad_val=0
-      )
-      expected_prefix_ids = py_utils.pad_or_trim_to(
-          expected_prefix_ids, [1, 1, total_len], pad_val=0
-      )
+      expected_output_ids = test_utils.to_np(
+          py_utils.pad_or_trim_to(
+              jnp.array(expected_output_ids), [1, 1, total_len], pad_val=0
+          )
+      ).astype(np.int32)
+      expected_prefix_ids = test_utils.to_np(
+          py_utils.pad_or_trim_to(
+              jnp.array(expected_prefix_ids), [1, 1, total_len], pad_val=0
+          )
+      ).astype(np.int32)
 
     self.assertArraysEqual(results.output_ids, expected_output_ids)
     self.assertArraysEqual(results.prefix_ids, expected_prefix_ids)
     self.assertArraysEqual(
         results.decode_lengths, np.array([[5]], dtype=np.int32)
     )
 
@@ -485,21 +489,25 @@
     )
 
     expected_output_ids = np.array([[[11, 5, 1, 3, 4]]], dtype=np.int32)
     expected_prefix_ids = np.array([[[11, 5, 0, 0, 0]]], dtype=np.int32)
 
     if fprop_for_prefix:
       total_len = p.seqlen + p.max_decode_steps
-      expected_output_ids = py_utils.pad_or_trim_to(
-          expected_output_ids, [1, 1, total_len], pad_val=0
-      )
+      expected_output_ids = test_utils.to_np(
+          py_utils.pad_or_trim_to(
+              jnp.array(expected_output_ids), [1, 1, total_len], pad_val=0
+          )
+      ).astype(np.int32)
       if not vanilla_sample_decode:
-        expected_prefix_ids = py_utils.pad_or_trim_to(
-            expected_prefix_ids, [1, 1, total_len], pad_val=0
-        )
+        expected_prefix_ids = test_utils.to_np(
+            py_utils.pad_or_trim_to(
+                jnp.array(expected_prefix_ids), [1, 1, total_len], pad_val=0
+            )
+        ).astype(np.int32)
 
     self.assertArraysEqual(results.output_ids, expected_output_ids)
     self.assertArraysEqual(results.prefix_ids, expected_prefix_ids)
     self.assertArraysEqual(
         results.decode_lengths, np.array([[5]], dtype=np.int32)
     )
 
@@ -1536,14 +1544,29 @@
     self.assertIn('output_ids', results)
     self.assertSequenceEqual(results.output_ids.shape, (1, 3, 5))
     self.assertArraysEqual(
         results.output_ids,
         [[[11, 10, 9, 1, 14], [11, 14, 14, 7, 14], [11, 11, 13, 3, 6]]],
     )
 
+  def test_cf_guidance_unimplemented_exception(self):
+    p = models.SampleDecoderHParams(seqlen=5, cf_guidance_scale=2.0)
+    data = NestedMap(
+        ids=jnp.array([[11, 12, 13, 14, 15]], dtype=jnp.int32),
+        paddings=jnp.array([[0, 1, 1, 1, 1]], dtype=jnp.float32),
+        labels=jnp.ones([1, 5], jnp.float32),
+        weights=jnp.ones([1, 5], jnp.float32),
+    )
+    input_batch = NestedMap(src=data, tgt=data)
+
+    with self.assertRaisesRegex(
+        NotImplementedError, 'SequenceModel does not support guidance.'
+    ):
+      self._run_decode(p, input_batch)
+
   def test_beam_search_decode(self):
     data = NestedMap(
         ids=jnp.array([[11, 12, 13, 14, 15]], dtype=jnp.int32),
         paddings=jnp.array([[0, 1, 1, 1, 1]], dtype=jnp.float32),
         labels=jnp.ones([1, 5], jnp.float32),
         weights=jnp.ones([1, 5], jnp.float32),
     )
@@ -1627,13 +1650,12 @@
     input_batch = _flatten_input_data(lm_input)
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       initial_vars = dpo_lm.init(prng_key, input_batch)
       outputs, _ = dpo_lm.apply(initial_vars, input_batch)
       logging.info('outputs: %s', outputs)
-      self.assertEqual(0.0, outputs.total_loss[0])
-      self.assertEqual(0.6931472, outputs.dpo_loss[0])
+      self.assertEqual(0.6931472, outputs.total_loss[0])
 
 
 if __name__ == '__main__':
   absltest.main()
```

## praxis/layers/multi_query_attention.py

```diff
@@ -599,27 +599,29 @@
   def _dot_atten_one_step(
       self,
       query: JTensor,
       key_state_name: str,
       value_state_name: str,
       atten_mask: JTensor,
       relative_bias: JTensor | None = None,
+      time_step: JTensor | None = None,
   ) -> tuple[JTensor, JTensor]:
     """Dot attention function for queries with 1 time step.
 
     Args:
       query: JTensor of shape [B, N, H] or [B, T, N, H].
       key_state_name: Name of the decoding key state variable.
       value_state_name: Name of the decoding value state variable.
       atten_mask: JTensor of shape [1/B, 1, S] or [B, 1, L, S] which is a mask
         that is applied to prevent attention between unwanted pairs. This has
         already been converted into large negative logits. The first dimension
         is allowed to be of size 1, if the mask is shared by all items in the
         batch (e.g., only a causal mask).
       relative_bias: Relative bias of shape [1/B, N, 1, S].
+      time_step: A scalar or JTensor. Current time-step, 0-based.
 
     Returns:
       encoded: JTensor of shape [B, N, H].
       probs: JTensor of shape [B, N, S].
     """
     extend_one_step = len(query.shape) == 3
     key = self.get_decode_state(key_state_name)
@@ -628,15 +630,15 @@
       query = self._shard_bnh(query)
     else:
       query = self._shard_blnh(query)
     if self.num_kv_heads == 1:
       key = self._shard_blh(key)
       value = self._shard_blh(value)
       encoded, probs = self._dot_atten_one_step_from_qkv(
-          query, key, value, atten_mask, relative_bias
+          query, key, value, atten_mask, relative_bias, time_step
       )
       return self._shard_bnh(encoded), probs
     else:
       b, n, h = query.shape
       _, _, nk, _ = key.shape
       key = self._shard_blnh(key)
       value = self._shard_blnh(value)
@@ -647,30 +649,32 @@
             relative_bias.shape[:1] + (nk, n // nk) + relative_bias.shape[2:],
         )
       else:
         v_rb = None
       with self._context_for_kv_vmap():
         encoded, probs = jax.vmap(
             self._dot_atten_one_step_from_qkv,
-            in_axes=(1, 2, 2, None, 1),
+            in_axes=(1, 2, 2, None, 1, None),
             out_axes=(1, 1),
-        )(v_q, key, value, atten_mask, v_rb)
+        )(v_q, key, value, atten_mask, v_rb, time_step)
         encoded = self._shard_bnh(jnp.reshape(encoded, (b, n, h)))
         probs = jnp.reshape(probs, (b, n, -1))
         return encoded, probs
 
   def _dot_atten_one_step_from_qkv(
       self,
       query: JTensor,
       key: JTensor,
       value: JTensor,
       atten_mask: JTensor,
       relative_bias: JTensor | None,
+      time_step: JTensor | None = None,
   ) -> tuple[JTensor, JTensor]:
     """_dot_atten_one_step with tensors instead of state names."""
+    del time_step
     # query is 3d.
     extend_one_step = len(query.shape) == 3
     b, s, h = key.shape
     if extend_one_step:
       base_layer.assert_has_shape(query, [b, -1, h])
       base_layer.assert_has_shape(atten_mask, [-1, -1, s])
     else:
@@ -1026,18 +1030,22 @@
     if self.relative_bias_tpl:
       # Relative bias uses time_step instead of segment_pos.
       relative_bias = self.relative_bias.extend_step(
           seq_length=self.decoding_state_sequence_length(), time_step=time_step)
     else:
       relative_bias = None
 
-    encoded, atten_prob = self._dot_atten_one_step(query_proj,
-                                                   key_state_name,
-                                                   value_state_name, atten_mask,
-                                                   relative_bias)
+    encoded, atten_prob = self._dot_atten_one_step(
+        query_proj,
+        key_state_name,
+        value_state_name,
+        atten_mask,
+        relative_bias,
+        time_step,
+    )
     # TODO(yonghui): return atten_probs back to the caller.
     del atten_prob
     # Post projection.
     if ap.bld and ap.blnh:
       # TODO(b/290067837): Workaround for problems when batch dim is sharded
       # differently in bld and blnh.
       ap_b = ap.bd[0] if ap.bd else ap.bld[0]
```

## praxis/layers/ngrammer_test.py

```diff
@@ -55,16 +55,18 @@
   @parameterized.parameters(
       (10000),
       (1000),
       (320000),
       (500),
   )
   def test_get_bigram_ids_with_packing(self, vocab_size):
-    ids = np.random.randint(vocab_size, size=(2, 8), dtype=np.int64)
-    segment_pos = np.array([[0, 1, 2, 3, 0, 1, 2, 3], [0, 1, 2, 0, 1, 2, 3, 4]])
+    ids = jnp.array(np.random.randint(vocab_size, size=(2, 8), dtype=np.int64))
+    segment_pos = jnp.array(
+        [[0, 1, 2, 3, 0, 1, 2, 3], [0, 1, 2, 0, 1, 2, 3, 4]]
+    )
     ngram_ids = ngrammer.get_bigram_ids(ids, vocab_size, segment_pos)
     np_ngram_ids = to_np(ngram_ids)
     self.assertLess(np.max(np_ngram_ids), vocab_size**2)
     self.assertEqual(np_ngram_ids[0, 0], ids[0, 0])
     self.assertEqual(np_ngram_ids[1, 0], ids[1, 0])
     self.assertEqual(np_ngram_ids[0, 4], ids[0, 4])
     self.assertEqual(np_ngram_ids[1, 3], ids[1, 3])
@@ -72,18 +74,21 @@
   @parameterized.parameters(
       (10000),
       (1000),
       (320000),
       (500),
   )
   def test_get_bigram_ids_with_packing_and_pair_ids(self, vocab_size):
-    ids = np.random.randint(vocab_size, size=(3, 8), dtype=np.int64)
-    segment_pos = np.array([[0, 1, 2, 3, 0, 1, 2, 3], [0, 1, 2, 0, 1, 2, 3, 4],
-                            [0, 1, 2, 3, 4, 5, 6, 7]])
-    pair_ids = np.array([[8, 0, 1, 2, 3, 4, 5, 6]] * 3)
+    ids = jnp.array(np.random.randint(vocab_size, size=(3, 8), dtype=np.int64))
+    segment_pos = jnp.array([
+        [0, 1, 2, 3, 0, 1, 2, 3],
+        [0, 1, 2, 0, 1, 2, 3, 4],
+        [0, 1, 2, 3, 4, 5, 6, 7],
+    ])
+    pair_ids = jnp.array([[8, 0, 1, 2, 3, 4, 5, 6]] * 3)
     ngram_ids = ngrammer.get_bigram_ids(ids, vocab_size, segment_pos)
     np_ngram_ids = to_np(ngram_ids)
     ngram_ids_pair_ids = ngrammer.get_bigram_ids(
         ids, vocab_size, segment_pos, pair_ids=pair_ids)
     np_ngram_ids_pair_ids = to_np(ngram_ids_pair_ids)
     self.assertArraysEqual(np_ngram_ids, np_ngram_ids_pair_ids)
```

## praxis/layers/quantizer_objectives_test.py

```diff
@@ -12,33 +12,47 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for quantizer_objectives."""
 
 from absl.testing import absltest
+import jax.numpy as jnp
 import numpy as np
 from praxis import test_utils
 from praxis.layers import quantizer_objectives
 
 
 class CodebookObjectivesTest(test_utils.TestCase):
-  codes = np.array([[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [0, 0]],
-                    [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [0, 0]]])
+  codes = np.array([
+      [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [0, 0]],
+      [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [0, 0]],
+  ])
 
   paddings = np.array([[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]])
   entropy = 1.609
   pplx = 5.000
   num_classes = 11
 
   def test_batch_pplx_entropy_from_codes(self):
     pplx, entropy, _ = quantizer_objectives.batch_pplx_entropy_from_codes(
-        codes=self.codes, num_classes=self.num_classes, paddings=self.paddings)
+        codes=jnp.array(self.codes),
+        num_classes=self.num_classes,
+        paddings=jnp.array(self.paddings),
+    )
 
     self.assertAlmostEqual(
-        pplx, self.pplx, delta=1e-3, msg='PPLX is not the same')
+        np.array(pplx),
+        np.array(self.pplx),
+        delta=1e-3,
+        msg='PPLX is not the same',
+    )
     self.assertAlmostEqual(
-        entropy, self.entropy, delta=1e-3, msg='Entropy is not the same')
+        np.array(entropy),
+        np.array(self.entropy),
+        delta=1e-3,
+        msg='Entropy is not the same',
+    )
 
 
 if __name__ == '__main__':
   absltest.main()
```

## praxis/layers/repeats_test.py

```diff
@@ -74,30 +74,31 @@
         ),
     )
     self.create_variable(
         'step',
         WeightHParams(shape=[], dtype=jnp.int32, init=WeightInit.Constant(0)),
         trainable=False)
 
-  def __call__(self, inputs):
+  def __call__(self, inputs, paddings=None):
+    del paddings  # Unused.
     self.add_summary('inputs_mean', jnp.mean(inputs))
-    self.add_aux_loss('z_loss', 1, 0.5)
+    self.add_aux_loss('z_loss', jnp.array(1), 0.5)
     self.update_var('step', self.get_var('step') + 1)
     out = jnp.einsum('...y,yz->...z', inputs, self.theta.w)
     self.sow(INTERMEDIATES, 'before_sigmoid', out)
     out = jax.nn.sigmoid(out)
     return out
 
 
 class FeedForwardWithPadding(FeedForward):
   """Feedforward layer with paddings in input."""
 
-  def __call__(self, inputs, paddings):
+  def __call__(self, inputs, paddings=None):
     self.add_summary('inputs_mean', jnp.mean(inputs))
-    self.add_aux_loss('z_loss', 1, 0.5)
+    self.add_aux_loss('z_loss', jnp.array(1), 0.5)
     self.update_var('step', self.get_var('step') + 1)
     out = jnp.einsum('...y,yz->...z', inputs, self.theta.w)
     out = jax.nn.sigmoid(out)
     out = py_utils.apply_padding(out, paddings)
     return out, paddings
```

## praxis/layers/sharding.py

```diff
@@ -16,14 +16,15 @@
 """Sharding utilities."""
 
 import math
 import re
 from typing import Sequence
 
 import jax
+from jax.interpreters import pxla
 from praxis import base_layer
 from praxis import py_utils
 
 DimSharding = str | Sequence[str] | None
 Sharding = Sequence[DimSharding] | None
 
 
@@ -121,15 +122,15 @@
   """Annotates x on one dim while other dims are unconstrained."""
   perm = '?' * dim + 'd' + '?' * (x.ndim - dim - 1)
   return shard(x, (s,), 'd->' + perm)
 
 
 def num_shards_on_dim(dim_sharding: DimSharding) -> int:
   """Returns the number of shards on one dimension in a sharding."""
-  mesh = jax.experimental.maps.thread_resources.env.physical_mesh
+  mesh = pxla.thread_resources.env.physical_mesh
   axis_sizes = dict(zip(mesh.axis_names, mesh.devices.shape))
 
   mapping = None
   if base_layer.JaxContext.has_context():
     mapping = base_layer.cur_jax_context().hparams.mesh_axes_transpose
 
   match dim_sharding:
```

## praxis/layers/transformer_models.py

```diff
@@ -923,20 +923,20 @@
 
     # [B, T, D]
     # During decoding for NGrammer, an extra prefix token is prepended to
     # compute the bi-gram representation for the token at time step t
     # when time_step > 0.
     ngrammer_prefix = self.get_decode_state('ngrammer_prefix')
     ngrammer_prefix_emb = self.get_decode_state('ngrammer_prefix_emb')
-    ngrammer_segment_pos_prefix = self.get_decode_state(
-        'ngrammer_prefix_segment_pos'
-    )
     input_ids = jnp.concatenate([ngrammer_prefix, input_ids], axis=1)
     input_emb = jnp.concatenate([ngrammer_prefix_emb, input_emb], axis=1)
     if segment_pos is not None:
+      ngrammer_segment_pos_prefix = self.get_decode_state(
+          'ngrammer_prefix_segment_pos'
+      )
       segment_pos = jnp.concatenate(
           [ngrammer_segment_pos_prefix, segment_pos], axis=1
       )
     # If the prefix is all 0's it corresponds to time step 0.
     input_emb = self.ngrammer(
         input_ids, input_emb, segment_pos=segment_pos, check_time_step_zero=True
     )
@@ -1800,50 +1800,48 @@
     self.add_summary('total_aux_loss', aux_loss)
     self.add_summary('total_aux_loss_weight', aux_loss_weight)
 
     # This is the loss to minimize.
     xent_output.total_loss = xent_output.avg_xent + xent_output.aux_loss
     return xent_output
 
-  def __call__(
+  def fprop_with_encoder_output(
       self,
-      inputs: JTensor,
+      encoder_output: JTensor,
       input_paddings: JTensor,
       targets: JTensor,
       target_paddings: JTensor,
       labels: NestedMap | None = None,
       input_segment_ids: JTensor | None = None,
       input_segment_pos: JTensor | None = None,
-      input_segment_mask: JTensor | None = None,
       target_segment_ids: JTensor | None = None,
       target_segment_pos: JTensor | None = None,
       target_segment_mask: JTensor | None = None,
       cross_segment_mask: JTensor | None = None,
       start_time_step: int = 0,
   ) -> NestedMap:
-    """Computes xent loss given the sequence model inputs.
+    """Computes xent loss given the encoder output.
 
     Args:
-      inputs: Input ids. An int32 JTensor of shape [B, S].
+      encoder_output: A JTensor of shape [B, S, D]. It's the output of the
+        encoder for the input.
       input_paddings: A 0/1 JTensor of shape [B, S] with 1 denoting padding
         corresponding to the input sequence.
       targets: Target ids. An int32 JTensor of shape [B, T].
       target_paddings: A 0/1 JTensor of shape [B, T] with 1 denoting padding
         corresponding to the target sequence.
       labels: A `.NestedMap` containing the following fields: class_weights, a
         JTensor with shape [batch, seqlen] containing weights for each target
         word. class_ids, a JTensor with shape [B, T] of int32 dtype containing
         the target class labels. class_probabilities, a JTensor with shape [B,
         T, V] of float values indicating class-membership probabilities.
       input_segment_ids: A JTensor of shape [B,S]. The segment that each input
         token belongs to.
       input_segment_pos: A JTensor of shape [B, S]. The position of each input
         token within a segment.
-      input_segment_mask: A JTensor or shape [B, 1, S, S]. The segment mask for
-        packed input tokens.
       target_segment_ids: A JTensor of shape [B,T]. The segment that each target
         token belongs to.
       target_segment_pos: A JTensor of shape [B, T]. The position of each target
         token within a segment.
       target_segment_mask: A JTensor or shape [B, 1, T, T]. The segment mask for
         packed target tokens.
       cross_segment_mask: A JTensor or shape [B, 1, T, S]. The encoder-decoder
@@ -1855,22 +1853,14 @@
       Returns xent_output, where
       `xent_output` is a `.NestedMap` as defined by `SoftmaxLayer`'s return. In
       addition, per_sequence_xent is added which equal to the sum of xent loss
       for tokens in a sequence.
     """
     batch, target_seq_length = targets.shape[:2]
 
-    encoder_output = self.encode(
-        inputs,
-        input_paddings,
-        input_segment_ids,
-        input_segment_pos,
-        input_segment_mask,
-    )
-
     if self.decoder_embedding_tpl is not None:
       # Targets have separate embedding params.
       target_emb = self.decoder_embedding_lookup.emb_lookup(targets)
     else:
       # Embedding parameters are shared with targets and softmax.
       target_emb = self.softmax.emb_lookup(targets)
 
@@ -1930,14 +1920,91 @@
     )
 
     # Final layer norm for decoder.
     output = self.decoder_ln(output)
 
     return self.compute_loss(output, labels)
 
+  def __call__(
+      self,
+      inputs: JTensor,
+      input_paddings: JTensor,
+      targets: JTensor,
+      target_paddings: JTensor,
+      labels: NestedMap | None = None,
+      input_segment_ids: JTensor | None = None,
+      input_segment_pos: JTensor | None = None,
+      input_segment_mask: JTensor | None = None,
+      target_segment_ids: JTensor | None = None,
+      target_segment_pos: JTensor | None = None,
+      target_segment_mask: JTensor | None = None,
+      cross_segment_mask: JTensor | None = None,
+      start_time_step: int = 0,
+  ) -> NestedMap:
+    """Computes xent loss given the sequence model inputs.
+
+    Args:
+      inputs: Input ids. An int32 JTensor of shape [B, S].
+      input_paddings: A 0/1 JTensor of shape [B, S] with 1 denoting padding
+        corresponding to the input sequence.
+      targets: Target ids. An int32 JTensor of shape [B, T].
+      target_paddings: A 0/1 JTensor of shape [B, T] with 1 denoting padding
+        corresponding to the target sequence.
+      labels: A `.NestedMap` containing the following fields: class_weights, a
+        JTensor with shape [batch, seqlen] containing weights for each target
+        word. class_ids, a JTensor with shape [B, T] of int32 dtype containing
+        the target class labels. class_probabilities, a JTensor with shape [B,
+        T, V] of float values indicating class-membership probabilities.
+      input_segment_ids: A JTensor of shape [B,S]. The segment that each input
+        token belongs to.
+      input_segment_pos: A JTensor of shape [B, S]. The position of each input
+        token within a segment.
+      input_segment_mask: A JTensor or shape [B, 1, S, S]. The segment mask for
+        packed input tokens.
+      target_segment_ids: A JTensor of shape [B,T]. The segment that each target
+        token belongs to.
+      target_segment_pos: A JTensor of shape [B, T]. The position of each target
+        token within a segment.
+      target_segment_mask: A JTensor or shape [B, 1, T, T]. The segment mask for
+        packed target tokens.
+      cross_segment_mask: A JTensor or shape [B, 1, T, S]. The encoder-decoder
+        segment mask.
+      start_time_step: Decode extend_step start time step. When decoding after
+        prefix, start_time_step will be prefix_len - 1.
+
+    Returns:
+      Returns xent_output, where
+      `xent_output` is a `.NestedMap` as defined by `SoftmaxLayer`'s return. In
+      addition, per_sequence_xent is added which equal to the sum of xent loss
+      for tokens in a sequence.
+    """
+    batch, target_seq_length = targets.shape[:2]
+
+    encoder_output = self.encode(
+        inputs,
+        input_paddings,
+        input_segment_ids,
+        input_segment_pos,
+        input_segment_mask,
+    )
+    return self.fprop_with_encoder_output(
+        encoder_output=encoder_output,
+        input_paddings=input_paddings,
+        targets=targets,
+        target_paddings=target_paddings,
+        labels=labels,
+        input_segment_ids=input_segment_ids,
+        input_segment_pos=input_segment_pos,
+        target_segment_ids=target_segment_ids,
+        target_segment_pos=target_segment_pos,
+        target_segment_mask=target_segment_mask,
+        cross_segment_mask=cross_segment_mask,
+        start_time_step=start_time_step,
+    )
+
   def transform_decode_state(
       self, transform_fn: base_layer.DecodeStateTransformFn
   ) -> None:
     """Transforms all decode state variables based on transform_fn."""
     self.decoder.transform_decode_state(transform_fn)
 
   def init_states(
```

## praxis/layers/transformers_test.py

```diff
@@ -76,15 +76,15 @@
     tf_segment_mask = None
     attention_mask = attentions.convert_paddings_to_mask(paddings)
     if mask_self_attention:
       causal_mask = attentions.causal_mask(inputs)
       attention_mask = jnp.minimum(attention_mask, causal_mask)
     if packed_input:
       segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
       attention_mask = jnp.minimum(attention_mask, segment_mask)
       if mask_self_attention:
         tf_segment_mask = batch_major_attention.CausalSegmentMask(
             segment_ids, tf.float32)
       else:
         tf_segment_mask = batch_major_attention.SegmentMask(
             segment_ids, segment_ids)
@@ -105,15 +105,16 @@
       cross_paddings = jnp.asarray(npy_cross_paddings)
       cross_attention_mask = attentions.convert_paddings_to_mask(cross_paddings)
       tf_cross_paddings = tf.constant(npy_cross_paddings, dtype=tf.float32)
       if packed_input:
         source_segment_ids = np.random.randint(0, 3,
                                                [batch_size, cross_seq_len])
         cross_segment_mask = attentions.segment_mask(
-            segment_ids, source_segment_ids, dtype=np.float32)
+            segment_ids, source_segment_ids, dtype=np.float32
+        )  # pytype: disable=wrong-arg-types
         cross_attention_mask = jnp.minimum(cross_attention_mask,
                                            cross_segment_mask)
         tf_cross_segment_mask = batch_major_attention.SegmentMask(
             segment_ids, source_segment_ids)
 
     with base_layer.JaxContext.new_context():
       initial_vars = transformer_layer.init(
@@ -199,15 +200,15 @@
     paddings = jnp.asarray(npy_paddings)
     attention_mask = attentions.convert_paddings_to_mask(paddings)
     segment_mask = None
     causal_mask = attentions.causal_mask(inputs)
     attention_mask = jnp.minimum(causal_mask, attention_mask)
     if packed_input:
       segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
       attention_mask = jnp.minimum(attention_mask, segment_mask)
     cross_inputs = None
     cross_paddings = None
     cross_attention_mask = None
     if cross_attention:
       cross_seq_len = np.random.randint(10, 32)
       npy_cross_inputs = np.random.normal(
@@ -217,15 +218,16 @@
           0, 1, [batch_size, cross_seq_len]).astype('float32')
       cross_paddings = jnp.asarray(npy_cross_paddings)
       cross_attention_mask = attentions.convert_paddings_to_mask(cross_paddings)
       if packed_input:
         source_segment_ids = np.random.randint(0, 3,
                                                [batch_size, cross_seq_len])
         cross_segment_mask = attentions.segment_mask(
-            segment_ids, source_segment_ids, dtype=np.float32)
+            segment_ids, source_segment_ids, dtype=np.float32
+        )  # pytype: disable=wrong-arg-types
         cross_attention_mask = jnp.minimum(cross_attention_mask,
                                            cross_segment_mask)
 
     with base_layer.JaxContext.new_context():
       initial_vars = transformer_layer.init(
           prng_key,
           jnp.zeros_like(inputs),
@@ -301,29 +303,30 @@
                                      [batch_size, seq_len]).astype('float32')
     paddings = jnp.asarray(npy_paddings)
     attention_mask = attentions.convert_paddings_to_mask(paddings)
     causal_mask = attentions.causal_mask(inputs)
     attention_mask = jnp.minimum(causal_mask, attention_mask)
     if packed_input:
       segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
       attention_mask = jnp.minimum(attention_mask, segment_mask)
 
     cross_seq_len = np.random.randint(10, 32)
     npy_cross_inputs = np.random.normal(
         1.0, 0.5, [batch_size, cross_seq_len, p.input_dims]).astype('float32')
     cross_inputs = jnp.asarray(npy_cross_inputs)
     npy_cross_paddings = np.random.randint(
         0, 1, [batch_size, cross_seq_len]).astype('float32')
     cross_paddings = jnp.asarray(npy_cross_paddings)
     cross_attention_mask = attentions.convert_paddings_to_mask(cross_paddings)
     if packed_input:
       source_segment_ids = np.random.randint(0, 3, [batch_size, cross_seq_len])
       cross_segment_mask = attentions.segment_mask(
-          segment_ids, source_segment_ids, dtype=np.float32)
+          segment_ids, source_segment_ids, dtype=np.float32
+      )  # pytype: disable=wrong-arg-types
       cross_attention_mask = jnp.minimum(cross_attention_mask,
                                          cross_segment_mask)
     with base_layer.JaxContext.new_context():
       transformer_layer = instantiate(p)
       prng_key = jax.random.PRNGKey(seed=123)
       initial_vars = transformer_layer.init(
           prng_key,
@@ -479,15 +482,15 @@
         1.0, 0.5, [batch_size, seq_len, block_p.model_dims]).astype('float32')
     inputs = jnp.asarray(npy_inputs)
     npy_paddings = np.random.randint(0, 1,
                                      [batch_size, seq_len]).astype('float32')
     paddings = jnp.asarray(npy_paddings)
     segment_mask = None
     segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-    segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+    segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
 
     cross_inputs = None
     cross_paddings = None
     cross_segment_mask = None
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
@@ -603,15 +606,15 @@
     inputs = jnp.asarray(npy_inputs)
     npy_paddings = np.random.randint(0, 1,
                                      [batch_size, seq_len]).astype('float32')
     paddings = jnp.asarray(npy_paddings)
     segment_mask = None
     if packed_input:
       segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
 
     cross_inputs = None
     cross_paddings = None
     cross_segment_mask = None
     if use_cross_attention:
       cross_seq_len = np.random.randint(10, 64)
       npy_cross_inputs = np.random.normal(
@@ -621,15 +624,16 @@
       npy_cross_paddings = np.random.randint(
           0, 1, [batch_size, cross_seq_len]).astype('float32')
       cross_paddings = jnp.asarray(npy_cross_paddings)
       if packed_input:
         source_segment_ids = np.random.randint(0, 3,
                                                [batch_size, cross_seq_len])
         cross_segment_mask = attentions.segment_mask(
-            segment_ids, source_segment_ids, dtype=np.float32)
+            segment_ids, source_segment_ids, dtype=np.float32
+        )  # pytype: disable=wrong-arg-types
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       prng_key, subkey = jax.random.split(prng_key)
       block_initial_vars = transformer_block.init(
           {
               PARAMS: prng_key,
@@ -704,15 +708,15 @@
     npy_paddings = np.random.randint(0, 1,
                                      [batch_size, seq_len]).astype('float32')
     paddings = jnp.asarray(npy_paddings)
     segment_mask = None
     tf_segment_mask = None
     if packed_input:
       segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
       if mask_self_attention:
         tf_segment_mask = batch_major_attention.CausalSegmentMask(
             segment_ids, tf.float32)
       else:
         tf_segment_mask = batch_major_attention.SegmentMask(
             segment_ids, segment_ids)
 
@@ -732,15 +736,16 @@
           0, 1, [batch_size, cross_seq_len]).astype('float32')
       cross_paddings = jnp.asarray(npy_cross_paddings)
       tf_cross_paddings = tf.constant(npy_cross_paddings, dtype=tf.float32)
       if packed_input:
         source_segment_ids = np.random.randint(0, 3,
                                                [batch_size, cross_seq_len])
         cross_segment_mask = attentions.segment_mask(
-            segment_ids, source_segment_ids, dtype=np.float32)
+            segment_ids, source_segment_ids, dtype=np.float32
+        )  # pytype: disable=wrong-arg-types
         tf_cross_segment_mask = batch_major_attention.SegmentMask(
             segment_ids, source_segment_ids)
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       initial_vars = stacked_transformer_layer.init(
           prng_key,
@@ -842,15 +847,15 @@
         'float32'
     )
     paddings = jnp.asarray(npy_paddings)
     segment_ids = None
     segment_mask = None
     if packed_input:
       segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
 
     cross_inputs = None
     cross_paddings = None
     cross_segment_mask = None
     if use_cross_attention:
       cross_seq_len = np.random.randint(10, 64)
       npy_cross_inputs = np.random.normal(
@@ -863,15 +868,15 @@
       cross_paddings = jnp.asarray(npy_cross_paddings)
       if packed_input:
         source_segment_ids = np.random.randint(
             0, 3, [batch_size, cross_seq_len]
         )
         cross_segment_mask = attentions.segment_mask(
             segment_ids, source_segment_ids, dtype=np.float32
-        )
+        )  # pytype: disable=wrong-arg-types
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       initial_vars = ref_layer.init(
           prng_key,
           inputs,
           paddings,
@@ -931,15 +936,15 @@
     inputs = jnp.asarray(npy_inputs)
     npy_paddings = np.random.randint(0, 1,
                                      [batch_size, seq_len]).astype('float32')
     paddings = jnp.asarray(npy_paddings)
     segment_mask = None
     if packed_input:
       segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
 
     cross_inputs = None
     cross_paddings = None
     cross_segment_mask = None
     if use_cross_attention:
       cross_seq_len = np.random.randint(10, 64)
       npy_cross_inputs = np.random.normal(
@@ -948,15 +953,16 @@
       npy_cross_paddings = np.random.randint(
           0, 1, [batch_size, cross_seq_len]).astype('float32')
       cross_paddings = jnp.asarray(npy_cross_paddings)
       if packed_input:
         source_segment_ids = np.random.randint(0, 3,
                                                [batch_size, cross_seq_len])
         cross_segment_mask = attentions.segment_mask(
-            segment_ids, source_segment_ids, dtype=np.float32)
+            segment_ids, source_segment_ids, dtype=np.float32
+        )  # pytype: disable=wrong-arg-types
 
     with base_layer.JaxContext.new_context():
       stacked_transformer_layer = instantiate(p1)
       repeated_transformer_layer = instantiate(p2)
       prng_key = jax.random.PRNGKey(seed=123)
       initial_vars = stacked_transformer_layer.init(
           prng_key,
@@ -1144,15 +1150,15 @@
           npy_paddings.astype('int32'))
       segment_ids = np.cumsum(segment_ids, axis=1)
       segment_pos = np.zeros_like(segment_ids)
       for b in range(batch_size):
         for t in range(1, seq_len):
           if (segment_ids[b, t] == segment_ids[b, t - 1]):
             segment_pos[b, t] = segment_pos[b, t - 1] + 1
-      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+      segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
       segment_pos = jnp.asarray(segment_pos)
 
     cross_inputs = None
     cross_paddings = None
     cross_segment_mask = None
     if cross_attention:
       cross_seq_len = np.random.randint(10, 32)
@@ -1162,22 +1168,25 @@
       npy_cross_paddings = np.random.randint(
           0, 1, [batch_size, cross_seq_len]).astype('float32')
       cross_paddings = jnp.asarray(npy_cross_paddings)
       if packed_input:
         source_segment_ids = np.random.randint(0, 3,
                                                [batch_size, cross_seq_len])
         cross_segment_mask = attentions.segment_mask(
-            segment_ids, source_segment_ids, dtype=np.float32)
+            segment_ids, source_segment_ids, dtype=np.float32
+        )  # pytype: disable=wrong-arg-types
 
     if use_custom_attention:
       custom_attention_mask = jnp.asarray(
           np.random.randint(0, 2, [batch_size, 1, seq_len, seq_len]).astype(
               'float32'
           )
-      ) * py_utils.get_large_negative_number('float32')
+      ) * py_utils.get_large_negative_number(
+          np.float32
+      )  # pytype: disable=wrong-arg-types
       custom_attention_mask = jnp.minimum(
           custom_attention_mask, attentions.convert_paddings_to_mask(paddings)
       )
       fprop_paddings = jnp.zeros_like(paddings, dtype=jnp.float32)
       custom_attention_mask = jnp.minimum(
           custom_attention_mask, attentions.causal_mask(inputs)
       )
@@ -1435,15 +1444,15 @@
     npy_paddings = np.random.randint(0, 1,
                                      [batch_size, seq_len]).astype('float32')
     paddings = jnp.asarray(npy_paddings)
     attention_mask = attentions.convert_paddings_to_mask(paddings)
     causal_mask = attentions.causal_mask(inputs)
     attention_mask = jnp.minimum(attention_mask, causal_mask)
     segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-    segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+    segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
     attention_mask = jnp.minimum(attention_mask, segment_mask)
 
     transformer_layer = instantiate(p)
     prng_key = jax.random.PRNGKey(seed=123)
     cross_inputs = None
     cross_attention_mask = None
     with base_layer.JaxContext.new_context():
@@ -1519,15 +1528,15 @@
     npy_paddings = np.random.randint(0, 1,
                                      [batch_size, seq_len]).astype('float32')
     paddings = jnp.asarray(npy_paddings)
     attention_mask = attentions.convert_paddings_to_mask(paddings)
     causal_mask = attentions.causal_mask(inputs)
     attention_mask = jnp.minimum(attention_mask, causal_mask)
     segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-    segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+    segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
     attention_mask = jnp.minimum(attention_mask, segment_mask)
 
     transformer_layer = instantiate(p)
     prng_key = jax.random.PRNGKey(seed=123)
     cross_seq_len = np.random.randint(10, 32)
     npy_cross_inputs = np.random.normal(
         1.0, 0.5, [batch_size, cross_seq_len, p.input_dims]
@@ -1537,15 +1546,15 @@
         0, 1, [batch_size, cross_seq_len]
     ).astype('float32')
     cross_paddings = jnp.asarray(npy_cross_paddings)
     cross_attention_mask = attentions.convert_paddings_to_mask(cross_paddings)
     source_segment_ids = np.random.randint(0, 3, [batch_size, cross_seq_len])
     cross_segment_mask = attentions.segment_mask(
         segment_ids, source_segment_ids, dtype=np.float32
-    )
+    )  # pytype: disable=wrong-arg-types
     cross_attention_mask = jnp.minimum(cross_attention_mask, cross_segment_mask)
     with base_layer.JaxContext.new_context():
       initial_vars = transformer_layer.init(
           prng_key,
           jnp.zeros_like(inputs),
           jnp.ones_like(paddings),
           attention_mask=attention_mask,
@@ -1622,15 +1631,15 @@
     npy_paddings = np.random.randint(0, 1,
                                      [batch_size, seq_len]).astype('float32')
     paddings = jnp.asarray(npy_paddings)
     attention_mask = attentions.convert_paddings_to_mask(paddings)
     causal_mask = attentions.causal_mask(inputs)
     attention_mask = jnp.minimum(attention_mask, causal_mask)
     segment_ids = np.random.randint(0, 3, [batch_size, seq_len])
-    segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)
+    segment_mask = attentions.segment_mask(segment_ids, dtype=np.float32)  # pytype: disable=wrong-arg-types
     attention_mask = jnp.minimum(attention_mask, segment_mask)
 
     if use_relative_bias:
       segment_pos = np.random.randint(0, seq_len,
                                       [batch_size, seq_len]).astype('int32')
       segment_pos = jnp.asarray(segment_pos)
     else:
```

## praxis/layers/chain/chain_test.py

```diff
@@ -42,15 +42,17 @@
 
 
 class _Scale(BaseLayer):
   """Simple scaling layer, useful for testing."""
 
   factor: float = 1.0
 
-  def __call__(self, inputs: JTensor, paddings: JTensor) -> JTensor:
+  def __call__(
+      self, inputs: JTensor, paddings: JTensor
+  ) -> tuple[JTensor, JTensor]:
     scaled = inputs * self.factor
     return scaled, paddings
 
 
 def _scale(factor: float, **kwargs) -> Config[_Scale]:
   """`Config(Scale)`; scales the input by a factor (mostly testing)."""
   return Config(
```

## praxis/layers/quantization/__init__.py

```diff
@@ -12,23 +12,25 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Exposes the public layer functionalities."""
 
 from praxis.layers.quantization.attentions import AttentionProjection
+from praxis.layers.quantization.attentions import AttentionProjectionLoRA
 from praxis.layers.quantization.attentions import CombinedQKVProjectionLayer
 from praxis.layers.quantization.attentions import DotProductAttention
 from praxis.layers.quantization.conformers import DotProductAttentionWithContext
 from praxis.layers.quantization.convolutions import Conv2D
 from praxis.layers.quantization.einsum import Einsum
 from praxis.layers.quantization.embedding_softmax import Embedding
 from praxis.layers.quantization.embedding_softmax import NClassMajorSharedEmbeddingSoftmax
 from praxis.layers.quantization.embedding_softmax import SharedEmbeddingSoftmax
 from praxis.layers.quantization.linears import Linear
+from praxis.layers.quantization.linears import LinearLoRA
 from praxis.layers.quantization.multi_query_attention import OneHeadedAttentionProjection
 from praxis.layers.quantization.ngrammer import Ngrammer
 from praxis.layers.quantization.ngrammer import VQNgrammer
 from praxis.layers.quantization.operations import einsum
 from praxis.layers.quantization.overflow_check import AttentionProjectionOverflowCheck, CombinedQKVProjectionLayerOverflowCheck, FeedForwardOverflowCheck, OneHeadedAttentionProjectionOverflowCheck
 from praxis.layers.quantization.searchable import SearchableAttentionProjection
 from praxis.layers.quantization.searchable import SearchableCombinedQKVProjectionLayer
```

## praxis/layers/quantization/attentions.py

```diff
@@ -12,80 +12,136 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Quantized and optionally sparsified Attention Layers."""
 
 import copy
+import math
 import string
 from typing import Any, Sequence
 
+from absl import logging
+import fiddle as fdl
 import jax
 from jax import numpy as jnp
 from jax.ad_checkpoint import checkpoint_name
 from praxis import base_layer
+from praxis import pax_fiddle
 from praxis import py_utils
 from praxis import pytypes
 from praxis.layers import attentions
+from praxis.layers import normalizations
 from praxis.layers.quantization import operations
 from praxis.layers.quantization import quantization_hparams
 from praxis.layers.quantization import quantizer
 from praxis.layers.quantization import utils
 from praxis.layers.quantization.sparsity import sparsifier
 
+
 QuantizationParams = quantization_hparams.QuantizationParams
 QuantizationMode = quantization_hparams.QuantizationMode
 QuantizationType = quantization_hparams.QuantizationType
 WeightInit = base_layer.WeightInit
 WeightHParams = base_layer.WeightHParams
-instance_field = base_layer.instance_field
 JTensor = pytypes.JTensor
 NestedJTensor = pytypes.NestedJTensor
+LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]
+
+instance_field = base_layer.instance_field
+template_field = base_layer.template_field
 
 
 class AttentionProjection(  # pytype: disable=signature-mismatch
     attentions.AttentionProjection,
     quantizer.QuantizationLayer,
     sparsifier.SparsityBaseLayer,
 ):
   """Layer that optionally computes quantized multi heads projection.
 
   This layer is expected to be used within DotProductAttention.
   """
 
   _PACK_4BIT_DIM = 0
 
-  def setup(self) -> None:
+  def _sub_channel_block_size(self) -> int:
+    """Determine sub-channels' block_size if it was given."""
+    if (
+        self.quantization is not None
+        and self.quantization.weight_params is not None
+        and self.quantization.weight_params.block_size > 0
+    ):
+      return self.quantization.weight_params.block_size
+    return 0
+
+  def _get_eqn(self) -> str:
+    # This matches the equation logic in __call__ for weights.
+    if self.is_output_projection:
+      if self.use_nhd_shape:
+        eqn = 'ANH,NHD->AD'
+      else:
+        eqn = 'ANH,DNH->AD'
+    else:
+      eqn = 'AD,DNH->ANH'
+    return eqn
+
+  def _get_weight_scale_shape(self, block_size, use_block_size):
     wp = self.weight_split_dims_mapping
     has_sharding = self.mesh_shape is not None and wp.wt is not None
     if self.attention_combine_dims:
       assert not self.use_bias
       hd_shape = [self.num_heads * self.dim_per_head]
     else:
       hd_shape = [self.num_heads, self.dim_per_head]
-
     if self.attention_combine_dims and has_sharding:
       if len(wp.wt) == 3:
         h_sharding = ()
         for axes in (wp.wt[0], wp.wt[1]):
           if isinstance(axes, (str, int)):
             h_sharding += (axes,)
           elif axes is not None:
             h_sharding += axes
         wt = [h_sharding, wp.wt[2]]
-        assert len(wt) == 2
+        assert len(self.wt) == 2
     else:
       wt = wp.wt
-    pc_shape = [self.input_dim] + hd_shape
+
     if self.is_output_projection and self.use_nhd_shape:
-      pc_shape = hd_shape + [self.input_dim]
+      weight_shape = hd_shape + [self.input_dim]
+    else:
+      weight_shape = [self.input_dim] + hd_shape
+
+    scale_shape = [self.input_dim] if self.is_output_projection else hd_shape
+
+    if block_size > 0 and use_block_size:
+      eqn = self._get_eqn()
+      new_contract_dims = operations.eqn_to_weight_contract_dims(eqn)
+      weight_shape, new_contract_dims = operations.get_sub_channel_shape(
+          list(weight_shape), block_size, new_contract_dims
+      )
+      scale_shape = operations.get_scale_shape(weight_shape, new_contract_dims)
+    return weight_shape, scale_shape, wt
+
+  def setup(self) -> None:
+    wp = self.weight_split_dims_mapping
+    has_sharding = self.mesh_shape is not None and wp.wt is not None
+    block_size = self._sub_channel_block_size()
+    use_block_size = (
+        self.quantization is not None
+        and self.quantization.mode == QuantizationMode.INFERENCE
+    )
+    weight_shape, scale_shape, self.wt = self._get_weight_scale_shape(
+        block_size, use_block_size
+    )
+
     pc = WeightHParams(
-        shape=pc_shape, mesh_shape=self.mesh_shape, tensor_split_dims_mapping=wt
+        shape=weight_shape,
+        mesh_shape=self.mesh_shape,
+        tensor_split_dims_mapping=self.wt,
     )
-    scale_shape = [self.input_dim] if self.is_output_projection else hd_shape
     self.set_up_weights(
         weight_name='w',
         weight_params=pc,
         scale_shape=scale_shape,
     )
     self.create_sparsity_variables('w', pc, scale_shape=scale_shape)
 
@@ -155,14 +211,32 @@
       assert (
           shape[-1] == self.input_dim
       ), f'Expecting shape[-1] == p.input_dim, {shape[-1]} != {self.input_dim}'
       batch_eqn = eqn_sym[: (rank - 1)] if rank else '...'
       eqn = f'{batch_eqn}D,DNH->{batch_eqn}NH'
 
     w = self.sparsifiy(theta.w, inputs=inputs, name='w')  # sparsify weight.
+
+    # Sub-channel
+    block_size = self._sub_channel_block_size()
+    if (
+        self.quantization is not None
+        and (self.quantization.mode == QuantizationMode.INFERENCE)
+        and block_size > 0
+    ):
+      # TODO(rybakov) Add sub channel support.
+      logging.warning(
+          'Weights are reshaped back to original shape. '
+          'Sub channel can be used only for weights '
+          'materialization.'
+      )
+      # Weight shape without sub channels.
+      weight_shape, _, _ = self._get_weight_scale_shape(0, False)
+      w = jnp.reshape(w, weight_shape)
+
     ret = self.quantized_einsum(
         eqn=eqn,
         x=inputs,
         w=w,
         reshape=pc_shape,
     )
 
@@ -214,44 +288,48 @@
     Returns:
       a map from names to quantized weights.
     """
     assert self.quantization is not None, (
         'quantize_weight is called during serving for quantized model, please'
         ' set quantized config for the model.'
     )
-    eqn = ''
-    # This matches the equation logic in __call__ for weights.
-    if self.is_output_projection:
-      if self.use_nhd_shape:
-        eqn = 'ANH,NHD->AD'
-      else:
-        eqn = 'ANH,DNH->AD'
-    else:
-      eqn = 'AD,DNH->ANH'
+
+    eqn = self._get_eqn()
+    w = self.theta.w
+
+    block_size = self._sub_channel_block_size()
+    new_contract_dims = operations.eqn_to_weight_contract_dims(eqn)
+
+    if block_size > 0:
+      # Weight shape with sub channels.
+      weight_shape, _, _ = self._get_weight_scale_shape(block_size, True)
+      w = jnp.reshape(w, weight_shape)
 
     # TODO(jihwanlee): Handle the cases for FQ and static quantization.
     if self.quantization.quantization_type in [
         QuantizationType.PTQ,
         QuantizationType.FQ_VN,
     ]:
       q_w, q_s, zp = operations.reduce_einsum_weight_precision(
-          eqn,
-          self.theta.w,
+          None,
+          w,
           calculation_dtype=self.dtype,
           need_gradient=False,
           bits=self.quantization.weight_params.precision,
           optimization_on_bound=False,
           percentile=self.quantization.weight_params.clipping_coeff,
           use_symmetric=self.quantization.weight_params.use_symmetric,
+          quant_method=self.quantization.weight_params.quant_method,
+          contract_dims=new_contract_dims,
       )
     elif self.quantization.quantization_type == QuantizationType.AQT:
       dimension_numbers, _ = utils.einsum_eqn_to_dimension_numbers(eqn)
       weight_contract_dims = dimension_numbers[0][1]
       q_w, q_s, zp = self.weight_quantizer.quantize(
-          self.theta.w,
+          w,
           weight_contract_dims,
           squeeze_scale=True,
           quantized_dtype=self.quantization.weight_params.dtype,
       )
     else:
       raise ValueError(
           f'Unsupported quantization_type {self.quantization.quantization_type}'
@@ -273,14 +351,149 @@
       zp_name = 'w' + base_layer.QUANTIZED_ZP_NAME_POSTFIX
       ret_params[zp_name] = zp
     if self.use_bias:
       ret_params['b'] = self.theta.b
     return {base_layer.PARAMS: ret_params}
 
 
+class AttentionProjectionLoRA(AttentionProjection):
+  """AttentionProjection with residual LoRA.
+
+  Attributes:
+    lora_rank: Rank of LoRA.
+    init_method: LoRA weights initialization method.
+    norm_tpl: Normalization layer type.
+    norm_order: Where to apply normalization layer:
+      * None: no normalization. * 'pre': normalization before LoRA projections.
+        * 'mid': normalization between LoRA projections. * 'post': normalization
+        after LoRA projections.
+    max_reduction: If True, it will select the reduction dim with the max size
+      and use it as LoRA dim, else it will use multiple reduction dims for LoRA
+      dims. It is applied only for a case when there are several reduction dims.
+  """
+
+  lora_rank: int = 0
+  init_method: str = 'one_zero'
+  norm_tpl: LayerTpl = template_field(normalizations.LayerNorm)
+  norm_order: str | None = None
+  max_reduction: bool = True
+
+  def setup(self):
+    super().setup()
+    weight_shape = self.theta.w.shape
+    if self.is_output_projection:
+      if self.use_nhd_shape:
+        eqn = '...NH,NHD->...D'
+        norm_input_dims = weight_shape[1]
+        norm_output_dims = weight_shape[2]
+        total_size_right = weight_shape[2]
+        total_size_left = max(weight_shape[0], weight_shape[1]) * self.lora_rank
+      else:
+        eqn = '...NH,DNH->...D'
+        norm_input_dims = weight_shape[2]
+        norm_output_dims = weight_shape[0]
+        total_size_right = weight_shape[0]
+        total_size_left = max(weight_shape[1], weight_shape[2]) * self.lora_rank
+    else:
+      eqn = '...D,DNH->...NH'
+      norm_input_dims = weight_shape[0]
+      norm_output_dims = weight_shape[1]
+      total_size_right = weight_shape[1] * weight_shape[2]
+      total_size_left = self.lora_rank
+
+    if self.init_method == 'one_zero':
+      w_left_scale = 1.0
+      w_right_scale = 0.0
+    elif self.init_method == 'output_dim':
+      w_left_scale = 1.0 / math.sqrt(total_size_left)
+      w_right_scale = 1.0 / math.sqrt(total_size_right)
+    else:
+      raise ValueError(f'Unrecognized init_method: {self.init_method}')
+
+    (
+        self.eqn_left,
+        self.eqn_right,
+        left_shape,
+        right_shape,
+        eqn_left_ind,
+        eqn_right_ind,
+    ) = utils.get_lora_shape_and_eqn(
+        weight_shape, self.lora_rank, eqn, max_reduction=self.max_reduction
+    )
+
+    self.create_variable(
+        'w_left',
+        WeightHParams(
+            shape=left_shape,
+            mesh_shape=self.mesh_shape,
+            init=WeightInit.Gaussian(w_left_scale),
+            tensor_split_dims_mapping=utils.get_left_weight_split_dims_mapping(
+                self.wt, eqn_left_ind
+            ),
+        ),
+    )
+
+    self.create_variable(
+        'w_right',
+        WeightHParams(
+            shape=right_shape,
+            mesh_shape=self.mesh_shape,
+            init=WeightInit.Constant(w_right_scale)
+            if w_right_scale == 0.0
+            else WeightInit.Gaussian(w_left_scale),
+            tensor_split_dims_mapping=utils.get_right_weight_split_dims_mapping(
+                self.wt, eqn_right_ind
+            ),
+        ),
+    )
+
+    if self.norm_order is not None:
+      norm_tpl = self.norm_tpl.clone()
+      if fdl.get_callable(norm_tpl) not in {
+          normalizations.BatchNorm,
+          normalizations.GroupNorm,
+          normalizations.LayerNorm,
+      }:
+        raise NotImplementedError(
+            '%s is not supported' % fdl.get_callable(norm_tpl)
+        )
+      if self.norm_order == 'pre':
+        norm_tpl.dim = norm_input_dims
+      elif self.norm_order == 'mid':
+        norm_tpl.dim = self.lora_rank
+      elif self.norm_order == 'post':
+        norm_tpl.dim = norm_output_dims
+      else:
+        raise ValueError(f'Unrecognized norm_order: {self.norm_order}')
+
+      self.create_child('norm', norm_tpl)
+
+  def __call__(
+      self,
+      inputs: JTensor,
+  ) -> JTensor:
+    """Computes the multi headed projection for inputs."""
+    inputs = self._cast_to_fprop_dtype(inputs)
+    out = super().__call__(inputs)
+
+    if self.lora_rank:
+      lora_output = inputs
+      if self.norm_order == 'pre':
+        lora_output = self.norm(lora_output)
+      lora_output = jnp.einsum(self.eqn_left, lora_output, self.theta.w_left)
+      if self.norm_order == 'mid':
+        lora_output = self.norm(lora_output)
+      lora_output = jnp.einsum(self.eqn_right, lora_output, self.theta.w_right)
+      if self.norm_order == 'post':
+        lora_output = self.norm(lora_output)
+      out += lora_output
+
+    return out
+
+
 class CombinedQKVProjectionLayer(  # pytype: disable=signature-mismatch
     attentions.CombinedQKVProjectionLayer,
     quantizer.QuantizationLayer,
     sparsifier.SparsityBaseLayer,
 ):
   """Layer that computes quantized QKV projection with a combined weight.
 
@@ -552,14 +765,15 @@
           theta.w,
           calculation_dtype=self.dtype,
           need_gradient=False,
           bits=self.quantization.weight_params.precision,
           optimization_on_bound=False,
           percentile=self.quantization.weight_params.clipping_coeff,
           use_symmetric=self.quantization.weight_params.use_symmetric,
+          quant_method=self.quantization.weight_params.quant_method,
       )
     elif self.quantization.quantization_type == QuantizationType.AQT:
       dimension_numbers, _ = utils.einsum_eqn_to_dimension_numbers(eqn)
       weight_contract_dims = dimension_numbers[0][1]
       q_w, q_s, zp = self.weight_quantizer.quantize(
           self.theta.w,
           weight_contract_dims,
```

## praxis/layers/quantization/attentions_test.py

```diff
@@ -262,15 +262,15 @@
       p.dim_per_head = 16
       p.is_output_projection = True
 
     attn_f = instantiate(p_f)
     attn_q = instantiate(p_q)
     inputs = np.random.normal(1.5, 2.0, [2, 16, 16]).astype(np.float32)
     prng_key = jax.random.PRNGKey(seed=123)
-    prng_key, init_key = jax.random.split(prng_key)
+    _, init_key = jax.random.split(prng_key)
     initial_vars_f = attn_f.init(init_key, inputs)
     initial_vars_q = attn_q.init(init_key, inputs)
     assert_var_stats_close(
         py_utils.NestedMap.FromNestedDict(initial_vars_f['params']),
         py_utils.NestedMap.FromNestedDict(initial_vars_q['params']), self)
 
   # test case copied from test_combine_qkv_with_attention_combine_dims.
@@ -331,15 +331,15 @@
         name='mh_quant',
         quantization=QuantizationParams(
             quantization_type=QuantizationType.AQT,
             mode=QuantizationMode.TRAINING,
             # Test using 23 bits to minimize the quantization error and test
             # for numerical correctness.
             act_params=quantization_hparams.ActQuantizationParams(precision=23),
-            weight_params=None,
+            weight_params=quantization_hparams.WeightQuantizationParams(),
         ),
     )
     for p in [atten_f_p, atten_q_p]:
       p.input_dim = mdl_dim
       p.hidden_dim = hidden_dim
       p.num_heads = num_heads
       p.dim_per_head = 16 if use_rotary_position_emb else None
@@ -349,15 +349,15 @@
       p.dconv_kernel_size = dconv_kernel_size
       p.use_rotary_position_emb = use_rotary_position_emb
       p.zero_fully_masked = zero_fully_masked
     atten_f = instantiate(atten_f_p)
     atten_q = instantiate(atten_q_p)
 
     prng_key = jax.random.PRNGKey(seed=123)
-    prng_key, init_key = jax.random.split(prng_key)
+    _, init_key = jax.random.split(prng_key)
     target_batch_size = 3
     source_max_length = 16
     target_max_length = 16
     query_vec = np.random.normal(
         size=[target_batch_size, source_max_length, mdl_dim]
     ).astype(np.float32)
     key_vec = query_vec
@@ -479,16 +479,16 @@
 
     with base_layer.JaxContext.new_context():
       prng_key = jax.random.PRNGKey(seed=123)
       initial_vars = layer.init(prng_key, inputs)
       res, _ = layer.apply(
           initial_vars, mutable=[], method=layer.quantize_weight)
 
-    self.assertEqual(len(res), 1)
-    self.assertEqual(len(res[base_layer.PARAMS]), 3 if use_bias else 2)
+    self.assertLen(res, 1)
+    self.assertLen(res[base_layer.PARAMS], 3 if use_bias else 2)
     self.assertEqual(res[base_layer.PARAMS]['w'].shape, (2, 5, 16))
     self.assertEqual(res[base_layer.PARAMS]['w_quantized_scale'].shape, (16,))
     if use_bias:
       self.assertEqual(res[base_layer.PARAMS]['b'].shape, (16,))
 
     pspec, _ = layer.apply(
         initial_vars, mutable=[], method=layer.quantized_partition_specs
@@ -506,14 +506,58 @@
           meta=jax.sharding.PartitionSpec(None)
       )
     expected_pspec = {
         'params': expected_pspec_params,
     }
     self.assertEqual(pspec, expected_pspec)
 
+  @parameterized.parameters(0, 2)
+  def test_quantize_attention_projection_sub_channel(self, block_size):
+    p = pax_fiddle.Config(
+        qattentions.AttentionProjection,
+        name='_attn_proj_q',
+        mesh_axis_names=['replica', 'mdl', 'data'],
+        weight_split_dims_mapping=base_layer.BaseLayer.WeightSharding(
+            wt=['mdl', 'data']
+        ),
+        quantization=QuantizationParams(
+            quantization_type=QuantizationType.PTQ,
+            mode=QuantizationMode.TRAINING,
+            weight_params=quantization_hparams.WeightQuantizationParams(
+                block_size=block_size,
+            ),
+        ),
+        use_bias=True,
+    )
+    p.input_dim = 32
+    p.num_heads = 4
+    p.dim_per_head = 8
+    p.is_output_projection = True
+    p.use_nhd_shape = True
+    layer = instantiate(p)
+    inputs = np.random.normal(1.5, 2.0, [8, 4, 8]).astype(np.float32)
+
+    with base_layer.JaxContext.new_context():
+      prng_key = jax.random.PRNGKey(seed=123)
+      initial_vars = layer.init(prng_key, inputs)
+      res, _ = layer.apply(
+          initial_vars, mutable=[], method=layer.quantize_weight
+      )
+
+    self.assertEqual(res[base_layer.PARAMS]['w'].dtype, jnp.int8)
+
+    if block_size == 0:
+      self.assertEqual(res[base_layer.PARAMS]['w'].shape, (4, 8, 32))
+      self.assertEqual(res[base_layer.PARAMS]['w_quantized_scale'].shape, (32,))
+    elif block_size == 2:
+      self.assertEqual(res[base_layer.PARAMS]['w'].shape, (4, 4, 2, 32))
+      self.assertEqual(
+          res[base_layer.PARAMS]['w_quantized_scale'].shape, (2, 32)
+      )
+
   @parameterized.product(
       quantization_type=[QuantizationType.PTQ, QuantizationType.AQT],
       use_symmetric=[True, False],
       use_bias=[True, False],
   )
   def test_quantize_attention_qkv(
       self, quantization_type, use_symmetric, use_bias
@@ -546,15 +590,15 @@
       initial_vars = layer.init(prng_key, inputs)
       res, _ = layer.apply(
           initial_vars, mutable=[], method=layer.quantize_weight)
       pspec, _ = layer.apply(
           initial_vars, mutable=[], method=layer.quantized_partition_specs
       )
 
-    self.assertEqual(len(res), 1)
+    self.assertLen(res, 1)
 
     shapes = jax.tree_map(lambda x: x.shape, res)
     types = jax.tree_map(lambda x: x.dtype, res)
 
     expected_shape = {
         base_layer.PARAMS: {'w': (3, 5, 6, 2), 'w_quantized_scale': (3, 6, 2)}
     }
@@ -589,9 +633,60 @@
       )
 
     self.assertEqual(shapes, expected_shape)
     self.assertEqual(types, expected_types)
     self.assertEqual(pspec, expected_pspec)
 
 
+class AttentionProjectionLoRATest(test_utils.TestCase):
+  """Quantize attention."""
+
+  def setUp(self):
+    super().setUp()
+    np.random.seed(123456)
+
+  @parameterized.parameters(
+      (None, False),
+      (None, True),
+      ('pre', False),
+      ('mid', False),
+      ('post', False),
+  )
+  def test_attention_projection_lora(self, norm_order, max_reduction):
+    p = pax_fiddle.Config(
+        qattentions.AttentionProjectionLoRA,
+        name='_attn_proj_q',
+        mesh_axis_names=['replica', 'mdl', 'data'],
+        weight_split_dims_mapping=base_layer.BaseLayer.WeightSharding(
+            wt=['mdl', 'data']
+        ),
+        quantization=QuantizationParams(
+            mode=QuantizationMode.TRAINING,
+        ),
+    )
+    p.input_dim = 16
+    p.num_heads = 2
+    p.dim_per_head = 5
+    p.lora_rank = 2
+    p.is_output_projection = True
+    p.use_nhd_shape = True
+    p.max_reduction = max_reduction
+    p.norm_order = norm_order
+
+    layer = instantiate(p)
+    inputs = np.random.normal(1.5, 2.0, [5, 2, 5]).astype(np.float32)
+
+    with base_layer.JaxContext.new_context():
+      prng_key = jax.random.PRNGKey(seed=123)
+      initial_vars = layer.init(prng_key, inputs)
+      outputs = layer.apply(initial_vars, inputs)
+
+    self.assertEqual(outputs.shape, (5, 16))
+    if max_reduction:
+      self.assertEqual(initial_vars['params']['w_left'].shape, (5, 2))
+    else:
+      self.assertEqual(initial_vars['params']['w_left'].shape, (2, 5, 2, 2))
+    self.assertEqual(initial_vars['params']['w_right'].shape, (2, 2, 16))
+
+
 if __name__ == '__main__':
   absltest.main()
```

## praxis/layers/quantization/conformers_test.py

```diff
@@ -51,15 +51,15 @@
         name='atten_q',
         quantization=QuantizationParams(
             quantization_type=QuantizationType.AQT,
             mode=QuantizationMode.TRAINING,
             # Test using 23 bits to minimize the quantization error and test
             # for numerical correctness.
             act_params=quantization_hparams.ActQuantizationParams(precision=23),
-            weight_params=None,
+            weight_params=quantization_hparams.WeightQuantizationParams(),
         ),
     )
     for p in [atten_f_p, atten_q_p]:
       p.input_dim = input_dim
       p.hidden_dim = 16
       p.left_context = 3
       p.right_context = 5
```

## praxis/layers/quantization/linears.py

```diff
@@ -12,34 +12,41 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Quantized and optionally sparsified Linear Layers."""
 
 import copy
+import math
 from typing import Any, Sequence, Tuple
 
+import fiddle as fdl
 from jax import numpy as jnp
 from praxis import base_layer
+from praxis import pax_fiddle
 from praxis import pytypes
 from praxis.layers import linears
+from praxis.layers import normalizations
 from praxis.layers.quantization import operations
 from praxis.layers.quantization import quantization_hparams
 from praxis.layers.quantization import quantizer
 from praxis.layers.quantization import utils
 from praxis.layers.quantization.sparsity import sparsifier
 
 QuantizationMode = quantization_hparams.QuantizationMode
 QuantizationType = quantization_hparams.QuantizationType
 QuantizationParams = quantization_hparams.QuantizationParams
 WeightHParams = base_layer.WeightHParams
-instance_field = base_layer.instance_field
 JTensor = pytypes.JTensor
 NestedJTensor = pytypes.NestedJTensor
 WeightInit = base_layer.WeightInit
+LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]
+
+instance_field = base_layer.instance_field
+template_field = base_layer.template_field
 
 
 class Linear(  # pytype: disable=signature-mismatch
     linears.Linear, quantizer.QuantizationLayer, sparsifier.SparsityBaseLayer
 ):
   """Quantized and low-rank Linear layer without bias.
 
@@ -53,15 +60,15 @@
   _PACK_4BIT_DIM = 0
 
   def _get_sub_channel_shape(
       self, shape: Sequence[int], block_size: int, contract_dim: int
   ) -> Sequence[int]:
     """Converts a shape's contract dim into sub-channel and block_size.
 
-    For activation, by => bsz
+    For activation, by => bsc
     For weight, yz => scz
 
     Args:
       shape: Tensor shape.
       block_size: Block size, it defines number of sub-channels.
       contract_dim: Contraction dim.
 
@@ -102,15 +109,15 @@
     scale_shape = [self.output_dims]
     block_size = self._sub_channel_block_size()
     if using_sub_channel:
       weight_shape = self._get_sub_channel_shape(weight_shape, block_size, 0)
       scale_shape = [weight_shape[0], weight_shape[2]]
       if wp.wt is not None:
         weight_sharding = wp.wt.copy()
-        weight_sharding.insert(1, -1)
+        weight_sharding.insert(1, None)
         scale_sharding = wp.wt.copy()
       else:
         weight_sharding = None
         scale_sharding = None
     else:
       weight_sharding = wp.wt
       if wp.wt is not None and len(wp.wt) > 1:
@@ -352,14 +359,15 @@
           w = w.astype(calculation_dtype)
         q_w, q_s, zp = operations.reduce_precision(
             w,
             contract_dims,
             bits=self.quantization.weight_params.precision,
             percentile=self.quantization.weight_params.clipping_coeff,
             use_symmetric=self.quantization.weight_params.use_symmetric,
+            quant_method=self.quantization.weight_params.quant_method,
         )
         q_s = jnp.squeeze(q_s)
         if zp is not None:
           zp = jnp.squeeze(zp)
     # Internal quantization type support.
     elif self.quantization.quantization_type == QuantizationType.AQT:
       if self._do_static_activation_quantization():
@@ -385,7 +393,162 @@
       )
 
     if self.quantization.weight_params.use_symmetric:
       return {base_layer.PARAMS: {'w': q_w, scale_name: q_s}}
     else:
       zp_name = 'w' + base_layer.QUANTIZED_ZP_NAME_POSTFIX
       return {base_layer.PARAMS: {'w': q_w, scale_name: q_s, zp_name: zp}}
+
+
+class LinearLoRA(Linear):
+  """Linear layer with residual LoRA.
+
+  Attributes:
+    lora_rank: Rank of LoRA.
+    init_method: LoRA weights initialization method.
+    norm_tpl: Normalization layer type.
+    norm_order: Where to apply normalization layer:
+      * None: no normalization. * 'pre': normalization before LoRA projections.
+        * 'mid': normalization between LoRA projections. * 'post': normalization
+        after LoRA projections.
+  """
+
+  lora_rank: int = 0
+  init_method: str = 'one_zero'
+  norm_tpl: LayerTpl = template_field(normalizations.LayerNorm)
+  norm_order: str | None = None
+
+  def setup(self):
+    super().setup()
+
+    eqn = '...y,yz->...z'
+    weight_shape = [self.input_dims, self.output_dims]
+    total_size_right = self.output_dims
+    total_size_left = self.lora_rank
+
+    if self.init_method == 'one_zero':
+      w_left_scale = 1.0
+      w_right_scale = 0.0
+    elif self.init_method == 'output_dim':
+      w_left_scale = 1.0 / math.sqrt(total_size_left)
+      w_right_scale = 1.0 / math.sqrt(total_size_right)
+    else:
+      raise ValueError(f'Unrecognized init_method: {self.init_method}')
+
+    (
+        self.eqn_left,
+        self.eqn_right,
+        left_shape,
+        right_shape,
+        eqn_left_ind,
+        eqn_right_ind,
+    ) = utils.get_lora_shape_and_eqn(weight_shape, self.lora_rank, eqn)
+
+    wp = self.weight_split_dims_mapping
+    self.create_variable(
+        'w_left',
+        WeightHParams(
+            shape=left_shape,
+            mesh_shape=self.mesh_shape,
+            tensor_split_dims_mapping=utils.get_left_weight_split_dims_mapping(
+                wp, eqn_left_ind
+            ),
+            init=WeightInit.Gaussian(w_left_scale),
+        ),
+    )
+    self.create_variable(
+        'w_right',
+        WeightHParams(
+            shape=right_shape,
+            mesh_shape=self.mesh_shape,
+            tensor_split_dims_mapping=utils.get_right_weight_split_dims_mapping(
+                wp, eqn_right_ind
+            ),
+            init=WeightInit.Constant(w_right_scale)
+            if w_right_scale == 0.0
+            else WeightInit.Gaussian(w_left_scale),
+        ),
+    )
+
+    if self.norm_order is not None:
+      norm_tpl = self.norm_tpl.clone()
+      if fdl.get_callable(norm_tpl) not in {
+          normalizations.BatchNorm,
+          normalizations.GroupNorm,
+          normalizations.LayerNorm,
+      }:
+        raise NotImplementedError(
+            '%s is not supported' % fdl.get_callable(norm_tpl)
+        )
+      if self.norm_order == 'pre':
+        norm_tpl.dim = self.input_dims
+      elif self.norm_order == 'mid':
+        norm_tpl.dim = self.lora_rank
+      elif self.norm_order == 'post':
+        norm_tpl.dim = self.output_dims
+      else:
+        raise ValueError(f'Unrecognized norm_order: {self.norm_order}')
+
+      self.create_child('norm', norm_tpl)
+
+  def __call__(self, inputs: JTensor) -> JTensor:
+    """Apply projection to inputs.
+
+    Args:
+      inputs: The inputs JTensor.  Shaped [..., input_dims].
+
+    Returns:
+      Projected inputs.
+    """
+    ap = self.activation_split_dims_mapping
+    out = super().__call__(inputs)
+
+    if self.lora_rank:
+      lora_output = inputs
+      if self.norm_order == 'pre':
+        lora_output = self.norm(lora_output)
+      lora_output = jnp.einsum(self.eqn_left, lora_output, self.theta.w_left)
+      if self.norm_order == 'mid':
+        lora_output = self.norm(lora_output)
+      lora_output = jnp.einsum(self.eqn_right, lora_output, self.theta.w_right)
+      if self.norm_order == 'post':
+        lora_output = self.norm(lora_output)
+      out += lora_output
+
+    # Adjust sharding annotation during decoding.
+    ap_out = ap.out
+    if ap_out is not None and len(ap_out) == 3 and out.ndim == 2:
+      ap_out = [ap_out[0], ap_out[2]]
+    out = base_layer.maybe_shard(out, ap_out, self.mesh_axis_names)
+    return out
+
+
+class LinearActScaling(Linear):
+  """Linear layer with extra Activation Scaling."""
+
+  def setup(self):
+    super().setup()
+    self.create_variable(
+        'w_per_channel_act_max',
+        WeightHParams(
+            shape=[self.input_dims],
+            init=WeightInit.Constant(0.0),
+        ),
+    )
+
+  def __call__(self, inputs: JTensor) -> JTensor:
+    """Apply Activation Scaling to inputs.
+
+    Args:
+      inputs: The inputs JTensor.  Shaped [..., input_dims].
+
+    Returns:
+      Projected inputs.
+    """
+    ap = self.activation_split_dims_mapping
+    inputs = inputs * self.theta.w_per_channel_act_max
+    out = super().__call__(inputs)
+    ap_out = ap.out
+    if ap_out is not None and len(ap_out) == 3 and out.ndim == 2:
+      ap_out = [ap_out[0], ap_out[2]]
+    out = base_layer.maybe_shard(out, ap_out, self.mesh_axis_names)
+    return out
```

## praxis/layers/quantization/linears_test.py

```diff
@@ -734,16 +734,16 @@
       )
       inference_vars_sc, _ = training_sc.apply(
           training_vars, mutable=[], method=training_sc.quantize_weight
       )
       expected_output = training_sc.apply(training_vars, inputs)
       quantized_output_pc = inference_pc.apply(inference_vars_pc, inputs)
       quantized_output_sc = inference_sc.apply(inference_vars_sc, inputs)
-      distortion_pc = jnp.sum(jnp.square(quantized_output_pc - expected_output))
-      distortion_sc = jnp.sum(jnp.square(quantized_output_sc - expected_output))
+      distortion_pc = np.sum(jnp.square(quantized_output_pc - expected_output))
+      distortion_sc = np.sum(jnp.square(quantized_output_sc - expected_output))
       training_pspec, _ = training_sc.apply(
           training_vars,
           mutable=[],
           method=training_sc.quantized_partition_specs,
       )
       inference_pspec, _ = inference_sc.apply(
           inference_vars_sc,
@@ -772,9 +772,39 @@
               meta=jax.sharding.PartitionSpec('mdl', None)
           )
       )
     self.assertEqual(expected_pspec, training_pspec)
     self.assertEqual(expected_pspec, inference_pspec)
 
 
+class LinearLoRATest(test_utils.TestCase):
+
+  def setUp(self):
+    super().setUp()
+    np.random.seed(123456)
+
+  @parameterized.parameters(None, 'pre', 'mid', 'post')
+  def test_linear_lora(self, norm_order):
+    p = pax_fiddle.Config(
+        qlinears.LinearLoRA,
+        name='_linear',
+        input_dims=8,
+        output_dims=4,
+        lora_rank=2,
+        norm_order=norm_order,
+    )
+    linear = instantiate(p)
+    inputs = jnp.array(
+        [[1, 2, 3, 4, 5, 6, 7, 8], [9, 10, 11, 12, 13, 14, 15, 16]],
+        dtype=p.dtype,
+    )
+    with base_layer.JaxContext.new_context():
+      prng_key = jax.random.PRNGKey(seed=123)
+      initial_vars = linear.init(prng_key, inputs)
+      outputs = linear.apply(initial_vars, inputs)
+    self.assertEqual(outputs.shape, (2, 4))
+    self.assertEqual(initial_vars['params']['w_left'].shape, (8, 2))
+    self.assertEqual(initial_vars['params']['w_right'].shape, (2, 4))
+
+
 if __name__ == '__main__':
   absltest.main()
```

## praxis/layers/quantization/multi_query_attention.py

```diff
@@ -187,14 +187,15 @@
         q_w, q_s, zp = operations.reduce_einsum_weight_precision(
             eqn,
             theta.w,
             calculation_dtype=self.dtype,
             bits=self.quantization.weight_params.precision,
             percentile=self.quantization.weight_params.clipping_coeff,
             use_symmetric=self.quantization.weight_params.use_symmetric,
+            quant_method=self.quantization.weight_params.quant_method,
         )
     elif self.quantization.quantization_type == QuantizationType.AQT:
       dimension_numbers, _ = utils.einsum_eqn_to_dimension_numbers(eqn)
       weight_contract_dims = dimension_numbers[0][1]
       q_w, q_s, zp = self.weight_quantizer.quantize(
           theta.w,
           weight_contract_dims,
```

## praxis/layers/quantization/operations.py

```diff
@@ -230,52 +230,92 @@
       differentiable_dot_general_int,
       (lhs, rhs),
       (lhs_dot, rhs_dot))
   return y, y_tangent
 
 
 @jax.custom_vjp
-def custom_einsum(x: JTensor, w: JTensor, key: jax.Array) -> jnp.ndarray:
+def custom_einsum(
+    x: JTensor,
+    w: JTensor,
+    prng_key: jax.Array,
+    bits_fwd: int | None = 8,
+    bits_bwd: int | None = 8,
+) -> jnp.ndarray:
   return jnp.einsum('abc,cd->abd', x, w)
 
 
-def custom_einsum_fwd(x: JTensor, w: JTensor, key: jax.Array):
+def custom_einsum_fwd(
+    x: JTensor,
+    w: JTensor,
+    prng_key: jax.Array,
+    bits_fwd: int | None,
+    bits_bwd: int | None,
+):
   """Custom forward pass for custom_einsum."""
   # Currently support only abc,cd->abd
   # TODO(jianlijianli): make this more general.
   assert x.ndim == 3
   assert w.ndim == 2
   assert x.shape[2] == w.shape[0]
-  qx, sx, _ = reduce_precision(x, bits=8, contract_dims=[2])
-  qw, sw, _ = reduce_precision(w, bits=8, contract_dims=[0])
+  if bits_fwd is not None:
+    qx, sx, _ = reduce_precision(x, bits=8, contract_dims=[2])
+    qw, sw, _ = reduce_precision(w, bits=8, contract_dims=[0])
+  else:
+    qx, sx = x, None
+    qw, sw = w, None
+
   acc = jnp.einsum('abc,cd->abd', qx, qw, preferred_element_type=jnp.bfloat16)
-  res = jnp.multiply(sx, jnp.multiply(acc, sw))
-  return res, (qx, qw, sx, sw, key)
+
+  if bits_fwd is not None:
+    res = jnp.multiply(sx, jnp.multiply(acc, sw))
+  else:
+    res = acc
+
+  return res, (qx, qw, sx, sw, prng_key, bits_bwd)
 
 
 def custom_einsum_bwd(res: Any, g: Any):
   """Custom gradient for custom_einsum."""
-  qx, qw, sx, sw, key = res
-  g_with_sw = jnp.multiply(g, sw)
-  g_with_sx = jnp.multiply(g, sx)
-  qg_for_w, sg_for_w, _ = reduce_precision(
-      t=g_with_sw, bits=8, contract_dims=[2], random_rounding=True, key=key
-  )
-  qg_for_x, sg_for_x, _ = reduce_precision(
-      t=g_with_sx, bits=8, contract_dims=[0, 1], random_rounding=True, key=key
-  )
+  qx, qw, sx, sw, prng_key, bits_bwd = res
+  if bits_bwd is not None:
+    g_with_sw = jnp.multiply(g, sw)
+    g_with_sx = jnp.multiply(g, sx)
+    qg_for_w, sg_for_w, _ = reduce_precision(
+        t=g_with_sw,
+        bits=bits_bwd,
+        contract_dims=[2],
+        random_rounding=True,
+        key=prng_key,
+    )
+    qg_for_x, sg_for_x, _ = reduce_precision(
+        t=g_with_sx,
+        bits=bits_bwd,
+        contract_dims=[0, 1],
+        random_rounding=True,
+        key=prng_key,
+    )
+  else:
+    qg_for_w = g
+    qg_for_x = g
+
   gx = jnp.einsum(
       'abd,cd->abc', qg_for_w, qw, preferred_element_type=jnp.bfloat16
   )
   gw = jnp.einsum(
       'abc,abd->cd', qx, qg_for_x, preferred_element_type=jnp.bfloat16
   )
-  gx = jnp.multiply(gx, sg_for_w)
-  gw = jnp.multiply(gw, jnp.squeeze(sg_for_x))
-  return gx, gw, None
+
+  if bits_bwd is not None:
+    gx = jnp.multiply(gx, sg_for_w)
+    gw = jnp.multiply(gw, jnp.squeeze(sg_for_x))
+
+  # Custom VJP bwd rule must produce an output with the same container (pytree)
+  # structure as the args tuple of the primal function (custom_einsum).
+  return gx, gw, None, None, None
 
 
 custom_einsum.defvjp(custom_einsum_fwd, custom_einsum_bwd)
 
 
 def einsum(
     eqn: str,
@@ -407,14 +447,15 @@
     use_symmetric: bool = True,
     use_fp: bool = False,
     add_scale_eps: bool = False,
     per_channel: bool = False,
     random_rounding: bool = False,
     key: jax.Array | None = None,
     save_fp8_to_int8: bool = True,
+    quant_method: str = 'default',
 ) -> tuple[JTensor, JTensor, JTensor | None]:
   """Reduce the precision of a tensor.
 
   Generic for all tensors.
 
   Args:
     t: Input tensor.
@@ -430,79 +471,97 @@
     add_scale_eps: Add eps value or replace zero value by 1 to avoid division by
       zero.
     per_channel: use per-channel clipping optimization.
     random_rounding: round with uniform random.
     key: rng key for rounding.
     save_fp8_to_int8: If fp8 will be saved as int8. Only works when use_fp is
       true and should be removed eventually.
+    quant_method: Quantization method: * 'default' - extracts min and max for
+      quantization scale estimation. It is well applied for int8, in4, int2
+      quantization. * 'bin' - binarization, where scale is defined by mean|w|. *
+      'bin_norm' - binarization with weight normalization.
 
   Returns:
     A tuple of quantized tensor, quantization scale
       and quantization zero point (optional).
   """
-  min_value, max_value = get_min_max(bits, use_fp=use_fp)
 
-  if use_symmetric:
-    bound = jnp.max(jnp.abs(t), axis=contract_dims, keepdims=True)
-    scale_bound = max_value
+  if bits == 1 and quant_method in ['bin', 'bin_norm']:
+    if quant_method == 'bin_norm':
+      mean = jnp.mean(t, axis=contract_dims, keepdims=True)
+      t = t - mean
+
+    # Remove zeros, so that below jnp.sign return only 1, -1.
+    t = jnp.where(t == 0.0, 1e-6, t)
+    scale = jnp.mean(jnp.abs(t), axis=contract_dims, keepdims=True)
+
+    # Binarize, (conditioned that all zeros are removed above).
+    t = pass_through(t, jnp.sign)
+    return t, scale, None
   else:
-    t_max = jnp.max(t, axis=contract_dims, keepdims=True)
-    t_min = jnp.min(t, axis=contract_dims, keepdims=True)
-    bound = t_max - t_min
-    scale_bound = max_value - min_value
-
-  if isinstance(percentile, JTensor) or percentile < 1.0:
-    bound = jnp.multiply(bound, percentile)
-  elif optimization_on_bound:
-    bound = optimization.get_best_bound(
-        t, bound, min_value, max_value, p_value, per_channel=per_channel
-    )
+    min_value, max_value = get_min_max(bits, use_fp=use_fp)
+
+    if use_symmetric:
+      bound = jnp.max(jnp.abs(t), axis=contract_dims, keepdims=True)
+      scale_bound = max_value
+    else:
+      t_max = jnp.max(t, axis=contract_dims, keepdims=True)
+      t_min = jnp.min(t, axis=contract_dims, keepdims=True)
+      bound = t_max - t_min
+      scale_bound = max_value - min_value
+
+    if isinstance(percentile, JTensor) or percentile < 1.0:
+      bound = jnp.multiply(bound, percentile)
+    elif optimization_on_bound:
+      bound = optimization.get_best_bound(
+          t, bound, min_value, max_value, p_value, per_channel=per_channel
+      )
 
-  scale = bound / scale_bound
+    scale = bound / scale_bound
 
-  if add_scale_eps:
-    # Add epsilon to avoid divide-by-zero.
-    scale = scale + jnp.finfo(t.dtype).eps
-  else:
-    scale = jnp.where(scale == 0.0, 1.0, scale)
+    if add_scale_eps:
+      # Add epsilon to avoid divide-by-zero.
+      scale = scale + jnp.finfo(t.dtype).eps
+    else:
+      scale = jnp.where(scale == 0.0, 1.0, scale)
 
-  if use_symmetric:
-    zp = None
-    t = jnp.divide(t, scale)
-  else:
-    zp = min_value - t_min / scale
-    t = jnp.divide(t, scale) + zp
-    zp = jnp.multiply(scale, zp)
-
-  if use_fp:
-    # No need to round.
-    t = jnp.clip(t, min_value, max_value).astype(jnp.float8_e4m3fn)
-    # TODO(jianlijianli): refactor to remove this logic.
-    if save_fp8_to_int8:
-      # This is needed since fp8 cannot be saved.
-      t = jax.lax.bitcast_convert_type(t, new_dtype=jnp.int8)
+    if use_symmetric:
+      zp = None
+      t = jnp.divide(t, scale)
     else:
-      # This is needed since bf16 x fp8 is not allowed.
-      t = t.astype(jnp.bfloat16)
-  else:
-    if need_gradient:
-      t = pass_through(t, jnp.round)
-      t = jnp.clip(t, min_value, max_value)
+      zp = min_value - t_min / scale
+      t = jnp.divide(t, scale) + zp
+      zp = jnp.multiply(scale, zp)
+
+    if use_fp:
+      # No need to round.
+      t = jnp.clip(t, min_value, max_value).astype(jnp.float8_e4m3fn)
+      # TODO(jianlijianli): refactor to remove this logic.
+      if save_fp8_to_int8:
+        # This is needed since fp8 cannot be saved.
+        t = jax.lax.bitcast_convert_type(t, new_dtype=jnp.int8)
+      else:
+        # This is needed since bf16 x fp8 is not allowed.
+        t = t.astype(jnp.bfloat16)
     else:
-      if random_rounding:
-        t = t + jax.random.uniform(
-            key=key, shape=t.shape, minval=-0.5, maxval=0.5
+      if need_gradient:
+        t = pass_through(t, jnp.round)
+        t = jnp.clip(t, min_value, max_value)
+      else:
+        if random_rounding:
+          t = t + jax.random.uniform(
+              key=key, shape=t.shape, minval=-0.5, maxval=0.5
+          )
+        t = jnp.round(t)
+        container_dtype = (
+            jnp.int8 if bits <= 8 else jnp.int16 if bits <= 16 else jnp.int32
         )
-      t = jnp.round(t)
-      container_dtype = (
-          jnp.int8 if bits <= 8 else jnp.int16 if bits <= 16 else jnp.int32
-      )
-      t = jnp.clip(t, min_value, max_value).astype(container_dtype)
+        t = jnp.clip(t, min_value, max_value).astype(container_dtype)
 
-  return t, scale, zp
+    return t, scale, zp
 
 
 def eqn_to_weight_contract_dims(eqn: str) -> list[int]:
   segs = eqn.split('->')
   ins = segs[0].split(',')
   w, out = ins[1].replace('.', ''), segs[1].replace('.', '')
   return [i for i, val in enumerate(w) if val not in out]
@@ -512,23 +571,25 @@
   segs = eqn.split('->')
   ins = segs[0].split(',')
   act, out = ins[0].replace('.', ''), segs[1].replace('.', '')
   return [-(i + 1) for i, val in enumerate(reversed(act)) if val not in out]
 
 
 def reduce_einsum_weight_precision(
-    eqn: str,
+    eqn: str | None,
     t: JTensor,
     calculation_dtype: jnp.dtype = jnp.bfloat16,
     squeeze: bool = True,
     need_gradient: bool = False,
     bits: int = 8,
     optimization_on_bound: bool = False,
     percentile: float = 1.0,
     use_symmetric: bool = True,
+    quant_method: str = 'default',
+    contract_dims: Sequence[int] | None = None,
 ) -> tuple[JTensor, JTensor, JTensor | None]:
   """Reduce the precision of the weight of einsum.
 
   It uses per-channel quantization so einsum equation is passed in as well.
 
   Args:
     eqn: The equation for the einsum.
@@ -536,32 +597,42 @@
     calculation_dtype: The type for calculation.
     squeeze: If the output scale is squeezed.
     need_gradient: If gradient is needed out of this function.
     bits: Target number of bits.
     optimization_on_bound: If MAE bound optimizer is used.
     percentile: Percentile factor to apply on the min/max range.
     use_symmetric: If weights are quantized symmetrically.
+    quant_method: Quantization method.
+    contract_dims: Contraction dims. It can be used if eqn is not defined.
 
   Returns:
     A tuple of JTensors. The first one is the quantized weight and the second
     one is the scaling factor.
   """
-  contract_dims = eqn_to_weight_contract_dims(eqn)
+  assert not (
+      contract_dims is not None and eqn is not None
+  ), 'both contract_dims and eqn can not be defined'
+
+  if eqn is not None:
+    contract_dims = eqn_to_weight_contract_dims(eqn)
+  else:
+    assert contract_dims, 'contract_dims must be defined if eqn is None'
 
   if t.dtype != calculation_dtype:
     t = t.astype(calculation_dtype)
 
   t, scale, zp = reduce_precision(
       t,
       contract_dims,
       need_gradient,
       bits,
       optimization_on_bound,
       percentile=percentile,
       use_symmetric=use_symmetric,
+      quant_method=quant_method,
   )
   if squeeze:
     scale = jnp.squeeze(scale)
     if zp is not None:
       zp = jnp.squeeze(zp)
   return t, scale, zp
 
@@ -578,36 +649,66 @@
 
   max_fp16 = jnp.finfo(jnp.float16).max  # min = -max for jnp.f16
   should_clip = jnp.abs(t) > max_fp16
   t = jax.lax.select(should_clip, _clip(t), _skip(t))
   return t
 
 
+def get_scale_shape(
+    weight_shape: Sequence[int], contract_dims: Sequence[int]
+) -> Sequence[int]:
+  """Gets scaler shape from weight_shape and contract_dims.
+
+  Args:
+    weight_shape: Weights shape.
+    contract_dims: List of contraction dims.
+
+  Returns:
+    A scale shape.
+  """
+  return [
+      dim_size
+      for i, dim_size in enumerate(weight_shape)
+      if i not in contract_dims
+  ]
+
+
 def get_sub_channel_shape(
     shape: Sequence[int],
     block_size: int,
     contract_dims: Sequence[int],
     insert_sub_channel: bool = True,
+    error_on_misaligned_shape: bool = False,
 ) -> tuple[Sequence[int], Sequence[int]]:
   """Converts a shape's contract dim into sub-channel and block_size.
 
+  It can be useful for reducing quantization error in post training quantization
+  or quantization aware training, shown in https://arxiv.org/pdf/2305.16619.pdf.
+
   Args:
     shape: Tensor shape.
     block_size: Block size, it defines number of sub-channels.
     contract_dims: List of contraction dims.
     insert_sub_channel: If True it will insert new dim for sub channel, else it
       will use existing feature dim.
+    error_on_misaligned_shape: If True it will raise an error for size not
+      aligned with block_size. By default it is False. It allows to apply sub
+      channel on layers with aligned shape and ignore layers with non aligned
+      shape, so it will not block an experiment.
 
   Returns:
     A tuple of new shape with new contract_dims.
   """
 
   new_contract_dims = list(contract_dims)
   sub_channel_shape = list(shape)
 
+  if block_size <= 0:
+    return sub_channel_shape, new_contract_dims
+
   contract_shape = [shape[i] for i in new_contract_dims]
   # Index of dim in new_contract_dims, which corresponds to max dim among
   # contraction dims of input shape.
   max_contract_dim_ind = np.argmax(contract_shape)
 
   contract_dim = new_contract_dims[max_contract_dim_ind]
   if block_size >= shape[contract_dim]:
@@ -617,18 +718,27 @@
         str(shape),
         str(contract_dims),
     )
     return sub_channel_shape, new_contract_dims
 
   sub_channels, rem = divmod(shape[contract_dim], block_size)
   if rem > 0:
-    raise ValueError(
-        f'block_size {block_size} must fully divide shape: {shape}'
-        f'with contract dims: {contract_dims}'
-    )
+    if error_on_misaligned_shape:
+      raise ValueError(
+          f'block_size {block_size} must fully divide shape: {shape}'
+          f'with contract dims: {contract_dims}'
+      )
+    else:
+      logging.warning(
+          'block_size %d must fully divide shape: %s; of contract dims: %s',
+          block_size,
+          str(shape),
+          str(contract_dims),
+      )
+      return sub_channel_shape, new_contract_dims
 
   if insert_sub_channel:
     sub_channel_shape[contract_dim] = block_size
     sub_channel_shape.insert(contract_dim, sub_channels)
 
     # Shift all contract dims starting from max_contract_dim_ind:
     for i in range(max_contract_dim_ind, len(new_contract_dims)):
@@ -645,28 +755,35 @@
     eqn: str,
     t: JTensor,
     bits: int = 8,
     calculation_dtype: jnp.dtype = jnp.float32,
     use_symmetric: bool = True,
     block_size: int = 0,
     use_fp: bool = False,
+    quant_method: str = 'default',
 ) -> JTensor:
   """Nudges weight of einsum with FakeQuant.
 
     It quantizes weights (using einsum equation for getting contract_dims) then
     de-quantizes it and returns it as an output.
   Args:
     eqn: The equation for the einsum. Determines the channel dimension.
     t: The weight tensor for the einsum.
     bits: Target number of bits.
     calculation_dtype: The type for calculation.
     use_symmetric: Use symmetric quantization for weights.
     block_size: Block wise quantization size. 0 to turn if off.
     use_fp: Use floating point.
 
+  quant_method: Quantization method:
+    * 'default' - extracts min and max for quantization scale estimation.
+      It is well applied for int8, in4, int2 quantization.
+    * 'bin' - binarization, where scale is defined by mean|w|.
+    * 'bin_norm' - binarization with weight normalization.
+
   Returns:
     The nudged weight tensor.
   """
   ret_type = t.dtype
   contract_dims = eqn_to_weight_contract_dims(eqn)
   original_shape = list(t.shape)
   if block_size > 0:
@@ -678,22 +795,26 @@
   if t.dtype != calculation_dtype:
     t = t.astype(calculation_dtype)
 
   if use_fp and bits == 4:
     # Short cut for fp4.
     fp4 = FP4()
     return fp4.nudge(t, contract_dims)
+
+  # Quantize input tensor.
   q, scale, zp = reduce_precision(
       t,
       contract_dims,
       need_gradient=True,
       bits=bits,
       optimization_on_bound=False,
       use_symmetric=use_symmetric,
+      quant_method=quant_method,
   )
+  # De-quantized q using scale and zp.
   res = jnp.multiply(q, scale)
   if zp is not None:
     res = jnp.subtract(res, zp)
   if block_size > 0:
     res = jnp.reshape(res, original_shape)
   return res.astype(ret_type)
```

## praxis/layers/quantization/operations_test.py

```diff
@@ -225,16 +225,16 @@
     q_o_channel_wise = operations.einsum(
         eqn,
         qx_channel_wise,
         qw,
         scale=sw,
         scale_act=sx_channel_wise,
     )
-    error_per_tensor = jnp.abs(o - q_o).mean()
-    error_per_token = jnp.abs(o - q_o_channel_wise).mean()
+    error_per_tensor = np.abs(o - q_o).mean()
+    error_per_token = np.abs(o - q_o_channel_wise).mean()
     self.assertLess(error_per_tensor, 0.01)
     self.assertLess(error_per_token, 0.01)
     self.assertLess(error_per_token, error_per_tensor)
 
   def test_min_max(self):
     self.assertEqual(operations.get_min_max(8), (-128, 127))
     self.assertEqual(operations.get_min_max(8, True), (0, 255))
@@ -300,15 +300,15 @@
 
   def setUp(self):
     super().setUp()
     np.random.seed(1234567)
 
   def test_precision_fp8(self):
 
-    inputs = np.array([[1.0, 2.0, 5.5, 2.9], [0.02, -0.01, 3.3, 4.0]])
+    inputs = jnp.array([[1.0, 2.0, 5.5, 2.9], [0.02, -0.01, 3.3, 4.0]])
     qx, scale, zp = operations.reduce_precision(
         inputs, contract_dims=[1], use_fp=True
     )
 
     self.assertAllClose(
         qx,
         np.array([[106, 114, 126, 119], [65, -71, 124, 126]], dtype=np.int8),
@@ -316,15 +316,15 @@
     self.assertAllClose(
         scale, np.array([[0.012277], [0.008929]], dtype=np.float32)
     )
     self.assertIsNone(zp)
 
   @parameterized.parameters(True, False)
   def test_precsion_int8_add_scale_eps(self, add_scale_eps):
-    inputs = np.array([[1.0, 2.0, 5.5, 2.9], [0.0, 0.0, 0.0, 0.0]])
+    inputs = jnp.array([[1.0, 2.0, 5.5, 2.9], [0.0, 0.0, 0.0, 0.0]])
     qx, scale, zp = operations.reduce_precision(
         inputs, contract_dims=[1], add_scale_eps=add_scale_eps
     )
     self.assertAllClose(
         qx, np.array([[23, 46, 127, 67], [0, 0, 0, 0]], dtype=np.int8)
     )
     if add_scale_eps:
@@ -334,15 +334,15 @@
     else:
       self.assertAllClose(
           scale, np.array([[0.04330709], [1.0]], dtype=np.float32)
       )
     self.assertIsNone(zp)
 
   def test_precision_random(self):
-    inputs = np.array([[1.0, 2.0, 5.5, 2.9], [0.02, -0.01, 3.3, 4.0]])
+    inputs = jnp.array([[1.0, 2.0, 5.5, 2.9], [0.02, -0.01, 3.3, 4.0]])
     qx, scale, zp = operations.reduce_precision(
         inputs,
         contract_dims=[1],
         random_rounding=True,
         key=jax.random.PRNGKey(0),
     )
 
@@ -351,14 +351,84 @@
         np.array([[23, 46, 127, 67], [0, 0, 104, 127]], dtype=np.int8),
     )
     self.assertAllClose(
         scale, np.array([[0.04330709], [0.03149606]], dtype=np.float32)
     )
     self.assertIsNone(zp)
 
+  def test_binarization(self):
+
+    inputs = jnp.array([[1.0, -2.0, 5.5, 0.0], [0.02, -0.01, 3.3, 0.0]])
+    qx, scale, zp = operations.reduce_precision(
+        inputs, contract_dims=[1], bits=1, quant_method='bin'
+    )
+
+    # There must be no zeros.
+    self.assertFalse(np.any(qx == 0.0))
+
+    # Output array has only 1, -1
+    self.assertArraysEqual(
+        qx,
+        np.array(
+            [[1.0, -1.0, 1.0, 1.0], [1.0, -1.0, 1.0, 1.0]], dtype=np.float32
+        ),
+    )
+    self.assertAllClose(
+        scale, np.array([[2.1250002], [0.8325002]], dtype=np.float32)
+    )
+
+    self.assertIsNone(zp)
+
+    qx, scale, zp = operations.reduce_precision(
+        inputs, contract_dims=[1], bits=1, quant_method='bin_norm'
+    )
+    # There must be no zeros.
+    self.assertFalse(np.any(qx == 0.0))
+
+    # Output array has only 1, -1
+    self.assertArraysEqual(
+        qx,
+        np.array(
+            [[-1.0, -1.0, 1.0, -1.0], [-1.0, -1.0, 1.0, -1.0]], dtype=np.float32
+        ),
+    )
+    self.assertAllClose(
+        scale, np.array([[2.1875], [1.2362499]], dtype=np.float32)
+    )
+
+    self.assertIsNone(zp)
+
+    # Binarization with 'default' symmetric approach does not work: returns 0s
+    qx, _, _ = operations.reduce_precision(
+        inputs,
+        contract_dims=[1],
+        bits=1,
+        quant_method='default',
+        use_symmetric=True,
+    )
+    self.assertArraysEqual(
+        qx,
+        np.array([[0, 0, 0, 0], [0, 0, 0, 0]], dtype=np.int8),
+    )
+
+    # Binarization with 'default' asymmetric approach can work.
+    qx, scale, zp = operations.reduce_precision(
+        inputs,
+        contract_dims=[1],
+        bits=1,
+        quant_method='default',
+        use_symmetric=False,
+    )
+    self.assertArraysEqual(
+        qx,
+        np.array([[-1, -1, 0, -1], [-1, -1, 0, -1]], dtype=np.int8),
+    )
+    self.assertAllClose(scale, np.array([[7.5], [3.31]], dtype=np.float32))
+    self.assertAllClose(zp, np.array([[-5.5], [-3.3]], dtype=np.float32))
+
 
 class ReducePrecisionEinsumTest(test_utils.TestCase):
 
   def setUp(self):
     super().setUp()
     np.random.seed(1234567)
 
@@ -700,15 +770,15 @@
     self.assertAllClose(expected, actual, rtol=0.01, atol=0.01)
 
 
 class QuantizationVNTest(test_utils.TestCase):
 
   def test_warmup_step(self):
 
-    wp = quantization_hparams.WeightQuantizationParams
+    wp = quantization_hparams.WeightQuantizationParams()
     wp.precision = 4
     wp.use_symmetric = True
     wp.vn_scale = 1. / 7
     wp.vn_start_step = 3
     wp.vn_noise_type = 'uniform'
     wp.vn_weight_norm_type = 'PerChannelLinf'
     wp.stop_scale_gradient = False
@@ -718,15 +788,15 @@
     eqn = 'ab,bc->ac'
 
     weight_ret = operations.fakequant_vn(
         eqn,
         weight,
         next_prng_key,
         wp,
-        step=wp.vn_start_step-1,
+        step=jnp.array(wp.vn_start_step - 1),
         do_eval=False,
         bits=wp.precision,
         use_symmetric=wp.use_symmetric,
     )
     self.assertArraysEqual(weight, weight_ret)
 
 
@@ -813,84 +883,112 @@
 
   def test_block_sub_channel_shape(self):
     shape = [4, 16, 32, 64]
     block_size = 8
     new_shape, new_contract_dims = operations.get_sub_channel_shape(
         shape, block_size, [2]
     )
+    new_scale_shape = operations.get_scale_shape(new_shape, new_contract_dims)
 
     # Contraction dim is replaced by two dims: remainder channels (4) and
     # new contraction dim with block_size (8)
     self.assertArraysEqual(new_shape, [4, 16, 4, 8, 64])
+    self.assertArraysEqual(new_scale_shape, [4, 16, 4, 64])
 
     # New contract dim points to dim with block_size
     self.assertArraysEqual(new_contract_dims, [3])
 
+  def test_block_sub_channel_shape_misaligned(self):
+    shape = [4, 16, 31, 64]
+    block_size = 8
+    new_shape, new_contract_dims = operations.get_sub_channel_shape(
+        shape, block_size, [2]
+    )
+    new_scale_shape = operations.get_scale_shape(new_shape, new_contract_dims)
+
+    # No changes in shape.
+    self.assertArraysEqual(new_shape, [4, 16, 31, 64])
+    self.assertArraysEqual(new_scale_shape, [4, 16, 64])
+    self.assertArraysEqual(new_contract_dims, [2])
+
+    with self.assertRaises(ValueError):
+      _, _ = operations.get_sub_channel_shape(
+          shape, block_size, [2], error_on_misaligned_shape=True
+      )
+
   def test_insert_block_sub_channel_shape_several_contract_dims(self):
 
     # Last contract dim is maximum.
     shape = [4, 16, 32, 64]
     block_size = 8
     new_shape, new_contract_dims = operations.get_sub_channel_shape(
         shape, block_size, [1, 2]
     )
+    new_scale_shape = operations.get_scale_shape(new_shape, new_contract_dims)
     self.assertArraysEqual(new_shape, [4, 16, 4, 8, 64])
+    self.assertArraysEqual(new_scale_shape, [4, 4, 64])
     self.assertArraysEqual(new_contract_dims, [1, 3])
 
     # If block_size is bigger than max reduction dim, new shape is not changed.
     block_size = 128
     new_shape, new_contract_dims = operations.get_sub_channel_shape(
         shape, block_size, [1, 2]
     )
+    new_scale_shape = operations.get_scale_shape(new_shape, new_contract_dims)
     self.assertArraysEqual(new_shape, shape)
+    self.assertArraysEqual(new_scale_shape, [4, 64])
     self.assertArraysEqual(new_contract_dims, [1, 2])
 
     # First contract dim is maximum.
     shape = [4, 32, 16, 64, 7]
     block_size = 8
     new_shape, new_contract_dims = operations.get_sub_channel_shape(
         shape, block_size, [1, 2, 4]
     )
+    new_scale_shape = operations.get_scale_shape(new_shape, new_contract_dims)
     self.assertArraysEqual(new_shape, [4, 4, 8, 16, 64, 7])
+    self.assertArraysEqual(new_scale_shape, [4, 4, 64])
     self.assertArraysEqual(new_contract_dims, [2, 3, 5])
 
     # Middle contract dim is maximum.
     shape = [4, 32, 16, 64, 7]
     block_size = 8
     new_shape, new_contract_dims = operations.get_sub_channel_shape(
         shape, block_size, [1, 2, 3, 4]
     )
+    new_scale_shape = operations.get_scale_shape(new_shape, new_contract_dims)
     self.assertArraysEqual(new_shape, [4, 32, 16, 8, 8, 7])
+    self.assertArraysEqual(new_scale_shape, [4, 8])
     self.assertArraysEqual(new_contract_dims, [1, 2, 4, 5])
 
   def test_inplace_block_sub_channel_shape_several_contract_dims(self):
 
     # Last contract dim is maximum.
     shape = [4, 16, 32, 64]
     block_size = 8
     new_shape, new_contract_dims = operations.get_sub_channel_shape(
-        shape, block_size, [1, 2], False
+        shape, block_size, [1, 2], insert_sub_channel=False
     )
     self.assertArraysEqual(new_shape, [16, 16, 8, 64])
     self.assertArraysEqual(new_contract_dims, [1, 2])
 
     # First contract dim is maximum.
     shape = [4, 32, 16, 64, 7]
     block_size = 8
     new_shape, new_contract_dims = operations.get_sub_channel_shape(
-        shape, block_size, [1, 2, 4], False
+        shape, block_size, [1, 2, 4], insert_sub_channel=False
     )
     self.assertArraysEqual(new_shape, [16, 8, 16, 64, 7])
     self.assertArraysEqual(new_contract_dims, [1, 2, 4])
 
     # Middle contract dim is maximum.
     shape = [4, 32, 16, 64, 7]
     block_size = 8
     new_shape, new_contract_dims = operations.get_sub_channel_shape(
-        shape, block_size, [1, 2, 3, 4], False
+        shape, block_size, [1, 2, 3, 4], insert_sub_channel=False
     )
     self.assertArraysEqual(new_shape, [32, 32, 16, 8, 7])
     self.assertArraysEqual(new_contract_dims, [1, 2, 3, 4])
 
 
 class ClipToFp16Test(test_utils.TestCase):
 
@@ -963,9 +1061,66 @@
     t = jnp.array([[-0.5, 2.1], [0.3, 12.5]], dtype=jnp.float32)
     self.assertAllClose(
         operations.fakequant_einsum(t=t, eqn='ab,bc->ac', bits=4, use_fp=True),
         t,
     )
 
 
+def _custom_loss(x, w, prng_key, bits_fwd, bits_bwd):
+  preds = operations.custom_einsum(x, w, prng_key, bits_fwd, bits_bwd)
+  return -jnp.sum(preds)
+
+
+def _loss(x, w, eqn):
+  preds = jnp.einsum(eqn, x, w, preferred_element_type=jnp.bfloat16)
+  return -jnp.sum(preds)
+
+
+class CustomEinsumTest(test_utils.TestCase):
+
+  def setUp(self):
+    super().setUp()
+    np.random.seed(0)
+
+  def test_custom_einsum_grad(self):
+    eqn = 'abc,cd->abd'
+    x = np.random.normal(size=[2, 2, 2])
+    w = np.random.normal(size=[2, 2])
+
+    prng_key = jax.random.PRNGKey(seed=0)
+    grads = jax.grad(_loss, argnums=[0, 1])(x, w, eqn)
+    custom_grads_float = jax.grad(_custom_loss, argnums=[0, 1])(
+        x, w, prng_key, bits_fwd=None, bits_bwd=None
+    )
+
+    # Validate that float custom gradient is the same with standard gradient.
+    self.assertLen(custom_grads_float, 2)
+    self.assertLen(grads, 2)
+    self.assertAllClose(custom_grads_float[0], grads[0].astype(jnp.bfloat16))
+    self.assertAllClose(custom_grads_float[1], grads[1].astype(jnp.bfloat16))
+
+    # Validate that gradient of int8 einsum is close to float one.
+    custom_grads_int8 = jax.grad(_custom_loss, argnums=[0, 1])(
+        x, w, prng_key, bits_fwd=8, bits_bwd=8
+    )
+    self.assertAllClose(custom_grads_int8[0], grads[0], rtol=0.008, atol=0.012)
+    self.assertAllClose(custom_grads_int8[1], grads[1], rtol=0.008, atol=0.012)
+
+  def test_custom_einsum_fwd_int8(self):
+    x = np.random.normal(size=[2, 2, 2])
+    w = np.random.normal(size=[2, 2])
+
+    prng_key = jax.random.PRNGKey(seed=0)
+    einsum_output = operations.custom_einsum(
+        x, w, prng_key, bits_fwd=8, bits_bwd=8
+    )
+    self.assertAllClose(
+        einsum_output,
+        np.array([
+            [[-0.1244434, 1.3062553], [0.22176202, 3.6607397]],
+            [[-0.3335378, -0.65441275], [-0.11986907, 0.16999012]],
+        ]),
+    )
+
+
 if __name__ == '__main__':
   absltest.main()
```

## praxis/layers/quantization/quantization_hparams.py

```diff
@@ -57,14 +57,39 @@
   TRAINING = 'training'
   MATERIALIZE = 'materialize'
   INFERENCE = 'inference'
   QT = 'qt'
   CALIB = 'calib'
 
 
+@enum.unique
+class TransformerLayer(str, enum.Enum):
+  """Transformer layer types used in quantization.
+
+  Users can use this enum to specify mixed precision quantization
+  for transformer models.
+
+  LINEAR: The FFN layer weight in a transformer.
+  LINEAR_ACT: The FFN layer activation in a transformer.
+  ATTENTION: The attention layer weight.
+  ATTENTION_ACT: The attention layer activation.
+  EMBEDDING_SOFTMAX: Embedding for the softmax layer.
+  EMBEDDING_SOFTMAX_ACT: Embedding activation for the softmax layer.
+  EMBEDDING_NGRAMMER: Embedding for the ngrammar layer.
+  """
+
+  LINEAR = 'linear'
+  LINEAR_ACT = 'linear_activation'
+  ATTENTION = 'attention'
+  ATTENTION_ACT = 'attention_activation'
+  EMBEDDING_SOFTMAX = 'embedding_softmax'
+  EMBEDDING_SOFTMAX_ACT = 'embedding_softmax_activation'
+  EMBEDDING_NGRAMMER = 'embedding_ngrammer'
+
+
 @dataclasses.dataclass
 class ActQuantizationParams:
   """Parameters for activation quantization.
 
   precision: The precision (number of bits) for activation quantization.
   unsigned_int_bounds: Whether or not to use unsigned_int_bounds.
   clipping_coeff: The coefficient to shrink the hard range for activation
@@ -144,14 +169,19 @@
     'PerChannelLinf'. Default value is 'PerChannelLinf' it is a
       standard scale normalization used by QAT.
   kurt_loss_weight: Weight for Kurtosis loss.
   kurt: Kurtosis target. By default it is 1.8 (uniform distribution).
     It is based on paper: "Robust Quantization: One Model to Rule Them All".
   block_size: block size for sub channel quantization. 0 to set it off. Defaults
     to off.
+  quant_method: Quantization method:
+    * 'default' - extracts min and max for quantization scale estimation.
+      It is well applied for int8, in4, int2 quantization.
+    * 'bin' - binarization, where scale is defined by mean|w|.
+    * 'bin_norm' - binarization with weight normalization.
   """
   precision: int = 8
   unsigned_int_bounds: bool = False
   clipping_coeff: float = 1.0
   stop_scale_gradient: bool = False
   min_clipping: float | None = None
   num_optimize_clipping: int | None = None
@@ -170,14 +200,15 @@
   vn_start_step: int = 0
   vn_noise_type: str = 'uniform'
   vn_weight_norm_type: str = 'PerChannelLinf'
   kurt_loss_weight: float | None = None
   kurt: float = 1.8
   block_size: int = 0
   # Internal quantization parameters.
+  quant_method: str = 'default'
 
 
 @dataclasses.dataclass
 class QuantizationParams:
   """Parameters for quantization.
 
   Attributes:
```

## praxis/layers/quantization/quantize.py

```diff
@@ -22,39 +22,63 @@
 class XYZModel():
   ...
 
 @for_transformer()
 class QuantizedXYZModel(XYZModel):
   pass
 
+# Mixed precision setup.
+@mixed_precision_for_transformer(
+    num_bits={
+        TransformerLayer.LINEAR: 4,
+        TransformerLayer.ATTENTION: 8,
+        TransformerLayer.EMBEDDING_SOFTMAX: 8,
+    },
+    sub_channel_blocksize={
+        TransformerLayer.LINEAR: 128,
+    },
+    dtype={
+        TransformerLayer.LINEAR: jnp.int4,
+    },
+    transposed_embedding_softmax=True,
+)
+class QuantizedXYZModel(XYZModel):
+  pass
+
+
 This creates a quantized model for the original XYZModel configuration by
 quantizing all transformer blocks.
-
 """
+
 import functools
-from typing import Sequence, Type, cast
+from typing import Dict, Sequence, Type, cast
 
+from absl import logging
 import fiddle as fdl
 from jax import numpy as jnp
 from praxis import base_layer
 from praxis import layers
 from praxis import pax_fiddle
 from praxis.layers import quantization
 from praxis.layers.quantization import quantization_hparams
 from praxis.layers.quantization import utils
 
 # Internal import for internal quantization hyper parameters.
+# Internal quantization helper.
 # Internal import for internal quantization long seq support.
+# Internal embedding implementation.
+
 
 LayerTpl = pax_fiddle.Config[base_layer.BaseLayer]
 QuantizationParams = quantization_hparams.QuantizationParams
 QuantizationType = quantization_hparams.QuantizationType
 QuantizationMode = quantization_hparams.QuantizationMode
 WeightQuantizationParams = quantization_hparams.WeightQuantizationParams
 ActQuantizationParams = quantization_hparams.ActQuantizationParams
+TransformerLayer = quantization_hparams.TransformerLayer
 
 
 def _quantize_embedding_softmax_layer_weights(
     layer_tpl: (
         pax_fiddle.Config[layers.TransformerLm]
         | pax_fiddle.Config[layers.TransformerEncoderDecoder]
     ),
@@ -62,51 +86,52 @@
     mode: QuantizationMode,
     weight_quantization_params: WeightQuantizationParams,
     act_quantization_params: ActQuantizationParams | None = None,
     transposed_embedding_softmax: bool = False,
     softmax_only: bool = True,
 ) -> None:
   """Rewrites Embedding HParam for weight only quantization."""
-  if transposed_embedding_softmax:
-    # Transposed embedding quantization.
-    # Replace softmax_tpl to quantized NClassMajorSharedEmbeddingSoftmax.
-    quant_embedding_softmax_tpl = pax_fiddle.Config(
-        quantization.NClassMajorSharedEmbeddingSoftmax,
-        quantization=QuantizationParams(
-            quantization_type=quantization_type,
-            mode=mode,
-            weight_params=weight_quantization_params,
-            act_params=act_quantization_params,
-        ),
-    )
-  else:
-    # Non-transposed embedding quantization.
-    # Replace softmax_tpl to quantized SharedEmbeddingSoftmax.
-    quant_embedding_softmax_tpl = pax_fiddle.Config(
-        quantization.SharedEmbeddingSoftmax,
-        quantization=QuantizationParams(
-            quantization_type=quantization_type,
-            mode=mode,
-            weight_params=weight_quantization_params,
-            act_params=act_quantization_params,
-        ),
-    )
+
+  quantization_params = QuantizationParams(
+      quantization_type=quantization_type,
+      mode=mode,
+      weight_params=weight_quantization_params,
+      act_params=act_quantization_params,
+  )
+
+  def _quantize_shared_embedding_softmax(embedding_softmax_tpl, name):
+    if transposed_embedding_softmax:
+      # Transposed embedding quantization.
+      # Replace softmax_tpl to quantized NClassMajorSharedEmbeddingSoftmax.
+      quant_embedding_softmax_tpl = pax_fiddle.Config(
+          quantization.NClassMajorSharedEmbeddingSoftmax,
+          quantization=quantization_params,
+      )
+    else:
+      # Non-transposed embedding quantization.
+      # Replace softmax_tpl to quantized SharedEmbeddingSoftmax.
+      quant_embedding_softmax_tpl = pax_fiddle.Config(
+          quantization.SharedEmbeddingSoftmax,
+          quantization=quantization_params,
+      )
+    new_embedding_softmax_tpl = quant_embedding_softmax_tpl.clone()
+    new_embedding_softmax_tpl.copy_fields_from(embedding_softmax_tpl)
+    setattr(layer_tpl, name, new_embedding_softmax_tpl)
 
   def _quantize(name: str):
     embedding_softmax_tpl = getattr(layer_tpl, name)
     if embedding_softmax_tpl is None:
       return
     if issubclass(embedding_softmax_tpl.cls, layers.SharedEmbeddingSoftmax):
-      new_embedding_softmax_tpl = quant_embedding_softmax_tpl.clone()
-      new_embedding_softmax_tpl.copy_fields_from(embedding_softmax_tpl)
-      setattr(layer_tpl, name, new_embedding_softmax_tpl)
+      _quantize_shared_embedding_softmax(embedding_softmax_tpl, name)
+ # Internal quantization support.
     else:
-      raise ValueError(
+      logging.info(
           f'layer_tpl.{name}.cls is : {embedding_softmax_tpl.cls}'
-          'but has to be layers.SharedEmbeddingSoftmax'
+          ' but has to be SharedSoftmaxEmbedding or MultiModalSoftmaxEmbedding.'
       )
 
   _quantize('softmax_tpl')
   if not softmax_only:
     if issubclass(layer_tpl.cls, layers.TransformerLm):
       _quantize('separate_embedding_tpl')
     if issubclass(layer_tpl.cls, layers.TransformerEncoderDecoder):
@@ -484,14 +509,116 @@
         return task_p
 
     return Wrapper
 
   return decorator
 
 
+def mixed_precision_for_transformer(
+    quantization_type: QuantizationType = QuantizationType.PTQ,
+    mode: QuantizationMode = QuantizationMode.INFERENCE,
+    num_bits: Dict[TransformerLayer, int] = {},
+    sub_channel_blocksize: Dict[TransformerLayer, int] = {},
+    use_symmetric: Dict[TransformerLayer, bool] = {},
+    dtype: Dict[TransformerLayer, jnp.dtype] = {},
+    *,
+    default_sub_channel_blocksize: int = 0,
+    default_use_symmetric: bool = True,
+    default_dtype: jnp.dtype = jnp.int8,
+    transposed_embedding_softmax: bool = False,
+    quantize_self_attention: bool = True,
+    quantize_cross_attention: bool = True,
+):
+  """Decorator for quantizing trasformer with mixed precision setup.
+
+  Example usage:
+
+    @mixed_precision_for_transformer(
+        num_bits={
+            TransformerLayer.LINEAR: 4,
+            TransformerLayer.ATTENTION: 8,
+            TransformerLayer.EMBEDDING_SOFTMAX: 8,
+        },
+        sub_channel_blocksize={
+            TransformerLayer.LINEAR: 128,
+        },
+        dtype={
+            TransformerLayer.LINEAR: jnp.int4,
+        },
+        transposed_embedding_softmax=True,
+    )
+    class QuantizedXYZModel(XYZModel):
+      pass
+
+  Args:
+    quantization_type: Indicates the quantization type among PTQ, FQ, and AQT.
+    mode: Indicates the quantization mode. Only TRAINING and INFERENCE
+      (excluding MATERIALIZE) are valid for non-servable models.
+    num_bits: Dictionary that map the transformer layer name to number of bits
+      used in quantization. If the layer wasn't specified in num_bits, it is not
+      quantized.
+    sub_channel_blocksize: Dictionary that map the transformer layer name to
+      sub-channel quantization block size. If the key was missing, the
+      `default_sub_channel_blocksize` is chosen.
+    use_symmetric: Dictionary that map the transformer layer name to boolean
+      specifying symmetric (True) or asymmetric (False) quantization. If the key
+      was missing, `default_use_symmetric` is chosen.
+    dtype: Dictionary that map the transformer layer name to a jax.numpy type.
+      If the key was missing, `default_dtype` is chosen.
+    default_sub_channel_blocksize: default sub-channel block size.
+    default_use_symmetric: default use symmetric (True) or asymmetric (False).
+    default_dtype: default dtype.
+    transposed_embedding_softmax: If the model is using transposed embedding for
+      embedding softmax layer. This applies to both softmax and embedding
+      layers.
+    quantize_self_attention: Quantize the self attention layer inside the
+      transformer layer.
+    quantize_cross_attention: Quantize the cross attention layer inside the
+      transformer layer.
+  """
+
+  def decorator(cls):
+    """decorator that quantize transformers."""
+
+    @functools.wraps(cls, updated=())  # to keep original class name.
+    class Wrapper(cls):
+      """Wrapper class for cls with Quantization enabled."""
+
+      def task(self):
+        config = super()
+        config.set_quant_mode(mode)
+        task_p = config.task()
+        for k in num_bits.keys():
+          if num_bits[k] not in [2, 4, 8]:
+            raise ValueError(
+                f'Valid bits are 2, 4, 8, but Layer {k} is having'
+                f' {num_bits[k]} bits'
+            )
+        set_transformer_mixed_precision_quantization(
+            task_p.model,
+            quantization_type=quantization_type,
+            mode=mode,
+            num_bits=num_bits,
+            sub_channel_blocksize=sub_channel_blocksize,
+            use_symmetric=use_symmetric,
+            dtype=dtype,
+            default_sub_channel_blocksize=default_sub_channel_blocksize,
+            default_use_symmetric=default_use_symmetric,
+            default_dtype=default_dtype,
+            transposed_embedding_softmax=transposed_embedding_softmax,
+            quantize_self_attention=quantize_self_attention,
+            quantize_cross_attention=quantize_self_attention,
+        )
+        return task_p
+
+    return Wrapper
+
+  return decorator
+
+
 # Ready-to-use quantization decorators for quantizing diffusion.
 def for_diffusion(
     target: Type[base_layer.BaseLayer],
     num_bits: int = 8,
     quantization_type: QuantizationType = QuantizationType.FQ,
     mode: QuantizationMode = QuantizationMode.TRAINING,
     use_symmetric: bool = True,
@@ -712,14 +839,143 @@
             lm_or_encdec_tpl,
             quantization_type,
             mode,
             weight_quantization_params,
         )  # pytype: disable=wrong-arg-types  # py310-upgrade
 
 
+def set_transformer_mixed_precision_quantization(
+    config: LayerTpl,
+    quantization_type: QuantizationType = QuantizationType.PTQ,
+    mode: QuantizationMode = QuantizationMode.INFERENCE,
+    num_bits: Dict[TransformerLayer, int] = {},
+    sub_channel_blocksize: Dict[TransformerLayer, int] = {},
+    use_symmetric: Dict[TransformerLayer, bool] = {},
+    dtype: Dict[TransformerLayer, jnp.dtype] = {},
+    *,
+    default_sub_channel_blocksize: int = 0,
+    default_use_symmetric: bool = True,
+    default_dtype: jnp.dtype = jnp.int8,
+    transposed_embedding_softmax: bool = False,
+    quantize_self_attention: bool = True,
+    quantize_cross_attention: bool = True,
+):
+  """Sets mixed precision quantization parameters for transformers.
+
+  Args:
+    config: The config to apply quantization on.
+    quantization_type: Indicates the quantization type among PTQ, FQ, and AQT.
+    mode: Indicates the quantization mode. Only TRAINING and INFERENCE
+      (excluding MATERIALIZE) are valid for non-servable models.
+    num_bits: Dictionary that map the transformer layer name to number of bits
+      used in quantization. If the layer wasn't specified in num_bits, it is not
+      quantized.
+    sub_channel_blocksize: Dictionary that map the transformer layer name to
+      sub-channel quantization block size. If the key was missing, the
+      `default_sub_channel_blocksize` is chosen.
+    use_symmetric: Dictionary that map the transformer layer name to boolean
+      specifying symmetric (True) or asymmetric (False) quantization. If the key
+      was missing, `default_use_symmetric` is chosen.
+    dtype: Dictionary that map the transformer layer name to a jax.numpy type.
+      If the key was missing, `default_dtype` is chosen.
+    default_sub_channel_blocksize: default sub-channel block size.
+    default_use_symmetric: default use symmetric (True) or asymmetric (False).
+    default_dtype: default dtype.
+    transposed_embedding_softmax: If the model is using transposed embedding for
+      embedding softmax layer. This applies to both softmax and embedding
+      layers.
+    quantize_self_attention: Quantize the self attention layer inside the
+      transformer layer.
+    quantize_cross_attention: Quantize the cross attention layer inside the
+      transformer layer.
+  """
+
+  def _build_weight_params(layer_type: TransformerLayer):
+    runtime_dtype = dtype.get(layer_type, default_dtype)
+    use_int4_packed_weights = False if runtime_dtype == jnp.int4 else True
+    return WeightQuantizationParams(
+        precision=num_bits[layer_type],
+        use_symmetric=use_symmetric.get(layer_type, default_use_symmetric),
+        dtype=runtime_dtype,
+        use_int4_packed_weights=use_int4_packed_weights,
+        block_size=sub_channel_blocksize.get(
+            layer_type, default_sub_channel_blocksize
+        ),
+    )
+
+  def _build_act_params(layer_type: TransformerLayer):
+    if layer_type in num_bits:
+      return ActQuantizationParams(
+          precision=num_bits[layer_type],
+          symmetric=use_symmetric.get(layer_type, default_use_symmetric),
+      )
+    else:
+      return None
+
+  transformer_tpls = utils.find_target_tpl(
+      config, layers.transformers.Transformer
+  )
+  # FFN
+  if TransformerLayer.LINEAR in num_bits:
+    weight_params = _build_weight_params(TransformerLayer.LINEAR)
+    act_params = _build_act_params(TransformerLayer.LINEAR_ACT)
+    for transformer_tpl in transformer_tpls:
+      transformer_ff_tpl = cast(
+          pax_fiddle.Config[layers.transformers.TransformerFeedForward],
+          transformer_tpl.tr_fflayer_tpl,
+      )
+      quantize_transformer_feed_forward_layer_weights(
+          transformer_ff_tpl,
+          quantization_type,
+          mode,
+          weight_params,
+          act_params,
+      )
+  # Attention
+  if TransformerLayer.ATTENTION in num_bits:
+    weight_params = _build_weight_params(TransformerLayer.ATTENTION)
+    act_params = _build_act_params(TransformerLayer.ATTENTION_ACT)
+    for transformer_tpl in transformer_tpls:
+      quantize_attention_layer_weights(
+          transformer_tpl,
+          quantization_type,
+          mode,
+          weight_params,
+          act_params,
+          quantize_self_attention,
+          quantize_cross_attention,
+      )  # pytype: disable=wrong-arg-types  # py310-upgrade
+  # Embedding
+  lm_or_encdec_tpls = utils.find_target_tpl(
+      config, [layers.TransformerLm, layers.TransformerEncoderDecoder]
+  )
+  if TransformerLayer.EMBEDDING_SOFTMAX in num_bits:
+    weight_params = _build_weight_params(TransformerLayer.EMBEDDING_SOFTMAX)
+    act_params = _build_act_params(TransformerLayer.EMBEDDING_SOFTMAX_ACT)
+    for lm_or_encdec_tpl in lm_or_encdec_tpls:
+      _quantize_embedding_softmax_layer_weights(
+          lm_or_encdec_tpl,
+          quantization_type,
+          mode,
+          weight_params,
+          act_quantization_params=act_params,
+          transposed_embedding_softmax=transposed_embedding_softmax,
+          softmax_only=True,
+      )  # pytype: disable=wrong-arg-types  # py310-upgrade
+  if TransformerLayer.EMBEDDING_NGRAMMER in num_bits:
+    weight_params = _build_weight_params(TransformerLayer.EMBEDDING_NGRAMMER)
+    for lm_or_encdec_tpl in lm_or_encdec_tpls:
+      _quantize_ngrammer_embedding_weights(
+          lm_or_encdec_tpl,
+          quantization_type,
+          mode,
+          weight_params,
+      )  # pytype: disable=wrong-arg-types  # py310-upgrade
+
+
 def set_diffusion_quantization(
     config: LayerTpl,
     target: Type[base_layer.BaseLayer],
     quantization_type: QuantizationType = QuantizationType.PTQ,
     mode: QuantizationMode = QuantizationMode.INFERENCE,
     num_bits: int = 8,
     use_symmetric: bool = True,
```

## praxis/layers/quantization/quantizer.py

```diff
@@ -322,14 +322,15 @@
         w = operations.fakequant_einsum(
             eqn,
             w,
             bits=self.quantization.weight_params.precision,
             use_symmetric=self.quantization.weight_params.use_symmetric,
             calculation_dtype=self.quantization.weight_params.calculation_dtype,
             block_size=self.quantization.weight_params.block_size,
+            quant_method=self.quantization.weight_params.quant_method,
         )
         out = jnp.einsum(eqn, x, w)
         if (
             self.quantization.act_params is not None
             and self.quantization.act_params.fp16
         ):
           out = operations.clip_to_fp16(out)
```

## praxis/layers/quantization/quantizer_test.py

```diff
@@ -31,15 +31,15 @@
 
 # TODO(b/277357920) Add tests for quantized einsum and setup weights.
 
 class QuantizerTest(test_utils.TestCase):
 
   def get_quantize_dequantized_and_scale(
       self, p_quant, sample, axis=None
-  ) -> tuple[JTensor, JTensor]:
+  ) -> tuple[JTensor, JTensor, JTensor]:
     # Computes quantized-dequantized and scale of input sample.
 
     quant = p_quant.Instantiate()
     state = quant.init(jax.random.PRNGKey(0))
 
     # Quantize.
     q_x, q_scale, zp_time_scale = quant.apply(
@@ -148,16 +148,16 @@
 
     per_example_result = jax.lax.dot(per_example_qx, y)
     per_example_result = per_example_result / per_example_scale
 
     per_tensor_result = jax.lax.dot(per_tensor_qx, y)
     per_tensor_result = per_tensor_result / per_tensor_scale
 
-    per_example_error = jnp.sum((float_result - per_example_result)**2)
-    per_tensor_error = jnp.sum((float_result - per_tensor_result)**2)
+    per_example_error = np.sum((float_result - per_example_result) ** 2)
+    per_tensor_error = np.sum((float_result - per_tensor_result) ** 2)
 
     self.assertLessEqual(per_example_error, per_tensor_error)
 
   @parameterized.named_parameters(
       dict(testcase_name='scale_gradient', stop_scale_gradient=False),
       dict(testcase_name='stop_scale_gradient', stop_scale_gradient=True),
   )
@@ -195,15 +195,15 @@
         state,
         q_x,
         q_s,
         contract_dims,
         zp_time_scale,
         method=quant.dequantize,
     )
-    sum_error = jnp.sum(jnp.abs(jnp.subtract(x, q_deq_x)))
+    sum_error = np.sum(jnp.abs(jnp.subtract(x, q_deq_x)))
     return q_x, sum_error
 
   def test_clipping_optimization(self):
     sym_p_quant = pax_fiddle.Config(
         quantizer.TensorQuantizer,
         name='sym_quant',
         precision=4,
@@ -400,15 +400,15 @@
         p_quant_asymmetric, x, axis=[1]
     )
     quant_error_asymmetric = jnp.sum(jnp.abs(x_dequant_asymmetric - x))
 
     _, x_dequant_symmetric, _ = self.get_quantize_dequantized_and_scale(
         p_quant_symmetric, x, axis=[1]
     )
-    quant_error_symmetric = jnp.sum(jnp.abs(x_dequant_symmetric - x))
+    quant_error_symmetric = np.sum(jnp.abs(x_dequant_symmetric - x))
 
     self.assertLessEqual(quant_error_asymmetric, quant_error_symmetric)
 
   @parameterized.named_parameters(
       dict(testcase_name='2bit', precision=2),
       dict(testcase_name='4bit', precision=4),
       dict(testcase_name='8bit', precision=8),
@@ -447,15 +447,15 @@
 
     x_quant_unsigned, x_dequant_unsigned, _ = (
         self.get_quantize_dequantized_and_scale(
             p_quant_unsigned, x, axis=contract_dims
         )
     )
     self.assertEqual(2**precision - 1, jnp.max(x_quant_unsigned))
-    quant_error_unsigned = jnp.sum(jnp.abs(x_dequant_unsigned - x))
+    quant_error_unsigned = np.sum(jnp.abs(x_dequant_unsigned - x))
 
     self.assertLessEqual(quant_error_asymmetric, quant_error_unsigned)
 
   def test_clipped_asymmetric_quant_error_less_than_non_clipped(self):
     precision = 4
     contract_dims = [1]
     p_clip = pax_fiddle.Config(
@@ -492,15 +492,15 @@
     x_quant_no_clip, x_dequant_no_clip, _ = (
         self.get_quantize_dequantized_and_scale(
             p_no_clip, x, axis=contract_dims
         )
     )
     self.assertEqual(2 ** (precision - 1) - 1, jnp.max(x_quant_no_clip))
     self.assertEqual(-2 ** (precision - 1), jnp.min(x_quant_no_clip))
-    quant_error_no_clip = jnp.sum(jnp.abs(x_dequant_no_clip - x))
+    quant_error_no_clip = np.sum(jnp.abs(x_dequant_no_clip - x))
 
     # Note that if x has uniform distribution then below will be false.
     self.assertLessEqual(quant_error_clip, quant_error_no_clip)
 
   @parameterized.named_parameters(
       dict(testcase_name='2bit', precision=2),
       dict(testcase_name='4bit', precision=4),
@@ -541,15 +541,15 @@
     x_quant_symmetric, x_dequant_symmetric, _ = (
         self.get_quantize_dequantized_and_scale(
             p_quant_symmetric, x, axis=contract_dims
         )
     )
     self.assertEqual(2 ** (precision - 1) - 1, jnp.max(x_quant_symmetric))
     self.assertLessEqual(-(2 ** (precision - 1)), jnp.min(x_quant_symmetric))
-    quant_error_symmetric = jnp.sum(jnp.abs(x_dequant_symmetric - x))
+    quant_error_symmetric = np.sum(jnp.abs(x_dequant_symmetric - x))
 
     self.assertLessEqual(quant_error_asymmetric, quant_error_symmetric)
 
   @parameterized.named_parameters(
       dict(testcase_name='quantization loss', loss_type=True),
       dict(testcase_name='kurtosis loss', loss_type=False),
   )
@@ -733,14 +733,14 @@
     x_quant_sub, x_dequant_sub, _ = (
         self.get_quantize_dequantized_and_scale(
             p_quant_sub, x, axis=contract_dims
         )
     )
     self.assertEqual(2 ** (precision - 1) - 1, jnp.max(x_quant_sub))
     self.assertLessEqual(-(2 ** (precision - 1)), jnp.min(x_quant_sub))
-    quant_error_sub = jnp.sum(jnp.abs(x_dequant_sub - x))
+    quant_error_sub = np.sum(jnp.abs(x_dequant_sub - x))
 
     self.assertLessEqual(quant_error, quant_error_sub)
 
 
 if __name__ == '__main__':
   absltest.main()
```

## praxis/layers/quantization/searchable_test.py

```diff
@@ -41,16 +41,16 @@
 class SearchableTest(test_utils.TestCase):
 
   def setUp(self):
     super().setUp()
     self.quantization_tpl = quantization_hparams.QuantizationParams(
         quantization_type=quantization_hparams.QuantizationType.AQT,
         mode=quantization_hparams.QuantizationMode.TRAINING,
-        act_params=quantization_hparams.ActQuantizationParams,
-        weight_params=quantization_hparams.WeightQuantizationParams,
+        act_params=quantization_hparams.ActQuantizationParams(),
+        weight_params=quantization_hparams.WeightQuantizationParams(),
     )
 
   def _test_common(self, p, x):
     m = instantiate(p)
 
     with base_layer.JaxContext.new_context():
       m_vars = m.init(jax.random.PRNGKey(0), x)
```

## praxis/layers/quantization/utils.py

```diff
@@ -11,15 +11,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Utilities for quantization."""
 
-from typing import Sequence, Type
+from typing import List, Sequence, Type
 
 import fiddle as fdl
 from jax import lax
 import jax.numpy as jnp
 from praxis import base_layer
 from praxis import pax_fiddle
 
@@ -305,19 +305,217 @@
     )
   return [
       d // packing_factor if i == pack_dim else d for i, d in enumerate(shape)
   ]
 
 
 def find_target_tpl(
-    config: pax_fiddle.Config[base_layer.BaseLayer],
+    config: (
+        pax_fiddle.Config[base_layer.BaseLayer]
+        | Sequence[pax_fiddle.Config[base_layer.BaseLayer]]
+    ),
     targets: Type[base_layer.BaseLayer] | Sequence[Type[base_layer.BaseLayer]],
 ) -> Sequence[fdl.Config]:
   """Traverses the entire config tree to find Configs of the target types."""
   targets = list(targets) if hasattr(targets, '__iter__') else [targets]
   target_tpl = []
   for node, _ in fdl.daglish.iterate(config):
     if isinstance(node, fdl.Config) and any(
         issubclass(fdl.get_callable(node), target) for target in targets
     ):
       target_tpl.append(node)
   return target_tpl
+
+
+def get_lora_shape_and_eqn(
+    shape: Sequence[int], lora_size: int, eqn: str, max_reduction=True
+) -> tuple[str, str, List[int], List[int], List[int], List[int]]:
+  """Gets equations and shapes for LoRA weights of einsum equation.
+
+  Args:
+    shape: Weight shape.
+    lora_size: Size of LoRA dimension.
+    eqn: Einsum equation.
+    max_reduction: It is used only for a case when there are two reduction dims.
+      If True, then max reduction dim will be used as LoRA dim, else it will use
+      both dims for two LoRA dims.
+
+  Returns:
+    Einsum equation for w_left.
+    Einsum equation for w_right.
+    Weight shape for w_left.
+    Weight shape for w_right.
+    Index of LoRA dim in w_left.
+    Index of LoRA dim in w_right.
+  """
+  # Below comments are for example of eqn='...y,yz->...z'.
+  eqn_split = eqn.split('->')
+  assert len(eqn_split) == 2
+  left_right = eqn_split[0]  # '...y,yz'
+  left_right = left_right.split(',')
+  assert len(left_right) == 2
+  left, right = left_right[0], left_right[1]  # ('...y', 'yz')
+
+  def map_str2ind(eqn_part):
+    ch_map = {}
+    ind = 0
+    for ch in eqn_part:
+      if ch != '.':
+        ch_map[ch] = ind
+        ind += 1
+    return ch_map
+
+  left_map = map_str2ind(left)
+  right_map = map_str2ind(right)
+
+  # Find unique character which is not part of eqn.
+  lora_ch1 = None
+  lora_ch2 = None
+  for x in range(97, 123):
+    ch = chr(x)
+    if ch not in left_map and ch not in right_map:
+      if lora_ch1 is None:
+        lora_ch1 = ch
+      elif lora_ch2 is None:
+        lora_ch2 = ch
+      else:
+        break
+  assert lora_ch1 is not None
+  assert lora_ch2 is not None
+
+  # Select reduction dimension.
+  ch_reductions = []
+  ch_reduction2 = None
+  for ch in left_map:
+    if ch in right_map:
+      ch_reductions.append(ch)
+  assert ch_reductions
+  if len(ch_reductions) == 1:
+    ch_reduction1 = ch_reductions[0]
+  elif len(ch_reductions) == 2:
+    # If there are several reduction dimensions then select the largest one
+    # as LoRA dim.
+    if max_reduction:
+      max_reduction_size = 0
+      for ch in ch_reductions:
+        eqn_right_ind = right_map[ch]
+        eqn_right_size = shape[eqn_right_ind]
+        if max_reduction_size < eqn_right_size:
+          max_reduction_size = eqn_right_size
+          ch_reduction1 = ch
+    else:
+      ch_reduction1 = ch_reductions[0]
+      ch_reduction2 = ch_reductions[1]
+  else:
+    raise ValueError(
+        f'Unsupported number of reduction dims: {len(ch_reductions)}'
+    )
+
+  offset = 0
+  if len(left) >= 3:
+    if left[:3] == '...':
+      offset = 3
+
+  if ch_reduction2 is None:
+    # Equation for w_left
+    eqn_left_ind1 = left_map[ch_reduction1]
+    new_right = [ch_reduction1, lora_ch1]
+    new_left = list(left)
+    new_left[eqn_left_ind1 + offset] = lora_ch1
+    new_eqn_left = list(left) + [','] + new_right + ['->'] + new_left
+    new_eqn_left = ''.join(new_eqn_left)
+
+    # Equation for w_right
+    new_right = list(right)
+    eqn_right_ind1 = right_map[ch_reduction1]
+    assert new_right[0] != '.'
+    new_right[eqn_right_ind1] = lora_ch1
+    new_eqn_right = new_left + [','] + new_right + ['->'] + list(eqn_split[1])
+    new_eqn_right = ''.join(new_eqn_right)
+
+    # Shapes for w_left and w_right
+    left_shape = [shape[eqn_right_ind1], lora_size]
+    right_shape = list(shape)
+    right_shape[eqn_right_ind1] = lora_size
+    return (
+        new_eqn_left,
+        new_eqn_right,
+        left_shape,
+        right_shape,
+        [1],
+        [eqn_right_ind1],
+    )
+  else:
+    # Equation for w_left
+    eqn_left_ind1 = left_map[ch_reduction1]
+    eqn_left_ind2 = left_map[ch_reduction2]
+    new_right = [ch_reduction1, ch_reduction2, lora_ch1, lora_ch2]
+    new_left = list(left)
+    new_left[eqn_left_ind1 + offset] = lora_ch1
+    new_left[eqn_left_ind2 + offset] = lora_ch2
+    new_eqn_left = list(left) + [','] + new_right + ['->'] + new_left
+    new_eqn_left = ''.join(new_eqn_left)
+
+    # Equation for w_right
+    new_right = list(right)
+    eqn_right_ind1 = right_map[ch_reduction1]
+    eqn_right_ind2 = right_map[ch_reduction2]
+    assert new_right[0] != '.'
+    new_right[eqn_right_ind1] = lora_ch1
+    new_right[eqn_right_ind2] = lora_ch2
+    new_eqn_right = new_left + [','] + new_right + ['->'] + list(eqn_split[1])
+    new_eqn_right = ''.join(new_eqn_right)
+
+    # Shapes for w_left and w_right
+    left_shape = [
+        shape[eqn_right_ind1],
+        shape[eqn_right_ind2],
+        lora_size,
+        lora_size,
+    ]
+    right_shape = list(shape)
+    right_shape[eqn_right_ind1] = lora_size
+    right_shape[eqn_right_ind2] = lora_size
+    return (
+        new_eqn_left,
+        new_eqn_right,
+        left_shape,
+        right_shape,
+        [2, 3],
+        [eqn_right_ind1, eqn_right_ind2],
+    )
+
+
+def get_left_weight_split_dims_mapping(
+    weight_split_dims_mapping: tuple[str | None, ...] | None,
+    eqn_left_ind: List[int],
+) -> tuple[str | None, ...] | None:
+  if weight_split_dims_mapping is None:
+    return None
+  else:
+    if len(eqn_left_ind) == 1:
+      return (weight_split_dims_mapping[0], None)
+    elif len(eqn_left_ind) == 2:
+      return (
+          weight_split_dims_mapping[0],
+          weight_split_dims_mapping[1],
+          None,
+          None,
+      )
+    else:
+      raise ValueError(
+          f'Usupported number of reduction dims {len(eqn_left_ind)}'
+      )
+
+
+def get_right_weight_split_dims_mapping(
+    weight_split_dims_mapping: tuple[str | None, ...] | None,
+    eqn_right_ind: List[int],
+) -> tuple[str | None, ...] | None:
+
+  if weight_split_dims_mapping is None:
+    return None
+  else:
+    out_weight_split_dims_mapping = list(weight_split_dims_mapping)
+    for ind in eqn_right_ind:
+      out_weight_split_dims_mapping[ind] = None
+    return tuple(out_weight_split_dims_mapping)
```

## praxis/layers/quantization/utils_test.py

```diff
@@ -11,26 +11,48 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for utilities."""
 
+from __future__ import annotations
+
 import dataclasses
 
 from absl.testing import absltest
 from absl.testing import parameterized
-import fiddle as fdl
 import jax
 from jax import numpy as jnp
 import numpy as np
+from praxis import pax_fiddle as fdl
 from praxis import test_utils
 from praxis.layers.quantization import utils
 
 
+@dataclasses.dataclass(frozen=True)
+class Target:
+  marker: str = 'default'
+
+
+@dataclasses.dataclass
+class Inner:
+  irrelevant: int
+  target1: Target
+
+
+@dataclasses.dataclass
+class Outer:
+  irrelevant: int
+  target2: Target
+  inner_direct: Inner
+  inner_list: list[Inner]
+  inner_dict: dict[str, Inner]
+
+
 class UtilsTest(test_utils.TestCase):
 
   @parameterized.parameters(
       dict(eqn='ANH,NHD->AD', lhs_shape=(2, 3, 4), rhs_shape=(3, 4, 2)),
       dict(eqn='ANH,DNH->AD', lhs_shape=(2, 3, 4), rhs_shape=(2, 3, 4)),
       dict(eqn='AD,DNH->ANH', lhs_shape=(2, 3), rhs_shape=(3, 4, 2)),
       dict(eqn='AD,KDNH->KANH', lhs_shape=(2, 3), rhs_shape=(2, 3, 4, 2)),
@@ -167,32 +189,15 @@
     self.assertRaises(ValueError, utils.get_packed_shape, (4, 7, 3), 1, 8)
 
   @parameterized.named_parameters(
       ('single target', True),
       ('multiple targets', False),
   )
   def test_find_target_tpl(self, sequence_of_inputs):
-    @dataclasses.dataclass(frozen=True)
-    class Target:
-      marker: str = 'default'
-
-    @dataclasses.dataclass()
-    class Inner:
-      irrelevant: int
-      target1: Target
-
-    @dataclasses.dataclass()
-    class Outer:
-      irrelevant: int
-      target2: Target
-      inner_direct: Inner
-      inner_list: list[Inner]
-      inner_dict: dict[str, Inner]
-
-    outer_p = (
+    outer_p = [
         fdl.Config(
             Outer,
             irrelevant=-1,
             target2=fdl.Config(Target, marker='target2'),
             inner_direct=fdl.Config(
                 Inner,
                 irrelevant=-2,
@@ -219,29 +224,110 @@
                 'b': fdl.Config(
                     Inner,
                     irrelevant=-5,
                     target1=fdl.Config(Target, marker='target1_5'),
                 ),
             },
         ),
-    )
+    ]
     if sequence_of_inputs:
-      targets = utils.find_target_tpl(outer_p, [Target, Target])
+      targets = utils.find_target_tpl(outer_p, [Target, Target])  # pytype: disable=wrong-arg-types
     else:
-      targets = utils.find_target_tpl(outer_p, Target)
+      targets = utils.find_target_tpl(outer_p, Target)  # pytype: disable=wrong-arg-types
     # NOTE(yinzhong): fdl.Config is not hashable or sortable, so we have to
     # build before comparing.
     self.assertSameElements(
         fdl.build(targets),
         fdl.build([
             fdl.Config(Target, marker='target2'),
             fdl.Config(Target, marker='target1_1'),
             fdl.Config(Target, marker='target1_2'),
             fdl.Config(Target, marker='target1_3'),
             fdl.Config(Target, marker='target1_4'),
             fdl.Config(Target, marker='target1_5'),
         ]),
     )
 
+  def test_lora_shape_and_eqn(self):
+    # One reduction dimension.
+    eqn = '...td,dD->...tD'
+    shape = (3, 5)
+    lora_size = 2
+    (
+        new_eqn_left,
+        new_eqn_right,
+        left_shape,
+        right_shape,
+        eqn_left_ind,
+        eqn_right_ind,
+    ) = utils.get_lora_shape_and_eqn(shape, lora_size, eqn)
+    self.assertEqual(new_eqn_left, '...td,da->...ta')
+    self.assertEqual(new_eqn_right, '...ta,aD->...tD')
+    self.assertEqual(left_shape, [3, 2])
+    self.assertEqual(right_shape, [2, 5])
+    self.assertEqual(eqn_left_ind, [1])
+    self.assertEqual(eqn_right_ind, [0])
+
+    # Two reduction dimensions.
+    eqn = '...tdh,dDh->...tD'
+    shape = (4, 5, 8)  # Max reduction dim is 'h' = 8
+    lora_size = 2
+    (
+        new_eqn_left,
+        new_eqn_right,
+        left_shape,
+        right_shape,
+        eqn_left_ind,
+        eqn_right_ind,
+    ) = utils.get_lora_shape_and_eqn(shape, lora_size, eqn)
+    self.assertEqual(new_eqn_left, '...tdh,ha->...tda')
+    self.assertEqual(new_eqn_right, '...tda,dDa->...tD')
+
+    self.assertEqual(left_shape, [8, 2])
+    self.assertEqual(right_shape, [4, 5, 2])
+    self.assertEqual(eqn_left_ind, [1])
+    self.assertEqual(eqn_right_ind, [2])
+
+    # Two reduction dimensions.
+    eqn = '...tdh,dDh->...tD'
+    shape = (8, 5, 4)  # Max reduction dim is 'd' = 8
+    lora_size = 2
+    (
+        new_eqn_left,
+        new_eqn_right,
+        left_shape,
+        right_shape,
+        eqn_left_ind,
+        eqn_right_ind,
+    ) = utils.get_lora_shape_and_eqn(shape, lora_size, eqn)
+    self.assertEqual(new_eqn_left, '...tdh,da->...tah')
+    self.assertEqual(new_eqn_right, '...tah,aDh->...tD')
+
+    self.assertEqual(left_shape, [8, 2])
+    self.assertEqual(right_shape, [2, 5, 4])
+    self.assertEqual(eqn_left_ind, [1])
+    self.assertEqual(eqn_right_ind, [0])
+
+    # Two reduction dimensions.
+    eqn = '...tdh,dDh->...tD'
+    shape = (8, 5, 4)  # Max reduction dim is 'd' = 8
+    lora_size = 2
+    (
+        new_eqn_left,
+        new_eqn_right,
+        left_shape,
+        right_shape,
+        eqn_left_ind,
+        eqn_right_ind,
+    ) = utils.get_lora_shape_and_eqn(shape, lora_size, eqn, max_reduction=False)
+
+    self.assertEqual(new_eqn_left, '...tdh,dhab->...tab')
+    self.assertEqual(new_eqn_right, '...tab,aDb->...tD')
+
+    self.assertEqual(left_shape, [8, 4, 2, 2])
+    self.assertEqual(right_shape, [2, 5, 2])
+    self.assertEqual(eqn_left_ind, [2, 3])
+    self.assertEqual(eqn_right_ind, [0, 2])
+
 
 if __name__ == '__main__':
   absltest.main()
```

## praxis/layers/quantization/sparsity/sparsifier.py

```diff
@@ -232,14 +232,16 @@
       ):
         return mask
       return sparsity.get_sparsity_mask(
           score,
           n_sparsity=self.sparsity.weight_params.prune_rate[0],
           m_sparsity=self.sparsity.weight_params.prune_rate[1],
           order=self.sparsity.order,
+          offset=self.sparsity.weight_params.offset,
+          block_size=self.sparsity.block_size,
       )
 
     if self.sparsity.sparsity_type == SparsityType.CHANNELWISE_PRUNING:
       if (
           self.sparsity.weight_params is None
           or self.sparsity.weight_params.prune_rate is None
       ):
@@ -559,15 +561,15 @@
         weight=weight,
         inputs=inputs,
         name=name,
         step=step,
     )
 
     # NOTE: Mask will be all True (as initialized) for steps before target step
-    # [case of few shot/one shot]; and for layer we dont want to sparsify
+    # [case of few shot/one shot]; and for layer we don't want to sparsify
     # so we apply mask for all the cases.
     mask_var_name = name + SPARSITY_NAME_POSTFIX
     mask = self.get_var(mask_var_name)
 
     if mask.shape != weight.shape:
       mask = jnp.reshape(mask, weight.shape)
```

## praxis/layers/quantization/sparsity/sparsifier_test.py

```diff
@@ -45,15 +45,15 @@
 InferenceMode = sparsity_modes.InferenceMode
 FewShotMode = sparsity_modes.FewShotMode
 OneShotMode = sparsity_modes.OneShotMode
 MaterializeMode = sparsity_modes.MaterializeMode
 TrainingMode = sparsity_modes.TrainingMode
 
 
-class SparseLinearTestLayer(sparsifier.SparsityBaseLayer, linears.Linear):
+class SparseLinearTestLayer(sparsifier.SparsityBaseLayer, linears.Linear):  # pytype: disable=signature-mismatch
 
   def setup(self):
     weight_hp = base_layer.WeightHParams(
         shape=[self.input_dims, self.output_dims],
         init=self.params_init,
         dtype=self.dtype,
     )
@@ -1131,14 +1131,96 @@
       self.assertArraysEqual(
           outputs,
           jnp.einsum(
               '...y,yz->...z',
               inputs,
               jnp.multiply(
                   params[PARAMS]['w'],
+                  updated_params[NON_TRAINABLE]['w' + SPARSITY_NAME_POSTFIX],
+              ),
+          ),
+      )
+
+  def test_block_sparsity_mask(self):
+    sparsity_p = pax_fiddle.Config(
+        SparsityHParams,
+        sparsity_type=SparsityType.STRUCTURED_NM,
+        mode=pax_fiddle.Config(TrainingMode, target_step=0),
+        weight_params=WeightSparsityParams(prune_rate=(1, 2)),
+        block_size=2,
+    )
+
+    p = pax_fiddle.Config(
+        SparseLinearTestLayer, sparsity=sparsity_p, input_dims=4, output_dims=4
+    )
+
+    test_layer = instantiate(p)
+    prng_key = jax.random.PRNGKey(seed=123)
+    inputs = jnp.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=p.dtype)
+    weights = jnp.array(
+        [
+            [10, 20, 3, 4],
+            [-3, 10, 4, 2],
+            [2, 4, -1, 30],
+            [40, -1, 2, -3],
+        ],
+        dtype=p.dtype,
+    )
+
+    def update(test_layer, params, inputs, updated_weights):
+      outputs, updated_params = test_layer.apply(params, inputs, mutable=True)
+      updated_params[PARAMS]['w'] = updated_weights
+
+      return outputs, updated_params
+
+    with base_layer.JaxContext.new_context():
+      params = test_layer.init(prng_key, inputs)
+      params[PARAMS]['w'] = weights
+
+      self.assertArraysEqual(
+          params[NON_TRAINABLE]['w' + SPARSITY_NAME_POSTFIX],
+          jnp.array([
+              [True, True, True, True],
+              [True, True, True, True],
+              [True, True, True, True],
+              [True, True, True, True],
+          ]),
+      )
+
+      # step 0, update mask, apply mask
+      outputs, updated_params = update(
+          test_layer,
+          params,
+          inputs,
+          jnp.array(
+              [
+                  [10, 20, 3, 4],
+                  [-3, 1, 4, 2],
+                  [20, 4, -10, 3],
+                  [4, -1, 2, -30],
+              ],
+              dtype=p.dtype,
+          ),
+      )
+      self.assertArraysEqual(
+          updated_params[NON_TRAINABLE]['w' + SPARSITY_NAME_POSTFIX],
+          jnp.array([
+              [False, True, True, False],
+              [False, True, True, False],
+              [True, False, False, True],
+              [True, False, False, True],
+          ]),
+      )
+      self.assertArraysEqual(
+          outputs,
+          jnp.einsum(
+              '...y,yz->...z',
+              inputs,
+              jnp.multiply(
+                  params[PARAMS]['w'],
                   updated_params[NON_TRAINABLE]['w' + SPARSITY_NAME_POSTFIX],
               ),
           ),
       )
 
   @parameterized.named_parameters(
       ('column_wise', -1),
```

## praxis/layers/quantization/sparsity/sparsity.py

```diff
@@ -50,64 +50,160 @@
   return jnp.where(mask, inputs, jnp.zeros(inputs.shape, inputs.dtype))
 
 
 def get_sparsity_mask(
     inputs: jnp.ndarray,
     n_sparsity: int = 0,
     m_sparsity: int = 0,
-    order: sparsity_hparams.SparsityOrder = sparsity_hparams.SparsityOrder.C,
+    order: str = 'R',
+    block_size: int = 0,
+    offset: int = 0,
 ) -> jnp.ndarray:
   """Returns sparsified inputs for n:m structured pruning.
 
   Args:
     inputs: Input array for which N:M pruning mask is computed.
     n_sparsity: Maximum number of non-zero values in each block.
     m_sparsity: Number of values in each block.
     order: Apply pruning using this index order. Supported values are `C`, `R`.
       `C` and `R` indicate column-wise and row-wise masking, respectively.
       Default is `R` indicating to applying N:M sparsity across rows of the
       input matrix. Default is `C` indicating to applying N:M sparsity across
       columns of the input matrix. The choice may intersect with hardware
       capabilities. For a weight tensor `C` corresponds to the reduction
       dimension, and `R' for activations.
+    block_size: Number of values in each weight block.
+    offset: Indicates the offset between the group of M elements on which
+      N:M sparsity is applied. The default is `0` (narrowly-separated),
+        indicating that `M` elements are selected from adjacent values in the
+        input matrix. Generally, because of the XLA layout (lanes 128/sublanes
+        8), another value for offset would be 128 (widely-separated). If offset
+        > 0, we only support scenarios where the input array size is equal to
+        (offset * m). Offset != 128 may not be best optimized for the memory
+        layout.
 
   Returns:
     A mask that indicates the pruning locations (`0`: no pruning, `1`: pruned).
   """
   assert (
       n_sparsity <= m_sparsity
   ), f'N must be lower than M for N:M ({n_sparsity}:{m_sparsity}) sparsity.'
+  if order not in ['C', 'R']:
+    raise ValueError(f'Index order {order} not supported.')
+  if offset < 0:
+    raise ValueError(f'Offset value must be positive. You provided {offset}.')
+
   length = jnp.size(inputs)
   if length % m_sparsity != 0:
     raise ValueError(
         f'inputs size must be divisible by m, provided {length} and'
         f' {m_sparsity}'
     )
   if order not in ['C', 'R']:
     raise ValueError(f'Index order {order} not supported.')
 
-  group = int(length / m_sparsity)
+  if block_size > 1:
+    blocks = int(length / block_size)
+    original_shape = inputs.shape
+    if order == 'R':
+      inputs_block = inputs.reshape(blocks, block_size, order='C')
+    else:
+      inputs_trans = jnp.einsum('...ij->...ji', inputs)
+      original_shape = inputs_trans.shape
+      inputs_block = inputs_trans.reshape(blocks, block_size, order='C')
+
+    def block_score(inputs: jnp.ndarray):
+      return jnp.sum(jnp.abs(inputs), axis=-1)
+
+    inputs_block_temp = jnp.apply_along_axis(
+        block_score, axis=-1, arr=inputs_block
+    )
+    mask_shape = tuple((
+        original_shape[i]
+        if i != jnp.size(original_shape) - 1
+        else int(original_shape[i] / block_size)
+        for i in range(jnp.size(original_shape))
+    ))
+    if order == 'R':
+      new_inputs = inputs_block_temp.reshape(mask_shape, order='C')
+    else:
+      new_inputs = jnp.einsum(
+          '...ij->...ji', inputs_block_temp.reshape(mask_shape, order='C')
+      )
+    inputs = new_inputs
+
+  length = jnp.size(inputs)
+  if offset > 0 and length % (offset * m_sparsity) != 0:
+    raise ValueError(
+        'When offset > 0, we only support an array size (length) equal to '
+        f'(offset * m_sparsity). Provided offset = {offset}, '
+        f'm_sparsity = {m_sparsity}, length = {length}.'
+    )
+
   inputs = jnp.abs(inputs)
   original_shape = inputs.shape
-  if order == 'R':
-    inputs_temp = inputs.reshape(group, m_sparsity, order='C')
+
+  if order == 'C':
+    inputs = jnp.einsum('...ij->...ji', inputs)
+    original_shape = inputs.shape
+
+  prac_offset = 1 if offset == 0 else offset
+  if original_shape[-1] % (m_sparsity * prac_offset) == 0:
+    group = original_shape[-1] // m_sparsity
+    # TODO(shivaniagrawal): we can always split in 3D with offset=1 too and
+    # do top-K in -2 dimension.
+    if offset > 1:
+      new_shape = (*original_shape[:-1], group // offset, m_sparsity, offset)
+      inputs = inputs.reshape(new_shape)
+      inputs = jnp.einsum('...ij->...ji', inputs)
+
+    new_shape = (*original_shape[:-1], group, m_sparsity)
+    inputs_temp = inputs.reshape(new_shape)
+
   else:
-    inputs_trans = jnp.einsum('...ij->...ji', inputs)
-    original_shape = inputs_trans.shape
-    inputs_temp = inputs_trans.reshape(group, m_sparsity, order='C')
-  # Extract the smallest elements and forcefully make them zero.
+    group = int(length / m_sparsity)
+    if offset > 0:
+      inputs = inputs.reshape((group // offset, m_sparsity, offset))
+      inputs = jnp.einsum('...ij->...ji', inputs)
+
+    inputs_temp = inputs.reshape(group, m_sparsity, order='C')
+
   _, top_k_indices = jax.lax.top_k(inputs_temp, k=n_sparsity)
   mask = jnp.any(
       jax.nn.one_hot(top_k_indices, m_sparsity, dtype=jnp.bool_), axis=-2
   )
 
+  if offset > 0:
+    # NOTE: without meeting this condition, we had flattened the whole matrix
+    # and mask as well.
+    if original_shape[-1] % (m_sparsity * offset) == 0:
+      # group = original_shape[-1] // m_sparsity in this case
+      mask = mask.reshape(
+          (*original_shape[:-1], group // offset, offset, m_sparsity)
+      )
+    else:
+      # group = length // m_sparsity in this case
+      mask = mask.reshape((group // offset, offset, m_sparsity))
+    mask = jnp.einsum('...ij->...ji', mask)
+
   if order == 'R':
-    return mask.reshape(original_shape, order='C')
+    result_mask = mask.reshape(original_shape, order='C')
+  else:
+    result_mask = jnp.einsum(
+        '...ij->...ji', mask.reshape(original_shape, order='C')
+    )
+
+  if block_size > 0:
+    if order == 'R':
+      expanded_mask = jnp.repeat(result_mask, block_size, axis=-1)
+    else:
+      expanded_mask = jnp.repeat(result_mask, block_size, axis=-2)
+    return expanded_mask
   else:
-    return jnp.einsum('...ij->...ji', mask.reshape(original_shape, order='C'))
+    return result_mask
 
 
 @jax.jit
 def _topk_mask_calculator_internal(inputs: jnp.ndarray, prune_rate: float):
   """Creates a binary mask given the prune rate on the scores."""
   flat_inputs = jnp.reshape(inputs, -1)
   num_ones = jnp.round(flat_inputs.size * (1 - prune_rate)).astype(int)
@@ -128,19 +224,19 @@
 
 @functools.partial(jax.jit, static_argnames=['channel_dim'])
 def get_sparsity_mask_channelwise(
     inputs: jnp.ndarray, prune_rate: float, channel_dim: int = -1
 ) -> jnp.ndarray:
   """Returns a mask for the channel-wise pruned input.
 
-    Args:
-      inputs: The input matrix to have channel-wise masking applied.
-      prune_rate: The rate in which values in the inputs are pruned.
-      channel_dim: The channel dimension, the dimension across which channels
-        will be pruned.
+  Args:
+    inputs: The input matrix to have channel-wise masking applied.
+    prune_rate: The rate in which values in the inputs are pruned.
+    channel_dim: The channel dimension, the dimension across which channels will
+      be pruned.
 
   Returns:
     A mask that indicates the pruning locations.
   """
   target_axis = channel_dim % inputs.ndim
 
   non_target_axes = [i for i in range(inputs.ndim) if i != target_axis]
@@ -186,15 +282,18 @@
 # TODO(shivaniagrawal): Only used for testing the functionality of
 # get_prune_mask; update the test to call get_pruning_n_m_mask instead.
 def prune_inputs_n_m(
     inputs: jnp.ndarray,
     *,
     n: int,
     m: int,
-    order: sparsity_hparams.SparsityOrder = sparsity_hparams.SparsityOrder.C,
+    order: (
+        sparsity_hparams.SparsityOrder | str
+    ) = sparsity_hparams.SparsityOrder.C,
+    offset: int = 0,
 ) -> jnp.ndarray:
   """Returns pruned array with N:M (structured) pruning.
 
   N:M pruning makes at most N values non-zero in each block of M consecutive
   values.
 
   Args:
@@ -203,19 +302,27 @@
     m: Number of values in each block.
     order: Apply pruning using this index order. Supported values are `C`, `R`.
       `C` and `R` indicate column-wise and row-wise masking, respectively.
       Default is `R` indicating to applying N:M sparsity across rows of the
       input matrix. The choice may intersect with hardware capabilities. For a
       weight tensor `C` corresponds to the reduction dimension, and `R' for
       activations.
+    offset: Indicates the offset between the group of M elements on which
+      N:M sparsity is applied. The default is `0` (narrowly-separated),
+        indicating that `M` elements are selected from adjacent values in the
+        input matrix. Generally, because of the XLA layout (lanes 128/sublanes
+        8), another value for offset would be 128 (widely-separated). If offset
+        > 0, we only support scenarios where the input array size is equal to
+        (offset * m). Offset != 128 may not be best optimized for the memory
+        layout.
 
   Returns:
     An array with the same shape as inputs pruned with N:M strategy.
   """
-  mask = get_sparsity_mask(inputs, n, m, order=order)
+  mask = get_sparsity_mask(inputs, n, m, order=order, offset=offset)
   return jnp.where(mask, inputs, jnp.zeros(inputs.shape, inputs.dtype))
 
 
 SparsityScore = sparsity_hparams.SparsityScore
 
 
 def compute_score(
```

## praxis/layers/quantization/sparsity/sparsity_hparams.py

```diff
@@ -96,22 +96,31 @@
       https://arxiv.org/pdf/2209.07617.pdf.
     sparse_ste: If True, a sparse-refined straight-through estimator (SR-STE) is
       applied, following the algorithm described in:
         https://arxiv.org/abs/2102.04010
     sparse_ste_weight: Denotes the relative weight for the sparse-refined term.
       As mentioned in the paper (https://arxiv.org/abs/2102.04010), the best
       default value is 0.0002 (lambda_w in the paper).
+    offset:  Indicates the offset between the group of M elements on which
+      N:M sparsity is applied. The default is `0` (narrowly-separated),
+        indicating that `M` elements are selected from adjacent values in the
+        input matrix. Generally, because of the XLA layout (lanes 128/sublanes
+        8), another value for offset would be 128 (widely-separated). If offset
+        > 0, we only support scenarios where the input array size is equal to
+        (offset * m). Offset != 128 may not be best optimized for the memory
+        layout.
   """
 
   # TODO(ayazdan): Add additional sparsity parameters (order, offset, etc.)
   prune_rate: None | float | tuple[int, int]
   structure_decay: bool = False
   mask_decay_weight: float = 0.0
   sparse_ste: bool = False
   sparse_ste_weight: float = 0.0002
+  offset: int = 0
 
   def __post_init__(self):
     assert self.mask_decay_weight >= 0.0, (
         'Invalid value for '
         f'{self.mask_decay_weight}. '
         '`mask_decay_weight` must be positive.'
     )
@@ -126,14 +135,15 @@
       if self.mask_decay_weight != 0.0:
         raise ValueError('SR-STE only works with non-decaying mask.')
       if self.structure_decay:
         raise ValueError(
             'SR-STE only works with non-decaying sparse structure.'
         )
 
+    assert self.offset >= 0, 'Offset must be positive.'
 
 # NOTE: Pay attention to which dimension, and type of tensor being sparsified.
 # Some hardware may support sparsity along the reduction dimension alone. This
 # would translate to `C' i.e., column-wise pruning for weights, and `R' i.e.,
 # row-wise pruning for activations. Enforcing the correct order may be required
 # to suitably target hardware capabilities.
 @enum.unique
@@ -166,14 +176,17 @@
       `C` and `R` indicate column-wise and row-wise masking, respectively.
       Default is `C` indicating to applying N:M sparsity across columns of the
       input matrix. The choice may intersect with hardware capabilities, that
       support sparsity only along a reduction dimension. For a weight tensor `C`
       corresponds to the reduction dimension, and `R' for activations.
     track_sad_metric: Should we track sparse architecture divergence metric?
     topk_estimator_type: Sets the type of top-k mask learning.
+    channelwise_pruning_dim: dimension along which we would want to do
+      channelwise pruning.
+    block_size: Number of values in each weight block.
   """
 
   sparsity_type: SparsityType = SparsityType.STRUCTURED_NM
   weight_params: WeightSparsityParams | None = None
   mode: BaseSparsityScheduleMode = dataclasses.field(
       default_factory=InferenceMode
   )
@@ -182,14 +195,15 @@
   polynomial_decay_schedule: PolynomialDecayParams | None = None
   order: SparsityOrder = SparsityOrder.C
   track_sad_metric: bool = False
   topk_estimator_type: str | None = None
   # TODO enable per layer dim i.e. linear 1 and 2
   # Enable unstacking
   channelwise_pruning_dim: int = -1
+  block_size: int = 0
 
   def __post_init__(self):
     if (
         self.weight_params is not None
     ):
       # Check sparsity types.
       if self.sparsity_type == SparsityType.STRUCTURED_NM:
@@ -218,7 +232,15 @@
           raise ValueError('SR-STE only works with structured sparsity.')
 
       else:
         assert False, f'Unrecognized sparsity type {self.sparsity_type}.'
 
       if self.order not in ['C', 'R']:
         raise ValueError(f'Index order {self.order} not supported.')
+
+      if self.block_size != 0:
+        assert self.block_size > 0, 'Block size must be positive.'
+        if self.sparsity_type != SparsityType.STRUCTURED_NM:
+          raise ValueError(
+              f'Block size {self.block_size} not supported for '
+              'unstructured sparsity.'
+          )
```

## praxis/layers/quantization/sparsity/sparsity_test.py

```diff
@@ -366,14 +366,226 @@
     inputs = jnp.array(inputs)
     output = sparsity.prune_inputs_n_m(
         inputs, n=n_sparsity, m=m_sparsity, order=order
     )
     np.testing.assert_array_equal(output, exp_output)
 
 
+class BlockPruningFunctionalityTest(parameterized.TestCase):
+
+  @parameterized.named_parameters(
+      dict(testcase_name='block_size_1', block_size=1),
+      dict(testcase_name='block_size_2', block_size=2),
+      dict(testcase_name='block_size_4', block_size=4),
+  )
+  def test_prune_inputs_n_m(self, block_size):
+    inputs = jnp.array(np.random.rand(10, 2, 4))
+    prune_rate = (1, 2)
+
+    block_mask = sparsity.get_sparsity_mask(
+        inputs,
+        n_sparsity=prune_rate[0],
+        m_sparsity=prune_rate[1],
+        order='R',
+        block_size=block_size,
+    )
+    out = sparsity.apply_sparsity(inputs, block_mask)
+    self.assertEqual(out.shape[0], inputs.shape[0])
+    self.assertEqual(out.shape[1], inputs.shape[1])
+    self.assertEqual(out.shape[2], inputs.shape[2])
+
+    # Only 40 non-zero elements must exist after pruning.
+    num_non_zero_elems = 0.5 * inputs.size
+    self.assertEqual(out[out != 0].shape[0], num_non_zero_elems)
+
+  @parameterized.named_parameters(
+      dict(
+          testcase_name='2d_row_wise_pruning',
+          order='R',
+          inputs=np.arange(1, 73).reshape(6, 12),
+          exp_output=[
+              [0, 0, 3, 4, 5, 6, 0, 0, 9, 10, 11, 12],
+              [0, 0, 15, 16, 17, 18, 0, 0, 21, 22, 23, 24],
+              [0, 0, 27, 28, 29, 30, 0, 0, 33, 34, 35, 36],
+              [0, 0, 39, 40, 41, 42, 0, 0, 45, 46, 47, 48],
+              [0, 0, 51, 52, 53, 54, 0, 0, 57, 58, 59, 60],
+              [0, 0, 63, 64, 65, 66, 0, 0, 69, 70, 71, 72],
+          ],
+          n_sparsity=2,
+          m_sparsity=3,
+          block_size=2,
+      ),
+      dict(
+          testcase_name='2d_row_wise_pruning_2',
+          order='R',
+          inputs=np.arange(1, 73).reshape(6, 12),
+          exp_output=[
+              [0, 0, 0, 0, 5, 6, 7, 8, 9, 10, 11, 12],
+              [0, 0, 0, 0, 17, 18, 19, 20, 21, 22, 23, 24],
+              [0, 0, 0, 0, 29, 30, 31, 32, 33, 34, 35, 36],
+              [0, 0, 0, 0, 41, 42, 43, 44, 45, 46, 47, 48],
+              [0, 0, 0, 0, 53, 54, 55, 56, 57, 58, 59, 60],
+              [0, 0, 0, 0, 65, 66, 67, 68, 69, 70, 71, 72],
+          ],
+          n_sparsity=2,
+          m_sparsity=3,
+          block_size=4,
+      ),
+      dict(
+          testcase_name='3d_row_wise_pruning',
+          order='R',
+          inputs=np.arange(1, 73).reshape(2, 6, 6),
+          exp_output=[
+              [
+                  [0, 0, 3, 4, 5, 6],
+                  [0, 0, 9, 10, 11, 12],
+                  [0, 0, 15, 16, 17, 18],
+                  [0, 0, 21, 22, 23, 24],
+                  [0, 0, 27, 28, 29, 30],
+                  [0, 0, 33, 34, 35, 36],
+              ],
+              [
+                  [0, 0, 39, 40, 41, 42],
+                  [0, 0, 45, 46, 47, 48],
+                  [0, 0, 51, 52, 53, 54],
+                  [0, 0, 57, 58, 59, 60],
+                  [0, 0, 63, 64, 65, 66],
+                  [0, 0, 69, 70, 71, 72],
+              ],
+          ],
+          n_sparsity=2,
+          m_sparsity=3,
+          block_size=2,
+      ),
+      dict(
+          testcase_name='3d_row_wise_pruning_2',
+          order='R',
+          inputs=np.arange(1, 73).reshape(1, 6, 12),
+          exp_output=[
+              [
+                  [0, 0, 0, 0, 5, 6, 7, 8, 9, 10, 11, 12],
+                  [0, 0, 0, 0, 17, 18, 19, 20, 21, 22, 23, 24],
+                  [0, 0, 0, 0, 29, 30, 31, 32, 33, 34, 35, 36],
+                  [0, 0, 0, 0, 41, 42, 43, 44, 45, 46, 47, 48],
+                  [0, 0, 0, 0, 53, 54, 55, 56, 57, 58, 59, 60],
+                  [0, 0, 0, 0, 65, 66, 67, 68, 69, 70, 71, 72],
+              ],
+          ],
+          n_sparsity=2,
+          m_sparsity=3,
+          block_size=4,
+      ),
+      dict(
+          testcase_name='2d_column_wise_pruning',
+          order='C',
+          inputs=np.arange(1, 73).reshape(6, 12),
+          exp_output=[
+              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
+              [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36],
+              [37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48],
+              [49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60],
+              [61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72],
+          ],
+          n_sparsity=2,
+          m_sparsity=3,
+          block_size=2,
+      ),
+      dict(
+          testcase_name='2d_column_wise_pruning_2',
+          order='C',
+          inputs=np.arange(1, 73).reshape(12, 6),
+          exp_output=[
+              [0, 0, 0, 0, 0, 0],
+              [0, 0, 0, 0, 0, 0],
+              [0, 0, 0, 0, 0, 0],
+              [0, 0, 0, 0, 0, 0],
+              [25, 26, 27, 28, 29, 30],
+              [31, 32, 33, 34, 35, 36],
+              [37, 38, 39, 40, 41, 42],
+              [43, 44, 45, 46, 47, 48],
+              [49, 50, 51, 52, 53, 54],
+              [55, 56, 57, 58, 59, 60],
+              [61, 62, 63, 64, 65, 66],
+              [67, 68, 69, 70, 71, 72],
+          ],
+          n_sparsity=2,
+          m_sparsity=3,
+          block_size=4,
+      ),
+      dict(
+          testcase_name='3d_column_wise_pruning',
+          order='C',
+          inputs=np.arange(1, 65).reshape(2, 8, 4),
+          exp_output=[
+              [
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [17, 18, 19, 20],
+                  [21, 22, 23, 24],
+                  [25, 26, 27, 28],
+                  [29, 30, 31, 32],
+              ],
+              [
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [49, 50, 51, 52],
+                  [53, 54, 55, 56],
+                  [57, 58, 59, 60],
+                  [61, 62, 63, 64],
+              ],
+          ],
+          n_sparsity=2,
+          m_sparsity=4,
+          block_size=2,
+      ),
+      dict(
+          testcase_name='3d_column_wise_pruning_2',
+          order='C',
+          inputs=np.arange(1, 65).reshape(1, 16, 4),
+          exp_output=[
+              [
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [0, 0, 0, 0],
+                  [33, 34, 35, 36],
+                  [37, 38, 39, 40],
+                  [41, 42, 43, 44],
+                  [45, 46, 47, 48],
+                  [49, 50, 51, 52],
+                  [53, 54, 55, 56],
+                  [57, 58, 59, 60],
+                  [61, 62, 63, 64],
+              ],
+          ],
+          n_sparsity=2,
+          m_sparsity=4,
+          block_size=4,
+      ),
+  )
+  def test_block_pruning(
+      self, order, inputs, exp_output, n_sparsity, m_sparsity, block_size
+  ):
+    inputs = jnp.array(inputs)
+    block_mask = sparsity.get_sparsity_mask(
+        inputs, n_sparsity, m_sparsity, order=order, block_size=block_size
+    )
+    output = sparsity.apply_sparsity(inputs, block_mask)
+    np.testing.assert_array_equal(output, exp_output)
+
+
 class PruningScoreTest(parameterized.TestCase):
 
   def test_score_activation_weighted(self):
     weight = jnp.array([[1.0, 2.0, 3.0, 4.0], [-4.0, -3.0, -2.0, -1.0]])
     activation = jnp.array([[1.0, 0.0], [1.0, 0.0], [-1.0, 0.0], [-1.0, 0.0]])
     expected_score = jnp.array([[4.0, 8.0, 12.0, 16.0], [0.0, 0.0, 0.0, 0.0]])
     score = sparsity.compute_score(
```

## Comparing `praxis-1.3.1.dist-info/LICENSE` & `praxis-1.4.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `praxis-1.3.1.dist-info/METADATA` & `praxis-1.4.0.dist-info/METADATA`

 * *Files 11% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 Metadata-Version: 2.1
 Name: praxis
-Version: 1.3.1
+Version: 1.4.0
 Summary: Functionalities such as a layers for building neural networks in Jax.
 Home-page: https://github.com/google/praxis
 Author: PAX team
 Author-email: pax-dev@google.com
 License: Apache-2.0
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Requires-Python: >=3.10
 License-File: LICENSE
-Requires-Dist: absl-py
+Requires-Dist: absl-py (==1.4.0)
 Requires-Dist: chex (>=0.1.85)
 Requires-Dist: clu (==0.0.11)
-Requires-Dist: einops
-Requires-Dist: etils
-Requires-Dist: fiddle (==0.2.11)
-Requires-Dist: flax (==0.8.1)
-Requires-Dist: jax-bitempered-loss
-Requires-Dist: jax (==0.4.24)
-Requires-Dist: jaxtyping
-Requires-Dist: lingvo
-Requires-Dist: numpy
-Requires-Dist: opt-einsum
-Requires-Dist: optax-shampoo
-Requires-Dist: optax (==0.1.9)
+Requires-Dist: einops (==0.7.0)
+Requires-Dist: etils (==1.7.0)
+Requires-Dist: fiddle (==0.3.0)
+Requires-Dist: flax (==0.8.2)
+Requires-Dist: jax-bitempered-loss (==0.0.2)
+Requires-Dist: jax (==0.4.26)
+Requires-Dist: jaxtyping (==0.2.28)
+Requires-Dist: lingvo (==0.12.7)
+Requires-Dist: numpy (==1.26.4)
+Requires-Dist: opt-einsum (==3.3.0)
+Requires-Dist: optax-shampoo (==0.0.6)
+Requires-Dist: optax (==0.2.2)
 Requires-Dist: sentencepiece (==0.1.99)
 Requires-Dist: tensorflow-datasets (==4.8.3)
 Requires-Dist: tensorflow-metadata (==1.12.0)
 Requires-Dist: tensorflow-text (~=2.9.0)
 Requires-Dist: tensorflow (~=2.9.2)
 Requires-Dist: tfds-nightly (==4.8.3.dev202303280045)
-Requires-Dist: typeguard
+Requires-Dist: typeguard (==2.13.3)
 
 UNKNOWN
```

## Comparing `praxis-1.3.1.dist-info/RECORD` & `praxis-1.4.0.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,180 +1,180 @@
 praxis/__init__.py,sha256=yRyT7qjhz2-cckhBdD_4WvjWbUZOBSr7gEo_VbuRdLE,596
 praxis/asserts.py,sha256=AI1pqW-04VQwiCp76Woqhi-6j6bJd6iXwDm7EBX3sxg,22000
 praxis/asserts_test.py,sha256=x50QaEq-_QO9e1xmJmVixFDPiEJOTP6r860Dw3L5Tlg,11563
 praxis/base_hyperparams.py,sha256=5JeMncPN253ZxSFFVU5ajrOHXKd5S5nmP76rXh--n_8,44079
-praxis/base_hyperparams_test.py,sha256=dyZd8t3ZhXk-NriylBWBXlNSu6UNmnFykyrri8Tkvpo,27369
+praxis/base_hyperparams_test.py,sha256=SPu1cF00kDoDeNNTaVaSS1h5oMnS0e1AAQBOLIRIwYU,27429
 praxis/base_input.py,sha256=nVfefZxJ6e7oyuF4s5TZ6Sdn8HuV74h4W9JuUQqEiqI,44661
-praxis/base_input_test.py,sha256=034PV2nzan-enAzZSKDOifV7pO8a_z6hs8XdDYGIvU8,34707
-praxis/base_layer.py,sha256=3EhJmIpv6BU5j3QByDeQLDAUU5suNLT0akLALhxxad0,100806
+praxis/base_input_test.py,sha256=O3IpGO6qwcQEIsMTDK9Qm8wd1n5zieL5SleSmigbCBM,34620
+praxis/base_layer.py,sha256=SCwYeGMwk3pd6ZbCGnB2Bv7biC40CD4_MgT6EpolFaQ,100935
 praxis/base_layer_test.py,sha256=3B-b1KEai5ENq9TduaWmTiOpzcGvaIu3x0dr3JkkH-c,31284
 praxis/base_model.py,sha256=5zcrGSbapXMHcPRwrJvlvIPjFhNlWtNHsMSpeJe1Zrg,5773
 praxis/beam_search.py,sha256=rZX92tXklUUIrd5X2eheDOHWwQHwTtcCV8k7iJLmkDQ,19218
 praxis/beam_search_test.py,sha256=dSU2lBPOJg-qRjc1z5uAcyLVurojWXryR0x_66XG2II,15704
-praxis/decoder_hparams.py,sha256=VVCACWpenqnoRxqN_o1cHFLtWI8jmCzLP3i2UgyHlKw,6514
-praxis/decoder_utils.py,sha256=_d001lmdfupzr_9Yoka22ZZZaWrblDjraj_1PAZZMww,25749
-praxis/decoder_utils_test.py,sha256=f7bHS1KTNn3N8VON_dtOjA0Hjll7FBUltkpc3my0aRQ,11628
+praxis/decoder_hparams.py,sha256=y5a1Zxg5oSIMbj0NlUii6fdzem7M3o_qHcLQE7xpCro,6540
+praxis/decoder_utils.py,sha256=oWeC5rHHbcKqGIaOHaWglIFF3KYsfRZADD5cncKD7Jw,25777
+praxis/decoder_utils_test.py,sha256=dY9EIcP9BIl9TUeZx5U_Tk7FKdjxx5mts8b_Usnf4HE,12772
 praxis/fiddle_tags.py,sha256=9HTB1o8TUQDrnCt_STgirwRtOIMFQu4cdqcWL9iyggw,1475
 praxis/flat_beam_search.py,sha256=DeZE7oY75bqceh6fA-1F1RQYzmxG3oQhJSHUYbDVy8Q,11591
-praxis/flat_beam_search_test.py,sha256=8Pj7oA1OX4pKbINKGTh5q755HV_wmeZ1Q0v_Q3c7FM4,4451
+praxis/flat_beam_search_test.py,sha256=voe4EReciYT6Z7xz5NPG1wn55QBnrlMwUlwPfvEmTHM,4407
 praxis/flax_utils.py,sha256=J34OGy9qqQ0gxA4Mhoyubi3D9BbS_wsgXIwFxtKhhZg,5602
 praxis/gshard_utils.py,sha256=i_KqV37JBob2Q2dGKE7wuaVPew3cEec9s02eqzAojxg,35272
 praxis/lazy_loader.py,sha256=fkRaTuSkRMavbyraF8yOpO-f4B2YkgThsFWp0vMxQTE,2600
 praxis/lingvo_lib.py,sha256=S7KDhY_hVRwtlJP5R0MLwKF1qa__VgFJXqZlk3EJmNk,1555
 praxis/metric_utils.py,sha256=XHfvtuSGeRBkGyQFJZpY7UlLkbBfR9DxgGbUXH6NzNE,2523
 praxis/optimizer_prefix_vectorization.py,sha256=qse2Vg0ymp5cxhGkVwtcIoOIu5xp6rhkpIgnxsygKqs,16997
-praxis/optimizer_prefix_vectorization_test.py,sha256=yMHSd3GZskE7cAa-e-oNpse6xqESeHEswLquisinmBg,3065
+praxis/optimizer_prefix_vectorization_test.py,sha256=7yz2O6Luj_VXPTOAyWoxqx2lGeOtOe1RlPLIUAaRwoE,3196
 praxis/optimizers.py,sha256=Xj75Qh3Bc8moje7PtUNffF8QjPbl4GfySgbBKCt_qqA,113212
 praxis/optimizers_test.py,sha256=UAgx8E4nDQyMrXfDu9WCw_0YrWe1Y7hxiAgNLG65sY0,13883
 praxis/pax_fiddle.py,sha256=mik_KaJFaB5_2_FnGBkAeKqv5Up-qDdnjx4zmPjw9Ao,37119
 praxis/pax_fiddle_test.py,sha256=H5oP0D-DPKX-nb7gUWmDmajb_2UGmr8vEXGGhF5_8k8,41079
-praxis/py_utils.py,sha256=AHfva31056qJTB4yoaf19mWBs02Br0bDmPaqNv8edmQ,39408
-praxis/py_utils_test.py,sha256=Uj1IS2Bkaszd_WEtPnd1Pl-kVK1eo3TphqgunFnLJk4,18672
+praxis/py_utils.py,sha256=UN-3kNmQDjdHLJ6eDCTskHTC5vEA-yI6mBTuHcp66GQ,39434
+praxis/py_utils_test.py,sha256=OO9gymorfkHeHCZBzybqJOGzYXX9DgWJ9qqiqWaFEs8,18905
 praxis/pytypes.py,sha256=yV26TiM48hjPumrcVDFZKj-5UneopNrtjBuvYFSGAuY,5338
 praxis/pytypes_test.py,sha256=xMh5ca1M4zsv3qRw8wjGoHwTWhCeDrBhg5cxxU2oVos,1583
-praxis/sample_decode.py,sha256=TWagYV4geJWkSyKuDXwXjx-NqlQ2lM0VzqO2UWKUgSY,76628
-praxis/sample_decode_test.py,sha256=RhesBGH5I8xo1oW5YQhsJM6dqgTRqYogg16IB-ElcdI,26693
+praxis/sample_decode.py,sha256=XSFZ0i_uaMU4s2sEwlGS3zmE0JGJ8-m3vi4gob5xNGQ,76553
+praxis/sample_decode_test.py,sha256=eIpYF3zb6gfQNLRWf6JqCVHQ75y1qkHcZJ3yE23nGaM,26698
 praxis/schedules.py,sha256=YYieDTayzh0W6rTNqcXK5xUFENLoy73l6awdp_fSMiI,22153
 praxis/schedules_test.py,sha256=sm51wiJrXBbGJyA7IKGd6AnJE3dWmdFOKaci8C-H1Ro,21498
 praxis/test_utils.py,sha256=yykd2e0XYbzY9QIXn7DcxY0CgOyBVI5ttIwS6dpuZwg,18628
 praxis/token_samplers.py,sha256=ULDzoM3joaeeTRLkrMN9w0DjEJM6NHlWrYPQyyJVrB0,24670
-praxis/token_samplers_test.py,sha256=aVXZh6wJwKswS941-kEv02VrKmcfE0IHe9rjxcNKLZo,11224
+praxis/token_samplers_test.py,sha256=rBMQXU1h94A2K81UatkHmAYO0NkvZfWJymYVhcK-xqw,11225
 praxis/trees.py,sha256=hx2peAktti7cf5_U6Zj4gx5q8XuThdik4wHT_GKvyEo,8428
-praxis/trees_test.py,sha256=01wka-sVwL32QFGAiPGHsoAqmjJe8KefeI34Dj8V9H4,12754
+praxis/trees_test.py,sha256=hjhQlrZdZBsZyJcsLxoHE_mG8WnZugMs8sA7tJPuoRg,12764
 praxis/layers/__init__.py,sha256=2hY8NGOVhakimqMd76pp09BjCAt7EfIB3B6YdN-1BhU,6959
 praxis/layers/activations.py,sha256=IM6PfE_21VXoIGbUFtKDbq3i4tGNWC_DQd8YqpBlyBo,5340
 praxis/layers/activations_test.py,sha256=OwXyYWTCLrAU4Lj0zfahVinBgsOy-yzzMIFL-Xek1zY,1858
 praxis/layers/adapters.py,sha256=8417K8nIXaGTQdRtv21skxN6FxiaciSzxXL8K33OPAk,8286
 praxis/layers/adapters_test.py,sha256=uQFpYN2w-RaPfaCdkBbr3xBXbytluysUlwe3QDGJXz0,6878
-praxis/layers/attentions.py,sha256=IAU12l5rUSFd5ASCllc2PuvpWj1KboqxfJZabTX8Bg8,130332
-praxis/layers/attentions_test.py,sha256=dpH9BAqk_33IKXpkdFuyedmeRgti5UC0YkKgb-VUcgE,87739
+praxis/layers/attentions.py,sha256=AixD1MVtqXEjK61F4_xjz22xNv_-jwbxifUgr0GZWy0,130505
+praxis/layers/attentions_test.py,sha256=1RwZsYQX5Hd_fD7rCwUoqDJc8wcT7eET-gmo8cJRNRk,88020
 praxis/layers/augmentations.py,sha256=xbYuLGu-WtVo6LKubwSZC_vxz5gFtPeR2nxbIq4YpJA,7169
-praxis/layers/augmentations_test.py,sha256=zd5XQd83UnXHRHCYndVDfST2qw4gYacSxj8T61ksnKE,8015
+praxis/layers/augmentations_test.py,sha256=BrRSi_U4uVaCA7gvIDtL99agNIwJrB5-M2rWFPjvgH4,8903
 praxis/layers/base_ops.py,sha256=GDC0S5gC2ppdR2fR-15DZ_IZfIDOL8-eZXnfEEHWLZw,1673
 praxis/layers/bregman.py,sha256=MNSJXdN-vzvRBKR4P4JA9oOWe3rklR_TSev7AGfojDQ,12641
 praxis/layers/bregman_test.py,sha256=Vkx2O9iqo-2yHrHr1lSdOoE51_QGfzgEtvyfMsjmq-c,5181
 praxis/layers/checkpoint_policy.py,sha256=NspsvaV3XDZXre9y7znDAwElqV84FWuP6iK3sKoE8AE,4889
 praxis/layers/chunk.py,sha256=ZrQpwlrkzpbSvu5KGAsJ_fQ3D2pMMT5KwiXnoVWITFU,2220
 praxis/layers/chunk_test.py,sha256=ERTbcfaBfUn0-GVFIQGt8MZNnTVpr0oW2M0q9DM_B-w,1472
 praxis/layers/conformers.py,sha256=hfPgzQ01nhlQzxWKgZ6BKqQhNoCWVltNfViS23lJhwE,19084
-praxis/layers/conformers_test.py,sha256=-GVKabbKXYGt-R4M8mLYTHzp8SjmE3z2IYjl5RRakig,11325
+praxis/layers/conformers_test.py,sha256=Xmxq5IlEmmdZvC7AytXB70g0gzM33WNMz4ketRjmpK4,11395
 praxis/layers/convolutions.py,sha256=6N4RhKE-W-l1LB8JNP6ok7Kgd8H05QmX7RZhSOi9COE,34498
 praxis/layers/convolutions_test.py,sha256=WqAufuzz-3QSnQfqENwSu0gB9hR0O9oIKdNtZSh-OoY,19012
-praxis/layers/ctc_objectives.py,sha256=BjnETomjWg_S2cbegUcn67mef8D6JYvVSXMnpO15CG8,17816
-praxis/layers/ctc_objectives_test.py,sha256=YnTxYHQPZk9mRx4UMCT-0-BFZj-i8nViXWsWlc3DuEk,13351
+praxis/layers/ctc_objectives.py,sha256=duOXtTSKvDJlEJt-Km3c9vW_fkgbmuB-XVR3IM87PWg,17834
+praxis/layers/ctc_objectives_test.py,sha256=11NWxLHpeR4qYQw_K2mpVNV4lv3fhX6vd4MjrXZJWHw,13365
 praxis/layers/einsum.py,sha256=F0RZhqR37NetfSsRUAWefboiYbAM1R2My8WnsysLNRc,3364
 praxis/layers/einsum_test.py,sha256=Nb9zDhaFvugd6u1bDhqNlrezVhjfdWYI_VkwfrAFEH8,2143
-praxis/layers/embedding_softmax.py,sha256=B8p7JRAkzVTs6_MQR7vsM-trLhz-A4ZF_Zgx3qPRj4k,50401
+praxis/layers/embedding_softmax.py,sha256=BFlLSDZ9a7twW43GNfBlLYpEcx9HozlvVahzGBDpD6A,50399
 praxis/layers/embedding_softmax_test.py,sha256=iv5_CQe_yZQMm3wX4F1s-Yty4xzQTZdnarb2zzaUwmA,39419
 praxis/layers/flax_adapter.py,sha256=_r1Y8uuJya9DIJiYipEhej0RgKkbRuRyPXQuUaynUY0,6733
-praxis/layers/flax_adapter_test.py,sha256=uMDMt6xBWS6KZS0mpRAUuozaO49UNiybShCDwriUvko,11257
+praxis/layers/flax_adapter_test.py,sha256=coT2ZPre-zwxuC79fIUmE0o9P2XOJYLyVRUMD0WDtlw,11273
 praxis/layers/frnn.py,sha256=my8USYXrawXJDlY7QMPzFnDCjympyKZzCj-PkyFYOck,17194
 praxis/layers/frnn_test.py,sha256=OliYKor0oz3LN-IChLhLQXnBpXcjFId70lFtz_u3HOU,17737
-praxis/layers/glam.py,sha256=e3fSBzB91cyeYRSsgnwWUZ4yll5CQLaj9LNRf_wU5hE,13776
-praxis/layers/gpu_fast_attention.py,sha256=1wdgqB2jTCwCKQPaAGhdPeMERkRibJOEo7a_GpR7jM8,12304
+praxis/layers/glam.py,sha256=32VwHN2BxigSL-pMr6ohYFc0eAVKKno5-Swz42R-3V4,13988
+praxis/layers/gpu_fast_attention.py,sha256=YhjhqEkuYg1T3NShG7kRcbHtJHBPzqoJ52UI4Rf3Fcg,17268
 praxis/layers/grouped_query_attention.py,sha256=Z6Md0I8BQlLEaTY-klc5mkhBhz5g7PwyUQX1EfMIgIE,16941
 praxis/layers/grouped_query_attention_test.py,sha256=WDPEO32b-hrKA5_I0NpQkn3Svw7MhZTdteLe_AD6-kc,6067
 praxis/layers/linears.py,sha256=hybBySc3jBuaPDNxFugkodosZreY5NG4G1RV_XT09G0,18696
 praxis/layers/linears_test.py,sha256=jU30PcXpzYVP48gtCo4yE5_5bvzqC4AJdKNCEysnwh0,29853
 praxis/layers/losses.py,sha256=ktTuvTywNtK7trNGOC13f3QO-Mvroctwux0vhxj39xM,3826
-praxis/layers/models.py,sha256=smoCOt4BC95HVahQ75NQ59Ux_kYI5EUee4N-IZIY0aQ,79712
-praxis/layers/models_test.py,sha256=Fv_dYhVjVE4HZOce6oFQ38ujYODova5o-WIAreH44PQ,56657
-praxis/layers/multi_query_attention.py,sha256=5JrsdcF-lzdjcy9CYuHsTQkJu8Ca76J_NT7WtXHgclI,65534
+praxis/layers/models.py,sha256=FeG7OxT0r-SbBJpyKy2gCkb-ozGFZEHms29v0HYIWxM,78696
+praxis/layers/models_test.py,sha256=xmBju_29cLrx2yOnc5k1kaTL3ZFyT6siO_sIbc8UZZA,57477
+praxis/layers/multi_query_attention.py,sha256=zp9HJ1RxOmDpi9krbwcgJDgNq9MU-NHvMGic5jfCC0Y,65639
 praxis/layers/multi_query_attention_test.py,sha256=2gScrXW-sEuUbeAQN1WYmcF0buGyYHOIQvNhiFORfSg,28992
 praxis/layers/ngrammer.py,sha256=aXre09FgYaVLzXmSY8bbXjqjFaTr4zGVtjT0BKkGv-k,51858
-praxis/layers/ngrammer_test.py,sha256=YikWMZfOL6mXSCEWjB764WAOthbaToeyVVlqyuf6msw,26471
+praxis/layers/ngrammer_test.py,sha256=A5Zv_vSjMbIlqyWuFG5lwdnty2kEV1fI9kZLHfotVAo,26513
 praxis/layers/normalizations.py,sha256=N5JhouAxCJpV5g3sBSSrKwFP6VZa1gSs5aMGtwgIMpM,22231
 praxis/layers/normalizations_test.py,sha256=yHnYhjzKiibfgeX8BeKtJfFWM6_XwtK4qf3cMMbXrFc,19447
 praxis/layers/pipeline.py,sha256=gmVfmKBZhHORQa4O8tDQ7yu1hmAk2i8Ufc_ce1zEyFA,48753
 praxis/layers/poolings.py,sha256=Rutv1xrIYgGVs-XCGnrNpCiDy_TDZ9IKo9vNUj9RIVY,13518
 praxis/layers/poolings_test.py,sha256=Yl8LR_5u1ZN5c5bMifqzdgXZwT7HDXsjNTfh0inTxPI,9528
 praxis/layers/quantizer.py,sha256=AHMeRy-p_Qb4PzuclZvdQQ-b50IQuWJbz2cLIt2pSmE,16337
 praxis/layers/quantizer_objectives.py,sha256=ZhoGMBxYiaAQXyvO68iC8uJpDQU93NU53VTBIyTBGUM,3892
-praxis/layers/quantizer_objectives_test.py,sha256=o67_dtAKn5GnTYDUAgXAr1mY1cHjTX0_ReI-xFfDva0,1525
+praxis/layers/quantizer_objectives_test.py,sha256=YH8lI1fosaYE1PUl9R3TaOqZCWpUQPX2dhbrUgrvMfA,1690
 praxis/layers/quantizer_test.py,sha256=EoAwcJo0-vi2zQ24sPbXl5V4o2imSa3jQU6DckTyTcE,4351
 praxis/layers/repeats.py,sha256=SHl4zIUAjfURls7jHo5Rt8Jq6uz36afQLQ1S3tRW6ww,23959
-praxis/layers/repeats_test.py,sha256=v_M3bcVVpyxoml7BvHo_df2hXQru7o5gNIp66Be4tsU,16308
+praxis/layers/repeats_test.py,sha256=YZZbyKEegkmceymXcO3fgpZXBf5eNfX0cq7iPjroWtU,16378
 praxis/layers/resnets.py,sha256=AO4itTc7kbCr15OFaXDUYtW-XWtHaIXKhLoY5jcNbdE,16250
 praxis/layers/rnn_cell.py,sha256=ZsQbZJy8XH7QdhoXF3daEyWySDufWxIGzCoN6HEoELo,17426
 praxis/layers/rnn_cell_test.py,sha256=cidbSZleSwlLi34TRWtCukALY99UKuY14pMqM44YCQ0,13660
 praxis/layers/searchable.py,sha256=BmZKDth0EXEbVhhcnz3A7g2HwHtHbPI8YiWzRyhm-ng,2290
 praxis/layers/searchable_test.py,sha256=F_XpvaGyYWaKfKq1fTb3UATowsN4er1g-KwsgeMfqTo,1910
 praxis/layers/sequential.py,sha256=8iPzdH44WmGabKEtLbXj6qEzZZeOwV7pGg1xQ5M2Wuk,1270
 praxis/layers/sequential_test.py,sha256=EhKQkCk3Egd04wwHSffMXtK1V5q2eEmc9lH3-xMyFis,3167
-praxis/layers/sharding.py,sha256=5gcAdsMgdNcXqpREQ4HXMBqejT94AFoyLZEQOYwH_Zc,4640
+praxis/layers/sharding.py,sha256=LBW7yJrKjhIYD26Sb_SPgMDfPizuBbvXnUHyA3mwOXw,4657
 praxis/layers/shared_layers_test.py,sha256=Qb-faxRUx_9JtHlulbLxfvaCLWtId7xSUYTSvgJtPqw,9571
 praxis/layers/spectrum_augmenter.py,sha256=MGb9GEkn7KVU9KHZ8Lk2R88A3aWKxrr7-89TuCTbF6o,10049
 praxis/layers/spectrum_augmenter_test.py,sha256=80dg-qJjpPmu8fUkP7YoOsPLWyl51NZYSjqHXvz5BFw,6714
 praxis/layers/ssm.py,sha256=-k72sWCr3zA-Gxgjr1zKNttmZbW3ZHQRqfUviJx4j84,13853
 praxis/layers/ssm_test.py,sha256=73G0-MLPvdZu4aF3o_uPgCP0KWKvhlx-fW75o4bZpSc,4454
 praxis/layers/ssm_transformers.py,sha256=1gsPBnnrbFoJtFWOCnJ2lM3Q0z9AAyeijPP0mpZpJfI,26821
 praxis/layers/ssm_transformers_test.py,sha256=7n6qdNGCbXI-El8-qrtKXN3C_92eln6e2G_l4G2Ftwk,5695
 praxis/layers/stats.py,sha256=uaCXihIB-XcUq4YouZCBJuLU2XMd1Fn3_IccpdL_nJQ,1537
 praxis/layers/stats_test.py,sha256=aAhFRqh-YGqEUWseyRfRDpPBBzaOpW_-a263kI5JI3I,1258
 praxis/layers/stochastics.py,sha256=SkfgpAYntDMgaYtQItvYL7cCu8B0DlDmytVSLw_m_RI,5159
 praxis/layers/stochastics_test.py,sha256=5T4keo-a2J4KG6leOn5XynYgma1GerHjLefpIpfRT_g,4517
 praxis/layers/test_layers.py,sha256=meIGQI033WbhbKcYlKilfIAIjimfviUUic1WBLUsl6o,7898
-praxis/layers/transformer_models.py,sha256=wzNE8wJjm4uoJTrUJB355k1CsQnfYBx_IhxMabmoF7s,77009
+praxis/layers/transformer_models.py,sha256=ZxwVFsinU-oejxuhXLlcWn8vY9RJ8EWjX6Lvplx7NHc,80038
 praxis/layers/transformer_models_encoder_decoder_test.py,sha256=MuoOoJgLsEMetdNAUE_cvk5mZ5qmt3cDtTpHSPgXaUU,9529
 praxis/layers/transformer_models_test.py,sha256=IVSgjjWkH1vsnbV1UgmzI0IDcmaj2luP0vF8upyRz50,58376
 praxis/layers/transformers.py,sha256=KHhSYcz3J-Eu7DJHY4OCg66R3A559OoEo2rP2_pJg2c,93733
-praxis/layers/transformers_test.py,sha256=ghhfSS1visBC5_4RhIF8uaiZKiwpHiipzIwDnGYTmi0,66804
+praxis/layers/transformers_test.py,sha256=bl5Vik0UAt5NZ-73lr3pfZHKduFkUUVJAcMhsQWo3MI,67654
 praxis/layers/vanillanets.py,sha256=UvjPtbiJdaj2VK81LBLEHclakhQPTBTk2osQYfkPsvQ,10068
 praxis/layers/vanillanets_test.py,sha256=2mT68jfnoTsOeRkLrT-2E3O9oNklucxnSlshl6AhVLc,2836
 praxis/layers/vits.py,sha256=vMtTKWnKLhQZjiXNIFw0W_lnyiNZCTjFGG2YW8qk_XI,18433
 praxis/layers/vits_test.py,sha256=FPB3hLOv0Q4d5-3hIhA389nZpMqZyeTfuFf1UFeAcmU,9661
 praxis/layers/chain/__init__.py,sha256=K9-2VKZPc1r62iDuzeSgENX8Xysnjq9R75CAizF4prU,1343
 praxis/layers/chain/chain.py,sha256=BPpiWrr67z72KwKKe-wbYHKSG7V_gOhp1hQyaDQ-RA4,5210
 praxis/layers/chain/chain_extensions.py,sha256=0tFGP9ME3cNwxveYU_a1UoTI9LKI9nEnJsHdHG41TXc,8837
-praxis/layers/chain/chain_test.py,sha256=r_RiZAHKvh3biNbI5tqF6iGdJZyrt4CJtpMX6Prv7iM,6887
+praxis/layers/chain/chain_test.py,sha256=xaysnpVH7D7qSHnRWbPPiKq3y2_XJuWs_d0-krAlegU,6913
 praxis/layers/injection/__init__.py,sha256=5epthk7t7Wb4jqn0zlhJZDt9l0jFD0V1p3_PuBRC2lA,710
 praxis/layers/injection/fp8_nvidia_gpu.py,sha256=40xT-6-dW9NcFKp07qiUfXlpNn_S3qcDfhGFY9LeXPU,3718
 praxis/layers/injection/fp8_nvidia_gpu_test.py,sha256=XVwM8bTLTn9O1fDsrpTrTgom2Zl8i780D7yffg9ivg4,3603
-praxis/layers/quantization/__init__.py,sha256=EGOHKJyajmLK5R6xhYOqVqBw7kDWNiQ4PUH-mCD8U9o,2041
-praxis/layers/quantization/attentions.py,sha256=qMAtkB2CUyWWhyTS5N9_bjqPmIcpKcby0FcrRXpsdk0,31334
-praxis/layers/quantization/attentions_test.py,sha256=Gj0KhwPzs-pcUS3v9J_nb_jFYR3pT9jCaUU6RdtijHI,20575
+praxis/layers/quantization/__init__.py,sha256=42CmAtm6P7l6daATWj4CGHW_SNqoVy34FMRIXRQCl4k,2173
+praxis/layers/quantization/attentions.py,sha256=DEecoqFdehEseuFxSGFJXucs7ZBeg3NFnEiESsVasqU,38463
+praxis/layers/quantization/attentions_test.py,sha256=t5jrr1FAVSfPL4NyoH0WR947ap-eRai20ezCGWezNio,23701
 praxis/layers/quantization/automl_select.py,sha256=cTdo4CCJyLSdSETPPC9Zy4HFYY1A6rfki_sBc-wNNZU,3101
 praxis/layers/quantization/automl_select_test.py,sha256=WgEWWQrY54CI0naaNm_h-VWKyPHt8phtfE61YPjW6Uc,1937
 praxis/layers/quantization/conformers.py,sha256=3pf73pv1OO6TCdYP-tH8u0t-rr3ZfPZDfzzaMMWJIHc,2703
-praxis/layers/quantization/conformers_test.py,sha256=LmmoMi_PAqr1DLiUhQoHwtfi6b9o0R71PENNL8G-nNc,3174
+praxis/layers/quantization/conformers_test.py,sha256=KePgABNvbyjmTufJ80K1imTAgS9ig1uZlJodfVMN1ZM,3217
 praxis/layers/quantization/convolutions.py,sha256=azQ4J6CiRya4JcejIQerGo54hmXrR-QPZVhvjdk1Axc,4105
 praxis/layers/quantization/convolutions_test.py,sha256=ss8Pz2tGlsX3o0tWI0kDQIDHGcFegBzcCkWlUVbs7jc,3698
 praxis/layers/quantization/einsum.py,sha256=sa83iEu8G69vCmiMScu-iyo4_ESl0QyTOxe6Oao44nI,3349
 praxis/layers/quantization/einsum_test.py,sha256=nttDQe-UiPVliJcnYmBPm9MXEeMzgQZzziknmR3z170,2338
 praxis/layers/quantization/embedding_softmax.py,sha256=1p-cWjAcpJrTIZQMwQ1lKRTDDxWWbxHwAkPKmMzKNYM,23612
 praxis/layers/quantization/embedding_softmax_test.py,sha256=2_krJMmayqluwi9k-EkP5dbTlsHFjwNuZjR-hUisWyY,20031
-praxis/layers/quantization/linears.py,sha256=vN-Wvq_UiqTcUo_BFpaqAVQJdSfcIBia_Vyz0KAcmOM,13073
-praxis/layers/quantization/linears_test.py,sha256=34d6GiTVULKGG774xmnEDP3VpfLIf6TkpFdnsI4hlEs,26152
-praxis/layers/quantization/multi_query_attention.py,sha256=qsAsJmZ0ykZg74t1EzVZbRW3l8HZuDztqMnUGTRiVxw,7762
+praxis/layers/quantization/linears.py,sha256=9ekV9C8k8rvT14ub8bvk97pkZf5VE5_Q6J65szCDUDg,18061
+praxis/layers/quantization/linears_test.py,sha256=mEAfQ3poKpb_KERtZtIQSG3HKQzaHMVmvk2c6opL660,27071
+praxis/layers/quantization/multi_query_attention.py,sha256=G4jsmBSqtIwtNFQdVxj8FZmlISrmJA81KetdWzTv-s8,7833
 praxis/layers/quantization/multi_query_attention_test.py,sha256=9Nw7KkDyAxGkuui1fP5D7B65wiO64A_v1ZuitscZArc,4889
 praxis/layers/quantization/ngrammer.py,sha256=vQdX5iuZpkkbYUB9pIUqUZGub18Zqx0DkhY1srm9ZsY,4853
 praxis/layers/quantization/ngrammer_test.py,sha256=iPecN4udH2yXjdx1ubGuj9_8Rqcr1XURGhbLSETCY84,17527
-praxis/layers/quantization/operations.py,sha256=9xGAu2A4-AJMtF2LnZl_ORybTgyHmih2LCLWRzv-Y0c,34582
-praxis/layers/quantization/operations_test.py,sha256=SyACX22rD2btnjX7gTXCttcbenTIxzbSuw3DdaJltGQ,31830
+praxis/layers/quantization/operations.py,sha256=d4-UUSbOhUIRd4mSFoxp2cRkjqmIVdVQj6PjABI6c5w,38313
+praxis/layers/quantization/operations_test.py,sha256=aJIA_BnJzNf1CXB6Wl29Oblg9b2Izjm12MObNjKIJa0,37067
 praxis/layers/quantization/optimization.py,sha256=H8MygGFt9r_dK7ydIhv153eUvGlVmoXCzjmRkhnSZvQ,5601
 praxis/layers/quantization/optimization_test.py,sha256=U9_bSMmTnJXzj9gQTrcgQC60jL-WloQtBbeLk7kPXI8,2479
 praxis/layers/quantization/overflow_check.py,sha256=5Hb3V6bSYczYZczl8MoTXM-WPc0pAHNFm2YOWO224Ug,5388
 praxis/layers/quantization/overflow_check_test.py,sha256=2-JnLZRPsK4j6v7mPDmwWiGGHq-wT38VwieX5rbhmQ0,6063
-praxis/layers/quantization/quantization_hparams.py,sha256=IbPny3DBN-iiuEQ9mXOhQEPnKtJl7JNrMXIJt67lZM8,7911
+praxis/layers/quantization/quantization_hparams.py,sha256=RfkMuN-JY7NtZmGS0O1wZkeCPDbZIycVz3Eo7LzNz1w,9082
 praxis/layers/quantization/quantization_test.py,sha256=jYwZn9VXfO-CR1vHB7OyfmvZcFk7CPHJ3hN0hWpMVfc,5626
-praxis/layers/quantization/quantize.py,sha256=K9vKta1jqE77uh38tCdJYVHsa-ZMQ4-lp5hx02NDFEY,30116
+praxis/layers/quantization/quantize.py,sha256=2YkBK8gg4sF9GvOvDVTzkjIOI9jDuDA69rDYls_IvWA,40176
 praxis/layers/quantization/quantize_test.py,sha256=JqjSxRXWpOPqq3fUz7PiXl8hGWHL20JYyzfMgBLQjVs,18578
-praxis/layers/quantization/quantizer.py,sha256=zEgICuce1kBQrcQiRGbXANCVRKciG2qj0UpQ5YavVB8,29519
-praxis/layers/quantization/quantizer_test.py,sha256=Pn2LjwimX0TWn6UBpvJPlXZ1ljzufzy1mP89tgldOOg,23801
+praxis/layers/quantization/quantizer.py,sha256=r7bggk2iKIIyV1vBXAY1pBWf7TVMygLl1Rt4ilH-D1M,29590
+praxis/layers/quantization/quantizer_test.py,sha256=iOCM2vmfid1Xp1sTsfbSjfQpjAz9vGc3-D3Etn9hxgs,23806
 praxis/layers/quantization/searchable.py,sha256=itGSYWW6si2Dw6_ZyV-2KqrfaEdYDcwhPdIAWDkNCss,3965
-praxis/layers/quantization/searchable_test.py,sha256=w0_UMrhFWVUUJeGtxhVAQLwWpy-E81EwwxNY74mTJzE,3786
-praxis/layers/quantization/utils.py,sha256=7pu_tnP3N9Ts22vpj9CEJwXx2at7VY-hK5ea1rsImpg,12190
-praxis/layers/quantization/utils_test.py,sha256=RnKYoh4LFtbzPDdk-ADUageWGTLeQzUmNSdGzp-VY0c,7667
+praxis/layers/quantization/searchable_test.py,sha256=vhfnGX6nh2d7UYoo0XAIOw15CzCFWzeoCGkcer5C744,3790
+praxis/layers/quantization/utils.py,sha256=yyZWSw4nqLmEmzTlC_dug4Wn2SQY0m4GeJeSuLDEJR4,18029
+praxis/layers/quantization/utils_test.py,sha256=2MuDULUevdFYt-UyYUBkqdPfP2PVcmw8uabm-d-3XBk,10211
 praxis/layers/quantization/sparsity/__init__.py,sha256=HB1NQ5f1jkelS-Vycrf4qYCo6foxYs9gp4umxo8Rlqw,644
 praxis/layers/quantization/sparsity/attentions_test.py,sha256=rWItjEyNE3XzXJgUiy7HlpeJo4JT3RQCnlRgqADqMZk,15376
 praxis/layers/quantization/sparsity/linears_test.py,sha256=btNGUNLbbCmWMy8sDhq9hxDz00LY-sYCAb325MGq8Cc,10473
-praxis/layers/quantization/sparsity/sparsifier.py,sha256=KVn6dfbopPDWQjTTqOd-SQI4IXoixYbKWXHfypAQex0,19054
-praxis/layers/quantization/sparsity/sparsifier_test.py,sha256=TrdrJBPaCsXPBmPxAvU9NrWVumjyNWIia-BZ69PQZmo,42196
-praxis/layers/quantization/sparsity/sparsity.py,sha256=ay6AUvvwICqV0Sh1o7_ntxg7rNOJDWUPtaSoJHkt-PM,9579
-praxis/layers/quantization/sparsity/sparsity_hparams.py,sha256=zrLiB5IxvUNUe8oUIftbhpRelyZnxb3h1ubt8hXoCI8,8299
+praxis/layers/quantization/sparsity/sparsifier.py,sha256=phsyfdFhkNL2jA15CNowkV88Kf3TJcIAfh3OZoffJCo,19155
+praxis/layers/quantization/sparsity/sparsifier_test.py,sha256=qrIXJDrWo6JY74GwiyBPH6Hj3FL5DL7VjgkUm-j8T-g,44642
+praxis/layers/quantization/sparsity/sparsity.py,sha256=-dmnH0QWu3dhJSqKgVda2ocbDXzTqBOP42qILj0h3Do,13598
+praxis/layers/quantization/sparsity/sparsity_hparams.py,sha256=XNezO_f1WFKLPlpyHrQdAxDLQYDZW7zlqjJnlUSh8dE,9407
 praxis/layers/quantization/sparsity/sparsity_modes.py,sha256=S7tUq2U6cFB65UEXb8PK_q7GgYbw-LkfWGXLB58i7zQ,5898
-praxis/layers/quantization/sparsity/sparsity_test.py,sha256=QJZtXzrvdGX54G7Ni0gyCozgXgXSFl1sfIKY3IewZj0,12254
-praxis-1.3.1.dist-info/LICENSE,sha256=WNHhf_5RCaeuKWyq_K39vmp9F28LxKsB4SpomwSZ2L0,11357
-praxis-1.3.1.dist-info/METADATA,sha256=_GsHD6HVSYr0rlllvOWfutQocgs16zhGmLiq_R1ongQ,1115
-praxis-1.3.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-praxis-1.3.1.dist-info/top_level.txt,sha256=jQcnqGC96gVr2kN5zV78HwY1gNcsZ-8JnoLEWogxu1g,7
-praxis-1.3.1.dist-info/RECORD,,
+praxis/layers/quantization/sparsity/sparsity_test.py,sha256=GdhEku5d11RV45ToIA5r-js52Vw_cdUr6QavPAbLwNg,19317
+praxis-1.4.0.dist-info/LICENSE,sha256=WNHhf_5RCaeuKWyq_K39vmp9F28LxKsB4SpomwSZ2L0,11357
+praxis-1.4.0.dist-info/METADATA,sha256=X2akryVtvH-cmqHviMYX00Je-72Ryy27xL7QxjFNaLE,1218
+praxis-1.4.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+praxis-1.4.0.dist-info/top_level.txt,sha256=jQcnqGC96gVr2kN5zV78HwY1gNcsZ-8JnoLEWogxu1g,7
+praxis-1.4.0.dist-info/RECORD,,
```

