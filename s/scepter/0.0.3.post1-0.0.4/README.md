# Comparing `tmp/scepter-0.0.3.post1-py3-none-any.whl.zip` & `tmp/scepter-0.0.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,280 +1,300 @@
-Zip file size: 530001 bytes, number of entries: 278
--rw-r--r--  2.0 unx      602 b- defN 24-Feb-29 12:19 scepter/__init__.py
--rw-r--r--  2.0 unx      211 b- defN 24-Feb-29 12:18 scepter/version.py
--rw-r--r--  2.0 unx    12618 b- defN 24-Feb-29 12:18 scepter/methods/examples/classification/example.yaml
--rw-r--r--  2.0 unx     5383 b- defN 24-Feb-29 12:18 scepter/methods/examples/generation/stable_diffusion_1.5_512.yaml
--rw-r--r--  2.0 unx     5568 b- defN 24-Feb-29 12:18 scepter/methods/examples/generation/stable_diffusion_1.5_512_lora.yaml
--rw-r--r--  2.0 unx     4792 b- defN 24-Feb-29 12:18 scepter/methods/examples/generation/stable_diffusion_2.1_512.yaml
--rw-r--r--  2.0 unx     4982 b- defN 24-Feb-29 12:18 scepter/methods/examples/generation/stable_diffusion_2.1_512_lora.yaml
--rw-r--r--  2.0 unx     4789 b- defN 24-Feb-29 12:18 scepter/methods/examples/generation/stable_diffusion_2.1_768.yaml
--rw-r--r--  2.0 unx     4978 b- defN 24-Feb-29 12:18 scepter/methods/examples/generation/stable_diffusion_2.1_768_lora.yaml
--rw-r--r--  2.0 unx     9088 b- defN 24-Feb-29 12:18 scepter/methods/examples/generation/stable_diffusion_xl_1024.yaml
--rw-r--r--  2.0 unx     9277 b- defN 24-Feb-29 12:18 scepter/methods/examples/generation/stable_diffusion_xl_1024_lora.yaml
--rw-r--r--  2.0 unx     6440 b- defN 24-Feb-29 12:18 scepter/methods/scedit/ctr/sd15_512_sce_ctr_hed.yaml
--rw-r--r--  2.0 unx     6315 b- defN 24-Feb-29 12:18 scepter/methods/scedit/ctr/sd21_768_sce_ctr_canny.yaml
--rw-r--r--  2.0 unx     6434 b- defN 24-Feb-29 12:18 scepter/methods/scedit/ctr/sd21_768_sce_ctr_pose.yaml
--rw-r--r--  2.0 unx    10042 b- defN 24-Feb-29 12:18 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_canny.yaml
--rw-r--r--  2.0 unx    10058 b- defN 24-Feb-29 12:18 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color.yaml
--rw-r--r--  2.0 unx    10420 b- defN 24-Feb-29 12:18 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color_datatxt.yaml
--rw-r--r--  2.0 unx    10135 b- defN 24-Feb-29 12:18 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_depth.yaml
--rw-r--r--  2.0 unx     5639 b- defN 24-Feb-29 12:18 scepter/methods/scedit/t2i/sd15_512_sce_t2i.yaml
--rw-r--r--  2.0 unx     5611 b- defN 24-Feb-29 12:18 scepter/methods/scedit/t2i/sd15_512_sce_t2i_swift.yaml
--rw-r--r--  2.0 unx     5045 b- defN 24-Feb-29 12:18 scepter/methods/scedit/t2i/sd21_768_sce_t2i.yaml
--rw-r--r--  2.0 unx     5017 b- defN 24-Feb-29 12:18 scepter/methods/scedit/t2i/sd21_768_sce_t2i_swift.yaml
--rw-r--r--  2.0 unx     9370 b- defN 24-Feb-29 12:18 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i.yaml
--rw-r--r--  2.0 unx     9514 b- defN 24-Feb-29 12:18 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_datatxt.yaml
--rw-r--r--  2.0 unx     9324 b- defN 24-Feb-29 12:18 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_swift.yaml
--rw-r--r--  2.0 unx     2275 b- defN 24-Feb-29 12:18 scepter/methods/studio/scepter_ui.yaml
--rw-r--r--  2.0 unx     1707 b- defN 24-Feb-29 12:18 scepter/methods/studio/extensions/controllers/official_controllers.yaml
--rw-r--r--  2.0 unx   294732 b- defN 24-Feb-29 12:18 scepter/methods/studio/extensions/mantra_book/mantra_book.yaml
--rw-r--r--  2.0 unx    29045 b- defN 24-Feb-29 12:18 scepter/methods/studio/extensions/tuners/official_tuners.yaml
--rw-r--r--  2.0 unx     2833 b- defN 24-Feb-29 12:18 scepter/methods/studio/home/home.yaml
--rw-r--r--  2.0 unx     3244 b- defN 24-Feb-29 12:18 scepter/methods/studio/inference/inference.yaml
--rw-r--r--  2.0 unx    10136 b- defN 24-Feb-29 12:18 scepter/methods/studio/inference/sdxl/sdxl1.0_pro.yaml
--rw-r--r--  2.0 unx     2880 b- defN 24-Feb-29 12:18 scepter/methods/studio/inference/stable_diffusion/sd15_pro.yaml
--rw-r--r--  2.0 unx     2788 b- defN 24-Feb-29 12:18 scepter/methods/studio/inference/stable_diffusion/sd21_pro.yaml
--rw-r--r--  2.0 unx      148 b- defN 24-Feb-29 12:18 scepter/methods/studio/preprocess/preprocess.yaml
--rw-r--r--  2.0 unx      188 b- defN 24-Feb-29 12:18 scepter/methods/studio/self_train/self_train.yaml
--rw-r--r--  2.0 unx    27890 b- defN 24-Feb-29 12:18 scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml
--rw-r--r--  2.0 unx     7598 b- defN 24-Feb-29 12:18 scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml
--rw-r--r--  2.0 unx     6103 b- defN 24-Feb-29 12:18 scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml
--rw-r--r--  2.0 unx      187 b- defN 24-Feb-29 12:18 scepter/modules/__init__.py
--rw-r--r--  2.0 unx      628 b- defN 24-Feb-29 12:18 scepter/modules/annotator/__init__.py
--rw-r--r--  2.0 unx     2095 b- defN 24-Feb-29 12:18 scepter/modules/annotator/base_annotator.py
--rw-r--r--  2.0 unx     2474 b- defN 24-Feb-29 12:18 scepter/modules/annotator/canny.py
--rw-r--r--  2.0 unx     2159 b- defN 24-Feb-29 12:18 scepter/modules/annotator/color.py
--rw-r--r--  2.0 unx     6346 b- defN 24-Feb-29 12:18 scepter/modules/annotator/hed.py
--rw-r--r--  2.0 unx      767 b- defN 24-Feb-29 12:18 scepter/modules/annotator/identity.py
--rw-r--r--  2.0 unx      769 b- defN 24-Feb-29 12:18 scepter/modules/annotator/invert.py
--rw-r--r--  2.0 unx     3261 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas_op.py
--rw-r--r--  2.0 unx     2891 b- defN 24-Feb-29 12:18 scepter/modules/annotator/mlsd_op.py
--rw-r--r--  2.0 unx    35386 b- defN 24-Feb-29 12:18 scepter/modules/annotator/openpose.py
--rw-r--r--  2.0 unx     1223 b- defN 24-Feb-29 12:18 scepter/modules/annotator/registry.py
--rw-r--r--  2.0 unx     3503 b- defN 24-Feb-29 12:18 scepter/modules/annotator/utils.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/__init__.py
--rw-r--r--  2.0 unx     5214 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/api.py
--rw-r--r--  2.0 unx      391 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/base_model.py
--rw-r--r--  2.0 unx    11717 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/blocks.py
--rw-r--r--  2.0 unx     3205 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/dpt_depth.py
--rw-r--r--  2.0 unx     2769 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/midas_net.py
--rw-r--r--  2.0 unx     6019 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/midas_net_custom.py
--rw-r--r--  2.0 unx     8178 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/transforms.py
--rw-r--r--  2.0 unx     4691 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/utils.py
--rw-r--r--  2.0 unx    15580 b- defN 24-Feb-29 12:18 scepter/modules/annotator/midas/vit.py
--rw-r--r--  2.0 unx        0 b- defN 24-Feb-29 12:18 scepter/modules/annotator/mlsd/__init__.py
--rw-r--r--  2.0 unx    10166 b- defN 24-Feb-29 12:18 scepter/modules/annotator/mlsd/mbv2_mlsd_large.py
--rw-r--r--  2.0 unx     9706 b- defN 24-Feb-29 12:18 scepter/modules/annotator/mlsd/mbv2_mlsd_tiny.py
--rw-r--r--  2.0 unx    25395 b- defN 24-Feb-29 12:18 scepter/modules/annotator/mlsd/utils.py
--rw-r--r--  2.0 unx      125 b- defN 24-Feb-29 12:18 scepter/modules/data/__init__.py
--rw-r--r--  2.0 unx      599 b- defN 24-Feb-29 12:18 scepter/modules/data/dataset/__init__.py
--rw-r--r--  2.0 unx     3941 b- defN 24-Feb-29 12:18 scepter/modules/data/dataset/base_dataset.py
--rw-r--r--  2.0 unx     8960 b- defN 24-Feb-29 12:18 scepter/modules/data/dataset/dataset.py
--rw-r--r--  2.0 unx    11650 b- defN 24-Feb-29 12:18 scepter/modules/data/dataset/ms_dataset.py
--rw-r--r--  2.0 unx    15112 b- defN 24-Feb-29 12:18 scepter/modules/data/dataset/registry.py
--rw-r--r--  2.0 unx      771 b- defN 24-Feb-29 12:18 scepter/modules/data/dataset/utils.py
--rw-r--r--  2.0 unx      407 b- defN 24-Feb-29 12:18 scepter/modules/data/sampler/__init__.py
--rw-r--r--  2.0 unx     1002 b- defN 24-Feb-29 12:18 scepter/modules/data/sampler/base_sampler.py
--rw-r--r--  2.0 unx     2013 b- defN 24-Feb-29 12:18 scepter/modules/data/sampler/registry.py
--rw-r--r--  2.0 unx    20503 b- defN 24-Feb-29 12:18 scepter/modules/data/sampler/sampler.py
--rw-r--r--  2.0 unx      151 b- defN 24-Feb-29 12:18 scepter/modules/inference/__init__.py
--rw-r--r--  2.0 unx     5014 b- defN 24-Feb-29 12:18 scepter/modules/inference/control_inference.py
--rw-r--r--  2.0 unx    30755 b- defN 24-Feb-29 12:18 scepter/modules/inference/diffusion_inference.py
--rw-r--r--  2.0 unx    11183 b- defN 24-Feb-29 12:18 scepter/modules/inference/tuner_inference.py
--rw-r--r--  2.0 unx      218 b- defN 24-Feb-29 12:18 scepter/modules/model/__init__.py
--rw-r--r--  2.0 unx     4022 b- defN 24-Feb-29 12:18 scepter/modules/model/base_model.py
--rw-r--r--  2.0 unx     1653 b- defN 24-Feb-29 12:18 scepter/modules/model/registry.py
--rw-r--r--  2.0 unx      202 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/__init__.py
--rw-r--r--  2.0 unx      224 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/autoencoder/__init__.py
--rw-r--r--  2.0 unx    11633 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/autoencoder/ae_module.py
--rw-r--r--  2.0 unx     9569 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/autoencoder/ae_utils.py
--rw-r--r--  2.0 unx      445 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/__init__.py
--rw-r--r--  2.0 unx     2791 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/mlp.py
--rw-r--r--  2.0 unx     3235 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/resnet.py
--rw-r--r--  2.0 unx    21928 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/resnet_impl.py
--rw-r--r--  2.0 unx     1625 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/timm_model.py
--rw-r--r--  2.0 unx     8783 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/vit_modify.py
--rw-r--r--  2.0 unx      133 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/utils/__init__.py
--rw-r--r--  2.0 unx    21405 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/utils/clip.py
--rw-r--r--  2.0 unx     5109 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/utils/simple_tokenizer.py
--rw-r--r--  2.0 unx    21302 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/image/utils/vit.py
--rw-r--r--  2.0 unx      148 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/unet/__init__.py
--rw-r--r--  2.0 unx    38189 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/unet/unet_module.py
--rw-r--r--  2.0 unx    36274 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/unet/unet_utils.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/utils/__init__.py
--rw-r--r--  2.0 unx     2031 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/utils/transformer.py
--rw-r--r--  2.0 unx      318 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/__init__.py
--rw-r--r--  2.0 unx     7081 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/init_helper.py
--rw-r--r--  2.0 unx    30685 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/resnet_3d.py
--rw-r--r--  2.0 unx    13371 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/video_transformer.py
--rw-r--r--  2.0 unx      683 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/__init__.py
--rw-r--r--  2.0 unx     1797 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/base_branch.py
--rw-r--r--  2.0 unx     4466 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/csn_branch.py
--rw-r--r--  2.0 unx     3405 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/non_local.py
--rw-r--r--  2.0 unx     5942 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/r2d3d_branch.py
--rw-r--r--  2.0 unx     8046 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/r2plus1d_branch.py
--rw-r--r--  2.0 unx    11916 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/tada_conv.py
--rw-r--r--  2.0 unx    12202 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/transformer_branch.py
--rw-r--r--  2.0 unx     2530 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/visualize_3d_module.py
--rw-r--r--  2.0 unx      575 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/stems/__init__.py
--rw-r--r--  2.0 unx     3006 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/stems/base_2d_stem.py
--rw-r--r--  2.0 unx     3160 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/stems/base_3d_stem.py
--rw-r--r--  2.0 unx     1240 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/stems/down_sample_stem.py
--rw-r--r--  2.0 unx     5534 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/stems/embedding_stem.py
--rw-r--r--  2.0 unx     2576 b- defN 24-Feb-29 12:18 scepter/modules/model/backbone/video/bricks/stems/r2plus1d_stem.py
--rw-r--r--  2.0 unx      454 b- defN 24-Feb-29 12:18 scepter/modules/model/embedder/__init__.py
--rw-r--r--  2.0 unx      915 b- defN 24-Feb-29 12:18 scepter/modules/model/embedder/base_embedder.py
--rw-r--r--  2.0 unx    24642 b- defN 24-Feb-29 12:18 scepter/modules/model/embedder/embedder.py
--rw-r--r--  2.0 unx      253 b- defN 24-Feb-29 12:18 scepter/modules/model/head/__init__.py
--rw-r--r--  2.0 unx    11687 b- defN 24-Feb-29 12:18 scepter/modules/model/head/classifier_head.py
--rw-r--r--  2.0 unx      214 b- defN 24-Feb-29 12:18 scepter/modules/model/loss/__init__.py
--rw-r--r--  2.0 unx     3955 b- defN 24-Feb-29 12:18 scepter/modules/model/loss/base_losses.py
--rw-r--r--  2.0 unx     2178 b- defN 24-Feb-29 12:18 scepter/modules/model/loss/rec_loss.py
--rw-r--r--  2.0 unx      286 b- defN 24-Feb-29 12:18 scepter/modules/model/metric/__init__.py
--rw-r--r--  2.0 unx      783 b- defN 24-Feb-29 12:18 scepter/modules/model/metric/base_metric.py
--rw-r--r--  2.0 unx     5076 b- defN 24-Feb-29 12:18 scepter/modules/model/metric/classification.py
--rw-r--r--  2.0 unx      183 b- defN 24-Feb-29 12:18 scepter/modules/model/metric/registry.py
--rw-r--r--  2.0 unx      220 b- defN 24-Feb-29 12:18 scepter/modules/model/neck/__init__.py
--rw-r--r--  2.0 unx     1870 b- defN 24-Feb-29 12:18 scepter/modules/model/neck/global_average_pooling.py
--rw-r--r--  2.0 unx      918 b- defN 24-Feb-29 12:18 scepter/modules/model/neck/identity.py
--rw-r--r--  2.0 unx      402 b- defN 24-Feb-29 12:18 scepter/modules/model/network/__init__.py
--rw-r--r--  2.0 unx     7188 b- defN 24-Feb-29 12:18 scepter/modules/model/network/classifier.py
--rw-r--r--  2.0 unx     1189 b- defN 24-Feb-29 12:18 scepter/modules/model/network/train_module.py
--rw-r--r--  2.0 unx      148 b- defN 24-Feb-29 12:18 scepter/modules/model/network/autoencoder/__init__.py
--rw-r--r--  2.0 unx     9599 b- defN 24-Feb-29 12:18 scepter/modules/model/network/autoencoder/ae_kl.py
--rw-r--r--  2.0 unx      211 b- defN 24-Feb-29 12:18 scepter/modules/model/network/diffusion/__init__.py
--rw-r--r--  2.0 unx    22667 b- defN 24-Feb-29 12:18 scepter/modules/model/network/diffusion/diffusion.py
--rw-r--r--  2.0 unx     6226 b- defN 24-Feb-29 12:18 scepter/modules/model/network/diffusion/schedules.py
--rw-r--r--  2.0 unx    22561 b- defN 24-Feb-29 12:18 scepter/modules/model/network/diffusion/solvers.py
--rw-r--r--  2.0 unx      385 b- defN 24-Feb-29 12:18 scepter/modules/model/network/ldm/__init__.py
--rw-r--r--  2.0 unx    19522 b- defN 24-Feb-29 12:18 scepter/modules/model/network/ldm/ldm.py
--rw-r--r--  2.0 unx     5860 b- defN 24-Feb-29 12:18 scepter/modules/model/network/ldm/ldm_sce.py
--rw-r--r--  2.0 unx    21293 b- defN 24-Feb-29 12:18 scepter/modules/model/network/ldm/ldm_xl.py
--rw-r--r--  2.0 unx      368 b- defN 24-Feb-29 12:18 scepter/modules/model/tokenizer/__init__.py
--rw-r--r--  2.0 unx      770 b- defN 24-Feb-29 12:18 scepter/modules/model/tokenizer/base_tokenizer.py
--rw-r--r--  2.0 unx     5670 b- defN 24-Feb-29 12:18 scepter/modules/model/tokenizer/tokenizer.py
--rw-r--r--  2.0 unx     1757 b- defN 24-Feb-29 12:18 scepter/modules/model/tokenizer/tokenizer_component.py
--rw-r--r--  2.0 unx      260 b- defN 24-Feb-29 12:18 scepter/modules/model/tuner/__init__.py
--rw-r--r--  2.0 unx      742 b- defN 24-Feb-29 12:18 scepter/modules/model/tuner/base_tuner.py
--rw-r--r--  2.0 unx     4381 b- defN 24-Feb-29 12:18 scepter/modules/model/tuner/swift_tuner.py
--rw-r--r--  2.0 unx     6852 b- defN 24-Feb-29 12:18 scepter/modules/model/tuner/tuner_component.py
--rw-r--r--  2.0 unx      965 b- defN 24-Feb-29 12:18 scepter/modules/model/tuner/tuner_utils.py
--rw-r--r--  2.0 unx      222 b- defN 24-Feb-29 12:18 scepter/modules/model/tuner/sce/__init__.py
--rw-r--r--  2.0 unx     6716 b- defN 24-Feb-29 12:18 scepter/modules/model/tuner/sce/scetuning.py
--rw-r--r--  2.0 unx     2485 b- defN 24-Feb-29 12:18 scepter/modules/model/tuner/sce/scetuning_component.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/modules/model/utils/__init__.py
--rw-r--r--  2.0 unx     3344 b- defN 24-Feb-29 12:18 scepter/modules/model/utils/basic_utils.py
--rw-r--r--  2.0 unx      133 b- defN 24-Feb-29 12:18 scepter/modules/opt/__init__.py
--rw-r--r--  2.0 unx      298 b- defN 24-Feb-29 12:18 scepter/modules/opt/lr_schedulers/__init__.py
--rw-r--r--  2.0 unx      227 b- defN 24-Feb-29 12:18 scepter/modules/opt/lr_schedulers/base_scheduler.py
--rw-r--r--  2.0 unx     3005 b- defN 24-Feb-29 12:18 scepter/modules/opt/lr_schedulers/define_schedulers.py
--rw-r--r--  2.0 unx    14312 b- defN 24-Feb-29 12:18 scepter/modules/opt/lr_schedulers/official_schedulers.py
--rw-r--r--  2.0 unx     1405 b- defN 24-Feb-29 12:18 scepter/modules/opt/lr_schedulers/registry.py
--rw-r--r--  2.0 unx     5420 b- defN 24-Feb-29 12:18 scepter/modules/opt/lr_schedulers/warmup.py
--rw-r--r--  2.0 unx      297 b- defN 24-Feb-29 12:18 scepter/modules/opt/optimizers/__init__.py
--rw-r--r--  2.0 unx      226 b- defN 24-Feb-29 12:18 scepter/modules/opt/optimizers/base_optimizer.py
--rw-r--r--  2.0 unx    19032 b- defN 24-Feb-29 12:18 scepter/modules/opt/optimizers/official_optimizers.py
--rw-r--r--  2.0 unx     1388 b- defN 24-Feb-29 12:18 scepter/modules/opt/optimizers/registry.py
--rw-r--r--  2.0 unx      314 b- defN 24-Feb-29 12:18 scepter/modules/solver/__init__.py
--rw-r--r--  2.0 unx    35587 b- defN 24-Feb-29 12:18 scepter/modules/solver/base_solver.py
--rw-r--r--  2.0 unx    31580 b- defN 24-Feb-29 12:18 scepter/modules/solver/diffusion_solver.py
--rw-r--r--  2.0 unx     1351 b- defN 24-Feb-29 12:18 scepter/modules/solver/registry.py
--rw-r--r--  2.0 unx     7538 b- defN 24-Feb-29 12:18 scepter/modules/solver/train_val_solver.py
--rw-r--r--  2.0 unx     1576 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/__init__.py
--rw-r--r--  2.0 unx     3617 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/backward.py
--rw-r--r--  2.0 unx    11909 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/checkpoint.py
--rw-r--r--  2.0 unx     3745 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/data_probe.py
--rw-r--r--  2.0 unx      611 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/hook.py
--rw-r--r--  2.0 unx    10821 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/log.py
--rw-r--r--  2.0 unx     5202 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/lr.py
--rw-r--r--  2.0 unx      154 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/registry.py
--rw-r--r--  2.0 unx     2192 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/safetensors.py
--rw-r--r--  2.0 unx     1544 b- defN 24-Feb-29 12:18 scepter/modules/solver/hooks/sampler.py
--rw-r--r--  2.0 unx     1718 b- defN 24-Feb-29 12:18 scepter/modules/transform/__init__.py
--rw-r--r--  2.0 unx    19927 b- defN 24-Feb-29 12:18 scepter/modules/transform/augmention.py
--rw-r--r--  2.0 unx     1011 b- defN 24-Feb-29 12:18 scepter/modules/transform/compose.py
--rw-r--r--  2.0 unx      725 b- defN 24-Feb-29 12:18 scepter/modules/transform/identity.py
--rw-r--r--  2.0 unx    22204 b- defN 24-Feb-29 12:18 scepter/modules/transform/image.py
--rw-r--r--  2.0 unx    12198 b- defN 24-Feb-29 12:18 scepter/modules/transform/io.py
--rw-r--r--  2.0 unx    18995 b- defN 24-Feb-29 12:18 scepter/modules/transform/io_video.py
--rw-r--r--  2.0 unx     1984 b- defN 24-Feb-29 12:18 scepter/modules/transform/registry.py
--rw-r--r--  2.0 unx     9951 b- defN 24-Feb-29 12:18 scepter/modules/transform/tensor.py
--rw-r--r--  2.0 unx     3201 b- defN 24-Feb-29 12:18 scepter/modules/transform/transform_xl.py
--rw-r--r--  2.0 unx     1865 b- defN 24-Feb-29 12:18 scepter/modules/transform/utils.py
--rw-r--r--  2.0 unx    18831 b- defN 24-Feb-29 12:18 scepter/modules/transform/video.py
--rw-r--r--  2.0 unx      154 b- defN 24-Feb-29 12:18 scepter/modules/utils/__init__.py
--rw-r--r--  2.0 unx    24633 b- defN 24-Feb-29 12:18 scepter/modules/utils/config.py
--rw-r--r--  2.0 unx     2865 b- defN 24-Feb-29 12:18 scepter/modules/utils/data.py
--rw-r--r--  2.0 unx      482 b- defN 24-Feb-29 12:18 scepter/modules/utils/directory.py
--rw-r--r--  2.0 unx    15775 b- defN 24-Feb-29 12:18 scepter/modules/utils/distribute.py
--rw-r--r--  2.0 unx     3755 b- defN 24-Feb-29 12:18 scepter/modules/utils/export_model.py
--rw-r--r--  2.0 unx    16852 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_system.py
--rw-r--r--  2.0 unx     5554 b- defN 24-Feb-29 12:18 scepter/modules/utils/logger.py
--rw-r--r--  2.0 unx     2735 b- defN 24-Feb-29 12:18 scepter/modules/utils/math_plot.py
--rw-r--r--  2.0 unx     5548 b- defN 24-Feb-29 12:18 scepter/modules/utils/model.py
--rw-r--r--  2.0 unx    17407 b- defN 24-Feb-29 12:18 scepter/modules/utils/probe.py
--rw-r--r--  2.0 unx     7995 b- defN 24-Feb-29 12:18 scepter/modules/utils/registry.py
--rw-r--r--  2.0 unx      423 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_clients/__init__.py
--rw-r--r--  2.0 unx    44598 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_clients/aliyun_oss_fs.py
--rw-r--r--  2.0 unx    11136 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_clients/base_fs.py
--rw-r--r--  2.0 unx     4610 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_clients/http_fs.py
--rw-r--r--  2.0 unx     6249 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_clients/huggingface_fs.py
--rw-r--r--  2.0 unx    13407 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_clients/local_fs.py
--rw-r--r--  2.0 unx     7207 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_clients/modelscope_fs.py
--rw-r--r--  2.0 unx      168 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_clients/registry.py
--rw-r--r--  2.0 unx     1067 b- defN 24-Feb-29 12:18 scepter/modules/utils/file_clients/utils.py
--rw-r--r--  2.0 unx      324 b- defN 24-Feb-29 12:18 scepter/modules/utils/video_reader/__init__.py
--rw-r--r--  2.0 unx     6201 b- defN 24-Feb-29 12:18 scepter/modules/utils/video_reader/frame_sampler.py
--rw-r--r--  2.0 unx     5133 b- defN 24-Feb-29 12:18 scepter/modules/utils/video_reader/video_reader.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/__init__.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/home/__init__.py
--rw-r--r--  2.0 unx     1751 b- defN 24-Feb-29 12:18 scepter/studio/home/home.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/home/home_ui/__init__.py
--rw-r--r--  2.0 unx      387 b- defN 24-Feb-29 12:18 scepter/studio/home/home_ui/component_names.py
--rw-r--r--  2.0 unx      719 b- defN 24-Feb-29 12:18 scepter/studio/home/home_ui/desc_ui.py
--rw-r--r--  2.0 unx      728 b- defN 24-Feb-29 12:18 scepter/studio/home/home_ui/guide_ui.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/inference/__init__.py
--rw-r--r--  2.0 unx     7239 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_manager/__init__.py
--rw-r--r--  2.0 unx     6755 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_manager/infer_runer.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_ui/__init__.py
--rw-r--r--  2.0 unx    15087 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_ui/component_names.py
--rw-r--r--  2.0 unx     8185 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_ui/control_ui.py
--rw-r--r--  2.0 unx     8631 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_ui/diffusion_ui.py
--rw-r--r--  2.0 unx    11520 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_ui/gallery_ui.py
--rw-r--r--  2.0 unx     8302 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_ui/mantra_ui.py
--rw-r--r--  2.0 unx    11233 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_ui/model_manage_ui.py
--rw-r--r--  2.0 unx     4281 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_ui/refiner_ui.py
--rw-r--r--  2.0 unx     6765 b- defN 24-Feb-29 12:18 scepter/studio/inference/inference_ui/tuner_ui.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/preprocess/__init__.py
--rw-r--r--  2.0 unx     2806 b- defN 24-Feb-29 12:18 scepter/studio/preprocess/preprocess.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/preprocess/caption_editor_ui/__init__.py
--rw-r--r--  2.0 unx     7823 b- defN 24-Feb-29 12:18 scepter/studio/preprocess/caption_editor_ui/component_names.py
--rw-r--r--  2.0 unx    29866 b- defN 24-Feb-29 12:18 scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py
--rw-r--r--  2.0 unx    13405 b- defN 24-Feb-29 12:18 scepter/studio/preprocess/caption_editor_ui/dataset_gallery_ui.py
--rw-r--r--  2.0 unx     6875 b- defN 24-Feb-29 12:18 scepter/studio/preprocess/caption_editor_ui/export_dataset_ui.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/self_train/__init__.py
--rw-r--r--  2.0 unx     2618 b- defN 24-Feb-29 12:18 scepter/studio/self_train/self_train.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/self_train/scripts/__init__.py
--rw-r--r--  2.0 unx     6632 b- defN 24-Feb-29 12:18 scepter/studio/self_train/scripts/run_task.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/self_train/self_train_ui/__init__.py
--rw-r--r--  2.0 unx     7958 b- defN 24-Feb-29 12:18 scepter/studio/self_train/self_train_ui/component_names.py
--rw-r--r--  2.0 unx     6843 b- defN 24-Feb-29 12:18 scepter/studio/self_train/self_train_ui/inference_ui.py
--rw-r--r--  2.0 unx    28918 b- defN 24-Feb-29 12:18 scepter/studio/self_train/self_train_ui/trainer_ui.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/self_train/utils/__init__.py
--rw-r--r--  2.0 unx    14826 b- defN 24-Feb-29 12:18 scepter/studio/self_train/utils/config_parser.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/studio/utils/__init__.py
--rw-r--r--  2.0 unx      589 b- defN 24-Feb-29 12:18 scepter/studio/utils/env.py
--rw-r--r--  2.0 unx      567 b- defN 24-Feb-29 12:18 scepter/studio/utils/file.py
--rw-r--r--  2.0 unx      276 b- defN 24-Feb-29 12:18 scepter/studio/utils/singleton.py
--rw-r--r--  2.0 unx      545 b- defN 24-Feb-29 12:18 scepter/studio/utils/uibase.py
--rw-r--r--  2.0 unx       74 b- defN 24-Feb-29 12:18 scepter/tools/__init__.py
--rw-r--r--  2.0 unx     3079 b- defN 24-Feb-29 12:18 scepter/tools/helper.py
--rw-r--r--  2.0 unx     9105 b- defN 24-Feb-29 12:18 scepter/tools/run_inference.py
--rw-r--r--  2.0 unx     1623 b- defN 24-Feb-29 12:18 scepter/tools/run_train.py
--rw-r--r--  2.0 unx     4898 b- defN 24-Feb-29 12:18 scepter/tools/webui.py
--rw-r--r--  2.0 unx    11357 b- defN 24-Feb-29 12:19 scepter-0.0.3.post1.dist-info/LICENSE
--rw-r--r--  2.0 unx    15046 b- defN 24-Feb-29 12:19 scepter-0.0.3.post1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Feb-29 12:19 scepter-0.0.3.post1.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 24-Feb-29 12:19 scepter-0.0.3.post1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    28337 b- defN 24-Feb-29 12:19 scepter-0.0.3.post1.dist-info/RECORD
-278 files, 2132225 bytes uncompressed, 483675 bytes compressed:  77.3%
+Zip file size: 571837 bytes, number of entries: 298
+-rw-r--r--  2.0 unx      602 b- defN 24-Apr-10 07:40 scepter/__init__.py
+-rw-r--r--  2.0 unx      205 b- defN 24-Apr-10 07:39 scepter/version.py
+-rw-r--r--  2.0 unx    12618 b- defN 24-Mar-30 17:03 scepter/methods/examples/classification/example.yaml
+-rw-r--r--  2.0 unx     5383 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_1.5_512.yaml
+-rw-r--r--  2.0 unx     5568 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_1.5_512_lora.yaml
+-rw-r--r--  2.0 unx     4792 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_2.1_512.yaml
+-rw-r--r--  2.0 unx     4982 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_2.1_512_lora.yaml
+-rw-r--r--  2.0 unx     4789 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_2.1_768.yaml
+-rw-r--r--  2.0 unx     4978 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_2.1_768_lora.yaml
+-rw-r--r--  2.0 unx     9088 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_xl_1024.yaml
+-rw-r--r--  2.0 unx     9277 b- defN 24-Mar-30 17:03 scepter/methods/examples/generation/stable_diffusion_xl_1024_lora.yaml
+-rw-r--r--  2.0 unx     6440 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sd15_512_sce_ctr_hed.yaml
+-rw-r--r--  2.0 unx     6315 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sd21_768_sce_ctr_canny.yaml
+-rw-r--r--  2.0 unx     6434 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sd21_768_sce_ctr_pose.yaml
+-rw-r--r--  2.0 unx    10042 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_canny.yaml
+-rw-r--r--  2.0 unx    10058 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color.yaml
+-rw-r--r--  2.0 unx    10420 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_color_datatxt.yaml
+-rw-r--r--  2.0 unx    10135 b- defN 24-Mar-30 17:03 scepter/methods/scedit/ctr/sdxl_1024_sce_ctr_depth.yaml
+-rw-r--r--  2.0 unx     5639 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sd15_512_sce_t2i.yaml
+-rw-r--r--  2.0 unx     5611 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sd15_512_sce_t2i_swift.yaml
+-rw-r--r--  2.0 unx     5045 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sd21_768_sce_t2i.yaml
+-rw-r--r--  2.0 unx     5017 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sd21_768_sce_t2i_swift.yaml
+-rw-r--r--  2.0 unx     9370 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i.yaml
+-rw-r--r--  2.0 unx     9514 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_datatxt.yaml
+-rw-r--r--  2.0 unx     9324 b- defN 24-Mar-30 17:03 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_swift.yaml
+-rw-r--r--  2.0 unx     2539 b- defN 24-Apr-10 07:39 scepter/methods/studio/scepter_ui.yaml
+-rw-r--r--  2.0 unx     1707 b- defN 24-Mar-30 17:03 scepter/methods/studio/extensions/controllers/official_controllers.yaml
+-rw-r--r--  2.0 unx   294732 b- defN 24-Mar-30 17:03 scepter/methods/studio/extensions/mantra_book/mantra_book.yaml
+-rw-r--r--  2.0 unx    29045 b- defN 24-Mar-30 17:03 scepter/methods/studio/extensions/tuners/official_tuners.yaml
+-rw-r--r--  2.0 unx     2833 b- defN 24-Mar-30 17:03 scepter/methods/studio/home/home.yaml
+-rw-r--r--  2.0 unx     3317 b- defN 24-Apr-10 07:39 scepter/methods/studio/inference/inference.yaml
+-rw-r--r--  2.0 unx    10894 b- defN 24-Apr-10 07:39 scepter/methods/studio/inference/largen/largen_pro.yaml
+-rw-r--r--  2.0 unx    10136 b- defN 24-Mar-30 17:03 scepter/methods/studio/inference/sdxl/sdxl1.0_pro.yaml
+-rw-r--r--  2.0 unx     2880 b- defN 24-Mar-30 17:03 scepter/methods/studio/inference/stable_diffusion/sd15_pro.yaml
+-rw-r--r--  2.0 unx     2788 b- defN 24-Mar-30 17:03 scepter/methods/studio/inference/stable_diffusion/sd21_pro.yaml
+-rw-r--r--  2.0 unx      148 b- defN 24-Mar-30 17:03 scepter/methods/studio/preprocess/preprocess.yaml
+-rw-r--r--  2.0 unx      519 b- defN 24-Apr-10 07:39 scepter/methods/studio/self_train/self_train.yaml
+-rw-r--r--  2.0 unx    28594 b- defN 24-Apr-10 07:39 scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml
+-rw-r--r--  2.0 unx     8290 b- defN 24-Apr-10 07:39 scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml
+-rw-r--r--  2.0 unx     6781 b- defN 24-Apr-10 07:39 scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml
+-rw-r--r--  2.0 unx       61 b- defN 24-Apr-10 07:39 scepter/methods/studio/tuner_manager/tuner_manager.yaml
+-rw-r--r--  2.0 unx      187 b- defN 24-Mar-30 17:03 scepter/modules/__init__.py
+-rw-r--r--  2.0 unx      628 b- defN 24-Mar-30 17:03 scepter/modules/annotator/__init__.py
+-rw-r--r--  2.0 unx     2095 b- defN 24-Mar-30 17:03 scepter/modules/annotator/base_annotator.py
+-rw-r--r--  2.0 unx     2474 b- defN 24-Mar-30 17:03 scepter/modules/annotator/canny.py
+-rw-r--r--  2.0 unx     2159 b- defN 24-Mar-30 17:03 scepter/modules/annotator/color.py
+-rw-r--r--  2.0 unx     6346 b- defN 24-Mar-30 17:03 scepter/modules/annotator/hed.py
+-rw-r--r--  2.0 unx      767 b- defN 24-Mar-30 17:03 scepter/modules/annotator/identity.py
+-rw-r--r--  2.0 unx      769 b- defN 24-Mar-30 17:03 scepter/modules/annotator/invert.py
+-rw-r--r--  2.0 unx     3261 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas_op.py
+-rw-r--r--  2.0 unx     2891 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd_op.py
+-rw-r--r--  2.0 unx    35386 b- defN 24-Mar-30 17:03 scepter/modules/annotator/openpose.py
+-rw-r--r--  2.0 unx     1223 b- defN 24-Mar-30 17:03 scepter/modules/annotator/registry.py
+-rw-r--r--  2.0 unx     3503 b- defN 24-Mar-30 17:03 scepter/modules/annotator/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/__init__.py
+-rw-r--r--  2.0 unx     5214 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/api.py
+-rw-r--r--  2.0 unx      391 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/base_model.py
+-rw-r--r--  2.0 unx    11717 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/blocks.py
+-rw-r--r--  2.0 unx     3205 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/dpt_depth.py
+-rw-r--r--  2.0 unx     2769 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/midas_net.py
+-rw-r--r--  2.0 unx     6019 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/midas_net_custom.py
+-rw-r--r--  2.0 unx     8178 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/transforms.py
+-rw-r--r--  2.0 unx     4691 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/utils.py
+-rw-r--r--  2.0 unx    15580 b- defN 24-Mar-30 17:03 scepter/modules/annotator/midas/vit.py
+-rw-r--r--  2.0 unx        0 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd/__init__.py
+-rw-r--r--  2.0 unx    10166 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd/mbv2_mlsd_large.py
+-rw-r--r--  2.0 unx     9706 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd/mbv2_mlsd_tiny.py
+-rw-r--r--  2.0 unx    25395 b- defN 24-Mar-30 17:03 scepter/modules/annotator/mlsd/utils.py
+-rw-r--r--  2.0 unx      125 b- defN 24-Mar-30 17:03 scepter/modules/data/__init__.py
+-rw-r--r--  2.0 unx      599 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/__init__.py
+-rw-r--r--  2.0 unx     3941 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/base_dataset.py
+-rw-r--r--  2.0 unx     9148 b- defN 24-Apr-10 07:39 scepter/modules/data/dataset/dataset.py
+-rw-r--r--  2.0 unx    11650 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/ms_dataset.py
+-rw-r--r--  2.0 unx    15112 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/registry.py
+-rw-r--r--  2.0 unx      771 b- defN 24-Mar-30 17:03 scepter/modules/data/dataset/utils.py
+-rw-r--r--  2.0 unx      407 b- defN 24-Mar-30 17:03 scepter/modules/data/sampler/__init__.py
+-rw-r--r--  2.0 unx     1002 b- defN 24-Mar-30 17:03 scepter/modules/data/sampler/base_sampler.py
+-rw-r--r--  2.0 unx     2013 b- defN 24-Mar-30 17:03 scepter/modules/data/sampler/registry.py
+-rw-r--r--  2.0 unx    20503 b- defN 24-Mar-30 17:03 scepter/modules/data/sampler/sampler.py
+-rw-r--r--  2.0 unx      151 b- defN 24-Mar-30 17:03 scepter/modules/inference/__init__.py
+-rw-r--r--  2.0 unx     5117 b- defN 24-Apr-10 07:39 scepter/modules/inference/control_inference.py
+-rw-r--r--  2.0 unx    31050 b- defN 24-Apr-10 07:39 scepter/modules/inference/diffusion_inference.py
+-rw-r--r--  2.0 unx    25815 b- defN 24-Apr-10 07:39 scepter/modules/inference/largen_inference.py
+-rw-r--r--  2.0 unx    11183 b- defN 24-Mar-30 17:03 scepter/modules/inference/tuner_inference.py
+-rw-r--r--  2.0 unx      218 b- defN 24-Mar-30 17:03 scepter/modules/model/__init__.py
+-rw-r--r--  2.0 unx     4022 b- defN 24-Mar-30 17:03 scepter/modules/model/base_model.py
+-rw-r--r--  2.0 unx     1653 b- defN 24-Mar-30 17:03 scepter/modules/model/registry.py
+-rw-r--r--  2.0 unx      202 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/__init__.py
+-rw-r--r--  2.0 unx      300 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/autoencoder/__init__.py
+-rw-r--r--  2.0 unx    12971 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/autoencoder/ae_module.py
+-rw-r--r--  2.0 unx     9569 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/autoencoder/ae_utils.py
+-rw-r--r--  2.0 unx      445 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/__init__.py
+-rw-r--r--  2.0 unx     2791 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/mlp.py
+-rw-r--r--  2.0 unx     3235 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/resnet.py
+-rw-r--r--  2.0 unx    21928 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/resnet_impl.py
+-rw-r--r--  2.0 unx     1625 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/timm_model.py
+-rw-r--r--  2.0 unx     8783 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/vit_modify.py
+-rw-r--r--  2.0 unx      133 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/utils/__init__.py
+-rw-r--r--  2.0 unx    21405 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/utils/clip.py
+-rw-r--r--  2.0 unx     5109 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/utils/simple_tokenizer.py
+-rw-r--r--  2.0 unx    21302 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/image/utils/vit.py
+-rw-r--r--  2.0 unx      303 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/unet/__init__.py
+-rw-r--r--  2.0 unx    56311 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/unet/unet_module.py
+-rw-r--r--  2.0 unx    46227 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/unet/unet_utils.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/utils/__init__.py
+-rw-r--r--  2.0 unx     2031 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/utils/transformer.py
+-rw-r--r--  2.0 unx      318 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/__init__.py
+-rw-r--r--  2.0 unx     7081 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/init_helper.py
+-rw-r--r--  2.0 unx    30685 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/resnet_3d.py
+-rw-r--r--  2.0 unx    13371 b- defN 24-Apr-10 07:39 scepter/modules/model/backbone/video/video_transformer.py
+-rw-r--r--  2.0 unx      683 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/__init__.py
+-rw-r--r--  2.0 unx     1797 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/base_branch.py
+-rw-r--r--  2.0 unx     4466 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/csn_branch.py
+-rw-r--r--  2.0 unx     3405 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/non_local.py
+-rw-r--r--  2.0 unx     5942 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/r2d3d_branch.py
+-rw-r--r--  2.0 unx     8046 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/r2plus1d_branch.py
+-rw-r--r--  2.0 unx    11916 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/tada_conv.py
+-rw-r--r--  2.0 unx    12202 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/transformer_branch.py
+-rw-r--r--  2.0 unx     2530 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/visualize_3d_module.py
+-rw-r--r--  2.0 unx      575 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/__init__.py
+-rw-r--r--  2.0 unx     3006 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/base_2d_stem.py
+-rw-r--r--  2.0 unx     3160 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/base_3d_stem.py
+-rw-r--r--  2.0 unx     1240 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/down_sample_stem.py
+-rw-r--r--  2.0 unx     5534 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/embedding_stem.py
+-rw-r--r--  2.0 unx     2576 b- defN 24-Mar-30 17:03 scepter/modules/model/backbone/video/bricks/stems/r2plus1d_stem.py
+-rw-r--r--  2.0 unx      602 b- defN 24-Apr-10 07:39 scepter/modules/model/embedder/__init__.py
+-rw-r--r--  2.0 unx      915 b- defN 24-Mar-30 17:03 scepter/modules/model/embedder/base_embedder.py
+-rw-r--r--  2.0 unx    28394 b- defN 24-Apr-10 07:39 scepter/modules/model/embedder/embedder.py
+-rw-r--r--  2.0 unx     5133 b- defN 24-Apr-10 07:39 scepter/modules/model/embedder/resampler.py
+-rw-r--r--  2.0 unx      524 b- defN 24-Apr-10 07:39 scepter/modules/model/head/__init__.py
+-rw-r--r--  2.0 unx    11687 b- defN 24-Mar-30 17:03 scepter/modules/model/head/classifier_head.py
+-rw-r--r--  2.0 unx      214 b- defN 24-Mar-30 17:03 scepter/modules/model/loss/__init__.py
+-rw-r--r--  2.0 unx     3955 b- defN 24-Mar-30 17:03 scepter/modules/model/loss/base_losses.py
+-rw-r--r--  2.0 unx     2178 b- defN 24-Mar-30 17:03 scepter/modules/model/loss/rec_loss.py
+-rw-r--r--  2.0 unx      286 b- defN 24-Apr-01 03:43 scepter/modules/model/metric/__init__.py
+-rw-r--r--  2.0 unx      783 b- defN 24-Mar-30 17:03 scepter/modules/model/metric/base_metric.py
+-rw-r--r--  2.0 unx     5076 b- defN 24-Mar-30 17:03 scepter/modules/model/metric/classification.py
+-rw-r--r--  2.0 unx      183 b- defN 24-Mar-30 17:03 scepter/modules/model/metric/registry.py
+-rw-r--r--  2.0 unx      220 b- defN 24-Mar-30 17:03 scepter/modules/model/neck/__init__.py
+-rw-r--r--  2.0 unx     1870 b- defN 24-Mar-30 17:03 scepter/modules/model/neck/global_average_pooling.py
+-rw-r--r--  2.0 unx      918 b- defN 24-Mar-30 17:03 scepter/modules/model/neck/identity.py
+-rw-r--r--  2.0 unx      402 b- defN 24-Mar-30 17:03 scepter/modules/model/network/__init__.py
+-rw-r--r--  2.0 unx     7188 b- defN 24-Apr-01 03:43 scepter/modules/model/network/classifier.py
+-rw-r--r--  2.0 unx     1189 b- defN 24-Mar-30 17:03 scepter/modules/model/network/train_module.py
+-rw-r--r--  2.0 unx      148 b- defN 24-Mar-30 17:03 scepter/modules/model/network/autoencoder/__init__.py
+-rw-r--r--  2.0 unx     9601 b- defN 24-Apr-10 07:39 scepter/modules/model/network/autoencoder/ae_kl.py
+-rw-r--r--  2.0 unx      211 b- defN 24-Mar-30 17:03 scepter/modules/model/network/diffusion/__init__.py
+-rw-r--r--  2.0 unx    25265 b- defN 24-Apr-10 07:39 scepter/modules/model/network/diffusion/diffusion.py
+-rw-r--r--  2.0 unx     6226 b- defN 24-Mar-30 17:03 scepter/modules/model/network/diffusion/schedules.py
+-rw-r--r--  2.0 unx    22561 b- defN 24-Mar-30 17:03 scepter/modules/model/network/diffusion/solvers.py
+-rw-r--r--  2.0 unx      385 b- defN 24-Mar-30 17:03 scepter/modules/model/network/ldm/__init__.py
+-rw-r--r--  2.0 unx    19522 b- defN 24-Mar-30 17:03 scepter/modules/model/network/ldm/ldm.py
+-rw-r--r--  2.0 unx     5860 b- defN 24-Mar-30 17:03 scepter/modules/model/network/ldm/ldm_sce.py
+-rw-r--r--  2.0 unx    21293 b- defN 24-Mar-30 17:03 scepter/modules/model/network/ldm/ldm_xl.py
+-rw-r--r--  2.0 unx      368 b- defN 24-Mar-30 17:03 scepter/modules/model/tokenizer/__init__.py
+-rw-r--r--  2.0 unx      770 b- defN 24-Mar-30 17:03 scepter/modules/model/tokenizer/base_tokenizer.py
+-rw-r--r--  2.0 unx     5670 b- defN 24-Mar-30 17:03 scepter/modules/model/tokenizer/tokenizer.py
+-rw-r--r--  2.0 unx     1757 b- defN 24-Mar-30 17:03 scepter/modules/model/tokenizer/tokenizer_component.py
+-rw-r--r--  2.0 unx      260 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/__init__.py
+-rw-r--r--  2.0 unx      742 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/base_tuner.py
+-rw-r--r--  2.0 unx     4381 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/swift_tuner.py
+-rw-r--r--  2.0 unx     6852 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/tuner_component.py
+-rw-r--r--  2.0 unx      965 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/tuner_utils.py
+-rw-r--r--  2.0 unx      222 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/sce/__init__.py
+-rw-r--r--  2.0 unx     6716 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/sce/scetuning.py
+-rw-r--r--  2.0 unx     2485 b- defN 24-Mar-30 17:03 scepter/modules/model/tuner/sce/scetuning_component.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/modules/model/utils/__init__.py
+-rw-r--r--  2.0 unx     3344 b- defN 24-Mar-30 17:03 scepter/modules/model/utils/basic_utils.py
+-rw-r--r--  2.0 unx     3776 b- defN 24-Apr-10 07:39 scepter/modules/model/utils/data_utils.py
+-rw-r--r--  2.0 unx      133 b- defN 24-Mar-30 17:03 scepter/modules/opt/__init__.py
+-rw-r--r--  2.0 unx      298 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/__init__.py
+-rw-r--r--  2.0 unx      227 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/base_scheduler.py
+-rw-r--r--  2.0 unx     3005 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/define_schedulers.py
+-rw-r--r--  2.0 unx    14312 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/official_schedulers.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/registry.py
+-rw-r--r--  2.0 unx     5420 b- defN 24-Mar-30 17:03 scepter/modules/opt/lr_schedulers/warmup.py
+-rw-r--r--  2.0 unx      608 b- defN 24-Apr-10 07:39 scepter/modules/opt/optimizers/__init__.py
+-rw-r--r--  2.0 unx      226 b- defN 24-Mar-30 17:03 scepter/modules/opt/optimizers/base_optimizer.py
+-rw-r--r--  2.0 unx    19032 b- defN 24-Mar-30 17:03 scepter/modules/opt/optimizers/official_optimizers.py
+-rw-r--r--  2.0 unx     1388 b- defN 24-Mar-30 17:03 scepter/modules/opt/optimizers/registry.py
+-rw-r--r--  2.0 unx      314 b- defN 24-Mar-30 17:03 scepter/modules/solver/__init__.py
+-rw-r--r--  2.0 unx    35587 b- defN 24-Mar-30 17:03 scepter/modules/solver/base_solver.py
+-rw-r--r--  2.0 unx    31755 b- defN 24-Apr-10 07:39 scepter/modules/solver/diffusion_solver.py
+-rw-r--r--  2.0 unx     1351 b- defN 24-Mar-30 17:03 scepter/modules/solver/registry.py
+-rw-r--r--  2.0 unx     7538 b- defN 24-Mar-30 17:03 scepter/modules/solver/train_val_solver.py
+-rw-r--r--  2.0 unx     1654 b- defN 24-Apr-10 07:39 scepter/modules/solver/hooks/__init__.py
+-rw-r--r--  2.0 unx     3617 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/backward.py
+-rw-r--r--  2.0 unx    11993 b- defN 24-Apr-10 07:39 scepter/modules/solver/hooks/checkpoint.py
+-rw-r--r--  2.0 unx     4944 b- defN 24-Apr-10 07:39 scepter/modules/solver/hooks/data_probe.py
+-rw-r--r--  2.0 unx     2090 b- defN 24-Apr-10 07:39 scepter/modules/solver/hooks/ema.py
+-rw-r--r--  2.0 unx      611 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/hook.py
+-rw-r--r--  2.0 unx    10821 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/log.py
+-rw-r--r--  2.0 unx     5202 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/lr.py
+-rw-r--r--  2.0 unx      154 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/registry.py
+-rw-r--r--  2.0 unx     2192 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/safetensors.py
+-rw-r--r--  2.0 unx     1544 b- defN 24-Mar-30 17:03 scepter/modules/solver/hooks/sampler.py
+-rw-r--r--  2.0 unx     1718 b- defN 24-Mar-30 17:03 scepter/modules/transform/__init__.py
+-rw-r--r--  2.0 unx    19927 b- defN 24-Mar-30 17:03 scepter/modules/transform/augmention.py
+-rw-r--r--  2.0 unx     1011 b- defN 24-Mar-30 17:03 scepter/modules/transform/compose.py
+-rw-r--r--  2.0 unx      725 b- defN 24-Mar-30 17:03 scepter/modules/transform/identity.py
+-rw-r--r--  2.0 unx    22592 b- defN 24-Apr-10 07:39 scepter/modules/transform/image.py
+-rw-r--r--  2.0 unx    12249 b- defN 24-Apr-10 07:39 scepter/modules/transform/io.py
+-rw-r--r--  2.0 unx    18995 b- defN 24-Mar-30 17:03 scepter/modules/transform/io_video.py
+-rw-r--r--  2.0 unx     1984 b- defN 24-Mar-30 17:03 scepter/modules/transform/registry.py
+-rw-r--r--  2.0 unx     9951 b- defN 24-Mar-30 17:03 scepter/modules/transform/tensor.py
+-rw-r--r--  2.0 unx     3201 b- defN 24-Mar-30 17:03 scepter/modules/transform/transform_xl.py
+-rw-r--r--  2.0 unx     1865 b- defN 24-Mar-30 17:03 scepter/modules/transform/utils.py
+-rw-r--r--  2.0 unx    18831 b- defN 24-Mar-30 17:03 scepter/modules/transform/video.py
+-rw-r--r--  2.0 unx      154 b- defN 24-Mar-30 17:03 scepter/modules/utils/__init__.py
+-rw-r--r--  2.0 unx    24691 b- defN 24-Apr-10 07:39 scepter/modules/utils/config.py
+-rw-r--r--  2.0 unx     2865 b- defN 24-Mar-30 17:03 scepter/modules/utils/data.py
+-rw-r--r--  2.0 unx      482 b- defN 24-Mar-30 17:03 scepter/modules/utils/directory.py
+-rw-r--r--  2.0 unx    15775 b- defN 24-Mar-30 17:03 scepter/modules/utils/distribute.py
+-rw-r--r--  2.0 unx     3755 b- defN 24-Apr-10 07:39 scepter/modules/utils/export_model.py
+-rw-r--r--  2.0 unx    17112 b- defN 24-Apr-10 07:39 scepter/modules/utils/file_system.py
+-rw-r--r--  2.0 unx     1684 b- defN 24-Apr-10 07:39 scepter/modules/utils/index.py
+-rw-r--r--  2.0 unx     5554 b- defN 24-Mar-30 17:03 scepter/modules/utils/logger.py
+-rw-r--r--  2.0 unx     2735 b- defN 24-Mar-30 17:03 scepter/modules/utils/math_plot.py
+-rw-r--r--  2.0 unx     5548 b- defN 24-Mar-30 17:03 scepter/modules/utils/model.py
+-rw-r--r--  2.0 unx    17482 b- defN 24-Apr-10 07:39 scepter/modules/utils/probe.py
+-rw-r--r--  2.0 unx     7995 b- defN 24-Mar-30 17:03 scepter/modules/utils/registry.py
+-rw-r--r--  2.0 unx      423 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/__init__.py
+-rw-r--r--  2.0 unx    44598 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/aliyun_oss_fs.py
+-rw-r--r--  2.0 unx    11136 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/base_fs.py
+-rw-r--r--  2.0 unx     4610 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/http_fs.py
+-rw-r--r--  2.0 unx     6249 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/huggingface_fs.py
+-rw-r--r--  2.0 unx    13560 b- defN 24-Apr-10 07:39 scepter/modules/utils/file_clients/local_fs.py
+-rw-r--r--  2.0 unx     7207 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/modelscope_fs.py
+-rw-r--r--  2.0 unx      168 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/registry.py
+-rw-r--r--  2.0 unx     1067 b- defN 24-Mar-30 17:03 scepter/modules/utils/file_clients/utils.py
+-rw-r--r--  2.0 unx      324 b- defN 24-Mar-30 17:03 scepter/modules/utils/video_reader/__init__.py
+-rw-r--r--  2.0 unx     6201 b- defN 24-Mar-30 17:03 scepter/modules/utils/video_reader/frame_sampler.py
+-rw-r--r--  2.0 unx     5133 b- defN 24-Mar-30 17:03 scepter/modules/utils/video_reader/video_reader.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/__init__.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/home/__init__.py
+-rw-r--r--  2.0 unx     1751 b- defN 24-Mar-30 17:03 scepter/studio/home/home.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/home/home_ui/__init__.py
+-rw-r--r--  2.0 unx      387 b- defN 24-Mar-30 17:03 scepter/studio/home/home_ui/component_names.py
+-rw-r--r--  2.0 unx      719 b- defN 24-Mar-30 17:03 scepter/studio/home/home_ui/desc_ui.py
+-rw-r--r--  2.0 unx      728 b- defN 24-Mar-30 17:03 scepter/studio/home/home_ui/guide_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/inference/__init__.py
+-rw-r--r--  2.0 unx    10711 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_manager/__init__.py
+-rw-r--r--  2.0 unx     7003 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_manager/infer_runer.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_ui/__init__.py
+-rw-r--r--  2.0 unx    21707 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/component_names.py
+-rw-r--r--  2.0 unx     8185 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_ui/control_ui.py
+-rw-r--r--  2.0 unx     8631 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_ui/diffusion_ui.py
+-rw-r--r--  2.0 unx    13683 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/gallery_ui.py
+-rw-r--r--  2.0 unx    22326 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/largen_ui.py
+-rw-r--r--  2.0 unx     8373 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/mantra_ui.py
+-rw-r--r--  2.0 unx    11945 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/model_manage_ui.py
+-rw-r--r--  2.0 unx     4281 b- defN 24-Mar-30 17:03 scepter/studio/inference/inference_ui/refiner_ui.py
+-rw-r--r--  2.0 unx     9619 b- defN 24-Apr-10 07:39 scepter/studio/inference/inference_ui/tuner_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/__init__.py
+-rw-r--r--  2.0 unx     2815 b- defN 24-Apr-10 07:39 scepter/studio/preprocess/preprocess.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/caption_editor_ui/__init__.py
+-rw-r--r--  2.0 unx     7823 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/caption_editor_ui/component_names.py
+-rw-r--r--  2.0 unx    32565 b- defN 24-Apr-10 07:39 scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py
+-rw-r--r--  2.0 unx    13405 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/caption_editor_ui/dataset_gallery_ui.py
+-rw-r--r--  2.0 unx     6875 b- defN 24-Mar-30 17:03 scepter/studio/preprocess/caption_editor_ui/export_dataset_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/self_train/__init__.py
+-rw-r--r--  2.0 unx     2575 b- defN 24-Apr-10 07:39 scepter/studio/self_train/self_train.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/self_train/scripts/__init__.py
+-rw-r--r--  2.0 unx     7438 b- defN 24-Apr-10 07:39 scepter/studio/self_train/scripts/run_task.py
+-rw-r--r--  2.0 unx      148 b- defN 24-Apr-10 07:39 scepter/studio/self_train/scripts/sleep.py
+-rw-r--r--  2.0 unx    14891 b- defN 24-Apr-10 07:39 scepter/studio/self_train/scripts/trainer.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/self_train/self_train_ui/__init__.py
+-rw-r--r--  2.0 unx     8907 b- defN 24-Apr-10 07:39 scepter/studio/self_train/self_train_ui/component_names.py
+-rw-r--r--  2.0 unx    22179 b- defN 24-Apr-10 07:39 scepter/studio/self_train/self_train_ui/model_ui.py
+-rw-r--r--  2.0 unx    32081 b- defN 24-Apr-10 07:39 scepter/studio/self_train/self_train_ui/trainer_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/self_train/utils/__init__.py
+-rw-r--r--  2.0 unx    12071 b- defN 24-Apr-10 07:39 scepter/studio/self_train/utils/config_parser.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/__init__.py
+-rw-r--r--  2.0 unx     1249 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/tuner_manager.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/manager_ui/__init__.py
+-rw-r--r--  2.0 unx    14704 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/manager_ui/browser_ui.py
+-rw-r--r--  2.0 unx     1632 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/manager_ui/component_names.py
+-rw-r--r--  2.0 unx     3226 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/manager_ui/info_ui.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/utils/__init__.py
+-rw-r--r--  2.0 unx      452 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/utils/dict.py
+-rw-r--r--  2.0 unx      217 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/utils/path.py
+-rw-r--r--  2.0 unx      327 b- defN 24-Apr-10 07:39 scepter/studio/tuner_manager/utils/yaml.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/studio/utils/__init__.py
+-rw-r--r--  2.0 unx      589 b- defN 24-Mar-30 17:03 scepter/studio/utils/env.py
+-rw-r--r--  2.0 unx      567 b- defN 24-Mar-30 17:03 scepter/studio/utils/file.py
+-rw-r--r--  2.0 unx      276 b- defN 24-Mar-30 17:03 scepter/studio/utils/singleton.py
+-rw-r--r--  2.0 unx      545 b- defN 24-Mar-30 17:03 scepter/studio/utils/uibase.py
+-rw-r--r--  2.0 unx       74 b- defN 24-Mar-30 17:03 scepter/tools/__init__.py
+-rw-r--r--  2.0 unx     3371 b- defN 24-Apr-10 07:39 scepter/tools/helper.py
+-rw-r--r--  2.0 unx     9409 b- defN 24-Apr-10 07:39 scepter/tools/run_inference.py
+-rw-r--r--  2.0 unx     2114 b- defN 24-Apr-10 07:39 scepter/tools/run_train.py
+-rw-r--r--  2.0 unx     6671 b- defN 24-Apr-10 07:39 scepter/tools/webui.py
+-rw-r--r--  2.0 unx    11357 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/LICENSE
+-rw-r--r--  2.0 unx    20162 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx        8 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    30347 b- defN 24-Apr-10 07:40 scepter-0.0.4.dist-info/RECORD
+298 files, 2329931 bytes uncompressed, 522241 bytes compressed:  77.6%
```

## zipnote {}

```diff
@@ -87,14 +87,17 @@
 
 Filename: scepter/methods/studio/home/home.yaml
 Comment: 
 
 Filename: scepter/methods/studio/inference/inference.yaml
 Comment: 
 
+Filename: scepter/methods/studio/inference/largen/largen_pro.yaml
+Comment: 
+
 Filename: scepter/methods/studio/inference/sdxl/sdxl1.0_pro.yaml
 Comment: 
 
 Filename: scepter/methods/studio/inference/stable_diffusion/sd15_pro.yaml
 Comment: 
 
 Filename: scepter/methods/studio/inference/stable_diffusion/sd21_pro.yaml
@@ -111,14 +114,17 @@
 
 Filename: scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml
 Comment: 
 
 Filename: scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml
 Comment: 
 
+Filename: scepter/methods/studio/tuner_manager/tuner_manager.yaml
+Comment: 
+
 Filename: scepter/modules/__init__.py
 Comment: 
 
 Filename: scepter/modules/annotator/__init__.py
 Comment: 
 
 Filename: scepter/modules/annotator/base_annotator.py
@@ -234,14 +240,17 @@
 
 Filename: scepter/modules/inference/control_inference.py
 Comment: 
 
 Filename: scepter/modules/inference/diffusion_inference.py
 Comment: 
 
+Filename: scepter/modules/inference/largen_inference.py
+Comment: 
+
 Filename: scepter/modules/inference/tuner_inference.py
 Comment: 
 
 Filename: scepter/modules/model/__init__.py
 Comment: 
 
 Filename: scepter/modules/model/base_model.py
@@ -369,14 +378,17 @@
 
 Filename: scepter/modules/model/embedder/base_embedder.py
 Comment: 
 
 Filename: scepter/modules/model/embedder/embedder.py
 Comment: 
 
+Filename: scepter/modules/model/embedder/resampler.py
+Comment: 
+
 Filename: scepter/modules/model/head/__init__.py
 Comment: 
 
 Filename: scepter/modules/model/head/classifier_head.py
 Comment: 
 
 Filename: scepter/modules/model/loss/__init__.py
@@ -486,14 +498,17 @@
 
 Filename: scepter/modules/model/utils/__init__.py
 Comment: 
 
 Filename: scepter/modules/model/utils/basic_utils.py
 Comment: 
 
+Filename: scepter/modules/model/utils/data_utils.py
+Comment: 
+
 Filename: scepter/modules/opt/__init__.py
 Comment: 
 
 Filename: scepter/modules/opt/lr_schedulers/__init__.py
 Comment: 
 
 Filename: scepter/modules/opt/lr_schedulers/base_scheduler.py
@@ -546,14 +561,17 @@
 
 Filename: scepter/modules/solver/hooks/checkpoint.py
 Comment: 
 
 Filename: scepter/modules/solver/hooks/data_probe.py
 Comment: 
 
+Filename: scepter/modules/solver/hooks/ema.py
+Comment: 
+
 Filename: scepter/modules/solver/hooks/hook.py
 Comment: 
 
 Filename: scepter/modules/solver/hooks/log.py
 Comment: 
 
 Filename: scepter/modules/solver/hooks/lr.py
@@ -621,14 +639,17 @@
 
 Filename: scepter/modules/utils/export_model.py
 Comment: 
 
 Filename: scepter/modules/utils/file_system.py
 Comment: 
 
+Filename: scepter/modules/utils/index.py
+Comment: 
+
 Filename: scepter/modules/utils/logger.py
 Comment: 
 
 Filename: scepter/modules/utils/math_plot.py
 Comment: 
 
 Filename: scepter/modules/utils/model.py
@@ -720,14 +741,17 @@
 
 Filename: scepter/studio/inference/inference_ui/diffusion_ui.py
 Comment: 
 
 Filename: scepter/studio/inference/inference_ui/gallery_ui.py
 Comment: 
 
+Filename: scepter/studio/inference/inference_ui/largen_ui.py
+Comment: 
+
 Filename: scepter/studio/inference/inference_ui/mantra_ui.py
 Comment: 
 
 Filename: scepter/studio/inference/inference_ui/model_manage_ui.py
 Comment: 
 
 Filename: scepter/studio/inference/inference_ui/refiner_ui.py
@@ -765,32 +789,68 @@
 
 Filename: scepter/studio/self_train/scripts/__init__.py
 Comment: 
 
 Filename: scepter/studio/self_train/scripts/run_task.py
 Comment: 
 
+Filename: scepter/studio/self_train/scripts/sleep.py
+Comment: 
+
+Filename: scepter/studio/self_train/scripts/trainer.py
+Comment: 
+
 Filename: scepter/studio/self_train/self_train_ui/__init__.py
 Comment: 
 
 Filename: scepter/studio/self_train/self_train_ui/component_names.py
 Comment: 
 
-Filename: scepter/studio/self_train/self_train_ui/inference_ui.py
+Filename: scepter/studio/self_train/self_train_ui/model_ui.py
 Comment: 
 
 Filename: scepter/studio/self_train/self_train_ui/trainer_ui.py
 Comment: 
 
 Filename: scepter/studio/self_train/utils/__init__.py
 Comment: 
 
 Filename: scepter/studio/self_train/utils/config_parser.py
 Comment: 
 
+Filename: scepter/studio/tuner_manager/__init__.py
+Comment: 
+
+Filename: scepter/studio/tuner_manager/tuner_manager.py
+Comment: 
+
+Filename: scepter/studio/tuner_manager/manager_ui/__init__.py
+Comment: 
+
+Filename: scepter/studio/tuner_manager/manager_ui/browser_ui.py
+Comment: 
+
+Filename: scepter/studio/tuner_manager/manager_ui/component_names.py
+Comment: 
+
+Filename: scepter/studio/tuner_manager/manager_ui/info_ui.py
+Comment: 
+
+Filename: scepter/studio/tuner_manager/utils/__init__.py
+Comment: 
+
+Filename: scepter/studio/tuner_manager/utils/dict.py
+Comment: 
+
+Filename: scepter/studio/tuner_manager/utils/path.py
+Comment: 
+
+Filename: scepter/studio/tuner_manager/utils/yaml.py
+Comment: 
+
 Filename: scepter/studio/utils/__init__.py
 Comment: 
 
 Filename: scepter/studio/utils/env.py
 Comment: 
 
 Filename: scepter/studio/utils/file.py
@@ -813,23 +873,23 @@
 
 Filename: scepter/tools/run_train.py
 Comment: 
 
 Filename: scepter/tools/webui.py
 Comment: 
 
-Filename: scepter-0.0.3.post1.dist-info/LICENSE
+Filename: scepter-0.0.4.dist-info/LICENSE
 Comment: 
 
-Filename: scepter-0.0.3.post1.dist-info/METADATA
+Filename: scepter-0.0.4.dist-info/METADATA
 Comment: 
 
-Filename: scepter-0.0.3.post1.dist-info/WHEEL
+Filename: scepter-0.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: scepter-0.0.3.post1.dist-info/top_level.txt
+Filename: scepter-0.0.4.dist-info/top_level.txt
 Comment: 
 
-Filename: scepter-0.0.3.post1.dist-info/RECORD
+Filename: scepter-0.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## scepter/version.py

```diff
@@ -1,8 +1,8 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-__version__ = '0.0.3.post1'
+__version__ = '0.0.4'
 
 version_info = tuple(int(x) for x in __version__.split('.')[0:3])
 
 __all__ = ['__version__', version_info]
```

## scepter/methods/studio/scepter_ui.yaml

```diff
@@ -45,19 +45,19 @@
   <div class="banner">
       <div class="title">
           <h1>SCEPTER Studio</h1>
       </div>
       <div class="qr-codes">
           <div class="qr-code-container">
               <img src="https://modelscope.cn/api/v1/models/damo/scepter/repo?Revision=master&FilePath=assets/scepter_studio/ms_scepter_studio_qr.png" alt="ms_scepter_studio_qr">
-              <div class="caption">Modelscope Studio</div>
+              <div class="caption"><a href="https://www.modelscope.cn/studios/iic/scepter_studio">Modelscope Studio</a></div>
           </div>
           <div class="qr-code-container">
               <img src="https://modelscope.cn/api/v1/models/damo/scepter/repo?Revision=master&FilePath=assets/scepter_studio/scepter_github_qr.png" alt="scepter_github_qr">
-              <div class="caption">Github</div>
+              <div class="caption"><a href="https://github.com/modelscope/scepter">Github</a></div>
           </div>
       </div>
   </div>
   </body>
 WORK_DIR: "cache/scepter_ui"
 FILE_SYSTEM:
   -
@@ -75,11 +75,15 @@
     NAME_EN: Dataset Management
     IFID: preprocess
     CONFIG: scepter/methods/studio/preprocess/preprocess.yaml
   - NAME: 
     NAME_EN: Train
     IFID: self_train
     CONFIG: scepter/methods/studio/self_train/self_train.yaml
+  - NAME: 
+    NAME_EN: Tuner Management
+    IFID: tuner_manager
+    CONFIG: scepter/methods/studio/tuner_manager/tuner_manager.yaml
   - NAME: 
     NAME_EN: Inference
     IFID: inference
     CONFIG: scepter/methods/studio/inference/inference.yaml
```

### html2text {}

```diff
@@ -1,14 +1,16 @@
 HOST: "localhost" PORT: 2024 ROOT: "" TITLE: SCEPTER Studio BANNER: |
 ************ ????SSCCEEPPTTEERR SSttuuddiioo ************
 [ms_scepter_studio_qr]
-Modelscope Studio
+_M_o_d_e_l_s_c_o_p_e_ _S_t_u_d_i_o
 [scepter_github_qr]
-Github
+_G_i_t_h_u_b
 WORK_DIR: "cache/scepter_ui" FILE_SYSTEM: - NAME: "ModelscopeFs" TEMP_DIR:
 "cache/cache_data" - NAME: "HttpFs" TEMP_DIR: "cache/cache_data" INTERFACE: -
 NAME:  NAME_EN: Home IFID: home CONFIG: scepter/methods/studio/home/
 home.yaml - NAME:  NAME_EN: Dataset Management IFID: preprocess
 CONFIG: scepter/methods/studio/preprocess/preprocess.yaml - NAME: 
 NAME_EN: Train IFID: self_train CONFIG: scepter/methods/studio/self_train/
-self_train.yaml - NAME:  NAME_EN: Inference IFID: inference CONFIG:
-scepter/methods/studio/inference/inference.yaml
+self_train.yaml - NAME:  NAME_EN: Tuner Management IFID:
+tuner_manager CONFIG: scepter/methods/studio/tuner_manager/tuner_manager.yaml -
+NAME:  NAME_EN: Inference IFID: inference CONFIG: scepter/methods/studio/
+inference/inference.yaml
```

## scepter/methods/studio/inference/inference.yaml

```diff
@@ -75,14 +75,15 @@
              [1472, 704], [1536, 640], [1600, 640],
              [1664, 576], [1728, 576]]
     DEFAULT: [1024, 1024]
 EXTENSION_PARAS:
   MANTRA_BOOK: scepter/methods/studio/extensions/mantra_book/mantra_book.yaml
   OFFICIAL_TUNERS: scepter/methods/studio/extensions/tuners/official_tuners.yaml
   OFFICIAL_CONTROLLERS: scepter/methods/studio/extensions/controllers/official_controllers.yaml
+  TUNER_MANAGER: scepter/methods/studio/tuner_manager/tuner_manager.yaml
 CONTROLABLE_ANNOTATORS:
   -
     NAME: "CannyAnnotator"
     TYPE: Canny
     IS_DEFAULT: True
   -
     NAME: "HedAnnotator"
```

## scepter/methods/studio/self_train/self_train.yaml

```diff
@@ -4,7 +4,17 @@
 SAMPLERS:
   -
     NAME: 'ddim'
   -
     NAME: 'dpmpp_2m_sde'
   -
     NAME: 'dpmpp_2s_ancestral'
+TRAIN_PARAS:
+  RESOLUTIONS:
+    VALUES: [[256, 256], [320, 180], [180, 320],
+             [512, 512], [640, 360], [360, 640],
+             [768, 768], [960, 540], [540, 960],
+             [1024, 1024], [1280, 720], [720, 1280]]
+    DEFAULT: [1024, 1024]
+  EVAL_PROMPTS:
+    - a boy wearing a jacket
+    - a dog running on the lawn
```

## scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml

```diff
@@ -7,69 +7,69 @@
   IS_DEFAULT: True
   INFERENCE_PARAS:
     INFERENCE_BATCH_SIZE: 1
     INFERENCE_PREFIX: ""
     DEFAULT_SAMPLER: "dpmpp_2s_ancestral"
     DEFAULT_SAMPLE_STEPS: 40
     INFERENCE_N_PROMPT: ""
-    RESOLUTION: 1024
+    RESOLUTION: [1024, 1024]
   PARAS:
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 1024
+      RESOLUTION: [1024, 1024]
       MEMORY: 29000
       EPOCHS: 50
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: FULL
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 1024
+      RESOLUTION: [1024, 1024]
       MEMORY: 29000
       EPOCHS: 50
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: LORA
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 1024
+      RESOLUTION: [1024, 1024]
       MEMORY: 29000
       EPOCHS: 200
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: SCE
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 1024
+      RESOLUTION: [1024, 1024]
       MEMORY: 29000
       EPOCHS: 200
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: True
       TUNER: TEXT_SCE
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 1024
+      RESOLUTION: [1024, 1024]
       MEMORY: 29000
       EPOCHS: 50
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: TEXT_LORA
@@ -142,18 +142,20 @@
   CHANNELS_LAST: False
   # RESUME_FROM DESCRIPTION: Resume from some state of training! TYPE: str default: ''
   RESUME_FROM:
   # MAX_EPOCHS DESCRIPTION: Max epochs for training. TYPE: int default: 10
   MAX_EPOCHS: -1
   # NUM_FOLDS DESCRIPTION: Num folds for training. TYPE: int default: 1
   NUM_FOLDS: 1
+  #
+  EVAL_INTERVAL: -1
   # WORK_DIR DESCRIPTION: Save dir of the training log or model. TYPE: str default: ''
   WORK_DIR:
   # LOG_FILE DESCRIPTION: Save log path. TYPE: str default: ''
-  LOG_FILE: stg_log.txt
+  LOG_FILE: std_log.txt
   FILE_SYSTEM:
     NAME: "ModelscopeFs"
     TEMP_DIR: "./cache/data"
   TUNER:
   # MODEL DESCRIPTION:  TYPE:  default: ''
   MODEL:
     # NAME DESCRIPTION:
@@ -582,19 +584,48 @@
       - NAME: Select
         KEYS: [ 'img', 'prompt', 'img_original_size_as_tuple', 'img_target_size_as_tuple', 'img_crop_coords_top_left' ]
         META_KEYS: ['data_key', 'img_path']
       - NAME: Rename
         INPUT_KEY: [ 'img', 'img_original_size_as_tuple', 'img_target_size_as_tuple', 'img_crop_coords_top_left']
         OUTPUT_KEY: [ 'image', 'original_size_as_tuple', 'target_size_as_tuple', 'crop_coords_top_left' ]
   #
+  EVAL_DATA:
+    NAME: Text2ImageDataset
+    MODE: eval
+    PROMPT_FILE:
+    PROMPT_DATA: [ "a boy wearing a jacket", "a dog running on the lawn" ]
+    IMAGE_SIZE: [ 1024, 1024 ]
+    FIELDS: [ "prompt" ]
+    DELIMITER: '#;#'
+    PROMPT_PREFIX: ''
+    PIN_MEMORY: True
+    BATCH_SIZE: 1
+    NUM_WORKERS: 4
+    TRANSFORMS:
+      - NAME: Select
+        KEYS: [ 'index', 'prompt' ]
+        META_KEYS: [ 'image_size' ]
+  #
   TRAIN_HOOKS:
-    - NAME: BackwardHook
-      # GRADIENT_CLIP: 1.0
+    -
+      NAME: BackwardHook
       PRIORITY: 0
-    - NAME: LogHook
-      LOG_INTERVAL: 50
+    -
+      NAME: LogHook
+      LOG_INTERVAL: 10
       SHOW_GPU_MEM: True
     -
+      NAME: TensorboardLogHook
+    -
       NAME: CheckpointHook
-      SAVE_LAST: True
       INTERVAL: 10000
       PRIORITY: 200
+      SAVE_LAST: True
+      SAVE_NAME_PREFIX: 'step'
+  #
+  EVAL_HOOKS:
+    -
+      NAME: ProbeDataHook
+      PROB_INTERVAL: 100
+      SAVE_LAST: True
+      SAVE_NAME_PREFIX: 'step'
+      SAVE_PROBE_PREFIX: 'image'
```

## scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml

```diff
@@ -6,71 +6,71 @@
   IS_DEFAULT: False
   INFERENCE_PARAS:
     INFERENCE_BATCH_SIZE: 1
     INFERENCE_PREFIX: ""
     DEFAULT_SAMPLER: "ddim"
     DEFAULT_SAMPLE_STEPS: 40
     INFERENCE_N_PROMPT: ""
-    RESOLUTION: 512
+    RESOLUTION: [512, 512]
   PARAS:
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 512
+      RESOLUTION: [512, 512]
       MEMORY: 29000
       EPOCHS: 50
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: FULL
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 512
+      RESOLUTION: [512, 512]
       MEMORY: 29000
       EPOCHS: 50
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: LORA
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 512
+      RESOLUTION: [512, 512]
       MEMORY: 29000
       EPOCHS: 200
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: SCE
 
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 512
+      RESOLUTION: [512, 512]
       MEMORY: 29000
       EPOCHS: 200
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: True
       TUNER: TEXT_SCE
 
     -
       TRAIN_BATCH_SIZE: 4
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 512
+      RESOLUTION: [512, 512]
       MEMORY: 29000
       EPOCHS: 50
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: TEXT_LORA
@@ -131,14 +131,15 @@
   USE_AMP: True
   DTYPE: float16
   CHANNELS_LAST: True
   MAX_STEPS: 1000
   MAX_EPOCHS: -1
   NUM_FOLDS: 1
   ACCU_STEP: 1
+  EVAL_INTERVAL: -1
   #
   WORK_DIR:
   LOG_FILE: std_log.txt
   #
   FILE_SYSTEM:
     NAME: "ModelscopeFs"
     TEMP_DIR: "./cache/data"
@@ -294,19 +295,48 @@
         INPUT_KEY: [ 'img' ]
         OUTPUT_KEY: [ 'image' ]
         BACKEND: torchvision
       - NAME: Select
         KEYS: [ 'image', 'prompt' ]
         META_KEYS: [ 'data_key' ]
   #
+  EVAL_DATA:
+    NAME: Text2ImageDataset
+    MODE: eval
+    PROMPT_FILE:
+    PROMPT_DATA: [ "a boy wearing a jacket", "a dog running on the lawn" ]
+    IMAGE_SIZE: [ 512, 512 ]
+    FIELDS: [ "prompt" ]
+    DELIMITER: '#;#'
+    PROMPT_PREFIX: ''
+    PIN_MEMORY: True
+    BATCH_SIZE: 1
+    NUM_WORKERS: 4
+    TRANSFORMS:
+      - NAME: Select
+        KEYS: [ 'index', 'prompt' ]
+        META_KEYS: [ 'image_size' ]
+  #
   TRAIN_HOOKS:
-    - NAME: BackwardHook
-      # GRADIENT_CLIP: 1.0
+    -
+      NAME: BackwardHook
       PRIORITY: 0
-    - NAME: LogHook
-      LOG_INTERVAL: 50
+    -
+      NAME: LogHook
+      LOG_INTERVAL: 10
       SHOW_GPU_MEM: True
     -
+      NAME: TensorboardLogHook
+    -
       NAME: CheckpointHook
-      SAVE_LAST: True
       INTERVAL: 10000
       PRIORITY: 200
+      SAVE_LAST: True
+      SAVE_NAME_PREFIX: 'step'
+  #
+  EVAL_HOOKS:
+    -
+      NAME: ProbeDataHook
+      PROB_INTERVAL: 100
+      SAVE_LAST: True
+      SAVE_NAME_PREFIX: 'step'
+      SAVE_PROBE_PREFIX: 'image'
```

## scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml

```diff
@@ -6,45 +6,45 @@
   IS_DEFAULT: False
   INFERENCE_PARAS:
     INFERENCE_BATCH_SIZE: 1
     INFERENCE_PREFIX: ""
     DEFAULT_SAMPLER: "ddim"
     DEFAULT_SAMPLE_STEPS: 40
     INFERENCE_N_PROMPT: ""
-    RESOLUTION: 768
+    RESOLUTION: [768, 768]
   PARAS:
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 768
+      RESOLUTION: [768, 768]
       MEMORY: 29000
       EPOCHS: 50
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: FULL
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 768
+      RESOLUTION: [768, 768]
       MEMORY: 29000
       EPOCHS: 50
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: False
       TUNER: LORA
     -
       TRAIN_BATCH_SIZE: 2
       TRAIN_PREFIX: ""
       TRAIN_N_PROMPT: ""
-      RESOLUTION: 768
+      RESOLUTION: [768, 768]
       MEMORY: 29000
       EPOCHS: 200
       SAVE_INTERVAL: 25
       EPSEC: 0.818
       LEARNING_RATE: 0.0001
       IS_DEFAULT: True
       TUNER: SCE
@@ -74,14 +74,15 @@
   USE_AMP: True
   DTYPE: float16
   CHANNELS_LAST: True
   MAX_STEPS: 1000
   MAX_EPOCHS: -1
   NUM_FOLDS: 1
   ACCU_STEP: 1
+  EVAL_INTERVAL: -1
   #
   WORK_DIR:
   LOG_FILE: std_log.txt
   #
   FILE_SYSTEM:
     NAME: "ModelscopeFs"
     TEMP_DIR: "./cache/data"
@@ -236,19 +237,48 @@
         INPUT_KEY: [ 'img' ]
         OUTPUT_KEY: [ 'image' ]
         BACKEND: torchvision
       - NAME: Select
         KEYS: [ 'image', 'prompt' ]
         META_KEYS: [ 'data_key' ]
   #
+  EVAL_DATA:
+    NAME: Text2ImageDataset
+    MODE: eval
+    PROMPT_FILE:
+    PROMPT_DATA: [ "a boy wearing a jacket", "a dog running on the lawn" ]
+    IMAGE_SIZE: [ 768, 768 ]
+    FIELDS: [ "prompt" ]
+    DELIMITER: '#;#'
+    PROMPT_PREFIX: ''
+    PIN_MEMORY: True
+    BATCH_SIZE: 1
+    NUM_WORKERS: 4
+    TRANSFORMS:
+      - NAME: Select
+        KEYS: [ 'index', 'prompt' ]
+        META_KEYS: [ 'image_size' ]
+  #
   TRAIN_HOOKS:
-    - NAME: BackwardHook
-      # GRADIENT_CLIP: 1.0
+    -
+      NAME: BackwardHook
       PRIORITY: 0
-    - NAME: LogHook
-      LOG_INTERVAL: 50
+    -
+      NAME: LogHook
+      LOG_INTERVAL: 10
       SHOW_GPU_MEM: True
     -
+      NAME: TensorboardLogHook
+    -
       NAME: CheckpointHook
-      SAVE_LAST: True
       INTERVAL: 10000
       PRIORITY: 200
+      SAVE_LAST: True
+      SAVE_NAME_PREFIX: 'step'
+  #
+  EVAL_HOOKS:
+    -
+      NAME: ProbeDataHook
+      PROB_INTERVAL: 100
+      SAVE_LAST: True
+      SAVE_NAME_PREFIX: 'step'
+      SAVE_PROBE_PREFIX: 'image'
```

## scepter/modules/data/dataset/dataset.py

```diff
@@ -240,37 +240,42 @@
         use_num = cfg.get('USE_NUM', -1)
 
         image_size = cfg.get('IMAGE_SIZE', 1024)
         if isinstance(image_size, numbers.Number):
             image_size = [image_size, image_size]
         assert isinstance(image_size, Iterable) and len(image_size) == 2
 
-        prompt_file = cfg.PROMPT_FILE
-        with FS.get_object(prompt_file) as local_data:
+        if cfg.PROMPT_FILE is not None and cfg.PROMPT_FILE != '':
+            prompt_file = cfg.PROMPT_FILE
+            with FS.get_object(prompt_file) as local_data:
+                rows = [
+                    i.split(delimiter,
+                            len(fields) - 1)
+                    for i in local_data.decode('utf-8').strip().split('\n')
+                ]
+        else:
             rows = [
                 i.split(delimiter,
-                        len(fields) - 1)
-                for i in local_data.decode('utf-8').strip().split('\n')
+                        len(fields) - 1) for i in cfg.PROMPT_DATA
             ]
 
         self.items = list()
         for i, row in enumerate(rows):
             item = {'index': i, 'meta': {'image_size': image_size}}
             for key, value in zip(fields, row):
                 if key in ['prompt', 'caption', 'text']:
                     item['ori_prompt'] = value
                     item['prompt'] = prompt_prefix + value
                 elif key in ['oss_key', 'path', 'img_path', 'target_img_path']:
                     item['meta']['img_path'] = os.path.join(path_prefix, value)
                 elif key in ['width', 'height']:
                     item['meta'][key] = int(value)
-                elif key != 'meta':
-                    item[key] = value
                 else:
-                    continue
+                    item['meta'][key] = value
+
             self.items.append(item)
         if use_num > 0:
             self.items = self.items[:use_num]
         if we.rank == 0:
             logger.info(f'eval prompt num: {len(self.items)}')
             logger.info('eval prompts: {}'.format(
                 [k['prompt'] for k in self.items]))
```

## scepter/modules/inference/control_inference.py

```diff
@@ -1,23 +1,28 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import copy
 import os
+import warnings
 
 import torch
 import torch.nn as nn
 import torchvision.transforms as TT
 from PIL.Image import Image
-from swift import SwiftModel
 
 from scepter.modules.model.registry import TUNERS
 from scepter.modules.utils.config import Config
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.file_system import FS
 
+try:
+    from swift import SwiftModel
+except Exception:
+    warnings.warn('Import swift failed, please check it.')
+
 
 class ControlInference():
     def __init__(self, logger=None):
         self.logger = logger
         self.is_register = False
 
     # @classmethod
```

## scepter/modules/inference/diffusion_inference.py

```diff
@@ -391,15 +391,15 @@
         with torch.autocast('cuda',
                             enabled=dtype == 'float16',
                             dtype=getattr(torch, dtype)):
             z = get_model(self.first_stage_model).encode(x)
             return self.first_stage_model['paras']['scale_factor'] * z
 
     def decode_first_stage(self, z):
-        _, dtype = self.get_function_info(self.first_stage_model, 'encode')
+        _, dtype = self.get_function_info(self.first_stage_model, 'decode')
         with torch.autocast('cuda',
                             enabled=dtype == 'float16',
                             dtype=getattr(torch, dtype)):
             z = 1. / self.first_stage_model['paras']['scale_factor'] * z
             return get_model(self.first_stage_model).decode(z)
 
     @torch.no_grad()
@@ -470,18 +470,22 @@
         # cond stage
         self.dynamic_load(self.cond_stage_model, 'cond_stage_model')
         function_name, dtype = self.get_function_info(self.cond_stage_model)
         with torch.autocast('cuda',
                             enabled=dtype == 'float16',
                             dtype=getattr(torch, dtype)):
             if self.tokenizer:
+                if not hasattr(get_model(self.cond_stage_model), 'tokenizer'):
+                    setattr(get_model(self.cond_stage_model), 'tokenizer',
+                            self.tokenizer)
                 context = getattr(get_model(self.cond_stage_model),
                                   function_name)(batch['tokens'])
                 null_context = getattr(get_model(self.cond_stage_model),
                                        function_name)(batch_uc['tokens'])
+
             else:
                 context = getattr(get_model(self.cond_stage_model),
                                   function_name)(batch)
                 null_context = getattr(get_model(self.cond_stage_model),
                                        function_name)(batch_uc)
         self.dynamic_unload(self.cond_stage_model,
                             'cond_stage_model',
@@ -554,20 +558,21 @@
                         guide_rescale=value_input.get('guide_rescale', 0.5),
                         discretization=value_input.get('discretization',
                                                        'trailing'),
                         show_progress=True,
                         seed=seed,
                         condition_fn=None,
                         clamp=None,
+                        sharpness=value_input.get('sharpness', 0.0),
                         percentile=None,
                         t_max=None,
                         t_min=None,
                         discard_penultimate_step=None,
                         intermediate_callback=intermediate_callback,
-                        cat_uc=cat_uc,
+                        cat_uc=value_input.get('cat_uc', cat_uc),
                         **kwargs)
 
                 self.dynamic_unload(self.diffusion_model,
                                     'diffusion_model',
                                     skip_loaded=True)
 
             # apply refiner
```

## scepter/modules/model/backbone/autoencoder/__init__.py

```diff
@@ -1,4 +1,5 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from scepter.modules.model.backbone.autoencoder.ae_module import (Decoder,
-                                                                  Encoder)
+                                                                  Encoder,
+                                                                  RDecoder)
```

## scepter/modules/model/backbone/autoencoder/ae_module.py

```diff
@@ -1,12 +1,13 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 import torch
 import torch.nn as nn
+from einops import repeat
 from torch.utils.checkpoint import checkpoint
 
 from scepter.modules.model.backbone.autoencoder.ae_utils import (
     XFORMERS_IS_AVAILBLE, AttnBlock, Downsample, MemoryEfficientAttention,
     Normalize, ResnetBlock, Upsample, nonlinearity)
 from scepter.modules.model.base_model import BaseModel
 from scepter.modules.model.registry import BACKBONES
@@ -240,14 +241,15 @@
 
     def construct_model(self):
         self.num_resolutions = len(self.ch_mult)
         AttentionBuilder = MemoryEfficientAttention if XFORMERS_IS_AVAILBLE else AttnBlock
 
         # compute in_ch_mult, block_in and curr_res at lowest res
         block_in = self.ch * self.ch_mult[self.num_resolutions - 1]
+        self.block_in = block_in
         curr_res = 1
         # z to block_in
         self.conv_in = torch.nn.Conv2d(self.z_channels,
                                        block_in,
                                        kernel_size=3,
                                        stride=1,
                                        padding=1)
@@ -319,14 +321,59 @@
 
         # middle
         if not self.use_checkpoint:
             h = self.mid_upsclae_transform(h, temb)
         else:
             h = checkpoint(self.mid_upsclae_transform, h, temb)
 
+        # end
+        if self.give_pre_end:
+            return h
+
+        h = self.norm_out(h)
+        h = nonlinearity(h)
+        h = self.conv_out(h)
+        if self.tanh_out:
+            h = torch.tanh(h)
+        return h
+
+    @staticmethod
+    def get_config_template():
+        return dict_to_yaml('BACKBONE',
+                            __class__.__name__,
+                            Decoder.para_dict,
+                            set_name=True)
+
+
+@BACKBONES.register_class()
+class RDecoder(Decoder):
+    def construct_model(self):
+        super().construct_model()
+        self.resize_level = nn.Sequential(
+            nn.Linear(self.block_in, self.block_in),
+            nn.SiLU(),
+            nn.Linear(self.block_in, self.block_in),
+        )
+
+    def forward(self, z, rembed=None):
+        # timestep embedding
+        temb = None
+        h = self.conv_in(z)
+        bs, channel, hdim, wdim = h.size()
+        if rembed is not None:
+            rembed = self.resize_level(rembed)
+            rembed = repeat(rembed, 'b e->  b e hd wd', hd=hdim, wd=wdim)
+            h = h + rembed
+
+        # middle
+        if not self.use_checkpoint:
+            h = self.mid_upsclae_transform(h, temb)
+        else:
+            h = checkpoint(self.mid_upsclae_transform, h, temb)
+
         # end
         if self.give_pre_end:
             return h
 
         h = self.norm_out(h)
         h = nonlinearity(h)
         h = self.conv_out(h)
```

## scepter/modules/model/backbone/unet/__init__.py

```diff
@@ -1,3 +1,5 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
-from scepter.modules.model.backbone.unet.unet_module import DiffusionUNet
+from scepter.modules.model.backbone.unet.unet_module import (DiffusionUNet,
+                                                             DiffusionUNetXL,
+                                                             LargenUNetXL)
```

## scepter/modules/model/backbone/unet/unet_module.py

```diff
@@ -4,16 +4,17 @@
 import copy
 from collections import OrderedDict
 
 import torch
 import torch.nn as nn
 
 from scepter.modules.model.backbone.unet.unet_utils import (
-    Downsample, ResBlock, SpatialTransformer, Timestep,
-    TimestepEmbedSequential, Upsample, conv_nd, linear, normalization,
+    BasicTransformerBlock, Downsample, ResBlock, SpatialTransformer,
+    SpatialTransformerV2, Timestep, TimestepEmbedSequential,
+    TransformerBlockV2, Upsample, conv_nd, linear, normalization,
     timestep_embedding, zero_module)
 from scepter.modules.model.base_model import BaseModel
 from scepter.modules.model.registry import BACKBONES
 from scepter.modules.model.utils.basic_utils import exists
 from scepter.modules.utils.config import dict_to_yaml
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.file_system import FS
@@ -947,7 +948,447 @@
 
     @staticmethod
     def get_config_template():
         return dict_to_yaml('BACKBONE',
                             __class__.__name__,
                             DiffusionUNetXL.para_dict,
                             set_name=True)
+
+
+@BACKBONES.register_class()
+class LargenUNetXL(DiffusionUNetXL):
+    para_dict = {
+        'TRANSFORMER_BLOCK_TYPE': {
+            'value': 'att_v1'
+        },
+        'IMAGE_SCALE': {
+            'value': 0.0,
+        },
+    }
+    para_dict.update(DiffusionUNetXL.para_dict)
+
+    def __init__(self, cfg, logger):
+        super().__init__(cfg, logger=logger)
+        self.init_params(cfg)
+        self.construct_network()
+
+    def init_params(self, cfg):
+        super().init_params(cfg)
+        self.transformer_block_type = cfg.get('TRANSFORMER_BLOCK_TYPE',
+                                              'att_v1')
+        TRANSFORMER_BLOCKS = {
+            'att_v1': BasicTransformerBlock,
+            'att_v2': TransformerBlockV2,
+        }
+        assert self.transformer_block_type in list(TRANSFORMER_BLOCKS.keys())
+        self.transformer_block = TRANSFORMER_BLOCKS[
+            self.transformer_block_type]
+        self.image_scale = cfg.get('IMAGE_SCALE', 0.0)
+        self.use_refine = cfg.get('USE_REFINE', False)
+
+    def construct_network(self):
+        in_channels = self.in_channels
+        model_channels = self.model_channels
+        out_channels = self.out_channels
+        attention_resolutions = self.attention_resolutions
+        channel_mult = self.channel_mult
+        num_classes = self.num_classes
+        num_heads = self.num_heads
+        num_head_channels = self.num_head_channels
+        dims = self.dims
+        dropout = self.dropout
+        use_checkpoint = self.use_checkpoint
+        use_scale_shift_norm = self.use_scale_shift_norm
+        disable_self_attentions = self.disable_self_attentions
+        disable_middle_self_attn = self.disable_middle_self_attn
+        transformer_depth = self.transformer_depth
+        transformer_depth_middle = self.transformer_depth_middle
+        context_dim = self.context_dim
+        use_linear_in_transformer = self.use_linear_in_transformer
+        resblock_updown = self.resblock_updown
+        conv_resample = self.conv_resample
+        adm_in_channels = self.adm_in_channels
+        transformer_block = self.transformer_block
+
+        time_embed_dim = model_channels * 4
+        self.time_embed = nn.Sequential(
+            linear(model_channels, time_embed_dim),
+            nn.SiLU(),
+            linear(time_embed_dim, time_embed_dim),
+        )
+
+        if self.num_classes is not None:
+            if isinstance(self.num_classes, int):
+                self.label_emb = nn.Embedding(num_classes, time_embed_dim)
+            elif self.num_classes == 'continuous':
+                print('setting up linear c_adm embedding layer')
+                self.label_emb = nn.Linear(1, time_embed_dim)
+            elif self.num_classes == 'timestep':
+                self.label_emb = nn.Sequential(
+                    Timestep(model_channels),
+                    nn.Sequential(
+                        linear(model_channels, time_embed_dim),
+                        nn.SiLU(),
+                        linear(time_embed_dim, time_embed_dim),
+                    ),
+                )
+            elif self.num_classes == 'sequential':
+                assert adm_in_channels is not None
+                self.label_emb = nn.Sequential(
+                    nn.Sequential(
+                        linear(adm_in_channels, time_embed_dim),
+                        nn.SiLU(),
+                        linear(time_embed_dim, time_embed_dim),
+                    ))
+            else:
+                raise ValueError()
+
+        self.input_blocks = nn.ModuleList([
+            TimestepEmbedSequential(
+                conv_nd(dims, in_channels, model_channels, 3, padding=1))
+        ])
+        self._feature_size = model_channels
+        input_block_chans = [model_channels]
+        input_down_flag = [False]
+        ch = model_channels
+        ds = 1
+        for level, mult in enumerate(channel_mult):
+            for nr in range(self.num_res_blocks[level]):
+                layers = [
+                    ResBlock(
+                        ch,
+                        time_embed_dim,
+                        dropout,
+                        out_channels=mult * model_channels,
+                        dims=dims,
+                        use_checkpoint=use_checkpoint,
+                        use_scale_shift_norm=use_scale_shift_norm,
+                    )
+                ]
+                ch = mult * model_channels
+                if ds in attention_resolutions:
+                    if num_head_channels == -1:
+                        dim_head = ch // num_heads
+                    else:
+                        num_heads = ch // num_head_channels
+                        dim_head = num_head_channels
+                    disabled_sa = disable_self_attentions[level] if exists(
+                        disable_self_attentions) else False
+
+                    layers.append(
+                        SpatialTransformerV2(
+                            ch,
+                            num_heads,
+                            dim_head,
+                            transformer_block=transformer_block,
+                            depth=transformer_depth[level],
+                            context_dim=context_dim,
+                            disable_self_attn=disabled_sa,
+                            use_linear=use_linear_in_transformer,
+                            use_checkpoint=use_checkpoint))
+                self.input_blocks.append(TimestepEmbedSequential(*layers))
+                self._feature_size += ch
+                input_block_chans.append(ch)
+                input_down_flag.append(False)
+            if level != len(channel_mult) - 1:
+                out_ch = ch
+                self.input_blocks.append(
+                    TimestepEmbedSequential(
+                        ResBlock(
+                            ch,
+                            time_embed_dim,
+                            dropout,
+                            out_channels=out_ch,
+                            dims=dims,
+                            use_checkpoint=use_checkpoint,
+                            use_scale_shift_norm=use_scale_shift_norm,
+                            down=True,
+                        ) if resblock_updown else Downsample(
+                            ch, conv_resample, dims=dims, out_channels=out_ch))
+                )
+                ch = out_ch
+                input_block_chans.append(ch)
+                input_down_flag.append(True)
+                ds *= 2
+                self._feature_size += ch
+        self._input_block_chans = copy.deepcopy(input_block_chans)
+        self._input_down_flag = input_down_flag
+
+        if num_head_channels == -1:
+            dim_head = ch // num_heads
+        else:
+            num_heads = ch // num_head_channels
+            dim_head = num_head_channels
+        self.middle_block = TimestepEmbedSequential(
+            ResBlock(
+                ch,
+                time_embed_dim,
+                dropout,
+                dims=dims,
+                use_checkpoint=use_checkpoint,
+                use_scale_shift_norm=use_scale_shift_norm,
+            ),
+            SpatialTransformerV2(ch,
+                                 num_heads,
+                                 dim_head,
+                                 transformer_block=transformer_block,
+                                 depth=transformer_depth_middle,
+                                 context_dim=context_dim,
+                                 disable_self_attn=disable_middle_self_attn,
+                                 use_linear=use_linear_in_transformer,
+                                 use_checkpoint=use_checkpoint),
+            ResBlock(
+                ch,
+                time_embed_dim,
+                dropout,
+                dims=dims,
+                use_checkpoint=use_checkpoint,
+                use_scale_shift_norm=use_scale_shift_norm,
+            ),
+        )
+        self._feature_size += ch
+        self._middle_block_chans = [ch]
+
+        self._output_block_chans = []
+        self.output_blocks = nn.ModuleList([])
+        for level, mult in list(enumerate(channel_mult))[::-1]:
+            for i in range(self.num_res_blocks[level] + 1):
+                ich = input_block_chans.pop()
+                layers = [
+                    ResBlock(
+                        ch + ich,
+                        time_embed_dim,
+                        dropout,
+                        out_channels=model_channels * mult,
+                        dims=dims,
+                        use_checkpoint=use_checkpoint,
+                        use_scale_shift_norm=use_scale_shift_norm,
+                    )
+                ]
+                ch = model_channels * mult
+                if ds in attention_resolutions:
+                    if num_head_channels == -1:
+                        dim_head = ch // num_heads
+                    else:
+                        num_heads = ch // num_head_channels
+                        dim_head = num_head_channels
+                    disabled_sa = disable_self_attentions[level] if exists(
+                        disable_self_attentions) else False
+                    layers.append(
+                        SpatialTransformerV2(
+                            ch,
+                            num_heads,
+                            dim_head,
+                            transformer_block=transformer_block,
+                            depth=transformer_depth[level],
+                            context_dim=context_dim,
+                            disable_self_attn=disabled_sa,
+                            use_linear=use_linear_in_transformer,
+                            use_checkpoint=use_checkpoint))
+                if level and i == self.num_res_blocks[level]:
+                    out_ch = ch
+                    layers.append(
+                        ResBlock(
+                            ch,
+                            time_embed_dim,
+                            dropout,
+                            out_channels=out_ch,
+                            dims=dims,
+                            use_checkpoint=use_checkpoint,
+                            use_scale_shift_norm=use_scale_shift_norm,
+                            up=True,
+                        ) if resblock_updown else Upsample(
+                            ch, conv_resample, dims=dims, out_channels=out_ch))
+                    ds //= 2
+
+                self.output_blocks.append(TimestepEmbedSequential(*layers))
+
+                self._feature_size += ch
+                self._output_block_chans.append(ch)
+
+        self.out = nn.Sequential(
+            normalization(ch),
+            nn.SiLU(),
+            zero_module(
+                conv_nd(dims, model_channels, out_channels, 3, padding=1)),
+        )
+
+        if self.use_refine:
+            self.ref_time_embed = copy.deepcopy(self.time_embed)
+            self.ref_label_emb = copy.deepcopy(self.label_emb)
+            self.ref_input_blocks = copy.deepcopy(self.input_blocks)
+            self.ref_input_blocks[0] = TimestepEmbedSequential(
+                conv_nd(dims, 4, model_channels, 3, padding=1))
+            self.ref_middle_block = copy.deepcopy(self.middle_block)
+            self.ref_output_blocks = copy.deepcopy(self.output_blocks)
+
+    def load_pretrained_model(self, pretrained_model):
+        if pretrained_model is not None:
+            with FS.get_from(pretrained_model,
+                             wait_finish=True) as local_model:
+                self.init_from_ckpt(local_model, ignore_keys=self.ignore_keys)
+
+    def init_from_ckpt(self, path, ignore_keys=list()):
+        if path.endswith('safetensors'):
+            from safetensors.torch import load_file as load_safetensors
+            sd = load_safetensors(path)
+        else:
+            sd = torch.load(path, map_location='cpu')
+
+        new_sd = OrderedDict()
+        for k, v in sd.items():
+            ignored = False
+            for ik in ignore_keys:
+                if ik in k:
+                    if we.rank == 0:
+                        self.logger.info(
+                            'Ignore key {} from state_dict.'.format(k))
+                    ignored = True
+                    break
+            if not ignored:
+                if k == 'input_blocks.0.0.weight':
+                    if we.rank == 0:
+                        self.logger.info(
+                            'Partial initial key {} from state_dict.'.format(
+                                k))
+                    new_v = torch.empty(320, self.in_channels, 3, 3)
+                    nn.init.zeros_(new_v)
+                    new_v[:, :v.shape[1]] = v
+                    new_sd[k] = new_v
+                    if self.use_refine:
+                        new_sd['ref_' + k] = v
+                else:
+                    new_sd[k] = v
+                    if self.use_refine:
+                        new_sd['ref_' + k] = v
+
+        missing, unexpected = self.load_state_dict(new_sd, strict=False)
+        if we.rank == 0:
+            self.logger.info(
+                f'Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys'
+            )
+            if len(missing) > 0:
+                self.logger.info(f'Missing Keys:\n {missing}')
+            if len(unexpected) > 0:
+                self.logger.info(f'\nUnexpected Keys:\n {unexpected}')
+
+    def forward(self, x, t=None, cond=dict(), **kwargs):
+        t_emb = timestep_embedding(t,
+                                   self.model_channels,
+                                   repeat_only=False,
+                                   legacy=True)
+        emb = self.time_embed(t_emb)
+
+        if isinstance(cond, dict):
+            if 'y' in cond:
+                assert self.num_classes is not None
+                emb = emb + self.label_emb(cond['y'])
+                if self.use_refine:
+                    ref_emb = self.ref_time_embed(t_emb)
+                    assert 'null_y' in cond
+                    cond_y = cond['y'].clone()
+                    cond_y[:, :cond['null_y'].shape[1]] = cond['null_y']
+                    ref_emb = ref_emb + self.ref_label_emb(cond_y)
+
+            if 'concat' in cond:
+                c = cond['concat']
+                x = torch.cat([x, c], dim=1)
+
+            context = cond.get('crossattn', None)
+            img_context = cond.get('img_crossattn', None)
+
+            task = cond['task']
+            image_scale = cond.get('image_scale', self.image_scale)
+            if 'Subject' in task and img_context is not None:
+                ip_enc_scale = image_scale
+                ip_dec_scale = image_scale
+                num_img_tokens = img_context.shape[1]
+                context = torch.cat([context, img_context], dim=1)
+            else:
+                ip_enc_scale = None
+                ip_dec_scale = None
+                num_img_tokens = None
+
+            ref = cond.get('ref_xt', None)
+            ref_context = cond.get('ref_crossattn', None)
+        else:
+            raise TypeError
+
+        hs = []
+        refs = []
+        h = x
+
+        if self.use_refine:
+            assert ref is not None and ref_context is not None
+            for i, (ref_module, module) in enumerate(
+                    zip(self.ref_input_blocks, self.input_blocks)):
+                ref = ref_module(ref, ref_emb, ref_context, caching=None)
+                h = module(h,
+                           emb,
+                           context,
+                           caching=None,
+                           scale=ip_enc_scale,
+                           num_img_token=num_img_tokens)
+                refs.append(ref)
+                hs.append(h)
+
+            ref = self.ref_middle_block(ref,
+                                        ref_emb,
+                                        ref_context,
+                                        caching=None)
+            h = self.middle_block(h,
+                                  emb,
+                                  context,
+                                  caching=None,
+                                  scale=ip_enc_scale,
+                                  num_img_token=num_img_tokens)
+
+            for i, (ref_module, module) in enumerate(
+                    zip(self.ref_output_blocks, self.output_blocks)):
+                cache = []
+                ref = torch.cat([ref, refs.pop()], dim=1)
+                ref = ref_module(ref,
+                                 ref_emb,
+                                 ref_context,
+                                 caching='write',
+                                 cache=cache)
+                h = torch.cat([h, hs.pop()], dim=1)
+                h = module(h,
+                           emb,
+                           context,
+                           caching='read',
+                           cache=cache,
+                           scale=ip_dec_scale,
+                           num_img_token=num_img_tokens)
+        else:
+            for module in self.input_blocks:
+                h = module(h,
+                           emb,
+                           context,
+                           caching=None,
+                           scale=ip_enc_scale,
+                           num_img_token=num_img_tokens)
+                hs.append(h)
+            h = self.middle_block(h,
+                                  emb,
+                                  context,
+                                  caching=None,
+                                  scale=ip_enc_scale,
+                                  num_img_token=num_img_tokens)
+            for module in self.output_blocks:
+                h = torch.cat([h, hs.pop()], dim=1)
+                h = module(h,
+                           emb,
+                           context,
+                           caching=None,
+                           scale=ip_dec_scale,
+                           num_img_token=num_img_tokens)
+
+        out = self.out(h)
+        return out
+
+    @staticmethod
+    def get_config_template():
+        return dict_to_yaml('BACKBONE',
+                            __class__.__name__,
+                            LargenUNetXL.para_dict,
+                            set_name=True)
```

## scepter/modules/model/backbone/unet/unet_utils.py

```diff
@@ -6,14 +6,15 @@
 from abc import abstractmethod
 from importlib import find_loader
 
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
+import torchvision.transforms.functional as TF
 from einops import rearrange, repeat
 from packaging import version
 
 from scepter.modules.model.utils.basic_utils import checkpoint, default, exists
 
 try:
     import xformers
@@ -167,20 +168,22 @@
 
 
 class TimestepEmbedSequential(nn.Sequential, TimestepBlock):
     """
     A sequential module that passes timestep embeddings to the children that
     support it as an extra input.
     """
-    def forward(self, x, emb, context=None, target_size=None):
+    def forward(self, x, emb, context=None, target_size=None, **kwargs):
         for layer in self:
             if isinstance(layer, TimestepBlock):
                 x = layer(x, emb)
             elif isinstance(layer, SpatialTransformer):
                 x = layer(x, context)
+            elif isinstance(layer, SpatialTransformerV2):
+                x = layer(x, context, **kwargs)
             elif isinstance(layer, Upsample):
                 x = layer(x, target_size)
             else:
                 x = layer(x)
         return x
 
 
@@ -860,14 +863,100 @@
             b, self.heads, out.shape[1],
             self.dim_head).permute(0, 2, 1,
                                    3).reshape(b, out.shape[1],
                                               self.heads * self.dim_head))
         return self.to_out(out)
 
 
+class XFormersMHA_IP(nn.Module):
+    def __init__(self,
+                 query_dim,
+                 context_dim=None,
+                 heads=8,
+                 dim_head=64,
+                 dropout=0.0):
+        super().__init__()
+        inner_dim = dim_head * heads
+        context_dim = default(context_dim, query_dim)
+
+        self.heads = heads
+        self.dim_head = dim_head
+
+        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)
+        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)
+        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)
+        self.to_k_ip = nn.Linear(context_dim, inner_dim, bias=False)
+        self.to_v_ip = nn.Linear(context_dim, inner_dim, bias=False)
+
+        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim),
+                                    nn.Dropout(dropout))
+        self.attention_op = None
+
+    def forward(self,
+                x,
+                context=None,
+                mask=None,
+                scale=None,
+                num_img_token=None):
+        q = self.to_q(x)
+        context = default(context, x)
+
+        if scale is not None and num_img_token is not None:
+            eos = context.shape[1] - num_img_token
+            txt_context = context[:, :eos, :]
+            img_context = context[:, eos:, :]
+
+            k = self.to_k(txt_context)
+            v = self.to_v(txt_context)
+            k_i = self.to_k_ip(img_context)
+            v_i = self.to_v_ip(img_context)
+
+            b, _, _ = q.shape
+            q, k, v, k_i, v_i = map(
+                lambda t: t.unsqueeze(3).reshape(b, t.shape[
+                    1], self.heads, self.dim_head).permute(0, 2, 1, 3).reshape(
+                        b * self.heads, t.shape[1], self.dim_head).contiguous(
+                        ),
+                (q, k, v, k_i, v_i),
+            )
+
+            # actually compute the attention, what we cannot get enough of
+            txt_out = xformers.ops.memory_efficient_attention(
+                q, k, v, attn_bias=None, op=self.attention_op)
+            img_out = xformers.ops.memory_efficient_attention(
+                q, k_i, v_i, attn_bias=None, op=self.attention_op)
+            out = txt_out + scale * img_out
+        else:
+            k = self.to_k(context)
+            v = self.to_v(context)
+            b, _, _ = q.shape
+            q, k, v = map(
+                lambda t: t.unsqueeze(3).reshape(b, t.shape[
+                    1], self.heads, self.dim_head).permute(0, 2, 1, 3).reshape(
+                        b * self.heads, t.shape[1], self.dim_head).contiguous(
+                        ),
+                (q, k, v),
+            )
+            out = xformers.ops.memory_efficient_attention(q,
+                                                          k,
+                                                          v,
+                                                          attn_bias=None,
+                                                          op=self.attention_op)
+
+        # TODO: Use this directly in the attention operation, as a bias
+        if exists(mask):
+            raise NotImplementedError
+        out = (out.unsqueeze(0).reshape(
+            b, self.heads, out.shape[1],
+            self.dim_head).permute(0, 2, 1,
+                                   3).reshape(b, out.shape[1],
+                                              self.heads * self.dim_head))
+        return self.to_out(out)
+
+
 class BasicTransformerBlock(nn.Module):
     def __init__(self,
                  dim,
                  n_heads,
                  d_head,
                  dropout=0.,
                  context_dim=None,
@@ -904,14 +993,73 @@
         x = self.attn1(self.norm1(x),
                        context=context if self.disable_self_attn else None) + x
         x = self.attn2(self.norm2(x), context=context) + x
         x = self.ff(self.norm3(x)) + x
         return x
 
 
+class TransformerBlockV2(nn.Module):
+    def __init__(self,
+                 query_dim,
+                 n_heads,
+                 d_head,
+                 dropout=0.,
+                 context_dim=None,
+                 gated_ff=True,
+                 use_checkpoint=False,
+                 disable_self_attn=False):
+        super().__init__()
+        self.disable_self_attn = disable_self_attn
+        self.attn1 = MemoryEfficientCrossAttention(query_dim=query_dim,
+                                                   heads=n_heads,
+                                                   dim_head=d_head,
+                                                   dropout=dropout,
+                                                   context_dim=None)
+        self.ff = FeedForward(query_dim, dropout=dropout, glu=gated_ff)
+        self.attn2 = XFormersMHA_IP(query_dim=query_dim,
+                                    heads=n_heads,
+                                    dim_head=d_head,
+                                    context_dim=context_dim)
+        self.norm1 = nn.LayerNorm(query_dim)
+        self.norm2 = nn.LayerNorm(query_dim)
+        self.norm3 = nn.LayerNorm(query_dim)
+        self.use_checkpoint = use_checkpoint
+
+    def forward(self,
+                x,
+                context,
+                caching=None,
+                cache=None,
+                scale=None,
+                num_img_token=None,
+                **kwargs):
+        y = self.norm1(x)
+        if caching == 'write':
+            assert isinstance(cache, list)
+            cache.append(y)
+            x = self.attn1(y, context=None) + x
+        elif caching == 'read':
+            assert isinstance(cache, list) and len(cache) > 0
+            c = cache.pop(0)
+            self_ctx = torch.cat([y, c], dim=1)
+            x = self.attn1(y, context=self_ctx) + x
+        elif caching is None:
+            x = self.attn1(y, context=None) + x
+        else:
+            assert False
+
+        x = self.attn2(self.norm2(x),
+                       context=context,
+                       scale=scale,
+                       num_img_token=num_img_token) + x
+        x = self.ff(self.norm3(x)) + x
+
+        return x
+
+
 class SpatialTransformer(nn.Module):
     """
     Transformer block for image-like data.
     First, project the input (aka embedding)
     and reshape to b, t, d.
     Then apply standard transformer action.
     Finally, reshape to image
@@ -999,7 +1147,112 @@
             x = block(x, context=context[i])
         if self.use_linear:
             x = self.proj_out(x)
         x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w).contiguous()
         if not self.use_linear:
             x = self.proj_out(x)
         return x + x_in
+
+
+class SpatialTransformerV2(nn.Module):
+    """
+    Transformer block for image-like data.
+    First, project the input (aka embedding)
+    and reshape to b, t, d.
+    Then apply standard transformer action.
+    Finally, reshape to image
+    NEW: use_linear for more efficiency instead of the 1x1 convs
+    """
+    def __init__(self,
+                 in_channels,
+                 n_heads,
+                 d_head,
+                 transformer_block,
+                 depth=1,
+                 dropout=0.,
+                 context_dim=None,
+                 disable_self_attn=False,
+                 use_linear=False,
+                 use_checkpoint=True):
+        super().__init__()
+        if exists(context_dim) and not isinstance(context_dim, list):
+            context_dim = [context_dim]
+
+        if exists(context_dim) and not isinstance(context_dim, (list)):
+            context_dim = [context_dim]
+        if exists(context_dim) and isinstance(context_dim, list):
+            if depth != len(context_dim):
+                print(
+                    f'WARNING: {self.__class__.__name__}: Found context dims {context_dim} of'
+                    f" depth {len(context_dim)}, which does not match the specified 'depth' of"
+                    f' {depth}. Setting context_dim to {depth * [context_dim[0]]} now.'
+                )
+                # depth does not match context dims.
+                assert all(
+                    map(lambda x: x == context_dim[0], context_dim)
+                ), 'need homogenous context_dim to match depth automatically'
+                context_dim = depth * [context_dim[0]]
+        elif context_dim is None:
+            context_dim = [None] * depth
+
+        self.in_channels = in_channels
+        inner_dim = n_heads * d_head
+        self.norm = normalization(in_channels)
+        if not use_linear:
+            self.proj_in = nn.Conv2d(in_channels,
+                                     inner_dim,
+                                     kernel_size=1,
+                                     stride=1,
+                                     padding=0)
+        else:
+            self.proj_in = nn.Linear(in_channels, inner_dim)
+
+        self.transformer_blocks = nn.ModuleList([
+            transformer_block(inner_dim,
+                              n_heads,
+                              d_head,
+                              dropout=dropout,
+                              context_dim=context_dim[d],
+                              disable_self_attn=disable_self_attn,
+                              use_checkpoint=use_checkpoint)
+            for d in range(depth)
+        ])
+        if not use_linear:
+            self.proj_out = zero_module(
+                nn.Conv2d(inner_dim,
+                          in_channels,
+                          kernel_size=1,
+                          stride=1,
+                          padding=0))
+        else:
+            self.proj_out = zero_module(nn.Linear(in_channels, inner_dim))
+        self.use_linear = use_linear
+
+    def forward(self, x, context=None, **kwargs):
+        # note: if no context is given, cross-attention defaults to self-attention
+        if not isinstance(context, list):
+            context = [context]
+        b, c, h, w = x.shape
+
+        ref_mask = kwargs.pop('ref_mask', None)
+        if ref_mask is not None:
+            ref_mask = TF.resize(ref_mask, (h, w), antialias=True)
+            ref_mask = (ref_mask > 0.5).float()
+            ref_mask = rearrange(ref_mask, 'b c h w -> b (h w) c').contiguous()
+
+        x_in = x
+        x = self.norm(x)
+        if not self.use_linear:
+            x = self.proj_in(x)
+        x = rearrange(x, 'b c h w -> b (h w) c').contiguous()
+        if self.use_linear:
+            x = self.proj_in(x)
+        for i, block in enumerate(self.transformer_blocks):
+            if i > 0 and len(context) == 1:
+                i = 0  # use same context for each block
+            x = block(x, context=context[i], ref_mask=ref_mask, **kwargs)
+        if self.use_linear:
+            x = self.proj_out(x)
+        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w).contiguous()
+        if not self.use_linear:
+            x = self.proj_out(x)
+        return x + x_in
```

## scepter/modules/model/backbone/video/video_transformer.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,9 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
-
-import math
-
-import torch
-import torch.nn as nn
-import torch.nn.functional
-from einops import rearrange
-
-from scepter.modules.model.backbone.video.init_helper import (
-    _init_transformer_weights, trunc_normal_)
-from scepter.modules.model.registry import BACKBONES, BRICKS, STEMS
-from scepter.modules.utils.config import dict_to_yaml
 '''
 The implementations of vivit as https://arxiv.org/abs/2103.15691.
 The following setting alined the proposed model in the paper above.
 Model1: Spatio-temporal attention.
     class: VideoTransformer
     stem: PatchEmbedStem/TubeletEmbeddingStem
     branch: BaseTransformerLayer
@@ -35,14 +23,26 @@
 TimesFormer:
     class: VideoTransformer
     stem: PatchEmbedStem [drop_path=0.0]
     branch: TimesformerLayer [attn_dropout = 0.1 ff_dropout=0.1]
     complexity: (n_h * n_w) ** 2 + O(attn_temp)
 '''
 
+import math
+
+import torch
+import torch.nn as nn
+import torch.nn.functional
+from einops import rearrange
+
+from scepter.modules.model.backbone.video.init_helper import (
+    _init_transformer_weights, trunc_normal_)
+from scepter.modules.model.registry import BACKBONES, BRICKS, STEMS
+from scepter.modules.utils.config import dict_to_yaml
+
 
 @BACKBONES.register_class()
 class VideoTransformer(nn.Module):
     para_dict = {
         'NUM_INPUT_CHANNELS': {
             'value': 3,
             'description': "the input frames's channel!"
```

## scepter/modules/model/embedder/__init__.py

```diff
@@ -1,7 +1,10 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
+
 from scepter.modules.model.embedder.embedder import (ConcatTimestepEmbedderND,
                                                      FrozenCLIPEmbedder,
                                                      FrozenOpenCLIPEmbedder,
                                                      FrozenOpenCLIPEmbedder2,
-                                                     GeneralConditioner)
+                                                     GeneralConditioner,
+                                                     IPAdapterPlusEmbedder,
+                                                     RefCrossEmbedder)
```

## scepter/modules/model/embedder/embedder.py

```diff
@@ -18,17 +18,18 @@
 from scepter.modules.model.registry import EMBEDDERS
 from scepter.modules.model.utils.basic_utils import expand_dims_like
 from scepter.modules.utils.config import dict_to_yaml
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.file_system import FS
 
 from .base_embedder import BaseEmbedder
+from .resampler import Resampler
 
 try:
-    from transformers import CLIPTextModel, CLIPTokenizer
+    from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection
 except Exception as e:
     warnings.warn(
         f'Import transformers error, please deal with this problem: {e}')
 
 
 def autocast(f, enabled=True):
     def do_autocast(*args, **kwargs):
@@ -510,14 +511,101 @@
         return dict_to_yaml('MODELS',
                             __class__.__name__,
                             ConcatTimestepEmbedderND.para_dict,
                             set_name=True)
 
 
 @EMBEDDERS.register_class()
+class IPAdapterPlusEmbedder(BaseEmbedder):
+    def __init__(self, cfg, logger=None):
+        super().__init__(cfg, logger=logger)
+
+        with FS.get_dir_to_local_dir(cfg.CLIP_DIR,
+                                     wait_finish=True) as local_path:
+            self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(
+                local_path)
+
+        self.image_proj_model = Resampler(
+            dim=self.cfg.get('IN_DIM', 768),
+            depth=self.cfg.get('DEPTH', 4),
+            dim_head=64,
+            heads=self.cfg.get('HEADS', 12),
+            num_queries=self.cfg.get('NUM_TOKENS', 16),
+            embedding_dim=self.image_encoder.config.hidden_size,
+            output_dim=self.cfg.get('CROSSATTN_DIM', 768),
+            ff_mult=4,
+        )
+
+        with FS.get_from(cfg.PRETRAINED_MODEL, wait_finish=True) as local_path:
+            ckpt = torch.load(local_path, map_location='cpu')
+            self.image_proj_model.load_state_dict(ckpt['image_proj'],
+                                                  strict=True)
+
+        self.patch_projector = nn.Linear(self.image_encoder.config.hidden_size,
+                                         self.cfg.get('CROSSATTN_DIM', 768))
+
+    def encode(self, ref_ip, ref_detail):
+        encoder_output = self.image_encoder(ref_ip, output_hidden_states=True)
+        image_prompt_embeds = self.image_proj_model(
+            encoder_output.hidden_states[-2])
+        encoder_output_2 = self.image_encoder(ref_detail,
+                                              output_hidden_states=True)
+        image_patch_embeds = self.patch_projector(
+            encoder_output_2.last_hidden_state)
+        out = {
+            'img_crossattn': image_prompt_embeds,
+            'ref_crossattn': image_patch_embeds,
+        }
+        return out
+
+    def forward(self, ref_ip, ref_detail):
+        return self.encode(ref_ip, ref_detail)
+
+
+class RefCrossEmbedder(BaseEmbedder):
+    def __init__(self, cfg, logger=None):
+        super().__init__(cfg, logger=logger)
+
+        with FS.get_dir_to_local_dir(cfg.CLIP_DIR,
+                                     wait_finish=True) as local_path:
+            self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(
+                local_path)
+
+        self.patch_projector = nn.Linear(self.image_encoder.config.hidden_size,
+                                         self.cfg.get('CROSSATTN_DIM', 768))
+
+    def encode(self, img):
+        encoder_output = self.image_encoder(img, output_hidden_states=True)
+        image_patch_embeds = self.patch_projector(
+            encoder_output.last_hidden_state)
+        out = {
+            'ref_crossattn': image_patch_embeds,
+        }
+        return out
+
+    def forward(self, img):
+        return self.encode(img)
+
+
+@EMBEDDERS.register_class()
+class TransparentEmbedder(BaseEmbedder):
+    def forward(self, *args):
+        out = dict()
+        for key, val in zip(self.input_keys, args):
+            out[key] = val
+        return out
+
+
+@EMBEDDERS.register_class()
+class NoiseConcatEmbedder(BaseEmbedder):
+    def forward(self, *args):
+        return {'concat': torch.cat(args, dim=1)}
+
+
+@EMBEDDERS.register_class()
 class GeneralConditioner(BaseEmbedder):
     OUTPUT_DIM2KEYS = {2: 'y', 3: 'crossattn', 4: 'concat', 5: 'concat'}
     KEY2CATDIM = {'y': 1, 'crossattn': 2, 'concat': 1}
     para_dict = {
         'EMBEDDERS': [],
         'USE_GRAD': {
             'value': False,
@@ -594,50 +682,66 @@
             force_zero_embeddings = []
         for embedder in self.embedders:
             embedding_context = nullcontext if hasattr(
                 embedder, 'use_grad') and embedder.use_grad else torch.no_grad
             with embedding_context():
                 if hasattr(embedder, 'input_key') and (embedder.input_key
                                                        is not None):
+                    if embedder.input_key not in batch:
+                        continue
                     if embedder.legacy_ucg_val is not None:
                         batch = self.possibly_get_ucg_val(embedder, batch)
                     emb_out = embedder(batch[embedder.input_key])
                 elif hasattr(embedder, 'input_keys'):
+                    if any([k not in batch for k in embedder.input_keys]):
+                        continue
                     emb_out = embedder(
                         *[batch[k] for k in embedder.input_keys])
-            assert isinstance(
-                emb_out, (torch.Tensor, list, tuple)
-            ), f'encoder outputs must be tensors or a sequence, but got {type(emb_out)}'
-            if not isinstance(emb_out, (list, tuple)):
-                emb_out = [emb_out]
-            for emb in emb_out:
-                # print("emb.shape", emb.shape)
-                # print("emb.input_keys", embedder.input_keys)
-                out_key = self.OUTPUT_DIM2KEYS[emb.dim()]
-                if embedder.ucg_rate > 0.0 and embedder.legacy_ucg_val is None:
-                    emb = (expand_dims_like(
-                        torch.bernoulli(
-                            (1.0 - embedder.ucg_rate) *
-                            torch.ones(emb.shape[0], device=emb.device)),
-                        emb,
-                    ) * emb)
-                if (hasattr(embedder, 'input_keys')):
-                    if np.sum(
-                            np.array([
-                                key in force_zero_embeddings
-                                for key in embedder.input_keys
-                            ])) > 0:
-                        emb = torch.zeros_like(emb)
-                if out_key in output:
-                    output[out_key] = torch.cat((output[out_key], emb),
-                                                self.KEY2CATDIM[out_key])
-                else:
-                    output[out_key] = emb
-            # if "y" in output:
-            #     print("out.shape", output["y"].shape)
+
+            if isinstance(emb_out, dict):
+                for key, val in emb_out.items():
+                    if key in output:
+                        assert key in self.KEY2CATDIM
+                        output[key] = torch.cat([output[key], val],
+                                                dim=self.KEY2CATDIM[key])
+                    else:
+                        output[key] = val
+            else:
+                assert isinstance(
+                    emb_out, (torch.Tensor, list, tuple)
+                ), f'encoder outputs must be tensors or a sequence, but got {type(emb_out)}'
+
+                if not isinstance(emb_out, (list, tuple)):
+                    emb_out = [emb_out]
+
+                for emb in emb_out:
+                    out_key = self.OUTPUT_DIM2KEYS[emb.dim()]
+
+                    if embedder.ucg_rate > 0.0 and embedder.legacy_ucg_val is None:
+                        emb = (expand_dims_like(
+                            torch.bernoulli(
+                                (1.0 - embedder.ucg_rate) *
+                                torch.ones(emb.shape[0], device=emb.device)),
+                            emb,
+                        ) * emb)
+
+                    if (hasattr(embedder, 'input_keys')):
+                        if np.sum(
+                                np.array([
+                                    key in force_zero_embeddings
+                                    for key in embedder.input_keys
+                                ])) > 0:
+                            emb = torch.zeros_like(emb)
+
+                    if out_key in output:
+                        output[out_key] = torch.cat((output[out_key], emb),
+                                                    self.KEY2CATDIM[out_key])
+                    else:
+                        output[out_key] = emb
+
         return output
 
     def get_unconditional_conditioning(self,
                                        batch_c,
                                        batch_uc=None,
                                        force_uc_zero_embeddings=None):
         if force_uc_zero_embeddings is None:
```

## scepter/modules/model/head/__init__.py

```diff
@@ -1,5 +1,8 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
-from scepter.modules.model.head.classifier_head import (
-    ClassifierHead, CosineLinearHead, TransformerHead, TransformerHeadx2,
-    VideoClassifierHead, VideoClassifierHeadx2)
+from scepter.modules.model.head.classifier_head import (ClassifierHead,
+                                                        CosineLinearHead,
+                                                        TransformerHead,
+                                                        TransformerHeadx2,
+                                                        VideoClassifierHead,
+                                                        VideoClassifierHeadx2)
```

## scepter/modules/model/network/autoencoder/ae_kl.py

```diff
@@ -48,15 +48,15 @@
             return torch.Tensor([0.])
         logtwopi = np.log(2.0 * np.pi)
         return 0.5 * torch.sum(logtwopi + self.logvar +
                                torch.pow(sample - self.mean, 2) / self.var,
                                dim=dims)
 
     def mode(self):
-        print('*** use DiagonalGaussianDistribution.mode() ***')
+        # print('*** use DiagonalGaussianDistribution.mode() ***')
         return self.mean
 
 
 @MODELS.register_class()
 class AutoencoderKL(TrainModule):
     para_dict = {
         'ENCODER': {},
```

## scepter/modules/model/network/diffusion/diffusion.py

```diff
@@ -9,28 +9,169 @@
 
 import torch
 
 from .schedules import karras_schedule
 from .solvers import (sample_ddim, sample_dpm_2, sample_dpm_2_ancestral,
                       sample_dpmpp_2m, sample_dpmpp_2m_sde,
                       sample_dpmpp_2s_ancestral, sample_dpmpp_sde,
-                      sample_euler, sample_euler_ancestral, sample_heun,
-                      sample_img2img_euler, sample_img2img_euler_ancestral)
+                      sample_euler, sample_euler_ancestral, sample_heun)
 
 __all__ = ['GaussianDiffusion']
 
 
 def _i(tensor, t, x):
     """
     Index tensor using t and format the output according to x.
     """
     shape = (x.size(0), ) + (1, ) * (x.ndim - 1)
     return tensor[t.to(tensor.device)].view(shape).to(x.device)
 
 
+def _unpack_2d_ks(kernel_size):
+    if isinstance(kernel_size, int):
+        ky = kx = kernel_size
+    else:
+        assert len(
+            kernel_size) == 2, '2D Kernel size should have a length of 2.'
+        ky, kx = kernel_size
+
+    ky = int(ky)
+    kx = int(kx)
+    return ky, kx
+
+
+def _compute_zero_padding(kernel_size):
+    ky, kx = _unpack_2d_ks(kernel_size)
+    return (ky - 1) // 2, (kx - 1) // 2
+
+
+def _bilateral_blur(
+    input,
+    guidance,
+    kernel_size,
+    sigma_color,
+    sigma_space,
+    border_type='reflect',
+    color_distance_type='l1',
+):
+
+    if isinstance(sigma_color, torch.Tensor):
+        sigma_color = sigma_color.to(device=input.device,
+                                     dtype=input.dtype).view(-1, 1, 1, 1, 1)
+
+    ky, kx = _unpack_2d_ks(kernel_size)
+    pad_y, pad_x = _compute_zero_padding(kernel_size)
+
+    padded_input = torch.nn.functional.pad(input, (pad_x, pad_x, pad_y, pad_y),
+                                           mode=border_type)
+    unfolded_input = padded_input.unfold(2, ky, 1).unfold(3, kx, 1).flatten(
+        -2)  # (B, C, H, W, Ky x Kx)
+
+    if guidance is None:
+        guidance = input
+        unfolded_guidance = unfolded_input
+    else:
+        padded_guidance = torch.nn.functional.pad(guidance,
+                                                  (pad_x, pad_x, pad_y, pad_y),
+                                                  mode=border_type)
+        unfolded_guidance = padded_guidance.unfold(2, ky, 1).unfold(
+            3, kx, 1).flatten(-2)  # (B, C, H, W, Ky x Kx)
+
+    diff = unfolded_guidance - guidance.unsqueeze(-1)
+    if color_distance_type == 'l1':
+        color_distance_sq = diff.abs().sum(1, keepdim=True).square()
+    elif color_distance_type == 'l2':
+        color_distance_sq = diff.square().sum(1, keepdim=True)
+    else:
+        raise ValueError('color_distance_type only acceps l1 or l2')
+    color_kernel = (-0.5 / sigma_color**2 *
+                    color_distance_sq).exp()  # (B, 1, H, W, Ky x Kx)
+
+    space_kernel = get_gaussian_kernel2d(kernel_size,
+                                         sigma_space,
+                                         device=input.device,
+                                         dtype=input.dtype)
+    space_kernel = space_kernel.view(-1, 1, 1, 1, kx * ky)
+
+    kernel = space_kernel * color_kernel
+    out = (unfolded_input * kernel).sum(-1) / kernel.sum(-1)
+    return out
+
+
+def get_gaussian_kernel1d(
+    kernel_size,
+    sigma,
+    force_even,
+    *,
+    device=None,
+    dtype=None,
+):
+
+    return gaussian(kernel_size, sigma, device=device, dtype=dtype)
+
+
+def gaussian(window_size, sigma, *, device=None, dtype=None):
+
+    batch_size = sigma.shape[0]
+
+    x = (torch.arange(window_size, device=sigma.device, dtype=sigma.dtype) -
+         window_size // 2).expand(batch_size, -1)
+
+    if window_size % 2 == 0:
+        x = x + 0.5
+
+    gauss = torch.exp(-x.pow(2.0) / (2 * sigma.pow(2.0)))
+
+    return gauss / gauss.sum(-1, keepdim=True)
+
+
+def get_gaussian_kernel2d(
+    kernel_size,
+    sigma,
+    force_even=False,
+    *,
+    device=None,
+    dtype=None,
+):
+
+    sigma = torch.Tensor([[sigma, sigma]]).to(device=device, dtype=dtype)
+
+    ksize_y, ksize_x = _unpack_2d_ks(kernel_size)
+    sigma_y, sigma_x = sigma[:, 0, None], sigma[:, 1, None]
+
+    kernel_y = get_gaussian_kernel1d(ksize_y,
+                                     sigma_y,
+                                     force_even,
+                                     device=device,
+                                     dtype=dtype)[..., None]
+    kernel_x = get_gaussian_kernel1d(ksize_x,
+                                     sigma_x,
+                                     force_even,
+                                     device=device,
+                                     dtype=dtype)[..., None]
+
+    return kernel_y * kernel_x.view(-1, 1, ksize_x)
+
+
+def adaptive_anisotropic_filter(x, g=None):
+    if g is None:
+        g = x
+    s, m = torch.std_mean(g, dim=(1, 2, 3), keepdim=True)
+    s = s + 1e-5
+    guidance = (g - m) / s
+    y = _bilateral_blur(x,
+                        guidance,
+                        kernel_size=(13, 13),
+                        sigma_color=3.0,
+                        sigma_space=3.0,
+                        border_type='reflect',
+                        color_distance_type='l1')
+    return y
+
+
 class GaussianDiffusion(object):
     def __init__(self, sigmas, prediction_type='eps'):
         assert prediction_type in {'x0', 'eps', 'v'}
         self.sigmas = sigmas  # noise coefficients
         self.alphas = torch.sqrt(1 - sigmas**2)  # signal coefficients
         self.num_timesteps = len(sigmas)
         self.prediction_type = prediction_type
@@ -49,14 +190,15 @@
                 t,
                 s,
                 model,
                 model_kwargs={},
                 guide_scale=None,
                 guide_rescale=None,
                 clamp=None,
+                sharpness=0.0,
                 percentile=None,
                 cat_uc=False,
                 **kwargs):
         """
         Apply one step of denoising from the posterior distribution q(x_s | x_t, x0).
         Since x0 is not available, estimate the denoising results using the learned
         distribution p(x_s | x_t, \hat{x}_0 == f(x_t)). # noqa
@@ -75,62 +217,107 @@
         coef1 = betas * alphas_s / sigmas**2
         coef2 = (alphas * sigmas_s**2) / (alphas_s * sigmas**2)
         var = betas * (sigmas_s / sigmas)**2
         log_var = torch.log(var).clamp_(-20, 20)
 
         # prediction
         if guide_scale is None:
-            assert isinstance(model_kwargs, dict)
-            out = model(xt, t=t, **model_kwargs, **kwargs)
+            if isinstance(model_kwargs, dict):
+                out = model(xt, t=t, **model_kwargs, **kwargs)
+            elif isinstance(model_kwargs, list) and len(model_kwargs) > 0:
+                out = model(xt, t=t, **model_kwargs[0], **kwargs)
+            else:
+                raise Exception('Error')
         else:
             # classifier-free guidance (arXiv:2207.12598)
             # model_kwargs[0]: conditional kwargs
             # model_kwargs[1]: non-conditional kwargs
-            assert isinstance(model_kwargs, list) and len(model_kwargs) == 2
-
-            if guide_scale == 1.:
-                out = model(xt, t=t, **model_kwargs[0], **kwargs)
-            else:
-                if cat_uc:
-
-                    def parse_model_kwargs(prev_value, value):
-                        if isinstance(value, torch.Tensor):
-                            prev_value = torch.cat([prev_value, value], dim=0)
-                        elif isinstance(value, dict):
-                            for k, v in value.items():
-                                prev_value[k] = parse_model_kwargs(
-                                    prev_value[k], v)
-                        elif isinstance(value, list):
-                            for idx, v in enumerate(value):
-                                prev_value[idx] = parse_model_kwargs(
-                                    prev_value[idx], v)
-                        return prev_value
-
-                    all_model_kwargs = copy.deepcopy(model_kwargs[0])
-                    for model_kwarg in model_kwargs[1:]:
-                        for key, value in model_kwarg.items():
-                            all_model_kwargs[key] = parse_model_kwargs(
-                                all_model_kwargs[key], value)
-                    all_out = model(xt.repeat(2, 1, 1, 1),
-                                    t=t.repeat(2),
-                                    **all_model_kwargs,
-                                    **kwargs)
-                    y_out, u_out = all_out.chunk(2)
+            assert isinstance(model_kwargs, list) and len(model_kwargs) >= 2
+            if isinstance(guide_scale, float) or isinstance(guide_scale, int):
+                assert len(model_kwargs) == 2
+                if guide_scale == 1.:
+                    out = model(xt, t=t, **model_kwargs[0], **kwargs)
                 else:
-                    y_out = model(xt, t=t, **model_kwargs[0], **kwargs)
-                    u_out = model(xt, t=t, **model_kwargs[1], **kwargs)
-                out = u_out + guide_scale * (y_out - u_out)
-
-                # rescale the output according to arXiv:2305.08891
-                if guide_rescale is not None:
-                    assert guide_rescale >= 0 and guide_rescale <= 1
-                    ratio = (y_out.flatten(1).std(dim=1) /
-                             (out.flatten(1).std(dim=1) +
-                              1e-12)).view((-1, ) + (1, ) * (y_out.ndim - 1))
-                    out *= guide_rescale * ratio + (1 - guide_rescale) * 1.0
+                    if cat_uc:
+
+                        def parse_model_kwargs(prev_value, value):
+                            if isinstance(value, torch.Tensor):
+                                prev_value = torch.cat([prev_value, value],
+                                                       dim=0)
+                            elif isinstance(value, dict):
+                                for k, v in value.items():
+                                    prev_value[k] = parse_model_kwargs(
+                                        prev_value[k], v)
+                            elif isinstance(value, list):
+                                for idx, v in enumerate(value):
+                                    prev_value[idx] = parse_model_kwargs(
+                                        prev_value[idx], v)
+                            return prev_value
+
+                        all_model_kwargs = copy.deepcopy(model_kwargs[0])
+                        for model_kwarg in model_kwargs[1:]:
+                            for key, value in model_kwarg.items():
+                                all_model_kwargs[key] = parse_model_kwargs(
+                                    all_model_kwargs[key], value)
+                        all_out = model(xt.repeat(2, 1, 1, 1),
+                                        t=t.repeat(2),
+                                        **all_model_kwargs,
+                                        **kwargs)
+                        y_out, u_out = all_out.chunk(2)
+                    else:
+                        y_out = model(xt, t=t, **model_kwargs[0], **kwargs)
+                        u_out = model(xt, t=t, **model_kwargs[1], **kwargs)
+                    # todo sharpness
+                    # sharpness sampling
+                    if sharpness is not None and sharpness > 0:
+                        positive_x0 = alphas * xt - sigmas * y_out
+                        negative_x0 = alphas * xt - sigmas * u_out
+
+                        positive_eps = xt - positive_x0
+                        negative_eps = xt - negative_x0
+
+                        global_diffusion_progress = (
+                            1 - t / 999.0).detach().cpu().numpy().tolist()[0]
+                        alpha = 0.001 * sharpness * global_diffusion_progress
+                        positive_eps_degraded = adaptive_anisotropic_filter(
+                            x=positive_eps, g=positive_x0)
+                        positive_eps_degraded_weighted = positive_eps_degraded * alpha + positive_eps * (
+                            1.0 - alpha)
+
+                        final_eps = negative_eps + guide_scale * (
+                            positive_eps_degraded_weighted - negative_eps)
+                        final_x0 = xt - final_eps
+                        out = (alphas * xt - final_x0) / sigmas
+                    else:
+                        out = u_out + guide_scale * (y_out - u_out)
+            elif isinstance(guide_scale, dict):
+                assert len(model_kwargs) == 3
+                y_out = model(xt, t=t, **model_kwargs[0], **kwargs)
+                m_out = model(xt, t=t, **model_kwargs[1], **kwargs)
+                u_out = model(xt, t=t, **model_kwargs[2], **kwargs)
+                out = u_out + guide_scale['image'] * (
+                    m_out - u_out) + guide_scale['text'] * (y_out - m_out)
+            elif isinstance(guide_scale, list):
+                assert len(guide_scale) == len(model_kwargs) - 1
+                y_out = model(xt, t=t, **model_kwargs[0], **kwargs)
+                outs = [y_out]
+                for i in range(1, len(model_kwargs)):
+                    outs.append(model(xt, t=t, **model_kwargs[i], **kwargs))
+                out = outs[-1]
+                for i in range(len(guide_scale)):
+                    out += guide_scale[i] * (outs[-i - 2] - outs[-i - 1])
+
+            # rescale the output according to arXiv:2305.08891
+            if guide_rescale is not None and guide_rescale > 0.0:
+                assert guide_rescale >= 0 and guide_rescale <= 1
+                ratio = (
+                    y_out.flatten(1).std(dim=1) /
+                    (out.flatten(1).std(dim=1) + 1e-12)).view((-1, ) + (1, ) *
+                                                              (y_out.ndim - 1))
+                out *= guide_rescale * ratio + (1 - guide_rescale) * 1.0
         # compute x0
         if self.prediction_type == 'x0':
             x0 = out
         elif self.prediction_type == 'eps':
             x0 = (xt - sigmas * out) / alphas
         elif self.prediction_type == 'v':
             x0 = alphas * xt - sigmas * out
@@ -193,32 +380,37 @@
                refine_stage=False,
                refine_strength=0.0,
                model_kwargs={},
                condition_fn=None,
                guide_scale=None,
                guide_rescale=None,
                clamp=None,
+               sharpness=0.0,
                percentile=None,
                solver='euler_a',
                steps=20,
                t_max=None,
                t_min=None,
                discretization=None,
                discard_penultimate_step=None,
                return_intermediate=None,
                show_progress=False,
                seed=-1,
                intermediate_callback=None,
                cat_uc=False,
+               add_noise=False,
+               free_steps=None,
+               step_offset=None,
                **kwargs):
         # sanity check
         assert isinstance(steps, (int, torch.LongTensor))
         assert t_max is None or (t_max > 0 and t_max <= self.num_timesteps - 1)
         assert t_min is None or (t_min >= 0 and t_min < self.num_timesteps - 1)
-        assert discretization in (None, 'leading', 'linspace', 'trailing')
+        assert discretization in (None, 'leading', 'linspace', 'trailing',
+                                  'free')
         assert discard_penultimate_step in (None, True, False)
         assert return_intermediate in (None, 'x0', 'xt')
 
         # function of diffusion solver
         solver_fn = {
             'ddim': sample_ddim,
             'euler_ancestral': sample_euler_ancestral,
@@ -251,25 +443,58 @@
 
         # function for denoising xt to get x0
         intermediates = []
 
         def model_fn(xt, sigma):
             # denoising
             t = self._sigma_to_t(sigma).repeat(len(xt)).round().long()
-            x0 = self.denoise(xt,
-                              t,
-                              None,
-                              model,
-                              model_kwargs,
-                              guide_scale,
-                              guide_rescale,
-                              clamp,
-                              percentile,
-                              cat_uc=cat_uc,
-                              **kwargs)[-2]
+
+            if isinstance(
+                    model_kwargs[0]['cond'], dict) and \
+                    'tar_x0' in model_kwargs[0]['cond'] and \
+                    'tar_mask_latent' in model_kwargs[0]['cond']:
+                tar_x0 = model_kwargs[0]['cond']['tar_x0']
+                tar_mask = model_kwargs[0]['cond']['tar_mask_latent']
+
+                tar_xt = self.diffuse(x0=tar_x0, t=t)
+                xt = tar_xt * (1.0 - tar_mask) + xt * tar_mask
+
+            if isinstance(model_kwargs[0]['cond'],
+                          dict) and 'ref_x0' in model_kwargs[0]['cond']:
+                model_kwargs[0]['cond']['ref_xt'] = self.diffuse(
+                    x0=model_kwargs[0]['cond']['ref_x0'], t=t)
+                model_kwargs[1]['cond']['ref_xt'] = self.diffuse(
+                    x0=model_kwargs[1]['cond']['ref_x0'], t=t)
+
+            if solver in ('onestep', 'multistep', 'multistep2', 'multistep3'):
+                x0 = self.denoise(xt,
+                                  t,
+                                  None,
+                                  model,
+                                  model_kwargs,
+                                  guide_scale,
+                                  guide_rescale,
+                                  clamp,
+                                  sharpness,
+                                  percentile,
+                                  cat_uc=cat_uc,
+                                  **kwargs)[-3]
+            else:
+                x0 = self.denoise(xt,
+                                  t,
+                                  None,
+                                  model,
+                                  model_kwargs,
+                                  guide_scale,
+                                  guide_rescale,
+                                  clamp,
+                                  sharpness,
+                                  percentile,
+                                  cat_uc=cat_uc,
+                                  **kwargs)[-2]
 
             # collect intermediate outputs
             if return_intermediate == 'xt':
                 intermediates.append(xt)
             elif return_intermediate == 'x0':
                 intermediates.append(x0)
             if intermediate_callback is not None:
@@ -287,18 +512,22 @@
                 steps = torch.arange(t_min, t_max + 1,
                                      (t_max - t_min + 1) / steps).flip(0)
             elif discretization == 'linspace':
                 steps = torch.linspace(t_max, t_min, steps)
             elif discretization == 'trailing':
                 steps = torch.arange(t_max, t_min - 1,
                                      -((t_max - t_min + 1) / steps))
+            elif discretization == 'free':
+                steps = torch.tensor(free_steps)
             else:
                 raise NotImplementedError(
                     f'{discretization} discretization not implemented')
             steps = steps.clamp_(t_min, t_max)
+        elif isinstance(steps, list):
+            steps = torch.tensor(steps)
         steps = torch.as_tensor(steps,
                                 dtype=torch.float32,
                                 device=noise.device)
 
         # get sigmas
         sigmas = self._t_to_sigma(steps)
         sigmas = torch.cat([sigmas, sigmas.new_zeros([1])])
@@ -331,14 +560,31 @@
                     sigma_min=sigmas[sigmas > 0].min().item(),
                     sigma_max=sigmas.max().item(),
                     rho=7.).to(sigmas)
                 sigmas = torch.cat([sigmas, sigmas.new_zeros([1])])
         if discard_penultimate_step:
             sigmas = torch.cat([sigmas[:-2], sigmas[-1:]])
         kwargs['seed'] = seed
+        # add noise to x0
+        if add_noise:
+            if 'dm_steps' in kwargs:
+                if step_offset:
+                    add_noise_step = -kwargs['dm_steps'] + step_offset
+                    if add_noise_step < 0:
+                        noise = self.diffuse(
+                            noise,
+                            torch.full((noise.shape[0], 1),
+                                       steps[add_noise_step],
+                                       dtype=torch.int))
+                else:
+                    noise = self.diffuse(
+                        noise,
+                        torch.full((noise.shape[0], 1),
+                                   steps[-kwargs['dm_steps'] - 1],
+                                   dtype=torch.int))
         # sampling
         x0 = solver_fn(noise,
                        model_fn,
                        sigmas,
                        show_progress=show_progress,
                        **kwargs)
         return (x0, intermediates) if return_intermediate is not None else x0
@@ -369,176 +615,14 @@
         log_sigmas = torch.sqrt(self.sigmas**2 /
                                 (1 - self.sigmas**2)).log().to(t)
         log_sigma = (1 - w) * log_sigmas[low_idx] + w * log_sigmas[high_idx]
         log_sigma[torch.isnan(log_sigma)
                   | torch.isinf(log_sigma)] = float('inf')
         return log_sigma.exp()
 
-    @torch.no_grad()
-    def stochastic_encode(self, x0, t, steps):
-        # fast, but does not allow for exact reconstruction
-        # t serves as an index to gather the correct alphas
-
-        t_max = None
-        t_min = None
-
-        # discretization method
-        discretization = 'trailing' if self.prediction_type == 'v' else 'leading'
-
-        # timesteps
-        if isinstance(steps, int):
-            t_max = self.num_timesteps - 1 if t_max is None else t_max
-            t_min = 0 if t_min is None else t_min
-            steps = discretize_timesteps(t_max, t_min, steps, discretization)
-        steps = torch.as_tensor(steps).round().long().flip(0).to(x0.device)
-        # steps = torch.as_tensor(steps).round().long().to(x0.device)
-
-        # self.alphas_bar = torch.cumprod(1 - self.sigmas ** 2, dim=0)
-        # print('sigma: ', self.sigmas, len(self.sigmas))
-        # print('alpha_bar: ', self.alphas_bar, len(self.alphas_bar))
-        # print('steps: ', steps, len(steps))
-        # sqrt_alphas_cumprod = torch.sqrt(self.alphas_bar).to(x0.device)[steps]
-        # sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - self.alphas_bar).to(x0.device)[steps]
-
-        sqrt_alphas_cumprod = self.alphas.to(x0.device)[steps]
-        sqrt_one_minus_alphas_cumprod = self.sigmas.to(x0.device)[steps]
-        # print('sigma: ', self.sigmas, len(self.sigmas))
-        # print('alpha: ', self.alphas, len(self.alphas))
-        # print('steps: ', steps, len(steps))
-
-        noise = torch.randn_like(x0)
-        return (
-            extract_into_tensor(sqrt_alphas_cumprod, t, x0.shape) * x0 +
-            extract_into_tensor(sqrt_one_minus_alphas_cumprod, t, x0.shape) *
-            noise)
-
-    @torch.no_grad()
-    def sample_img2img(self,
-                       x,
-                       noise,
-                       model,
-                       denoising_strength=1,
-                       model_kwargs={},
-                       condition_fn=None,
-                       guide_scale=None,
-                       guide_rescale=None,
-                       clamp=None,
-                       percentile=None,
-                       solver='euler_a',
-                       steps=20,
-                       t_max=None,
-                       t_min=None,
-                       discretization=None,
-                       discard_penultimate_step=None,
-                       return_intermediate=None,
-                       show_progress=False,
-                       seed=-1,
-                       **kwargs):
-        # sanity check
-        assert isinstance(steps, (int, torch.LongTensor))
-        assert t_max is None or (t_max > 0 and t_max <= self.num_timesteps - 1)
-        assert t_min is None or (t_min >= 0 and t_min < self.num_timesteps - 1)
-        assert discretization in (None, 'leading', 'linspace', 'trailing')
-        assert discard_penultimate_step in (None, True, False)
-        assert return_intermediate in (None, 'x0', 'xt')
-        # function of diffusion solver
-        solver_fn = {
-            'euler_ancestral': sample_img2img_euler_ancestral,
-            'euler': sample_img2img_euler,
-        }[solver]
-        # options
-        schedule = 'karras' if 'karras' in solver else None
-        discretization = discretization or 'linspace'
-        seed = seed if seed >= 0 else random.randint(0, 2**31)
-        if isinstance(steps, torch.LongTensor):
-            discard_penultimate_step = False
-        if discard_penultimate_step is None:
-            discard_penultimate_step = True if solver in (
-                'dpm2', 'dpm2_ancestral', 'dpmpp_2m_sde', 'dpm2_karras',
-                'dpm2_ancestral_karras', 'dpmpp_2m_sde_karras') else False
-
-        # function for denoising xt to get x0
-        intermediates = []
-
-        def get_scalings(sigma):
-            c_out = -sigma
-            c_in = 1 / (sigma**2 + 1.**2)**0.5
-            return c_out, c_in
-
-        def model_fn(xt, sigma):
-            # denoising
-            c_out, c_in = get_scalings(sigma)
-            t = self._sigma_to_t(sigma).repeat(len(xt)).round().long()
-
-            x0 = self.denoise(xt * c_in, t, None, model, model_kwargs,
-                              guide_scale, guide_rescale, clamp, percentile,
-                              **kwargs)[-2]
-            # collect intermediate outputs
-            if return_intermediate == 'xt':
-                intermediates.append(xt)
-            elif return_intermediate == 'x0':
-                intermediates.append(x0)
-            return xt + x0 * c_out
-
-        # get timesteps
-        if isinstance(steps, int):
-            steps += 1 if discard_penultimate_step else 0
-            t_max = self.num_timesteps - 1 if t_max is None else t_max
-            t_min = 0 if t_min is None else t_min
-            # discretize timesteps
-            if discretization == 'leading':
-                steps = torch.arange(t_min, t_max + 1,
-                                     (t_max - t_min + 1) / steps).flip(0)
-            elif discretization == 'linspace':
-                steps = torch.linspace(t_max, t_min, steps)
-            elif discretization == 'trailing':
-                steps = torch.arange(t_max, t_min - 1,
-                                     -((t_max - t_min + 1) / steps))
-            else:
-                raise NotImplementedError(
-                    f'{discretization} discretization not implemented')
-            steps = steps.clamp_(t_min, t_max)
-        steps = torch.as_tensor(steps, dtype=torch.float32, device=x.device)
-        # get sigmas
-        sigmas = self._t_to_sigma(steps)
-        sigmas = torch.cat([sigmas, sigmas.new_zeros([1])])
-        t_enc = int(min(denoising_strength, 0.999) * len(steps))
-        sigmas = sigmas[len(steps) - t_enc - 1:]
-        noise = x + noise * sigmas[0]
-
-        if schedule == 'karras':
-            if sigmas[0] == float('inf'):
-                sigmas = karras_schedule(
-                    n=len(steps) - 1,
-                    sigma_min=sigmas[sigmas > 0].min().item(),
-                    sigma_max=sigmas[sigmas < float('inf')].max().item(),
-                    rho=7.).to(sigmas)
-                sigmas = torch.cat([
-                    sigmas.new_tensor([float('inf')]), sigmas,
-                    sigmas.new_zeros([1])
-                ])
-            else:
-                sigmas = karras_schedule(
-                    n=len(steps),
-                    sigma_min=sigmas[sigmas > 0].min().item(),
-                    sigma_max=sigmas.max().item(),
-                    rho=7.).to(sigmas)
-                sigmas = torch.cat([sigmas, sigmas.new_zeros([1])])
-        if discard_penultimate_step:
-            sigmas = torch.cat([sigmas[:-2], sigmas[-1:]])
-
-        # sampling
-        x0 = solver_fn(noise,
-                       model_fn,
-                       sigmas,
-                       seed=seed,
-                       show_progress=show_progress,
-                       **kwargs)
-        return (x0, intermediates) if return_intermediate is not None else x0
-
 
 def extract_into_tensor(a, t, x_shape):
     b, *_ = t.shape
     out = a.gather(-1, t)
     return out.reshape(b, *((1, ) * (len(x_shape) - 1)))
```

## scepter/modules/opt/optimizers/__init__.py

```diff
@@ -1,7 +1,10 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
-from scepter.modules.opt.optimizers.official_optimizers import (
-    ASGD, LBFGS, SGD, Adadelta, Adagrad, Adam, Adamax, AdamW, RMSprop, Rprop,
-    SparseAdam)
+from scepter.modules.opt.optimizers.official_optimizers import (ASGD, LBFGS,
+                                                                SGD, Adadelta,
+                                                                Adagrad, Adam,
+                                                                Adamax, AdamW,
+                                                                RMSprop, Rprop,
+                                                                SparseAdam)
 from scepter.modules.opt.optimizers.registry import OPTIMIZERS
```

## scepter/modules/solver/diffusion_solver.py

```diff
@@ -329,20 +329,20 @@
 
     def run_train(self):
         self.train_mode()
         self.before_all_iter(self.hooks_dict[self._mode])
         data_iter = iter(self.datas[self._mode].dataloader)
         self.print_memory_status()
         for step in range(self.max_steps):
-            if 'eval' in self._mode_set and (step % self.eval_interval == 0
-                                             or step == self.max_steps - 1):
+            if 'eval' in self._mode_set and (self.eval_interval > 0 and
+                                             step % self.eval_interval == 0):
                 self.run_eval()
                 self.train_mode()
-            self.before_iter(self.hooks_dict[self._mode])
             batch_data = next(data_iter)
+            self.before_iter(self.hooks_dict[self._mode])
             if self.sample_args:
                 batch_data.update(self.sample_args.get_lowercase_dict())
             if 'meta' in batch_data:
                 self.register_probe({
                     'data_key':
                     ProbeData(batch_data['meta'].get('data_key', []),
                               view_distribute=True)
@@ -360,14 +360,17 @@
                     step,
                     step=self.total_iter,
                     rank=we.rank)
                 self._iter_outputs[self._mode] = self._reduce_scalar(results)
             self.after_iter(self.hooks_dict[self._mode])
             if we.debug:
                 self.print_trainable_params_status(prefix='model.')
+            if 'eval' in self._mode_set and (self.eval_interval > 0
+                                             and step == self.max_steps - 1):
+                self.run_eval()
         self.after_all_iter(self.hooks_dict[self._mode])
 
     @torch.no_grad()
     def run_eval(self):
         self.eval_mode()
         self.before_all_iter(self.hooks_dict[self._mode])
         all_results = []
```

## scepter/modules/solver/hooks/__init__.py

```diff
@@ -1,13 +1,14 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 from scepter.modules.solver.hooks.backward import BackwardHook
 from scepter.modules.solver.hooks.checkpoint import CheckpointHook
 from scepter.modules.solver.hooks.data_probe import ProbeDataHook
+from scepter.modules.solver.hooks.ema import ModelEmaHook
 from scepter.modules.solver.hooks.hook import Hook
 from scepter.modules.solver.hooks.log import LogHook, TensorboardLogHook
 from scepter.modules.solver.hooks.lr import LrHook
 from scepter.modules.solver.hooks.registry import HOOKS
 from scepter.modules.solver.hooks.safetensors import SafetensorsHook
 from scepter.modules.solver.hooks.sampler import DistSamplerHook
 """
@@ -43,9 +44,10 @@
 
 after solve:
     TensorboardLogHook: close file handler
 """
 
 __all__ = [
     'HOOKS', 'BackwardHook', 'CheckpointHook', 'Hook', 'LrHook', 'LogHook',
-    'TensorboardLogHook', 'DistSamplerHook', 'ProbeDataHook', 'SafetensorsHook'
+    'TensorboardLogHook', 'DistSamplerHook', 'ProbeDataHook',
+    'SafetensorsHook', 'ModelEmaHook'
 ]
```

## scepter/modules/solver/hooks/checkpoint.py

```diff
@@ -143,15 +143,17 @@
                                     local_folder,
                                     'configuration.json')) as local_path:
                             json.dump(cfg, open(local_path, 'w'))
                         FS.put_dir_from_local_dir(local_folder, save_path)
 
                 if self.save_last and solver.total_iter == solver.max_steps - 1:
                     with FS.get_fs_client(save_path) as client:
-                        last_path = osp.join(solver.work_dir, 'checkpoint.pth')
+                        last_path = osp.join(
+                            solver.work_dir,
+                            f'checkpoints/{self.save_name_prefix}-last')
                         client.make_link(last_path, save_path)
                 self.last_ckpt = save_path
 
             torch.cuda.synchronize()
             if we.is_distributed:
                 torch.distributed.barrier()
```

## scepter/modules/solver/hooks/data_probe.py

```diff
@@ -26,36 +26,43 @@
             'description': 'the interval for log print!'
         }
     }]
 
     def __init__(self, cfg, logger=None):
         super(ProbeDataHook, self).__init__(cfg, logger=logger)
         self.priority = cfg.get('PRIORITY', _DEFAULT_PROBE_PRIORITY)
-        self.log_interval = cfg.get('PROB_INTERVAL', 1000)
+        self.prob_interval = cfg.get('PROB_INTERVAL', 1000)
+        self.save_name_prefix = cfg.get('SAVE_NAME_PREFIX', 'step')
+        self.save_probe_prefix = cfg.get('SAVE_PROBE_PREFIX', None)
+        self.save_last = cfg.get('SAVE_LAST', False)
 
     def before_all_iter(self, solver):
         pass
 
     def before_iter(self, solver):
         pass
 
     def after_iter(self, solver):
-        if solver.mode == 'train' and solver.total_iter % self.log_interval == 0:
+        if solver.mode == 'train' and solver.total_iter % self.prob_interval == 0:
             probe_dict = solver.probe_data
             if we.rank == 0:
                 save_folder = os.path.join(
                     solver.work_dir,
-                    f'{solver.mode}_probe/step_{solver.total_iter}')
+                    f'{solver.mode}_probe/{self.save_name_prefix}-{solver.total_iter}'
+                )
                 ret_data = {}
                 for k, v in probe_dict.items():
-                    ret_one = v.to_log(
-                        os.path.join(
+                    if self.save_probe_prefix is not None:
+                        ret_prefix = os.path.join(save_folder,
+                                                  self.save_probe_prefix)
+                    else:
+                        ret_prefix = os.path.join(
                             save_folder,
-                            k.replace('/', '_') +
-                            f'_step_{solver.total_iter}'))
+                            k.replace('/', '_') + f'_step_{solver.total_iter}')
+                    ret_one = v.to_log(ret_prefix)
                     if (isinstance(ret_one, list)
                             or isinstance(ret_one, dict)) and len(ret_one) < 1:
                         continue
                     ret_data[k] = ret_one
                 with FS.put_to(os.path.join(save_folder,
                                             'meta.json')) as local_path:
                     json.dump(ret_data,
@@ -67,30 +74,46 @@
 
     def after_all_iter(self, solver):
         if not solver.mode == 'train':
             probe_dict = solver.probe_data
             if we.rank == 0:
                 step = solver._total_iter[
                     'train'] if 'train' in solver._total_iter else 0
-                save_folder = os.path.join(solver.work_dir,
-                                           f'{solver.mode}_probe/step_{step}')
+                save_folder = os.path.join(
+                    solver.work_dir,
+                    f'{solver.mode}_probe/{self.save_name_prefix}-{step}')
                 ret_data = {}
                 for k, v in probe_dict.items():
-                    ret_one = v.to_log(
-                        os.path.join(save_folder,
-                                     k.replace('/', '_') + f'_step_{step}'))
+                    if self.save_probe_prefix is not None:
+                        ret_prefix = os.path.join(save_folder,
+                                                  self.save_probe_prefix)
+                    else:
+                        ret_prefix = os.path.join(
+                            save_folder,
+                            k.replace('/', '_') + f'_step_{step}')
+                    ret_one = v.to_log(ret_prefix)
                     if (isinstance(ret_one, list)
                             or isinstance(ret_one, dict)) and len(ret_one) < 1:
                         continue
                     ret_data[k] = ret_one
                 with FS.put_to(os.path.join(save_folder,
                                             'meta.json')) as local_path:
                     json.dump(ret_data,
                               open(local_path, 'w'),
                               ensure_ascii=False)
+
+                if self.save_last and step == solver.max_steps:
+                    with FS.get_fs_client(save_folder) as client:
+                        last_save_folder = os.path.join(
+                            solver.work_dir,
+                            f'{solver.mode}_probe/{self.save_name_prefix}-last'
+                        )
+                        print(last_save_folder, save_folder)
+                        client.make_link(last_save_folder, save_folder)
+
             solver.clear_probe()
             torch.cuda.synchronize()
             barrier()
 
     @staticmethod
     def get_config_template():
         return dict_to_yaml('HOOK',
```

## scepter/modules/transform/image.py

```diff
@@ -6,19 +6,24 @@
 import opencv_transforms.functional as cv2_TF
 import opencv_transforms.transforms as cv2_transforms
 import torch
 import torchvision.transforms as transforms
 import torchvision.transforms.functional as TF
 
 from scepter.modules.transform.registry import TRANSFORMS
-from scepter.modules.transform.utils import (
-    BACKEND_CV2, BACKEND_PILLOW, BACKEND_TORCHVISION, INPUT_CV2_TYPE_WARNING,
-    INPUT_PIL_TYPE_WARNING, INPUT_TENSOR_TYPE_WARNING, INTERPOLATION_STYLE,
-    INTERPOLATION_STYLE_CV2, TORCHVISION_CAPABILITY, is_cv2_image,
-    is_pil_image, is_tensor)
+from scepter.modules.transform.utils import (BACKEND_CV2, BACKEND_PILLOW,
+                                             BACKEND_TORCHVISION,
+                                             INPUT_CV2_TYPE_WARNING,
+                                             INPUT_PIL_TYPE_WARNING,
+                                             INPUT_TENSOR_TYPE_WARNING,
+                                             INTERPOLATION_STYLE,
+                                             INTERPOLATION_STYLE_CV2,
+                                             TORCHVISION_CAPABILITY,
+                                             is_cv2_image, is_pil_image,
+                                             is_tensor)
 from scepter.modules.utils.config import dict_to_yaml
 
 if TORCHVISION_CAPABILITY:
     BACKENDS = (BACKEND_PILLOW, BACKEND_CV2, BACKEND_TORCHVISION)
 else:
     BACKENDS = (BACKEND_PILLOW, BACKEND_CV2)
```

## scepter/modules/transform/io.py

```diff
@@ -2,21 +2,23 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import io
 import os
 
 import cv2
 import numpy as np
 import torch
-from PIL import Image
+from PIL import Image, ImageFile
 
 from scepter.modules.transform.registry import TRANSFORMS
 from scepter.modules.utils.config import dict_to_yaml
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.file_system import DATA_FS as FS
 
+ImageFile.LOAD_TRUNCATED_IMAGES = True
+
 
 def pillow_convert(image, rgb_order):
     if image.mode != rgb_order:
         if image.mode == 'P':
             image = image.convert(f'{rgb_order}A')
         if image.mode == f'{rgb_order}A':
             bg = Image.new(rgb_order,
```

## scepter/modules/utils/config.py

```diff
@@ -599,7 +599,10 @@
                 if isinstance(val, (Config, dict, list)):
                     cfg_new.append(Config.get_plain_cfg(val))
                 elif isinstance(val, (str, numbers.Number)):
                     cfg_new.append(val)
             return cfg_new
         else:
             return cfg
+
+    def pop(self, name):
+        self.cfg_dict.pop(name)
```

## scepter/modules/utils/export_model.py

 * *Ordering differences only*

```diff
@@ -1,17 +1,17 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import io
 from io import BytesIO
 
 import onnx
+import onnxruntime
 import torch
 from torch.onnx import OperatorExportTypes
 
-import onnxruntime
 from scepter.modules.utils.distribute import we
 
 type_map = {
     'float32': torch.float32,
     'float16': torch.float16,
     'int64': torch.int64,
     'int32': torch.int32,
```

## scepter/modules/utils/file_system.py

```diff
@@ -1,9 +1,10 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
+import io
 import os
 import threading
 import time
 import warnings
 from contextlib import contextmanager
 from queue import Queue
 
@@ -323,14 +324,18 @@
         R = threading.Lock()
 
         def put_one_object(local_path_list, target_path_list):
             for local_path, target_path in zip(local_path_list,
                                                target_path_list):
                 if local_path is None or target_path is None:
                     flg = False
+                elif isinstance(local_path, io.BytesIO):
+                    flg = FS.put_object(local_path.getvalue(), target_path)
+                elif isinstance(local_path, bytes):
+                    flg = FS.put_object(local_path, target_path)
                 elif self.exists(local_path):
                     local_cache = self.get_from(local_path,
                                                 local_path + f'{time.time()}',
                                                 wait_finish=wait_finish)
                     flg = self.put_object_from_local_file(
                         local_cache, target_path)
                     try:
```

## scepter/modules/utils/probe.py

```diff
@@ -259,15 +259,16 @@
                     for idx, one_data in enumerate(zip(save_path, save_label)):
                         one_path, one_label = one_data
                         one_label = one_label.replace('<', '&lt;').replace(
                             '>', '&gt;')
                         url = FS.get_url(one_path,
                                          lifecycle=3600 * 365 * 24).replace(
                                              '.oss-internal.aliyun-inc.',
-                                             '.oss.aliyuncs.')
+                                             '.oss.aliyuncs.').replace(
+                                                 '-internal', '')
                         one_rank += (
                             f'<td align="center"><input type="image" src="{url}" >'
                             f'<br><font size="4"><strong>{save_id}-{idx}|{one_label}<strong></font><br/></td>'
                         )
                     one_rank += '</tr></table><hr/>'
                     all_ranks.append(one_rank)
                 f.writelines('\n'.join(all_ranks))
```

## scepter/modules/utils/file_clients/local_fs.py

```diff
@@ -289,29 +289,32 @@
             try:
                 os.remove(target_path)
             except Exception:
                 return False
         return True
 
     def get_logging_handler(self, target_logging_path):
+        dirname = os.path.dirname(target_logging_path)
+        if not os.path.exists(dirname):
+            os.makedirs(dirname, exist_ok=True)
         return logging.FileHandler(target_logging_path)
 
     def put_dir_from_local_dir(self,
                                local_dir,
                                target_dir,
                                multi_thread=False) -> bool:
         local_dir = self.reconstruct_path(local_dir)
         target_dir = self.reconstruct_path(target_dir)
         if local_dir == target_dir:
             return True
-        # cp -f local_dir/* target_dir/*
-        if not osp.exists(target_dir):
-            status = os.system(f'mkdir -p {target_dir}')
-            if status != 0:
-                return False
+        # # cp -f local_dir/* target_dir/*
+        # if not osp.exists(target_dir):
+        #     status = os.system(f'mkdir -p {target_dir}')
+        #     if status != 0:
+        #         return False
         try:
             shutil.copytree(local_dir, target_dir, symlinks=True)
         except Exception:
             return False
         return True
 
     def size(self, target_path) -> Optional[int]:
```

## scepter/studio/inference/inference.py

```diff
@@ -12,22 +12,23 @@
 from scepter.studio.inference.inference_manager.infer_runer import \
     PipelineManager
 from scepter.studio.inference.inference_ui.component_names import \
     InferenceUIName
 from scepter.studio.inference.inference_ui.control_ui import ControlUI
 from scepter.studio.inference.inference_ui.diffusion_ui import DiffusionUI
 from scepter.studio.inference.inference_ui.gallery_ui import GalleryUI
+from scepter.studio.inference.inference_ui.largen_ui import LargenUI
 from scepter.studio.inference.inference_ui.mantra_ui import MantraUI
 from scepter.studio.inference.inference_ui.model_manage_ui import ModelManageUI
 from scepter.studio.inference.inference_ui.refiner_ui import RefinerUI
 from scepter.studio.inference.inference_ui.tuner_ui import TunerUI
 from scepter.studio.utils.env import init_env
 
 UI_MAP = [('diffusion', DiffusionUI), ('mantra', MantraUI), ('tuner', TunerUI),
-          ('control', ControlUI), ('refiner', RefinerUI)]
+          ('control', ControlUI), ('refiner', RefinerUI), ('largen', LargenUI)]
 
 
 class InferenceUI():
     def __init__(self,
                  cfg_general_file,
                  is_debug=False,
                  language='en',
@@ -50,28 +51,47 @@
                                   cfg_general.EXTENSION_PARAS.OFFICIAL_TUNERS))
         cfg_general.TUNERS = official_tuners.TUNERS
         official_controllers = Config(cfg_file=os.path.join(
             os.path.dirname(scepter.dirname),
             cfg_general.EXTENSION_PARAS.OFFICIAL_CONTROLLERS))
         cfg_general.CONTROLLERS = official_controllers.CONTROLLERS
 
+        # customized tuners
+        tuner_manager = Config(
+            cfg_file=os.path.join(os.path.dirname(scepter.dirname),
+                                  cfg_general.EXTENSION_PARAS.TUNER_MANAGER))
+        tuner_manager = os.path.join(root_work_dir, tuner_manager.WORK_DIR,
+                                     tuner_manager.TUNER_LIST_YAML)
+        if FS.exists(tuner_manager):
+            with FS.get_from(tuner_manager) as local_path:
+                custom_tuners = Config(cfg_file=local_path)
+            cfg_general.CUSTOM_TUNERS = custom_tuners.get('TUNERS', [])
+        else:
+            cfg_general.CUSTOM_TUNERS = []
+
         pipe_manager = PipelineManager()
         config_list = glob(os.path.join(config_dir, '*/*_pro.yaml'),
                            recursive=True)
         for config_file in config_list:
             pipe_manager.register_pipeline(Config(cfg_file=config_file))
 
         for one_tuner in cfg_general.TUNERS:
             pipe_manager.register_tuner(
                 one_tuner,
                 name=one_tuner.NAME_ZH if language == 'zh' else one_tuner.NAME)
 
         for one_controller in cfg_general.CONTROLLERS:
             pipe_manager.register_controllers(one_controller)
 
+        for one_tuner in cfg_general.CUSTOM_TUNERS:
+            pipe_manager.register_tuner(
+                one_tuner,
+                name=one_tuner.NAME_ZH if language == 'zh' else one_tuner.NAME,
+                is_customized=True)
+
         self.model_manage_ui = ModelManageUI(cfg_general,
                                              pipe_manager,
                                              is_debug=is_debug,
                                              language=language)
         self.gallery_ui = GalleryUI(cfg_general,
                                     pipe_manager,
                                     is_debug=is_debug,
@@ -84,22 +104,24 @@
                     pipe_manager,
                     is_debug=is_debug,
                     language=language)
             self.tab_ui[name] = ui
             self.tab_ui_kwargs[f'{name}_ui'] = ui
             self.__setattr__(f'{name}_ui', ui)
 
-        self.check_box_controlled_tabs = ['mantra', 'tuner', 'control']
+        self.check_box_controlled_tabs = ['mantra', 'tuner', 'control', 'largen']
+        self.pipe_manager = pipe_manager
         assert len(self.component_names.check_box_for_setting) == len(
             self.check_box_controlled_tabs)
 
     def create_ui(self):
         # create model
         self.model_manage_ui.create_ui()
         self.gallery_ui.create_ui()
+        self.infer_info = gr.State(value=None)
 
         # create tabs
         def create_tab(name, ui):
             label = getattr(self.component_names, f'{name}_paras')
             if name in ['refiner']:
                 ui.create_ui()
             else:
@@ -119,39 +141,85 @@
     def set_callbacks(self, manager):
         self.model_manage_ui.set_callbacks(**self.tab_ui_kwargs)
         self.gallery_ui.set_callbacks(self, self.model_manage_ui,
                                       **self.tab_ui_kwargs)
         for name, ui in self.tab_ui_kwargs.items():
             ui.set_callbacks(self.model_manage_ui,
                              **self.tab_ui_kwargs,
-                             gallery_ui=self.gallery_ui)
+                             gallery_ui=self.gallery_ui,
+                             manager=manager)
 
-        def change_setting_tab(check_box, *args):
+        def change_setting_tab(check_box, default_diffusion_model, *args):
             selected_tab = 'diffusion_ui'
             ui_tabs_state = [False] * len(args)
+            largen_index = self.check_box_controlled_tabs.index('largen')
+            largen_key = self.component_names.check_box_for_setting[largen_index]
+            largen_status = args[largen_index]
             for key in check_box:
                 i = self.component_names.check_box_for_setting.index(key)
                 ui_tabs_state[i] = True
                 if ui_tabs_state[i] != args[i]:
                     selected_tab = self.check_box_controlled_tabs[i] + '_ui'
+            new_check_box_value = check_box
+            for key in check_box:
+                i = self.component_names.check_box_for_setting.index(key)
+                if ui_tabs_state[i] != args[i]:
+                    if i in [largen_index]:
+                        new_check_box_value = [key]
+                        for j in range(len(ui_tabs_state)):
+                            ui_tabs_state[j] = j == i
+                    else:
+                        new_check_box_value = [
+                            k for k in check_box
+                            if k not in [largen_key]
+                        ]
+                        ui_tabs_state[largen_index] = False
+
             ui_tabs_updates = [gr.update(visible=v) for v in ui_tabs_state]
 
-            return gr.update(
-                selected=selected_tab), *ui_tabs_state, *ui_tabs_updates
+            if ui_tabs_state[largen_index]:
+                diffusion_model = gr.Dropdown(
+                    label=self.model_manage_ui.component_names.diffusion_model,
+                    choices=self.model_manage_ui.
+                    default_choices['diffusion_model']['choices'],
+                    value='LARGEN_LargenUNetXL',
+                    interactive=False)
+            elif largen_status:
+                diffusion_model = gr.Dropdown(
+                    label=self.model_manage_ui.component_names.diffusion_model,
+                    choices=self.model_manage_ui.
+                    default_choices['diffusion_model']['choices'],
+                    value=self.model_manage_ui.
+                    default_choices['diffusion_model']['default'],
+                    interactive=True)
+            else:
+                diffusion_model = gr.Dropdown(
+                    label=self.model_manage_ui.component_names.diffusion_model,
+                    choices=self.model_manage_ui.
+                    default_choices['diffusion_model']['choices'],
+                    value=default_diffusion_model,
+                    interactive=True)
+
+            return gr.CheckboxGroup(
+                choices=self.component_names.check_box_for_setting,
+                value=new_check_box_value,
+                show_label=False), gr.update(
+                selected=selected_tab), *ui_tabs_state, *ui_tabs_updates, diffusion_model
 
         gr_states = [
             self.tab_ui[name].state for name in self.check_box_controlled_tabs
         ]
         gr_tabs = [
             self.tab_ui[name].tab for name in self.check_box_controlled_tabs
         ]
         self.check_box_for_setting.change(
             change_setting_tab,
-            inputs=[self.check_box_for_setting, *gr_states],
-            outputs=[self.setting_tab, *gr_states, *gr_tabs],
+            inputs=[self.check_box_for_setting, self.model_manage_ui.diffusion_model, *gr_states],
+            outputs=[self.check_box_for_setting, self.setting_tab, *gr_states,
+                     *gr_tabs, self.model_manage_ui.diffusion_model],
             queue=False)
 
 
 if __name__ == '__main__':
     infer_ins = InferenceUI('scepter/methods/studio/inference/inference.yaml',
                             is_debug=True,
                             language='en',
```

## scepter/studio/inference/inference_manager/infer_runer.py

```diff
@@ -1,10 +1,11 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from scepter.modules.inference.diffusion_inference import DiffusionInference
+from scepter.modules.inference.largen_inference import LargenInference
 from scepter.modules.utils.logger import get_logger
 
 
 class PipelineManager():
     def __init__(self, logger=None):
         '''
         Args:
@@ -91,15 +92,20 @@
             self.model_level_info[model_name]['pipeline'].append(pipeline_name)
             self.model_level_info[model_name]['model_info'] = module
 
     def construct_new_pipeline(self):
         pass
 
     def register_pipeline(self, cfg):
-        new_inference = DiffusionInference(logger=self.logger)
+        pipeline_name = cfg.NAME
+        if 'LARGEN' in pipeline_name:
+            PipelineBuilder = LargenInference
+        else:
+            PipelineBuilder = DiffusionInference
+        new_inference = PipelineBuilder(logger=self.logger)
         new_inference.init_from_cfg(cfg)
         self.contruct_models_index(cfg.NAME, new_inference)
 
     def register_tuner(self, cfg, name=None, is_customized=False):
         '''
         Args:
             cfg: {
```

## scepter/studio/inference/inference_ui/component_names.py

```diff
@@ -1,47 +1,50 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 # For dataset manager
-from scepter.modules.utils.file_system import FS
 from scepter.modules.utils.directory import get_md5
+from scepter.modules.utils.file_system import FS
 
 
 def download_image(image):
     if image is not None:
         client = FS.get_fs_client(image)
-        if client.tmp_dir.startswith("/home"):
+        if client.tmp_dir.startswith('/home'):
             name = get_md5(image)
-            local_path = FS.get_from(image, f"/tmp/gradio/scepter_examples/{name}")
+            local_path = FS.get_from(image,
+                                     f'/tmp/gradio/scepter_examples/{name}')
         else:
             local_path = FS.get_from(image)
         return local_path
     else:
         return image
 
 
 class InferenceUIName():
     def __init__(self, language='en'):
         if language == 'en':
             self.advance_block_name = 'Advance Setting'
             self.check_box_for_setting = [
-                'Use Mantra', 'Use Tuners', 'Use Controller'
+                'Use Mantra', 'Use Tuners', 'Use Controller', 'LAR-Gen'
             ]
             self.diffusion_paras = 'Generation Setting'
             self.mantra_paras = 'Mantra Book'
             self.tuner_paras = 'Tuners'
             self.control_paras = 'Controlable Generation'
             self.refiner_paras = 'Refiner Setting'
+            self.largen_paras = 'LAR-Gen'
         elif language == 'zh':
             self.advance_block_name = ''
-            self.check_box_for_setting = ['', '', '']
+            self.check_box_for_setting = ['', '', '', 'LAR-Gen']
             self.diffusion_paras = ''
             self.mantra_paras = ''
             self.tuner_paras = ''
             self.control_paras = ''
             self.refiner_paras = 'Refine'
+            self.largen_paras = 'LAR-Gen'
 
 
 class ModelManageUIName():
     def __init__(self, language='en'):
         if language == 'en':
             self.model_block_name = 'Model Management'
             self.postprocess_model_name = 'Refiners and Tuners'
@@ -212,14 +215,15 @@
             self.base_model = 'Base Model Name'
             self.custom_tuner_model = 'Customized Model'
             self.advance_block_name = 'Advance Setting'
             self.tuner_scale = 'Tuner Scale'
             self.example_block_name = 'Examples'
             self.examples = [[['Pencil Sketch Drawing'], 'a girl in a jacket'],
                              [['Flat 2D Art'], 'a cat']]
+            self.save_button = 'Save'
 
         elif language == 'zh':
             self.tuner_model = ''
             self.tuner_name = ''
             self.tuner_type = ''
             self.tuner_desc = ''
             self.tuner_example = ''
@@ -227,14 +231,15 @@
             self.base_model = ''
             self.custom_tuner_model = ''
             self.advance_block_name = ''
             self.tuner_scale = ''
             self.example_block_name = ''
             self.examples = [[[''], 'a girl in a jacket'],
                              [['2D'], 'a cat']]
+            self.save_button = ''
 
 
 class ControlUIName():
     def __init__(self, language='en'):
         self.examples = [
             [
                 'Canny',
@@ -338,7 +343,159 @@
                               '2\n')
             self.control_err1 = ''
             self.control_err2 = ''
             self.control_model = ''
             self.advance_block_name = ''
             self.control_scale = ''
             self.example_block_name = ''
+
+
+class LargenUIName():
+    def __init__(self, language='en'):
+
+        self.tasks = [
+            'Text_Guided_Outpainting', 'Subject_Guided_Inpainting',
+            'Text_Guided_Inpainting', 'Text_Subject_Guided_Inpainting'
+        ]
+
+        if language == 'en':
+            self.apps = [
+                'Zoom Out', 'Virtual Try On', 'Inpainting (text guided)',
+                'Inpainting (text + reference image guided)'
+            ]
+            self.dropdown_name = 'Application'
+            self.subject_image = 'Reference Image'
+            self.subject_mask = 'Reference Mask'
+            self.scene_image = 'Scene Image'
+            self.scene_mask = 'Scene Mask'
+            self.prompt = 'Prompt'
+            self.masked_image = 'Masked Image'
+            self.preprocess = 'Input Preprocess'
+            self.button_name = 'Data Preprocess'
+            self.direction = (
+                'Instruction: \n\n'
+                'For customized data: \n\n'
+                'a.1) Select the task; \n\n'
+                'a.1) Upload scene image; \n\n'
+                'a.2) Upload reference image (if needed); \n\n'
+                'a.3) Use the brush tool to cover the areas on the scene'
+                'image and reference image (to generate corresponding scene'
+                'mask and reference mask); \n\n'
+                'a.4) Click Data Preprocess button; \n\n'
+                'a.5) Input text prompt; \n\n'
+                'For example data: \n\n'
+                'b.1) click example row \n\n'
+                'Finally, click Generate button and get the output image!')
+            self.out_direction_label = 'Out Direction'
+            self.out_directions = [
+                'CenterAround',
+                'RightDown',
+                'LeftDown',
+                'RightUp',
+                'LeftUp',
+            ]
+        elif language == 'zh':
+            self.apps = ['', '', '', '+']
+            self.dropdown_name = ''
+            self.subject_image = ''
+            self.subject_mask = ''
+            self.scene_image = ''
+            self.scene_mask = ''
+            self.prompt = ''
+            self.masked_image = ''
+            self.preprocess = ''
+            self.button_name = ''
+            self.direction = ('\n'
+                              '\n'
+                              'a.1\n'
+                              'a.2\n'
+                              'a.3\n'
+                              'a.4\n'
+                              'a.5\n'
+                              ': \n'
+                              'b.1 \n'
+                              '')
+            self.out_direction_label = ''
+            self.out_directions = [
+                '',
+                '',
+                '',
+                '',
+                '',
+            ]
+
+        self.examples = [
+            [
+                self.apps[0],
+                'a temple on fire',
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex1_scene_im.png'  # noqa
+                ),
+                None,
+                None,
+                None,
+                1.0,
+                0.75,
+                'CenterAround',
+                1024,
+                1024
+            ],
+            [
+                self.apps[1],
+                '',
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex2_scene_im.jpg'  # noqa
+                ),
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex2_scene_mask.png'  # noqa
+                ),
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex2_subject_im.jpg'  # noqa
+                ),
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex2_subject_mask.jpg'  # noqa
+                ),
+                1.0,
+                0.0,
+                '',
+                1024,
+                1024
+            ],
+            [
+                self.apps[2],
+                'a blue and white porcelain',
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex3_scene_im.png'  # noqa
+                ),
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex3_scene_mask.png'  # noqa
+                ),
+                None,
+                None,
+                1.0,
+                0.0,
+                '',
+                1024,
+                1024
+            ],
+            [
+                self.apps[3],
+                'a dog wearing sunglasses',
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex4_scene_im.png'  # noqa
+                ),
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex4_scene_mask.png'  # noqa
+                ),
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex4_subject_im.png'  # noqa
+                ),
+                download_image(
+                    'https://modelscope.cn/api/v1/models/iic/LARGEN/repo?Revision=master&FilePath=examples/ex4_subject_mask.png'  # noqa
+                ),
+                0.45,
+                0.0,
+                '',
+                1024,
+                1024
+            ],
+        ]
```

## scepter/studio/inference/inference_ui/gallery_ui.py

```diff
@@ -2,23 +2,27 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
 
 import gradio as gr
 import numpy as np
 from PIL import Image
 
+from scepter.modules.utils.file_system import FS
 from scepter.studio.inference.inference_ui.component_names import GalleryUIName
 from scepter.studio.utils.uibase import UIBase
 
 
 class GalleryUI(UIBase):
     def __init__(self, cfg, pipe_manager, is_debug=False, language='en'):
         self.pipe_manager = pipe_manager
         self.component_names = GalleryUIName(language)
         self.cfg = cfg
+        self.work_dir = cfg.WORK_DIR
+        self.local_work_dir, _ = FS.map_to_local(self.work_dir)
+        os.makedirs(self.local_work_dir, exist_ok=True)
 
     def create_ui(self, *args, **kwargs):
         with gr.Group():
             gr.Markdown(value=self.component_names.gallery_block_name)
             with gr.Row(variant='panel', equal_height=True):
                 with gr.Column(scale=2, min_width=0,
                                visible=False) as self.before_refine_panel:
@@ -37,14 +41,15 @@
                     self.prompt = gr.Textbox(
                         show_label=False,
                         placeholder=self.component_names.prompt_input,
                         elem_id='positive_prompt',
                         container=False,
                         autofocus=True,
                         elem_classes='type_row',
+                        submit_on_enter=True,
                         lines=1)
 
                 with gr.Column(scale=3, min_width=0):
                     self.generate_button = gr.Button(
                         label='Generate',
                         value=self.component_names.generate,
                         elem_classes='type_row',
@@ -83,14 +88,27 @@
                          refine_sampler,
                          refine_discretization,
                          refine_guide_scale,
                          refine_guide_rescale,
                          style_template,
                          style_negative_template,
                          image_seed,
+                         largen_state,
+                         largen_task,
+                         largen_image_scale,
+                         largen_tar_image,
+                         largen_tar_mask,
+                         largen_masked_image,
+                         largen_ref_image,
+                         largen_ref_mask,
+                         largen_ref_clip,
+                         largen_base_image,
+                         largen_extra_sizes,
+                         largen_bbox_yyxx,
+                         largen_history,
                          show_jpeg_image=True):
         if control_state and control_cond_image is None:
             raise gr.Error(self.component_names.control_err1)
 
         current_pipeline = self.pipe_manager.get_pipeline_given_modules({
             'diffusion_model':
             diffusion_model,
@@ -107,17 +125,16 @@
             'pipeline'][0]
         used_tuner_model = []
         if not isinstance(tuner_model, list):
             tuner_model = [tuner_model]
         for tuner_m in tuner_model:
             if tuner_m is None or tuner_m == '':
                 continue
-            if (now_pipeline in self.pipe_manager.model_level_info['tuners']
-                    and tuner_m in self.pipe_manager.model_level_info['tuners']
-                [now_pipeline]):
+            if now_pipeline in self.pipe_manager.model_level_info['tuners'] and \
+               tuner_m in self.pipe_manager.model_level_info['tuners'][now_pipeline]:
                 tuner_m = self.pipe_manager.model_level_info['tuners'][
                     now_pipeline][tuner_m]['model_info']
                 used_tuner_model.append(tuner_m)
         used_custom_tuner_model = []
         if not isinstance(custom_tuner_model, list):
             custom_tuner_model = [custom_tuner_model]
         for tuner_m in custom_tuner_model:
@@ -159,29 +176,47 @@
         if refine_state:
             pipeline_input['refine_sampler'] = refine_sampler
             pipeline_input['refine_discretization'] = refine_discretization
             pipeline_input['refine_guide_scale'] = refine_guide_scale
             pipeline_input['refine_guide_rescale'] = refine_guide_rescale
         else:
             refine_strength = 0
+        if largen_state:
+            largen_cfg = {
+                'largen_task': largen_task,
+                'largen_image_scale': largen_image_scale,
+                'largen_tar_image': largen_tar_image,
+                'largen_tar_mask': largen_tar_mask,
+                'largen_ref_image': largen_ref_image,
+                'largen_ref_mask': largen_ref_mask,
+                'largen_masked_image': largen_masked_image,
+                'largen_ref_clip': largen_ref_clip,
+                'largen_base_image': largen_base_image,
+                'largen_extra_sizes': largen_extra_sizes,
+                'largen_bbox_yyxx': largen_bbox_yyxx,
+            }
+        else:
+            largen_cfg = {}
+
         results = current_pipeline(
             pipeline_input,
             num_samples=image_number,
             intermediate_callback=None,
             refine_strength=refine_strength,
             img_to_img_strength=0,
             tuner_model=used_tuner_model +
             used_custom_tuner_model if tuner_state else None,
             tuner_scale=tuner_scale if tuner_state or control_state else None,
             control_model=control_model if control_state else None,
             control_scale=control_scale
             if tuner_state or control_state else None,
             control_cond_image=control_cond_image if control_state else None,
             crop_type=crop_type if control_state else None,
-            seed=int(image_seed))
+            seed=int(image_seed),
+            **largen_cfg)
         images = []
         before_images = []
         if 'images' in results:
             images_tensor = results['images'] * 255
             images = [
                 Image.fromarray(images_tensor[idx].permute(
                     1, 2, 0).cpu().numpy().astype(np.uint8))
@@ -194,35 +229,42 @@
                 Image.fromarray(before_refine_images_tensor[idx].permute(
                     1, 2, 0).cpu().numpy().astype(np.uint8))
                 for idx in range(before_refine_images_tensor.shape[0])
             ]
         if 'seed' in results:
             print(results['seed'])
         print(images, before_images)
+        largen_history.extend(images)
+        if len(largen_history) > 10:
+            largen_history = largen_history[-10:]
         if show_jpeg_image:
             save_list = []
             for i, img in enumerate(images):
-                save_image = os.path.join(self.cfg.WORK_DIR,
+                save_image = os.path.join(self.local_work_dir,
                                           f'cur_gallery_{i}.jpg')
                 img.save(save_image)
                 save_list.append(save_image)
             images = save_list
+
         return (
             gr.Column(visible=len(before_images) > 0),
             before_images,
             images,
+            largen_history,
+            gr.update(value=largen_history),
         )
 
     def generate_image(self, *args, **kwargs):
         gallery_result = self.generate_gallery(*args, **kwargs)
-        before_refine_panel, before_refine_gallery, output_gallery = gallery_result
+        before_refine_panel, before_refine_gallery, output_gallery, _ = gallery_result
         return (before_refine_panel, before_refine_gallery, output_gallery[0])
 
     def set_callbacks(self, inference_ui, model_manage_ui, diffusion_ui,
-                      mantra_ui, tuner_ui, refiner_ui, control_ui, **kwargs):
+                      mantra_ui, tuner_ui, refiner_ui, control_ui, largen_ui,
+                      **kwargs):
 
         self.gen_inputs = [
             self.prompt, mantra_ui.state, tuner_ui.state, control_ui.state,
             refiner_ui.state, model_manage_ui.diffusion_model,
             model_manage_ui.first_stage_model,
             model_manage_ui.cond_stage_model, refiner_ui.refiner_cond_model,
             refiner_ui.refiner_diffusion_model, tuner_ui.tuner_model,
@@ -233,20 +275,28 @@
             diffusion_ui.sampler, diffusion_ui.discretization,
             diffusion_ui.output_height, diffusion_ui.output_width,
             diffusion_ui.image_number, diffusion_ui.sample_steps,
             diffusion_ui.guide_scale, diffusion_ui.guide_rescale,
             refiner_ui.refine_strength, refiner_ui.refine_sampler,
             refiner_ui.refine_discretization, refiner_ui.refine_guide_scale,
             refiner_ui.refine_guide_rescale, mantra_ui.style_template,
-            mantra_ui.style_negative_template, diffusion_ui.image_seed
+            mantra_ui.style_negative_template, diffusion_ui.image_seed,
+            largen_ui.state, largen_ui.task, largen_ui.image_scale,
+            largen_ui.tar_image, largen_ui.tar_mask, largen_ui.masked_image,
+            largen_ui.ref_image, largen_ui.ref_mask, largen_ui.ref_clip,
+            largen_ui.base_image, largen_ui.extra_sizes, largen_ui.bbox_yyxx,
+            largen_ui.image_history
         ]
 
         self.gen_outputs = [
-            self.before_refine_panel, self.before_refine_gallery,
-            self.output_gallery
+            self.before_refine_panel,
+            self.before_refine_gallery,
+            self.output_gallery,
+            largen_ui.image_history,
+            largen_ui.gallery,
         ]
 
         self.generate_button.click(self.generate_gallery,
                                    inputs=self.gen_inputs,
                                    outputs=self.gen_outputs,
                                    queue=True)
```

## scepter/studio/inference/inference_ui/mantra_ui.py

```diff
@@ -43,22 +43,23 @@
                 all_styles[one_style.BASE_MODEL].append(one_style.NAME)
             # if one_style.get('IMAGE_PATH', None):
             #     one_style.IMAGE_PATH = FS.get_from(one_style.IMAGE_PATH)
         return name_level_style, all_styles
 
     def create_ui(self, *args, **kwargs):
         self.state = gr.State(value=False)
-        with gr.Column(visible=False) as self.tab:
-            with gr.Row():
+        with gr.Column(equal_height=True, visible=False) as self.tab:
+            with gr.Row(scale=1):
                 with gr.Column(scale=1):
                     with gr.Group(visible=True):
                         with gr.Row(equal_height=True):
                             self.style = gr.Dropdown(
                                 label=self.component_names.mantra_styles,
-                                choices=self.all_styles[self.default_pipeline],
+                                choices=self.all_styles.get(
+                                    self.default_pipeline, []),
                                 value=None,
                                 multiselect=True,
                                 interactive=True)
                         with gr.Row(equal_height=True):
                             with gr.Column(scale=1):
                                 self.style_name = gr.Text(
                                     value='',
```

## scepter/studio/inference/inference_ui/model_manage_ui.py

```diff
@@ -142,19 +142,28 @@
             all_module_name = {}
             for module_name in self.pipe_manager.module_list:
                 module = getattr(pipeline_ins, module_name)
                 if module is None:
                     continue
                 model_name = f"{now_pipeline}_{module['name']}"
                 all_module_name[module_name] = model_name
+            tunner_choices = []
             if now_pipeline in self.default_choices['tuners']:
                 tunner_choices = self.default_choices['tuners'][now_pipeline][
                     'choices']
-            else:
-                tunner_choices = []
+            custom_tunner_choices = []
+            custom_tunner_default = []
+            if now_pipeline in self.default_choices.get(
+                    'customized_tuners', []):
+                custom_tunner_choices = self.default_choices[
+                    'customized_tuners'][now_pipeline]['choices']
+                custom_tunner_default = self.default_choices[
+                    'customized_tuners'][now_pipeline]['default']
+                if isinstance(custom_tunner_default, str):
+                    custom_tunner_default = [custom_tunner_default]
 
             if now_pipeline in self.default_choices[
                     'controllers'] and control_mode in self.default_choices[
                         'controllers'][now_pipeline]:
                 controller_choices = self.default_choices['controllers'][
                     now_pipeline][control_mode]['choices']
                 controller_default = self.default_choices['controllers'][
@@ -175,17 +184,19 @@
                                                  default_input)
             diffusion_ui.cur_paras = cur_paras
             return (
                 diffusion_model,
                 gr.Dropdown(value=all_module_name['first_stage_model']),
                 gr.Dropdown(value=all_module_name['cond_stage_model']),
                 gr.Dropdown(choices=tunner_choices, value=[]),
+                gr.Dropdown(choices=custom_tunner_choices,
+                            value=custom_tunner_default),
                 gr.Dropdown(choices=controller_choices,
                             value=controller_default),
-                gr.Dropdown(choices=mantra_ui.all_styles[now_pipeline],
+                gr.Dropdown(choices=mantra_ui.all_styles.get(now_pipeline, []),
                             value=[]),
                 gr.Textbox(choices=cur_paras.NEGATIVE_PROMPT.get('VALUES', []),
                            value=cur_paras.NEGATIVE_PROMPT.get('DEFAULT', '')),
                 gr.Textbox(choices=cur_paras.PROMPT_PREFIX.get('VALUES', []),
                            value=cur_paras.PROMPT_PREFIX.get('DEFAULT', '')),
                 gr.Dropdown(choices=[key for key in h_level_dict.keys()],
                             value=default_res[0]),
@@ -202,14 +213,15 @@
             inputs=[
                 self.diffusion_state, self.diffusion_model,
                 control_ui.control_mode
             ],
             outputs=[
                 self.diffusion_state, self.first_stage_model,
                 self.cond_stage_model, tuner_ui.tuner_model,
-                control_ui.control_model, mantra_ui.style,
-                diffusion_ui.negative_prompt, diffusion_ui.prompt_prefix,
-                diffusion_ui.output_height, diffusion_ui.sampler,
-                diffusion_ui.discretization, diffusion_ui.sample_steps,
-                diffusion_ui.guide_scale, diffusion_ui.guide_rescale
+                tuner_ui.custom_tuner_model, control_ui.control_model,
+                mantra_ui.style, diffusion_ui.negative_prompt,
+                diffusion_ui.prompt_prefix, diffusion_ui.output_height,
+                diffusion_ui.sampler, diffusion_ui.discretization,
+                diffusion_ui.sample_steps, diffusion_ui.guide_scale,
+                diffusion_ui.guide_rescale
             ],
             queue=True)
```

## scepter/studio/inference/inference_ui/tuner_ui.py

```diff
@@ -17,25 +17,29 @@
         self.cfg = cfg
         self.pipe_manager = pipe_manager
         self.default_choices = pipe_manager.module_level_choices
         default_diffusion_model = self.default_choices['diffusion_model'][
             'default']
         self.default_pipeline = pipe_manager.model_level_info[
             default_diffusion_model]['pipeline'][0]
+        self.tunner_choices = []
         if self.default_pipeline in self.default_choices['tuners']:
             self.tunner_choices = self.default_choices['tuners'][
                 self.default_pipeline]['choices']
             self.tunner_default = self.default_choices['tuners'][
                 self.default_pipeline]['default']
-        else:
-            self.tunner_choices = []
+        self.custom_tuner_choices = []
+        if self.default_pipeline in self.default_choices.get(
+                'customized_tuners', []):
+            self.custom_tuner_choices = self.default_choices[
+                'customized_tuners'][self.default_pipeline]['choices']
 
         self.tunner_default = None
         self.component_names = TunerUIName(language)
-        self.cfg_tuners = cfg.TUNERS
+        self.cfg_tuners = cfg.TUNERS + cfg.CUSTOM_TUNERS
         self.name_level_tuners = {}
         for one_tuner in tqdm(self.cfg_tuners):
             if one_tuner.BASE_MODEL not in self.name_level_tuners:
                 self.name_level_tuners[one_tuner.BASE_MODEL] = {}
             # if one_tuner.get('IMAGE_PATH', None):
             #     one_tuner.IMAGE_PATH = FS.get_from(one_tuner.IMAGE_PATH)
             if language == 'zh':
@@ -43,34 +47,40 @@
                     one_tuner.NAME_ZH] = one_tuner
             else:
                 self.name_level_tuners[one_tuner.BASE_MODEL][
                     one_tuner.NAME] = one_tuner
 
     def create_ui(self, *args, **kwargs):
         self.state = gr.State(value=False)
-        with gr.Column(visible=False) as self.tab:
-            with gr.Row():
+        with gr.Column(equal_height=True, visible=False) as self.tab:
+            with gr.Row(scale=1):
                 with gr.Column(variant='panel', scale=1, min_width=0):
                     with gr.Group(visible=True):
                         with gr.Row(equal_height=True):
                             with gr.Column(scale=1):
                                 self.tuner_model = gr.Dropdown(
                                     label=self.component_names.tuner_model,
                                     choices=self.tunner_choices,
                                     value=None,
                                     multiselect=True,
                                     interactive=True)
                             with gr.Column(scale=1):
                                 self.custom_tuner_model = gr.Dropdown(
                                     label=self.component_names.
                                     custom_tuner_model,
-                                    choices=[],
+                                    choices=self.custom_tuner_choices,
                                     value=None,
                                     multiselect=True,
                                     interactive=True)
+                                self.save_button = gr.Button(
+                                    label=self.component_names.save_button,
+                                    value=self.component_names.save_button,
+                                    elem_classes='type_row',
+                                    elem_id='save_button',
+                                    visible=True)
                         with gr.Row(equal_height=True):
                             with gr.Column(scale=1):
                                 self.tuner_type = gr.Text(
                                     value='',
                                     label=self.component_names.tuner_type)
                             with gr.Column(scale=1):
                                 self.base_model = gr.Text(
@@ -106,26 +116,27 @@
                     value=1.0,
                     interactive=True)
 
             self.example_block = gr.Accordion(
                 label=self.component_names.example_block_name, open=True)
 
     def set_callbacks(self, model_manage_ui, **kwargs):
+        manager = kwargs.pop('manager')
         gallery_ui = kwargs.pop('gallery_ui')
         with self.example_block:
             gr.Examples(examples=self.component_names.examples,
                         inputs=[self.tuner_model, gallery_ui.prompt])
 
         def tuner_model_change(tuner_model, diffusion_model):
             diffusion_model_info = self.pipe_manager.model_level_info[
                 diffusion_model]
             now_pipeline = diffusion_model_info['pipeline'][0]
             tuner_info = {}
             if tuner_model is not None and len(tuner_model) > 0:
-                tuner_info = self.name_level_tuners[now_pipeline].get(
+                tuner_info = self.name_level_tuners.get(now_pipeline, {}).get(
                     tuner_model[-1], {})
             if tuner_info.get(
                     'IMAGE_PATH',
                     None) and not os.path.exists(tuner_info.IMAGE_PATH):
                 tuner_info.IMAGE_PATH = FS.get_from(tuner_info.IMAGE_PATH)
             return (gr.Text(value=tuner_info.get('TUNER_TYPE', '')),
                     gr.Text(value=tuner_info.get('BASE_MODEL', '')),
@@ -137,7 +148,49 @@
             tuner_model_change,
             inputs=[self.tuner_model, model_manage_ui.diffusion_model],
             outputs=[
                 self.tuner_type, self.base_model, self.tuner_desc,
                 self.tuner_example, self.tuner_prompt_example
             ],
             queue=False)
+
+        self.custom_tuner_model.change(
+            tuner_model_change,
+            inputs=[self.custom_tuner_model, model_manage_ui.diffusion_model],
+            outputs=[
+                self.tuner_type, self.base_model, self.tuner_desc,
+                self.tuner_example, self.tuner_prompt_example
+            ],
+            queue=False)
+
+        def save_customized_tuner(tuner_model, diffusion_model):
+            diffusion_model_info = self.pipe_manager.model_level_info[
+                diffusion_model]
+            now_pipeline = diffusion_model_info['pipeline'][0]
+            tuner_info = {}
+            if tuner_model is not None and len(tuner_model) > 0:
+                tuner_info = self.name_level_tuners.get(now_pipeline, {}).get(
+                    tuner_model[-1], {})
+            if tuner_info.get(
+                    'IMAGE_PATH',
+                    None) and not os.path.exists(tuner_info.IMAGE_PATH):
+                tuner_info.IMAGE_PATH = FS.get_from(tuner_info.IMAGE_PATH)
+            return (gr.Tabs(selected='tuner_manager'),
+                    gr.Text(value=tuner_info.NAME), gr.Text(value=''),
+                    gr.Text(value=tuner_info.get('TUNER_TYPE', '')),
+                    gr.Text(value=tuner_info.get('BASE_MODEL', '')),
+                    gr.Text(value=tuner_info.get('DESCRIPTION', '')),
+                    gr.Image(value=tuner_info.get('IMAGE_PATH', None)),
+                    gr.Text(value=tuner_info.get('PROMPT_EXAMPLE', '')))
+
+        self.save_button.click(
+            save_customized_tuner,
+            inputs=[self.custom_tuner_model, model_manage_ui.diffusion_model],
+            outputs=[
+                manager.tabs, manager.tuner_manager.info_ui.tuner_name,
+                manager.tuner_manager.info_ui.new_name,
+                manager.tuner_manager.info_ui.tuner_type,
+                manager.tuner_manager.info_ui.base_model,
+                manager.tuner_manager.info_ui.tuner_desc,
+                manager.tuner_manager.info_ui.tuner_example,
+                manager.tuner_manager.info_ui.tuner_prompt_example
+            ])
```

## scepter/studio/preprocess/preprocess.py

```diff
@@ -42,15 +42,15 @@
     def create_ui(self):
         self.create_dataset.create_ui()
         self.dataset_gallery.create_ui()
         self.export_dataset.create_ui()
 
     def set_callbacks(self, manager):
         self.create_dataset.set_callbacks(self.dataset_gallery,
-                                          self.export_dataset)
+                                          self.export_dataset, manager)
         self.dataset_gallery.set_callbacks(self.create_dataset)
         self.export_dataset.set_callbacks(self.create_dataset, manager)
 
 
 if __name__ == '__main__':
     pre_ui = PreprocessUI('scepter/methods/studio/preprocess/preprocess.yaml',
                           root_work_dir='./cache')
```

## scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py

```diff
@@ -20,15 +20,14 @@
 
 refresh_symbol = '\U0001f504'  # 
 
 
 class CreateDatasetUI(UIBase):
     def __init__(self, cfg, is_debug=False, language='en'):
         self.work_dir = cfg.WORK_DIR
-        self.dir_list = FS.walk_dir(self.work_dir, recurse=False)
         self.cache_file = {}
         self.meta_dict = {}
         self.dataset_list = self.load_history()
         self.components_name = CreateDatasetUIName(language)
 
     def load_meta(self, meta_file):
         dataset_meta = json.load(open(meta_file, 'r'))
@@ -67,34 +66,41 @@
             save_meta.pop('local_work_dir')
         if 'work_dir' in meta:
             save_meta.pop('work_dir')
         with FS.put_to(meta_file) as local_path:
             json.dump(save_meta, open(local_path, 'w'))
         return meta_file
 
-    def construct_meta(self, cursor, file_list, dataset_folder, user_name):
+    def construct_meta(self, cursor, file_list, dataset_folder, user_name,
+                       login_user_name):
         '''
             {
                 "dataset_name": "xxxx",
                 "dataset_scale": 100,
                 "file_list": "xxxxx", # image_path#;#width#;#height#;#caption
                 "update_time": "",
                 "create_time": ""
             }
         '''
         train_csv = os.path.join(dataset_folder, 'train.csv')
         train_csv = self.write_csv(file_list, train_csv, dataset_folder)
         save_file_list = os.path.join(dataset_folder, 'file.csv')
         save_file_list = self.write_file_list(file_list, save_file_list)
         meta = {
-            'dataset_name': user_name,
-            'cursor': cursor,
-            'file_list': file_list,
-            'train_csv': train_csv,
-            'save_file_list': save_file_list
+            'dataset_name':
+            user_name if login_user_name == '' or login_user_name is None else
+            '_'.join([login_user_name, user_name]),
+            'cursor':
+            cursor,
+            'file_list':
+            file_list,
+            'train_csv':
+            train_csv,
+            'save_file_list':
+            save_file_list
         }
         self.save_meta(meta, dataset_folder)
         return meta
 
     def load_from_list(self, save_file, dataset_folder, local_dataset_folder):
         file_list = []
         images_folder = os.path.join(local_dataset_folder, 'images')
@@ -176,67 +182,88 @@
                 f"unzip -o '{local_path}' -d '{local_dataset_folder}'")
             res = res.readlines()
         if not os.path.exists(local_dataset_folder):
             raise gr.Error(f'{save_file}{str(res)}')
         file_folder = None
         train_list = None
         hit_dir = None
-        raw_list = []
+        raw_list = {}
         mac_osx = os.path.join(local_dataset_folder, '__MACOSX')
         if os.path.exists(mac_osx):
             res = os.popen(f"rm -rf '{mac_osx}'")
             res = res.readlines()
         for one_dir in FS.walk_dir(local_dataset_folder, recurse=False):
             if one_dir.endswith('__MACOSX'):
                 res = os.popen(f"rm -rf '{one_dir}'")
                 res = res.readlines()
                 continue
             if FS.isdir(one_dir):
-                sub_dir = FS.walk_dir(one_dir)
-                for one_s_dir in sub_dir:
-                    if FS.isdir(one_s_dir) and one_s_dir.split(
-                            one_dir)[1].replace('/', '') == 'images':
-                        file_folder = one_s_dir
-                        hit_dir = one_dir
-                    if FS.isfile(one_s_dir) and one_s_dir.split(
-                            one_dir)[1].replace('/', '') == 'train.csv':
-                        train_list = one_s_dir
-                    if file_folder is not None and train_list is not None:
-                        break
-                    if (one_s_dir.endswith('.jpg')
-                            or one_s_dir.endswith('.jpeg')
-                            or one_s_dir.endswith('.png')
-                            or one_s_dir.endswith('.webp')):
-                        raw_list.append(one_s_dir)
+                if one_dir.endswith('images') or one_dir.endswith('images/'):
+                    file_folder = one_dir
+                    hit_dir = one_dir
+                else:
+                    sub_dir = FS.walk_dir(one_dir)
+                    for one_s_dir in sub_dir:
+                        if FS.isdir(one_s_dir) and one_s_dir.split(
+                                one_dir)[1].replace('/', '') == 'images':
+                            file_folder = one_s_dir
+                            hit_dir = one_dir
+                        if FS.isfile(one_s_dir) and one_s_dir.split(
+                                one_dir)[1].replace('/', '') == 'train.csv':
+                            train_list = one_s_dir
+                        if file_folder is not None and train_list is not None:
+                            break
+                        if (one_s_dir.endswith('.jpg')
+                                or one_s_dir.endswith('.jpeg')
+                                or one_s_dir.endswith('.png')
+                                or one_s_dir.endswith('.webp')):
+                            file_name, surfix = os.path.splitext(one_s_dir)
+                            txt_file = file_name + '.txt'
+                            if os.path.exists(txt_file):
+                                raw_list[one_s_dir] = txt_file
+                            else:
+                                raw_list[one_s_dir] = None
+            elif one_dir.endswith('train.csv'):
+                train_list = one_dir
             else:
                 if (one_dir.endswith('.jpg') or one_dir.endswith('.jpeg')
                         or one_dir.endswith('.png')
                         or one_dir.endswith('.webp')):
-                    raw_list.append(one_dir)
-
+                    file_name, surfix = os.path.splitext(one_dir)
+                    txt_file = file_name + '.txt'
+                    if os.path.exists(txt_file):
+                        raw_list[one_dir] = txt_file
+                    else:
+                        raw_list[one_dir] = None
+            if file_folder is not None and train_list is not None:
+                break
         if file_folder is None and len(raw_list) < 1:
             raise gr.Error(
                 "images folder or train.csv doesn't exists, or nothing exists in your zip"
             )
         new_file_folder = f'{local_dataset_folder}/images'
         os.makedirs(new_file_folder, exist_ok=True)
         if file_folder is not None:
             _ = FS.get_dir_to_local_dir(file_folder, new_file_folder)
         elif len(raw_list) > 0:
-            raw_list = list(set(raw_list))
+            raw_list = [[k, v] for k, v in raw_list.items()]
             for img_id, cur_image in enumerate(raw_list):
-                _, surfix = os.path.splitext(cur_image)
+                image_name, surfix = os.path.splitext(cur_image[0])
+                if cur_image[1] is not None and os.path.exists(cur_image[1]):
+                    prompt = open(cur_image[1], 'r').read()
+                else:
+                    prompt = image_name.split('/')[-1]
                 try:
                     os.rename(
-                        os.path.abspath(cur_image),
-                        f'{new_file_folder}/{get_md5(cur_image)}{surfix}')
+                        os.path.abspath(cur_image[0]),
+                        f'{new_file_folder}/{get_md5(cur_image[0])}{surfix}')
                     raw_list[img_id] = [
                         os.path.join('images',
-                                     f'{get_md5(cur_image)}{surfix}'),
-                        cur_image.split('/')[-1]
+                                     f'{get_md5(cur_image[0])}{surfix}'),
+                        prompt
                     ]
                 except Exception as e:
                     print(e)
 
         if not os.path.exists(new_file_folder):
             raise gr.Error(f'{str(res)}')
         new_train_list = f'{local_dataset_folder}/train.csv'
@@ -247,21 +274,22 @@
                 for cur_image, cur_prompt in raw_list:
                     writer.writerow([cur_image, cur_prompt])
         else:
             res = os.popen(f"mv '{train_list}' '{new_train_list}'")
             res = res.readlines()
         if not os.path.exists(new_train_list):
             raise gr.Error(f'{str(res)}')
-        try:
-            res = os.popen(f"rm -rf '{hit_dir}/images/*'")
-            _ = res.readlines()
-            res = os.popen(f"rm -rf '{hit_dir}'")
-            _ = res.readlines()
-        except Exception:
-            pass
+        if not file_folder == hit_dir:
+            try:
+                res = os.popen(f"rm -rf '{hit_dir}/images/*'")
+                _ = res.readlines()
+                res = os.popen(f"rm -rf '{hit_dir}'")
+                _ = res.readlines()
+            except Exception:
+                pass
         file_list = self.load_train_csv(new_train_list, data_folder)
         return file_list
 
     def get_image_meta(self, image):
         img = Image.open(image)
         return img.size
 
@@ -313,29 +341,33 @@
                 if not prefix == '' and file_path.startswith(prefix):
                     file_path = file_path.split(prefix)[-1]
                 while file_path.startswith('/'):
                     file_path = file_path[1:]
                 return True, file_path
         return False, file_path
 
-    def load_history(self):
+    def load_history(self, login_user_name=''):
         dataset_list = []
+        self.dir_list = FS.walk_dir(self.work_dir, recurse=False)
         for one_dir in self.dir_list:
             if FS.isdir(one_dir):
                 meta_file = os.path.join(one_dir, 'meta.json')
                 if FS.exists(meta_file):
                     local_dataset_folder, _ = FS.map_to_local(one_dir)
-                    local_dataset_folder = FS.get_dir_to_local_dir(
-                        one_dir, local_dataset_folder, multi_thread=True)
+                    if not FS.exists(
+                            os.path.join(local_dataset_folder, 'meta.json')):
+                        local_dataset_folder = FS.get_dir_to_local_dir(
+                            one_dir, local_dataset_folder, multi_thread=True)
                     meta_data = self.load_meta(
                         os.path.join(local_dataset_folder, 'meta.json'))
                     meta_data['local_work_dir'] = local_dataset_folder
                     meta_data['work_dir'] = one_dir
-                    dataset_list.append(meta_data['dataset_name'])
-                    self.meta_dict[meta_data['dataset_name']] = meta_data
+                    if meta_data['dataset_name'].startswith(login_user_name):
+                        dataset_list.append(meta_data['dataset_name'])
+                        self.meta_dict[meta_data['dataset_name']] = meta_data
         return dataset_list
 
     def create_ui(self):
         with gr.Box():
             gr.Markdown(self.components_name.user_direction)
         with gr.Box():
             with gr.Row():
@@ -392,15 +424,15 @@
                                 value=self.components_name.modify_data_button)
 
         self.dataset_panel = panel
         self.btn_panel = btn_panel
         self.file_panel = file_panel
         self.modify_panel = modify_panel
 
-    def set_callbacks(self, gallery_dataset, export_dataset):
+    def set_callbacks(self, gallery_dataset, export_dataset, manager):
         def show_dataset_panel():
             return (gr.Column(visible=False), gr.Column(visible=True),
                     gr.Column(visible=True),
                     gr.Checkbox(value=False, visible=False),
                     gr.Text(value=get_random_dataset_name(),
                             interactive=True), 1)
 
@@ -415,38 +447,42 @@
                     gr.Checkbox(value=False, visible=True))
 
         def get_random_dataset_name():
             data_name = 'name-version-{0:%Y%m%d_%H_%M_%S}'.format(
                 datetime.datetime.now())
             return data_name
 
-        def refresh():
-            return gr.Dropdown(value=self.dataset_list[-1]
-                               if len(self.dataset_list) > 0 else '',
-                               choices=self.dataset_list)
+        def refresh(login_user_name):
+            dataset_list = self.load_history(login_user_name=login_user_name)
+            return gr.Dropdown(
+                value=dataset_list[-1] if len(dataset_list) > 0 else '',
+                choices=dataset_list)
 
         self.refresh_dataset_name.click(refresh,
+                                        inputs=[manager.user_name],
                                         outputs=[self.dataset_name],
                                         queue=False)
 
         def confirm_create_dataset(user_name, create_mode, file_url, file_path,
-                                   panel_state):
+                                   panel_state, login_user_name):
             if user_name.strip() == '' or ' ' in user_name or '/' in user_name:
                 raise gr.Error(self.components_name.illegal_data_name_err1)
 
             if len(user_name.split('-')) < 3:
                 raise gr.Error(self.components_name.illegal_data_name_err2)
 
             if '.' in user_name:
                 raise gr.Error(self.components_name.illegal_data_name_err3)
 
             if not file_url.strip() == '' and file_path is not None:
                 raise gr.Error(self.components_name.illegal_data_name_err4)
             if create_mode == 3 and not file_url.strip() == '':
-                file_name, surfix = os.path.splitext(file_url.split('?')[0])
+                if 'oss' in file_url:
+                    file_url = file_url.split('?')[0]
+                file_name, surfix = os.path.splitext(file_url)
                 save_file = os.path.join(self.work_dir, f'{user_name}{surfix}')
                 local_path, _ = FS.map_to_local(save_file)
                 res = os.popen(f"wget -c '{file_url}' -O '{local_path}'")
                 res.readlines()
                 FS.put_object_from_local_file(local_path, save_file)
                 if not FS.exists(save_file):
                     raise gr.Error(
@@ -486,29 +522,29 @@
                                                 dataset_folder,
                                                 multi_thread=True)
             if not is_flag:
                 raise gr.Error(f'{self.components_name.illegal_data_err3}')
 
             cursor = 0 if len(file_list) > 0 else -1
             meta = self.construct_meta(cursor, file_list, dataset_folder,
-                                       user_name)
+                                       user_name, login_user_name)
 
             meta['local_work_dir'] = local_dataset_folder
             meta['work_dir'] = dataset_folder
 
             self.meta_dict[meta['dataset_name']] = meta
             if meta['dataset_name'] not in self.dataset_list:
                 self.dataset_list.append(meta['dataset_name'])
-            return (
-                gr.Checkbox(value=True, visible=False),
-                gr.Dropdown(value=user_name, choices=self.dataset_list),
-            )
+            return (gr.Checkbox(value=True, visible=False),
+                    gr.Dropdown(value=meta['dataset_name'],
+                                choices=self.dataset_list),
+                    gr.Text(value=meta['dataset_name']))
 
         def clear_file():
-            return gr.Text(visible=True)
+            return gr.Text(visible=False)
 
         # Click Create
         self.btn_create_datasets.click(show_dataset_panel, [], [
             self.file_panel, self.dataset_panel, self.btn_panel,
             self.panel_state, self.user_data_name, self.create_mode
         ],
                                        queue=False)
@@ -541,17 +577,17 @@
         self.file_path.clear(clear_file,
                              outputs=[self.file_path_url],
                              queue=False)
 
         # Click Confirm
         self.confirm_data_button.click(confirm_create_dataset, [
             self.user_data_name, self.create_mode, self.file_path_url,
-            self.file_path, self.panel_state
-        ], [self.panel_state, self.dataset_name],
-                                       queue=True)
+            self.file_path, self.panel_state, manager.user_name
+        ], [self.panel_state, self.dataset_name, self.user_data_name],
+                                       queue=False)
 
         def show_edit_panel(panel_state, data_name):
             if panel_state:
                 return (gr.Row(visible=True), gr.Row(visible=True),
                         gr.Row(visible=True), gr.Column(visible=True),
                         gr.Column(visible=False), gr.Column(visible=False),
                         data_name)
@@ -564,15 +600,15 @@
             show_edit_panel, [self.panel_state, self.dataset_name], [
                 gallery_dataset.gallery_panel, gallery_dataset.upload_panel,
                 export_dataset.export_panel, self.modify_panel,
                 self.file_panel, self.btn_panel, self.user_data_name_state
             ],
             queue=False)
 
-        def modify_data_name(user_name, prev_data_name):
+        def modify_data_name(user_name, prev_data_name, login_user_name):
             print(
                 f'Current file name {prev_data_name}, new file name {user_name}.'
             )
             if user_name.strip() == '' or ' ' in user_name or '/' in user_name:
                 raise gr.Error(self.components_name.illegal_data_name_err1)
             if len(user_name.split('-')) < 3:
                 raise gr.Error(self.components_name.illegal_data_name_err2)
@@ -596,15 +632,16 @@
                     is_flag = FS.put_dir_from_local_dir(local_dataset_folder,
                                                         dataset_folder,
                                                         multi_thread=True)
                     if not is_flag:
                         raise gr.Error(self.components_name.illegal_data_err3)
                     cursor = ori_meta['cursor']
                     meta = self.construct_meta(cursor, file_list,
-                                               dataset_folder, user_name)
+                                               dataset_folder, user_name,
+                                               login_user_name)
                     meta['local_work_dir'] = local_dataset_folder
                     meta['work_dir'] = dataset_folder
 
                     if prev_data_name in self.dataset_list:
                         self.dataset_list.remove(prev_data_name)
                         self.dataset_list.append(user_name)
                     self.meta_dict.pop(prev_data_name)
@@ -620,15 +657,18 @@
                     value=user_name,
                     select_index=len(self.dataset_list) - 1)
             else:
                 return user_name, gr.Dropdown()
 
         self.modify_data_button.click(
             modify_data_name,
-            inputs=[self.user_data_name, self.user_data_name_state],
+            inputs=[
+                self.user_data_name, self.user_data_name_state,
+                manager.user_name
+            ],
             outputs=[self.user_data_name_state, self.dataset_name],
             queue=False)
 
         def dataset_change(user_name):
             if user_name is None or user_name == '':
                 raise gr.Error(self.components_name.illegal_data_name_err5 +
                                f'{user_name}')
@@ -643,7 +683,18 @@
                                  inputs=[self.dataset_name],
                                  outputs=[
                                      self.dataset_panel, self.file_panel,
                                      self.panel_state, self.user_data_name,
                                      gallery_dataset.gallery_state
                                  ],
                                  queue=False)
+
+        def login_user_name_change(login_user_name):
+            dataset_list = self.load_history(login_user_name=login_user_name)
+            return gr.Dropdown(
+                value=dataset_list[-1] if len(dataset_list) > 0 else '',
+                choices=dataset_list)
+
+        manager.user_name.change(login_user_name_change,
+                                 inputs=[manager.user_name],
+                                 outputs=[self.dataset_name],
+                                 queue=False)
```

## scepter/studio/self_train/self_train.py

```diff
@@ -3,15 +3,15 @@
 import os
 
 import gradio as gr
 
 import scepter
 from scepter.modules.utils.config import Config
 from scepter.modules.utils.file_system import FS
-from scepter.studio.self_train.self_train_ui.inference_ui import InferenceUI
+from scepter.studio.self_train.self_train_ui.model_ui import ModelUI
 from scepter.studio.self_train.self_train_ui.trainer_ui import TrainerUI
 from scepter.studio.self_train.utils.config_parser import get_all_config
 from scepter.studio.utils.env import init_env
 
 
 class SelfTrainUI():
     def __init__(self,
@@ -30,28 +30,28 @@
             FS.make_dir(cfg_general.WORK_DIR)
         cfg_general = init_env(cfg_general)
 
         self.trainer_ui = TrainerUI(cfg_general,
                                     BASE_CFG_VALUE,
                                     is_debug=is_debug,
                                     language=language)
-        self.inference_ui = InferenceUI(cfg_general,
-                                        BASE_CFG_VALUE,
-                                        is_debug=is_debug,
-                                        language=language)
+        self.model_ui = ModelUI(cfg_general,
+                                BASE_CFG_VALUE,
+                                is_debug=is_debug,
+                                language=language)
 
     def create_ui(self):
         with gr.Row():
             self.trainer_ui.create_ui()
         with gr.Row():
-            self.inference_ui.create_ui()
+            self.model_ui.create_ui()
 
     def set_callbacks(self, manager):
-        self.trainer_ui.set_callbacks(self.inference_ui)
-        self.inference_ui.set_callbacks(self.trainer_ui, manager)
+        self.trainer_ui.set_callbacks(self.model_ui, manager)
+        self.model_ui.set_callbacks(self.trainer_ui, manager)
 
 
 if __name__ == '__main__':
     st_ins = SelfTrainUI(os.path.join(
         scepter.dirname, 'scepter/methods/studio/self_train/self_train.yaml'),
                          is_debug=True,
                          language='zh',
```

## scepter/studio/self_train/scripts/run_task.py

```diff
@@ -4,14 +4,15 @@
 import os
 
 import cv2
 import numpy as np
 import torch
 
 from scepter.modules.solver.hooks.checkpoint import CheckpointHook
+from scepter.modules.solver.hooks.data_probe import ProbeDataHook
 from scepter.modules.solver.registry import SOLVERS
 from scepter.modules.utils.config import Config
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.file_system import FS
 from scepter.modules.utils.logger import get_logger
 
 
@@ -23,14 +24,15 @@
     # std_logger.info(f"Os environment: {os.environ}")
     if cfg.args.stage == 'train':
         solver = SOLVERS.build(cfg.SOLVER, logger=std_logger)
         save_config(cfg)
         solver.set_up_pre()
         solver.set_up()
         ori_steps = solver.max_steps
+
         if 'train' in solver.datas:
             dataset = solver.datas['train'].dataset
             if hasattr(dataset, 'real_number'):
                 solver.max_steps = int(
                     cfg.SOLVER.MAX_EPOCHS * dataset.real_number /
                     (solver.datas['train'].batch_size * we.world_size))
                 std_logger.info(
@@ -44,15 +46,28 @@
                             hook.interval = int(hook.interval *
                                                 solver.max_steps /
                                                 cfg.SOLVER.MAX_EPOCHS)
                             std_logger.info(
                                 f'checkpoint save interval is changed from {ori_interval} '
                                 f'to {hook.interval} according to the setting epoches '
                                 f'interval {ori_interval}')
-            # size 
+
+                if 'eval' in solver.hooks_dict:
+                    for hook in solver.hooks_dict['eval']:
+                        if isinstance(hook, ProbeDataHook):
+                            ori_interval = hook.prob_interval
+                            hook.prob_interval = int(hook.prob_interval *
+                                                     solver.max_steps /
+                                                     cfg.SOLVER.MAX_EPOCHS)
+                            std_logger.info(
+                                f'prob interval is changed from {ori_interval} '
+                                f'to {hook.prob_interval} according to the setting epoches '
+                                f'interval {ori_interval}')
+                        solver.eval_interval = hook.prob_interval
+
         solver.solve()
 
 
 def save_image(image, save_path, backend='cv2'):
     if backend == 'cv2':
         image = image.copy()
         cv2.cvtColor(image, cv2.COLOR_RGB2BGR, image)
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## scepter/studio/self_train/self_train_ui/component_names.py

```diff
@@ -1,15 +1,16 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 # For dataset manager
-class InferenceUIName():
+class ModelUIName():
     def __init__(self, language='en'):
         if language == 'en':
             self.output_model_block = 'Model Output'
             self.output_model_name = 'Output Model Name'
+            self.output_ckpt_name = 'Output Ckpt Name'
             self.test_prompt = 'Test Prompt'
             self.test_prefix = 'Test Prefix'
             self.test_n_prompt = 'Negative Prompt'
             self.sampler = 'Sampler'
             self.num_inference_steps = 'Sampling Step Length'
             self.inference_num = 'Number of Inferences'
             self.generator_seed = 'Sampling Seed'
@@ -17,43 +18,59 @@
             self.inference_resolution = 'Inference Resolution'
             self.output_image = 'Output Result'
             self.display_button = 'Infer'
             self.extra_model_gtxt = 'Extra Model'
             self.extra_model_gbtn = 'Add Model'
             self.refresh_model_gbtn = 'Refresh Model'
             self.go_to_inference = 'Go to inference'
+            self.btn_export_log = 'Export Log'
+            self.export_file = 'Log File'
+            self.log_block = 'Training Log...'
+            self.gallery_block = 'Gallery Log...'
+            self.eval_gallery = 'Eval Gallery'
             # Error or Warning
             self.inference_err1 = 'Inference failed, please try again.'
             self.inference_err2 = 'Test prompt is empty.'
-            self.inference_err3 = "Doesn't surpport this base model"
-            self.inference_err4 = "This model maybe not finish training, because model doesn't exist."
+            self.model_err3 = "Doesn't surpport this base model"
+            self.model_err4 = "This model maybe not finish training, because model doesn't exist."
+            self.model_err5 = "Model {} doesn't exist."
+            self.training_warn1 = 'No log message util now.'
 
         elif language == 'zh':
             self.output_model_block = ''
-            self.output_model_name = ''
+            self.output_model_name = ''
+            self.output_ckpt_name = ''
             self.test_prompt = ''
             self.test_prefix = ''
             self.test_n_prompt = ''
             self.sampler = ''
             self.num_inference_steps = ''
             self.inference_num = ''
             self.generator_seed = ''
             self.tuner_method = ''
             self.inference_resolution = ''
             self.output_image = ''
             self.display_button = ''
             self.extra_model_gtxt = ''
             self.extra_model_gbtn = ''
             self.refresh_model_gbtn = ''
+            self.btn_export_log = ''
+            self.export_file = ''
+            self.log_block = '...'
+            self.training_button = ''
+            self.gallery_block = '...'
+            self.eval_gallery = ''
             # Error or Warning
             self.inference_err1 = ''
             self.inference_err2 = ''
-            self.inference_err3 = ''
+            self.model_err3 = ''
             self.go_to_inference = ''
-            self.inference_err4 = ''
+            self.model_err4 = ''
+            self.model_err5 = '{}'
+            self.training_warn1 = ''
 
 
 class TrainerUIName():
     def __init__(self, language='en'):
         if language == 'en':
             self.user_direction = '''
                 ### User Guide
@@ -76,26 +93,26 @@
             self.ms_data_name_place_hold = 'Supports MaaS dataset/local/HTTP Zip package'
             self.ms_data_space = 'ModelScope Space'
             self.ms_data_subname = 'MaaS Dataset - Subset'
             self.training_block = 'Training Parameters'
             self.base_model = 'Base Model'
             self.tuner_name = 'Fine-tuning Method'
             self.base_model_revision = 'Model Version Number'
-            self.resolution = 'Resolution'
+            self.resolution_height = 'Resolution Height'
+            self.resolution_width = 'Resolution Width'
             self.train_epoch = 'Number of Training Epochs'
             self.learning_rate = 'Learning Rate'
             self.save_interval = 'Save Interval'
             self.train_batch_size = 'Training Batch Size'
             self.prompt_prefix = 'Prefix'
             self.replace_keywords = 'Trigger Keywords'
             self.work_name = 'Save Model Name (refresh to get a random value)'
             self.push_to_hub = 'Push to hub'
-            self.log_block = 'Training Log...'
             self.training_button = 'Start Training'
-
+            self.eval_prompts = 'Eval Prompts'
             # Error or Warning
             self.training_err1 = 'CUDA is unavailable.'
             self.training_err2 = 'Currently insufficient VRAM, training failed!'
             self.training_err3 = 'You need to prepare training data.'
             self.training_err4 = 'Save model name already exists or is None, please regenerate this name.'
             self.training_err5 = 'Training failed.'
             self.training_err6 = "Can't process training data"
@@ -117,25 +134,25 @@
             self.ms_data_name_place_hold = 'MaaS//Http Zip'
             self.ms_data_space = 'ModelScope '
             self.ms_data_subname = 'MaaS-'
             self.training_block = ''
             self.base_model = ''
             self.tuner_name = ''
             self.base_model_revision = ''
-            self.resolution = ''
+            self.resolution_height = ''
+            self.resolution_width = ''
             self.train_epoch = ''
             self.learning_rate = ''
             self.save_interval = ''
             self.train_batch_size = ''
             self.prompt_prefix = ''
             self.replace_keywords = ''
             self.work_name = ''
             self.push_to_hub = ''
-            self.log_block = '...'
-            self.training_button = ''
+            self.eval_prompts = ''
             # Error or Warning
             self.training_err1 = 'CUDA.'
             self.training_err2 = ''
             self.training_err3 = ''
             self.training_err4 = ''
             self.training_err5 = ''
             self.training_err6 = ''
```

## scepter/studio/self_train/self_train_ui/trainer_ui.py

```diff
@@ -1,59 +1,80 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import copy
 import datetime
+import json
 import os
 import random
-import time
+from collections import OrderedDict
 
 import gradio as gr
 import torch
 import yaml
 
 import scepter
 from scepter.modules.utils.file_system import FS
+from scepter.studio.self_train.scripts.trainer import TrainManager
 from scepter.studio.self_train.self_train_ui.component_names import \
     TrainerUIName
 from scepter.studio.self_train.utils.config_parser import (
     get_default, get_values_by_model, get_values_by_model_version,
-    get_values_by_model_version_tuner,
-    get_values_by_model_version_tuner_resolution)
+    get_values_by_model_version_tuner)
 from scepter.studio.utils.uibase import UIBase
 
 
 def print_memory_status(is_debug):
     if not is_debug:
         nvi_info = os.popen('nvidia-smi').read()
         gpu_mem = nvi_info.split('\n')[9].split('|')[2].split('/')[0].strip()
     else:
         gpu_mem = 0
     return gpu_mem
 
 
+def is_basic_or_container_type(obj):
+    basic_and_container_types = (int, float, str, bool, complex, list, tuple,
+                                 dict, set)
+    return isinstance(obj, basic_and_container_types)
+
+
 refresh_symbol = '\U0001f504'  # 
 
 
-def get_work_name(model, version, tuner, resolution):
-    model_prefix = f'Swift@{model}@{version}@{tuner}@{resolution}'
+def get_work_name(model, version, tuner):
+    model_prefix = f'Swift@{model}@{version}@{tuner}'
     return model_prefix + '@' + '{0:%Y%m%d%H%M%S%f}'.format(
         datetime.datetime.now()) + ''.join(
             [str(random.randint(1, 10)) for i in range(3)])
 
 
 class TrainerUI(UIBase):
     def __init__(self, cfg, all_cfg_value, is_debug=False, language='en'):
         self.BASE_CFG_VALUE = all_cfg_value
         self.para_data = get_default(self.BASE_CFG_VALUE)
+        self.train_para_data = cfg.TRAIN_PARAS
         self.run_script = os.path.join(os.path.dirname(scepter.dirname),
                                        cfg.SCRIPT_DIR, 'run_task.py')
         self.work_dir_pre, _ = FS.map_to_local(cfg.WORK_DIR)
         self.is_debug = is_debug
+        self.current_train_model = None
+        self.trainer_ins = TrainManager(self.run_script, self.work_dir_pre)
         self.component_names = TrainerUIName(language=language)
 
+        self.h_level_dict = {}
+        for hw_tuple in self.train_para_data.RESOLUTIONS.get('VALUES', []):
+            h, w = hw_tuple
+            if h not in self.h_level_dict:
+                self.h_level_dict[h] = []
+            self.h_level_dict[h].append(w)
+        self.h_level_dict = OrderedDict(
+            sorted(self.h_level_dict.items(),
+                   key=lambda x: int(x[0]),
+                   reverse=False))
+
     def create_ui(self):
         with gr.Box():
             with gr.Row(variant='panel', equal_height=True):
                 with gr.Column(variant='panel'):
                     gr.Markdown(self.component_names.user_direction)
                     self.data_type = gr.Dropdown(
                         choices=self.component_names.data_type_choices,
@@ -85,39 +106,52 @@
                                     choices=self.para_data.get(
                                         'model_choices', []),
                                     value=self.para_data.get(
                                         'model_default', ''),
                                     label=self.component_names.base_model,
                                     interactive=True)
                             with gr.Column(scale=1, min_width=0):
+                                self.base_model_revision = gr.Dropdown(
+                                    choices=self.para_data.get(
+                                        'version_choices', []),
+                                    value=self.para_data.get(
+                                        'version_default', ''),
+                                    label=self.component_names.
+                                    base_model_revision,
+                                    interactive=True)
+
+                        with gr.Row():
+                            with gr.Column(scale=1, min_width=0):
                                 self.tuner_name = gr.Dropdown(
                                     choices=self.para_data.get(
                                         'tuner_choices', []),
                                     value=self.para_data.get(
                                         'tuner_default', ''),
                                     label=self.component_names.tuner_name,
                                     interactive=True)
+
                         with gr.Row():
                             with gr.Column(scale=1, min_width=0):
-                                self.base_model_revision = gr.Dropdown(
-                                    choices=self.para_data.get(
-                                        'version_choices', []),
+                                self.resolution_height = gr.Dropdown(
+                                    choices=list(self.h_level_dict.keys()),
                                     value=self.para_data.get(
-                                        'version_default', ''),
+                                        'RESOLUTION', 1024)[0],
                                     label=self.component_names.
-                                    base_model_revision,
+                                    resolution_height,
+                                    allow_custom_value=True,
                                     interactive=True)
 
                             with gr.Column(scale=1, min_width=0):
-                                self.resolution = gr.Dropdown(
-                                    choices=self.para_data.get(
-                                        'resolution_choices', []),
+                                self.resolution_width = gr.Dropdown(
+                                    choices=self.h_level_dict[
+                                        self.resolution_height.value],
                                     value=self.para_data.get(
-                                        'resolution_default', 1024),
-                                    label=self.component_names.resolution,
+                                        'RESOLUTION', 1024)[1],
+                                    label=self.component_names.
+                                    resolution_width,
                                     allow_custom_value=True,
                                     interactive=True)
 
                         with gr.Row():
                             with gr.Column(scale=1, min_width=0):
                                 self.train_epoch = gr.Number(
                                     label=self.component_names.train_epoch,
@@ -157,27 +191,44 @@
                             with gr.Column(scale=1, min_width=0):
                                 self.replace_keywords = gr.Text(
                                     label=self.component_names.
                                     replace_keywords,
                                     value='')
 
                         with gr.Row():
-                            with gr.Column(scale=5, min_width=0):
-                                self.work_name = gr.Text(
-                                    label=self.component_names.work_name,
-                                    value=None,
-                                    interactive=False)
                             with gr.Column(scale=1, min_width=0):
-                                self.work_name_button = gr.Button(
-                                    value=refresh_symbol)
-                            with gr.Column(scale=2, min_width=0):
-                                self.push_to_hub = gr.Checkbox(
-                                    label=self.component_names.push_to_hub,
-                                    value=False,
-                                    visible=False)
+                                self.eval_prompts = gr.Dropdown(
+                                    value=None,
+                                    choices=self.train_para_data.get(
+                                        'EVAL_PROMPTS', []),
+                                    label=self.component_names.eval_prompts,
+                                    interactive=True,
+                                    multiselect=True,
+                                    allow_custom_value=True,
+                                    max_choices=20)
+
+            with gr.Row(variant='panel', equal_height=True):
+                with gr.Box():
+                    with gr.Row(variant='panel', equal_height=True):
+                        gr.Markdown(self.component_names.work_name)
+                    with gr.Row(variant='panel', equal_height=True):
+                        with gr.Column(scale=6, min_width=0):
+                            self.work_name = gr.Text(value=None,
+                                                     container=False,
+                                                     interactive=False)
+                        with gr.Column(scale=2, min_width=0):
+                            self.work_name_button = gr.Button(
+                                value=refresh_symbol, size='lg')
+
+            with gr.Row(variant='panel', visible=False, equal_height=True):
+                with gr.Column(scale=2, min_width=0):
+                    self.push_to_hub = gr.Checkbox(
+                        label=self.component_names.push_to_hub,
+                        value=False,
+                        visible=False)
 
             with gr.Row(variant='panel', equal_height=True):
                 self.examples = gr.Examples(
                     examples=[
                         [
                             self.component_names.data_type_choices[0],
                             '',
@@ -189,23 +240,18 @@
                             'style_custom_dataset', '3D'
                         ]
                     ],
                     inputs=[
                         self.data_type, self.ms_data_space, self.ms_data_name,
                         self.ms_data_subname
                     ])
-
-            with gr.Row(variant='panel', equal_height=True):
-                with gr.Box():
-                    gr.Markdown(self.component_names.log_block)
-                    self.training_message = gr.Markdown()
             with gr.Row(variant='panel', equal_height=True):
                 self.training_button = gr.Button()
 
-    def set_callbacks(self, inference_ui):
+    def set_callbacks(self, inference_ui, manager):
         def change_data_type(data_type):
             if data_type == self.component_names.data_type_choices[0]:
                 return gr.Box(visible=False)
             elif data_type == self.component_names.data_type_choices[1]:
                 return gr.Box(visible=True)
 
         self.data_type.change(fn=change_data_type,
@@ -213,15 +259,15 @@
                               outputs=[self.ms_data_box],
                               queue=False)
 
         self.work_name_button.click(fn=get_work_name,
                                     inputs=[
                                         self.base_model,
                                         self.base_model_revision,
-                                        self.tuner_name, self.resolution
+                                        self.tuner_name
                                     ],
                                     outputs=[self.work_name],
                                     queue=False)
 
         def change_train_value_by_model(base_model):
             '''
                 Changes to the base model will affect the training parameters,
@@ -241,25 +287,30 @@
                 ret_data.get('SAVE_INTERVAL', 10), \
                 ret_data.get('TRAIN_BATCH_SIZE', 4), \
                 ret_data.get('TRAIN_PREFIX', ''), \
                 gr.Dropdown(value=ret_data.get('version_default', ''), choices=ret_data.get('version_choices', []),
                             interactive=True), \
                 gr.Dropdown(value=ret_data.get('tuner_default', ''), choices=ret_data.get('tuner_choices', []),
                             interactive=True), \
-                gr.Dropdown(value=ret_data.get('resolution_default', 1024),
-                            choices=ret_data.get('resolution_choices', []), interactive=True)
+                gr.Dropdown(value=ret_data.get('RESOLUTION', 1024)[0],
+                            choices=list(self.h_level_dict.keys()),
+                            interactive=True), \
+                gr.Dropdown(value=ret_data.get('RESOLUTION', 1024)[1],
+                            choices=self.h_level_dict[ret_data.get('RESOLUTION', 1024)[0]],
+                            interactive=True)
 
         self.base_model.change(fn=change_train_value_by_model,
                                inputs=[self.base_model],
                                outputs=[
                                    self.train_epoch, self.learning_rate,
                                    self.save_interval, self.train_batch_size,
                                    self.prompt_prefix,
                                    self.base_model_revision, self.tuner_name,
-                                   self.resolution
+                                   self.resolution_height,
+                                   self.resolution_width
                                ],
                                queue=False)
 
         #
         def change_train_value_by_model_version(base_model,
                                                 base_model_revision):
             '''
@@ -278,32 +329,36 @@
                                                    base_model,
                                                    base_model_revision)
             return ret_data.get('EPOCHS', 10), \
                 ret_data.get('LEARNING_RATE', 0.0001), \
                 ret_data.get('SAVE_INTERVAL', 10), \
                 ret_data.get('TRAIN_BATCH_SIZE', 4), \
                 ret_data.get('TRAIN_PREFIX', ''), \
-                gr.Dropdown(value=ret_data.get('tuner_default', ''), choices=ret_data.get('tuner_choices', []),
+                gr.Dropdown(value=ret_data.get('tuner_default', ''),
+                            choices=ret_data.get('tuner_choices', []),
+                            interactive=True), \
+                gr.Dropdown(value=ret_data.get('RESOLUTION', 1024)[0],
+                            choices=list(self.h_level_dict.keys()),
                             interactive=True), \
-                gr.Dropdown(value=ret_data.get('resolution_default', 1024),
-                            choices=ret_data.get('resolution_choices', []), interactive=True)
+                gr.Dropdown(value=ret_data.get('RESOLUTION', 1024)[1],
+                            choices=self.h_level_dict[ret_data.get('RESOLUTION', 1024)[0]],
+                            interactive=True)
 
         #
         self.base_model_revision.change(
             fn=change_train_value_by_model_version,
             inputs=[self.base_model, self.base_model_revision],
             outputs=[
                 self.train_epoch, self.learning_rate, self.save_interval,
                 self.train_batch_size, self.prompt_prefix, self.tuner_name,
-                self.resolution
+                self.resolution_height, self.resolution_width
             ],
             queue=False)
 
         #
-        #
         def change_train_value_by_model_version_tuner(base_model,
                                                       base_model_revision,
                                                       tuner_name):
             '''
                 Changes to the base model will affect the training parameters,
                 and it is best to define the related default values in the YAML.
                 Training Iterations:
@@ -319,117 +374,105 @@
                 self.BASE_CFG_VALUE, base_model, base_model_revision,
                 tuner_name)
             return ret_data.get('EPOCHS', 10), \
                 ret_data.get('LEARNING_RATE', 0.0001), \
                 ret_data.get('SAVE_INTERVAL', 10), \
                 ret_data.get('TRAIN_BATCH_SIZE', 4), \
                 ret_data.get('TRAIN_PREFIX', ''), \
-                gr.Dropdown(value=ret_data.get('resolution_default', 1024),
-                            choices=ret_data.get('resolution_choices', []), interactive=True)
+                gr.Dropdown(value=ret_data.get('RESOLUTION', 1024)[0],
+                            choices=list(self.h_level_dict.keys()),
+                            interactive=True), \
+                gr.Dropdown(value=ret_data.get('RESOLUTION', 1024)[1],
+                            choices=self.h_level_dict[ret_data.get('RESOLUTION', 1024)[0]],
+                            interactive=True)
 
         #
         self.tuner_name.change(fn=change_train_value_by_model_version_tuner,
                                inputs=[
                                    self.base_model, self.base_model_revision,
                                    self.tuner_name
                                ],
                                outputs=[
                                    self.train_epoch, self.learning_rate,
                                    self.save_interval, self.train_batch_size,
-                                   self.prompt_prefix, self.resolution
+                                   self.prompt_prefix, self.resolution_height,
+                                   self.resolution_width
                                ],
                                queue=False)
 
-        #
-        def change_train_value_by_model_version_tuner_resolution(
-                base_model, base_model_revision, tuner_name, resolution):
-            '''
-                Changes to the base model will affect the training parameters,
-                and it is best to define the related default values in the YAML.
-                Training Iterations:
-                Learning Rate:
-                Training Prefix Used:
-                Training Batch Size:
-                Supported Resolutions:
-                Supported Fine-tuning Methods:
-                Supported Fine-tuning Methods:
-                Prefix for Saved Model:
-            '''
-            ret_data = get_values_by_model_version_tuner_resolution(
-                self.BASE_CFG_VALUE, base_model, base_model_revision,
-                tuner_name, resolution)
-            print('change_train_value_by_model_version_tuner_resolution',
-                  ret_data)
-            # work_name = get_work_name(base_model, base_model_revision,
-            #                           tuner_name, resolution)
-            return ret_data.get('EPOCHS', 10), \
-                ret_data.get('LEARNING_RATE', 0.0001), \
-                ret_data.get('SAVE_INTERVAL', 10), \
-                ret_data.get('TRAIN_BATCH_SIZE', 4), \
-                ret_data.get('TRAIN_PREFIX', '')
-
-        #
-        self.resolution.change(
-            fn=change_train_value_by_model_version_tuner_resolution,
-            inputs=[
-                self.base_model, self.base_model_revision, self.tuner_name,
-                self.resolution
-            ],
-            outputs=[
-                self.train_epoch, self.learning_rate, self.save_interval,
-                self.train_batch_size, self.prompt_prefix
-            ],
-            queue=False)
+        def change_resolution(h):
+            if h not in self.h_level_dict:
+                return gr.Dropdown()
+            all_choices = self.h_level_dict[h]
+            default = all_choices[0]
+            return gr.Dropdown(choices=all_choices, value=default)
+
+        self.resolution_height.change(change_resolution,
+                                      inputs=[self.resolution_height],
+                                      outputs=[self.resolution_width],
+                                      queue=False)
 
         def run_train(work_name, data_type, ms_data_space, ms_data_name,
                       ms_data_subname, base_model, base_model_revision,
-                      tuner_name, resolution, train_epoch, learning_rate,
-                      save_interval, train_batch_size, prompt_prefix,
-                      replace_keywords, push_to_hub):
+                      tuner_name, resolution_height, resolution_width,
+                      train_epoch, learning_rate, save_interval,
+                      train_batch_size, prompt_prefix, replace_keywords,
+                      push_to_hub, eval_prompts):
+
             # Check Cuda
             if not torch.cuda.is_available() and not self.is_debug:
                 raise gr.Error(self.component_names.training_err1)
 
             if work_name == 'custom' or work_name is None or work_name == '':
                 raise gr.Error(self.component_names.training_err4)
             work_dir = os.path.join(self.work_dir_pre, work_name)
-            if not os.path.exists(work_dir):
-                os.makedirs(work_dir)
-            else:
+            self.current_train_model = work_name
+            if os.path.exists(work_dir) or os.path.exists(
+                    f'.flag/{work_name}.tmp'):
                 raise gr.Error(self.component_names.training_err4)
+            else:
+                os.makedirs(work_dir)
+            os.makedirs('.flag', exist_ok=True)
+            with open(f'.flag/{work_name}.tmp', 'w') as f:
+                f.write('new line.')
+            # save params
+            with open(os.path.join(work_dir, 'params.json'), 'w') as f_out:
+                json.dump(
+                    {
+                        key: val
+                        for key, val in locals().items()
+                        if is_basic_or_container_type(val)
+                    },
+                    f_out,
+                    ensure_ascii=False)
 
             if push_to_hub:
                 model_id = work_name
                 model_id = model_id.replace('@', '-')
                 hub_model_id = f'scepter/{model_id}'
             else:
                 hub_model_id = ''
 
-            # Check Cuda Memory
-            if torch.cuda.is_available() and not self.is_debug:
-                device = torch.device('cuda:0')
-                required_memory_bytes = 4 * (1024**3)
-                try:
-                    tensor = torch.empty(  # noqa
-                        (required_memory_bytes // 4, ), device=device
-                    )  # create 4GB tensor to check the memory if enough
-                    del tensor
-                except RuntimeError:
-                    raise gr.Error(self.component_names.training_err2)
-
             # Check Instance Valid
             if ms_data_name is None:
                 raise gr.Error(self.component_names.training_err3)
 
-            st_time = time.time()
-
-            def prepare_data(data_cfg):
+            def prepare_train_data(data_cfg):
                 data_cfg['BATCH_SIZE'] = int(train_batch_size)
                 data_cfg['PROMPT_PREFIX'] = prompt_prefix
                 data_cfg['REPLACE_KEYWORDS'] = replace_keywords
+                for trans in data_cfg['TRANSFORMS']:
+                    if trans['NAME'] in [
+                            'Resize', 'FlexibleResize', 'CenterCrop',
+                            'FlexibleCenterCrop'
+                    ]:
+                        trans['SIZE'] = [
+                            int(resolution_height),
+                            int(resolution_width)
+                        ]
                 if data_type in self.component_names.data_type_choices:
                     if ms_data_name.startswith(
                             'http') or ms_data_name.endswith('zip'):
                         work_data_dir = os.path.join(work_dir, 'data')
                         if not os.path.exists(work_data_dir):
                             os.makedirs(work_data_dir)
                         ms_data_http_zip_path = ms_data_name
@@ -478,23 +521,35 @@
                             data_cfg['MS_REMAP_KEYS'] = {
                                 'Image:FILE': 'Target:FILE'
                             }
                         elif ms_data_name == 'lora-stable-diffusion-finetune':
                             data_cfg['MS_REMAP_KEYS'] = {'Text': 'Prompt'}
                         else:
                             data_cfg['MS_REMAP_KEYS'] = None
-                    data_cfg['OUTPUT_SIZE'] = int(resolution)
+                    data_cfg['OUTPUT_SIZE'] = [
+                        int(resolution_height),
+                        int(resolution_width)
+                    ]
+                return data_cfg
+
+            def prepare_eval_data(data_cfg):
+                data_cfg['PROMPT_PREFIX'] = prompt_prefix
+                data_cfg['IMAGE_SIZE'] = [
+                    int(resolution_height),
+                    int(resolution_width)
+                ]
+                data_cfg['PROMPT_DATA'] = eval_prompts
                 return data_cfg
 
             def prepare_train_config():
                 cfg_file = os.path.join(work_dir, 'train.yaml')
                 current_model_info = self.BASE_CFG_VALUE[base_model][
                     base_model_revision]
                 modify_para = current_model_info['modify_para']
-                cfg = current_model_info['config_value']
+                cfg = copy.deepcopy(current_model_info['config_value'])
                 if isinstance(modify_para, dict) and tuner_name in modify_para:
                     modify_c = modify_para[tuner_name]
                     if isinstance(modify_c, dict) and 'TRAIN' in modify_c:
                         train_modify_c = modify_c['TRAIN']
                         if isinstance(train_modify_c, dict):
                             for key, val in train_modify_c.items():
                                 cache_value = [cfg]
@@ -514,73 +569,73 @@
                 # update config
                 cfg['SOLVER']['WORK_DIR'] = work_dir
                 cfg['SOLVER']['OPTIMIZER']['LEARNING_RATE'] = float(
                     learning_rate * 640 / int(train_batch_size))
                 cfg['SOLVER']['MAX_EPOCHS'] = int(train_epoch)
                 cfg['SOLVER']['TRAIN_DATA']['BATCH_SIZE'] = int(
                     train_batch_size)
-                cfg['SOLVER']['TUNER'] = current_model_info[
-                    'tuner_para'][tuner_name] if isinstance(
-                        current_model_info['tuner_para'],
-                        dict) and tuner_name in current_model_info[
-                            'tuner_para'] else None
-                cfg['SOLVER']['TRAIN_DATA'] = prepare_data(
+                if 'TUNER' in cfg['SOLVER']:
+                    cfg['SOLVER']['TUNER'] = current_model_info['tuner_para'][
+                        tuner_name] if isinstance(
+                            current_model_info['tuner_para'],
+                            dict) and tuner_name in current_model_info[
+                                'tuner_para'] else None
+                cfg['SOLVER']['TRAIN_DATA'] = prepare_train_data(
                     cfg['SOLVER']['TRAIN_DATA'])
+                if eval_prompts is not None and len(eval_prompts) > 0:
+                    cfg['SOLVER']['EVAL_DATA'] = prepare_eval_data(
+                        cfg['SOLVER']['EVAL_DATA'])
+                else:
+                    cfg['SOLVER'].pop('EVAL_DATA')
+                if 'SAMPLE_ARGS' in cfg['SOLVER']:
+                    cfg['SOLVER']['SAMPLE_ARGS']['IMAGE_SIZE'] = [
+                        int(resolution_height),
+                        int(resolution_width)
+                    ]
                 for hook in cfg['SOLVER']['TRAIN_HOOKS']:
                     if hook['NAME'] == 'CheckpointHook':
                         hook['INTERVAL'] = save_interval
                         hook['PUSH_TO_HUB'] = push_to_hub
                         hook['HUB_MODEL_ID'] = hub_model_id
+                if 'EVAL_HOOKS' in cfg['SOLVER']:
+                    for hook in cfg['SOLVER']['EVAL_HOOKS']:
+                        if hook['NAME'] == 'ProbeDataHook':
+                            hook['PROB_INTERVAL'] = save_interval
 
                 with open(cfg_file, 'w') as f_out:
                     yaml.dump(cfg,
                               f_out,
                               encoding='utf-8',
                               allow_unicode=True,
                               default_flow_style=False)
                 return cfg_file
 
-            cfg = prepare_train_config()
-
-            def train_fn(cfg_file):
-                torch.cuda.empty_cache()
-                cmd = f'PYTHONPATH=. python {self.run_script} ' \
-                      f'--cfg={cfg_file}  2> {self.work_dir_pre}/std_out.txt'
-                print(cmd)
-                if not self.is_debug:
-                    res = os.system(cmd)
-                else:
-                    res = 0
-                if res != 0:
-                    error_info = '\n'.join(
-                        open(f'{self.work_dir_pre}/std_out.txt',
-                             'r').read().split('\n')[-20:])
-                    raise gr.Error(
-                        f'{self.component_names.training_err5} ({error_info}) '
-                    )
-
-            train_fn(cfg)
-
+            before_kill_inference = self.trainer_ins.check_memory()
+            for k, v in manager.inference.pipe_manager.pipeline_level_modules.items(
+            ):
+                if hasattr(v, 'dynamic_unload'):
+                    v.dynamic_unload(name='all')
+            after_kill_inference = self.trainer_ins.check_memory()
+            message = f'GPU info: {before_kill_inference}. \n\n'
+            message += f'After unloading inference models, the GPU info: {after_kill_inference}. \n\n'
+            _ = prepare_train_config()
+            self.trainer_ins.start_task(work_name)
+            message += self.trainer_ins.get_log(work_name)
             if work_name not in inference_ui.model_list:
                 inference_ui.model_list.append(work_name)
-            message = f'''
-                        Training completed! \n
-                        Save in [ {work_name} ] \n
-                        Take time [ {time.time() - st_time:.4f}s ] \n
-                        Mem: [ {print_memory_status(self.is_debug)} ]
-                        '''
-            print(message)
-            return message, gr.Dropdown.update(choices=inference_ui.model_list,
-                                               value=work_name)
+            gr.Info('Start Training!' + message)
+            return gr.Dropdown.update(choices=inference_ui.model_list,
+                                      value=work_name)
 
         self.training_button.click(
             run_train,
             inputs=[
                 self.work_name, self.data_type, self.ms_data_space,
                 self.ms_data_name, self.ms_data_subname, self.base_model,
-                self.base_model_revision, self.tuner_name, self.resolution,
+                self.base_model_revision, self.tuner_name,
+                self.resolution_height, self.resolution_width,
                 self.train_epoch, self.learning_rate, self.save_interval,
                 self.train_batch_size, self.prompt_prefix,
-                self.replace_keywords, self.push_to_hub
+                self.replace_keywords, self.push_to_hub, self.eval_prompts
             ],
-            outputs=[self.training_message, inference_ui.output_model_name],
+            outputs=[inference_ui.output_model_name],
             queue=True)
```

## scepter/studio/self_train/utils/config_parser.py

```diff
@@ -6,49 +6,39 @@
 import yaml
 
 paras_keys = [
     'TRAIN_BATCH_SIZE', 'TRAIN_PREFIX', 'TRAIN_N_PROMPT', 'RESOLUTION',
     'MEMORY', 'EPOCHS', 'SAVE_INTERVAL', 'EPSEC', 'LEARNING_RATE',
     'IS_DEFAULT', 'TUNER'
 ]
-
 control_paras_keys = ['CONTROL_MODE', 'RESOLUTION', 'IS_DEFAULT']
 
 
 def build_meta_index(meta_cfg, config_file):
     tuner_type = {}
     paras = meta_cfg.get('PARAS', None)
     if paras:
         for idx, para in enumerate(paras):
             for key in paras_keys:
                 if key not in para:
                     print(
                         f'Para key {key} not defined in {config_file} META/RARAS[{idx}]'
                     )
                 assert key in para
-            tuner_type[para['TUNER'] + '@' + str(para['RESOLUTION'])] = para
+            tuner_type[para['TUNER']] = para
             if para['IS_DEFAULT']:
-                tuner_type['default'] = para['TUNER'] + '@' + str(
-                    para['RESOLUTION'])
+                tuner_type['default'] = para['TUNER']
 
     tuner_type['choices'] = list(tuner_type.keys())
     if 'default' in tuner_type['choices']:
         tuner_type['choices'].remove('default')
     if 'default' not in tuner_type:
         tuner_type['default'] = tuner_type['choices'][0] if len(
             tuner_type['choices']) > 0 else ''
 
-    # 
-    choices = {}
-    for t_type in tuner_type['choices']:
-        t_name, resolution = t_type.split('@')
-        if t_name not in choices:
-            choices[t_name] = []
-        choices[t_name].append(int(resolution))
-    tuner_type['choices'] = choices
     tuner_paras = meta_cfg.get('TUNERS', None)
     return tuner_type, tuner_paras
 
 
 def build_meta_index_control(meta_cfg, config_file):
     control_type = {}
     paras = meta_cfg.get('CONTROL_PARAS', None)
@@ -56,37 +46,26 @@
         for idx, para in enumerate(paras):
             for key in control_paras_keys:
                 if key not in para:
                     print(
                         f'Para key {key} not defined in {config_file} META/RARAS[{idx}]'
                     )
                 assert key in para
-            control_type[para['CONTROL_MODE'] + '@' +
-                         str(para['RESOLUTION'])] = para
+            control_type[para['CONTROL_MODE']] = para
             # control_type[para["CONTROL_MODE"]] = para
             if para['IS_DEFAULT']:
-                control_type['default'] = para['CONTROL_MODE'] + '@' + str(
-                    para['RESOLUTION'])
-                # control_type["default"] = para["CONTROL_MODE"]
+                control_type['default'] = para['CONTROL_MODE']
 
     control_type['choices'] = list(control_type.keys())
     if 'default' in control_type['choices']:
         control_type['choices'].remove('default')
     if 'default' not in control_type:
         control_type['default'] = control_type['choices'][0] if len(
             control_type['choices']) > 0 else ''
 
-    # 
-    choices = {}
-    for t_type in control_type['choices']:
-        t_name, resolution = t_type.split('@')
-        if t_name not in choices:
-            choices[t_name] = []
-        choices[t_name].append(int(resolution))
-    control_type['choices'] = choices
     return control_type, paras
 
 
 def get_all_config(config_root, global_meta):
     config_dict = {}
     config_list = glob(os.path.join(config_root, '*/*_pro.yaml'),
                        recursive=True)
@@ -162,125 +141,83 @@
     ret_data['model_default'] = config_dict['default']
     default_version_cfg = config_dict.get(config_dict['default'], None)
     # 
     if default_version_cfg is None:
         return ret_data
     ret_data['version_choices'] = default_version_cfg['choices']
     ret_data['version_default'] = default_version_cfg['default']
-    default_tuner_cfg = default_version_cfg.get(default_version_cfg['default'],
+    default_model_cfg = default_version_cfg.get(default_version_cfg['default'],
                                                 None)
-    if default_tuner_cfg is None:
+    if default_model_cfg is None:
         return ret_data
-    if 'tuner_type' in default_tuner_cfg and default_tuner_cfg['tuner_type'][
+    if 'tuner_type' in default_model_cfg and default_model_cfg['tuner_type'][
             'default'] != '':
-        default_tuner_cfg = default_tuner_cfg['tuner_type']
+        default_tuner_cfg = default_model_cfg['tuner_type']
     else:
         return ret_data
-    ret_data['tuner_choices'] = list(default_tuner_cfg['choices'].keys())
+    ret_data['tuner_choices'] = default_tuner_cfg['choices']
     defalt_t_type = default_tuner_cfg['default']
+    ret_data['tuner_default'] = defalt_t_type
     type_paras = default_tuner_cfg.get(defalt_t_type, None)
-
-    default_t_n = defalt_t_type.split('@')[0]
-    default_r_n = int(defalt_t_type.split('@')[1])
-
-    ret_data['resolution_choices'] = default_tuner_cfg['choices'].get(
-        default_t_n, [])
-    ret_data['tuner_default'] = default_t_n
-    ret_data['resolution_default'] = default_r_n
     if type_paras is not None:
         ret_data.update(type_paras)
     return ret_data
 
 
 def get_values_by_model(config_dict, model_name):
     ret_data = {}
     version_cfg = config_dict.get(model_name, None)
     if version_cfg is None:
         return ret_data
     ret_data['version_choices'] = version_cfg['choices']
     ret_data['version_default'] = version_cfg['default']
-    default_tuner_cfg = version_cfg.get(version_cfg['default'], None)
-    if default_tuner_cfg is None:
+    default_model_cfg = version_cfg.get(version_cfg['default'], None)
+    if default_model_cfg is None:
         return ret_data
-    default_tuner_cfg = default_tuner_cfg['tuner_type']
-    ret_data['tuner_choices'] = list(default_tuner_cfg['choices'].keys())
+
+    default_tuner_cfg = default_model_cfg['tuner_type']
+    ret_data['tuner_choices'] = default_tuner_cfg['choices']
     defalt_t_type = default_tuner_cfg['default']
+    ret_data['tuner_default'] = defalt_t_type
     type_paras = default_tuner_cfg.get(defalt_t_type, None)
-
-    default_t_n = defalt_t_type.split('@')[0]
-    default_r_n = int(defalt_t_type.split('@')[1])
-
-    ret_data['resolution_choices'] = default_tuner_cfg['choices'].get(
-        default_t_n, [])
-    ret_data['tuner_default'] = default_t_n
-    ret_data['resolution_default'] = default_r_n
     if type_paras is not None:
         ret_data.update(type_paras)
     return ret_data
 
 
 def get_values_by_model_version(config_dict, model_name, version):
     ret_data = {}
     version_cfg = config_dict.get(model_name, None)
     if version_cfg is None:
         return ret_data
     tuner_cfg = version_cfg.get(version, None)
     if tuner_cfg is None:
         return ret_data
+
     default_tuner_cfg = tuner_cfg['tuner_type']
-    ret_data['tuner_choices'] = list(default_tuner_cfg['choices'].keys())
+    ret_data['tuner_choices'] = default_tuner_cfg['choices']
     defalt_t_type = default_tuner_cfg['default']
+    ret_data['tuner_default'] = defalt_t_type
     type_paras = default_tuner_cfg.get(defalt_t_type, None)
-
-    default_t_n = defalt_t_type.split('@')[0]
-    default_r_n = int(defalt_t_type.split('@')[1])
-
-    ret_data['resolution_choices'] = default_tuner_cfg['choices'].get(
-        default_t_n, [])
-    ret_data['tuner_default'] = default_t_n
-    ret_data['resolution_default'] = default_r_n
     if type_paras is not None:
         ret_data.update(type_paras)
     return ret_data
 
 
 def get_values_by_model_version_tuner(config_dict, model_name, version,
                                       tuner_name):
     ret_data = {}
     version_cfg = config_dict.get(model_name, None)
     if version_cfg is None:
         return ret_data
-    tuner_cfg = version_cfg.get(version, None)
-    if tuner_cfg is None:
+    model_cfg = version_cfg.get(version, None)
+    if model_cfg is None:
         return ret_data
-    tuner_cfg = tuner_cfg['tuner_type']
-
-    ret_data['resolution_choices'] = tuner_cfg['choices'].get(tuner_name, [])
-
-    if len(ret_data['resolution_choices']) > 0:
-        t_type = '{}@{}'.format(tuner_name, ret_data['resolution_choices'][0])
-        ret_data['resolution_default'] = ret_data['resolution_choices'][0]
-        type_paras = tuner_cfg.get(t_type, None)
-        if type_paras is not None:
-            ret_data.update(type_paras)
-    return ret_data
-
-
-def get_values_by_model_version_tuner_resolution(config_dict, model_name,
-                                                 version, tuner_name,
-                                                 resolution):
-    ret_data = {}
-    version_cfg = config_dict.get(model_name, None)
-    if version_cfg is None:
-        return ret_data
-    tuner_cfg = version_cfg.get(version, None)
-    if tuner_cfg is None:
-        return ret_data
-    tuner_cfg = tuner_cfg['tuner_type']
-    type_paras = tuner_cfg.get('{}@{}'.format(tuner_name, resolution), None)
+    tuner_cfg = model_cfg['tuner_type']
+    type_paras = tuner_cfg.get(tuner_name, None)
     if type_paras is not None:
         ret_data.update(type_paras)
     return ret_data
 
 
 def get_inference_para_by_model_version(config_dict, model_name, version):
     ret_data = {}
@@ -367,19 +304,12 @@
             'control_type']['default'] != '':
         default_control_cfg = default_control_cfg['control_type']
     else:
         return ret_data
 
     ret_data['control_choices'] = list(default_control_cfg['choices'].keys())
     defalt_t_type = default_control_cfg['default']
+    ret_data['control_default'] = defalt_t_type
     type_paras = default_control_cfg.get(defalt_t_type, None)
-
-    default_t_n = defalt_t_type.split('@')[0]
-    default_r_n = int(defalt_t_type.split('@')[1])
-
-    ret_data['resolution_choices'] = default_control_cfg['choices'].get(
-        default_t_n, [])
-    ret_data['control_default'] = default_t_n
-    ret_data['resolution_default'] = default_r_n
     if type_paras is not None:
         ret_data.update(type_paras)
     return ret_data
```

## scepter/tools/helper.py

```diff
@@ -1,16 +1,23 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import argparse
+import importlib
 import os
 import sys
 
 #
 from scepter.modules.utils.registry import REGISTRY_LIST
 
+if os.path.exists('__init__.py'):
+    package_name = 'scepter_ext'
+    spec = importlib.util.spec_from_file_location(package_name, '__init__.py')
+    package = importlib.util.module_from_spec(spec)
+    sys.modules[package_name] = package
+    spec.loader.exec_module(package)
 sys.path.insert(0, os.path.abspath(os.curdir))
 
 
 def get_module_list():
     ret_msg = [v.name for v in REGISTRY_LIST]
     print(ret_msg)
     return None
```

## scepter/tools/run_inference.py

```diff
@@ -1,25 +1,34 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import argparse
+import importlib
 import os
+import sys
 
 import numpy as np
 import torch
 import torch.cuda.amp as amp
 import torchvision.transforms as TT
 from PIL import Image
 
 from scepter.modules.solver.registry import SOLVERS
 from scepter.modules.utils.config import Config
 from scepter.modules.utils.data import transfer_data_to_cuda
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.file_system import FS
 from scepter.modules.utils.logger import get_logger
 
+if os.path.exists('__init__.py'):
+    package_name = 'scepter_ext'
+    spec = importlib.util.spec_from_file_location(package_name, '__init__.py')
+    package = importlib.util.module_from_spec(spec)
+    sys.modules[package_name] = package
+    spec.loader.exec_module(package)
+
 
 def run_task(cfg):
     std_logger = get_logger(name='scepter')
     solver = SOLVERS.build(cfg.SOLVER, logger=std_logger)
     solver.set_up()
     if not cfg.args.pretrained_model == '':
         with FS.get_from(cfg.args.pretrained_model,
```

## scepter/tools/run_train.py

```diff
@@ -1,46 +1,62 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import argparse
+import importlib
+import os
+import sys
 
 from scepter.modules.solver.registry import SOLVERS
 from scepter.modules.utils.config import Config
 from scepter.modules.utils.distribute import we
 from scepter.modules.utils.logger import get_logger
 
+if os.path.exists('__init__.py'):
+    package_name = 'scepter_ext'
+    spec = importlib.util.spec_from_file_location(package_name, '__init__.py')
+    package = importlib.util.module_from_spec(spec)
+    sys.modules[package_name] = package
+    spec.loader.exec_module(package)
+
 
 def run_task(cfg):
     std_logger = get_logger(name='scepter')
     solver = SOLVERS.build(cfg.SOLVER, logger=std_logger)
     solver.set_up_pre()
     solver.set_up()
     solver.solve()
 
 
 def update_config(cfg):
     if hasattr(cfg.args, 'learning_rate') and cfg.args.learning_rate:
-        print(
-            f'learning_rate change from {cfg.SOLVER.OPTIMIZER.LEARNING_RATE} to {cfg.args.learning_rate}'
-        )
+        if cfg.SOLVER.OPTIMIZER.get('LEARNING_RATE', None) is not None:
+            print(
+                f'learning_rate change from {cfg.SOLVER.OPTIMIZER.LEARNING_RATE} to {cfg.args.learning_rate}'
+            )
         cfg.SOLVER.OPTIMIZER.LEARNING_RATE = float(cfg.args.learning_rate)
     if hasattr(cfg.args, 'max_steps') and cfg.args.max_steps:
-        print(
-            f'max_steps change from {cfg.SOLVER.MAX_STEPS} to {cfg.args.max_steps}'
-        )
+        if cfg.SOLVER.get('MAX_STEPS', None) is not None:
+            print(
+                f'max_steps change from {cfg.SOLVER.MAX_STEPS} to {cfg.args.max_steps}'
+            )
         cfg.SOLVER.MAX_STEPS = int(cfg.args.max_steps)
     return cfg
 
 
-if __name__ == '__main__':
+def run():
     parser = argparse.ArgumentParser(description='Argparser for Scepter:\n')
     parser.add_argument('--learning_rate',
                         dest='learning_rate',
                         help='The learning rate for our network!',
                         default=None)
     parser.add_argument('--max_steps',
                         dest='max_steps',
                         help='The max steps for training!',
                         default=None)
 
     cfg = Config(load=True, parser_ins=parser)
     cfg = update_config(cfg)
     we.init_env(cfg, logger=None, fn=run_task)
+
+
+if __name__ == '__main__':
+    run()
```

## scepter/tools/webui.py

```diff
@@ -1,21 +1,30 @@
 # -*- coding: utf-8 -*-
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import argparse
 import datetime
+import importlib
 import os
 import random
+import sys
 
 import gradio as gr
 
 import scepter
 from scepter.modules.utils.config import Config
 from scepter.modules.utils.file_system import FS
 from scepter.modules.utils.logger import get_logger, init_logger
 
+if os.path.exists('__init__.py'):
+    package_name = 'scepter_ext'
+    spec = importlib.util.spec_from_file_location(package_name, '__init__.py')
+    package = importlib.util.module_from_spec(spec)
+    sys.modules[package_name] = package
+    spec.loader.exec_module(package)
+
 
 def prepare(config):
     if 'FILE_SYSTEM' in config:
         for fs_info in config['FILE_SYSTEM']:
             FS.init_fs_client(fs_info)
 
     if 'LOG_FILE' in config:
@@ -53,14 +62,21 @@
                         help='The port of Gradio.')
     parser.add_argument('--language',
                         dest='language',
                         choices=['en', 'zh'],
                         default='en',
                         help='Now we only support english(en) and chinese(zh)')
     args = parser.parse_args()
+    if not os.path.exists(args.config):
+        print(
+            f"{args.config} doesn't exist, find this file in {os.path.dirname(scepter.dirname)}"
+        )
+        args.config = os.path.join(os.path.dirname(scepter.dirname),
+                                   args.config)
+        assert os.path.exists(args.config)
     config = Config(load=True, cfg_file=args.config)
     prepare(config)
 
     tab_manager = TabManager()
     interfaces = []
     for info in config['INTERFACE']:
         name = info.get('NAME_EN',
@@ -70,32 +86,42 @@
             info['CONFIG'] = os.path.join(os.path.dirname(scepter.dirname),
                                           info['CONFIG'])
         if not FS.exists(info['CONFIG']):
             raise f"{info['CONFIG']} doesn't exist."
         interface = None
         if ifid == 'home':
             from scepter.studio.home.home import HomeUI
+
             interface = HomeUI(info['CONFIG'],
                                is_debug=args.debug,
                                language=args.language,
                                root_work_dir=config.WORK_DIR)
         if ifid == 'preprocess':
             from scepter.studio.preprocess.preprocess import PreprocessUI
+
             interface = PreprocessUI(info['CONFIG'],
                                      is_debug=args.debug,
                                      language=args.language,
                                      root_work_dir=config.WORK_DIR)
         if ifid == 'self_train':
             from scepter.studio.self_train.self_train import SelfTrainUI
+
             interface = SelfTrainUI(info['CONFIG'],
                                     is_debug=args.debug,
                                     language=args.language,
                                     root_work_dir=config.WORK_DIR)
+        if ifid == 'tuner_manager':
+            from scepter.studio.tuner_manager.tuner_manager import TunerManagerUI
+            interface = TunerManagerUI(info['CONFIG'],
+                                       is_debug=args.debug,
+                                       language=args.language,
+                                       root_work_dir=config.WORK_DIR)
         if ifid == 'inference':
             from scepter.studio.inference.inference import InferenceUI
+
             interface = InferenceUI(info['CONFIG'],
                                     is_debug=args.debug,
                                     language=args.language,
                                     root_work_dir=config.WORK_DIR)
         if ifid == '':
             pass  # TODO: Add New Features
         if interface:
@@ -105,22 +131,42 @@
     with gr.Blocks() as demo:
         if 'BANNER' in config:
             gr.HTML(config.BANNER)
         else:
             gr.Markdown(
                 f"<h2><center>{config.get('TITLE', 'scepter studio')}</center></h2>"
             )
+        setattr(tab_manager, 'user_name',
+                gr.Text(value='', visible=False, show_label=False))
         with gr.Tabs(elem_id='tabs') as tabs:
             setattr(tab_manager, 'tabs', tabs)
             for interface, label, ifid in interfaces:
                 with gr.TabItem(label, id=ifid, elem_id=f'tab_{ifid}'):
                     interface.create_ui()
             for interface, label, ifid in interfaces:
                 interface.set_callbacks(tab_manager)
+        auth_info = {}
+        if config.have('AUTH_INFO'):
+            for auth_user in config.AUTH_INFO:
+                auth_info[auth_user.USER] = auth_user.PASSWD
+
+        def check_auth(user_name, password):
+            if user_name in auth_info:
+                return auth_info[user_name] == password
+            else:
+                return False
+
+        def init_value(req: gr.Request):
+            print(req.username, 'have login')
+            return gr.Text(value=req.username, visible=False)
+
+        if len(auth_info) > 0:
+            demo.load(init_value, outputs=[tab_manager.user_name])
 
     demo.queue(status_update_rate=1).launch(
         server_name=args.host if args.host else config['HOST'],
-        server_port=args.port if args.port else config['PORT'],
+        server_port=int(args.port) if args.port else config['PORT'],
         root_path=config['ROOT'],
         show_error=True,
         debug=True,
-        enable_queue=True)
+        enable_queue=True,
+        auth=check_auth if len(auth_info) > 0 else None)
```

## Comparing `scepter-0.0.3.post1.dist-info/LICENSE` & `scepter-0.0.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `scepter-0.0.3.post1.dist-info/METADATA` & `scepter-0.0.4.dist-info/METADATA`

 * *Files 20% similar despite different names*

```diff
@@ -1,88 +1,95 @@
 Metadata-Version: 2.1
 Name: scepter
-Version: 0.0.3.post1
+Version: 0.0.4
 Home-page: 
 Author: Tongyi Lab
 Author-email: 
 Keywords: compute vision,framework,generation,image edition.
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Description-Content-Type: text/markdown
 License-File: LICENSE
+Requires-Dist: albumentations
+Requires-Dist: bezier
 Requires-Dist: einops
 Requires-Dist: modelscope
 Requires-Dist: ms-swift (>=1.5.2)
 Requires-Dist: numpy
 Requires-Dist: open-clip-torch
 Requires-Dist: opencv-python
 Requires-Dist: opencv-transforms (>=0.0.6)
 Requires-Dist: oss2 (>=2.15.0)
+Requires-Dist: pycocotools
 Requires-Dist: pyyaml (>=5.3.1)
 Requires-Dist: scikit-image
 Requires-Dist: torchsde
 Requires-Dist: transformers
 Requires-Dist: gradio (<4.0.0,>=3.47.1)
 Requires-Dist: imagehash
+Requires-Dist: psutil
 
 <h1 align="center">SCEPTER</h1>
 
 <p align="center">
 <img src="https://img.shields.io/badge/python-%E2%89%A53.8-5be.svg">
 <img src="https://img.shields.io/badge/pytorch-%E2%89%A51.12%20%7C%20%E2%89%A52.0-orange.svg">
 <a href="https://pypi.org/project/scepter/"><img src="https://badge.fury.io/py/scepter.svg"></a>
 <a href="https://github.com/modelscope/scepter/blob/main/LICENSE"><img src="https://img.shields.io/github/license/modelscope/scepter"></a>
 <a href="https://github.com/modelscope/scepter/"><img src="https://img.shields.io/badge/scepter-Build from source-6FEBB9.svg"></a>
 </p>
 
 ##  Table of Contents
-- [Introduction](#-introduction)
 - [News](#-news)
+- [Introduction](#-introduction)
 - [Installation](#%EF%B8%8F-installation)
 - [Getting Started](#-getting-started)
 - [SCEPTER Studio](#%EF%B8%8F-scepter-studio)
 - [Gallery](#%EF%B8%8F-gallery)
 - [Features](#-features)
 - [Learn More](#-learn-more)
 - [License](#license)
+- [Acknowledgement](#acknowledgement)
+
+##  News
+- [2024.03]: We optimize the training UI and checkpoint management. New [LAR-Gen](https://arxiv.org/abs/2403.19534) model has been added on SCEPTER Studio, supporting `zoom-out`, `virtual try on`, `inpainting`.
+- [2024.02]: We release new SCEdit controllable image synthesis models for SD v2.1 and SD XL. Multiple strategies applied to accelerate inference time for SCEPTER Studio.
+- [2024.01]: We release **SCEPTER Studio**, an integrated toolkit for data management, model training and inference based on [Gradio](https://www.gradio.app/).
+- [2024.01]: [SCEdit](https://arxiv.org/abs/2312.11392) support controllable image synthesis for training and inference.
+- [2023.12]: We propose [SCEdit](https://arxiv.org/abs/2312.11392), an efficient and controllable generation framework.
+- [2023.12]: We release [SCEPTER](https://github.com/modelscope/scepter/) library.
 
 ##  Introduction
 
 SCEPTER is an open-source code repository dedicated to generative training, fine-tuning, and inference, encompassing a suite of downstream tasks such as image generation, transfer, editing. It integrates popular community-driven implementations as well as proprietary methods by Tongyi Lab of Alibaba Group, offering a comprehensive toolkit for researchers and practitioners in the field of AIGC. This versatile library is designed to facilitate innovation and accelerate development in the rapidly evolving domain of generative models.
 
 Main Feature:
 
 - Task:
   - Text-to-image generation
   - Controllable image synthesis
-  - Image editing (TODO)
+  - Image editing
 - Training / Inference:
   - Distribute: DDP / FSDP / FairScale / Xformers
   - File system: Local / Http / OSS / Modelscope
 - Deploy:
   - Data management
   - Training
   - Inference
 
 Currently supported approaches (and counting):
 
 1. SD Series: [Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5) / [Stable Diffusion v2.1](https://huggingface.co/runwayml/stable-diffusion-v1-5) / [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
-2. SCEdit: [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392)  [![Arxiv link](https://img.shields.io/static/v1?label=arXiv&message=SCEdit&color=red&logo=arxiv)](https://arxiv.org/abs/2312.11392) [![Page link](https://img.shields.io/badge/Page-SCEdit-Gree)](https://scedit.github.io/)
-3. Res-Tuning(TODO): [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859) [![Arxiv link](https://img.shields.io/static/v1?label=arXiv&message=ResTuning&color=red&logo=arxiv)](https://arxiv.org/abs/2310.19859) [![Page link](https://img.shields.io/badge/Page-ResTuning-Gree)](https://res-tuning.github.io/)
-
-##  News
-- [2024.02]: We release new SCEdit controllable image synthesis models for SD v2.1 and SD XL. Multiple strategies applied to accelerate inference time for SCEPTER Studio.
-- [2024.01]: We release **SCEPTER Studio**, an integrated toolkit for data management, model training and inference based on [Gradio](https://www.gradio.app/).
-- [2024.01]: [SCEdit](https://arxiv.org/abs/2312.11392) support controllable image synthesis for training and inference.
-- [2023.12]: We propose [SCEdit](https://arxiv.org/abs/2312.11392), an efficient and controllable generation framework.
-- [2023.12]: We release [SCEPTER](https://github.com/modelscope/scepter/) library.
+2. SCEdit(CVPR2024): [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392)  [![Arxiv link](https://img.shields.io/static/v1?label=arXiv&message=SCEdit&color=red&logo=arxiv)](https://arxiv.org/abs/2312.11392) [![Page link](https://img.shields.io/badge/Page-SCEdit-Gree)](https://scedit.github.io/)
+3. Res-Tuning(NeurIPS2023 TODO): [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859) [![Arxiv link](https://img.shields.io/static/v1?label=arXiv&message=ResTuning&color=red&logo=arxiv)](https://arxiv.org/abs/2310.19859) [![Page link](https://img.shields.io/badge/Page-ResTuning-Gree)](https://res-tuning.github.io/)
+4. LAR-Gen: [Locate, Assign, Refine: Taming Customized Image Inpainting with Text-Subject Guidance](https://arxiv.org/abs/2403.19534)  [![Arxiv link](https://img.shields.io/static/v1?label=arXiv&message=LARGen&color=red&logo=arxiv)](https://arxiv.org/abs/2403.19534) [![Page link](https://img.shields.io/badge/Page-LARGen-Gree)](https://ali-vilab.github.io/largen-page/)
 
 ##  Installation
 
 - Create new environment
 
 ```shell
 conda env create -f environment.yaml
@@ -120,15 +127,15 @@
 For the data format used by SCEPTER Studio, please refer to [3D_example_csv.zip](https://modelscope.cn/api/v1/models/damo/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip).
 
 #### TXT Format
 
 To facilitate starting training in command-line mode, you can use a dataset in text format, please refer to [3D_example_txt.zip](https://modelscope.cn/api/v1/models/damo/scepter/repo?Revision=master&FilePath=datasets/3D_example_txt.zip)
 
 ```shell
-mkdir -p cache/dataset/ && wget 'https://modelscope.cn/api/v1/models/damo/scepter_scedit/repo?Revision=master&FilePath=dataset/3D_example_txt.zip' -O cache/dataset/3D_example_txt.zip && unzip cache/dataset/3D_example_txt.zip -d cache/dataset/ && rm cache/dataset/3D_example_txt.zip
+mkdir -p cache/datasets/ && wget 'https://modelscope.cn/api/v1/models/damo/scepter_scedit/repo?Revision=master&FilePath=dataset/3D_example_txt.zip' -O cache/datasets/3D_example_txt.zip && unzip cache/datasets/3D_example_txt.zip -d cache/datasets/ && rm cache/datasets/3D_example_txt.zip
 ```
 
 ### Training
 
 We provide a framework for training and inference, so the script below is just for illustration purposes. To achieve better results, you can modify the corresponding parameters as needed.
 
 #### Text-to-Image Generation
@@ -191,14 +198,22 @@
 
 - SCEdit
 ```python
 python scepter/tools/run_inference.py --cfg scepter/methods/scedit/ctr/sd21_768_sce_ctr_canny.yaml --num_samples 1 --prompt 'a single flower is shown in front of a tree' --save_folder 'test_flower_canny' --image_size 768 --task control --image 'asset/images/flower.jpg' --control_mode canny --pretrained_model ms://damo/scepter_scedit@controllable_model/SD2.1/canny_control/0_SwiftSCETuning/pytorch_model.bin   # canny
 python scepter/tools/run_inference.py --cfg scepter/methods/scedit/ctr/sd21_768_sce_ctr_pose.yaml --num_samples 1 --prompt 'super mario' --save_folder 'test_mario_pose' --image_size 768 --task control --image 'asset/images/pose_source.png' --control_mode source --pretrained_model ms://damo/scepter_scedit@controllable_model/SD2.1/pose_control/0_SwiftSCETuning/pytorch_model.bin   # pose
 ```
 
+### Customize Modules
+Refer to `example`, build the modules of your task in `example/{task}`.
+```python
+cd example/classifier
+python run.py --cfg classifier.yaml
+```
+
+
 ##  SCEPTER Studio
 
 ### Launch
 
 To fully experience **SCEPTER Studio**, you can launch the following command line:
 
 ```shell
@@ -207,24 +222,102 @@
 ```
 or run after clone repo code
 ```shell
 git clone https://github.com/modelscope/scepter.git
 PYTHONPATH=. python scepter/tools/webui.py --cfg scepter/methods/studio/scepter_ui.yaml
 ```
 
-The startup of **SCEPTER Studio** eliminates the need for manual downloading and organizing of models; it will automatically load the corresponding models and store them in a local directory. 
-Depending on the network and hardware situation, the initial startup usually requires 15-60 minutes, primarily involving the download and processing of SDv1.5, SDv2.1, and SDXL models. 
+The startup of **SCEPTER Studio** eliminates the need for manual downloading and organizing of models; it will automatically load the corresponding models and store them in a local directory.
+Depending on the network and hardware situation, the initial startup usually requires 15-60 minutes, primarily involving the download and processing of SDv1.5, SDv2.1, and SDXL models.
 Therefore, subsequent startups will become much faster (about one minute) as downloading is no longer required.
 
+* LAR-Gen: we release `zoom-out`, `virtual try on`, `inpainting(text guided)`, `inpainting(text + reference image guided)` image editing capabilities.
+Please note that the **Data Preprocess** button must be clicked before clicking the **Generate** button.
+<p align="center">
+<img src="https://raw.githubusercontent.com/ali-vilab/largen-page/main/public/images/largen.gif">
+</p>
+
 ### Modelscope Studio
 
 We deploy a work studio on Modelscope that includes only the inference tab, please refer to [ms_scepter_studio](https://www.modelscope.cn/studios/damo/scepter_studio/summary)
 
 ##  Gallery
 
+### LAR-Gen: Zoom Out
+<table>
+  <tr>
+    <td><strong>Origin Image</strong><br>Prompt: a temple on fire</td>
+    <td><strong>Zoom-Out</strong><br>CenterAround:0.75</td>
+    <td><strong>Zoom-Out</strong><br>CenterAround:0.75</td>
+    <td><strong>Zoom-Out</strong><br>CenterAround:0.75</td>
+    <td><strong>Zoom-Out</strong><br>CenterAround:0.75</td>
+  </tr>
+  <tr>
+    <td><img src="asset/images/zoom_out/ex1_scene_im.jpg" width="240"></td>
+    <td><img src="asset/images/zoom_out/ex1_zoom_out1.jpg" width="240"></td>
+    <td><img src="./asset/images/zoom_out/ex1_zoom_out2.jpg" width="240"></td>
+    <td><img src="./asset/images/zoom_out/ex1_zoom_out3.jpg" width="240"></td>
+    <td><img src="./asset/images/zoom_out/ex1_zoom_out4.jpg" width="240"></td>
+  </tr>
+</table>
+
+### LAR-Gen: Virtual Try-on
+<table>
+  <tr>
+    <td><strong>Model Image</strong></td>
+    <td><strong>Model Mask</strong></td>
+    <td><strong>Clothing Image</strong></td>
+    <td><strong>Clothing Mask</strong></td>
+    <td><strong>Try-on Output</strong></td>
+  </tr>
+  <tr>
+    <td><img src="asset/images/virtual_try_on/model.jpg" width="240"></td>
+    <td><img src="asset/images/virtual_try_on/ex2_scene_mask.jpg" width="240"></td>
+    <td><img src="asset/images/virtual_try_on/tshirt.jpg" width="240"></td>
+    <td><img src="asset/images/virtual_try_on/ex2_subject_mask.jpg" width="240"></td>
+    <td><img src="asset/images/virtual_try_on/try_on_out.jpg" width="240"></td>
+  </tr>
+</table>
+
+### LAR-Gen: Inpainting (Text guided)
+<table>
+  <tr>
+    <td><strong>Origin Image</strong><br>Prompt: a blue and white porcelain</td>
+    <td><strong>Inpainting Mask1</strong></td>
+    <td><strong>Inpainting Output1</strong></td>
+    <td><strong>Inpainting Mask2</strong><br>Prompt: a clock</td>
+    <td><strong>Inpainting Output2</strong></td>
+  </tr>
+  <tr>
+    <td><img src="asset/images/inpainting_text/ex3_scene_im.jpg" width="240"></td>
+    <td><img src="asset/images/inpainting_text/ex3_scene_mask.jpg" width="240"></td>
+    <td><img src="asset/images/inpainting_text/inpainting_text.jpg" width="240"></td>
+    <td><img src="asset/images/inpainting_text/ex3_scene_mask2.jpg" width="240"></td>
+    <td><img src="asset/images/inpainting_text/inpainting_text2.jpg" width="240"></td>
+  </tr>
+</table>
+
+### LAR-Gen: Inpainting (Text and Subject guided)
+<table>
+  <tr>
+    <td><strong>Origin Image</strong><br>Prompt: a dog wearing sunglasses</td>
+    <td><strong>Origin Mask</strong></td>
+    <td><strong>Reference Image</strong></td>
+    <td><strong>Reference Mask</strong></td>
+    <td><strong>Inpainting Output</strong></td>
+  </tr>
+  <tr>
+    <td><img src="asset/images/inpainting_text_ref/ex4_scene_im.jpg" width="240"></td>
+    <td><img src="asset/images/inpainting_text_ref/ex4_scene_mask.jpg" width="240"></td>
+    <td><img src="asset/images/inpainting_text_ref/ex4_subject_im.jpg" width="240"></td>
+    <td><img src="asset/images/inpainting_text_ref/ex4_subject_mask.jpg" width="240"></td>
+    <td><img src="asset/images/inpainting_text_ref/inpainting_text_ref.jpg" width="240"></td>
+  </tr>
+</table>
+
 ### Dragon Year Special: Dragon Tuner
 
 <table>
   <tr>
     <td><strong>Gold Dragon Tuner</strong></td>
     <td><strong>Sloppy Dragon Tuner</strong></td>
     <td><strong>Red Dragon Tuner</strong><br> + Papercraft Mantra</td>
@@ -270,38 +363,60 @@
 
 | **Model** | **Canny** | **HED** | **Depth** | **Pose** | **Color** |
 |:---------:|:---------:|:-------:|:---------:|:--------:|:---------:|
 |   SD 1.5  |          |        |          |         |          |
 |   SD 2.1  |          |        |          |         |          |
 |   SD XL   |          |        |          |         |          |
 
+### Image Editing
+- LAR-Gen
+
+| **Model** | **Locate** | **Assign** | **Refine** |
+|:---------:|:----------:|:----------:|:----------:|
+|   SD XL   |          |           |          |
+
 ### Model URL
 
 -  indicates support for both training and inference.
 -  denotes that the model has been published.
+-  denotes that the module has not been integrated currently.
 - More models will be released in the future.
 
 | Model  | URL                                                                                                                                            |
 |--------|------------------------------------------------------------------------------------------------------------------------------------------------|
-| SCEdit | [ModelScope](https://modelscope.cn/models/damo/scepter_scedit/summary) [HuggingFace](https://huggingface.co/scepter-studio/scepter_scedit) |
+| SCEdit | [ModelScope](https://modelscope.cn/models/iic/scepter_scedit/summary) [HuggingFace](https://huggingface.co/scepter-studio/scepter_scedit) |
+| LAR-Gen | [ModelScope](https://www.modelscope.cn/models/iic/LARGEN/summary) |
 
 PS: Scripts running within the SCEPTER framework will automatically fetch and load models based on the required dependency files, eliminating the need for manual downloads.
 
 
 ##  Learn More
 
-- [Alibaba TongYi Vision Intelligence Lab](https://github.com/damo-vilab)
+- [Alibaba TongYi Vision Intelligence Lab](https://github.com/ali-vilab)
 
   Discover more about open-source projects on image generation, video generation, and editing tasks.
 
 - [ModelScope library](https://github.com/modelscope/modelscope/)
 
   ModelScope Library is the model library of ModelScope project, which contains a large number of popular models.
 
 - [SWIFT library](https://github.com/modelscope/swift/)
 
   SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning) is an extensible framwork designed to faciliate lightweight model fine-tuning and inference.
 
+## BibTeX
+If our work is useful for your research, please consider citing:
+```bibtex
+@misc{scepter,
+    title = {SCEPTER, https://github.com/modelscope/scepter},
+    author = {SCEPTER},
+    year = {2023}
+}
+```
+
 
 ## License
 
 This project is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
+
+## Acknowledgement
+Thanks to [Stability-AI](https://github.com/Stability-AI), [SWIFT library](https://github.com/modelscope/swift/) and [Fooocus](https://github.com/lllyasviel/Fooocus) for their awesome work.
```

### html2text {}

```diff
@@ -1,87 +1,97 @@
-Metadata-Version: 2.1 Name: scepter Version: 0.0.3.post1 Home-page: Author:
-Tongyi Lab Author-email: Keywords: compute vision,framework,generation,image
-edition. Classifier: Programming Language :: Python :: 3.8 Classifier:
-Programming Language :: Python :: 3.9 Classifier: Programming Language ::
-Python :: 3.10 Classifier: Programming Language :: Python :: 3.11 Classifier:
-License :: OSI Approved :: Apache Software License Classifier: Operating System
-:: OS Independent Description-Content-Type: text/markdown License-File: LICENSE
-Requires-Dist: einops Requires-Dist: modelscope Requires-Dist: ms-swift
-(>=1.5.2) Requires-Dist: numpy Requires-Dist: open-clip-torch Requires-Dist:
-opencv-python Requires-Dist: opencv-transforms (>=0.0.6) Requires-Dist: oss2
-(>=2.15.0) Requires-Dist: pyyaml (>=5.3.1) Requires-Dist: scikit-image
+Metadata-Version: 2.1 Name: scepter Version: 0.0.4 Home-page: Author: Tongyi
+Lab Author-email: Keywords: compute vision,framework,generation,image edition.
+Classifier: Programming Language :: Python :: 3.8 Classifier: Programming
+Language :: Python :: 3.9 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11 Classifier: License :: OSI
+Approved :: Apache Software License Classifier: Operating System :: OS
+Independent Description-Content-Type: text/markdown License-File: LICENSE
+Requires-Dist: albumentations Requires-Dist: bezier Requires-Dist: einops
+Requires-Dist: modelscope Requires-Dist: ms-swift (>=1.5.2) Requires-Dist:
+numpy Requires-Dist: open-clip-torch Requires-Dist: opencv-python Requires-
+Dist: opencv-transforms (>=0.0.6) Requires-Dist: oss2 (>=2.15.0) Requires-Dist:
+pycocotools Requires-Dist: pyyaml (>=5.3.1) Requires-Dist: scikit-image
 Requires-Dist: torchsde Requires-Dist: transformers Requires-Dist: gradio
-(<4.0.0,>=3.47.1) Requires-Dist: imagehash
+(<4.0.0,>=3.47.1) Requires-Dist: imagehash Requires-Dist: psutil
                            ************ ????SSCCEEPPTTEERR ************
       [https://img.shields.io/badge/python-%E2%89%A53.8-5be.svg][https://
   img.shields.io/badge/pytorch-%E2%89%A51.12%20%7C%20%E2%89%A52.0-orange.svg]
  _[_h_t_t_p_s_:_/_/_b_a_d_g_e_._f_u_r_y_._i_o_/_p_y_/_s_c_e_p_t_e_r_._s_v_g_]_[_h_t_t_p_s_:_/_/_i_m_g_._s_h_i_e_l_d_s_._i_o_/_g_i_t_h_u_b_/_l_i_c_e_n_s_e_/
   _m_o_d_e_l_s_c_o_p_e_/_s_c_e_p_t_e_r_]_[_h_t_t_p_s_:_/_/_i_m_g_._s_h_i_e_l_d_s_._i_o_/_b_a_d_g_e_/_s_c_e_p_t_e_r_-_B_u_i_l_d_ _f_r_o_m_ _s_o_u_r_c_e_-
                                   _6_F_E_B_B_9_._s_v_g_]
-##  Table of Contents - [Introduction](#-introduction) - [News](#-news) -
+##  Table of Contents - [News](#-news) - [Introduction](#-introduction) -
 [Installation](#%EF%B8%8F-installation) - [Getting Started](#-getting-started)
 - [SCEPTER Studio](#%EF%B8%8F-scepter-studio) - [Gallery](#%EF%B8%8F-gallery) -
-[Features](#-features) - [Learn More](#-learn-more) - [License](#license) ##
+[Features](#-features) - [Learn More](#-learn-more) - [License](#license) -
+[Acknowledgement](#acknowledgement) ##  News - [2024.03]: We optimize the
+training UI and checkpoint management. New [LAR-Gen](https://arxiv.org/abs/
+2403.19534) model has been added on SCEPTER Studio, supporting `zoom-out`,
+`virtual try on`, `inpainting`. - [2024.02]: We release new SCEdit controllable
+image synthesis models for SD v2.1 and SD XL. Multiple strategies applied to
+accelerate inference time for SCEPTER Studio. - [2024.01]: We release **SCEPTER
+Studio**, an integrated toolkit for data management, model training and
+inference based on [Gradio](https://www.gradio.app/). - [2024.01]: [SCEdit]
+(https://arxiv.org/abs/2312.11392) support controllable image synthesis for
+training and inference. - [2023.12]: We propose [SCEdit](https://arxiv.org/abs/
+2312.11392), an efficient and controllable generation framework. - [2023.12]:
+We release [SCEPTER](https://github.com/modelscope/scepter/) library. ##
  Introduction SCEPTER is an open-source code repository dedicated to
 generative training, fine-tuning, and inference, encompassing a suite of
 downstream tasks such as image generation, transfer, editing. It integrates
 popular community-driven implementations as well as proprietary methods by
 Tongyi Lab of Alibaba Group, offering a comprehensive toolkit for researchers
 and practitioners in the field of AIGC. This versatile library is designed to
 facilitate innovation and accelerate development in the rapidly evolving domain
 of generative models. Main Feature: - Task: - Text-to-image generation -
-Controllable image synthesis - Image editing (TODO) - Training / Inference: -
+Controllable image synthesis - Image editing - Training / Inference: -
 Distribute: DDP / FSDP / FairScale / Xformers - File system: Local / Http / OSS
 / Modelscope - Deploy: - Data management - Training - Inference Currently
 supported approaches (and counting): 1. SD Series: [Stable Diffusion v1.5]
 (https://huggingface.co/runwayml/stable-diffusion-v1-5) / [Stable Diffusion
 v2.1](https://huggingface.co/runwayml/stable-diffusion-v1-5) / [Stable
 Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
-2. SCEdit: [SCEdit: Efficient and Controllable Image Diffusion Generation via
-Skip Connection Editing](https://arxiv.org/abs/2312.11392) [![Arxiv link]
-(https://img.shields.io/static/
+2. SCEdit(CVPR2024): [SCEdit: Efficient and Controllable Image Diffusion
+Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392) [!
+[Arxiv link](https://img.shields.io/static/
 v1?label=arXiv&message=SCEdit&color=red&logo=arxiv)](https://arxiv.org/abs/
 2312.11392) [![Page link](https://img.shields.io/badge/Page-SCEdit-Gree)]
-(https://scedit.github.io/) 3. Res-Tuning(TODO): [Res-Tuning: A Flexible and
-Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/
-abs/2310.19859) [![Arxiv link](https://img.shields.io/static/
-v1?label=arXiv&message=ResTuning&color=red&logo=arxiv)](https://arxiv.org/abs/
-2310.19859) [![Page link](https://img.shields.io/badge/Page-ResTuning-Gree)]
-(https://res-tuning.github.io/) ##  News - [2024.02]: We release new SCEdit
-controllable image synthesis models for SD v2.1 and SD XL. Multiple strategies
-applied to accelerate inference time for SCEPTER Studio. - [2024.01]: We
-release **SCEPTER Studio**, an integrated toolkit for data management, model
-training and inference based on [Gradio](https://www.gradio.app/). - [2024.01]:
-[SCEdit](https://arxiv.org/abs/2312.11392) support controllable image synthesis
-for training and inference. - [2023.12]: We propose [SCEdit](https://arxiv.org/
-abs/2312.11392), an efficient and controllable generation framework. -
-[2023.12]: We release [SCEPTER](https://github.com/modelscope/scepter/
-) library. ##  Installation - Create new environment ```shell conda env
-create -f environment.yaml conda activate scepter ``` - We recommend installing
-the specific version of PyTorch and accelerate toolbox [xFormers](https://
-pypi.org/project/xformers/). You can install these recommended version by pip:
-```shell pip install -r requirements/recommended.txt ``` - Install SCEPTER by
-the `pip` command: ```shell pip install scepter ``` ##  Getting Started ###
-Dataset #### Modelscope Format We use a [custom-stylized dataset](https://
-modelscope.cn/datasets/damo/style_custom_dataset/summary), which included
-classes 3D, anime, flat illustration, oil painting, sketch, and watercolor,
-each with 30 image-text pairs. ```python # pip install modelscope from
-modelscope.msdatasets import MsDataset ms_train_dataset = MsDataset.load
-('style_custom_dataset', namespace='damo', subset_name='3D',
-split='train_short') print(next(iter(ms_train_dataset))) ``` #### CSV Format
-For the data format used by SCEPTER Studio, please refer to
-[3D_example_csv.zip](https://modelscope.cn/api/v1/models/damo/scepter/
-repo?Revision=master&FilePath=datasets/3D_example_csv.zip). #### TXT Format To
-facilitate starting training in command-line mode, you can use a dataset in
-text format, please refer to [3D_example_txt.zip](https://modelscope.cn/api/v1/
-models/damo/scepter/repo?Revision=master&FilePath=datasets/3D_example_txt.zip)
-```shell mkdir -p cache/dataset/ && wget 'https://modelscope.cn/api/v1/models/
-damo/scepter_scedit/repo?Revision=master&FilePath=dataset/3D_example_txt.zip' -
-O cache/dataset/3D_example_txt.zip && unzip cache/dataset/3D_example_txt.zip -
-d cache/dataset/ && rm cache/dataset/3D_example_txt.zip ``` ### Training We
+(https://scedit.github.io/) 3. Res-Tuning(NeurIPS2023 TODO): [Res-Tuning: A
+Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone]
+(https://arxiv.org/abs/2310.19859) [![Arxiv link](https://img.shields.io/
+static/v1?label=arXiv&message=ResTuning&color=red&logo=arxiv)](https://
+arxiv.org/abs/2310.19859) [![Page link](https://img.shields.io/badge/Page-
+ResTuning-Gree)](https://res-tuning.github.io/) 4. LAR-Gen: [Locate, Assign,
+Refine: Taming Customized Image Inpainting with Text-Subject Guidance](https://
+arxiv.org/abs/2403.19534) [![Arxiv link](https://img.shields.io/static/
+v1?label=arXiv&message=LARGen&color=red&logo=arxiv)](https://arxiv.org/abs/
+2403.19534) [![Page link](https://img.shields.io/badge/Page-LARGen-Gree)]
+(https://ali-vilab.github.io/largen-page/) ##  Installation - Create new
+environment ```shell conda env create -f environment.yaml conda activate
+scepter ``` - We recommend installing the specific version of PyTorch and
+accelerate toolbox [xFormers](https://pypi.org/project/xformers/). You can
+install these recommended version by pip: ```shell pip install -r requirements/
+recommended.txt ``` - Install SCEPTER by the `pip` command: ```shell pip
+install scepter ``` ##  Getting Started ### Dataset #### Modelscope Format
+We use a [custom-stylized dataset](https://modelscope.cn/datasets/damo/
+style_custom_dataset/summary), which included classes 3D, anime, flat
+illustration, oil painting, sketch, and watercolor, each with 30 image-text
+pairs. ```python # pip install modelscope from modelscope.msdatasets import
+MsDataset ms_train_dataset = MsDataset.load('style_custom_dataset',
+namespace='damo', subset_name='3D', split='train_short') print(next(iter
+(ms_train_dataset))) ``` #### CSV Format For the data format used by SCEPTER
+Studio, please refer to [3D_example_csv.zip](https://modelscope.cn/api/v1/
+models/damo/scepter/repo?Revision=master&FilePath=datasets/3D_example_csv.zip).
+#### TXT Format To facilitate starting training in command-line mode, you can
+use a dataset in text format, please refer to [3D_example_txt.zip](https://
+modelscope.cn/api/v1/models/damo/scepter/
+repo?Revision=master&FilePath=datasets/3D_example_txt.zip) ```shell mkdir -
+p cache/datasets/ && wget 'https://modelscope.cn/api/v1/models/damo/
+scepter_scedit/repo?Revision=master&FilePath=dataset/3D_example_txt.zip' -
+O cache/datasets/3D_example_txt.zip && unzip cache/datasets/3D_example_txt.zip
+-d cache/datasets/ && rm cache/datasets/3D_example_txt.zip ``` ### Training We
 provide a framework for training and inference, so the script below is just for
 illustration purposes. To achieve better results, you can modify the
 corresponding parameters as needed. #### Text-to-Image Generation - SCEdit
 ```python python scepter/tools/run_train.py --cfg scepter/methods/scedit/t2i/
 sd15_512_sce_t2i.yaml # SD v1.5 python scepter/tools/run_train.py --cfg
 scepter/methods/scedit/t2i/sd21_768_sce_t2i.yaml # SD v2.1 python scepter/
 tools/run_train.py --cfg scepter/methods/scedit/t2i/sdxl_1024_sce_t2i.yaml # SD
@@ -125,29 +135,64 @@
 canny --pretrained_model ms://damo/scepter_scedit@controllable_model/SD2.1/
 canny_control/0_SwiftSCETuning/pytorch_model.bin # canny python scepter/tools/
 run_inference.py --cfg scepter/methods/scedit/ctr/sd21_768_sce_ctr_pose.yaml --
 num_samples 1 --prompt 'super mario' --save_folder 'test_mario_pose' --
 image_size 768 --task control --image 'asset/images/pose_source.png' --
 control_mode source --pretrained_model ms://damo/
 scepter_scedit@controllable_model/SD2.1/pose_control/0_SwiftSCETuning/
-pytorch_model.bin # pose ``` ##  SCEPTER Studio ### Launch To fully
-experience **SCEPTER Studio**, you can launch the following command line:
+pytorch_model.bin # pose ``` ### Customize Modules Refer to `example`, build
+the modules of your task in `example/{task}`. ```python cd example/classifier
+python run.py --cfg classifier.yaml ``` ##  SCEPTER Studio ### Launch To
+fully experience **SCEPTER Studio**, you can launch the following command line:
 ```shell pip install scepter python -m scepter.tools.webui ``` or run after
 clone repo code ```shell git clone https://github.com/modelscope/scepter.git
 PYTHONPATH=. python scepter/tools/webui.py --cfg scepter/methods/studio/
 scepter_ui.yaml ``` The startup of **SCEPTER Studio** eliminates the need for
 manual downloading and organizing of models; it will automatically load the
 corresponding models and store them in a local directory. Depending on the
 network and hardware situation, the initial startup usually requires 15-60
 minutes, primarily involving the download and processing of SDv1.5, SDv2.1, and
 SDXL models. Therefore, subsequent startups will become much faster (about one
-minute) as downloading is no longer required. ### Modelscope Studio We deploy a
-work studio on Modelscope that includes only the inference tab, please refer to
-[ms_scepter_studio](https://www.modelscope.cn/studios/damo/scepter_studio/
-summary) ##  Gallery ### Dragon Year Special: Dragon Tuner
+minute) as downloading is no longer required. * LAR-Gen: we release `zoom-out`,
+`virtual try on`, `inpainting(text guided)`, `inpainting(text + reference image
+guided)` image editing capabilities. Please note that the **Data Preprocess**
+button must be clicked before clicking the **Generate** button.
+ [https://raw.githubusercontent.com/ali-vilab/largen-page/main/public/images/
+                                  largen.gif]
+### Modelscope Studio We deploy a work studio on Modelscope that includes only
+the inference tab, please refer to [ms_scepter_studio](https://
+www.modelscope.cn/studios/damo/scepter_studio/summary) ##  Gallery ###
+LAR-Gen: Zoom Out
+OOrriiggiinn IImmaaggee      ZZoooomm--OOuutt           ZZoooomm--OOuutt           ZZoooomm--OOuutt           ZZoooomm--OOuutt
+Prompt: a temple  CenterAround:0.75  CenterAround:0.75  CenterAround:0.75  CenterAround:0.75
+on fire
+[asset/images/    [asset/images/     [./asset/images/   [./asset/images/   [./asset/images/
+zoom_out/         zoom_out/          zoom_out/          zoom_out/          zoom_out/
+ex1_scene_im.jpg] ex1_zoom_out1.jpg] ex1_zoom_out2.jpg] ex1_zoom_out3.jpg] ex1_zoom_out4.jpg]
+### LAR-Gen: Virtual Try-on
+MMooddeell IImmaaggee     MMooddeell MMaasskk          CCllootthhiinngg IImmaaggee  CCllootthhiinngg MMaasskk         TTrryy--oonn OOuuttppuutt
+[asset/images/  [asset/images/      [asset/images/  [asset/images/        [asset/images/
+virtual_try_on/ virtual_try_on/     virtual_try_on/ virtual_try_on/       virtual_try_on/
+model.jpg]      ex2_scene_mask.jpg] tshirt.jpg]     ex2_subject_mask.jpg] try_on_out.jpg]
+### LAR-Gen: Inpainting (Text guided)
+OOrriiggiinn IImmaaggee
+Prompt: a blue    IInnppaaiinnttiinngg MMaasskk11    IInnppaaiinnttiinngg OOuuttppuutt11   IInnppaaiinnttiinngg MMaasskk22     IInnppaaiinnttiinngg OOuuttppuutt22
+and white                                                  Prompt: a clock
+porcelain
+[asset/images/    [asset/images/      [asset/images/       [asset/images/       [asset/images/
+inpainting_text/  inpainting_text/    inpainting_text/     inpainting_text/     inpainting_text/
+ex3_scene_im.jpg] ex3_scene_mask.jpg] inpainting_text.jpg] ex3_scene_mask2.jpg] inpainting_text2.jpg]
+### LAR-Gen: Inpainting (Text and Subject guided)
+OOrriiggiinn IImmaaggee
+Prompt: a dog        OOrriiggiinn MMaasskk          RReeffeerreennccee IImmaaggee      RReeffeerreennccee MMaasskk        IInnppaaiinnttiinngg OOuuttppuutt
+wearing sunglasses
+[asset/images/       [asset/images/       [asset/images/       [asset/images/        [asset/images/
+inpainting_text_ref/ inpainting_text_ref/ inpainting_text_ref/ inpainting_text_ref/  inpainting_text_ref/
+ex4_scene_im.jpg]    ex4_scene_mask.jpg]  ex4_subject_im.jpg]  ex4_subject_mask.jpg] inpainting_text_ref.jpg]
+### Dragon Year Special: Dragon Tuner
 GGoolldd DDrraaggoonn TTuunneerr                SSllooppppyy DDrraaggoonn TTuunneerr                RReedd DDrraaggoonn TTuunneerr                              AAzzuurree DDrraaggoonn TTuunneerr
                                                                     + Papercraft Mantra                           + Pose Control
 [https://github.com/hanzhn/      [https://github.com/hanzhn/datas/  [https://github.com/hanzhn/datas/blob/main/   [https://github.com/
 datas/blob/main/scepter/readme/  blob/main/scepter/readme/          scepter/readme/                               hanzhn/datas/blob/main/
 tuner_gold_dragon.jpeg?raw=true] tuner_sloppy_dragon.jpeg?raw=true] tuner_mantra_papercraft_dragon.jpeg?raw=true] scepter/readme/
                                                                                                                   tuner_pose.jpeg?raw=true]
 ### Text Effect Image
@@ -162,25 +207,35 @@
 ##  Features ### Text-to-Image Generation | **Model** | **SCEdit** |
 **Full** | **LoRA** | |:---------:|:----------:|:--------:|:--------:| | SD 1.5
 |  |  |  | | SD 2.1 |  |  |  | | SD XL |  |  |  |
 ### Controllable Image Synthesis - SCEdit | **Model** | **Canny** | **HED** |
 **Depth** | **Pose** | **Color** | |:---------:|:---------:|:-------:|:--------
 -:|:--------:|:---------:| | SD 1.5 |  |  |  |  |  | | SD 2.1 |
  |  |  |  |  | | SD XL |  |  |  |  |  |
-### Model URL -  indicates support for both training and inference. - 
-denotes that the model has been published. - More models will be released in
-the future. | Model | URL | |--------|-----------------------------------------
+### Image Editing - LAR-Gen | **Model** | **Locate** | **Assign** | **Refine**
+| |:---------:|:----------:|:----------:|:----------:| | SD XL |  |  |
+ | ### Model URL -  indicates support for both training and inference. -
+ denotes that the model has been published. -  denotes that the module
+has not been integrated currently. - More models will be released in the
+future. | Model | URL | |--------|---------------------------------------------
 -------------------------------------------------------------------------------
-------------------------| | SCEdit | [ModelScope](https://modelscope.cn/models/
-damo/scepter_scedit/summary) [HuggingFace](https://huggingface.co/scepter-
-studio/scepter_scedit) | PS: Scripts running within the SCEPTER framework will
+--------------------| | SCEdit | [ModelScope](https://modelscope.cn/models/iic/
+scepter_scedit/summary) [HuggingFace](https://huggingface.co/scepter-studio/
+scepter_scedit) | | LAR-Gen | [ModelScope](https://www.modelscope.cn/models/
+iic/LARGEN/summary) | PS: Scripts running within the SCEPTER framework will
 automatically fetch and load models based on the required dependency files,
 eliminating the need for manual downloads. ##  Learn More - [Alibaba TongYi
-Vision Intelligence Lab](https://github.com/damo-vilab) Discover more about
+Vision Intelligence Lab](https://github.com/ali-vilab) Discover more about
 open-source projects on image generation, video generation, and editing tasks.
 - [ModelScope library](https://github.com/modelscope/modelscope/) ModelScope
 Library is the model library of ModelScope project, which contains a large
 number of popular models. - [SWIFT library](https://github.com/modelscope/
 swift/) SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning) is an
 extensible framwork designed to faciliate lightweight model fine-tuning and
-inference. ## License This project is licensed under the [Apache License
-(Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE).
+inference. ## BibTeX If our work is useful for your research, please consider
+citing: ```bibtex @misc{scepter, title = {SCEPTER, https://github.com/
+modelscope/scepter}, author = {SCEPTER}, year = {2023} } ``` ## License This
+project is licensed under the [Apache License (Version 2.0)](https://
+github.com/modelscope/modelscope/blob/master/LICENSE). ## Acknowledgement
+Thanks to [Stability-AI](https://github.com/Stability-AI), [SWIFT library]
+(https://github.com/modelscope/swift/) and [Fooocus](https://github.com/
+lllyasviel/Fooocus) for their awesome work.
```

## Comparing `scepter-0.0.3.post1.dist-info/RECORD` & `scepter-0.0.4.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 scepter/__init__.py,sha256=EBJ3e1-bjeH1OYlPb-zCg60axF4cC5YDZX5xODEAsSc,602
-scepter/version.py,sha256=fYVHKtH2OhSlX1_vc4mkmX_CzN5CMBI6b80G901MESU,211
+scepter/version.py,sha256=0jkVIqgLD4WczOM-4OFKYa85sX0YIBjAHnyMkDyKKls,205
 scepter/methods/examples/classification/example.yaml,sha256=jyB6_Qh5wvht__v0AWLe6oAoNo_-KMOSTZi6amRpRiQ,12618
 scepter/methods/examples/generation/stable_diffusion_1.5_512.yaml,sha256=mKFddVuHkEF3Ae4MIpuPxij6_FdzQeXbfx7yOnOKHU0,5383
 scepter/methods/examples/generation/stable_diffusion_1.5_512_lora.yaml,sha256=RVteGNuYR2NEDnKsRsa2BX0i7HNa-3wlU29yqC-BylM,5568
 scepter/methods/examples/generation/stable_diffusion_2.1_512.yaml,sha256=yOJIZWIozb24_dPTIyizhm1gO21H_2GOsROvrT_VrLs,4792
 scepter/methods/examples/generation/stable_diffusion_2.1_512_lora.yaml,sha256=PWozRgfdLvA1aYowOSf71O-TOAaJUeEnfTUMDwX_3IA,4982
 scepter/methods/examples/generation/stable_diffusion_2.1_768.yaml,sha256=zdRrwKVbG2FEMg3ZlBNfo5FaGUh2EX55wNNl-TDOrZQ,4789
 scepter/methods/examples/generation/stable_diffusion_2.1_768_lora.yaml,sha256=g81cJDDgjxhNpfzVLuXcdMyTmpCPnn2dzQRzmYgqfNc,4978
@@ -19,28 +19,30 @@
 scepter/methods/scedit/t2i/sd15_512_sce_t2i.yaml,sha256=xgvV9wAAVCBvDXkrIh-EK2yNmZcrcArZlqbA3jQGfXA,5639
 scepter/methods/scedit/t2i/sd15_512_sce_t2i_swift.yaml,sha256=nKYtbEvQjXBVwHonmap_ScLomUpI4aIzdytjF0x5uXg,5611
 scepter/methods/scedit/t2i/sd21_768_sce_t2i.yaml,sha256=Q5iS01J5FxvNEVMRiXQvJum4gl2nIUt04rUCIVZAsvk,5045
 scepter/methods/scedit/t2i/sd21_768_sce_t2i_swift.yaml,sha256=YoIlyaBhr35wdtX0ocOrwot1nAdaUAcV5Dau-j33vZ8,5017
 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i.yaml,sha256=v5BUpf8-NVI4gIcuYnjTxCJC-wWYbR52Ojb9dOeHdn0,9370
 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_datatxt.yaml,sha256=jwdaR-0k2RMavb9EVKaSVEoyolxgcNnE1bswhCB7xWI,9514
 scepter/methods/scedit/t2i/sdxl_1024_sce_t2i_swift.yaml,sha256=x16N0e4xYdcNYrw8bS-8yNBqgyDhlT5F7nsf1QyCpGk,9324
-scepter/methods/studio/scepter_ui.yaml,sha256=zRFxgb6QnKSBL9o4KQ41i4HQm_otjOhGpxpdx-5JvGQ,2275
+scepter/methods/studio/scepter_ui.yaml,sha256=DiPvYhGdYwk8I-zwLCH5sZqNiv90Gr97s-cXyF0_3E4,2539
 scepter/methods/studio/extensions/controllers/official_controllers.yaml,sha256=oWSC2Iua5qFvIpk9tIjLEEZJ8kCOCIhv88BqMc1obXQ,1707
 scepter/methods/studio/extensions/mantra_book/mantra_book.yaml,sha256=Jt4NQYlVeIksIJePEpu5CQhV3xsOy4Zr_pRIYWLAAKs,294732
 scepter/methods/studio/extensions/tuners/official_tuners.yaml,sha256=OZFERJyO35D511n_9jj-iYpP5hVabl5qaaKp6vaJ6oM,29045
 scepter/methods/studio/home/home.yaml,sha256=iVD6Hb2VlNzLEJ9OrH4GJnW70w5IPNmeBxm7_Pmw_c0,2833
-scepter/methods/studio/inference/inference.yaml,sha256=WYmBq6DA2fyFK7Sh5ydKdv2j7BtHBPsLhIrBa9khvaU,3244
+scepter/methods/studio/inference/inference.yaml,sha256=d5c7dFZySYDJvOh-bassKz2lBwCsLcWCQTdAKB7BgRQ,3317
+scepter/methods/studio/inference/largen/largen_pro.yaml,sha256=RcrjhcAF6441Tgi3MHcBS66B_HddkoMqr-Q78IfuPpM,10894
 scepter/methods/studio/inference/sdxl/sdxl1.0_pro.yaml,sha256=J-y_qtPqLUULf1mvoylXR4hcpZDH7tGWrAc4Fyr4CFE,10136
 scepter/methods/studio/inference/stable_diffusion/sd15_pro.yaml,sha256=vTbtprC8Dv1UmV4QEempr11orWIxUAauPZaAM5Mu7Kk,2880
 scepter/methods/studio/inference/stable_diffusion/sd21_pro.yaml,sha256=VCd36LyIF2Ye5wEWei9m8OH1z1IRmcR9VMnva7NfQsY,2788
 scepter/methods/studio/preprocess/preprocess.yaml,sha256=N4FewmrR3YR7Q5p79lV9BkmJm_0pf8DMYc5k-78uCCU,148
-scepter/methods/studio/self_train/self_train.yaml,sha256=z20TVocHC__XgNEIUKpd-gOKGLV8TMjkz_mQ1fnDFu0,188
-scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml,sha256=GdD2EgiFkefEoxUPCTCZ9vcqv96T8WvXTV0ndRuOHBk,27890
-scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml,sha256=GlI3Huz1qZ2dXHvJ6PdKHoD7BVyzkGmlq8Kt2UuRF28,7598
-scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml,sha256=IPgOTZr9MuWjYjGlFDRQhztH2vsR7m1djMvEMFkuRB4,6103
+scepter/methods/studio/self_train/self_train.yaml,sha256=gOPb1ExaykTDAbR9-X0wpsSAErwX-xx-_moP20mg4qg,519
+scepter/methods/studio/self_train/sd_xl/sdxl_pro.yaml,sha256=uXjLvQ8qidbU74LqIlncxdXo19YCimqbUREgZGfI_LY,28594
+scepter/methods/studio/self_train/stable_diffusion/sd15_pro.yaml,sha256=FqQpv0P0wQrTQZupRaMm8RYCjHlV-fAc0hOUDFqFR-E,8290
+scepter/methods/studio/self_train/stable_diffusion/sd21_pro.yaml,sha256=wujsHz7hHLgB7PDeziTD6PjOE6mTn4aQ4G7pw_--FfA,6781
+scepter/methods/studio/tuner_manager/tuner_manager.yaml,sha256=vH9Q9wPNo7RcgiNf05D1S358u-kxrTfZOCo-R04FpRw,61
 scepter/modules/__init__.py,sha256=pY0acKRZ2Q-2mGTta5uW7TtslR4ANmhennHqB1TXJQM,187
 scepter/modules/annotator/__init__.py,sha256=l5Yu-hIW4b9KJrYzbCyzliWmePxwl4_5T6-onLv7qyA,628
 scepter/modules/annotator/base_annotator.py,sha256=kWBQR7w0st9xWO_m6ud-C7ia-YxcFdA8GWSgAGXASb4,2095
 scepter/modules/annotator/canny.py,sha256=jYBdWJl4xuasm367jRgQQDATITxg_8IydxSavMDWcDw,2474
 scepter/modules/annotator/color.py,sha256=snZcvvrBt78qfYVdx18wdXp8toNBo33rdtU5v6BEq-E,2159
 scepter/modules/annotator/hed.py,sha256=1HnnsLrTv1yVkmYCUutLurX2W_uj6EatvHEAXxxEh60,6346
 scepter/modules/annotator/identity.py,sha256=jytDJMq40ELMc3BHb-FXHOc6HKzbEoLWdKzAylTmeRs,767
@@ -63,52 +65,53 @@
 scepter/modules/annotator/mlsd/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 scepter/modules/annotator/mlsd/mbv2_mlsd_large.py,sha256=SVZBTEORkbuoPO8H-NyJPnwZMuSc-IUdjtNSg9ROwMU,10166
 scepter/modules/annotator/mlsd/mbv2_mlsd_tiny.py,sha256=8ngw8s7ByEVpgli4YooQg4W3EQMGc5km1v0riGmQ8K8,9706
 scepter/modules/annotator/mlsd/utils.py,sha256=QSaGxOh269CjXM_iy3NjE9BKuc8Cf8CgdPDj_a3VAMc,25395
 scepter/modules/data/__init__.py,sha256=ybb08d8eVgj5mm7qp8Obf-wEhJrrWg6CNuYngrijO5A,125
 scepter/modules/data/dataset/__init__.py,sha256=eD4yKWUbrJdYAVGbH0r-qngQxHmm1ZrN2F4KqL6ZltQ,599
 scepter/modules/data/dataset/base_dataset.py,sha256=tFLNRpXDVW65MXGZGEJXaYoD0kRcSoXzoo6Ovow-aTA,3941
-scepter/modules/data/dataset/dataset.py,sha256=B7-0pVD4-Sta_UXSxqc2Lots6Xsy_pdvtwiIsRnJr1c,8960
+scepter/modules/data/dataset/dataset.py,sha256=_apyp8NJaU7OYdfDoxwnCEGpZC0uMUPOo9GFsizOOIc,9148
 scepter/modules/data/dataset/ms_dataset.py,sha256=fVTLHUtB2Tr0pF60qsZhmzn2E1bMAODfx32VTVvnFFw,11650
 scepter/modules/data/dataset/registry.py,sha256=0_Gp2zQnKwWqOimFuHNJ-REelQZPuB7U5smEXczK5Ew,15112
 scepter/modules/data/dataset/utils.py,sha256=Od_qzQVQc-5u4DuVqVqx8tub0p2nweMZe_gQOJoVvPQ,771
 scepter/modules/data/sampler/__init__.py,sha256=KPHaW0uTTANTAJV3bZeIjdx7EaXFhDc-zKXs85cmn0M,407
 scepter/modules/data/sampler/base_sampler.py,sha256=R-1vdxA7JxdbisXkPMU5WOh6-uEInVhTe1yN7L8GtcA,1002
 scepter/modules/data/sampler/registry.py,sha256=JvfZnyiJ8HVtKL-PcSZDimWoGTYqoHU6yFRxAE3ObyE,2013
 scepter/modules/data/sampler/sampler.py,sha256=5uMNHRU7qR0g8EZGakwvREigJLA3NB7X1k_41ruci14,20503
 scepter/modules/inference/__init__.py,sha256=rm0_DKvx5QCjU9R62mbhj3k4WZk-vEXyFtfG--3KVLo,151
-scepter/modules/inference/control_inference.py,sha256=Nou4v9f9FXLk6E93JBBmgJqzNH9FffFCJl_I2t8ul_E,5014
-scepter/modules/inference/diffusion_inference.py,sha256=NkL8pmbo9p7uHWgIQwWo_10KU-jbtS3wQpVNAuewGUk,30755
+scepter/modules/inference/control_inference.py,sha256=Wk-COmU961u9qWfZLOzzOTIrbN27M9aNj9Jr_CHDFEw,5117
+scepter/modules/inference/diffusion_inference.py,sha256=Lm53QdZwtEhgGbltpNb8AaClSgbgvIX4ziIih3j8rG8,31050
+scepter/modules/inference/largen_inference.py,sha256=_FmTYScNpJcrkUstEwXx668eZFr8PaUxu_uHoe4CGlg,25815
 scepter/modules/inference/tuner_inference.py,sha256=MTj3qBuX__tmrhsFDXXFbRJQiSZ0aVcmTrZ-QRzd-EM,11183
 scepter/modules/model/__init__.py,sha256=miIofw9iOkmpcOdPw_cROKJxEIgz5F4VWFelYRKuO9w,218
 scepter/modules/model/base_model.py,sha256=7W-F96cDBa-pZJw5148KsiC6QP6XaeLkZ2O-Q4A9HOo,4022
 scepter/modules/model/registry.py,sha256=SyTUwIR5nbvuwSlmGa37TiSLKgB40ZaPFl2lbfsRc7Y,1653
 scepter/modules/model/backbone/__init__.py,sha256=bDhKv-uYJ90OOA-85MBvby7BKMU_8-saoFrkiM5qpFw,202
-scepter/modules/model/backbone/autoencoder/__init__.py,sha256=3VOXZD2wnb5lI20A9Nhvnpka9L_g5bWkqaUa8G3tQJs,224
-scepter/modules/model/backbone/autoencoder/ae_module.py,sha256=lxlmjoj3jMKCU9yIfiCCR443zLINJlz3ssooj_QSvUM,11633
+scepter/modules/model/backbone/autoencoder/__init__.py,sha256=V90piBn8-E3IuD4mKgLz2FZTP1itxAo7dDnKI36Ltr4,300
+scepter/modules/model/backbone/autoencoder/ae_module.py,sha256=9_DF90kdBYKTecoTUg8ZajSPrHPtPDsm1_aSzKZPu20,12971
 scepter/modules/model/backbone/autoencoder/ae_utils.py,sha256=qU3k6faaDj5z96j96T1IZ-ErcOwwLoj8mMHYEQDi-7w,9569
 scepter/modules/model/backbone/image/__init__.py,sha256=FWTTWYLyVcn-V5grpzS_9zMCuGLxZglKbm_vWn_q4Bk,445
 scepter/modules/model/backbone/image/mlp.py,sha256=W1jhV3gZJzXkvBFSD3qrNKU20ddcczk-1t3U5YW6kaM,2791
 scepter/modules/model/backbone/image/resnet.py,sha256=MCdmBO0GIaXNfTlV16l8ovMJ9l7up6DdP_t2RnCIuvg,3235
 scepter/modules/model/backbone/image/resnet_impl.py,sha256=rVyiJXDBEyXcHVr1a1UduXZjJ7ATawP6I9ykB8hgcTg,21928
 scepter/modules/model/backbone/image/timm_model.py,sha256=goK3NhcLfFwcFuB-escZHhfGuIi1rO3nLJ3rhBFqr8M,1625
 scepter/modules/model/backbone/image/vit_modify.py,sha256=9Blqsl2B2zelvso65IaR_s227-QGVVxvAxxcUmVNmko,8783
 scepter/modules/model/backbone/image/utils/__init__.py,sha256=HNrujjlO25vLPbER-hL1pWMqTMUadDQPLI4vPQo7Cb8,133
 scepter/modules/model/backbone/image/utils/clip.py,sha256=eJ0Xf0tScIpHepOBMGnzD_JwS-cfULQ6TC_2tXurvxk,21405
 scepter/modules/model/backbone/image/utils/simple_tokenizer.py,sha256=QtOuwCsM9a1m7bE_chV0byHtIKFxs9taBfEJyWeaBYQ,5109
 scepter/modules/model/backbone/image/utils/vit.py,sha256=JIy-121urJaLEOJEmVCd1w-L6z2x0QdPc6l0gCOH4jk,21302
-scepter/modules/model/backbone/unet/__init__.py,sha256=LmJ6XcJ2TT3R8IbTAmNhvYHvqMAWyESVAw34UbSJI0g,148
-scepter/modules/model/backbone/unet/unet_module.py,sha256=t0MuiYoGPj4iqm56dCkjI-hBx8Q7JD0spRO9dOmM00Y,38189
-scepter/modules/model/backbone/unet/unet_utils.py,sha256=hvm2_qDqOCSWjUX7kRBu1n7OfBvvtl_35_wrT7A4Yos,36274
+scepter/modules/model/backbone/unet/__init__.py,sha256=v0lr_6oJuxRkmiBlnHVA4smzYBGNVK8w5HDNClGwI2I,303
+scepter/modules/model/backbone/unet/unet_module.py,sha256=I6E7Q8QoiIF74gALAibzDFqmTFNQleZsVrsrQJLBoNo,56311
+scepter/modules/model/backbone/unet/unet_utils.py,sha256=DvsCVO-Yt5Beu8sPyGOcD8Hv0nywDwKyTgwmdFGm85E,46227
 scepter/modules/model/backbone/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/modules/model/backbone/utils/transformer.py,sha256=w4-z1JTREitVjb5gdsbEoSvSVl-E6rMf3l98ziJvYWo,2031
 scepter/modules/model/backbone/video/__init__.py,sha256=yqIWhv2LrwdSyZF3Y_NsSNM04S8QgkeJPd6TmVtoybI,318
 scepter/modules/model/backbone/video/init_helper.py,sha256=P-ljDhyItxfqhkI8elnyzR--ji2Z9tYLFbhjEhNs2jc,7081
 scepter/modules/model/backbone/video/resnet_3d.py,sha256=yRthN57nR029FhM3CHGsCeZWAO-6VDlHTdJUGGXrELo,30685
-scepter/modules/model/backbone/video/video_transformer.py,sha256=EYdfMHvkVg7zk3kYSX22eD4QHyj5PaUw3Dw8E43MsHI,13371
+scepter/modules/model/backbone/video/video_transformer.py,sha256=fOjbbSp4YkYWpkMfD-VeMkVfg4VnXsFVTaz-AVL8JfU,13371
 scepter/modules/model/backbone/video/bricks/__init__.py,sha256=RpROQPDeyFTSs4B69mn0bDpSPLiNk8MgzYiyMRoi4c4,683
 scepter/modules/model/backbone/video/bricks/base_branch.py,sha256=lEDIWEHS_jWtR8nI80tLgXNfNcq3zb86TOFUd3Lc6LY,1797
 scepter/modules/model/backbone/video/bricks/csn_branch.py,sha256=wA0ow_nHHNeKgyxByYfzr4ArgjyGqvwgYDleurflBFk,4466
 scepter/modules/model/backbone/video/bricks/non_local.py,sha256=AUBZYzAqzV-fX1F597R_h9kdifrenljOSAtVv7FcIDA,3405
 scepter/modules/model/backbone/video/bricks/r2d3d_branch.py,sha256=YAr94GFcT2gFb7Nu-GOf82rq2ZUuakakFtSz3-HmzTA,5942
 scepter/modules/model/backbone/video/bricks/r2plus1d_branch.py,sha256=U1eMUu3AFJs7M7c07aqKKqTfNQvoX0OqK3ReEs2fcyM,8046
 scepter/modules/model/backbone/video/bricks/tada_conv.py,sha256=RVwl53wHMrLBd8AWwhuIzDfJ9_I-FtX_jtMkZQDgqck,11916
@@ -116,18 +119,19 @@
 scepter/modules/model/backbone/video/bricks/visualize_3d_module.py,sha256=ipdePnDQlewntQMq-oBsZNuKXHGowVSBSwuN4qOXfc0,2530
 scepter/modules/model/backbone/video/bricks/stems/__init__.py,sha256=HI05tzKiYJ8jQ6zYBkUREbZuH8fGjt6uzUYKWy-xV0c,575
 scepter/modules/model/backbone/video/bricks/stems/base_2d_stem.py,sha256=XYppnQh0ZKPTnMd2_hk-qbc-Yyrpz3nfp_fi2_PFM-w,3006
 scepter/modules/model/backbone/video/bricks/stems/base_3d_stem.py,sha256=BM6V3Id7os4eWHWlN3Gm2905EQARPQyU29oFNWzyYkQ,3160
 scepter/modules/model/backbone/video/bricks/stems/down_sample_stem.py,sha256=dEt3Wd9bZZkt_TiMDgXGDciBdb5Ytu18Dru8agS0q48,1240
 scepter/modules/model/backbone/video/bricks/stems/embedding_stem.py,sha256=N7bxSxRUJthvJ7mG6giyBWyamFpsX7Ex94Ts7Vkg_mI,5534
 scepter/modules/model/backbone/video/bricks/stems/r2plus1d_stem.py,sha256=5V2OiEiC12HPGafQI0RRPoaAhfW6lO4kJiwzt4pjUI0,2576
-scepter/modules/model/embedder/__init__.py,sha256=NF_zUzUqvln3ON-XpBXYTNfcb0xJK4hnJERdEkAsM4E,454
+scepter/modules/model/embedder/__init__.py,sha256=SSFXSwHX__Ke1HSDN5rwu5MQvONlUKSgWGu8DmDsO-A,602
 scepter/modules/model/embedder/base_embedder.py,sha256=PVtV2kwI0lsl8B3VmEZ1PSNm_szORM-perui6obI0CI,915
-scepter/modules/model/embedder/embedder.py,sha256=rqeQeUaC7GgEBOSvTZgO8oX5yeoiCUujoEOBhrivhiI,24642
-scepter/modules/model/head/__init__.py,sha256=Gnm7S9Ta0d-GkrEeOOVL44o35HIuBwhN2nO7NPTHYw8,253
+scepter/modules/model/embedder/embedder.py,sha256=5Pn8bUn_4hdDIe0nh6yKZ_qnLCUKauuQWTVOL5pyIr0,28394
+scepter/modules/model/embedder/resampler.py,sha256=BOODSJOFuRpdNfiGEBWLCpfQf3Um6x-9q5eFyK2vZL8,5133
+scepter/modules/model/head/__init__.py,sha256=kmwV46k7Oi57osOnaumbHI5u-sM1q0NIrmA3T8GbCTw,524
 scepter/modules/model/head/classifier_head.py,sha256=6BzgQxO4JAwwSbYEWl3lG5-tMLSmPNSEGcjo5YnCWhk,11687
 scepter/modules/model/loss/__init__.py,sha256=MYaQNATlIOS-5oSRfutAW_vuSRnawABz__paEDzwS0Y,214
 scepter/modules/model/loss/base_losses.py,sha256=SKKeVVnfJHEdQZh3sgTI2f4I_3jLMa3PUtUoyPicO_I,3955
 scepter/modules/model/loss/rec_loss.py,sha256=clrLSbQCt6kh33uaA8kF9T3L-xO-4VOWmcu8TFiYvxo,2178
 scepter/modules/model/metric/__init__.py,sha256=EqZ8qdJG4aMfSdojh7VMaBTfUls4I8dxpnZESusaUOs,286
 scepter/modules/model/metric/base_metric.py,sha256=-uiCGCqnKojvXA9nAz8uYcyzvOnK1HjTnCDelHCnozo,783
 scepter/modules/model/metric/classification.py,sha256=bQ_gYPgaeQgByRrMGVn0qNxdj69UbjS0fnPPZqHvfQI,5076
@@ -135,17 +139,17 @@
 scepter/modules/model/neck/__init__.py,sha256=xtZV4C7EqNWZlftpbZLDeozvWDSLxXCaHFqS3enMDtg,220
 scepter/modules/model/neck/global_average_pooling.py,sha256=kzv8TVtUG5Gw4VWUvtA3FpsB35LfT0ZC9VEUQ3tA29o,1870
 scepter/modules/model/neck/identity.py,sha256=jewod2iymjX1s9ePxaoUR6kl-sVh9uJrsjT9CY7xSvM,918
 scepter/modules/model/network/__init__.py,sha256=TeOJRmt9mYI6GOliYLdVZSBiKaN1V3QS2rKbpOEs1Dc,402
 scepter/modules/model/network/classifier.py,sha256=4T5T4WJkHdkXuqwsjPpafOGAFhm5kmBeqkPiDme36H4,7188
 scepter/modules/model/network/train_module.py,sha256=tJpmsprgYvmncrZBDqNWJWVW5KdxWn4dXX_Bfoh0nb4,1189
 scepter/modules/model/network/autoencoder/__init__.py,sha256=5ZnQ7sjnpRI1KXaM-E8KeyeQl449_e9Kd4SfjzAGsWc,148
-scepter/modules/model/network/autoencoder/ae_kl.py,sha256=OYtac3nfI3j8D2nuOHnpToZUee4tKU4N_6YW9ewesfI,9599
+scepter/modules/model/network/autoencoder/ae_kl.py,sha256=JAWJpK5aaZwGqdTRBMy09tIcmnukPg9UD7ExbS58W4I,9601
 scepter/modules/model/network/diffusion/__init__.py,sha256=hPnNnhWF0r-YgU5KLanOLCFPwcGZO25YOHF6V-E2_wg,211
-scepter/modules/model/network/diffusion/diffusion.py,sha256=_YmUiQslGyjmW-KXEIqaxxlHrJCDlcwxQEsLYxdBzVE,22667
+scepter/modules/model/network/diffusion/diffusion.py,sha256=mytZcp2p86IvSEbaYX1B2X1FfZWuEmoQ59caSLg2Saw,25265
 scepter/modules/model/network/diffusion/schedules.py,sha256=O6Ivs1biFNBliFhscc9dcuRHoU15j5s1i9xmH8jEZeI,6226
 scepter/modules/model/network/diffusion/solvers.py,sha256=pur7SDg2-4cO9ssGergjUl686oC1g2Vu6zdBmOJ2tzI,22561
 scepter/modules/model/network/ldm/__init__.py,sha256=DYmZcVOGKcBUDfCmxbwo9oNUy5KduetaCd8HEN6L25U,385
 scepter/modules/model/network/ldm/ldm.py,sha256=tlB5m_HtmRcHLxa7oAxEhCRQAMYYpL7HU44amhY6_zo,19522
 scepter/modules/model/network/ldm/ldm_sce.py,sha256=NcvEsml6feN7ImPLPk_7ZA1o0H08lbxEMc3k90tDnRs,5860
 scepter/modules/model/network/ldm/ldm_xl.py,sha256=12pGU4UkykHCzodjaBxdvOxuHimBAgRA_tCAgjYq8po,21293
 scepter/modules/model/tokenizer/__init__.py,sha256=Gs2lmyHvhLJREg5iVfuHMWL4JgeF1dUepCyvNY7rHBc,368
@@ -158,121 +162,137 @@
 scepter/modules/model/tuner/tuner_component.py,sha256=jvjLji8kjbr3HdL35SaP30svxgZKG_gKhnYKWjNH-RE,6852
 scepter/modules/model/tuner/tuner_utils.py,sha256=8k9qh3wI1_7T5HnBY5IxzJ6Gderv7qbhyyj36s3A6n4,965
 scepter/modules/model/tuner/sce/__init__.py,sha256=38HST4QX_DcrYl9N519RnmrLjHac2UFp84zsiIFUpCQ,222
 scepter/modules/model/tuner/sce/scetuning.py,sha256=t86P6CO2wanpOxnl9sJhsum8q0q6q4RYu7NHU0xQF_E,6716
 scepter/modules/model/tuner/sce/scetuning_component.py,sha256=XPeH5hBfna2QX-_IkUHoioaB4NfGXawMQeXdV6Vjqsk,2485
 scepter/modules/model/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/modules/model/utils/basic_utils.py,sha256=qZrCY5abuKIEg_5_qvqZOu5_pPlKch-QJCvt9efQ6R0,3344
+scepter/modules/model/utils/data_utils.py,sha256=JfNj_BnhxnIbRFH845oCb9AKN9GeV2YQoAPqa1zfHW0,3776
 scepter/modules/opt/__init__.py,sha256=SDOnU7zqJSWGu1CyYK7dRYku2KB36hsNQjBuJHE1LSo,133
 scepter/modules/opt/lr_schedulers/__init__.py,sha256=nDSd4Fc8gHX_Xdwflu-ACF0dpnaVc8PLaSFtzfCpCGg,298
 scepter/modules/opt/lr_schedulers/base_scheduler.py,sha256=9o6SHelilMdxvHBEpWbnU6MH6sYJUJghUy6gzF1UMgY,227
 scepter/modules/opt/lr_schedulers/define_schedulers.py,sha256=a5W5TYLWG56toXR1nBn-rNNdaWG0kj-x3ORyXiICXRc,3005
 scepter/modules/opt/lr_schedulers/official_schedulers.py,sha256=r81T2IUHGCrpOiw7Vjzcu2YUEwZWsGXK6Rul44WwNsQ,14312
 scepter/modules/opt/lr_schedulers/registry.py,sha256=YD0Z7U7eIHsSvy9d9V_bOGMhbQEJRgN4KDZ0siPldYc,1405
 scepter/modules/opt/lr_schedulers/warmup.py,sha256=Y_sh8yzQpJ4MJY4JZ2Yvhn_55psMDLd-Yt3yugemRbQ,5420
-scepter/modules/opt/optimizers/__init__.py,sha256=nrBWNU5VyePkMSjOKlLvESIMu57YY-RDfkqsiAsYU4o,297
+scepter/modules/opt/optimizers/__init__.py,sha256=wACa7WxPl6nbyeyHRsSRzN0GkubKZ_XAZ5WgXYfnZB4,608
 scepter/modules/opt/optimizers/base_optimizer.py,sha256=BvI5QIossVYBt_4-tJUOEVyNz6GGvkd6z4WnLtXcaYE,226
 scepter/modules/opt/optimizers/official_optimizers.py,sha256=3lBTEaL7iPeg1Hcktyyd7XaoYtcM_hIJsHTVzR4gx0Q,19032
 scepter/modules/opt/optimizers/registry.py,sha256=h4PAtZGKNVWki07Rq58JrQBbDz9lfmSJb1F3_ssC2e4,1388
 scepter/modules/solver/__init__.py,sha256=E9F7wakzeszP9JXqmZbwRCThTPOFfR_HxbkJXVmOEb8,314
 scepter/modules/solver/base_solver.py,sha256=I4zzsAug0fv0aYP9aBLiJ4WYd9QfceghbkFf5xwX8pY,35587
-scepter/modules/solver/diffusion_solver.py,sha256=o7LQp8Xank0BYVEd-S-PmuADvloUVQo4Pp9tU5nKKOE,31580
+scepter/modules/solver/diffusion_solver.py,sha256=aNUWI6HdvDptfzP6LeBxl2vnJDYF8sQGctkTamvOfhM,31755
 scepter/modules/solver/registry.py,sha256=py-UMNwR3MHa-Cf0B_jh3hUIZ7VSoPZGXPnVcgPcbMk,1351
 scepter/modules/solver/train_val_solver.py,sha256=_fUhbuaEJetga5R-3rJjuRVeWVrqEQNPmy1FK0MZPm0,7538
-scepter/modules/solver/hooks/__init__.py,sha256=AASftwS9CFBc-5Y69ZqCv3NiiLSyedlHSRVfgG7IxGQ,1576
+scepter/modules/solver/hooks/__init__.py,sha256=sKIjOVIO3eOXQxRlkpQ_xFRbSDANcOaC3rcijSOJOoI,1654
 scepter/modules/solver/hooks/backward.py,sha256=vmdgigEzLUhu15lyLli3Gh816s2w25Rr-D8-1kEQcaM,3617
-scepter/modules/solver/hooks/checkpoint.py,sha256=DINQ3rfvg14GYqRMBNmFwETbqmnzG1ItK2bGJPrpDcc,11909
-scepter/modules/solver/hooks/data_probe.py,sha256=xDhyAOJxhV5Nvq6B198MeENK4li2lJ5nJnaQVyA0yJE,3745
+scepter/modules/solver/hooks/checkpoint.py,sha256=VEuQ5roI7blP5b0d0Jc-C2OrKV3KaL_Ih3Kvbijfpf8,11993
+scepter/modules/solver/hooks/data_probe.py,sha256=uGRdlpR37Z-Xap4ABN4ymM5X2FODn2pptYbsUsmtpv8,4944
+scepter/modules/solver/hooks/ema.py,sha256=McqGd__JCp69gOPGsuYFyJhbUBZU4uQV_yBzIbmXhc4,2090
 scepter/modules/solver/hooks/hook.py,sha256=4CKZzAKobSbydX2It5wg5wK-Qnu0j0UVnNeV7dvMyoc,611
 scepter/modules/solver/hooks/log.py,sha256=RbwWgv0JkWLbJ3dj-lS5RehdrmzX3B8KQTqvZn_4ueQ,10821
 scepter/modules/solver/hooks/lr.py,sha256=yie_1DmDv0-LKlX1DaaG3vVMn_gDiHQFnLsnAoaVLSc,5202
 scepter/modules/solver/hooks/registry.py,sha256=S_4CniuqNs4rmt-qQ6F4ijEo8c-UVV01njflEw_EEd4,154
 scepter/modules/solver/hooks/safetensors.py,sha256=SveRWQ5cYyXe_6uGEowsojoAkEXesOhz-qXa9EuSC94,2192
 scepter/modules/solver/hooks/sampler.py,sha256=LQ2_7UWCw_JPsbKwhALoR5BEHcRQq8S074L4yj6RIH8,1544
 scepter/modules/transform/__init__.py,sha256=cO5D439eas7hOS__Pc09xaa-k4MZYwy8IGLplLbgEYE,1718
 scepter/modules/transform/augmention.py,sha256=ANrNmx7ERVipdkfkvHRRufbzLP1tktOHgY7fHZ2r8s0,19927
 scepter/modules/transform/compose.py,sha256=2kkiY8_wopn6fPWVZErGs-KcO9TJohWaFt9dFOVDGpU,1011
 scepter/modules/transform/identity.py,sha256=kAb8sfmGjtYsXA9Gp6RfF-MoFDMCfT1ODBHMQZ56DYA,725
-scepter/modules/transform/image.py,sha256=Wwot-nRehYMazOx_ikLOwJtofECjmT1NjdogLdTxiLE,22204
-scepter/modules/transform/io.py,sha256=brdpTVxNoBL4_dxvzqiQnxET84CxBAtDBxzT4Ghn0F4,12198
+scepter/modules/transform/image.py,sha256=9qA5TsUDWIsVFksossBp5luJWOKklBviMBB0R7Jzft0,22592
+scepter/modules/transform/io.py,sha256=gkBomg3Kw7yWH3Mgk3_rSsjaEM5QXD06YDle6mep9L8,12249
 scepter/modules/transform/io_video.py,sha256=JGF5sxgQ6qJk-b6TPBCa5ji0HhAavNG7c6NTbNM3lO0,18995
 scepter/modules/transform/registry.py,sha256=CSfKJqS5ZjGwD-3PKu_tr3dnI9x3Pom1zugUdi6Vesk,1984
 scepter/modules/transform/tensor.py,sha256=8veL-D1xoWOJm3yLEX34Ed6md702AIItiRF_GaNdGZk,9951
 scepter/modules/transform/transform_xl.py,sha256=m6Q99CwF5YUIwuAgYK1D-ps4aXhXoSPHp96nitE3xdA,3201
 scepter/modules/transform/utils.py,sha256=D719oKO9Mjoe_zKPcpYqoiXRBC20x0A9WY7IrCmvMco,1865
 scepter/modules/transform/video.py,sha256=pwEjgopnUxxRSyyx-YjR_HWo-QMR_3khy-9rd6V5Td4,18831
 scepter/modules/utils/__init__.py,sha256=LZn9TqvmCnPtUHH-XP2xLt7T4xDSLepF98vdolKslfQ,154
-scepter/modules/utils/config.py,sha256=e8uJP-3q6fmsR6yjvfD5D1WIoS-vvCshlZme7leiKgA,24633
+scepter/modules/utils/config.py,sha256=TNChnt8AU_xEFP14A-gt-V2l_J-reEvsD3rhGTrKX-A,24691
 scepter/modules/utils/data.py,sha256=5JpG9RFUPxC4lPZw_nTEX6AkgQYHmwCH0kgSQonb0cs,2865
 scepter/modules/utils/directory.py,sha256=0L7k0fKb5l2RGKexM2YC-E2Y3Rniho4ApAP2ioGtCUM,482
 scepter/modules/utils/distribute.py,sha256=mb8poXOyaY5dWX4R3D4GCJfFMNnXfR-XO_5SZ7UZeiU,15775
-scepter/modules/utils/export_model.py,sha256=su17rUFXkpMHy6zrM_IMOzHIdOBgOpcqJ3DiaDJkPBk,3755
-scepter/modules/utils/file_system.py,sha256=TjCqn8PWUzteRd_FRChzUfa7x1sahjarg_4K7gJuP-Y,16852
+scepter/modules/utils/export_model.py,sha256=pbKAVxxA72jVnM_meRp5mAmzwOETjaYM0jW3ygKUBGo,3755
+scepter/modules/utils/file_system.py,sha256=tO__SZJjecSdQ7C6S4pj193MKcqUtjTXQna8rWLPPWo,17112
+scepter/modules/utils/index.py,sha256=T1MLOd1WwFoCKT7jsKOaEtk-bo_e8W6wE_ebso4CkaA,1684
 scepter/modules/utils/logger.py,sha256=7C1aQKKPTJe0unMtrxNszZYvgDxUpcEJRyD7qNg_EUY,5554
 scepter/modules/utils/math_plot.py,sha256=hbT75inJuSqQRezsx5IvDduZGXmWc-dqXyrHIu-YiVE,2735
 scepter/modules/utils/model.py,sha256=_QbbyunhuiNQnf5PpKrW3sv3YkJtKc4pXvfmHHvLX0o,5548
-scepter/modules/utils/probe.py,sha256=EKvFnittOPPDKCPc83zz1AgwQghk5reXJO-qYT4SMvE,17407
+scepter/modules/utils/probe.py,sha256=Dng3k9wtWILrOlgJ-ObC9f4TejS-44RTFbh8IZxquK0,17482
 scepter/modules/utils/registry.py,sha256=yz77lqnJ33MDNULo4uVKFKXABLFiaaQeHx0ytdGxx-k,7995
 scepter/modules/utils/file_clients/__init__.py,sha256=v_KZEJsenhG8MNrRXickswfnEXbUjUo_0X2ljBWZx2c,423
 scepter/modules/utils/file_clients/aliyun_oss_fs.py,sha256=-e4AMnMt3az11WQ8XMW5iQv4iXxy8K2GtgOI7TjY7jo,44598
 scepter/modules/utils/file_clients/base_fs.py,sha256=Jbu0Mzjfgpz1bZ6k7Vy5Ak7ILxroqJeHAT9kKZ8dB_g,11136
 scepter/modules/utils/file_clients/http_fs.py,sha256=Gc6Ynfzod23nylmbEJAL4RhOe701GwujcSQYV09sjvQ,4610
 scepter/modules/utils/file_clients/huggingface_fs.py,sha256=-l37F2s3XeRby2BOcG3qpTFLqpcGgF94KmP2d2VHI60,6249
-scepter/modules/utils/file_clients/local_fs.py,sha256=Ry_xKKPsaHyrYKUWxak38M4w3QTqWmveACNMlAJiwCI,13407
+scepter/modules/utils/file_clients/local_fs.py,sha256=cPV_Zod31Y10lJ3RjsbLh88TZcOTw1X_NmatQrspefY,13560
 scepter/modules/utils/file_clients/modelscope_fs.py,sha256=yY6-66wfK7zY9Z2KwTFV8qNYdy-zBv5pFEkoErmTcnA,7207
 scepter/modules/utils/file_clients/registry.py,sha256=j6qvcbSpVJO82KC34my2Wrwv0ndWrBfdQ_41oM-W_BI,168
 scepter/modules/utils/file_clients/utils.py,sha256=utAoEjwCpf2qdXn5_zSmk4SHCTzE3cKS2nAaergJoRM,1067
 scepter/modules/utils/video_reader/__init__.py,sha256=s3YbM1LNrDuDRdgMPhP_qR0Zw-IOIJkopuSTqDZMCec,324
 scepter/modules/utils/video_reader/frame_sampler.py,sha256=K8zsuTrTYxsjXXHRqUsYkDHtOwL-qrqLKU--UHNAOpw,6201
 scepter/modules/utils/video_reader/video_reader.py,sha256=LUlZp3zslBwsyT1iip-8USkEyuOGcrGuE6uGocXz9gA,5133
 scepter/studio/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/home/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/home/home.py,sha256=vciZwJtQe3p-6I_-xpc3h2IBOJ2wn0Y2o9WlwzFRXcU,1751
 scepter/studio/home/home_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/home/home_ui/component_names.py,sha256=KkKa_SOrg61N8x293n-_6jrKrQ0bXLVdmglXec6RNjk,387
 scepter/studio/home/home_ui/desc_ui.py,sha256=vJM7hgfUaAmbuUHiBgRDoJjZSMpuP0-uMsmVrKJ8NRM,719
 scepter/studio/home/home_ui/guide_ui.py,sha256=DHsW0aXzXvR15cCfs62hjSscKMoZ5Eh8ZSruKB3XK3M,728
 scepter/studio/inference/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/inference/inference.py,sha256=A25DP4ICfOeCoryA7XoTtCVxoc9yOuWVh8YkOJQVgI8,7239
+scepter/studio/inference/inference.py,sha256=ZcC53TizC1-j236od7EHgSjuGKozG49h7F5d3jlTYto,10711
 scepter/studio/inference/inference_manager/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/inference/inference_manager/infer_runer.py,sha256=UJzZIlE66ywXuw_b2ESRunmDwMjpm3-IWnO6P8Firgs,6755
+scepter/studio/inference/inference_manager/infer_runer.py,sha256=ymwnqx7S9dKOB9wOv9BIT0DEEGQECaQFLGVzAabAFjU,7003
 scepter/studio/inference/inference_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/inference/inference_ui/component_names.py,sha256=Ne29xQlqYsVt2NjI6pIjGe4pOT-PotY3m5r9i0bLvTQ,15087
+scepter/studio/inference/inference_ui/component_names.py,sha256=5QBS9cboaha-A5Jo2VK6-_0jBlaQjEnA81DOtx3kPtM,21707
 scepter/studio/inference/inference_ui/control_ui.py,sha256=q5GwqvStDjDemyjCMrJnc-dCx2cWLbuobxMBUbbR-ZQ,8185
 scepter/studio/inference/inference_ui/diffusion_ui.py,sha256=Z2HBT-nPgqAROntzg8UH-iCCAprVfggsHrTePanfKAg,8631
-scepter/studio/inference/inference_ui/gallery_ui.py,sha256=yR3JQl8WIffwwKA_s-hS2adhiUuRLqKl776trHImFuo,11520
-scepter/studio/inference/inference_ui/mantra_ui.py,sha256=7XsZkj3JDDzoEtXcKdtChtHoujwvULtV74IzXu_9ouE,8302
-scepter/studio/inference/inference_ui/model_manage_ui.py,sha256=hXlHGPhrp3j6e4bbc-AMQdlkeMZusU2QLsbhXCwPjZU,11233
+scepter/studio/inference/inference_ui/gallery_ui.py,sha256=3_6pm5zSTedXdVeRfP4516IX0B_Brml1OTZCXn2FKiE,13683
+scepter/studio/inference/inference_ui/largen_ui.py,sha256=ez-DrovhW9g6qs8Kc1bwD0BckNHFnBs3U20QG2i8GcE,22326
+scepter/studio/inference/inference_ui/mantra_ui.py,sha256=2gxI3DVZG9Bczde9O2OjCZFn5dhbTllXIHIB5zhrcOQ,8373
+scepter/studio/inference/inference_ui/model_manage_ui.py,sha256=6rF6Vzk3bXvXV0v87aJ05yYkoM0ItV5XlK7VUQX6mXM,11945
 scepter/studio/inference/inference_ui/refiner_ui.py,sha256=3FbbzQr8duMO-8lyTcNf9-FoZWWPXBE017pzXoSLruc,4281
-scepter/studio/inference/inference_ui/tuner_ui.py,sha256=VKbBY4oLCzfNcw9Ewxvztg4lLXFX3lYN0ynFljDcZoQ,6765
+scepter/studio/inference/inference_ui/tuner_ui.py,sha256=PqEOa6wCkDaosA0kZlE5cYjGQX6T_7kpQ0f4YBCtgE4,9619
 scepter/studio/preprocess/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/preprocess/preprocess.py,sha256=0b21sb-R7Ke5pQhbCeYDCdkwrCIcxSb6_7OLbslfO40,2806
+scepter/studio/preprocess/preprocess.py,sha256=61v9BYjo8CjeyoRCXeTVZZIvaqIE_mwHcWQ6zkqvLAc,2815
 scepter/studio/preprocess/caption_editor_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/preprocess/caption_editor_ui/component_names.py,sha256=0UybSU5PBi68Xc6g0trdLgCwgq9f0zSoi2UJzRKD6XY,7823
-scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py,sha256=yYerlJH9Y50RivZRJt09hxw2bgCeQHFnIaGL7AokJeA,29866
+scepter/studio/preprocess/caption_editor_ui/create_dataset_ui.py,sha256=cwS3M6ekcBOsggoXdwZABCtER1sZFtrIQLAALzc4K_4,32565
 scepter/studio/preprocess/caption_editor_ui/dataset_gallery_ui.py,sha256=v3rikH1FebdCUDf9K3MPIavQtedBCutAckDNAlhVPy8,13405
 scepter/studio/preprocess/caption_editor_ui/export_dataset_ui.py,sha256=lk4k-wjaXNeqcbjEpwTrXniEZ_0WXyEQ1CeF_9dUKaE,6875
 scepter/studio/self_train/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/self_train/self_train.py,sha256=7mZgjNyjxhUgd1i3gg5zHx0C6S-7zZNIKfwfmkctBV8,2618
+scepter/studio/self_train/self_train.py,sha256=haPlS2bNOVMoEDCF7iLnfbMKdzfn3jsH6xBhWMY74ho,2575
 scepter/studio/self_train/scripts/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/self_train/scripts/run_task.py,sha256=k0bq6kPZdIikQohxdk1cqYZM5pdG1J4uy04zPx5vANs,6632
+scepter/studio/self_train/scripts/run_task.py,sha256=JF85l8-C2D0uFTUrWp_pw7eUHxs49kgR6bs7X43WKoE,7438
+scepter/studio/self_train/scripts/sleep.py,sha256=pNhV7zhrQVuzsB3keTVRVDeNYBLwtp0tAewIKme8I2M,148
+scepter/studio/self_train/scripts/trainer.py,sha256=q_7adcA4yLNhkiBVqQ6v-cZmxrHWi4RisPNbPb-isMo,14891
 scepter/studio/self_train/self_train_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/self_train/self_train_ui/component_names.py,sha256=cQFfkRw0rHZOyrspVsssVvD4e98QHlrhkwBxGd_r97c,7958
-scepter/studio/self_train/self_train_ui/inference_ui.py,sha256=tzSroG897l3cxlgDYpoTXZnOYw7so2_Yi4tjle6Nc4s,6843
-scepter/studio/self_train/self_train_ui/trainer_ui.py,sha256=NEWPAITO7uf2uIqpFl82cSYjAbM1OlwlXRRG2YN5ffc,28918
+scepter/studio/self_train/self_train_ui/component_names.py,sha256=BexlTz4MfqKyuaiRjFioLUvwcNPxj4SmPPDFgeJnQO8,8907
+scepter/studio/self_train/self_train_ui/model_ui.py,sha256=34RL89KV5ZuP_tl7ZH3PWtMY2Pk_4YfJLIcHQBoGV34,22179
+scepter/studio/self_train/self_train_ui/trainer_ui.py,sha256=cM3I8mAMhaWQqNRT6uDPfByG58ONPl9L2rX0539H8Mo,32081
 scepter/studio/self_train/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/studio/self_train/utils/config_parser.py,sha256=vGNJa6ZGpbfz-xOdikUG57BIJzTSI8tN8c2S3JYzl70,14826
+scepter/studio/self_train/utils/config_parser.py,sha256=BgUt5BuW9D98ASu6PfEwUTcujUt3fqi4Od3sgBS9feE,12071
+scepter/studio/tuner_manager/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
+scepter/studio/tuner_manager/tuner_manager.py,sha256=5WEg4ofLMHHdeJVhQ7hoznHCljJTUigiIS3DKFgFoNA,1249
+scepter/studio/tuner_manager/manager_ui/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
+scepter/studio/tuner_manager/manager_ui/browser_ui.py,sha256=s5vQRsUgcXfY-hEu6a0-91gTpOX65L4tk3XIZkxKYow,14704
+scepter/studio/tuner_manager/manager_ui/component_names.py,sha256=2hxVqvEXjvbsUeN4BdJiLFlAVxiNt3MvQtSPwurfqqo,1632
+scepter/studio/tuner_manager/manager_ui/info_ui.py,sha256=qj50S8CoZFVWAsu4YX9AzBrMCt7p41NVeQEMrBV1D4A,3226
+scepter/studio/tuner_manager/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
+scepter/studio/tuner_manager/utils/dict.py,sha256=mzVQXIbIKLSmyHL2aGbeV7cYmiX2rdhFEV062cQH1jI,452
+scepter/studio/tuner_manager/utils/path.py,sha256=EbYBAkgzq4g1HhAJ7H7_0rtMj8cNRpUMx0qDqwUk7Hk,217
+scepter/studio/tuner_manager/utils/yaml.py,sha256=x9JnYzgiIW9rvw3GvuqGIk0TIyeLDHJM4xhOU4OsJWg,327
 scepter/studio/utils/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
 scepter/studio/utils/env.py,sha256=x-7Qu2eJodeZO7qjDBxS0IzK3cumpl-JoRDpi2Xxx0Y,589
 scepter/studio/utils/file.py,sha256=fQ__QerL3rESJ-0qzE39cbfJciYAArP0FACXfgacoxI,567
 scepter/studio/utils/singleton.py,sha256=G9BVZxPb-NBvNxXJkxTUof6txosAaE6Pl24P3VLJwxU,276
 scepter/studio/utils/uibase.py,sha256=A9onH8lWu6LUYcBWiPxtq6mtvAKexHUvqOdoNj8wbGg,545
 scepter/tools/__init__.py,sha256=O85d-vRDaH0wtiovk7SG-PXyelCGfTiXrhiUqL3k4bg,74
-scepter/tools/helper.py,sha256=-DtePfE4oW55dPyxc4ySL0ZGP3D2zzNADfG902ZO5Y4,3079
-scepter/tools/run_inference.py,sha256=hub0N2EO82hxBn2KGhOuVh5Yh1H8fdJYFRxvaVGj5N4,9105
-scepter/tools/run_train.py,sha256=nhlCftEHzrdNXErZk0sz2-ANqeXZqcagSh0qAf_Juqo,1623
-scepter/tools/webui.py,sha256=T0ISmQ3gQjpVXbfhjmx0v08pyUSddTHPeDgx5Ox0hAU,4898
-scepter-0.0.3.post1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-scepter-0.0.3.post1.dist-info/METADATA,sha256=PmNhkf_GZrnZkUDFI1qumxeeZH0IrnpzOELyuImF41s,15046
-scepter-0.0.3.post1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-scepter-0.0.3.post1.dist-info/top_level.txt,sha256=McPcsMl7sOYhmjnZsk8LNKmlk5jEH-78WdVVrRy1pqQ,8
-scepter-0.0.3.post1.dist-info/RECORD,,
+scepter/tools/helper.py,sha256=amLZ2C6KJjess9zf5uNPTc_zn59o5iDHHcbpM0H4zF4,3371
+scepter/tools/run_inference.py,sha256=Gnh6GJ4Mo0A6IwUDu1BH7UN4x_XryMoc7QSNhk-Dczk,9409
+scepter/tools/run_train.py,sha256=TGaGesDmnRyLdboXT1SvlDs_BSR4D2iEttoATRyB3Ts,2114
+scepter/tools/webui.py,sha256=Xd5rzURqT_bFFYnMfKLm4WAzM_hEkYHYQ1FR-ZYHPUs,6671
+scepter-0.0.4.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+scepter-0.0.4.dist-info/METADATA,sha256=yjIHVBpHlM3CmCELavSdPho3c_FstCo9k991qfDNIiQ,20162
+scepter-0.0.4.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+scepter-0.0.4.dist-info/top_level.txt,sha256=McPcsMl7sOYhmjnZsk8LNKmlk5jEH-78WdVVrRy1pqQ,8
+scepter-0.0.4.dist-info/RECORD,,
```

